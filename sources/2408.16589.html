<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.16589] CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions</title><meta property="og:description" content="We demonstrate that carefully adjusting the tokenizer of the Whisper speech recognition model significantly improves the precision of word-level timestamps when applying dynamic time warping to the decoder's cross-atteâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.16589">

<!--Generated on Thu Sep  5 13:57:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=1]LaurinWagner
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=1]BernhardThallinger
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=1]MarioZusag




</p>
</div>
<h1 class="ltx_title ltx_title_document">CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">We demonstrate that carefully adjusting the tokenizer of the Whisper speech recognition model significantly improves the precision of word-level timestamps when applying dynamic time warping to the decoder's cross-attention scores. We fine-tune the model to produce more verbatim speech transcriptions and employ several techniques to increase robustness against multiple speakers and background noise. These adjustments achieve state-of-the-art performance on benchmarks for verbatim speech transcription, word segmentation, and the timed detection of filler events, and can further mitigate transcription hallucinations. The code is available open source<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/nyrahealth/CrisperWhisper" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/nyrahealth/CrisperWhisper</a></span></span></span>.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>speech recognition, word-level timestamp precision, disfluency detection
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Training deep-learning models on large-scale, weakly supervised speech datasets have proven very effective for extracting rich representations, which perform well on versatile speech processing tasks, such as automatic speech recognition (ASR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> or speaker verification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Notably, Radford et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> trained Whisper, a sequence-to-sequence (Seq2Seq) transformer model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> on 680,000 hours of weakly supervised speech recognition data, demonstrating strong generalisation abilities across domains, languages and datasets.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> show that Whisper eliminates many filler words, recurring utterances and other artifacts, which Lea et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> refer to as an <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">intended</span> transcription style, suitable for contexts where clarity of intent is prioritized over detailed speech analysis. This style, however, does not support the detection, categorization, or analysis of disfluencies and therefore omits many clinically relevant aspects of speech. <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">Verbatim</span> speech transcriptions capture all articulated utterances and can efficiently be used for clinical assessment of speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. A speech disfluency occurs when there's an interruption in the normal rhythm of speech, typically manifesting as filled pauses, word repetitions, or corrections. Filled pauses, beyond their linguistic interest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, provide insight into the language planning process and are indicators of cognitive load <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Therefore, analyzing the timing and frequency of disfluencies, particularly common fillers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> such as 'uh' and 'um', offers valuable insights into a speaker's cognitive processes. Many clinically relevant biomarkers like speech rate or productive time ratio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> rely on time accurate detection of all aspects of speech. Wagner et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> show that fluency markers derived from timing information alone are sufficient to differentiate between four different aphasia sub-types with an <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S1.p2.1.m1.1a"><msub id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml"><mi id="S1.p2.1.m1.1.1.2" xref="S1.p2.1.m1.1.1.2.cmml">F</mi><mn id="S1.p2.1.m1.1.1.3" xref="S1.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><apply id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p2.1.m1.1.1.1.cmml" xref="S1.p2.1.m1.1.1">subscript</csymbol><ci id="S1.p2.1.m1.1.1.2.cmml" xref="S1.p2.1.m1.1.1.2">ğ¹</ci><cn type="integer" id="S1.p2.1.m1.1.1.3.cmml" xref="S1.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">F_{1}</annotation></semantics></math>-score of 81.6.
Ge et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> developed a filled pause detection dataset and a pipeline combining ASR, a Voice Activity Detection (VAD) model, and a classifier. Speech regions that remained untranscribed by the ASR model are fed to a VAD model. The resulting voice-active regions are then further classified to identify filled pause events. This approach outperforms a convolutional recurrent neural network (CRNN), which operates on log-mel spectograms with 128 bins computed from 1 second clips and a forced-aligner based method called Gentle <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> combined with an acoustic model. However, distinguishing between filler words and other disfluency types such as false starts, remains challenging for this uncontextualized system. In contrast, Whisper's contextual capabilities could offer improved speech analysis capabilities in noisy scenarios.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Whisper does not provide word-level timestamps natively. To this end, WhisperX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> uses force-alignment between Whisper's transcriptions and a connectionist temporal classification (CTC) based phoneme model. This forced phoneme alignment transfers the timing information from the CTC-based Wav2Vec2.0 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> onto Whisper's transcripts. Their VAD-based cut and merge approach allows for segmenting audio efficiently before transcription, improving both speed and accuracy of the transcriptions. However, this method faces challenges, since discrepancies between model transcripts can further degrade timestamp precision. Additionally, employing a second model increases complexity and the VAD-based segmentation approach, while efficient, lacks robustness in noisy environments. Moreover, Wav2Vec2.0 tends to be less noise robust than Whisper, further degrading timestamps in noisy scenarios. Another approach that gained popularity for inferring word-level timestamps uses Dynamic Time Warping(DTW) on the cross-attention scores of the Whisper decoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
Further, in the context of disfluencies, Koenecke et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> examined Whisper's issues of producing hallucinated content when transcribing speech from people with aphasia. Utilizing samples of AphasiaBank <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, they showed that roughly 1% of the produced transcripts contained hallucinated content.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We show that by adjusting Whisper's tokenizer and carefully fine-tuning Whisper on artificial perturbations for noise robustness and single-speaker focus (i) the word-level timestamps can be improved significantly using a single model (ii) the verbatim transcription style reaches state-of-the-art results on more verbatim datasets such as the AMI Meeting Corpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> or TED-LIUM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, while maintaining the same accuracy on datasets such as Librispeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> or CommonVoice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> (iii) the model achieves near perfect filled pause detection accuracy and (iv) we can substantially mitigate hallucinations. We call the resulting model CrisperWhisper for its <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">crisp</span> timestamps and are open-sourcing a synthetic dataset with accurate word-level timestamps as well as the code for CrisperWhisper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>CrisperWhisper</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>DTW for Timestamp Prediction</h3>

<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Intuition</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.7" class="ltx_p">Whisper employs an encoder-decoder structure, where the encoder incorporates multiple Transformer encoder blocks to process audio. Initially, audio is re-sampled to <math id="S2.SS1.SSS1.p1.1.m1.3" class="ltx_Math" alttext="16\text{\,}\mathrm{kHz}" display="inline"><semantics id="S2.SS1.SSS1.p1.1.m1.3a"><mrow id="S2.SS1.SSS1.p1.1.m1.3.3" xref="S2.SS1.SSS1.p1.1.m1.3.3.cmml"><mn id="S2.SS1.SSS1.p1.1.m1.1.1.1.1.1.1" xref="S2.SS1.SSS1.p1.1.m1.1.1.1.1.1.1.cmml">16</mn><mtext id="S2.SS1.SSS1.p1.1.m1.2.2.2.2.2.2" xref="S2.SS1.SSS1.p1.1.m1.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" id="S2.SS1.SSS1.p1.1.m1.3.3.3.3.3.3" xref="S2.SS1.SSS1.p1.1.m1.3.3.3.3.3.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.1.m1.3b"><apply id="S2.SS1.SSS1.p1.1.m1.3.3.cmml" xref="S2.SS1.SSS1.p1.1.m1.3.3"><csymbol cd="latexml" id="S2.SS1.SSS1.p1.1.m1.2.2.2.2.2.2.cmml" xref="S2.SS1.SSS1.p1.1.m1.2.2.2.2.2.2">times</csymbol><cn type="integer" id="S2.SS1.SSS1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS1.p1.1.m1.1.1.1.1.1.1">16</cn><csymbol cd="latexml" id="S2.SS1.SSS1.p1.1.m1.3.3.3.3.3.3.cmml" xref="S2.SS1.SSS1.p1.1.m1.3.3.3.3.3.3">kilohertz</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.1.m1.3c">16\text{\,}\mathrm{kHz}</annotation></semantics></math> and converted into an <math id="S2.SS1.SSS1.p1.2.m2.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S2.SS1.SSS1.p1.2.m2.1a"><mn id="S2.SS1.SSS1.p1.2.m2.1.1" xref="S2.SS1.SSS1.p1.2.m2.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.2.m2.1b"><cn type="integer" id="S2.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS1.p1.2.m2.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.2.m2.1c">80</annotation></semantics></math>-channel log-magnitude Mel spectrogram and downsample via convolutions. The encoder operates on these downsampled spectograms in <math id="S2.SS1.SSS1.p1.3.m3.3" class="ltx_Math" alttext="25\text{\,}\mathrm{ms}" display="inline"><semantics id="S2.SS1.SSS1.p1.3.m3.3a"><mrow id="S2.SS1.SSS1.p1.3.m3.3.3" xref="S2.SS1.SSS1.p1.3.m3.3.3.cmml"><mn id="S2.SS1.SSS1.p1.3.m3.1.1.1.1.1.1" xref="S2.SS1.SSS1.p1.3.m3.1.1.1.1.1.1.cmml">25</mn><mtext id="S2.SS1.SSS1.p1.3.m3.2.2.2.2.2.2" xref="S2.SS1.SSS1.p1.3.m3.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" id="S2.SS1.SSS1.p1.3.m3.3.3.3.3.3.3" xref="S2.SS1.SSS1.p1.3.m3.3.3.3.3.3.3.cmml">ms</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.3.m3.3b"><apply id="S2.SS1.SSS1.p1.3.m3.3.3.cmml" xref="S2.SS1.SSS1.p1.3.m3.3.3"><csymbol cd="latexml" id="S2.SS1.SSS1.p1.3.m3.2.2.2.2.2.2.cmml" xref="S2.SS1.SSS1.p1.3.m3.2.2.2.2.2.2">times</csymbol><cn type="integer" id="S2.SS1.SSS1.p1.3.m3.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS1.p1.3.m3.1.1.1.1.1.1">25</cn><csymbol cd="latexml" id="S2.SS1.SSS1.p1.3.m3.3.3.3.3.3.3.cmml" xref="S2.SS1.SSS1.p1.3.m3.3.3.3.3.3.3">millisecond</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.3.m3.3c">25\text{\,}\mathrm{ms}</annotation></semantics></math> windows with a stride of <math id="S2.SS1.SSS1.p1.4.m4.3" class="ltx_Math" alttext="20\text{\,}\mathrm{ms}" display="inline"><semantics id="S2.SS1.SSS1.p1.4.m4.3a"><mrow id="S2.SS1.SSS1.p1.4.m4.3.3" xref="S2.SS1.SSS1.p1.4.m4.3.3.cmml"><mn id="S2.SS1.SSS1.p1.4.m4.1.1.1.1.1.1" xref="S2.SS1.SSS1.p1.4.m4.1.1.1.1.1.1.cmml">20</mn><mtext id="S2.SS1.SSS1.p1.4.m4.2.2.2.2.2.2" xref="S2.SS1.SSS1.p1.4.m4.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" id="S2.SS1.SSS1.p1.4.m4.3.3.3.3.3.3" xref="S2.SS1.SSS1.p1.4.m4.3.3.3.3.3.3.cmml">ms</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.4.m4.3b"><apply id="S2.SS1.SSS1.p1.4.m4.3.3.cmml" xref="S2.SS1.SSS1.p1.4.m4.3.3"><csymbol cd="latexml" id="S2.SS1.SSS1.p1.4.m4.2.2.2.2.2.2.cmml" xref="S2.SS1.SSS1.p1.4.m4.2.2.2.2.2.2">times</csymbol><cn type="integer" id="S2.SS1.SSS1.p1.4.m4.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1.1.1.1.1">20</cn><csymbol cd="latexml" id="S2.SS1.SSS1.p1.4.m4.3.3.3.3.3.3.cmml" xref="S2.SS1.SSS1.p1.4.m4.3.3.3.3.3.3">millisecond</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.4.m4.3c">20\text{\,}\mathrm{ms}</annotation></semantics></math>, meaning that each processed state represents <math id="S2.SS1.SSS1.p1.5.m5.3" class="ltx_Math" alttext="25\text{\,}\mathrm{ms}" display="inline"><semantics id="S2.SS1.SSS1.p1.5.m5.3a"><mrow id="S2.SS1.SSS1.p1.5.m5.3.3" xref="S2.SS1.SSS1.p1.5.m5.3.3.cmml"><mn id="S2.SS1.SSS1.p1.5.m5.1.1.1.1.1.1" xref="S2.SS1.SSS1.p1.5.m5.1.1.1.1.1.1.cmml">25</mn><mtext id="S2.SS1.SSS1.p1.5.m5.2.2.2.2.2.2" xref="S2.SS1.SSS1.p1.5.m5.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" id="S2.SS1.SSS1.p1.5.m5.3.3.3.3.3.3" xref="S2.SS1.SSS1.p1.5.m5.3.3.3.3.3.3.cmml">ms</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.5.m5.3b"><apply id="S2.SS1.SSS1.p1.5.m5.3.3.cmml" xref="S2.SS1.SSS1.p1.5.m5.3.3"><csymbol cd="latexml" id="S2.SS1.SSS1.p1.5.m5.2.2.2.2.2.2.cmml" xref="S2.SS1.SSS1.p1.5.m5.2.2.2.2.2.2">times</csymbol><cn type="integer" id="S2.SS1.SSS1.p1.5.m5.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1.1.1.1.1">25</cn><csymbol cd="latexml" id="S2.SS1.SSS1.p1.5.m5.3.3.3.3.3.3.cmml" xref="S2.SS1.SSS1.p1.5.m5.3.3.3.3.3.3">millisecond</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.5.m5.3c">25\text{\,}\mathrm{ms}</annotation></semantics></math> of audio, which is shifted by <math id="S2.SS1.SSS1.p1.6.m6.3" class="ltx_Math" alttext="20\text{\,}\mathrm{ms}" display="inline"><semantics id="S2.SS1.SSS1.p1.6.m6.3a"><mrow id="S2.SS1.SSS1.p1.6.m6.3.3" xref="S2.SS1.SSS1.p1.6.m6.3.3.cmml"><mn id="S2.SS1.SSS1.p1.6.m6.1.1.1.1.1.1" xref="S2.SS1.SSS1.p1.6.m6.1.1.1.1.1.1.cmml">20</mn><mtext id="S2.SS1.SSS1.p1.6.m6.2.2.2.2.2.2" xref="S2.SS1.SSS1.p1.6.m6.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" id="S2.SS1.SSS1.p1.6.m6.3.3.3.3.3.3" xref="S2.SS1.SSS1.p1.6.m6.3.3.3.3.3.3.cmml">ms</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.6.m6.3b"><apply id="S2.SS1.SSS1.p1.6.m6.3.3.cmml" xref="S2.SS1.SSS1.p1.6.m6.3.3"><csymbol cd="latexml" id="S2.SS1.SSS1.p1.6.m6.2.2.2.2.2.2.cmml" xref="S2.SS1.SSS1.p1.6.m6.2.2.2.2.2.2">times</csymbol><cn type="integer" id="S2.SS1.SSS1.p1.6.m6.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS1.p1.6.m6.1.1.1.1.1.1">20</cn><csymbol cd="latexml" id="S2.SS1.SSS1.p1.6.m6.3.3.3.3.3.3.cmml" xref="S2.SS1.SSS1.p1.6.m6.3.3.3.3.3.3">millisecond</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.6.m6.3c">20\text{\,}\mathrm{ms}</annotation></semantics></math> steps. The Whisper large model series uses a byte-level Byte Pair Encoding (BPE) text tokenizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, which produces the targets during training. Whisper's Transformer decoder uses cross-attention layers. The resulting cross-attention scores effectively reflect the decoder's focus on specific segments of the encoder output during the token prediction process. The intuition is that this focus is indicative of the decoder's strategy to prioritize encoder output regions most relevant to the current token's prediction. The goal is therefore to use the network's cross attention scores to assess which <math id="S2.SS1.SSS1.p1.7.m7.3" class="ltx_Math" alttext="25\text{\,}\mathrm{ms}" display="inline"><semantics id="S2.SS1.SSS1.p1.7.m7.3a"><mrow id="S2.SS1.SSS1.p1.7.m7.3.3" xref="S2.SS1.SSS1.p1.7.m7.3.3.cmml"><mn id="S2.SS1.SSS1.p1.7.m7.1.1.1.1.1.1" xref="S2.SS1.SSS1.p1.7.m7.1.1.1.1.1.1.cmml">25</mn><mtext id="S2.SS1.SSS1.p1.7.m7.2.2.2.2.2.2" xref="S2.SS1.SSS1.p1.7.m7.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" id="S2.SS1.SSS1.p1.7.m7.3.3.3.3.3.3" xref="S2.SS1.SSS1.p1.7.m7.3.3.3.3.3.3.cmml">ms</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.7.m7.3b"><apply id="S2.SS1.SSS1.p1.7.m7.3.3.cmml" xref="S2.SS1.SSS1.p1.7.m7.3.3"><csymbol cd="latexml" id="S2.SS1.SSS1.p1.7.m7.2.2.2.2.2.2.cmml" xref="S2.SS1.SSS1.p1.7.m7.2.2.2.2.2.2">times</csymbol><cn type="integer" id="S2.SS1.SSS1.p1.7.m7.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS1.p1.7.m7.1.1.1.1.1.1">25</cn><csymbol cd="latexml" id="S2.SS1.SSS1.p1.7.m7.3.3.3.3.3.3.cmml" xref="S2.SS1.SSS1.p1.7.m7.3.3.3.3.3.3">millisecond</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.7.m7.3c">25\text{\,}\mathrm{ms}</annotation></semantics></math> audio frames were important to decode the current token and use the aggregate of these frames as a timestamp <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>DTW and Cost Matrix Construction</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.11" class="ltx_p">We employ the DTW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> algorithm to find the optimal cost, monotonic and continuous alignment between two sequences, requiring a cost matrix to measure the alignment expense between elements of these sequences. In our case these sequences are the encoder outputs <math id="S2.SS1.SSS2.p1.1.m1.4" class="ltx_Math" alttext="E=\{e_{1},e_{2},\ldots,e_{n}\}" display="inline"><semantics id="S2.SS1.SSS2.p1.1.m1.4a"><mrow id="S2.SS1.SSS2.p1.1.m1.4.4" xref="S2.SS1.SSS2.p1.1.m1.4.4.cmml"><mi id="S2.SS1.SSS2.p1.1.m1.4.4.5" xref="S2.SS1.SSS2.p1.1.m1.4.4.5.cmml">E</mi><mo id="S2.SS1.SSS2.p1.1.m1.4.4.4" xref="S2.SS1.SSS2.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S2.SS1.SSS2.p1.1.m1.4.4.3.3" xref="S2.SS1.SSS2.p1.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S2.SS1.SSS2.p1.1.m1.4.4.3.3.4" xref="S2.SS1.SSS2.p1.1.m1.4.4.3.4.cmml">{</mo><msub id="S2.SS1.SSS2.p1.1.m1.2.2.1.1.1" xref="S2.SS1.SSS2.p1.1.m1.2.2.1.1.1.cmml"><mi id="S2.SS1.SSS2.p1.1.m1.2.2.1.1.1.2" xref="S2.SS1.SSS2.p1.1.m1.2.2.1.1.1.2.cmml">e</mi><mn id="S2.SS1.SSS2.p1.1.m1.2.2.1.1.1.3" xref="S2.SS1.SSS2.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.SSS2.p1.1.m1.4.4.3.3.5" xref="S2.SS1.SSS2.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.SS1.SSS2.p1.1.m1.3.3.2.2.2" xref="S2.SS1.SSS2.p1.1.m1.3.3.2.2.2.cmml"><mi id="S2.SS1.SSS2.p1.1.m1.3.3.2.2.2.2" xref="S2.SS1.SSS2.p1.1.m1.3.3.2.2.2.2.cmml">e</mi><mn id="S2.SS1.SSS2.p1.1.m1.3.3.2.2.2.3" xref="S2.SS1.SSS2.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.SSS2.p1.1.m1.4.4.3.3.6" xref="S2.SS1.SSS2.p1.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.SSS2.p1.1.m1.1.1" xref="S2.SS1.SSS2.p1.1.m1.1.1.cmml">â€¦</mi><mo id="S2.SS1.SSS2.p1.1.m1.4.4.3.3.7" xref="S2.SS1.SSS2.p1.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.SS1.SSS2.p1.1.m1.4.4.3.3.3" xref="S2.SS1.SSS2.p1.1.m1.4.4.3.3.3.cmml"><mi id="S2.SS1.SSS2.p1.1.m1.4.4.3.3.3.2" xref="S2.SS1.SSS2.p1.1.m1.4.4.3.3.3.2.cmml">e</mi><mi id="S2.SS1.SSS2.p1.1.m1.4.4.3.3.3.3" xref="S2.SS1.SSS2.p1.1.m1.4.4.3.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="S2.SS1.SSS2.p1.1.m1.4.4.3.3.8" xref="S2.SS1.SSS2.p1.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.1.m1.4b"><apply id="S2.SS1.SSS2.p1.1.m1.4.4.cmml" xref="S2.SS1.SSS2.p1.1.m1.4.4"><eq id="S2.SS1.SSS2.p1.1.m1.4.4.4.cmml" xref="S2.SS1.SSS2.p1.1.m1.4.4.4"></eq><ci id="S2.SS1.SSS2.p1.1.m1.4.4.5.cmml" xref="S2.SS1.SSS2.p1.1.m1.4.4.5">ğ¸</ci><set id="S2.SS1.SSS2.p1.1.m1.4.4.3.4.cmml" xref="S2.SS1.SSS2.p1.1.m1.4.4.3.3"><apply id="S2.SS1.SSS2.p1.1.m1.2.2.1.1.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.1.m1.2.2.1.1.1.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p1.1.m1.2.2.1.1.1.2.cmml" xref="S2.SS1.SSS2.p1.1.m1.2.2.1.1.1.2">ğ‘’</ci><cn type="integer" id="S2.SS1.SSS2.p1.1.m1.2.2.1.1.1.3.cmml" xref="S2.SS1.SSS2.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS1.SSS2.p1.1.m1.3.3.2.2.2.cmml" xref="S2.SS1.SSS2.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.1.m1.3.3.2.2.2.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S2.SS1.SSS2.p1.1.m1.3.3.2.2.2.2.cmml" xref="S2.SS1.SSS2.p1.1.m1.3.3.2.2.2.2">ğ‘’</ci><cn type="integer" id="S2.SS1.SSS2.p1.1.m1.3.3.2.2.2.3.cmml" xref="S2.SS1.SSS2.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1">â€¦</ci><apply id="S2.SS1.SSS2.p1.1.m1.4.4.3.3.3.cmml" xref="S2.SS1.SSS2.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.1.m1.4.4.3.3.3.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S2.SS1.SSS2.p1.1.m1.4.4.3.3.3.2.cmml" xref="S2.SS1.SSS2.p1.1.m1.4.4.3.3.3.2">ğ‘’</ci><ci id="S2.SS1.SSS2.p1.1.m1.4.4.3.3.3.3.cmml" xref="S2.SS1.SSS2.p1.1.m1.4.4.3.3.3.3">ğ‘›</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.1.m1.4c">E=\{e_{1},e_{2},\ldots,e_{n}\}</annotation></semantics></math> representing the encoded acoustic signal and the sequence of decoder token predictions <math id="S2.SS1.SSS2.p1.2.m2.4" class="ltx_Math" alttext="D=\{d_{1},d_{2},\ldots,d_{m}\}" display="inline"><semantics id="S2.SS1.SSS2.p1.2.m2.4a"><mrow id="S2.SS1.SSS2.p1.2.m2.4.4" xref="S2.SS1.SSS2.p1.2.m2.4.4.cmml"><mi id="S2.SS1.SSS2.p1.2.m2.4.4.5" xref="S2.SS1.SSS2.p1.2.m2.4.4.5.cmml">D</mi><mo id="S2.SS1.SSS2.p1.2.m2.4.4.4" xref="S2.SS1.SSS2.p1.2.m2.4.4.4.cmml">=</mo><mrow id="S2.SS1.SSS2.p1.2.m2.4.4.3.3" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.4.cmml"><mo stretchy="false" id="S2.SS1.SSS2.p1.2.m2.4.4.3.3.4" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.4.cmml">{</mo><msub id="S2.SS1.SSS2.p1.2.m2.2.2.1.1.1" xref="S2.SS1.SSS2.p1.2.m2.2.2.1.1.1.cmml"><mi id="S2.SS1.SSS2.p1.2.m2.2.2.1.1.1.2" xref="S2.SS1.SSS2.p1.2.m2.2.2.1.1.1.2.cmml">d</mi><mn id="S2.SS1.SSS2.p1.2.m2.2.2.1.1.1.3" xref="S2.SS1.SSS2.p1.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.SSS2.p1.2.m2.4.4.3.3.5" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S2.SS1.SSS2.p1.2.m2.3.3.2.2.2" xref="S2.SS1.SSS2.p1.2.m2.3.3.2.2.2.cmml"><mi id="S2.SS1.SSS2.p1.2.m2.3.3.2.2.2.2" xref="S2.SS1.SSS2.p1.2.m2.3.3.2.2.2.2.cmml">d</mi><mn id="S2.SS1.SSS2.p1.2.m2.3.3.2.2.2.3" xref="S2.SS1.SSS2.p1.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.SSS2.p1.2.m2.4.4.3.3.6" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.SSS2.p1.2.m2.1.1" xref="S2.SS1.SSS2.p1.2.m2.1.1.cmml">â€¦</mi><mo id="S2.SS1.SSS2.p1.2.m2.4.4.3.3.7" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S2.SS1.SSS2.p1.2.m2.4.4.3.3.3" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.3.3.cmml"><mi id="S2.SS1.SSS2.p1.2.m2.4.4.3.3.3.2" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.3.3.2.cmml">d</mi><mi id="S2.SS1.SSS2.p1.2.m2.4.4.3.3.3.3" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.3.3.3.cmml">m</mi></msub><mo stretchy="false" id="S2.SS1.SSS2.p1.2.m2.4.4.3.3.8" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.2.m2.4b"><apply id="S2.SS1.SSS2.p1.2.m2.4.4.cmml" xref="S2.SS1.SSS2.p1.2.m2.4.4"><eq id="S2.SS1.SSS2.p1.2.m2.4.4.4.cmml" xref="S2.SS1.SSS2.p1.2.m2.4.4.4"></eq><ci id="S2.SS1.SSS2.p1.2.m2.4.4.5.cmml" xref="S2.SS1.SSS2.p1.2.m2.4.4.5">ğ·</ci><set id="S2.SS1.SSS2.p1.2.m2.4.4.3.4.cmml" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.3"><apply id="S2.SS1.SSS2.p1.2.m2.2.2.1.1.1.cmml" xref="S2.SS1.SSS2.p1.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.2.m2.2.2.1.1.1.1.cmml" xref="S2.SS1.SSS2.p1.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p1.2.m2.2.2.1.1.1.2.cmml" xref="S2.SS1.SSS2.p1.2.m2.2.2.1.1.1.2">ğ‘‘</ci><cn type="integer" id="S2.SS1.SSS2.p1.2.m2.2.2.1.1.1.3.cmml" xref="S2.SS1.SSS2.p1.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS1.SSS2.p1.2.m2.3.3.2.2.2.cmml" xref="S2.SS1.SSS2.p1.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.2.m2.3.3.2.2.2.1.cmml" xref="S2.SS1.SSS2.p1.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S2.SS1.SSS2.p1.2.m2.3.3.2.2.2.2.cmml" xref="S2.SS1.SSS2.p1.2.m2.3.3.2.2.2.2">ğ‘‘</ci><cn type="integer" id="S2.SS1.SSS2.p1.2.m2.3.3.2.2.2.3.cmml" xref="S2.SS1.SSS2.p1.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1">â€¦</ci><apply id="S2.SS1.SSS2.p1.2.m2.4.4.3.3.3.cmml" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.2.m2.4.4.3.3.3.1.cmml" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S2.SS1.SSS2.p1.2.m2.4.4.3.3.3.2.cmml" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.3.3.2">ğ‘‘</ci><ci id="S2.SS1.SSS2.p1.2.m2.4.4.3.3.3.3.cmml" xref="S2.SS1.SSS2.p1.2.m2.4.4.3.3.3.3">ğ‘š</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.2.m2.4c">D=\{d_{1},d_{2},\ldots,d_{m}\}</annotation></semantics></math>. Given a set of suitable attention heads from the decoder <math id="S2.SS1.SSS2.p1.3.m3.4" class="ltx_Math" alttext="H=\{h_{1},h_{2},\ldots,h_{l}\}" display="inline"><semantics id="S2.SS1.SSS2.p1.3.m3.4a"><mrow id="S2.SS1.SSS2.p1.3.m3.4.4" xref="S2.SS1.SSS2.p1.3.m3.4.4.cmml"><mi id="S2.SS1.SSS2.p1.3.m3.4.4.5" xref="S2.SS1.SSS2.p1.3.m3.4.4.5.cmml">H</mi><mo id="S2.SS1.SSS2.p1.3.m3.4.4.4" xref="S2.SS1.SSS2.p1.3.m3.4.4.4.cmml">=</mo><mrow id="S2.SS1.SSS2.p1.3.m3.4.4.3.3" xref="S2.SS1.SSS2.p1.3.m3.4.4.3.4.cmml"><mo stretchy="false" id="S2.SS1.SSS2.p1.3.m3.4.4.3.3.4" xref="S2.SS1.SSS2.p1.3.m3.4.4.3.4.cmml">{</mo><msub id="S2.SS1.SSS2.p1.3.m3.2.2.1.1.1" xref="S2.SS1.SSS2.p1.3.m3.2.2.1.1.1.cmml"><mi id="S2.SS1.SSS2.p1.3.m3.2.2.1.1.1.2" xref="S2.SS1.SSS2.p1.3.m3.2.2.1.1.1.2.cmml">h</mi><mn id="S2.SS1.SSS2.p1.3.m3.2.2.1.1.1.3" xref="S2.SS1.SSS2.p1.3.m3.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.SSS2.p1.3.m3.4.4.3.3.5" xref="S2.SS1.SSS2.p1.3.m3.4.4.3.4.cmml">,</mo><msub id="S2.SS1.SSS2.p1.3.m3.3.3.2.2.2" xref="S2.SS1.SSS2.p1.3.m3.3.3.2.2.2.cmml"><mi id="S2.SS1.SSS2.p1.3.m3.3.3.2.2.2.2" xref="S2.SS1.SSS2.p1.3.m3.3.3.2.2.2.2.cmml">h</mi><mn id="S2.SS1.SSS2.p1.3.m3.3.3.2.2.2.3" xref="S2.SS1.SSS2.p1.3.m3.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.SSS2.p1.3.m3.4.4.3.3.6" xref="S2.SS1.SSS2.p1.3.m3.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.SSS2.p1.3.m3.1.1" xref="S2.SS1.SSS2.p1.3.m3.1.1.cmml">â€¦</mi><mo id="S2.SS1.SSS2.p1.3.m3.4.4.3.3.7" xref="S2.SS1.SSS2.p1.3.m3.4.4.3.4.cmml">,</mo><msub id="S2.SS1.SSS2.p1.3.m3.4.4.3.3.3" xref="S2.SS1.SSS2.p1.3.m3.4.4.3.3.3.cmml"><mi id="S2.SS1.SSS2.p1.3.m3.4.4.3.3.3.2" xref="S2.SS1.SSS2.p1.3.m3.4.4.3.3.3.2.cmml">h</mi><mi id="S2.SS1.SSS2.p1.3.m3.4.4.3.3.3.3" xref="S2.SS1.SSS2.p1.3.m3.4.4.3.3.3.3.cmml">l</mi></msub><mo stretchy="false" id="S2.SS1.SSS2.p1.3.m3.4.4.3.3.8" xref="S2.SS1.SSS2.p1.3.m3.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.3.m3.4b"><apply id="S2.SS1.SSS2.p1.3.m3.4.4.cmml" xref="S2.SS1.SSS2.p1.3.m3.4.4"><eq id="S2.SS1.SSS2.p1.3.m3.4.4.4.cmml" xref="S2.SS1.SSS2.p1.3.m3.4.4.4"></eq><ci id="S2.SS1.SSS2.p1.3.m3.4.4.5.cmml" xref="S2.SS1.SSS2.p1.3.m3.4.4.5">ğ»</ci><set id="S2.SS1.SSS2.p1.3.m3.4.4.3.4.cmml" xref="S2.SS1.SSS2.p1.3.m3.4.4.3.3"><apply id="S2.SS1.SSS2.p1.3.m3.2.2.1.1.1.cmml" xref="S2.SS1.SSS2.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.3.m3.2.2.1.1.1.1.cmml" xref="S2.SS1.SSS2.p1.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p1.3.m3.2.2.1.1.1.2.cmml" xref="S2.SS1.SSS2.p1.3.m3.2.2.1.1.1.2">â„</ci><cn type="integer" id="S2.SS1.SSS2.p1.3.m3.2.2.1.1.1.3.cmml" xref="S2.SS1.SSS2.p1.3.m3.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS1.SSS2.p1.3.m3.3.3.2.2.2.cmml" xref="S2.SS1.SSS2.p1.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.3.m3.3.3.2.2.2.1.cmml" xref="S2.SS1.SSS2.p1.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S2.SS1.SSS2.p1.3.m3.3.3.2.2.2.2.cmml" xref="S2.SS1.SSS2.p1.3.m3.3.3.2.2.2.2">â„</ci><cn type="integer" id="S2.SS1.SSS2.p1.3.m3.3.3.2.2.2.3.cmml" xref="S2.SS1.SSS2.p1.3.m3.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS2.p1.3.m3.1.1">â€¦</ci><apply id="S2.SS1.SSS2.p1.3.m3.4.4.3.3.3.cmml" xref="S2.SS1.SSS2.p1.3.m3.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.3.m3.4.4.3.3.3.1.cmml" xref="S2.SS1.SSS2.p1.3.m3.4.4.3.3.3">subscript</csymbol><ci id="S2.SS1.SSS2.p1.3.m3.4.4.3.3.3.2.cmml" xref="S2.SS1.SSS2.p1.3.m3.4.4.3.3.3.2">â„</ci><ci id="S2.SS1.SSS2.p1.3.m3.4.4.3.3.3.3.cmml" xref="S2.SS1.SSS2.p1.3.m3.4.4.3.3.3.3">ğ‘™</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.3.m3.4c">H=\{h_{1},h_{2},\ldots,h_{l}\}</annotation></semantics></math> the cost matrix is defined as follows. Each of the <math id="S2.SS1.SSS2.p1.4.m4.1" class="ltx_Math" alttext="d_{i}" display="inline"><semantics id="S2.SS1.SSS2.p1.4.m4.1a"><msub id="S2.SS1.SSS2.p1.4.m4.1.1" xref="S2.SS1.SSS2.p1.4.m4.1.1.cmml"><mi id="S2.SS1.SSS2.p1.4.m4.1.1.2" xref="S2.SS1.SSS2.p1.4.m4.1.1.2.cmml">d</mi><mi id="S2.SS1.SSS2.p1.4.m4.1.1.3" xref="S2.SS1.SSS2.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.4.m4.1b"><apply id="S2.SS1.SSS2.p1.4.m4.1.1.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.4.m4.1.1.1.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p1.4.m4.1.1.2.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1.2">ğ‘‘</ci><ci id="S2.SS1.SSS2.p1.4.m4.1.1.3.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.4.m4.1c">d_{i}</annotation></semantics></math> is associated with a set of cross-attention vectors <math id="S2.SS1.SSS2.p1.5.m5.4" class="ltx_Math" alttext="A_{i}=\{a_{i1},a_{i2},\ldots,a_{il}\}" display="inline"><semantics id="S2.SS1.SSS2.p1.5.m5.4a"><mrow id="S2.SS1.SSS2.p1.5.m5.4.4" xref="S2.SS1.SSS2.p1.5.m5.4.4.cmml"><msub id="S2.SS1.SSS2.p1.5.m5.4.4.5" xref="S2.SS1.SSS2.p1.5.m5.4.4.5.cmml"><mi id="S2.SS1.SSS2.p1.5.m5.4.4.5.2" xref="S2.SS1.SSS2.p1.5.m5.4.4.5.2.cmml">A</mi><mi id="S2.SS1.SSS2.p1.5.m5.4.4.5.3" xref="S2.SS1.SSS2.p1.5.m5.4.4.5.3.cmml">i</mi></msub><mo id="S2.SS1.SSS2.p1.5.m5.4.4.4" xref="S2.SS1.SSS2.p1.5.m5.4.4.4.cmml">=</mo><mrow id="S2.SS1.SSS2.p1.5.m5.4.4.3.3" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.4.cmml"><mo stretchy="false" id="S2.SS1.SSS2.p1.5.m5.4.4.3.3.4" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.4.cmml">{</mo><msub id="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1" xref="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.cmml"><mi id="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.2" xref="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.2.cmml">a</mi><mrow id="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.3" xref="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.3.cmml"><mi id="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.3.2" xref="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.3.1" xref="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.3.1.cmml">â€‹</mo><mn id="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.3.3" xref="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S2.SS1.SSS2.p1.5.m5.4.4.3.3.5" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.4.cmml">,</mo><msub id="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2" xref="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.cmml"><mi id="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.2" xref="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.2.cmml">a</mi><mrow id="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.3" xref="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.3.cmml"><mi id="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.3.2" xref="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.3.1" xref="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.3.1.cmml">â€‹</mo><mn id="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.3.3" xref="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.3.3.cmml">2</mn></mrow></msub><mo id="S2.SS1.SSS2.p1.5.m5.4.4.3.3.6" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.SSS2.p1.5.m5.1.1" xref="S2.SS1.SSS2.p1.5.m5.1.1.cmml">â€¦</mi><mo id="S2.SS1.SSS2.p1.5.m5.4.4.3.3.7" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.4.cmml">,</mo><msub id="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.cmml"><mi id="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.2" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.2.cmml">a</mi><mrow id="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.3" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.3.cmml"><mi id="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.3.2" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.3.1" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.3.1.cmml">â€‹</mo><mi id="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.3.3" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.3.3.cmml">l</mi></mrow></msub><mo stretchy="false" id="S2.SS1.SSS2.p1.5.m5.4.4.3.3.8" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.5.m5.4b"><apply id="S2.SS1.SSS2.p1.5.m5.4.4.cmml" xref="S2.SS1.SSS2.p1.5.m5.4.4"><eq id="S2.SS1.SSS2.p1.5.m5.4.4.4.cmml" xref="S2.SS1.SSS2.p1.5.m5.4.4.4"></eq><apply id="S2.SS1.SSS2.p1.5.m5.4.4.5.cmml" xref="S2.SS1.SSS2.p1.5.m5.4.4.5"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.5.m5.4.4.5.1.cmml" xref="S2.SS1.SSS2.p1.5.m5.4.4.5">subscript</csymbol><ci id="S2.SS1.SSS2.p1.5.m5.4.4.5.2.cmml" xref="S2.SS1.SSS2.p1.5.m5.4.4.5.2">ğ´</ci><ci id="S2.SS1.SSS2.p1.5.m5.4.4.5.3.cmml" xref="S2.SS1.SSS2.p1.5.m5.4.4.5.3">ğ‘–</ci></apply><set id="S2.SS1.SSS2.p1.5.m5.4.4.3.4.cmml" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.3"><apply id="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.cmml" xref="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.1.cmml" xref="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.2.cmml" xref="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.2">ğ‘</ci><apply id="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.3.cmml" xref="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.3"><times id="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.3.1.cmml" xref="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.3.1"></times><ci id="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.3.2.cmml" xref="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.3.3.cmml" xref="S2.SS1.SSS2.p1.5.m5.2.2.1.1.1.3.3">1</cn></apply></apply><apply id="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.cmml" xref="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.1.cmml" xref="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2">subscript</csymbol><ci id="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.2.cmml" xref="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.2">ğ‘</ci><apply id="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.3.cmml" xref="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.3"><times id="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.3.1.cmml" xref="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.3.1"></times><ci id="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.3.2.cmml" xref="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.3.2">ğ‘–</ci><cn type="integer" id="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.3.3.cmml" xref="S2.SS1.SSS2.p1.5.m5.3.3.2.2.2.3.3">2</cn></apply></apply><ci id="S2.SS1.SSS2.p1.5.m5.1.1.cmml" xref="S2.SS1.SSS2.p1.5.m5.1.1">â€¦</ci><apply id="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.cmml" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.1.cmml" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3">subscript</csymbol><ci id="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.2.cmml" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.2">ğ‘</ci><apply id="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.3.cmml" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.3"><times id="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.3.1.cmml" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.3.1"></times><ci id="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.3.2.cmml" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.3.2">ğ‘–</ci><ci id="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.3.3.cmml" xref="S2.SS1.SSS2.p1.5.m5.4.4.3.3.3.3.3">ğ‘™</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.5.m5.4c">A_{i}=\{a_{i1},a_{i2},\ldots,a_{il}\}</annotation></semantics></math>, where <math id="S2.SS1.SSS2.p1.6.m6.1" class="ltx_Math" alttext="a_{ik}" display="inline"><semantics id="S2.SS1.SSS2.p1.6.m6.1a"><msub id="S2.SS1.SSS2.p1.6.m6.1.1" xref="S2.SS1.SSS2.p1.6.m6.1.1.cmml"><mi id="S2.SS1.SSS2.p1.6.m6.1.1.2" xref="S2.SS1.SSS2.p1.6.m6.1.1.2.cmml">a</mi><mrow id="S2.SS1.SSS2.p1.6.m6.1.1.3" xref="S2.SS1.SSS2.p1.6.m6.1.1.3.cmml"><mi id="S2.SS1.SSS2.p1.6.m6.1.1.3.2" xref="S2.SS1.SSS2.p1.6.m6.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.6.m6.1.1.3.1" xref="S2.SS1.SSS2.p1.6.m6.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS1.SSS2.p1.6.m6.1.1.3.3" xref="S2.SS1.SSS2.p1.6.m6.1.1.3.3.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.6.m6.1b"><apply id="S2.SS1.SSS2.p1.6.m6.1.1.cmml" xref="S2.SS1.SSS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.6.m6.1.1.1.cmml" xref="S2.SS1.SSS2.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p1.6.m6.1.1.2.cmml" xref="S2.SS1.SSS2.p1.6.m6.1.1.2">ğ‘</ci><apply id="S2.SS1.SSS2.p1.6.m6.1.1.3.cmml" xref="S2.SS1.SSS2.p1.6.m6.1.1.3"><times id="S2.SS1.SSS2.p1.6.m6.1.1.3.1.cmml" xref="S2.SS1.SSS2.p1.6.m6.1.1.3.1"></times><ci id="S2.SS1.SSS2.p1.6.m6.1.1.3.2.cmml" xref="S2.SS1.SSS2.p1.6.m6.1.1.3.2">ğ‘–</ci><ci id="S2.SS1.SSS2.p1.6.m6.1.1.3.3.cmml" xref="S2.SS1.SSS2.p1.6.m6.1.1.3.3">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.6.m6.1c">a_{ik}</annotation></semantics></math> denotes the attention score from the <math id="S2.SS1.SSS2.p1.7.m7.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.SSS2.p1.7.m7.1a"><mi id="S2.SS1.SSS2.p1.7.m7.1.1" xref="S2.SS1.SSS2.p1.7.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.7.m7.1b"><ci id="S2.SS1.SSS2.p1.7.m7.1.1.cmml" xref="S2.SS1.SSS2.p1.7.m7.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.7.m7.1c">k</annotation></semantics></math>-th attention head when decoding the <math id="S2.SS1.SSS2.p1.8.m8.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.SSS2.p1.8.m8.1a"><mi id="S2.SS1.SSS2.p1.8.m8.1.1" xref="S2.SS1.SSS2.p1.8.m8.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.8.m8.1b"><ci id="S2.SS1.SSS2.p1.8.m8.1.1.cmml" xref="S2.SS1.SSS2.p1.8.m8.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.8.m8.1c">i</annotation></semantics></math>-th token, with each <math id="S2.SS1.SSS2.p1.9.m9.1" class="ltx_Math" alttext="a_{ik}\in\mathbb{R}^{n}" display="inline"><semantics id="S2.SS1.SSS2.p1.9.m9.1a"><mrow id="S2.SS1.SSS2.p1.9.m9.1.1" xref="S2.SS1.SSS2.p1.9.m9.1.1.cmml"><msub id="S2.SS1.SSS2.p1.9.m9.1.1.2" xref="S2.SS1.SSS2.p1.9.m9.1.1.2.cmml"><mi id="S2.SS1.SSS2.p1.9.m9.1.1.2.2" xref="S2.SS1.SSS2.p1.9.m9.1.1.2.2.cmml">a</mi><mrow id="S2.SS1.SSS2.p1.9.m9.1.1.2.3" xref="S2.SS1.SSS2.p1.9.m9.1.1.2.3.cmml"><mi id="S2.SS1.SSS2.p1.9.m9.1.1.2.3.2" xref="S2.SS1.SSS2.p1.9.m9.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.9.m9.1.1.2.3.1" xref="S2.SS1.SSS2.p1.9.m9.1.1.2.3.1.cmml">â€‹</mo><mi id="S2.SS1.SSS2.p1.9.m9.1.1.2.3.3" xref="S2.SS1.SSS2.p1.9.m9.1.1.2.3.3.cmml">k</mi></mrow></msub><mo id="S2.SS1.SSS2.p1.9.m9.1.1.1" xref="S2.SS1.SSS2.p1.9.m9.1.1.1.cmml">âˆˆ</mo><msup id="S2.SS1.SSS2.p1.9.m9.1.1.3" xref="S2.SS1.SSS2.p1.9.m9.1.1.3.cmml"><mi id="S2.SS1.SSS2.p1.9.m9.1.1.3.2" xref="S2.SS1.SSS2.p1.9.m9.1.1.3.2.cmml">â„</mi><mi id="S2.SS1.SSS2.p1.9.m9.1.1.3.3" xref="S2.SS1.SSS2.p1.9.m9.1.1.3.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.9.m9.1b"><apply id="S2.SS1.SSS2.p1.9.m9.1.1.cmml" xref="S2.SS1.SSS2.p1.9.m9.1.1"><in id="S2.SS1.SSS2.p1.9.m9.1.1.1.cmml" xref="S2.SS1.SSS2.p1.9.m9.1.1.1"></in><apply id="S2.SS1.SSS2.p1.9.m9.1.1.2.cmml" xref="S2.SS1.SSS2.p1.9.m9.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.9.m9.1.1.2.1.cmml" xref="S2.SS1.SSS2.p1.9.m9.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS2.p1.9.m9.1.1.2.2.cmml" xref="S2.SS1.SSS2.p1.9.m9.1.1.2.2">ğ‘</ci><apply id="S2.SS1.SSS2.p1.9.m9.1.1.2.3.cmml" xref="S2.SS1.SSS2.p1.9.m9.1.1.2.3"><times id="S2.SS1.SSS2.p1.9.m9.1.1.2.3.1.cmml" xref="S2.SS1.SSS2.p1.9.m9.1.1.2.3.1"></times><ci id="S2.SS1.SSS2.p1.9.m9.1.1.2.3.2.cmml" xref="S2.SS1.SSS2.p1.9.m9.1.1.2.3.2">ğ‘–</ci><ci id="S2.SS1.SSS2.p1.9.m9.1.1.2.3.3.cmml" xref="S2.SS1.SSS2.p1.9.m9.1.1.2.3.3">ğ‘˜</ci></apply></apply><apply id="S2.SS1.SSS2.p1.9.m9.1.1.3.cmml" xref="S2.SS1.SSS2.p1.9.m9.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.9.m9.1.1.3.1.cmml" xref="S2.SS1.SSS2.p1.9.m9.1.1.3">superscript</csymbol><ci id="S2.SS1.SSS2.p1.9.m9.1.1.3.2.cmml" xref="S2.SS1.SSS2.p1.9.m9.1.1.3.2">â„</ci><ci id="S2.SS1.SSS2.p1.9.m9.1.1.3.3.cmml" xref="S2.SS1.SSS2.p1.9.m9.1.1.3.3">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.9.m9.1c">a_{ik}\in\mathbb{R}^{n}</annotation></semantics></math>. We average these vectors (<math id="S2.SS1.SSS2.p1.10.m10.1" class="ltx_Math" alttext="\bar{A}_{i}=\frac{1}{l}\sum_{k=1}^{l}a_{ik}" display="inline"><semantics id="S2.SS1.SSS2.p1.10.m10.1a"><mrow id="S2.SS1.SSS2.p1.10.m10.1.1" xref="S2.SS1.SSS2.p1.10.m10.1.1.cmml"><msub id="S2.SS1.SSS2.p1.10.m10.1.1.2" xref="S2.SS1.SSS2.p1.10.m10.1.1.2.cmml"><mover accent="true" id="S2.SS1.SSS2.p1.10.m10.1.1.2.2" xref="S2.SS1.SSS2.p1.10.m10.1.1.2.2.cmml"><mi id="S2.SS1.SSS2.p1.10.m10.1.1.2.2.2" xref="S2.SS1.SSS2.p1.10.m10.1.1.2.2.2.cmml">A</mi><mo id="S2.SS1.SSS2.p1.10.m10.1.1.2.2.1" xref="S2.SS1.SSS2.p1.10.m10.1.1.2.2.1.cmml">Â¯</mo></mover><mi id="S2.SS1.SSS2.p1.10.m10.1.1.2.3" xref="S2.SS1.SSS2.p1.10.m10.1.1.2.3.cmml">i</mi></msub><mo id="S2.SS1.SSS2.p1.10.m10.1.1.1" xref="S2.SS1.SSS2.p1.10.m10.1.1.1.cmml">=</mo><mrow id="S2.SS1.SSS2.p1.10.m10.1.1.3" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.cmml"><mfrac id="S2.SS1.SSS2.p1.10.m10.1.1.3.2" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.2.cmml"><mn id="S2.SS1.SSS2.p1.10.m10.1.1.3.2.2" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.2.2.cmml">1</mn><mi id="S2.SS1.SSS2.p1.10.m10.1.1.3.2.3" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.2.3.cmml">l</mi></mfrac><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.10.m10.1.1.3.1" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.1.cmml">â€‹</mo><mrow id="S2.SS1.SSS2.p1.10.m10.1.1.3.3" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.cmml"><msubsup id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.cmml"><mo id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.2" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.2.cmml">âˆ‘</mo><mrow id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.3" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.3.cmml"><mi id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.3.2" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.3.2.cmml">k</mi><mo id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.3.1" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.3.3" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.3" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.3.cmml">l</mi></msubsup><msub id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.cmml"><mi id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.2" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.2.cmml">a</mi><mrow id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.3" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.3.cmml"><mi id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.3.2" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.3.1" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.3.1.cmml">â€‹</mo><mi id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.3.3" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.3.3.cmml">k</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.10.m10.1b"><apply id="S2.SS1.SSS2.p1.10.m10.1.1.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1"><eq id="S2.SS1.SSS2.p1.10.m10.1.1.1.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.1"></eq><apply id="S2.SS1.SSS2.p1.10.m10.1.1.2.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.10.m10.1.1.2.1.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.2">subscript</csymbol><apply id="S2.SS1.SSS2.p1.10.m10.1.1.2.2.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.2.2"><ci id="S2.SS1.SSS2.p1.10.m10.1.1.2.2.1.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.2.2.1">Â¯</ci><ci id="S2.SS1.SSS2.p1.10.m10.1.1.2.2.2.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.2.2.2">ğ´</ci></apply><ci id="S2.SS1.SSS2.p1.10.m10.1.1.2.3.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.2.3">ğ‘–</ci></apply><apply id="S2.SS1.SSS2.p1.10.m10.1.1.3.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3"><times id="S2.SS1.SSS2.p1.10.m10.1.1.3.1.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.1"></times><apply id="S2.SS1.SSS2.p1.10.m10.1.1.3.2.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.2"><divide id="S2.SS1.SSS2.p1.10.m10.1.1.3.2.1.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.2"></divide><cn type="integer" id="S2.SS1.SSS2.p1.10.m10.1.1.3.2.2.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.2.2">1</cn><ci id="S2.SS1.SSS2.p1.10.m10.1.1.3.2.3.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.2.3">ğ‘™</ci></apply><apply id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3"><apply id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.1.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1">superscript</csymbol><apply id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.1.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1">subscript</csymbol><sum id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.2.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.2"></sum><apply id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.3.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.3"><eq id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.3.1.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.3.1"></eq><ci id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.3.2.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.3.2">ğ‘˜</ci><cn type="integer" id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.3.3.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.3.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.1.3">ğ‘™</ci></apply><apply id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.1.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2">subscript</csymbol><ci id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.2.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.2">ğ‘</ci><apply id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.3.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.3"><times id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.3.1.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.3.1"></times><ci id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.3.2.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.3.2">ğ‘–</ci><ci id="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.3.3.cmml" xref="S2.SS1.SSS2.p1.10.m10.1.1.3.3.2.3.3">ğ‘˜</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.10.m10.1c">\bar{A}_{i}=\frac{1}{l}\sum_{k=1}^{l}a_{ik}</annotation></semantics></math>) and normalize them to construct the cost matrix <math id="S2.SS1.SSS2.p1.11.m11.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S2.SS1.SSS2.p1.11.m11.1a"><mi id="S2.SS1.SSS2.p1.11.m11.1.1" xref="S2.SS1.SSS2.p1.11.m11.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.11.m11.1b"><ci id="S2.SS1.SSS2.p1.11.m11.1.1.cmml" xref="S2.SS1.SSS2.p1.11.m11.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.11.m11.1c">C</annotation></semantics></math>:</p>
<table id="S2.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex1.m1.1" class="ltx_Math" alttext="C:=-\begin{bmatrix}\frac{\bar{A}_{1}}{\left\lVert\bar{A}_{1}\right\rVert_{2}}\\
\vdots\\
\frac{\bar{A}_{m}}{\left\lVert\bar{A}_{m}\right\rVert_{2}}\end{bmatrix}" display="block"><semantics id="S2.Ex1.m1.1a"><mrow id="S2.Ex1.m1.1.2" xref="S2.Ex1.m1.1.2.cmml"><mi id="S2.Ex1.m1.1.2.2" xref="S2.Ex1.m1.1.2.2.cmml">C</mi><mo lspace="0.278em" rspace="0.278em" id="S2.Ex1.m1.1.2.1" xref="S2.Ex1.m1.1.2.1.cmml">:=</mo><mrow id="S2.Ex1.m1.1.2.3" xref="S2.Ex1.m1.1.2.3.cmml"><mo id="S2.Ex1.m1.1.2.3a" xref="S2.Ex1.m1.1.2.3.cmml">âˆ’</mo><mrow id="S2.Ex1.m1.1.1.3" xref="S2.Ex1.m1.1.1.2.cmml"><mo id="S2.Ex1.m1.1.1.3.1" xref="S2.Ex1.m1.1.1.2.1.cmml">[</mo><mtable displaystyle="true" rowspacing="0pt" id="S2.Ex1.m1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.cmml"><mtr id="S2.Ex1.m1.1.1.1.1a" xref="S2.Ex1.m1.1.1.1.1.cmml"><mtd id="S2.Ex1.m1.1.1.1.1b" xref="S2.Ex1.m1.1.1.1.1.cmml"><mstyle displaystyle="false" id="S2.Ex1.m1.1.1.1.1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.cmml"><mfrac id="S2.Ex1.m1.1.1.1.1.1.1.1.1a" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.Ex1.m1.1.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S2.Ex1.m1.1.1.1.1.1.1.1.1.3.2" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S2.Ex1.m1.1.1.1.1.1.1.1.1.3.2.2" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.3.2.2.cmml">A</mi><mo id="S2.Ex1.m1.1.1.1.1.1.1.1.1.3.2.1" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.3.2.1.cmml">Â¯</mo></mover><mn id="S2.Ex1.m1.1.1.1.1.1.1.1.1.3.3" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></msub><msub id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo fence="true" rspace="0em" stretchy="true" id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">âˆ¥</mo><msub id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">A</mi><mo id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">Â¯</mo></mover><mn id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo fence="true" lspace="0em" stretchy="true" id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">âˆ¥</mo></mrow><mn id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">2</mn></msub></mfrac></mstyle></mtd></mtr><mtr id="S2.Ex1.m1.1.1.1.1c" xref="S2.Ex1.m1.1.1.1.1.cmml"><mtd id="S2.Ex1.m1.1.1.1.1d" xref="S2.Ex1.m1.1.1.1.1.cmml"><mi mathvariant="normal" id="S2.Ex1.m1.1.1.1.1.3.1.1" xref="S2.Ex1.m1.1.1.1.1.3.1.1.cmml">â‹®</mi></mtd></mtr><mtr id="S2.Ex1.m1.1.1.1.1e" xref="S2.Ex1.m1.1.1.1.1.cmml"><mtd id="S2.Ex1.m1.1.1.1.1f" xref="S2.Ex1.m1.1.1.1.1.cmml"><mstyle displaystyle="false" id="S2.Ex1.m1.1.1.1.1.2.2.1.1" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.cmml"><mfrac id="S2.Ex1.m1.1.1.1.1.2.2.1.1a" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.cmml"><msub id="S2.Ex1.m1.1.1.1.1.2.2.1.1.3" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.3.cmml"><mover accent="true" id="S2.Ex1.m1.1.1.1.1.2.2.1.1.3.2" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.3.2.cmml"><mi id="S2.Ex1.m1.1.1.1.1.2.2.1.1.3.2.2" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.3.2.2.cmml">A</mi><mo id="S2.Ex1.m1.1.1.1.1.2.2.1.1.3.2.1" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.3.2.1.cmml">Â¯</mo></mover><mi id="S2.Ex1.m1.1.1.1.1.2.2.1.1.3.3" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.3.3.cmml">m</mi></msub><msub id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.cmml"><mrow id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.2.cmml"><mo fence="true" rspace="0em" stretchy="true" id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.2" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.2.1.cmml">âˆ¥</mo><msub id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.2.cmml"><mi id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.2.2" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.2.2.cmml">A</mi><mo id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.2.1" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.2.1.cmml">Â¯</mo></mover><mi id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.3.cmml">m</mi></msub><mo fence="true" lspace="0em" stretchy="true" id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.3" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.2.1.cmml">âˆ¥</mo></mrow><mn id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.3" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.3.cmml">2</mn></msub></mfrac></mstyle></mtd></mtr></mtable><mo id="S2.Ex1.m1.1.1.3.2" xref="S2.Ex1.m1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.1b"><apply id="S2.Ex1.m1.1.2.cmml" xref="S2.Ex1.m1.1.2"><csymbol cd="latexml" id="S2.Ex1.m1.1.2.1.cmml" xref="S2.Ex1.m1.1.2.1">assign</csymbol><ci id="S2.Ex1.m1.1.2.2.cmml" xref="S2.Ex1.m1.1.2.2">ğ¶</ci><apply id="S2.Ex1.m1.1.2.3.cmml" xref="S2.Ex1.m1.1.2.3"><minus id="S2.Ex1.m1.1.2.3.1.cmml" xref="S2.Ex1.m1.1.2.3"></minus><apply id="S2.Ex1.m1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.3"><csymbol cd="latexml" id="S2.Ex1.m1.1.1.2.1.cmml" xref="S2.Ex1.m1.1.1.3.1">matrix</csymbol><matrix id="S2.Ex1.m1.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1"><matrixrow id="S2.Ex1.m1.1.1.1.1a.cmml" xref="S2.Ex1.m1.1.1.1.1"><apply id="S2.Ex1.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1"><divide id="S2.Ex1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1"></divide><apply id="S2.Ex1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S2.Ex1.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.3.2"><ci id="S2.Ex1.m1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.3.2.1">Â¯</ci><ci id="S2.Ex1.m1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.3.2.2">ğ´</ci></apply><cn type="integer" id="S2.Ex1.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.3.3">1</cn></apply><apply id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">delimited-âˆ¥âˆ¥</csymbol><apply id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><ci id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1">Â¯</ci><ci id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">ğ´</ci></apply><cn type="integer" id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">1</cn></apply></apply><cn type="integer" id="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1.1.1.1.3">2</cn></apply></apply></matrixrow><matrixrow id="S2.Ex1.m1.1.1.1.1b.cmml" xref="S2.Ex1.m1.1.1.1.1"><ci id="S2.Ex1.m1.1.1.1.1.3.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1.3.1.1">â‹®</ci></matrixrow><matrixrow id="S2.Ex1.m1.1.1.1.1c.cmml" xref="S2.Ex1.m1.1.1.1.1"><apply id="S2.Ex1.m1.1.1.1.1.2.2.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1"><divide id="S2.Ex1.m1.1.1.1.1.2.2.1.1.2.cmml" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1"></divide><apply id="S2.Ex1.m1.1.1.1.1.2.2.1.1.3.cmml" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.1.1.2.2.1.1.3.1.cmml" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.3">subscript</csymbol><apply id="S2.Ex1.m1.1.1.1.1.2.2.1.1.3.2.cmml" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.3.2"><ci id="S2.Ex1.m1.1.1.1.1.2.2.1.1.3.2.1.cmml" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.3.2.1">Â¯</ci><ci id="S2.Ex1.m1.1.1.1.1.2.2.1.1.3.2.2.cmml" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.3.2.2">ğ´</ci></apply><ci id="S2.Ex1.m1.1.1.1.1.2.2.1.1.3.3.cmml" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.3.3">ğ‘š</ci></apply><apply id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1">subscript</csymbol><apply id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.2.1.cmml" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.2">delimited-âˆ¥âˆ¥</csymbol><apply id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1">subscript</csymbol><apply id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.2"><ci id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.2.1">Â¯</ci><ci id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.2.2">ğ´</ci></apply><ci id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.3">ğ‘š</ci></apply></apply><cn type="integer" id="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.3.cmml" xref="S2.Ex1.m1.1.1.1.1.2.2.1.1.1.1.3">2</cn></apply></apply></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.1c">C:=-\begin{bmatrix}\frac{\bar{A}_{1}}{\left\lVert\bar{A}_{1}\right\rVert_{2}}\\
\vdots\\
\frac{\bar{A}_{m}}{\left\lVert\bar{A}_{m}\right\rVert_{2}}\end{bmatrix}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS1.SSS2.p1.12" class="ltx_p">Crucially and in contrast to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, we remove all tokens corresponding to punctuation from <math id="S2.SS1.SSS2.p1.12.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S2.SS1.SSS2.p1.12.m1.1a"><mi id="S2.SS1.SSS2.p1.12.m1.1.1" xref="S2.SS1.SSS2.p1.12.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.12.m1.1b"><ci id="S2.SS1.SSS2.p1.12.m1.1.1.cmml" xref="S2.SS1.SSS2.p1.12.m1.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.12.m1.1c">D</annotation></semantics></math> before constructing the cost matrix since punctuation has no clear acoustic representation and should therefore not be given a timestamp in the alignment.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Retokenization</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Taking a closer look at the tokens in the vocabulary of Whisper, we identify that many tokens are prefixed with a space. When applying the BPE algorithm to all CommonVoice14 transcripts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> using Whisper's vocabulary, we observe that only <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="13" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mn id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">13</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><cn type="integer" id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">13</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">13</annotation></semantics></math>% of spaces in the original transcripts are mapped to the explicit space token. For instance, tokenizing the sentence 'This is a long pause.' with Whisper's original tokenizer results in ['This', ' is', ' a', ' long', ' pause', '.'], where spaces are included at the start of tokens rather than as standalone entities. This tokenization approach impacts the application of DTW for aligning audio segments to tokens, as it inadvertently integrates pauses at the beginning of tokens into their timings. We observe that spaces are exclusively found at the beginning of tokens but never at the end or in the middle. Therefore, we propose to simply strip all tokens in the vocabulary of spaces, except the space token itself, and keep only the unique tokens. We adjust the merges in the tokenizer to be congruent with this reduced vocabulary. This simple adjustment ensures that all spaces will be tokenized individually, theoretically enabling the DTW algorithm to detect pauses between words. Retokenizing our example with the adjustment yields ['This', ' ', 'is', ' ', 'a', ' ', 'long', ' ', 'pause', '.']. The difference between the DTW paths on the cross attention scores of Whisper's large-v2 version with its original tokenizer can be found in Figure <a href="#S2.F1" title="Figure 1 â€£ 2.2 Retokenization â€£ 2 CrisperWhisper â€£ CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> with the fine-tuned CrisperWhisper model and its adjusted tokenizer in Figure <a href="#S2.F2" title="Figure 2 â€£ 2.2 Retokenization â€£ 2 CrisperWhisper â€£ CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, visualising the close alignment with the ground truth timings. We further re-purpose tokens for 'uh' and 'um' to canonically transcribe filled pause events.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2408.16589/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="237" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">Example of a DTW path through the cross-attention weights matrix of Whisper large-v2 as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. White lines represent the ground truth.</span></figcaption>
</figure>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2408.16589/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="237" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">Example of a DTW path through the cross-attention weights matrix after CrisperWhisper retokenization. White lines represent the ground truth.</span></figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Pause Heuristics</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">To address the overestimation of pause durations by the DTW algorithm due to non-distinct attentions, we introduced a heuristic that splits the duration of pauses evenly between the preceding and subsequent words, setting a cap at <math id="S2.SS3.p1.1.m1.3" class="ltx_Math" alttext="160\text{\,}\mathrm{ms}" display="inline"><semantics id="S2.SS3.p1.1.m1.3a"><mrow id="S2.SS3.p1.1.m1.3.3" xref="S2.SS3.p1.1.m1.3.3.cmml"><mn id="S2.SS3.p1.1.m1.1.1.1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.1.1.1.cmml">160</mn><mtext id="S2.SS3.p1.1.m1.2.2.2.2.2.2" xref="S2.SS3.p1.1.m1.2.2.2.2.2.2.cmml">Â </mtext><mi class="ltx_unit" id="S2.SS3.p1.1.m1.3.3.3.3.3.3" xref="S2.SS3.p1.1.m1.3.3.3.3.3.3.cmml">ms</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.3b"><apply id="S2.SS3.p1.1.m1.3.3.cmml" xref="S2.SS3.p1.1.m1.3.3"><csymbol cd="latexml" id="S2.SS3.p1.1.m1.2.2.2.2.2.2.cmml" xref="S2.SS3.p1.1.m1.2.2.2.2.2.2">times</csymbol><cn type="integer" id="S2.SS3.p1.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1.1.1.1">160</cn><csymbol cd="latexml" id="S2.SS3.p1.1.m1.3.3.3.3.3.3.cmml" xref="S2.SS3.p1.1.m1.3.3.3.3.3.3">millisecond</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.3c">160\text{\,}\mathrm{ms}</annotation></semantics></math>. This cap is based on the observed distribution of pause durations, effectively distinguishing between insubstantial "artifact" pauses and meaningful speech pauses. Durations surpassing this cap are identified and timed as genuine pauses, ensuring a more accurate representation of speech rhythm.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Training</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our training dataset consists of two spontaneous speech datasets with a verbatim transcription style, namely the <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">AMI Meeting Corpus</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and a specially adapted version of the <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_bold">PodcastFillers Corpus</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, along with a cleaned segment of the <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_bold">CommonVoice14 Corpus</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> English subset. Additionally, we use two noise datasets, <span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_bold">FSDnoisy18k</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and <span id="S3.SS1.p1.1.5" class="ltx_text ltx_font_bold">AudioSet</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, to make the model more noise invariant. Specifically, for the AMI dataset, we utilize the training split of the AMI-IHM subset, which contains approximately 29,000 meeting recording clips with canonical transcriptions of filler events. Moreover, we utilize a subset of the PodcastFillers dataset, which comprises approximately 35,000 instances of filler words such as 'uh' or 'um' used in podcast episodes. The dataset includes timings and automatically generated timed transcriptions for the podcast episodes. Our process for reformatting this dataset involves the following steps:</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Sampling Context:</span> For each timed filler word in the training set, we generate three distinct audio segments by choosing varying context lengths ranging from 1 to 5 seconds from both before and after the filler. This selection is made with care to avoid including partial words on both ends, slightly adjusting the sampled context length when needed to include partial words fully. This expands our dataset to approximately 105,000 samples.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Cut Audio Clips with Aligned Transcripts:</span> The chosen audio segments, along with their aligned transcripts, are extracted. Filler words are explicitly marked as either 'uh' or 'um' in their respective positions within the transcripts.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Transcript Correction:</span> To address inaccuracies in punctuation and capitalization within the original transcripts, we utilized GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> after observing that the original transcript quality adversely affected Whisper's ability to correctly apply punctuation.</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">For CommonVoice14, we employed Whisper's medium model on the English train subset to identify and remove samples likely not transcribed verbatim. Any sample with a character error rate exceeding <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mn id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><cn type="integer" id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">3</annotation></semantics></math>% compared to its label was excluded.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Implementation Details</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To deploy CrisperWhisper as a comprehensive speech analysis model in practical applications, it is essential that the model is trained to prioritize the primary speaker's voice and to be generally noise robust. To this end we use the noisy/overlapped speech simulation proposed in WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> during fine-tuning from the whisper-large-v2 checkpoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. As noise data, we use FSDnoisy18k, AudioSet, random Gaussian noise and random speech samples drawn from the dataset. To counteract hallucinations, we introduce noise-only samples (containing no speech) with empty transcriptions in 1% of the training samples. The training process spans 6,000 steps with a batch size of 256, utilizing a 0.00005 learning rate, a linear learning rate decay with an 800-step warmup phase, amounting to approximately 2 epochs.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">AMI Meeting Corpus</span>: We use the official test split on the AMI-IHM subset with approximately 11,500 samples. <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_bold">AMI disfluency subset</span>: The AMI Meeting Corpus contains filler words transcribed in a canonical way. We extend the filler words by using GPT-4 to label repetitions, false starts and revisions on the transcripts of the AMI-IHM test set. Our disfluency subset consists of all files and transcripts that contain at least one of the labeled disfluencies and contain more than 5 words, which are approximately 4,000 samples. <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_bold">PodcastFillers Corpus</span>: We use the same approach as described in Section <a href="#S3.SS1" title="3.1 Datasets â€£ 3 Training â€£ CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> for creating annotated filler samples on the official test subset, choosing 1 second of context before and after each filler, which results in approximately 5,000 samples. <span id="S4.SS1.p1.1.4" class="ltx_text ltx_font_bold">Synthetic dataset</span>: To compare word-level timestamp accuracy, we created 200 samples of spontaneous speech transcripts with GPT-4, which contain natural pauses in sentences, indicated by 'â€¦' in the transcripts. These transcripts were subsequently synthesized with ElevenLabs <a target="_blank" href="https://www.elevenlabs.io" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.elevenlabs.io</a>, creating naturally sounding spontaneous speech samples for which timestamps were manually annotated. <span id="S4.SS1.p1.1.5" class="ltx_text ltx_font_bold">AphasiaBank Corpus</span>: AphasiaBank <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> comprises a collection of interviews between clinicians and subjects afflicted with aphasia, as well as healthy control subjects. We are using the same files that Koenecke et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> have identified to cause hallucinated content when transcribed with Whisper large-v2. <span id="S4.SS1.p1.1.6" class="ltx_text ltx_font_bold">TED-LIUM</span>: We use the test split of TED-LIUM Release 3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, using the segmented manual test transcripts included in the release. <span id="S4.SS1.p1.1.7" class="ltx_text ltx_font_bold">LibriSpeech</span>: We use both of the popular LibriSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> 'test clean' and 'test other' splits for evaluation.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Metrics</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To evaluate transcription accuracy, we use word error rate (<span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">WER</span>) and insertion error rate (<span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_bold">IER</span>) to quantify word omissions. For timing accuracy, we use the <span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_bold">F<sub id="S4.SS2.p1.1.3.1" class="ltx_sub">1</sub>-score</span>, which is well defined via basic confusion matrix terminology. In this context, we define a <span id="S4.SS2.p1.1.4" class="ltx_text ltx_font_bold">true positive</span> as a predicted word that both overlaps temporally with a reference word and matches its content. Each reference word can only contribute to a single true positive. A <span id="S4.SS2.p1.1.5" class="ltx_text ltx_font_bold">false positive</span> is defined as a predicted word that does not have temporal overlap or content match with any reference word. Conversely, a <span id="S4.SS2.p1.1.6" class="ltx_text ltx_font_bold">false negative</span> is a reference word that does not have temporal overlap or content match with any predicted word. Temporal overlap occurs when the start (onset) and end (offset) timestamps of a prediction fall within a predefined collar of the corresponding timestamps in the reference. Additionally, we evaluate localization accuracy using the <span id="S4.SS2.p1.1.7" class="ltx_text ltx_font_bold">mean Intersection over Union (mIoU)</span> metric, which compares each predicted word against the reference for both string match and temporal overlap, calculating the IoU based on timestamps, or assigning a score of 0 if no match exists. The highest IoU score for each word represents its IoU, ensuring each word is matched only once.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In the following, we are referring to Whisper's large-v2 model with the DTW implementation of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> as <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">WhisperT</span>, the default configuration of WhisperX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> with an underlying Whisper large-v2 and Wav2vec2.0 alignment model as <span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_bold">WhisperX</span>.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Word Segmentation Performance</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">Figure <a href="#S4.F3.sf1" title="In Figure 3 â€£ 4.3.1 Word Segmentation Performance â€£ 4.3 Results â€£ 4 Evaluation â€£ CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a> shows how CrisperWhisper outperforms previous state-of-the-art models using different collar values on the test set of the AMI Corpus. We ensure that the normalized transcripts of our prediction, the normalized reference and the normalized predictions of the other models coincide completely. This is to account for the fact that our more verbatim approach gives us an unfair advantage and we want to ensure that we evaluate the localization performance separately from the transcription accuracy. Figure <a href="#S4.F3.sf2" title="In Figure 3 â€£ 4.3.1 Word Segmentation Performance â€£ 4.3 Results â€£ 4 Evaluation â€£ CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a> depicts the segmentation performance using different collar values on the clean, synthetic dataset mentioned in Section <a href="#S4.SS1" title="4.1 Datasets â€£ 4 Evaluation â€£ CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> with manually annotated timestamps.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.16589/assets/x3.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="461" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">Matching AMI subset</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.16589/assets/x4.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="461" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">Synthetic dataset</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.4.2" class="ltx_text" style="font-size:90%;">Word segmentation performance showing the F<sub id="S4.F3.4.2.1" class="ltx_sub">1</sub>-score for different collar values.</span></figcaption>
</figure>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p">We further evaluate the noise robustness of our model by adding random secondary voice samples from the LibriSpeech 'test clean' subset, white noise, and random noise samples from FSDnoisy18k with a signal-to-noise ratio of 1:5 to the synthetic samples. As detailed in Table <a href="#S4.T1" title="Table 1 â€£ 4.3.1 Word Segmentation Performance â€£ 4.3 Results â€£ 4 Evaluation â€£ CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, CrisperWhisper demonstrates superior robustness in terms of mIoU and F<sub id="S4.SS3.SSS1.p2.1.1" class="ltx_sub">1</sub>-score under noisy conditions. In contrast, WhisperX exhibits a more significant performance decline than WhisperT, attributable to Wav2Vec2.0's lesser noise resilience. Notably, CrisperWhisper achieves markedly higher mIoU metrics and F<sub id="S4.SS3.SSS1.p2.1.2" class="ltx_sub">1</sub>-scores, particularly with narrower collars, underscoring its enhanced accuracy in timestamping pauses compared to other evaluated methods.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.8.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.9.2" class="ltx_text" style="font-size:90%;">Noise robustness of word segmentation performance on synthetic data using a collar of 0.2 seconds.</span></figcaption>
<table id="S4.T1.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.6.7.1" class="ltx_tr">
<th id="S4.T1.6.7.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T1.6.7.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T1.6.7.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T1.6.7.1.2.1" class="ltx_text ltx_font_bold">Synthetic</span></th>
<th id="S4.T1.6.7.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T1.6.7.1.3.1" class="ltx_text ltx_font_bold">Synthetic noisy</span></th>
</tr>
<tr id="S4.T1.6.6" class="ltx_tr">
<th id="S4.T1.6.6.7" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><msub id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml"><mi id="S4.T1.1.1.1.m1.1.1.2" xref="S4.T1.1.1.1.m1.1.1.2.cmml">F</mi><mn id="S4.T1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.m1.1.1.2">ğ¹</ci><cn type="integer" id="S4.T1.1.1.1.m1.1.1.3.cmml" xref="S4.T1.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">F_{1}</annotation></semantics></math> <math id="S4.T1.2.2.2.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.2.2.2.m2.1a"><mo stretchy="false" id="S4.T1.2.2.2.m2.1.1" xref="S4.T1.2.2.2.m2.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m2.1b"><ci id="S4.T1.2.2.2.m2.1.1.cmml" xref="S4.T1.2.2.2.m2.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m2.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mIoU <math id="S4.T1.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.3.3.3.m1.1a"><mo stretchy="false" id="S4.T1.3.3.3.m1.1.1" xref="S4.T1.3.3.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<math id="S4.T1.4.4.4.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S4.T1.4.4.4.m1.1a"><msub id="S4.T1.4.4.4.m1.1.1" xref="S4.T1.4.4.4.m1.1.1.cmml"><mi id="S4.T1.4.4.4.m1.1.1.2" xref="S4.T1.4.4.4.m1.1.1.2.cmml">F</mi><mn id="S4.T1.4.4.4.m1.1.1.3" xref="S4.T1.4.4.4.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.m1.1b"><apply id="S4.T1.4.4.4.m1.1.1.cmml" xref="S4.T1.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.4.4.4.m1.1.1.1.cmml" xref="S4.T1.4.4.4.m1.1.1">subscript</csymbol><ci id="S4.T1.4.4.4.m1.1.1.2.cmml" xref="S4.T1.4.4.4.m1.1.1.2">ğ¹</ci><cn type="integer" id="S4.T1.4.4.4.m1.1.1.3.cmml" xref="S4.T1.4.4.4.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.m1.1c">F_{1}</annotation></semantics></math> <math id="S4.T1.5.5.5.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.5.5.5.m2.1a"><mo stretchy="false" id="S4.T1.5.5.5.m2.1.1" xref="S4.T1.5.5.5.m2.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.m2.1b"><ci id="S4.T1.5.5.5.m2.1.1.cmml" xref="S4.T1.5.5.5.m2.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.m2.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">mIoU <math id="S4.T1.6.6.6.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.6.6.6.m1.1a"><mo stretchy="false" id="S4.T1.6.6.6.m1.1.1" xref="S4.T1.6.6.6.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.m1.1b"><ci id="S4.T1.6.6.6.m1.1.1.cmml" xref="S4.T1.6.6.6.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.6.8.1" class="ltx_tr">
<th id="S4.T1.6.8.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">WhisperT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</th>
<td id="S4.T1.6.8.1.2" class="ltx_td ltx_align_center ltx_border_t">74.7</td>
<td id="S4.T1.6.8.1.3" class="ltx_td ltx_align_center ltx_border_t">51.4</td>
<td id="S4.T1.6.8.1.4" class="ltx_td ltx_align_center ltx_border_t">68.3</td>
<td id="S4.T1.6.8.1.5" class="ltx_td ltx_align_center ltx_border_t">49.8</td>
</tr>
<tr id="S4.T1.6.9.2" class="ltx_tr">
<th id="S4.T1.6.9.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">WhisperX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</th>
<td id="S4.T1.6.9.2.2" class="ltx_td ltx_align_center">76.7</td>
<td id="S4.T1.6.9.2.3" class="ltx_td ltx_align_center">61.5</td>
<td id="S4.T1.6.9.2.4" class="ltx_td ltx_align_center">59.0</td>
<td id="S4.T1.6.9.2.5" class="ltx_td ltx_align_center">44.3</td>
</tr>
<tr id="S4.T1.6.10.3" class="ltx_tr">
<th id="S4.T1.6.10.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">CrisperWhisper</th>
<td id="S4.T1.6.10.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.6.10.3.2.1" class="ltx_text ltx_font_bold">84.7</span></td>
<td id="S4.T1.6.10.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.6.10.3.3.1" class="ltx_text ltx_font_bold">63.4</span></td>
<td id="S4.T1.6.10.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.6.10.3.4.1" class="ltx_text ltx_font_bold">79.5</span></td>
<td id="S4.T1.6.10.3.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.6.10.3.5.1" class="ltx_text ltx_font_bold">60.5</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Disfluency Segmentation Performance</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.2" class="ltx_p">For evaluating the filler word detection and segmentation performance, we transcribe the audio examples of our adjusted test split of the Podcast-Fillers Corpus as described in Section <a href="#S4" title="4 Evaluation â€£ CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> with CrisperWhisper and calculated the <math id="S4.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S4.SS3.SSS2.p1.1.m1.1a"><msub id="S4.SS3.SSS2.p1.1.m1.1.1" xref="S4.SS3.SSS2.p1.1.m1.1.1.cmml"><mi id="S4.SS3.SSS2.p1.1.m1.1.1.2" xref="S4.SS3.SSS2.p1.1.m1.1.1.2.cmml">F</mi><mn id="S4.SS3.SSS2.p1.1.m1.1.1.3" xref="S4.SS3.SSS2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.1.m1.1b"><apply id="S4.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1.2">ğ¹</ci><cn type="integer" id="S4.SS3.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.1.m1.1c">F_{1}</annotation></semantics></math>-scores as described in Section <a href="#S4.SS2" title="4.2 Metrics â€£ 4 Evaluation â€£ CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> for various collar values. The results can be seen in Figure <a href="#S4.F4.sf1" title="In Figure 4 â€£ 4.3.2 Disfluency Segmentation Performance â€£ 4.3 Results â€£ 4 Evaluation â€£ CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(a)</span></a> with the reported <math id="S4.SS3.SSS2.p1.2.m2.1" class="ltx_Math" alttext="F_{1}" display="inline"><semantics id="S4.SS3.SSS2.p1.2.m2.1a"><msub id="S4.SS3.SSS2.p1.2.m2.1.1" xref="S4.SS3.SSS2.p1.2.m2.1.1.cmml"><mi id="S4.SS3.SSS2.p1.2.m2.1.1.2" xref="S4.SS3.SSS2.p1.2.m2.1.1.2.cmml">F</mi><mn id="S4.SS3.SSS2.p1.2.m2.1.1.3" xref="S4.SS3.SSS2.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.2.m2.1b"><apply id="S4.SS3.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.2">ğ¹</ci><cn type="integer" id="S4.SS3.SSS2.p1.2.m2.1.1.3.cmml" xref="S4.SS3.SSS2.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.2.m2.1c">F_{1}</annotation></semantics></math>-score eventually exceeding the acoustic model reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, although the segmentation is worse for collar values smaller than 0.5 seconds. Since our model transcribes verbatim, we also detect and segment other disfluency types, such as repetitions, false starts or partial words. Figure <a href="#S4.F4.sf2" title="In Figure 4 â€£ 4.3.2 Disfluency Segmentation Performance â€£ 4.3 Results â€£ 4 Evaluation â€£ CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a> shows the localization performance on disfluent speech samples of the AMI disfluency subset described in Section <a href="#S4.SS1" title="4.1 Datasets â€£ 4 Evaluation â€£ CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.16589/assets/x5.png" id="S4.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="461" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F4.sf1.3.2" class="ltx_text" style="font-size:90%;">PodcastFillers Dataset</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.16589/assets/x6.png" id="S4.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="461" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F4.sf2.3.2" class="ltx_text" style="font-size:90%;">AMI disfluent subset</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Disfluency Localization Performance</span></figcaption>
</figure>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Verbatim Transcription Performance</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 â€£ 4.3.3 Verbatim Transcription Performance â€£ 4.3 Results â€£ 4 Evaluation â€£ CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the significantly improved ASR performance on spontaneous speech datasets with more verbatim transcriptions. We have further validated that CrisperWhisper's transcription accuracy does not degrade on intended speech datasets. All transcriptions of the 'test other' and 'test clean' subsets of the LibriSpeech Corpus or the test split of CommonVoice14 lie within 0.01 WER of the original Whisper large-v2 model.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.6.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.7.2" class="ltx_text" style="font-size:90%;">ASR performance on verbatim datasets.</span></figcaption>
<table id="S4.T2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.4.5.1" class="ltx_tr">
<th id="S4.T2.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T2.4.5.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T2.4.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T2.4.5.1.2.1" class="ltx_text ltx_font_bold">AMI</span></th>
<th id="S4.T2.4.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T2.4.5.1.3.1" class="ltx_text ltx_font_bold">TED-LIUM</span></th>
</tr>
<tr id="S4.T2.4.4" class="ltx_tr">
<th id="S4.T2.4.4.5" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">WER <math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mo stretchy="false" id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">IER <math id="S4.T2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T2.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">WER <math id="S4.T2.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.3.3.3.m1.1a"><mo stretchy="false" id="S4.T2.3.3.3.m1.1.1" xref="S4.T2.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T2.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">IER <math id="S4.T2.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.4.4.4.m1.1a"><mo stretchy="false" id="S4.T2.4.4.4.m1.1.1" xref="S4.T2.4.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.m1.1b"><ci id="S4.T2.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.4.6.1" class="ltx_tr">
<th id="S4.T2.4.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</th>
<td id="S4.T2.4.6.1.2" class="ltx_td ltx_align_center ltx_border_t">16.82</td>
<td id="S4.T2.4.6.1.3" class="ltx_td ltx_align_center ltx_border_t">11.77</td>
<td id="S4.T2.4.6.1.4" class="ltx_td ltx_align_center ltx_border_t">4.01</td>
<td id="S4.T2.4.6.1.5" class="ltx_td ltx_align_center ltx_border_t">3.08</td>
</tr>
<tr id="S4.T2.4.7.2" class="ltx_tr">
<th id="S4.T2.4.7.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">CrisperWhisper</th>
<td id="S4.T2.4.7.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.4.7.2.2.1" class="ltx_text ltx_font_bold">9.72</span></td>
<td id="S4.T2.4.7.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.4.7.2.3.1" class="ltx_text ltx_font_bold">2.26</span></td>
<td id="S4.T2.4.7.2.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.4.7.2.4.1" class="ltx_text ltx_font_bold">3.26</span></td>
<td id="S4.T2.4.7.2.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.4.7.2.5.1" class="ltx_text ltx_font_bold">0.75</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.4 </span>Hallucinations</h4>

<div id="S4.SS3.SSS4.p1" class="ltx_para">
<p id="S4.SS3.SSS4.p1.2" class="ltx_p">To validate hallucination mitigation, we use the same audio files from AphasiaBank as analyzed by Koenecke et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. CrisperWhisper does not produce harmful hallucinations on any of the identified speech recordings, but produces repetitive transcription loops on 10 recordings. Since CrisperWhisper is producing accurate word-level timestamps, we are simply removing tokens with a duration below <math id="S4.SS3.SSS4.p1.1.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S4.SS3.SSS4.p1.1.m1.1a"><mn id="S4.SS3.SSS4.p1.1.m1.1.1" xref="S4.SS3.SSS4.p1.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS4.p1.1.m1.1b"><cn type="integer" id="S4.SS3.SSS4.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS4.p1.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS4.p1.1.m1.1c">50</annotation></semantics></math> <math id="S4.SS3.SSS4.p1.2.m2.1" class="ltx_Math" alttext="\mathrm{m}\mathrm{s}" display="inline"><semantics id="S4.SS3.SSS4.p1.2.m2.1a"><mi id="S4.SS3.SSS4.p1.2.m2.1.1" xref="S4.SS3.SSS4.p1.2.m2.1.1.cmml">ms</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS4.p1.2.m2.1b"><ci id="S4.SS3.SSS4.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS4.p1.2.m2.1.1">ms</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS4.p1.2.m2.1c">\mathrm{m}\mathrm{s}</annotation></semantics></math>, effectively eliminating this type of artefact of hallucinating speech during inactivate speech regions.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We proposed CrisperWhisper, a robust end-to-end speech transcription model producing accurate word-level timestamps in a verbatim, single speaker focused transcription style. We trace the problem of unsharp timestamps around disfluencies and pauses back to Whisper's tokenizer and present a strategy to alleviate this problem. One weakness of our approach is the arbitrary selection of attention heads used for alignment. In the near future, we want to investigate the verbatim transcription and segmentation capabilities for quantifying speech deficits, scale the approach with more high quality verbatim data and explore how these capabilities can be transferred to other languages.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A.Â Baevski, H.Â Zhou, A.Â Mohamed, and M.Â Auli, ``wav2vec 2.0: A framework for self-supervised learning of speech representations,'' 2020. [Online]. Available: <a target="_blank" href="https://arxiv.org/abs/2006.11477" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2006.11477</a>

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S.Â Chen, C.Â Wang, Z.Â Chen, Y.Â Wu, S.Â Liu, Z.Â Chen, J.Â Li, N.Â Kanda, T.Â Yoshioka, X.Â Xiao <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Wavlm: Large-scale self-supervised pre-training for full stack speech processing,'' <em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, vol.Â 16, no.Â 6, pp. 1505â€“1518, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
W.Â Hsu, B.Â Bolte, Y.Â H. Tsai, K.Â Lakhotia, R.Â Salakhutdinov, and A.Â Mohamed, ``Hubert: Self-supervised speech representation learning by masked prediction of hidden units,'' <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/2106.07447, 2021. [Online]. Available: <a target="_blank" href="https://arxiv.org/abs/2106.07447" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2106.07447</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y.Â Zhang, D.Â S. Park, W.Â Han, J.Â Qin, A.Â Gulati, J.Â Shor, A.Â Jansen, Y.Â Xu, Y.Â Huang, S.Â Wang <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Bigssl: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition,'' <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, vol.Â 16, no.Â 6, pp. 1519â€“1532, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J.Â Kang, J.Â Huh, H.Â S. Heo, and J.Â S. Chung, ``Augmentation adversarial training for self-supervised speaker representation learning,'' <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, vol.Â 16, no.Â 6, pp. 1253â€“1262, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A.Â Radford, J.Â W. Kim, T.Â Xu, G.Â Brockman, C.Â McLeavey, and I.Â Sutskever, ``Robust speech recognition via large-scale weak supervision,'' <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.04356</em>, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A.Â Vaswani, N.Â Shazeer, N.Â Parmar, J.Â Uszkoreit, L.Â Jones, A.Â N. Gomez, Å.Â Kaiser, and I.Â Polosukhin, ``Attention is all you need,'' <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol.Â 30, 2017.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S.Â Wollin-Giering, M.Â Hoffmann, J.Â HÃ¶fting, C.Â Ventzke <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Automatic transcription of english and german qualitative interviews,'' in <em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic">Forum Qualitative Sozialforschung/Forum: Qualitative Social Research</em>, vol.Â 25, no.Â 1, 2024.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A.Â Romana, K.Â Koishida, and E.Â M. Provost, ``Automatic disfluency detection from untranscribed speech,'' <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.00867</em>, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
C.Â Lea, Z.Â Huang, J.Â Narain, L.Â Tooley, D.Â Yee, D.Â T. Tran, P.Â Georgiou, J.Â P. Bigham, and L.Â Findlater, ``From user perceptions to technical improvement: Enabling people who stutter to better use speech recognition,'' in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, 2023, pp. 1â€“16.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A.Â Romana, J.Â Bandon, M.Â Perez, S.Â Gutierrez, R.Â Richter, A.Â Roberts, and E.Â M. Provost, ``Automatically detecting errors and disfluencies in read speech to predict cognitive impairment in people with parkinson's disease.'' in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, 2021, pp. 1907â€“1911.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
L.Â Wagner, M.Â Zusag, and T.Â Bloder, ``Careful whisper â€“ leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification,'' in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
H.Â H. Clark and J.Â E.Â F. Tree, ``Using uh and um in spontaneous speaking,'' <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Cognition</em>, vol.Â 84, no.Â 1, pp. 73â€“111, 2002.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A.Â LindstrÃ¶m, J.Â Villing, S.Â Larsson, A.Â Seward, N.Â Ã…berg, and C.Â Holtelius, ``The effect of cognitive load on disfluencies during in-vehicle spoken dialogue,'' <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH, 2008</em>, pp. 1196â€“1199, 09 2008.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M.Â Corley and O.Â Stewart, ``Hesitation disfluencies in spontaneous speech: The meaning of,'' <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Language and Linguistics Compass</em>, vol.Â 2, pp. 589â€“602, 07 2008.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
E.Â G. Bard, R.Â J. Lickley, and M.Â P. Aylett, ``Is disfluency just difficulty?'' in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proc. ITRW on Disfluency in Spontaneous Speech (DiSS 2001)</em>, 2001, pp. 97â€“100.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
K.Â Womack, W.Â McCoy, C.Â OvesdotterÂ Alm, C.Â Calvelli, J.Â B. Pelz, P.Â Shi, and A.Â Haake, ``Disfluencies as extra-propositional indicators of cognitive processing,'' in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics</em>, R.Â Morante and C.Â Sporleder, Eds.Â Â Â Jeju, Republic of Korea: Association for Computational Linguistics, Jul. 2012, pp. 1â€“9. [Online]. Available: <a target="_blank" href="https://aclanthology.org/W12-3801" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/W12-3801</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
G.Â Zhu, J.-P. Caceres, and J.Â Salamon, ``Filler word detection and classification: A dataset and benchmark,'' in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH, 2023</em>, Incheon, Korea, Sep. 2022. [Online]. Available: <a target="_blank" href="https://arxiv.org/abs/2203.15135" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2203.15135</a>

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
R.Â Ochshorn and M.Â M. Hawkins, ``Gentle: A robust yet lenient forced aligner built on kaldi,'' <a target="_blank" href="https://lowerquality.com/gentle/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://lowerquality.com/gentle/</a>, 2015.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M.Â Bain, J.Â Huh, T.Â Han, and A.Â Zisserman, ``Whisperx: Time-accurate speech transcription of long-form audio,'' <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J.Â W. Kim, ``openai-dtw,'' <a target="_blank" href="https://github.com/openai/whisper/blob/main/notebooks/Multilingual_ASR.ipynb" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/openai/whisper/blob/main/notebooks/Multilingual_ASR.ipynb</a>, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J.Â Louradour, ``whisper-timestamped,'' <a target="_blank" href="https://github.com/linto-ai/whisper-timestamped" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/linto-ai/whisper-timestamped</a>, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A.Â Koenecke, A.Â S.Â G. Choi, K.Â Mei, H.Â Schellmann, and M.Â Sloane, ``Careless whisper: Speech-to-text hallucination harms,'' <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.08021</em>, 2024.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
B.Â MacWhinney, D.Â Fromm, M.Â Forbes, and A.Â Holland, ``Aphasiabank: Methods for studying discourse,'' <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Aphasiology</em>, vol.Â 25, no.Â 11, pp. 1286â€“1307, 2011.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
J.Â Carletta, S.Â Ashby, S.Â Bourban, M.Â Flynn, M.Â Guillemot, T.Â Hain, J.Â Kadlec, V.Â Karaiskos, W.Â Kraaij, M.Â Kronenthal <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``The ami meeting corpus: A pre-announcement,'' in <em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic">International workshop on machine learning for multimodal interaction</em>.Â Â Â Springer, 2005, pp. 28â€“39.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
F.Â Hernandez, V.Â Nguyen, S.Â Ghannay, N.Â Tomashenko, and Y.Â EstÃ¨ve, ``Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker adaptation,'' in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Speech and Computer</em>.Â Â Â Springer International Publishing, 2018, pp. 198â€“208.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
V.Â Panayotov, G.Â Chen, D.Â Povey, and S.Â Khudanpur, ``Librispeech: an asr corpus based on public domain audio books,'' in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on</em>.Â Â Â IEEE, 2015, pp. 5206â€“5210.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
R.Â Ardila, M.Â Branson, K.Â Davis, M.Â Henretty, M.Â Kohler, J.Â Meyer, R.Â Morais, L.Â Saunders, F.Â M. Tyers, and G.Â Weber, ``Common voice: A massively-multilingual speech corpus,'' in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020)</em>, 2020, pp. 4211â€“4215.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
P.Â Gage, ``A new algorithm for data compression,'' <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">C Users Journal</em>, vol.Â 12, no.Â 2, pp. 23â€“38, 1994.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
T.Â Giorgino, ``Computing and visualizing dynamic time warping alignments in r: The dtw package,'' <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Journal of Statistical Software</em>, vol.Â 31, no.Â 7, 2009.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
E.Â Fonseca, M.Â Plakal, D.Â P. Ellis, F.Â Font, X.Â Favory, and X.Â Serra, ``Learning sound event classifiers from web audio with noisy labels,'' in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2019, pp. 21â€“25.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
J.Â F. Gemmeke, D.Â P.Â W. Ellis, D.Â Freedman, A.Â Jansen, W.Â Lawrence, R.Â C. Moore, M.Â Plakal, and M.Â Ritter, ``Audio set: An ontology and human-labeled dataset for audio events,'' in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2017, pp. 776â€“780.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J.Â Achiam, S.Â Adler, S.Â Agarwal, L.Â Ahmad, I.Â Akkaya, F.Â L. Aleman, D.Â Almeida, J.Â Altenschmidt, S.Â Altman, S.Â Anadkat <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Gpt-4 technical report,'' <em id="bib.bib33.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
OpenAI, ``Whisper large v2,'' <a target="_blank" href="https://huggingface.co/openai/whisper-large-v2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/openai/whisper-large-v2</a>, 2024, version of 20.02.2024.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.16588" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.16589" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.16589">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.16589" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.16590" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 13:57:59 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
