<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.05980] MMIS: Multimodal Dataset for Interior Scene Visual Generation and Recognition</title><meta property="og:description" content="We introduce MMIS, a novel dataset designed to advance Multi-Modal Interior Scene generation and recognition. MMIS consists of nearly  images. Each image within the dataset is accompanied by its corresponding textual d…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MMIS: Multimodal Dataset for Interior Scene Visual Generation and Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="MMIS: Multimodal Dataset for Interior Scene Visual Generation and Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.05980">

<!--Generated on Mon Aug  5 13:03:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">MMIS: Multimodal Dataset for Interior Scene Visual Generation and Recognition</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Hozaifa Kassab, Ahmed Mahmoud, Mohamed Bahaa, Ammar Mohamed, Ali Hamdi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.1.id1" class="ltx_text ltx_font_italic">MSA University</span>
<br class="ltx_break">Giza, Egypt 
<br class="ltx_break">{hozaifa.fadl,ahmed.mahmoud60, mohamed.bahaa4,ammohammed,ahamdi}@msa.edu.eg 
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.1" class="ltx_p">We introduce MMIS, a novel dataset designed to advance <span id="id1.1.1" class="ltx_text ltx_font_italic">M</span>ulti-<span id="id1.1.2" class="ltx_text ltx_font_italic">M</span>odal <span id="id1.1.3" class="ltx_text ltx_font_italic">I</span>nterior <span id="id1.1.4" class="ltx_text ltx_font_italic">S</span>cene generation and recognition. <span id="id1.1.5" class="ltx_text ltx_font_italic">MMIS</span> consists of nearly <math id="id1.1.m1.2" class="ltx_Math" alttext="160,000" display="inline"><semantics id="id1.1.m1.2a"><mrow id="id1.1.m1.2.3.2" xref="id1.1.m1.2.3.1.cmml"><mn id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">160</mn><mo id="id1.1.m1.2.3.2.1" xref="id1.1.m1.2.3.1.cmml">,</mo><mn id="id1.1.m1.2.2" xref="id1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.2b"><list id="id1.1.m1.2.3.1.cmml" xref="id1.1.m1.2.3.2"><cn type="integer" id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">160</cn><cn type="integer" id="id1.1.m1.2.2.cmml" xref="id1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.2c">160,000</annotation></semantics></math> images. Each image within the dataset is accompanied by its corresponding textual description and an audio recording of that description, providing rich and diverse sources of information for scene generation and recognition. <span id="id1.1.6" class="ltx_text ltx_font_italic">MMIS</span> encompasses a wide range of interior spaces, capturing various styles, layouts, and furnishings. To construct this dataset, we employed careful processes involving the collection of images, the generation of textual descriptions, and corresponding speech annotations. The presented dataset contributes to research in multi-modal representation learning tasks such as image generation, retrieval, captioning, and classification. The dataset is available at the following URL: <a target="_blank" href="https://github.com/AhmedMahmoudMostafa/MMIS" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/AhmedMahmoudMostafa/MMIS</a>.</p>
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_publicationid"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">publicationid: </span>pubid: <span id="id1.1a" class="ltx_text ltx_inline-block" style="width:433.6pt;">979-8-3503-6263-6/24/$31.00 ©2024 IEEE  </span>
<span id="id1.2" class="ltx_text ltx_inline-block" style="width:433.6pt;"> </span></span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Multi-modal deep learning<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> is an active, multi-disciplinary research field that encompasses a diverse range of disciplines focused on creating intelligent computer systems capable of understanding, reasoning, and learning from various types of information from multiple sources or modalities, such as images, text, audio, and sensor data<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. The field has gained traction in recent years, particularly with the growing interest in multimodal tasks such as text-to-image generation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, text-image retrieval<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, image captioning<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, and visual question answering<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. By combining insights from various modalities, multi-modal deep learning models can capture rich and nuanced patterns that may not be discernible from any single modality alone<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. One of the driving forces behind advancements in multi-modal deep learning is the availability of large-scale datasets that encompass multiple modalities, such as images, text, and speech. These datasets not only facilitate the development and evaluation of novel and robust models but also pave the way for interdisciplinary research aimed at leveraging the synergies between different data types.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2407.05980/assets/taxonomy.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="171" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Dataset Taxonomy</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.2" class="ltx_p">In this paper, we introduce a novel dataset specifically designed to explore the fusion of multiple modalities for various tasks: image generation, retrieval, and captioning. The dataset, composed of images, textual descriptions, and corresponding speech samples, offers a rich resource for investigating the complementary nature of different modalities in capturing and representing complex visual scenes. Our dataset focuses on interior design images, a domain that presents unique challenges and opportunities for multi-modal analysis due to its rich visual and semantic content. The motivation behind creating this dataset stems from the growing interest in multi-modal AI systems that can understand and generate content across different modalities. While significant progress has been made in individual modalities such as image processing, text analysis, and speech recognition, there remains a gap in our understanding of how these modalities can be effectively integrated to solve real-world problems. By providing a diverse collection of images along with textual descriptions and speech annotations, we aim to facilitate research into multi-modal learning techniques that can leverage the complementary strengths of each modality. Our dataset comprises images representing various styles of interior design, ranging from modern and minimalist to bohemian and traditional. Each style category includes images depicting five different attributes: bedroom, living room, bathroom, kitchen, and dining room. The dataset is structured such that each attribute contains a variable number of images, with counts ranging from <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="600" display="inline"><semantics id="S1.p2.1.m1.1a"><mn id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">600</mn><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><cn type="integer" id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">600</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">600</annotation></semantics></math> to <math id="S1.p2.2.m2.2" class="ltx_Math" alttext="1,200" display="inline"><semantics id="S1.p2.2.m2.2a"><mrow id="S1.p2.2.m2.2.3.2" xref="S1.p2.2.m2.2.3.1.cmml"><mn id="S1.p2.2.m2.1.1" xref="S1.p2.2.m2.1.1.cmml">1</mn><mo id="S1.p2.2.m2.2.3.2.1" xref="S1.p2.2.m2.2.3.1.cmml">,</mo><mn id="S1.p2.2.m2.2.2" xref="S1.p2.2.m2.2.2.cmml">200</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.2.m2.2b"><list id="S1.p2.2.m2.2.3.1.cmml" xref="S1.p2.2.m2.2.3.2"><cn type="integer" id="S1.p2.2.m2.1.1.cmml" xref="S1.p2.2.m2.1.1">1</cn><cn type="integer" id="S1.p2.2.m2.2.2.cmml" xref="S1.p2.2.m2.2.2">200</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.2.m2.2c">1,200</annotation></semantics></math> images per attribute. Additionally, each image is accompanied by a textual description and an audio recording of the description, providing multiple modalities of data for analysis. While our dataset can be utilized for a wide range of tasks, including image classification, captioning, and speech recognition, our primary focus lies in exploring its potential for image generation and retrieval tasks. By leveraging the unique characteristics of our dataset, we aim to explore how different techniques for fusing information from images, text, and speech can enhance the performance of image generation and retrieval tasks. Additionally, we seek to investigate methods for learning joint representations across various modalities to enable seamless information exchange and integration. These endeavors will aid in developing models capable of comprehending the semantic content of images through the simultaneous analysis of visual, textual, and auditory cues. To ensure comprehensive assessment, we have defined appropriate evaluation metrics and benchmarks for assessing the performance of multi-modal systems on our dataset.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2407.05980/assets/artdeco_sample.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="87" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Sample from the interior design styles the Art Deco Style with the five Rooms </figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we review existing datasets relevant to our multi-modal dataset, focusing on three main categories: image datasets, image-text datasets, and image-text-audio datasets. These datasets have played a crucial role in advancing research in machine learning tasks, including image classification, captioning, and generation. Table II summarizes some representative datasets and their specific statistics.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic"> </span><span id="S2.SS1.7.3" class="ltx_text ltx_font_bold">Image Datasets</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.6" class="ltx_p">Image datasets, such as ImageNet, CIFAR-10 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, and Oxford-102 Flowers, serve as foundational resources for a wide range of computer vision tasks. These datasets provide labeled images across diverse categories, enabling researchers to train and evaluate machine learning models for tasks such as image classification, object detection, and scene recognition. For instance, in image classification, where the goal is to assign a single label to an image from a predefined set of categories, these datasets offer a large and varied collection of images with corresponding ground truth labels, allowing researchers to develop and benchmark classification algorithms. Similarly, in object detection tasks, where the goal is to identify and localize multiple objects within an image, they provide annotations specifying the bounding boxes or segmentation masks for each object instance, facilitating the training and evaluation of object detection models. Moreover, scene recognition tasks, which involve categorizing images based on their overall scene category (e.g., indoor vs. outdoor), benefit from the diverse scene categories and annotations provided by these datasets. We will explore our proposed MMIS dataset, which contains images from different classes and labels for those images.

<br class="ltx_break">TThe <span id="S2.SS1.p1.6.1" class="ltx_text ltx_font_bold">ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite></span> dataset comprises <math id="S2.SS1.p1.1.m1.3" class="ltx_Math" alttext="14,197,122" display="inline"><semantics id="S2.SS1.p1.1.m1.3a"><mrow id="S2.SS1.p1.1.m1.3.4.2" xref="S2.SS1.p1.1.m1.3.4.1.cmml"><mn id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">14</mn><mo id="S2.SS1.p1.1.m1.3.4.2.1" xref="S2.SS1.p1.1.m1.3.4.1.cmml">,</mo><mn id="S2.SS1.p1.1.m1.2.2" xref="S2.SS1.p1.1.m1.2.2.cmml">197</mn><mo id="S2.SS1.p1.1.m1.3.4.2.2" xref="S2.SS1.p1.1.m1.3.4.1.cmml">,</mo><mn id="S2.SS1.p1.1.m1.3.3" xref="S2.SS1.p1.1.m1.3.3.cmml">122</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.3b"><list id="S2.SS1.p1.1.m1.3.4.1.cmml" xref="S2.SS1.p1.1.m1.3.4.2"><cn type="integer" id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">14</cn><cn type="integer" id="S2.SS1.p1.1.m1.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2">197</cn><cn type="integer" id="S2.SS1.p1.1.m1.3.3.cmml" xref="S2.SS1.p1.1.m1.3.3">122</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.3c">14,197,122</annotation></semantics></math> annotated images organized according to the WordNet hierarchy. Since <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="2010" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mn id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">2010</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><cn type="integer" id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">2010</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">2010</annotation></semantics></math>, it has been utilized in the ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> Large Scale Visual Recognition Challenge (ILSVRC), serving as a benchmark for tasks like image classification and object detection. This dataset includes manually annotated training images and a separate set of test images with withheld annotations. The <span id="S2.SS1.p1.6.2" class="ltx_text ltx_font_bold">CIFAR-100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite></span> dataset, a subset of the Tiny Images dataset, contains $60,000$ color images sized <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="32\times 32" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mrow id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><mn id="S2.SS1.p1.3.m3.1.1.2" xref="S2.SS1.p1.3.m3.1.1.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p1.3.m3.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.cmml">×</mo><mn id="S2.SS1.p1.3.m3.1.1.3" xref="S2.SS1.p1.3.m3.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><times id="S2.SS1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1"></times><cn type="integer" id="S2.SS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2">32</cn><cn type="integer" id="S2.SS1.p1.3.m3.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">32\times 32</annotation></semantics></math> pixels. It encompasses 100 classes, which are further grouped into 20 super-classes. Each class comprises <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="600" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mn id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">600</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><cn type="integer" id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">600</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">600</annotation></semantics></math> images, with both ”fine” labels and ”coarse” labels. The dataset includes 500 training images and 100 testing images per class. It is the same as <span id="S2.SS1.p1.6.3" class="ltx_text ltx_font_bold">CIFAR-10 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite></span> but it is categorized into 10 classes. The <span id="S2.SS1.p1.6.4" class="ltx_text ltx_font_bold">MNIST</span> database is a large collection of handwritten digits. It has a training set of <math id="S2.SS1.p1.5.m5.2" class="ltx_Math" alttext="60,000" display="inline"><semantics id="S2.SS1.p1.5.m5.2a"><mrow id="S2.SS1.p1.5.m5.2.3.2" xref="S2.SS1.p1.5.m5.2.3.1.cmml"><mn id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">60</mn><mo id="S2.SS1.p1.5.m5.2.3.2.1" xref="S2.SS1.p1.5.m5.2.3.1.cmml">,</mo><mn id="S2.SS1.p1.5.m5.2.2" xref="S2.SS1.p1.5.m5.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.2b"><list id="S2.SS1.p1.5.m5.2.3.1.cmml" xref="S2.SS1.p1.5.m5.2.3.2"><cn type="integer" id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">60</cn><cn type="integer" id="S2.SS1.p1.5.m5.2.2.cmml" xref="S2.SS1.p1.5.m5.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.2c">60,000</annotation></semantics></math> examples, and a test set of <math id="S2.SS1.p1.6.m6.2" class="ltx_Math" alttext="10,000" display="inline"><semantics id="S2.SS1.p1.6.m6.2a"><mrow id="S2.SS1.p1.6.m6.2.3.2" xref="S2.SS1.p1.6.m6.2.3.1.cmml"><mn id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml">10</mn><mo id="S2.SS1.p1.6.m6.2.3.2.1" xref="S2.SS1.p1.6.m6.2.3.1.cmml">,</mo><mn id="S2.SS1.p1.6.m6.2.2" xref="S2.SS1.p1.6.m6.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.2b"><list id="S2.SS1.p1.6.m6.2.3.1.cmml" xref="S2.SS1.p1.6.m6.2.3.2"><cn type="integer" id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">10</cn><cn type="integer" id="S2.SS1.p1.6.m6.2.2.cmml" xref="S2.SS1.p1.6.m6.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.2c">10,000</annotation></semantics></math> examples. It is a subset of a larger NIST Special Database 3 and Special Database 1, which contain monochrome images of handwritten digits. The digits have been size-normalized and centered in a fixed-size image. The <span id="S2.SS1.p1.6.5" class="ltx_text ltx_font_bold">Oxford 102 Flower</span> dataset is designed for image classification and comprises 102 categories of flowers commonly found in the United Kingdom. Each category contains between 40 and 258 images. The dataset is challenging due to variations in scale, pose, and lighting across images. However, the proposed MMIS establishes a new challenge to classify those sets of related scenes and styles, which makes it a powerful measure for classification algorithms.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_bold">Image-Text Datasets</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.7" class="ltx_p">Image-text datasets, such as MSCOCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, CelebA-HQ, Visual Genome, and Fashion-Gen Dataset, offer rich resources for exploring the interaction between visual and textual modalities. These datasets provide paired images and textual descriptions, enabling researchers to tackle tasks that require understanding and generating content across both modalities. One key application is image generation, where the goal is to synthesize images from textual descriptions. By leveraging the paired image-text data, researchers can achieve this by utilizing techniques such as generative adversarial networks (GANs) and transformer models to translate text inputs into coherent visual outputs. Additionally, these datasets are valuable for tasks such as visual question answering (VQA), where models are tasked with answering questions about the content of images using both visual and textual cues. Furthermore, researchers can utilize these datasets for tasks like image retrieval, where the textual descriptions serve as additional semantic cues to improve the accuracy and relevance of retrieved images.
The <span id="S2.SS2.p1.7.1" class="ltx_text ltx_font_bold">Microsoft COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite></span> dataset comprises over <math id="S2.SS2.p1.1.m1.2" class="ltx_Math" alttext="200,000" display="inline"><semantics id="S2.SS2.p1.1.m1.2a"><mrow id="S2.SS2.p1.1.m1.2.3.2" xref="S2.SS2.p1.1.m1.2.3.1.cmml"><mn id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">200</mn><mo id="S2.SS2.p1.1.m1.2.3.2.1" xref="S2.SS2.p1.1.m1.2.3.1.cmml">,</mo><mn id="S2.SS2.p1.1.m1.2.2" xref="S2.SS2.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.2b"><list id="S2.SS2.p1.1.m1.2.3.1.cmml" xref="S2.SS2.p1.1.m1.2.3.2"><cn type="integer" id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">200</cn><cn type="integer" id="S2.SS2.p1.1.m1.2.2.cmml" xref="S2.SS2.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.2c">200,000</annotation></semantics></math> high-quality images, each accompanied by multiple human-annotated captions, providing detailed descriptions of the visual content. Annotations include bounding boxes and segmentation masks, ensuring spatial understanding of objects. The dataset’s diversity spans various scenes, objects, and activities, facilitating robust model training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. The <span id="S2.SS2.p1.7.2" class="ltx_text ltx_font_bold">VQA</span> dataset, referred to as VQA dataset19, is tailored for open-ended questions related to images. It comprises <math id="S2.SS2.p1.2.m2.2" class="ltx_Math" alttext="265,016" display="inline"><semantics id="S2.SS2.p1.2.m2.2a"><mrow id="S2.SS2.p1.2.m2.2.3.2" xref="S2.SS2.p1.2.m2.2.3.1.cmml"><mn id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">265</mn><mo id="S2.SS2.p1.2.m2.2.3.2.1" xref="S2.SS2.p1.2.m2.2.3.1.cmml">,</mo><mn id="S2.SS2.p1.2.m2.2.2" xref="S2.SS2.p1.2.m2.2.2.cmml">016</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.2b"><list id="S2.SS2.p1.2.m2.2.3.1.cmml" xref="S2.SS2.p1.2.m2.2.3.2"><cn type="integer" id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">265</cn><cn type="integer" id="S2.SS2.p1.2.m2.2.2.cmml" xref="S2.SS2.p1.2.m2.2.2">016</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.2c">265,016</annotation></semantics></math> images, with each image associated with a minimum of 3 questions (averaging 5.4 questions) and 10 answers provided for each question. The <span id="S2.SS2.p1.7.3" class="ltx_text ltx_font_bold">Fashion-Gen</span> dataset is a recently introduced collection consisting of <math id="S2.SS2.p1.3.m3.2" class="ltx_Math" alttext="293,008" display="inline"><semantics id="S2.SS2.p1.3.m3.2a"><mrow id="S2.SS2.p1.3.m3.2.3.2" xref="S2.SS2.p1.3.m3.2.3.1.cmml"><mn id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml">293</mn><mo id="S2.SS2.p1.3.m3.2.3.2.1" xref="S2.SS2.p1.3.m3.2.3.1.cmml">,</mo><mn id="S2.SS2.p1.3.m3.2.2" xref="S2.SS2.p1.3.m3.2.2.cmml">008</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.2b"><list id="S2.SS2.p1.3.m3.2.3.1.cmml" xref="S2.SS2.p1.3.m3.2.3.2"><cn type="integer" id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">293</cn><cn type="integer" id="S2.SS2.p1.3.m3.2.2.cmml" xref="S2.SS2.p1.3.m3.2.2">008</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.2c">293,008</annotation></semantics></math> high-definition fashion images, each sized at <math id="S2.SS2.p1.4.m4.1" class="ltx_Math" alttext="1360\times 1360" display="inline"><semantics id="S2.SS2.p1.4.m4.1a"><mrow id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml"><mn id="S2.SS2.p1.4.m4.1.1.2" xref="S2.SS2.p1.4.m4.1.1.2.cmml">1360</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p1.4.m4.1.1.1" xref="S2.SS2.p1.4.m4.1.1.1.cmml">×</mo><mn id="S2.SS2.p1.4.m4.1.1.3" xref="S2.SS2.p1.4.m4.1.1.3.cmml">1360</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><apply id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1"><times id="S2.SS2.p1.4.m4.1.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1.1"></times><cn type="integer" id="S2.SS2.p1.4.m4.1.1.2.cmml" xref="S2.SS2.p1.4.m4.1.1.2">1360</cn><cn type="integer" id="S2.SS2.p1.4.m4.1.1.3.cmml" xref="S2.SS2.p1.4.m4.1.1.3">1360</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">1360\times 1360</annotation></semantics></math> pixels. Additionally, this dataset includes item descriptions meticulously crafted by professional stylists, enhancing its utility for various fashion-related tasks. The <span id="S2.SS2.p1.7.4" class="ltx_text ltx_font_bold">Image-Chat</span> dataset is a component of a vast conversational database, containing hundreds of millions of examples. Specifically, it comprises <math id="S2.SS2.p1.5.m5.2" class="ltx_Math" alttext="202,000" display="inline"><semantics id="S2.SS2.p1.5.m5.2a"><mrow id="S2.SS2.p1.5.m5.2.3.2" xref="S2.SS2.p1.5.m5.2.3.1.cmml"><mn id="S2.SS2.p1.5.m5.1.1" xref="S2.SS2.p1.5.m5.1.1.cmml">202</mn><mo id="S2.SS2.p1.5.m5.2.3.2.1" xref="S2.SS2.p1.5.m5.2.3.1.cmml">,</mo><mn id="S2.SS2.p1.5.m5.2.2" xref="S2.SS2.p1.5.m5.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.2b"><list id="S2.SS2.p1.5.m5.2.3.1.cmml" xref="S2.SS2.p1.5.m5.2.3.2"><cn type="integer" id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1">202</cn><cn type="integer" id="S2.SS2.p1.5.m5.2.2.cmml" xref="S2.SS2.p1.5.m5.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.2c">202,000</annotation></semantics></math> dialogues and <math id="S2.SS2.p1.6.m6.2" class="ltx_Math" alttext="401,000" display="inline"><semantics id="S2.SS2.p1.6.m6.2a"><mrow id="S2.SS2.p1.6.m6.2.3.2" xref="S2.SS2.p1.6.m6.2.3.1.cmml"><mn id="S2.SS2.p1.6.m6.1.1" xref="S2.SS2.p1.6.m6.1.1.cmml">401</mn><mo id="S2.SS2.p1.6.m6.2.3.2.1" xref="S2.SS2.p1.6.m6.2.3.1.cmml">,</mo><mn id="S2.SS2.p1.6.m6.2.2" xref="S2.SS2.p1.6.m6.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.6.m6.2b"><list id="S2.SS2.p1.6.m6.2.3.1.cmml" xref="S2.SS2.p1.6.m6.2.3.2"><cn type="integer" id="S2.SS2.p1.6.m6.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1">401</cn><cn type="integer" id="S2.SS2.p1.6.m6.2.2.cmml" xref="S2.SS2.p1.6.m6.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.6.m6.2c">401,000</annotation></semantics></math> utterances, all associated with <math id="S2.SS2.p1.7.m7.2" class="ltx_Math" alttext="202,000" display="inline"><semantics id="S2.SS2.p1.7.m7.2a"><mrow id="S2.SS2.p1.7.m7.2.3.2" xref="S2.SS2.p1.7.m7.2.3.1.cmml"><mn id="S2.SS2.p1.7.m7.1.1" xref="S2.SS2.p1.7.m7.1.1.cmml">202</mn><mo id="S2.SS2.p1.7.m7.2.3.2.1" xref="S2.SS2.p1.7.m7.2.3.1.cmml">,</mo><mn id="S2.SS2.p1.7.m7.2.2" xref="S2.SS2.p1.7.m7.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.7.m7.2b"><list id="S2.SS2.p1.7.m7.2.3.1.cmml" xref="S2.SS2.p1.7.m7.2.3.2"><cn type="integer" id="S2.SS2.p1.7.m7.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1">202</cn><cn type="integer" id="S2.SS2.p1.7.m7.2.2.cmml" xref="S2.SS2.p1.7.m7.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.7.m7.2c">202,000</annotation></semantics></math> images. Additionally, this dataset incorporates 215 possible personality traits, enriching the conversational context for analysis and modeling purposes. Our proposed MMIS contains a detailed description of the different interior design styles, focusing on the semantic meaning of the images and the details of the spatial arrangement of objects, which makes it suitable for image generation and retrieval tasks.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_bold">Image-Text-Audio Datasets</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">While initially focused on images and text, recent developments have seen the inclusion of audio modalities in datasets like <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_bold">CUB-200</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, <span id="S2.SS3.p1.1.2" class="ltx_text ltx_font_bold">MIT Sub-places</span>, and <span id="S2.SS3.p1.1.3" class="ltx_text ltx_font_bold">Flicker30k</span>. Although the original versions of these datasets were devoid of audio annotations, researchers have since expanded them to incorporate audio descriptions for specific subsets. These additions have significantly broadened the scope of multimodal research, especially in tasks like image generation and retrieval. By combining images, text, and now audio, these datasets provide a rich resource where each modality offers complementary insights, enhancing the overall understanding and representation of the underlying concepts. For instance, in CUB-200, the introduction of textual descriptions alongside images of bird species facilitates a deeper semantic comprehension of the depicted birds, despite the absence of original audio annotations. Similarly, MIT Sub-places now includes audio descriptions paired with images, thereby advancing research in audio-visual scene understanding, a feature not present in the dataset’s initial release. Flicker30k, originally focused on images and textual descriptions, has expanded to accommodate audio annotations, allowing researchers to explore the alignment between visual, textual, and auditory cues.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S2.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.1.1.1" class="ltx_p" style="width:42.7pt;">Style</span>
</span>
</th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Bedroom</th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Bathroom</th>
<th id="S2.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Dining Room</th>
<th id="S2.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Living</th>
<th id="S2.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Kitchen</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<th id="S2.T1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_t">
<span id="S2.T1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.1.1.1" class="ltx_p" style="width:42.7pt;">Art-Deco</span>
</span>
</th>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">627</td>
<td id="S2.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">321</td>
<td id="S2.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">745</td>
<td id="S2.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">713</td>
<td id="S2.T1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">788</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<th id="S2.T1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.1.1.1" class="ltx_p" style="width:42.7pt;">Bohemian</span>
</span>
</th>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_center">597</td>
<td id="S2.T1.1.3.2.3" class="ltx_td ltx_align_center">634</td>
<td id="S2.T1.1.3.2.4" class="ltx_td ltx_align_center">687</td>
<td id="S2.T1.1.3.2.5" class="ltx_td ltx_align_center">583</td>
<td id="S2.T1.1.3.2.6" class="ltx_td ltx_align_center">703</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<th id="S2.T1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.1.1.1" class="ltx_p" style="width:42.7pt;">Coastal</span>
</span>
</th>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_align_center">719</td>
<td id="S2.T1.1.4.3.3" class="ltx_td ltx_align_center">824</td>
<td id="S2.T1.1.4.3.4" class="ltx_td ltx_align_center">792</td>
<td id="S2.T1.1.4.3.5" class="ltx_td ltx_align_center">504</td>
<td id="S2.T1.1.4.3.6" class="ltx_td ltx_align_center">894</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<th id="S2.T1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.1.1.1" class="ltx_p" style="width:42.7pt;">Transitional</span>
</span>
</th>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_align_center">816</td>
<td id="S2.T1.1.5.4.3" class="ltx_td ltx_align_center">845</td>
<td id="S2.T1.1.5.4.4" class="ltx_td ltx_align_center">806</td>
<td id="S2.T1.1.5.4.5" class="ltx_td ltx_align_center">842</td>
<td id="S2.T1.1.5.4.6" class="ltx_td ltx_align_center">782</td>
</tr>
<tr id="S2.T1.1.6.5" class="ltx_tr">
<th id="S2.T1.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.6.5.1.1.1" class="ltx_p" style="width:42.7pt;">Mediterranean</span>
</span>
</th>
<td id="S2.T1.1.6.5.2" class="ltx_td ltx_align_center">748</td>
<td id="S2.T1.1.6.5.3" class="ltx_td ltx_align_center">631</td>
<td id="S2.T1.1.6.5.4" class="ltx_td ltx_align_center">630</td>
<td id="S2.T1.1.6.5.5" class="ltx_td ltx_align_center">756</td>
<td id="S2.T1.1.6.5.6" class="ltx_td ltx_align_center">865</td>
</tr>
<tr id="S2.T1.1.7.6" class="ltx_tr">
<th id="S2.T1.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.7.6.1.1.1" class="ltx_p" style="width:42.7pt;">Eclectic</span>
</span>
</th>
<td id="S2.T1.1.7.6.2" class="ltx_td ltx_align_center">861</td>
<td id="S2.T1.1.7.6.3" class="ltx_td ltx_align_center">858</td>
<td id="S2.T1.1.7.6.4" class="ltx_td ltx_align_center">828</td>
<td id="S2.T1.1.7.6.5" class="ltx_td ltx_align_center">851</td>
<td id="S2.T1.1.7.6.6" class="ltx_td ltx_align_center">850</td>
</tr>
<tr id="S2.T1.1.8.7" class="ltx_tr">
<th id="S2.T1.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.8.7.1.1.1" class="ltx_p" style="width:42.7pt;">Rustic</span>
</span>
</th>
<td id="S2.T1.1.8.7.2" class="ltx_td ltx_align_center">714</td>
<td id="S2.T1.1.8.7.3" class="ltx_td ltx_align_center">864</td>
<td id="S2.T1.1.8.7.4" class="ltx_td ltx_align_center">821</td>
<td id="S2.T1.1.8.7.5" class="ltx_td ltx_align_center">803</td>
<td id="S2.T1.1.8.7.6" class="ltx_td ltx_align_center">947</td>
</tr>
<tr id="S2.T1.1.9.8" class="ltx_tr">
<th id="S2.T1.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.9.8.1.1.1" class="ltx_p" style="width:42.7pt;">Mid-Century</span>
</span>
</th>
<td id="S2.T1.1.9.8.2" class="ltx_td ltx_align_center">702</td>
<td id="S2.T1.1.9.8.3" class="ltx_td ltx_align_center">790</td>
<td id="S2.T1.1.9.8.4" class="ltx_td ltx_align_center">678</td>
<td id="S2.T1.1.9.8.5" class="ltx_td ltx_align_center">810</td>
<td id="S2.T1.1.9.8.6" class="ltx_td ltx_align_center">800</td>
</tr>
<tr id="S2.T1.1.10.9" class="ltx_tr">
<th id="S2.T1.1.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.10.9.1.1.1" class="ltx_p" style="width:42.7pt;">Farmhouse</span>
</span>
</th>
<td id="S2.T1.1.10.9.2" class="ltx_td ltx_align_center">642</td>
<td id="S2.T1.1.10.9.3" class="ltx_td ltx_align_center">802</td>
<td id="S2.T1.1.10.9.4" class="ltx_td ltx_align_center">794</td>
<td id="S2.T1.1.10.9.5" class="ltx_td ltx_align_center">805</td>
<td id="S2.T1.1.10.9.6" class="ltx_td ltx_align_center">803</td>
</tr>
<tr id="S2.T1.1.11.10" class="ltx_tr">
<th id="S2.T1.1.11.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.11.10.1.1.1" class="ltx_p" style="width:42.7pt;">Japanese Zen</span>
</span>
</th>
<td id="S2.T1.1.11.10.2" class="ltx_td ltx_align_center">721</td>
<td id="S2.T1.1.11.10.3" class="ltx_td ltx_align_center">937</td>
<td id="S2.T1.1.11.10.4" class="ltx_td ltx_align_center">866</td>
<td id="S2.T1.1.11.10.5" class="ltx_td ltx_align_center">736</td>
<td id="S2.T1.1.11.10.6" class="ltx_td ltx_align_center">712</td>
</tr>
<tr id="S2.T1.1.12.11" class="ltx_tr">
<th id="S2.T1.1.12.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.12.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.12.11.1.1.1" class="ltx_p" style="width:42.7pt;">Mediterranean</span>
</span>
</th>
<td id="S2.T1.1.12.11.2" class="ltx_td ltx_align_center">1005</td>
<td id="S2.T1.1.12.11.3" class="ltx_td ltx_align_center">783</td>
<td id="S2.T1.1.12.11.4" class="ltx_td ltx_align_center">607</td>
<td id="S2.T1.1.12.11.5" class="ltx_td ltx_align_center">820</td>
<td id="S2.T1.1.12.11.6" class="ltx_td ltx_align_center">786</td>
</tr>
<tr id="S2.T1.1.13.12" class="ltx_tr">
<th id="S2.T1.1.13.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.13.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.13.12.1.1.1" class="ltx_p" style="width:42.7pt;">Victorian</span>
</span>
</th>
<td id="S2.T1.1.13.12.2" class="ltx_td ltx_align_center">1046</td>
<td id="S2.T1.1.13.12.3" class="ltx_td ltx_align_center">984</td>
<td id="S2.T1.1.13.12.4" class="ltx_td ltx_align_center">921</td>
<td id="S2.T1.1.13.12.5" class="ltx_td ltx_align_center">929</td>
<td id="S2.T1.1.13.12.6" class="ltx_td ltx_align_center">600</td>
</tr>
<tr id="S2.T1.1.14.13" class="ltx_tr">
<th id="S2.T1.1.14.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.14.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.14.13.1.1.1" class="ltx_p" style="width:42.7pt;">French</span>
</span>
</th>
<td id="S2.T1.1.14.13.2" class="ltx_td ltx_align_center">1002</td>
<td id="S2.T1.1.14.13.3" class="ltx_td ltx_align_center">852</td>
<td id="S2.T1.1.14.13.4" class="ltx_td ltx_align_center">882</td>
<td id="S2.T1.1.14.13.5" class="ltx_td ltx_align_center">751</td>
<td id="S2.T1.1.14.13.6" class="ltx_td ltx_align_center">870</td>
</tr>
<tr id="S2.T1.1.15.14" class="ltx_tr">
<th id="S2.T1.1.15.14.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.15.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.15.14.1.1.1" class="ltx_p" style="width:42.7pt;">Tropical</span>
</span>
</th>
<td id="S2.T1.1.15.14.2" class="ltx_td ltx_align_center">853</td>
<td id="S2.T1.1.15.14.3" class="ltx_td ltx_align_center">980</td>
<td id="S2.T1.1.15.14.4" class="ltx_td ltx_align_center">714</td>
<td id="S2.T1.1.15.14.5" class="ltx_td ltx_align_center">978</td>
<td id="S2.T1.1.15.14.6" class="ltx_td ltx_align_center">783</td>
</tr>
<tr id="S2.T1.1.16.15" class="ltx_tr">
<th id="S2.T1.1.16.15.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.16.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.16.15.1.1.1" class="ltx_p" style="width:42.7pt;">Southwestern</span>
</span>
</th>
<td id="S2.T1.1.16.15.2" class="ltx_td ltx_align_center">882</td>
<td id="S2.T1.1.16.15.3" class="ltx_td ltx_align_center">900</td>
<td id="S2.T1.1.16.15.4" class="ltx_td ltx_align_center">1000</td>
<td id="S2.T1.1.16.15.5" class="ltx_td ltx_align_center">980</td>
<td id="S2.T1.1.16.15.6" class="ltx_td ltx_align_center">780</td>
</tr>
<tr id="S2.T1.1.17.16" class="ltx_tr">
<th id="S2.T1.1.17.16.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.17.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.17.16.1.1.1" class="ltx_p" style="width:42.7pt;">Gothic</span>
</span>
</th>
<td id="S2.T1.1.17.16.2" class="ltx_td ltx_align_center">642</td>
<td id="S2.T1.1.17.16.3" class="ltx_td ltx_align_center">988</td>
<td id="S2.T1.1.17.16.4" class="ltx_td ltx_align_center">683</td>
<td id="S2.T1.1.17.16.5" class="ltx_td ltx_align_center">720</td>
<td id="S2.T1.1.17.16.6" class="ltx_td ltx_align_center">975</td>
</tr>
<tr id="S2.T1.1.18.17" class="ltx_tr">
<th id="S2.T1.1.18.17.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.18.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.18.17.1.1.1" class="ltx_p" style="width:42.7pt;">Nouveau</span>
</span>
</th>
<td id="S2.T1.1.18.17.2" class="ltx_td ltx_align_center">1028</td>
<td id="S2.T1.1.18.17.3" class="ltx_td ltx_align_center">783</td>
<td id="S2.T1.1.18.17.4" class="ltx_td ltx_align_center">691</td>
<td id="S2.T1.1.18.17.5" class="ltx_td ltx_align_center">714</td>
<td id="S2.T1.1.18.17.6" class="ltx_td ltx_align_center">1011</td>
</tr>
<tr id="S2.T1.1.19.18" class="ltx_tr">
<th id="S2.T1.1.19.18.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.19.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.19.18.1.1.1" class="ltx_p" style="width:42.7pt;">Retro</span>
</span>
</th>
<td id="S2.T1.1.19.18.2" class="ltx_td ltx_align_center">707</td>
<td id="S2.T1.1.19.18.3" class="ltx_td ltx_align_center">704</td>
<td id="S2.T1.1.19.18.4" class="ltx_td ltx_align_center">1025</td>
<td id="S2.T1.1.19.18.5" class="ltx_td ltx_align_center">744</td>
<td id="S2.T1.1.19.18.6" class="ltx_td ltx_align_center">849</td>
</tr>
<tr id="S2.T1.1.20.19" class="ltx_tr">
<th id="S2.T1.1.20.19.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.20.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.20.19.1.1.1" class="ltx_p" style="width:42.7pt;">Bauhaus</span>
</span>
</th>
<td id="S2.T1.1.20.19.2" class="ltx_td ltx_align_center">668</td>
<td id="S2.T1.1.20.19.3" class="ltx_td ltx_align_center">821</td>
<td id="S2.T1.1.20.19.4" class="ltx_td ltx_align_center">904</td>
<td id="S2.T1.1.20.19.5" class="ltx_td ltx_align_center">784</td>
<td id="S2.T1.1.20.19.6" class="ltx_td ltx_align_center">1028</td>
</tr>
<tr id="S2.T1.1.21.20" class="ltx_tr">
<th id="S2.T1.1.21.20.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.21.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.21.20.1.1.1" class="ltx_p" style="width:42.7pt;">Nautical</span>
</span>
</th>
<td id="S2.T1.1.21.20.2" class="ltx_td ltx_align_center">936</td>
<td id="S2.T1.1.21.20.3" class="ltx_td ltx_align_center">721</td>
<td id="S2.T1.1.21.20.4" class="ltx_td ltx_align_center">711</td>
<td id="S2.T1.1.21.20.5" class="ltx_td ltx_align_center">637</td>
<td id="S2.T1.1.21.20.6" class="ltx_td ltx_align_center">769</td>
</tr>
<tr id="S2.T1.1.22.21" class="ltx_tr">
<th id="S2.T1.1.22.21.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.22.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.22.21.1.1.1" class="ltx_p" style="width:42.7pt;">Hollywood</span>
</span>
</th>
<td id="S2.T1.1.22.21.2" class="ltx_td ltx_align_center">850</td>
<td id="S2.T1.1.22.21.3" class="ltx_td ltx_align_center">1048</td>
<td id="S2.T1.1.22.21.4" class="ltx_td ltx_align_center">989</td>
<td id="S2.T1.1.22.21.5" class="ltx_td ltx_align_center">814</td>
<td id="S2.T1.1.22.21.6" class="ltx_td ltx_align_center">669</td>
</tr>
<tr id="S2.T1.1.23.22" class="ltx_tr">
<th id="S2.T1.1.23.22.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.23.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.23.22.1.1.1" class="ltx_p" style="width:42.7pt;">Moroccan</span>
</span>
</th>
<td id="S2.T1.1.23.22.2" class="ltx_td ltx_align_center">626</td>
<td id="S2.T1.1.23.22.3" class="ltx_td ltx_align_center">684</td>
<td id="S2.T1.1.23.22.4" class="ltx_td ltx_align_center">753</td>
<td id="S2.T1.1.23.22.5" class="ltx_td ltx_align_center">877</td>
<td id="S2.T1.1.23.22.6" class="ltx_td ltx_align_center">1093</td>
</tr>
<tr id="S2.T1.1.24.23" class="ltx_tr">
<th id="S2.T1.1.24.23.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.24.23.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.24.23.1.1.1" class="ltx_p" style="width:42.7pt;">Tribal</span>
</span>
</th>
<td id="S2.T1.1.24.23.2" class="ltx_td ltx_align_center">920</td>
<td id="S2.T1.1.24.23.3" class="ltx_td ltx_align_center">889</td>
<td id="S2.T1.1.24.23.4" class="ltx_td ltx_align_center">1025</td>
<td id="S2.T1.1.24.23.5" class="ltx_td ltx_align_center">607</td>
<td id="S2.T1.1.24.23.6" class="ltx_td ltx_align_center">996</td>
</tr>
<tr id="S2.T1.1.25.24" class="ltx_tr">
<th id="S2.T1.1.25.24.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.25.24.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.25.24.1.1.1" class="ltx_p" style="width:42.7pt;">Asian-inspired</span>
</span>
</th>
<td id="S2.T1.1.25.24.2" class="ltx_td ltx_align_center">904</td>
<td id="S2.T1.1.25.24.3" class="ltx_td ltx_align_center">867</td>
<td id="S2.T1.1.25.24.4" class="ltx_td ltx_align_center">951</td>
<td id="S2.T1.1.25.24.5" class="ltx_td ltx_align_center">1057</td>
<td id="S2.T1.1.25.24.6" class="ltx_td ltx_align_center">790</td>
</tr>
<tr id="S2.T1.1.26.25" class="ltx_tr">
<th id="S2.T1.1.26.25.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.26.25.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.26.25.1.1.1" class="ltx_p" style="width:42.7pt;">Urban</span>
</span>
</th>
<td id="S2.T1.1.26.25.2" class="ltx_td ltx_align_center">863</td>
<td id="S2.T1.1.26.25.3" class="ltx_td ltx_align_center">825</td>
<td id="S2.T1.1.26.25.4" class="ltx_td ltx_align_center">740</td>
<td id="S2.T1.1.26.25.5" class="ltx_td ltx_align_center">826</td>
<td id="S2.T1.1.26.25.6" class="ltx_td ltx_align_center">1055</td>
</tr>
<tr id="S2.T1.1.27.26" class="ltx_tr">
<th id="S2.T1.1.27.26.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.27.26.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.27.26.1.1.1" class="ltx_p" style="width:42.7pt;">Cottagecore</span>
</span>
</th>
<td id="S2.T1.1.27.26.2" class="ltx_td ltx_align_center">786</td>
<td id="S2.T1.1.27.26.3" class="ltx_td ltx_align_center">695</td>
<td id="S2.T1.1.27.26.4" class="ltx_td ltx_align_center">605</td>
<td id="S2.T1.1.27.26.5" class="ltx_td ltx_align_center">786</td>
<td id="S2.T1.1.27.26.6" class="ltx_td ltx_align_center">1040</td>
</tr>
<tr id="S2.T1.1.28.27" class="ltx_tr">
<th id="S2.T1.1.28.27.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.28.27.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.28.27.1.1.1" class="ltx_p" style="width:42.7pt;">Contemporary-classic</span>
</span>
</th>
<td id="S2.T1.1.28.27.2" class="ltx_td ltx_align_center">990</td>
<td id="S2.T1.1.28.27.3" class="ltx_td ltx_align_center">669</td>
<td id="S2.T1.1.28.27.4" class="ltx_td ltx_align_center">1036</td>
<td id="S2.T1.1.28.27.5" class="ltx_td ltx_align_center">872</td>
<td id="S2.T1.1.28.27.6" class="ltx_td ltx_align_center">670</td>
</tr>
<tr id="S2.T1.1.29.28" class="ltx_tr">
<th id="S2.T1.1.29.28.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.29.28.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.29.28.1.1.1" class="ltx_p" style="width:42.7pt;">Retro</span>
</span>
</th>
<td id="S2.T1.1.29.28.2" class="ltx_td ltx_align_center">763</td>
<td id="S2.T1.1.29.28.3" class="ltx_td ltx_align_center">682</td>
<td id="S2.T1.1.29.28.4" class="ltx_td ltx_align_center">723</td>
<td id="S2.T1.1.29.28.5" class="ltx_td ltx_align_center">772</td>
<td id="S2.T1.1.29.28.6" class="ltx_td ltx_align_center">1050</td>
</tr>
<tr id="S2.T1.1.30.29" class="ltx_tr">
<th id="S2.T1.1.30.29.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.30.29.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.30.29.1.1.1" class="ltx_p" style="width:42.7pt;">Zen</span>
</span>
</th>
<td id="S2.T1.1.30.29.2" class="ltx_td ltx_align_center">764</td>
<td id="S2.T1.1.30.29.3" class="ltx_td ltx_align_center">693</td>
<td id="S2.T1.1.30.29.4" class="ltx_td ltx_align_center">658</td>
<td id="S2.T1.1.30.29.5" class="ltx_td ltx_align_center">962</td>
<td id="S2.T1.1.30.29.6" class="ltx_td ltx_align_center">1082</td>
</tr>
<tr id="S2.T1.1.31.30" class="ltx_tr">
<th id="S2.T1.1.31.30.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.31.30.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.31.30.1.1.1" class="ltx_p" style="width:42.7pt;">Steampunk</span>
</span>
</th>
<td id="S2.T1.1.31.30.2" class="ltx_td ltx_align_center">827</td>
<td id="S2.T1.1.31.30.3" class="ltx_td ltx_align_center">853</td>
<td id="S2.T1.1.31.30.4" class="ltx_td ltx_align_center">1080</td>
<td id="S2.T1.1.31.30.5" class="ltx_td ltx_align_center">818</td>
<td id="S2.T1.1.31.30.6" class="ltx_td ltx_align_center">704</td>
</tr>
<tr id="S2.T1.1.32.31" class="ltx_tr">
<th id="S2.T1.1.32.31.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.32.31.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.32.31.1.1.1" class="ltx_p" style="width:42.7pt;">Space-saving</span>
</span>
</th>
<td id="S2.T1.1.32.31.2" class="ltx_td ltx_align_center">988</td>
<td id="S2.T1.1.32.31.3" class="ltx_td ltx_align_center">967</td>
<td id="S2.T1.1.32.31.4" class="ltx_td ltx_align_center">994</td>
<td id="S2.T1.1.32.31.5" class="ltx_td ltx_align_center">870</td>
<td id="S2.T1.1.32.31.6" class="ltx_td ltx_align_center">1023</td>
</tr>
<tr id="S2.T1.1.33.32" class="ltx_tr">
<th id="S2.T1.1.33.32.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.33.32.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.33.32.1.1.1" class="ltx_p" style="width:42.7pt;">High-tech</span>
</span>
</th>
<td id="S2.T1.1.33.32.2" class="ltx_td ltx_align_center">644</td>
<td id="S2.T1.1.33.32.3" class="ltx_td ltx_align_center">642</td>
<td id="S2.T1.1.33.32.4" class="ltx_td ltx_align_center">1053</td>
<td id="S2.T1.1.33.32.5" class="ltx_td ltx_align_center">884</td>
<td id="S2.T1.1.33.32.6" class="ltx_td ltx_align_center">925</td>
</tr>
<tr id="S2.T1.1.34.33" class="ltx_tr">
<th id="S2.T1.1.34.33.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.34.33.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.34.33.1.1.1" class="ltx_p" style="width:42.7pt;">Neoclassical</span>
</span>
</th>
<td id="S2.T1.1.34.33.2" class="ltx_td ltx_align_center">1062</td>
<td id="S2.T1.1.34.33.3" class="ltx_td ltx_align_center">1049</td>
<td id="S2.T1.1.34.33.4" class="ltx_td ltx_align_center">1046</td>
<td id="S2.T1.1.34.33.5" class="ltx_td ltx_align_center">637</td>
<td id="S2.T1.1.34.33.6" class="ltx_td ltx_align_center">1043</td>
</tr>
<tr id="S2.T1.1.35.34" class="ltx_tr">
<th id="S2.T1.1.35.34.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row">
<span id="S2.T1.1.35.34.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.35.34.1.1.1" class="ltx_p" style="width:42.7pt;">Scandinavian</span>
</span>
</th>
<td id="S2.T1.1.35.34.2" class="ltx_td ltx_align_center">831</td>
<td id="S2.T1.1.35.34.3" class="ltx_td ltx_align_center">765</td>
<td id="S2.T1.1.35.34.4" class="ltx_td ltx_align_center">1074</td>
<td id="S2.T1.1.35.34.5" class="ltx_td ltx_align_center">608</td>
<td id="S2.T1.1.35.34.6" class="ltx_td ltx_align_center">1057</td>
</tr>
<tr id="S2.T1.1.36.35" class="ltx_tr">
<th id="S2.T1.1.36.35.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_bb">
<span id="S2.T1.1.36.35.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.36.35.1.1.1" class="ltx_p" style="width:42.7pt;">Neo-tradition</span>
</span>
</th>
<td id="S2.T1.1.36.35.2" class="ltx_td ltx_align_center ltx_border_bb">1080</td>
<td id="S2.T1.1.36.35.3" class="ltx_td ltx_align_center ltx_border_bb">941</td>
<td id="S2.T1.1.36.35.4" class="ltx_td ltx_align_center ltx_border_bb">848</td>
<td id="S2.T1.1.36.35.5" class="ltx_td ltx_align_center ltx_border_bb">671</td>
<td id="S2.T1.1.36.35.6" class="ltx_td ltx_align_center ltx_border_bb">804</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Dataset statstics</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_bold">Methodology</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we outline the steps taken to create and refine the dataset. We explain how we collected, cleaned, pre-processed, and annotated the data, ensuring its quality and reliability. Our goal is to provide a clear understanding of the process involved in integrating images, textual descriptions, and audio recordings into the dataset. By detailing each phase of the methodology, we aim to create a dataset that can be used effectively for research in multi-modal scene understanding and related areas.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_bold">image data</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The initial phase of dataset creation involved a systematic approach to sourcing images from various online platforms. Recognizing that the detailed layout of any apartment typically encompasses five primary rooms—living room, bedroom, dining room, bathroom, and kitchen—we structured our search methodology around these fundamental spaces. Additionally, we acknowledged the diverse spectrum of interior design styles prevalent in contemporary spaces, ranging from minimalist and Moroccan to bohemian and modern. To ensure comprehensive coverage, we conducted focused searches for each style in conjunction with the five rooms, thereby facilitating a holistic representation of interior design aesthetics. Subsequently, leveraging automated web scraping techniques, we systematically extracted images from reliable online sources, ensuring adherence to copyright regulations and ethical considerations. The MMIS dataset was then subjected to cleaning procedures to eliminate duplicates, low-quality representations, and irrelevant content, thereby refining its composition to include only high-fidelity, pertinent imagery. Additionally, recognizing the importance of standardization for downstream processing tasks, all collected images were resized to a uniform dimension of <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mn id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">256</cn><cn type="integer" id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">256\times 256</annotation></semantics></math> pixels, ensuring consistency across the dataset. By organizing and standardizing the dataset through a combination of systematic search, automated extraction, and rigorous cleaning procedures, we ensured the integrity and reliability of the collected dataset. This curated dataset forms the foundation for subsequent annotation and pre-processing stages, enabling comprehensive analysis and exploration of multi-modal scene understanding tasks.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_bold"> Text Data</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Image captions play a pivotal role in enriching the semantic understanding of visual data, providing textual descriptions that complement and contextualize the associated images. In the context of our dataset, captions serve as invaluable annotations, offering insights into the spatial arrangement, stylistic elements, and functional attributes of interior design compositions. By associating each image with a descriptive caption, we augment the dataset with rich, multi-modal information, enabling deeper comprehension and analysis of the underlying visual scenes. To automate the process of caption generation, we employed the LLaVA v2 model—a state-of-the-art architecture tailored for visual question-answering (VQA) tasks. The LLaVA v2 model leverages advancements in deep learning and natural language processing to generate coherent and contextually relevant captions in response to queries posed about the interior design depicted in the images. This approach not only streamlines the annotation process but also enhances the interpretability and accessibility of the dataset, facilitating diverse applications ranging from image retrieval to content generation. At its core, the LLaVA v2 model harnesses a transformer-based architecture, known for its ability to capture long-range dependencies and semantic relationships in sequential data. Through a process of self-attention mechanisms, the model dynamically weighs the importance of different regions within the input image and integrates this information with contextual cues from the query text. By iteratively attending to relevant image features and refining its predictions based on feedback signals, the model generates captions that are tailored to the specific attributes and nuances of the interior design depicted in each image. By integrating the LLaVA v2 model into the dataset creation pipeline, we not only streamline the annotation process but also enhance the richness and granularity of the dataset annotations. The resulting dataset, enriched with descriptive captions generated by the LLaVA v2 model, serves as a valuable resource for advancing research in multi-modal interior scene generation and recognition, image captioning, and related tasks.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic"> </span><span id="S3.SS3.7.3" class="ltx_text ltx_font_bold">Audio Data</span><span id="S3.SS3.8.4" class="ltx_text ltx_font_italic"> </span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Incorporating audio data into the MMIS allows for a more comprehensive understanding and interpretation of interior design scenes. To generate speech corresponding to the textual descriptions associated with each image, we utilized the Multi-Speaker Neural Text-to-Speech model—a cutting-edge architecture designed to synthesize high-quality speech with various voices from a single model. The importance of audio data lies in its ability to provide auditory context and sensory cues that complement the visual and textual aspects of the dataset. By incorporating speech synthesis capabilities, we enhance the accessibility and inclusivity of the dataset. Additionally, audio annotations offer valuable supplementary information, enabling a deeper analysis and understanding of different data composition techniques. The Multi-Speaker Neural Text-to-Speech model introduces a novel technique for augmenting neural text-to-speech synthesis with low-dimensional trainable speaker embeddings, allowing for the generation of different voices from a single model. This approach builds upon the advancements made in single-speaker neural TTS models such as Deep Voice 1 and Tacotron, demonstrating improvements in audio quality and speaker diversity. The Multi-Speaker Neural Text-to-Speech model demonstrates the capability for multi-speaker speech synthesis, allowing a single neural TTS system to learn hundreds of unique voices from minimal data per speaker while preserving speaker identities almost perfectly. The model synthesizes high-quality speech with diverse voices, enriching the dataset with a spectrum of auditory experiences. By integrating audio data into the dataset, we enhance the richness and diversity of the multi-modal annotations, facilitating advanced research and applications in multi-modal scene understanding and related domains.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">DataSet</span></th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">No. of images</span></th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Image resolution</span></th>
<th id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">No. of classes</span></th>
<th id="S3.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.1.1.1.5.1" class="ltx_text ltx_font_bold">Image</span></th>
<th id="S3.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.1.1.1.6.1" class="ltx_text ltx_font_bold">Text</span></th>
<th id="S3.T2.1.1.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.1.1.1.7.1" class="ltx_text ltx_font_bold">Audio</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<td id="S3.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">COCO</td>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">123,287</td>
<td id="S3.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">Arbitrary</td>
<td id="S3.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">_</td>
<td id="S3.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">Yes</td>
<td id="S3.T2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">yes</td>
<td id="S3.T2.1.2.1.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">no</td>
</tr>
<tr id="S3.T2.1.3.2" class="ltx_tr">
<td id="S3.T2.1.3.2.1" class="ltx_td ltx_align_left">Flickr</td>
<td id="S3.T2.1.3.2.2" class="ltx_td ltx_align_center">100,000</td>
<td id="S3.T2.1.3.2.3" class="ltx_td ltx_align_center">Variable</td>
<td id="S3.T2.1.3.2.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.3.2.5" class="ltx_td ltx_align_center">Yes</td>
<td id="S3.T2.1.3.2.6" class="ltx_td ltx_align_center">yes</td>
<td id="S3.T2.1.3.2.7" class="ltx_td ltx_nopad_r ltx_align_center">no</td>
</tr>
<tr id="S3.T2.1.4.3" class="ltx_tr">
<td id="S3.T2.1.4.3.1" class="ltx_td ltx_align_left">MIT-places</td>
<td id="S3.T2.1.4.3.2" class="ltx_td ltx_align_center">10,000</td>
<td id="S3.T2.1.4.3.3" class="ltx_td ltx_align_center">Variable</td>
<td id="S3.T2.1.4.3.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.1.4.3.5" class="ltx_td ltx_align_center">Yes</td>
<td id="S3.T2.1.4.3.6" class="ltx_td ltx_align_center">yes</td>
<td id="S3.T2.1.4.3.7" class="ltx_td ltx_nopad_r ltx_align_center">No</td>
</tr>
<tr id="S3.T2.1.5.4" class="ltx_tr">
<td id="S3.T2.1.5.4.1" class="ltx_td ltx_align_left">COCO</td>
<td id="S3.T2.1.5.4.2" class="ltx_td ltx_align_center">330,000</td>
<td id="S3.T2.1.5.4.3" class="ltx_td ltx_align_center">Variable</td>
<td id="S3.T2.1.5.4.4" class="ltx_td ltx_align_center">80</td>
<td id="S3.T2.1.5.4.5" class="ltx_td ltx_align_center">Yes</td>
<td id="S3.T2.1.5.4.6" class="ltx_td ltx_align_center">No</td>
<td id="S3.T2.1.5.4.7" class="ltx_td ltx_nopad_r ltx_align_center">No</td>
</tr>
<tr id="S3.T2.1.6.5" class="ltx_tr">
<td id="S3.T2.1.6.5.1" class="ltx_td ltx_align_left">CIFAR</td>
<td id="S3.T2.1.6.5.2" class="ltx_td ltx_align_center">60,000</td>
<td id="S3.T2.1.6.5.3" class="ltx_td ltx_align_center">32x32</td>
<td id="S3.T2.1.6.5.4" class="ltx_td ltx_align_center">10</td>
<td id="S3.T2.1.6.5.5" class="ltx_td ltx_align_center">Yes</td>
<td id="S3.T2.1.6.5.6" class="ltx_td ltx_align_center">No</td>
<td id="S3.T2.1.6.5.7" class="ltx_td ltx_nopad_r ltx_align_center">No</td>
</tr>
<tr id="S3.T2.1.7.6" class="ltx_tr">
<td id="S3.T2.1.7.6.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S3.T2.1.7.6.1.1" class="ltx_text ltx_font_bold">MMIS (Ours)</span></td>
<td id="S3.T2.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb">200,000</td>
<td id="S3.T2.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb">256x256</td>
<td id="S3.T2.1.7.6.4" class="ltx_td ltx_align_center ltx_border_bb">30</td>
<td id="S3.T2.1.7.6.5" class="ltx_td ltx_align_center ltx_border_bb">Yes</td>
<td id="S3.T2.1.7.6.6" class="ltx_td ltx_align_center ltx_border_bb">Yes</td>
<td id="S3.T2.1.7.6.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">Yes</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Comparison in terms of modalities availability with common Datasets</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_bold">MMIS Dataset</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.4" class="ltx_p">Our multi-modal dataset, termed ”MMIS,” is specifically curated to facilitate research and development in multimodal deep learning. The dataset comprises a total of <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S4.p1.1.m1.1a"><mn id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><cn type="integer" id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">40</annotation></semantics></math> different classes, each representing a distinct interior design style. These styles encompass a wide spectrum of aesthetics, ranging from modern and minimalist to classic and eclectic. Each class within the dataset is further subdivided into five distinct rooms commonly found in interior spaces: living room, bedroom, dining room, bathroom, and kitchen. The dataset is meticulously curated to ensure diversity and comprehensiveness, with each class containing a variable number of images ranging from <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="500" display="inline"><semantics id="S4.p1.2.m2.1a"><mn id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><cn type="integer" id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">500</annotation></semantics></math> to <math id="S4.p1.3.m3.2" class="ltx_Math" alttext="1,000" display="inline"><semantics id="S4.p1.3.m3.2a"><mrow id="S4.p1.3.m3.2.3.2" xref="S4.p1.3.m3.2.3.1.cmml"><mn id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">1</mn><mo id="S4.p1.3.m3.2.3.2.1" xref="S4.p1.3.m3.2.3.1.cmml">,</mo><mn id="S4.p1.3.m3.2.2" xref="S4.p1.3.m3.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.2b"><list id="S4.p1.3.m3.2.3.1.cmml" xref="S4.p1.3.m3.2.3.2"><cn type="integer" id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">1</cn><cn type="integer" id="S4.p1.3.m3.2.2.cmml" xref="S4.p1.3.m3.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.2c">1,000</annotation></semantics></math>. In total, the dataset comprises approximately <math id="S4.p1.4.m4.2" class="ltx_Math" alttext="200,000" display="inline"><semantics id="S4.p1.4.m4.2a"><mrow id="S4.p1.4.m4.2.3.2" xref="S4.p1.4.m4.2.3.1.cmml"><mn id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml">200</mn><mo id="S4.p1.4.m4.2.3.2.1" xref="S4.p1.4.m4.2.3.1.cmml">,</mo><mn id="S4.p1.4.m4.2.2" xref="S4.p1.4.m4.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.2b"><list id="S4.p1.4.m4.2.3.1.cmml" xref="S4.p1.4.m4.2.3.2"><cn type="integer" id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1">200</cn><cn type="integer" id="S4.p1.4.m4.2.2.cmml" xref="S4.p1.4.m4.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.2c">200,000</annotation></semantics></math> samples, providing a rich and extensive resource for training, validation, and evaluation purposes. For each image in the dataset, accompanying textual descriptions are provided to capture the semantic content and contextual details of the interior space depicted. Additionally, speech annotations corresponding to the textual descriptions are included.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Table-1</span> This section summarizes the key statistics of the MMIS dataset, including the number of classes, rooms per class, images per room, and the total number of samples. These statistics provide insights into the composition and scale of the dataset, highlighting its richness and diversity as a resource for interior design research and development.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_bold">Benchmark</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">To thoroughly assess the quality and versatility of our interior design multimodal dataset, we designed and conducted an extensive suite of benchmark experiments, focusing on two primary machine learning tasks: classification and image generation.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S5.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Models</span></th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Without fine-tuning</span></th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">With fine-tuning</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<th id="S5.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">VGG16</th>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">42%</td>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">82%</td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<th id="S5.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VGG19</th>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_center">44.7%</td>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_nopad_r ltx_align_center">87%</td>
</tr>
<tr id="S5.T3.1.4.3" class="ltx_tr">
<th id="S5.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">RESNET18</th>
<td id="S5.T3.1.4.3.2" class="ltx_td ltx_align_center">42%</td>
<td id="S5.T3.1.4.3.3" class="ltx_td ltx_nopad_r ltx_align_center">80%</td>
</tr>
<tr id="S5.T3.1.5.4" class="ltx_tr">
<th id="S5.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">RESNET34</th>
<td id="S5.T3.1.5.4.2" class="ltx_td ltx_align_center">40%</td>
<td id="S5.T3.1.5.4.3" class="ltx_td ltx_nopad_r ltx_align_center">82%</td>
</tr>
<tr id="S5.T3.1.6.5" class="ltx_tr">
<th id="S5.T3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">RESNET50</th>
<td id="S5.T3.1.6.5.2" class="ltx_td ltx_align_center">42%</td>
<td id="S5.T3.1.6.5.3" class="ltx_td ltx_nopad_r ltx_align_center">84%</td>
</tr>
<tr id="S5.T3.1.7.6" class="ltx_tr">
<th id="S5.T3.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EfficientNetB0</th>
<td id="S5.T3.1.7.6.2" class="ltx_td ltx_align_center">45%</td>
<td id="S5.T3.1.7.6.3" class="ltx_td ltx_nopad_r ltx_align_center">88%</td>
</tr>
<tr id="S5.T3.1.8.7" class="ltx_tr">
<th id="S5.T3.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">densenet121</th>
<td id="S5.T3.1.8.7.2" class="ltx_td ltx_align_center">42%</td>
<td id="S5.T3.1.8.7.3" class="ltx_td ltx_nopad_r ltx_align_center">83%</td>
</tr>
<tr id="S5.T3.1.9.8" class="ltx_tr">
<th id="S5.T3.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">densenet169</th>
<td id="S5.T3.1.9.8.2" class="ltx_td ltx_align_center">44.6%</td>
<td id="S5.T3.1.9.8.3" class="ltx_td ltx_nopad_r ltx_align_center">88%</td>
</tr>
<tr id="S5.T3.1.10.9" class="ltx_tr">
<th id="S5.T3.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">densenet201</th>
<td id="S5.T3.1.10.9.2" class="ltx_td ltx_align_center ltx_border_bb">45%</td>
<td id="S5.T3.1.10.9.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">85%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Classification benchmark on the five categories”Rooms”</figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Classification Task</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The classification task is crucial in machine learning, requiring models to accurately categorize data into predefined classes. For this benchmark, we leveraged our dataset’s diverse range of interior design styles and categories to evaluate a state-of-the-art classification model’s ability. We curated a balanced subset of the dataset across 35 styles and their respective room categories, with nearly 65k images. The classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> is done for the five main categories: Bedroom, Dining Room, Living Room, Bathroom, and Kitchen. Eight different pretrained models were selected as the baseline architectures due to their proven performance in visual recognition tasks. These models are VGG16, VGG19, ResNet-18, ResNet-34, ResNet-50, DenseNet121, DenseNet169, and DenseNet201. Our training pipeline included standard data augmentation strategies, such as random cropping and horizontal flipping, to improve generalization. Cross-entropy loss and an adaptive learning rate were utilized for training, and performance was measured using the top-1 accuracy metric. We followed two evaluation methods to assess the performance of these models: with fine-tuning the models on the proposed dataset by making the bottom layers trainable, and without fine-tuning. <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">Table-3</span> shows the difference in accuracy for each pretrained model using the two methods.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The results demonstrated the dataset’s potential to achieve high-accuracy predictive models. For example, the DenseNet169 model attained a top-1 accuracy score of 88%, and the ResNet-50 model achieved an accuracy score of 86% across the different styles and categories, highlighting the dataset’s robustness in training models that excel in classification tasks with nuanced visual differences.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.1" class="ltx_tr">
<th id="S5.T4.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S5.T4.1.1.2.1" class="ltx_text ltx_font_bold">Models</span></th>
<th id="S5.T4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.1.1.1.1" class="ltx_text ltx_font_bold">Inception Score <math id="S5.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T4.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T4.1.1.1.1.m1.1.1" xref="S5.T4.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S5.T4.1.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.1.1.3.1" class="ltx_text ltx_font_bold">FID</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.2.1" class="ltx_tr">
<th id="S5.T4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">RAT-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</th>
<td id="S5.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">4.04</td>
<td id="S5.T4.1.2.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">24.02</td>
</tr>
<tr id="S5.T4.1.3.2" class="ltx_tr">
<th id="S5.T4.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DC-GAN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<td id="S5.T4.1.3.2.2" class="ltx_td ltx_align_center">3.38</td>
<td id="S5.T4.1.3.2.3" class="ltx_td ltx_nopad_r ltx_align_center">30</td>
</tr>
<tr id="S5.T4.1.4.3" class="ltx_tr">
<th id="S5.T4.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Projected-GAN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</th>
<td id="S5.T4.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">4.33</td>
<td id="S5.T4.1.4.3.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">22.02</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>GAN MODELS PERFOMANCE</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_bold">Text to image</span>
</h3>

<figure id="S5.F3" class="ltx_figure"><img src="/html/2407.05980/assets/text_to_image.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="519" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Text to Image Using RAT-GAN</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.5.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.6.2" class="ltx_text ltx_font_italic">Generation Benchmark</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">For the image generation benchmark, we employed three distinct GAN models to thoroughly evaluate the capabilities of our interior design multimodal dataset: RATGAN, ProjectedGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, and DCGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. RATGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> was specifically used for text-to-image generation, where the model synthesized interior design images from textual descriptions. This enabled it to translate detailed prompts into realistic images that accurately captured the stylistic nuances of the design styles. ProjectedGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, on the other hand, was employed for image-to-image translation, transforming existing images within the dataset to reflect different styles or room categories. This model demonstrated an impressive ability to adapt and translate source images to new design contexts, creatively reimagining the interior of a given room in another style or category while maintaining stylistic consistency. DCGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, another model used for image-to-image translation, synthesized high-quality images that adhered to the original dataset’s structural characteristics while offering inventive variations across different categories. The visual samples generated by all three models revealed the nuanced diversity of styles and categories within our dataset, confirming that our multimodal dataset is a rich resource for training GAN models capable of producing accurate, diverse, and creative representations of interior design. <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_bold">Table-4</span> shows the inception score and the Frechet Inception Distance for the three models.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2407.05980/assets/image_to_image.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="569" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Image to Image using projected GAN</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion </span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we introduced a novel interior design multimodal dataset that advances research and development in multimodal-related machine learning applications. Our benchmark experiments demonstrated the dataset’s robustness and applicability, achieving high predictive accuracy for classification. Additionally, our exploration of GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> models—RATGAN, ProjectedGAN, and DCGAN—showcased their ability to generate accurate, diverse, and creative outputs, reflecting the original dataset’s visual fidelity and variety. This comprehensive resource facilitates training and evaluating machine learning models in various tasks, including classification, image generation, and retrieval. this dataset will substantially accelerate research and applications in interior design, contributing valuable advancements to the fields of computer vision and natural language processing.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Acknowledgment</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We extend our heartfelt gratitude to <span id="S7.p1.1.1" class="ltx_text ltx_font_italic">AiTech for Artificial Intelligence &amp; Software Development</span> (<a target="_blank" href="https://aitech.net.au" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aitech.net.au</a>) for providing the computational resources essential for our experiments. Their support has been crucial to the successful completion of this research.
.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering, 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Cem Akkus, Luyang Chu, Vladana Djakovic, Steffen Jauch-Walser, Philipp Koch, Giacomo Loss, Christopher Marquardt, Marco Moldovan, Nadja Sauter, Maximilian Schneider, Rickmer Schulte, Karol Urbanczyk, Jann Goschenhofer, Christian Heumann, Rasmus Hvingelby, Daniel Schalk, and Matthias Aßenmacher.

</span>
<span class="ltx_bibblock">Multimodal deep learning, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Emanuel Ben-Baruch, Tal Ridnik, Nadav Zamir, Asaf Noy, Itamar Friedman, Matan Protter, and Lihi Zelnik-Manor.

</span>
<span class="ltx_bibblock">Asymmetric loss for multi-label classification, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Fengxiang Bie, Yibo Yang, Zhongzhu Zhou, Adam Ghanem, Minjia Zhang, Zhewei Yao, Xiaoxia Wu, Connor Holmes, Pareesa Golnari, David A. Clifton, Yuxiong He, Dacheng Tao, and Shuaiwen Leon Song.

</span>
<span class="ltx_bibblock">Renaissance: A survey into ai text-to-image generation in the era of large model, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Min Cao, Shiping Li, Juntao Li, Liqiang Nie, and Min Zhang.

</span>
<span class="ltx_bibblock">Image-text retrieval: A survey on recent research and development, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Hao Chen, Xiaojuan Qi, Lequan Yu, and Pheng-Ann Heng.

</span>
<span class="ltx_bibblock">Dcan: Deep contour-aware networks for accurate gland segmentation, 2016.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">2009 IEEE Conference on Computer Vision and Pattern Recognition</span>, pages 248–255, 2009.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Generative adversarial networks, 2014.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Ali Hamdi, Amr Aboeleneen, and Khaled Shaban.

</span>
<span class="ltx_bibblock">Marl: multimodal attentional representation learning for disease prediction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">International Conference on Computer Vision Systems</span>, pages 14–27. Springer, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Xiangteng He and Yuxin Peng.

</span>
<span class="ltx_bibblock">Fine-grained visual-textual representation learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Circuits and Systems for Video Technology</span>, 30(2):520–531, February 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.

</span>
<span class="ltx_bibblock">Cifar-10 (canadian institute for advanced research).

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency.

</span>
<span class="ltx_bibblock">Foundations and trends in multimodal machine learning: Principles, challenges, and open questions, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context, 2015.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Axel Sauer, Kashyap Chitta, Jens Müller, and Andreas Geiger.

</span>
<span class="ltx_bibblock">Projected gans converge faster, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Siddharth Srivastava and Gaurav Sharma.

</span>
<span class="ltx_bibblock">Omnivec: Learning robust representations with cross modal sharing, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan.

</span>
<span class="ltx_bibblock">Show and tell: A neural image caption generator, 2015.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Xi Xu, Jianqiang Li, Zhichao Zhu, Linna Zhao, Huina Wang, Changwei Song, Yining Chen, Qing Zhao, Jijiang Yang, and Yan Pei.

</span>
<span class="ltx_bibblock">A comprehensive review on synergy of multi-modal data and ai technologies in medical diagnosis.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Bioengineering</span>, 11(3), 2024.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Senmao Ye, Fei Liu, and Minkui Tan.

</span>
<span class="ltx_bibblock">Recurrent affine transformation for text-to-image synthesis, 2022.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="Overleaf Example"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.05979" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.05980" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.05980">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.05980" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.05981" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 13:03:55 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
