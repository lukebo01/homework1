<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.01309] VNLP: Turkish NLP Package</title><meta property="og:description" content="In this work, we present VNLP: the first dedicated, complete, open-source, well-documented, lightweight, production-ready, state-of-the-art Natural Language Processing (NLP) package for the Turkish language.
It contain…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="VNLP: Turkish NLP Package">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="VNLP: Turkish NLP Package">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.01309">

<!--Generated on Fri Apr  5 15:25:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">VNLP: Turkish NLP Package</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">In this work, we present VNLP: the first dedicated, complete, open-source, well-documented, lightweight, production-ready, state-of-the-art Natural Language Processing (NLP) package for the Turkish language.
It contains a wide variety of tools, ranging from the simplest tasks, such as sentence splitting and text normalization,
to the more advanced ones, such as text and token classification models.
Its token classification models are based on "Context Model", a novel architecture that is both an encoder and an auto-regressive model.
NLP tasks solved by VNLP models include but are not limited to Sentiment Analysis, Named Entity Recognition, Morphological Analysis &amp; Disambiguation and Part-of-Speech Tagging.
Moreover, it comes with pre-trained word embeddings and corresponding SentencePiece Unigram tokenizers.
VNLP has an open-source GitHub repository,
ReadtheDocs documentation, PyPi package for convenient installation, Python and command-line API and a demo page to test all the functionality.
Consequently, our main contribution is a complete, compact, easy-to-install and easy-to-use NLP package for Turkish.

<br class="ltx_break">
<br class="ltx_break">
<span id="id2.id1.1" class="ltx_text ltx_font_bold">Keywords: </span>
Turkish NLP, Sentiment Analysis, Named Entity Recognition, Part-of-Speech Tagging, Spelling Correction, Dependency Parsing, Sentence Splitting, Text Normalization.</p>
</div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\NAT@set@cites</span>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text"></span></p>
</div>
<div id="id1" class="ltx_logical-block">
<div id="id1.p1" class="ltx_para">
<p id="id1.p1.1" class="ltx_p ltx_align_center"><span id="id1.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">VNLP: Turkish NLP Package</span></p>
<br class="ltx_break ltx_centering">
<table id="id1.p1.2" class="ltx_tabular ltx_centering ltx_align_top">
<tbody class="ltx_tbody">
<tr id="id1.p1.2.1.1" class="ltx_tr">
<td id="id1.p1.2.1.1.1" class="ltx_td ltx_align_center"><span id="id1.p1.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:120%;">Melikşah Türker, Mehmet Erdi Arı, Aydın Han</span></td>
</tr>
<tr id="id1.p1.2.2.2" class="ltx_tr">
<td id="id1.p1.2.2.2.1" class="ltx_td ltx_align_center">VNGRS</td>
</tr>
<tr id="id1.p1.2.3.3" class="ltx_tr">
<td id="id1.p1.2.3.3.1" class="ltx_td ltx_align_center">YTÜ Teknopark B2 103 Davutpaşa, İstanbul, Turkey</td>
</tr>
<tr id="id1.p1.2.4.4" class="ltx_tr">
<td id="id1.p1.2.4.4.1" class="ltx_td ltx_align_center">{meliksah.turker, erdi.ari, aydin.han}@vngrs.com</td>
</tr>
</tbody>
</table>
<p id="id1.p1.3" class="ltx_p ltx_align_center"><span id="id1.p1.3.1" class="ltx_text ltx_font_italic">Abstract content</span></p>
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.   Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Although frequently considered a low-resource language, Turkish Natural Language Processing (NLP) research has recently attracted more attention <cite class="ltx_cite ltx_citemacro_cite">Safaya et al. (<a href="#bib.bib41" title="" class="ltx_ref">2022</a>); Alecakir et al. (<a href="#bib.bib4" title="" class="ltx_ref">2022</a>); Baykara and
Güngör (<a href="#bib.bib7" title="" class="ltx_ref">2022</a>); Çöltekin et al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>); Uskudarli et al. (<a href="#bib.bib52" title="" class="ltx_ref">2023</a>)</cite>.
Despite this increased attention, there is a gap between research papers and their inference-ready tools.
In most cases, the research paper is specific to one or a few NLP tasks with a GitHub repository that allows reproducibility of evaluation metrics and contains open-source codes.
However, this is nowhere near a complete, state-of-the-art, well-documented, lightweight and inference-ready tool.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Seeing this gap, we present VNLP to be the solution.
VNLP contains a wide range of tools, namely;
Sentence Splitter, Text Normalizer, Named Entity Recognizer, Part-of-Speech Tagger, Dependency Parser, Morphological Analyzer &amp; Disambiguator and Sentiment Analyzer.
Deep Learning models in VNLP are very compact and lightweight, ranging from 2.3M to 5.7M parameters.
Moreover, we release the pre-trained word embeddings and corresponding SentencePiece Unigram tokenizers that are used by these models.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The code base is well-structured, readable and comes with documentation hosted on ReadtheDocs.
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://vnlp.readthedocs.io/en/latest/" title="" class="ltx_ref ltx_href">https://VNLP.readthedocs.io/en/latest/</a></span></span></span>.
The package is available on PyPi
<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://pypi.org/project/vngrs-nlp/" title="" class="ltx_ref ltx_href">https://pypi.org/project/vngrs-nlp/</a></span></span></span>
and can be installed using pip.
<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span id="footnote3.1" class="ltx_text ltx_font_typewriter">pip install vngrs-nlp</span></span></span></span>.
It has Python and CLI APIs that allow integration with other systems.
Lastly, there is a demo page
<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://demo.vnlp.io/" title="" class="ltx_ref ltx_href">https://demo.VNLP.io/</a></span></span></span>
where the mentioned models can be tested.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2403.01309/assets/context_model.png" id="S1.F1.g1" class="ltx_graphics ltx_img_landscape" width="1196" height="564" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Context Model consists of 4 components. Word Model that processes the subword tokens in a word, Left Context Model that processes words in the left context left-to-right, Right Context Model that processes words in the right context right-to-left and Left Context Tag Model that processes the classified tags in the left context.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.   Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">NLP research has attracted a significant amount of research in the past decade.
The development and publication of text-processing technologies have taken a crucial role.
Although it was developed way back in 1997, democratization of Long Short-Term Memory(LSTM) <cite class="ltx_cite ltx_citemacro_cite">Hochreiter and Schmidhuber (<a href="#bib.bib23" title="" class="ltx_ref">1997</a>)</cite> via open-source Deep Learning frameworks <cite class="ltx_cite ltx_citemacro_cite">Chollet et al. (<a href="#bib.bib13" title="" class="ltx_ref">2015</a>); Abadi et al. (<a href="#bib.bib1" title="" class="ltx_ref">2015</a>); Paszke et al. (<a href="#bib.bib37" title="" class="ltx_ref">2019</a>)</cite> has made an important contribution in surge of NLP research by lowering the barrier to entry.
Following the ideas proposed in LSTM, the invention of Gated Recurrent Unit (GRU) allowed the reduction of the number of parameters by about 25%, decreasing the computation cost without significant performance loss.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Word embedding models like Word2Vec and GloVe have laid the foundations for transfer learning and have been improved by their successor FastText.
Using these pre-trained word embeddings and transferring the knowledge obtained during their training to the downstream task’s model improved the performance of NLP models.
However, the mentioned word embedding methods are not context-aware; that is, each word’s embedding vector is static regardless of the words it is surrounded by in the downstream task.
ELMo has come to the rescue and further improved the transfer learning with context-aware word embeddings.
Frameworks like spaCy <cite class="ltx_cite ltx_citemacro_cite">Honnibal and Montani (<a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite>, NLTK <cite class="ltx_cite ltx_citemacro_cite">Bird et al. (<a href="#bib.bib10" title="" class="ltx_ref">2009</a>)</cite> and gensim <cite class="ltx_cite ltx_citemacro_cite">Řehůřek and Sojka (<a href="#bib.bib40" title="" class="ltx_ref">2010</a>)</cite> allowed anyone to use these new technologies by offering methods and pre-trained models that are ready to use in production environment for inference.
Huggingface <cite class="ltx_cite ltx_citemacro_cite">Wolf et al. (<a href="#bib.bib53" title="" class="ltx_ref">2019</a>)</cite> and BERTurk <cite class="ltx_cite ltx_citemacro_cite">Dbmdz (<a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite> filled the gap for higher-level tasks like text and sentence classification and provided state-of-the-art results.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Turkish NLP researchers have utilized these methods to develop models for tasks such as Morphological Disambiguation, Syntactic Parsing, Dependency Parsing, Part-of-Speech Tagging, Named Entity Recognition and Sentiment Analysis for Turkish.
Often, they were developed as individual models that solve a specific problem and are published as separate research papers.
If one is lucky, the paper would contain a link to the GitHub repository that hosts the code to reproduce the results.
Rarely would the repository contain a Docker image or a CLI API to use the model for inference.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">With the attempt to bring separate models and research directions for Turkish under a single banner, three recent works have been published.
Mukayese <cite class="ltx_cite ltx_citemacro_cite">Safaya et al. (<a href="#bib.bib41" title="" class="ltx_ref">2022</a>)</cite> has aimed to be the benchmark for datasets and task evaluation.
TurkishDelight <cite class="ltx_cite ltx_citemacro_cite">Alecakir et al. (<a href="#bib.bib4" title="" class="ltx_ref">2022</a>)</cite> has aimed to bring several models together and serve them in a demo page.
TULAP <cite class="ltx_cite ltx_citemacro_cite">Uskudarli et al. (<a href="#bib.bib52" title="" class="ltx_ref">2023</a>)</cite> has aimed to open source Turkish NLP resources developed at Boğaziçi University, offering a demo page, hosting datasets and Docker images to allow inference.
Although these tools solve the mentioned problems partially, there is still no complete NLP package for Turkish that is open-source, well-documented, PyPi installable and comes with an easy-to-use API.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.   Functionality &amp; Models</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.1.   Sentence Splitter</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Sentence splitting is the task of splitting a bulk text into separate sentences. Although this looks trivial, in order to obtain a well-working sentence splitter, there are exceptions that must be handled correctly, such as numbers and abbreviations.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">For this task, we use the implementation of Koehn and Schroeder <cite class="ltx_cite ltx_citemacro_cite">Koehn (<a href="#bib.bib28" title="" class="ltx_ref">2023</a>)</cite>,
by simplifying the code for Turkish and expanding its lexicon of abbreviations.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.2.   Normalizer</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Normalization is the task of standardizing the text input, which can be in different forms as it may come from various sources such as social media, customer feedback, and news articles.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2">Accuracy</th>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<th id="S3.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Dataset</th>
<th id="S3.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Ambiguous Words</th>
<th id="S3.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">All Words</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.3.1" class="ltx_tr">
<th id="S3.T1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">TrMorph2006 <cite class="ltx_cite ltx_citemacro_cite">Yuret and Ture (<a href="#bib.bib58" title="" class="ltx_ref">2006</a>)</cite>
</th>
<td id="S3.T1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_tt">94.67</td>
<td id="S3.T1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_tt">96.64</td>
</tr>
<tr id="S3.T1.1.4.2" class="ltx_tr">
<th id="S3.T1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">TrMorph2018 <cite class="ltx_cite ltx_citemacro_cite">Dayanik et al. (<a href="#bib.bib16" title="" class="ltx_ref">2018</a>)</cite>
</th>
<td id="S3.T1.1.4.2.2" class="ltx_td ltx_align_center ltx_border_b">93.76</td>
<td id="S3.T1.1.4.2.3" class="ltx_td ltx_align_center ltx_border_b">95.35</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Stemmer: Morphological Analyzer &amp; Disambiguator</figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row">Dataset</th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Accuracy</th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">F1 Macro</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<th id="S3.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">WikiAnn <cite class="ltx_cite ltx_citemacro_cite">Pan et al. (<a href="#bib.bib36" title="" class="ltx_ref">2017</a>)</cite>
</th>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">98.80</td>
<td id="S3.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">98.14</td>
</tr>
<tr id="S3.T2.1.3.2" class="ltx_tr">
<th id="S3.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Gungor <cite class="ltx_cite ltx_citemacro_cite">Güngör et al. (<a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite>
</th>
<td id="S3.T2.1.3.2.2" class="ltx_td ltx_align_center">99.70</td>
<td id="S3.T2.1.3.2.3" class="ltx_td ltx_align_center">98.59</td>
</tr>
<tr id="S3.T2.1.4.3" class="ltx_tr">
<th id="S3.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">TEGHub <cite class="ltx_cite ltx_citemacro_cite">Teghub (<a href="#bib.bib50" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S3.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b">99.74</td>
<td id="S3.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b">98.91</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Named Entity Recognizer</figcaption>
</figure>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">3.2.1.   Spelling Correction</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Spelling correction is the task of detecting and then correcting misspelled and mistyped words.
VNLP uses Jamspell <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib34" title="" class="ltx_ref">Ozinov </a></cite>, a spell-checking library written in C++.
It was chosen over other libraries as it is faster and produces lower error rates.
Jamspell uses adjacent words to correct spelling errors, which is the reason behind the lower error rates compared to alternatives.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">Jamspell requires a dictionary of word frequencies.
We use a custom dictionary file generated by training on a mixed corpus consisting of OPUS-100 <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib59" title="" class="ltx_ref">2020</a>)</cite>, Bilkent Turkish Writings <cite class="ltx_cite ltx_citemacro_cite">Yilmaz (<a href="#bib.bib57" title="" class="ltx_ref">2018</a>)</cite> and TED Talks <cite class="ltx_cite ltx_citemacro_cite">Siarohin et al. (<a href="#bib.bib48" title="" class="ltx_ref">2021</a>)</cite> datasets.
These datasets were chosen over others since they were observed to contain less noise.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">3.2.2.   Deasciifier</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Deasciification is the process of converting a text input written in ASCII-only characters to its correct version in the target language.
VNLP Deasciifer converts the text written in an English keyboard to the correct version of how it would be had it been written using a Turkish keyboard.
It is directly taken from Sevinç’s implementation <cite class="ltx_cite ltx_citemacro_cite">Sevinc (<a href="#bib.bib46" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">3.2.3.   Number to Word</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">Written texts may contain numbers in both numerical and written forms.
In order to standardize these, one could seek to convert numbers to written text forms.
Number to Word function implements this.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">On top of these, the VNLP Normalizer class offers more trivial functions for lowercasing, punctuation removal and accent mark removal.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row">Universal Dependencies 2.9 <cite class="ltx_cite ltx_citemacro_cite">de Marneffe et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>
</th>
<th id="S3.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">LAS</th>
<th id="S3.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">UAS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.1.2.1" class="ltx_tr">
<th id="S3.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">UD_Turkish-Atis</th>
<td id="S3.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">88.52</td>
<td id="S3.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">91.54</td>
</tr>
<tr id="S3.T3.1.3.2" class="ltx_tr">
<th id="S3.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">UD_Turkish-BOUN</th>
<td id="S3.T3.1.3.2.2" class="ltx_td ltx_align_center">67.64</td>
<td id="S3.T3.1.3.2.3" class="ltx_td ltx_align_center">78.15</td>
</tr>
<tr id="S3.T3.1.4.3" class="ltx_tr">
<th id="S3.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">UD_Turkish-FrameNet</th>
<td id="S3.T3.1.4.3.2" class="ltx_td ltx_align_center">81.12</td>
<td id="S3.T3.1.4.3.3" class="ltx_td ltx_align_center">92.30</td>
</tr>
<tr id="S3.T3.1.5.4" class="ltx_tr">
<th id="S3.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">UD_Turkish-GB</th>
<td id="S3.T3.1.5.4.2" class="ltx_td ltx_align_center">72.97</td>
<td id="S3.T3.1.5.4.3" class="ltx_td ltx_align_center">88.58</td>
</tr>
<tr id="S3.T3.1.6.5" class="ltx_tr">
<th id="S3.T3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">UD_Turkish-IMST</th>
<td id="S3.T3.1.6.5.2" class="ltx_td ltx_align_center">63.32</td>
<td id="S3.T3.1.6.5.3" class="ltx_td ltx_align_center">76.53</td>
</tr>
<tr id="S3.T3.1.7.6" class="ltx_tr">
<th id="S3.T3.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">UD_Turkish-Kenet</th>
<td id="S3.T3.1.7.6.2" class="ltx_td ltx_align_center">68.80</td>
<td id="S3.T3.1.7.6.3" class="ltx_td ltx_align_center">83.51</td>
</tr>
<tr id="S3.T3.1.8.7" class="ltx_tr">
<th id="S3.T3.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">UD_Turkish-Penn</th>
<td id="S3.T3.1.8.7.2" class="ltx_td ltx_align_center">70.72</td>
<td id="S3.T3.1.8.7.3" class="ltx_td ltx_align_center">85.24</td>
</tr>
<tr id="S3.T3.1.9.8" class="ltx_tr">
<th id="S3.T3.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">UD_Turkish-PUD</th>
<td id="S3.T3.1.9.8.2" class="ltx_td ltx_align_center">61.31</td>
<td id="S3.T3.1.9.8.3" class="ltx_td ltx_align_center">74.77</td>
</tr>
<tr id="S3.T3.1.10.9" class="ltx_tr">
<th id="S3.T3.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">UD_Turkish-Tourism</th>
<td id="S3.T3.1.10.9.2" class="ltx_td ltx_align_center ltx_border_b">90.96</td>
<td id="S3.T3.1.10.9.3" class="ltx_td ltx_align_center ltx_border_b">97.31</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Dependency Parser</figcaption>
</figure>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.3.   Stopword Remover</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Stopwords are the words that take place in virtually any text and provide no context information alone, such as "and", "such", or "if".
While working on a wide variety of NLP tasks, one can seek to get rid of them before further analysis.
VNLP offers two algorithms to get rid of Turkish stopwords.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">3.3.1.   Static Method</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">The static method is the conventional method that contains a pre-defined static stopword lexicon and removes these words from the text input.
VNLP uses an improved version of the stopword lexicon offered in the Zemberek package <cite class="ltx_cite ltx_citemacro_cite">Akin (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">3.3.2.   Dynamic Method</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">The dynamic method is the more advanced version, where stopwords are determined depending on the given corpus.
The implemented method determines the stopwords by looking at the word frequencies<cite class="ltx_cite ltx_citemacro_cite">Saif et al. (<a href="#bib.bib43" title="" class="ltx_ref">2014</a>)</cite>
and their breaking point <cite class="ltx_cite ltx_citemacro_cite">Satopaa et al. (<a href="#bib.bib45" title="" class="ltx_ref">2011</a>)</cite>
to set a threshold of frequencies and consider the words above the threshold as stopwords.
Moreover, this method is language agnostic and can be used for all kinds of texts to obtain the most frequent words.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.4.   SentencePiece Unigram Tokenizer</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Deep Learning models in VNLP use subword tokens, tokenized by SentencePiece Unigram Model <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a href="#bib.bib31" title="" class="ltx_ref">2018</a>)</cite> to process the text.
SentencePiece Unigram Tokenizer is trained from scratch on a corpus of 10 GB Turkish text, which consists of random samples from OSCAR <cite class="ltx_cite ltx_citemacro_cite">Abadji et al. (<a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite>, OPUS <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib59" title="" class="ltx_ref">2020</a>)</cite> and Wikipedia dump datasets.
It comes in 2 sizes, with vocabulary sizes of 16,000 and 32,000.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.5.   Pre-trained Word Embeddings</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">VNLP offers pre-trained word embeddings for two types of tokens.</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">TreebankWord</span> embeddings are tokenized by NLTK’s <cite class="ltx_cite ltx_citemacro_cite">Bird et al. (<a href="#bib.bib10" title="" class="ltx_ref">2009</a>)</cite> TreebankWord Tokenizer.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">SentencePiece Unigram</span> embeddings are tokenized by SentencePiece Unigram Tokenizer.</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">Embeddings for these tokens are trained using Word2Vec <cite class="ltx_cite ltx_citemacro_cite">Mikolov et al. (<a href="#bib.bib33" title="" class="ltx_ref">2013</a>)</cite> and FastText <cite class="ltx_cite ltx_citemacro_cite">Bojanowski et al. (<a href="#bib.bib11" title="" class="ltx_ref">2017</a>)</cite> algorithms implemented by gensim <cite class="ltx_cite ltx_citemacro_cite">Řehůřek and Sojka (<a href="#bib.bib40" title="" class="ltx_ref">2010</a>)</cite> framework.</p>
</div>
<section id="S3.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">3.5.1.   Word2Vec</h4>

<div id="S3.SS5.SSS1.p1" class="ltx_para">
<p id="S3.SS5.SSS1.p1.1" class="ltx_p">Word2Vec embeddings are trained for both of the tokenization methods mentioned above.
TreebankWord tokenized Word2Vec embeddings come in 3 sizes.</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Large</span>: vocabulary size: 128,000, embedding dimension: 256</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Medium</span>: vocabulary size: 64,000, embedding dimension: 128</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p"><span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Small</span>: vocabulary size: 32,000, embedding dimension: 64</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS5.SSS1.p2" class="ltx_para">
<p id="S3.SS5.SSS1.p2.1" class="ltx_p">SentencePiece Unigram tokenized Word2Vec embeddings come in 2 sizes:</p>
<ul id="S3.I3" class="ltx_itemize">
<li id="S3.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I3.i1.p1" class="ltx_para">
<p id="S3.I3.i1.p1.1" class="ltx_p"><span id="S3.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">Large</span>: vocabulary size: 32,000, embedding dimension: 256</p>
</div>
</li>
<li id="S3.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I3.i2.p1" class="ltx_para">
<p id="S3.I3.i2.p1.1" class="ltx_p"><span id="S3.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">Small</span>: vocabulary size: 16,000, embedding dimension: 128</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS5.SSS1.p3" class="ltx_para">
<p id="S3.SS5.SSS1.p3.1" class="ltx_p">The difference in vocabulary sizes of the two tokenization methods is due to the fact that the Unigram tokenizer is never out of vocabulary and 32,000 is a reasonable size, being often used for state-of-the-art monolingual models <cite class="ltx_cite ltx_citemacro_cite">Dbmdz (<a href="#bib.bib17" title="" class="ltx_ref">2023</a>); Raffel et al. (<a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">3.5.2.   FastText</h4>

<div id="S3.SS5.SSS2.p1" class="ltx_para">
<p id="S3.SS5.SSS2.p1.1" class="ltx_p">FastText embeddings are trained for TreebankWord tokenized tokens only.
Similar to Word2Vec configuration, they come in 3 sizes.</p>
<ul id="S3.I4" class="ltx_itemize">
<li id="S3.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I4.i1.p1" class="ltx_para">
<p id="S3.I4.i1.p1.1" class="ltx_p"><span id="S3.I4.i1.p1.1.1" class="ltx_text ltx_font_bold">Large</span>: vocabulary size: 128,000, embedding dimension: 256</p>
</div>
</li>
<li id="S3.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I4.i2.p1" class="ltx_para">
<p id="S3.I4.i2.p1.1" class="ltx_p"><span id="S3.I4.i2.p1.1.1" class="ltx_text ltx_font_bold">Medium</span>: vocabulary size: 64,000, embedding dimension: 128</p>
</div>
</li>
<li id="S3.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I4.i3.p1" class="ltx_para">
<p id="S3.I4.i3.p1.1" class="ltx_p"><span id="S3.I4.i3.p1.1.1" class="ltx_text ltx_font_bold">Small</span>: vocabulary size: 32,000, embedding dimension: 64</p>
</div>
</li>
</ul>
</div>
<figure id="S3.T4" class="ltx_table">
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.1.1.1" class="ltx_tr">
<th id="S3.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row">Universal Dependencies 2.9 <cite class="ltx_cite ltx_citemacro_cite">de Marneffe et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>
</th>
<th id="S3.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Accuracy</th>
<th id="S3.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">F1 Macro</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.1.2.1" class="ltx_tr">
<th id="S3.T4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">UD_Turkish-Atis</th>
<td id="S3.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">98.74</td>
<td id="S3.T4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">98.80</td>
</tr>
<tr id="S3.T4.1.3.2" class="ltx_tr">
<th id="S3.T4.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">UD_Turkish-BOUN</th>
<td id="S3.T4.1.3.2.2" class="ltx_td ltx_align_center">87.08</td>
<td id="S3.T4.1.3.2.3" class="ltx_td ltx_align_center">78.84</td>
</tr>
<tr id="S3.T4.1.4.3" class="ltx_tr">
<th id="S3.T4.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">UD_Turkish-FrameNet</th>
<td id="S3.T4.1.4.3.2" class="ltx_td ltx_align_center">95.09</td>
<td id="S3.T4.1.4.3.3" class="ltx_td ltx_align_center">90.39</td>
</tr>
<tr id="S3.T4.1.5.4" class="ltx_tr">
<th id="S3.T4.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">UD_Turkish-GB</th>
<td id="S3.T4.1.5.4.2" class="ltx_td ltx_align_center">85.59</td>
<td id="S3.T4.1.5.4.3" class="ltx_td ltx_align_center">66.20</td>
</tr>
<tr id="S3.T4.1.6.5" class="ltx_tr">
<th id="S3.T4.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">UD_Turkish-IMST</th>
<td id="S3.T4.1.6.5.2" class="ltx_td ltx_align_center">90.69</td>
<td id="S3.T4.1.6.5.3" class="ltx_td ltx_align_center">78.45</td>
</tr>
<tr id="S3.T4.1.7.6" class="ltx_tr">
<th id="S3.T4.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">UD_Turkish-Kenet</th>
<td id="S3.T4.1.7.6.2" class="ltx_td ltx_align_center">91.94</td>
<td id="S3.T4.1.7.6.3" class="ltx_td ltx_align_center">87.66</td>
</tr>
<tr id="S3.T4.1.8.7" class="ltx_tr">
<th id="S3.T4.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">UD_Turkish-Penn</th>
<td id="S3.T4.1.8.7.2" class="ltx_td ltx_align_center">94.52</td>
<td id="S3.T4.1.8.7.3" class="ltx_td ltx_align_center">93.29</td>
</tr>
<tr id="S3.T4.1.9.8" class="ltx_tr">
<th id="S3.T4.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">UD_Turkish-PUD</th>
<td id="S3.T4.1.9.8.2" class="ltx_td ltx_align_center">83.87</td>
<td id="S3.T4.1.9.8.3" class="ltx_td ltx_align_center">65.59</td>
</tr>
<tr id="S3.T4.1.10.9" class="ltx_tr">
<th id="S3.T4.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">UD_Turkish-Tourism</th>
<td id="S3.T4.1.10.9.2" class="ltx_td ltx_align_center ltx_border_b">98.45</td>
<td id="S3.T4.1.10.9.3" class="ltx_td ltx_align_center ltx_border_b">93.25</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Part-of-Speech Tagger</figcaption>
</figure>
<figure id="S3.T5" class="ltx_table">
<table id="S3.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T5.1.1.1" class="ltx_tr">
<th id="S3.T5.1.1.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Accuracy</th>
<th id="S3.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">F1 Macro</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T5.1.2.1" class="ltx_tr">
<th id="S3.T5.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_tt">Mixture of Datasets</th>
<td id="S3.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_tt">94.69</td>
<td id="S3.T5.1.2.1.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_tt">93.81</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Sentiment Analyzer</figcaption>
</figure>
</section>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.6.   Context Model</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">Context Model is the base architecture used in several deep learning models in VNLP.
It is inspired by Ref. <cite class="ltx_cite ltx_citemacro_cite">Shen et al. (<a href="#bib.bib47" title="" class="ltx_ref">2016</a>)</cite>
and consists of 4 input components as shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ VNLP: Turkish NLP Package" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
They are Left Context Word Model, Left Context Tag Model, Current Word Model and Right Context Word Model, respectively.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">The model input is tokenized by SentencePiece Unigram Tokenizer, and corresponding pre-trained Word2Vec embeddings are fed to the network.
It processes each word one by one, and each word can be represented by multiple subword tokens.
For this reason, the model contains a Word RNN Model that processes all subword tokens in a word and returns a single word embedding in the last time step.
Word RNN Model is used by Current Word, Left and Right Context Models and its parameters are shared among them.
Left and Right Context Word Models process the words on left from left to right and the words on right from right to left, respectively.
Left Context Tag Model processes the classification results of prior words.
In the end, 4 components produce 4 embedding vectors, which are concatenated and processed by 2 fully connected layers followed by a classification head that produces classification logits.</p>
</div>
<div id="S3.SS6.p3" class="ltx_para">
<p id="S3.SS6.p3.1" class="ltx_p">The network is made of GRU <cite class="ltx_cite ltx_citemacro_cite">Cho et al. (<a href="#bib.bib12" title="" class="ltx_ref">2014</a>)</cite> cells, which provide a computation advantage over conventional LSTM <cite class="ltx_cite ltx_citemacro_cite">Hochreiter and Schmidhuber (<a href="#bib.bib23" title="" class="ltx_ref">1997</a>)</cite> cells.
Throughout this work, all RNNs are made of GRU cells.</p>
</div>
<div id="S3.SS6.p4" class="ltx_para">
<p id="S3.SS6.p4.1" class="ltx_p">The main advantage of the Context Model is it combines the idea of auto-regressive sequence-to-sequence models with token classifier encoder-only models.
This is actuated by taking the classification results of prior words on the left context as input while classifying words instead of subwords.
This schema has two benefits over BERT-based <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib19" title="" class="ltx_ref">2018</a>)</cite> encoder-only models.
First, its auto-regressive structure allows for taking the classification results of earlier words into account.
Second, classifying words instead of tokens/subwords guarantees the alignment of words and tags.</p>
</div>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.7.   Stemmer: Morphological Analyzer &amp; Disambiguator</h3>

<div id="S3.SS7.p1" class="ltx_para">
<p id="S3.SS7.p1.1" class="ltx_p">Stemming is the task of obtaining the stems of the words in a sentence, depending on the context.
It is useful to standardize the text input, especially in agglutinative languages such as Turkish.</p>
</div>
<div id="S3.SS7.p2" class="ltx_para">
<p id="S3.SS7.p2.1" class="ltx_p">A morphological analyzer allows obtaining the stem and the morphological tags of a given word.
However, it returns multiple candidates since the word may have multiple meanings depending on the context of the sentence.
See the two examples below:</p>
</div>
<div id="S3.SS7.p3" class="ltx_para">
<ul id="S3.I5" class="ltx_itemize">
<li id="S3.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I5.i1.p1" class="ltx_para">
<p id="S3.I5.i1.p1.1" class="ltx_p">Üniversite sınavlarına canla başla çalışıyorlardı. (They were studying really hard for the university entrance exams.)</p>
</div>
</li>
<li id="S3.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I5.i2.p1" class="ltx_para">
<p id="S3.I5.i2.p1.1" class="ltx_p">Şimdi baştan başla. (Now, start over.)</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS7.p4" class="ltx_para">
<p id="S3.SS7.p4.1" class="ltx_p">In the first sentence, the word "başla" is a noun, meaning "hard", describing the struggle of studying while
in the second sentence, the word "başla" is the verb, meaning "start".</p>
</div>
<div id="S3.SS7.p5" class="ltx_para">
<p id="S3.SS7.p5.1" class="ltx_p">Hence, a morphological analyzer is context-agnostic and simply provides all of the potential analyses or candidates.
Given the context and the potential analyses, a morphological disambiguator selects the correct analysis result.
Stemmer class implements Shen’s <cite class="ltx_cite ltx_citemacro_cite">Shen et al. (<a href="#bib.bib47" title="" class="ltx_ref">2016</a>)</cite>
morphological disambiguation model with slight differences that result from a different model config and using GRU instead of LSTM.
Stemmer consists of 2.3M parameters.
It uses Yildiz’s work <cite class="ltx_cite ltx_citemacro_cite">Yildiz (<a href="#bib.bib54" title="" class="ltx_ref">2023</a>)</cite> as the morphological analyzer.</p>
</div>
<div id="S3.SS7.p6" class="ltx_para">
<p id="S3.SS7.p6.1" class="ltx_p">Then, having a morphological disambiguator on top of a morphological analyzer allows finding the correct stem of a word depending on the context.
Stemmer model utilizes the Turkish pre-trained Word2Vec embeddings described in Section. <a href="#S3.SS5.SSS1" title="3.5.1. Word2Vec ‣ 3.5. Pre-trained Word Embeddings ‣ 3. Functionality &amp; Models ‣ VNLP: Turkish NLP Package" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5.1</span></a> and is trained on TrMorph2006 <cite class="ltx_cite ltx_citemacro_cite">Yuret and Ture (<a href="#bib.bib58" title="" class="ltx_ref">2006</a>)</cite>, TrMorph2016 <cite class="ltx_cite ltx_citemacro_cite">Yildiz et al. (<a href="#bib.bib55" title="" class="ltx_ref">2016</a>)</cite> and TrMorph2018 <cite class="ltx_cite ltx_citemacro_cite">Dayanik et al. (<a href="#bib.bib16" title="" class="ltx_ref">2018</a>)</cite> datasets.</p>
</div>
</section>
<section id="S3.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.8.   Named Entity Recognizer</h3>

<div id="S3.SS8.p1" class="ltx_para">
<p id="S3.SS8.p1.1" class="ltx_p">Named Entity Recognition (NER) is the task of finding the named entities in a sentence.
Although there are several variants of entities and how they are represented, VNLP’s Named Entity Recognizer allows finding Person, Location and Organization entities in the given sentence using IO format.
It is based on the Context Model architecture <a href="#S3.SS6" title="3.6. Context Model ‣ 3. Functionality &amp; Models ‣ VNLP: Turkish NLP Package" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.6</span></a>, consists of 5.6M parameters and is trained on a collection of open-source NER datasets <cite class="ltx_cite ltx_citemacro_cite">Güngör et al. (<a href="#bib.bib21" title="" class="ltx_ref">2018</a>); Tür et al. (<a href="#bib.bib51" title="" class="ltx_ref">2003</a>); Küçük et al. (<a href="#bib.bib30" title="" class="ltx_ref">2016</a>); Teghub (<a href="#bib.bib50" title="" class="ltx_ref">2023</a>); Küçük et al. (<a href="#bib.bib29" title="" class="ltx_ref">2014</a>); Pan et al. (<a href="#bib.bib36" title="" class="ltx_ref">2017</a>); Hu et al. (<a href="#bib.bib25" title="" class="ltx_ref">2020</a>)</cite>.
TWNERTC <cite class="ltx_cite ltx_citemacro_cite">Sahin et al. (<a href="#bib.bib42" title="" class="ltx_ref">2017</a>)</cite> is also considered; however, it is excluded due to being too noisy and actually deteriorating the model performance.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2403.01309/assets/dependency_parsing.png" id="S3.F2.g1" class="ltx_graphics ltx_img_landscape" width="1196" height="427" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Dependency Parser produces arcs and labels to indicate the relations between words. Part-of-speech tags below are produced by Part-of-Speech Tagger.</figcaption>
</figure>
</section>
<section id="S3.SS9" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.9.   Dependency Parser</h3>

<div id="S3.SS9.p1" class="ltx_para">
<p id="S3.SS9.p1.1" class="ltx_p">Dependency Parsing is the task of showing the dependencies of words in a sentence, along with the dependency labels.
An example can be seen in Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.8. Named Entity Recognizer ‣ 3. Functionality &amp; Models ‣ VNLP: Turkish NLP Package" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
VNLP Dependency Parser is based on the Context Model architecture with a slight difference in classification head and left context tag inputs.
The difference arises from the fact that the model makes two classification decisions for each word, that is, arc (the index of the word it depends on) and the dependency tag.
This is implemented by a single vector in the classification head where the first part of the vector represents the arc and the second part represents the dependency tag.
Consequently, Binary Cross Entropy is used to train the model, as it is a multi-label classifier.
It consists of 5.7M parameters.
The model is trained on Universal Dependencies 2.9 <cite class="ltx_cite ltx_citemacro_cite">de Marneffe et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite> dataset.</p>
</div>
</section>
<section id="S3.SS10" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.10.   Part-of-Speech Tagger</h3>

<div id="S3.SS10.p1" class="ltx_para">
<p id="S3.SS10.p1.1" class="ltx_p">Part-of-speech tagging is the task of assigning part-of-speech tags to the words in a sentence, such as nouns, pronouns, verbs, adjectives, punctuation and so on.
VNLP Part-of-Speech Tagger is also based on the Context Model architecture and consists of 2.6M parameters.</p>
</div>
<div id="S3.SS10.p2" class="ltx_para">
<p id="S3.SS10.p2.1" class="ltx_p">Part-of-Speech Tagger is trained on Universal Dependencies 2.9 dataset as well.</p>
</div>
</section>
<section id="S3.SS11" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.11.   Sentiment Analyzer</h3>

<div id="S3.SS11.p1" class="ltx_para">
<p id="S3.SS11.p1.1" class="ltx_p">Sentiment Analysis is the task of classifying a text into positive or negative.
Some of the typical use cases are understanding the sentiment of social media text and measuring customer satisfaction through comments and feedback.
VNLP Sentiment Analyzer implements a text classifier for this purpose.
Similar to other models in VNLP, Sentiment Analyzer uses SentencePiece Unigram Tokenized pre-trained Word2Vec embeddings and is based on GRU.
However, as it is a text classifier, compared to the word tagger models so far, its architecture is different.
It uses a stack of Bidirectional RNNs to process the input tokens, followed by a GlobalAveragePooling1D and fully connected layer before the final classification head.</p>
</div>
<div id="S3.SS11.p2" class="ltx_para">
<p id="S3.SS11.p2.1" class="ltx_p">Sentiment Analyzer consists of 2.8M parameters and is trained on a large mix of social media, customer comments and research datasets <cite class="ltx_cite ltx_citemacro_cite">Basturk (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>); Bilen (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>); BİLEN and HORASAN (<a href="#bib.bib9" title="" class="ltx_ref">2021</a>); Köksal and Özgür (<a href="#bib.bib32" title="" class="ltx_ref">2021</a>); Coskuner (<a href="#bib.bib15" title="" class="ltx_ref">2023</a>); Gokmen (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>); Guven (<a href="#bib.bib22" title="" class="ltx_ref">2023</a>); Ozler (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>); Sarigil (<a href="#bib.bib44" title="" class="ltx_ref">2023</a>); Subasi (<a href="#bib.bib49" title="" class="ltx_ref">2023</a>); Kahyaoglu (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>); Yilmaz (<a href="#bib.bib56" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.   Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Models are trained using Adam <cite class="ltx_cite ltx_citemacro_cite">Kingma and Ba (<a href="#bib.bib27" title="" class="ltx_ref">2014</a>)</cite> optimizer (<math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\epsilon=1e-3" display="inline"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">ϵ</mi><mo id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml"><mrow id="S4.p1.1.m1.1.1.3.2" xref="S4.p1.1.m1.1.1.3.2.cmml"><mn id="S4.p1.1.m1.1.1.3.2.2" xref="S4.p1.1.m1.1.1.3.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.p1.1.m1.1.1.3.2.1" xref="S4.p1.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S4.p1.1.m1.1.1.3.2.3" xref="S4.p1.1.m1.1.1.3.2.3.cmml">e</mi></mrow><mo id="S4.p1.1.m1.1.1.3.1" xref="S4.p1.1.m1.1.1.3.1.cmml">−</mo><mn id="S4.p1.1.m1.1.1.3.3" xref="S4.p1.1.m1.1.1.3.3.cmml">3</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><eq id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1"></eq><ci id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">italic-ϵ</ci><apply id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3"><minus id="S4.p1.1.m1.1.1.3.1.cmml" xref="S4.p1.1.m1.1.1.3.1"></minus><apply id="S4.p1.1.m1.1.1.3.2.cmml" xref="S4.p1.1.m1.1.1.3.2"><times id="S4.p1.1.m1.1.1.3.2.1.cmml" xref="S4.p1.1.m1.1.1.3.2.1"></times><cn type="integer" id="S4.p1.1.m1.1.1.3.2.2.cmml" xref="S4.p1.1.m1.1.1.3.2.2">1</cn><ci id="S4.p1.1.m1.1.1.3.2.3.cmml" xref="S4.p1.1.m1.1.1.3.2.3">𝑒</ci></apply><cn type="integer" id="S4.p1.1.m1.1.1.3.3.cmml" xref="S4.p1.1.m1.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\epsilon=1e-3</annotation></semantics></math>) along with a linear learning rate decay of 0.95 per epoch.
The number of epochs varies from task to task and is determined according to validation loss.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Stemmer</span>: is evaluated on the test splits of TrMorph2006 <cite class="ltx_cite ltx_citemacro_cite">Yuret and Ture (<a href="#bib.bib58" title="" class="ltx_ref">2006</a>)</cite> and TrMorph2018 <cite class="ltx_cite ltx_citemacro_cite">Dayanik et al. (<a href="#bib.bib16" title="" class="ltx_ref">2018</a>)</cite> datasets.
A word is ambiguous if the morphological analyzer returns several candidates as the result.
Following the original work <cite class="ltx_cite ltx_citemacro_cite">Shen et al. (<a href="#bib.bib47" title="" class="ltx_ref">2016</a>)</cite>, Accuracy and F1 Macro scores are reported for both ambiguous words and all words in Table. <a href="#S3.T1" title="Table 1 ‣ 3.2. Normalizer ‣ 3. Functionality &amp; Models ‣ VNLP: Turkish NLP Package" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Named Entity Recognizer</span>: is evaluated on the test splits of WikiAnn <cite class="ltx_cite ltx_citemacro_cite">Pan et al. (<a href="#bib.bib36" title="" class="ltx_ref">2017</a>)</cite>, Gungor <cite class="ltx_cite ltx_citemacro_cite">Güngör et al. (<a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite> and TurkishNER-BERT <cite class="ltx_cite ltx_citemacro_cite">Teghub (<a href="#bib.bib50" title="" class="ltx_ref">2023</a>)</cite> datasets.
Accuracy and F1 Macro scores are reported in Table. <a href="#S3.T2" title="Table 2 ‣ 3.2. Normalizer ‣ 3. Functionality &amp; Models ‣ VNLP: Turkish NLP Package" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Dependency Parser</span>: is evaluated on the test splits of Universal Dependencies 2.9 <cite class="ltx_cite ltx_citemacro_cite">de Marneffe et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite> dataset.
Labelled Attachment Score (LAS) and Unlabelled Attachment Score (UAS) are reported in Table. <a href="#S3.T3" title="Table 3 ‣ 3.2.3. Number to Word ‣ 3.2. Normalizer ‣ 3. Functionality &amp; Models ‣ VNLP: Turkish NLP Package" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Part-of-Speech Tagger</span>: is evaluated on the test splits of Universal Dependencies 2.9 <cite class="ltx_cite ltx_citemacro_cite">de Marneffe et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite> dataset.
Accuracy and F1 Macro scores are reported in Table. <a href="#S3.T4" title="Table 4 ‣ 3.5.2. FastText ‣ 3.5. Pre-trained Word Embeddings ‣ 3. Functionality &amp; Models ‣ VNLP: Turkish NLP Package" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_bold">Sentiment Analyzer</span>: is evaluated on the test split of the combined dataset mentioned in <a href="#S3.SS11" title="3.11. Sentiment Analyzer ‣ 3. Functionality &amp; Models ‣ VNLP: Turkish NLP Package" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.11</span></a>.
It is generated by scikit-learn’s <cite class="ltx_cite ltx_citemacro_cite">Pedregosa et al. (<a href="#bib.bib38" title="" class="ltx_ref">2011</a>)</cite> train test split function with the following config: test_size = 0.10, random_state = 0, shuffle = True, stratify by label.
The reason we created the test split from scratch is due to the fact that the compiled dataset does not contain any pre-defined test split except for Ref. <cite class="ltx_cite ltx_citemacro_cite">Köksal and Özgür (<a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite>.
Instead of evaluating on this small sample only, we preferred evaluating on a larger and more diverse test set that comes from various sources.
Accuracy and F1 Macro scores are reported in Table. <a href="#S3.T5" title="Table 5 ‣ 3.5.2. FastText ‣ 3.5. Pre-trained Word Embeddings ‣ 3. Functionality &amp; Models ‣ VNLP: Turkish NLP Package" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p"><span id="S4.p7.1.1" class="ltx_text ltx_font_bold">Spelling Corrector</span>: is evaluated on 100 random samples taken from My Dear Watson <cite class="ltx_cite ltx_citemacro_cite">Amy Jang (<a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite> and TED Talks <cite class="ltx_cite ltx_citemacro_cite">Siarohin et al. (<a href="#bib.bib48" title="" class="ltx_ref">2021</a>)</cite> datasets.
Although it scores an Accuracy of 0.69 and a Word Error Rate (WER) of 0.09 on this sample, we report these numbers for informative purposes only, as it is an under-development module of the package.
A more comprehensive study for Spelling will be conducted later on.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.   Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We presented VNLP in this work. It is the first complete, open-source, production-ready, well-documented, PyPi installable NLP library for Turkish.
It contains a wide range of tools, including both low and high-level NLP tasks.
Implemented deep learning models are compact yet competitive.
The Context Model presented in this work brings two advantages over BERT-based classification models by taking the prediction results of earlier words into account and guaranteeing the word-tag alignments.
Hence, our contribution is a well-engineered, documented, easy-to-use NLP package based on its novel Context Model architecture.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.   Bibliographical References</h2>

<div id="S6.p1" class="ltx_para">
<span id="S6.p1.1" class="ltx_ERROR undefined">\c@NAT@ctr</span>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography"></h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadi et al. (2015)</span>
<span class="ltx_bibblock">
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris
Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal
Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas,
Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and
Xiaoqiang Zheng. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.tensorflow.org/" title="" class="ltx_ref ltx_href">TensorFlow: Large-scale
machine learning on heterogeneous systems</a>.

</span>
<span class="ltx_bibblock">Software available from tensorflow.org.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadji et al. (2022)</span>
<span class="ltx_bibblock">
Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Benoît
Sagot. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2201.06642" title="" class="ltx_ref ltx_href">Towards a Cleaner
Document-Oriented Multilingual Crawled Corpus</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, page arXiv:2201.06642.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akin (2023)</span>
<span class="ltx_bibblock">
Akin. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/ahmetaa/zemberek-nlp" title="" class="ltx_ref ltx_href">Zemberek-nlp</a>.

</span>
<span class="ltx_bibblock">Online; accessed 07-Aug-2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alecakir et al. (2022)</span>
<span class="ltx_bibblock">
Huseyin Alecakir, Necva Bölücü, and Burcu Can. 2022.

</span>
<span class="ltx_bibblock">Turkishdelightnlp: A neural turkish nlp toolkit.

</span>
<span class="ltx_bibblock">ACL.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amy Jang (2020)</span>
<span class="ltx_bibblock">
Phil Culliton Amy Jang, Ana Sofia Uzsoy. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://kaggle.com/competitions/contradictory-my-dear-watson" title="" class="ltx_ref ltx_href">Contradictory, my dear watson</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Basturk (2023)</span>
<span class="ltx_bibblock">
Basturk. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.kaggle.com/code/egebasturk1/yemeksepeti-sentiment-analysis/input" title="" class="ltx_ref ltx_href">Yemeksepeti sentiment analysis</a>.

</span>
<span class="ltx_bibblock">Online; accessed 07-Aug-2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baykara and
Güngör (2022)</span>
<span class="ltx_bibblock">
Batuhan Baykara and Tunga Güngör. 2022.

</span>
<span class="ltx_bibblock">Turkish abstractive text summarization using pretrained
sequence-to-sequence models.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Natural Language Engineering</em>, pages 1–30.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bilen (2023)</span>
<span class="ltx_bibblock">
Bilen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.kaggle.com/datasets/burhanbilenn/duygu-analizi-icin-urun-yorumlari" title="" class="ltx_ref ltx_href">Duygu analizi veri seti</a>.

</span>
<span class="ltx_bibblock">Online; accessed 07-Aug-2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BİLEN and HORASAN (2021)</span>
<span class="ltx_bibblock">
Burhan BİLEN and Fahrettin HORASAN. 2021.

</span>
<span class="ltx_bibblock">Lstm network based sentiment analysis for customer reviews.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Politeknik Dergisi</em>, 25(3):959–966.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bird et al. (2009)</span>
<span class="ltx_bibblock">
Steven Bird, Ewan Klein, and Edward Loper. 2009.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Natural language processing with Python: analyzing text with
the natural language toolkit</em>.

</span>
<span class="ltx_bibblock">" O’Reilly Media, Inc.".

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bojanowski et al. (2017)</span>
<span class="ltx_bibblock">
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017.

</span>
<span class="ltx_bibblock">Enriching word vectors with subword information.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Transactions of the association for computational linguistics</em>,
5:135–146.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al. (2014)</span>
<span class="ltx_bibblock">
Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014.

</span>
<span class="ltx_bibblock">Learning phrase representations using rnn encoder-decoder for
statistical machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1406.1078</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chollet et al. (2015)</span>
<span class="ltx_bibblock">
François Chollet et al. 2015.

</span>
<span class="ltx_bibblock">Keras.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://keras.io" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://keras.io</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Çöltekin et al. (2023)</span>
<span class="ltx_bibblock">
Çağrı Çöltekin, A Seza Doğruöz, and
Özlem Çetinoğlu. 2023.

</span>
<span class="ltx_bibblock">Resources for turkish natural language processing: A critical survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Language Resources and Evaluation</em>, 57(1):449–488.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coskuner (2023)</span>
<span class="ltx_bibblock">
Coskuner. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.kaggle.com/datasets/berkaycokuner/yemek-sepeti-comments" title="" class="ltx_ref ltx_href">Yemek
sepeti comments</a>.

</span>
<span class="ltx_bibblock">Online; accessed 07-Aug-2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dayanik et al. (2018)</span>
<span class="ltx_bibblock">
Erenay Dayanik, Ekin Akyürek, and Deniz Yuret. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1805.07946" title="" class="ltx_ref ltx_href">Morphnet: A
sequence-to-sequence model that combines morphological analysis and
disambiguation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1805.07946.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dbmdz (2023)</span>
<span class="ltx_bibblock">
Dbmdz. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://huggingface.co/dbmdz/bert-base-turkish-uncased" title="" class="ltx_ref ltx_href">dbmdz/bert-base-turkish-uncased</a>.

</span>
<span class="ltx_bibblock">Online; accessed 07-Aug-2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">de Marneffe et al. (2021)</span>
<span class="ltx_bibblock">
Marie-Catherine de Marneffe, Christopher D. Manning, Joakim Nivre, and Daniel
Zeman. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/coli_a_00402" title="" class="ltx_ref ltx_href">Universal
Dependencies</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Computational Linguistics</em>, 47(2):255–308.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gokmen (2023)</span>
<span class="ltx_bibblock">
Gokmen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.kaggle.com/datasets/busra88/turkish-reviews-dataset" title="" class="ltx_ref ltx_href">Turkish
reviews dataset</a>.

</span>
<span class="ltx_bibblock">Online; accessed 07-Aug-2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Güngör et al. (2018)</span>
<span class="ltx_bibblock">
Onur Güngör, Suzan Üsküdarlı, and Tunga Güngör.
2018.

</span>
<span class="ltx_bibblock">Improving named entity recognition by jointly learning to
disambiguate morphological tags.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.06683</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guven (2023)</span>
<span class="ltx_bibblock">
Guven. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.kaggle.com/datasets/anil1055/turkish-tweet-dataset" title="" class="ltx_ref ltx_href">Turkish
tweets dataset</a>.

</span>
<span class="ltx_bibblock">Online; accessed 07-Aug-2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hochreiter and Schmidhuber (1997)</span>
<span class="ltx_bibblock">
Sepp Hochreiter and Jürgen Schmidhuber. 1997.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Neural computation</em>, 9(8):1735–1780.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honnibal and Montani (2017)</span>
<span class="ltx_bibblock">
Matthew Honnibal and Ines Montani. 2017.

</span>
<span class="ltx_bibblock">spaCy 2: Natural language understanding with Bloom embeddings,
convolutional neural networks and incremental parsing.

</span>
<span class="ltx_bibblock">To appear.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2020)</span>
<span class="ltx_bibblock">
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and
Melvin Johnson. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2003.11080" title="" class="ltx_ref ltx_href">Xtreme: A massively
multilingual multi-task benchmark for evaluating cross-lingual
generalization</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2003.11080.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kahyaoglu (2023)</span>
<span class="ltx_bibblock">
Kahyaoglu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/mertkahyaoglu/twitter-sentiment-analysis" title="" class="ltx_ref ltx_href">twitter-sentiment-analysis</a>.

</span>
<span class="ltx_bibblock">Online; accessed 07-Aug-2023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2014)</span>
<span class="ltx_bibblock">
Diederik P Kingma and Jimmy Ba. 2014.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.6980</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn (2023)</span>
<span class="ltx_bibblock">
Schroeder. Koehn. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/mediacloud/sentence-splitter" title="" class="ltx_ref ltx_href">Text to
sentence splitter</a>.

</span>
<span class="ltx_bibblock">Online; accessed 07-Aug-2023.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Küçük et al. (2014)</span>
<span class="ltx_bibblock">
Dilek Küçük, Guillaume Jacquet, and Ralf Steinberger. 2014.

</span>
<span class="ltx_bibblock">Named entity recognition on turkish tweets.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Ninth International Conference on
Language Resources and Evaluation (LREC’14)</em>, pages 450–454.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Küçük et al. (2016)</span>
<span class="ltx_bibblock">
Dilek Küçük, Doğan Küçük, and Nursal
Arıcı. 2016.

</span>
<span class="ltx_bibblock">A named entity recognition dataset for turkish.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">2016 24th Signal Processing and Communication Application
Conference (SIU)</em>, pages 329–332. IEEE.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson. 2018.

</span>
<span class="ltx_bibblock">Sentencepiece: A simple and language independent subword tokenizer
and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1808.06226</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Köksal and Özgür (2021)</span>
<span class="ltx_bibblock">
Abdullatif Köksal and Arzucan Özgür. 2021.

</span>
<span class="ltx_bibblock">Twitter dataset and evaluation of transformers for turkish sentiment
analysis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">2021 29th Signal Processing and Communications Applications
Conference (SIU)</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov et al. (2013)</span>
<span class="ltx_bibblock">
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.

</span>
<span class="ltx_bibblock">Efficient estimation of word representations in vector space.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1301.3781</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(34)</span>
<span class="ltx_bibblock">
Filipp Ozinov.

</span>
<span class="ltx_bibblock">Jamspell.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://web.archive.org/web/20230314091411/http://github.com/bakwc/JamSpell/" title="" class="ltx_ref ltx_href">http://github.com/bakwc/JamSpell/</a>.

</span>
<span class="ltx_bibblock">Archived: 14-Mar-2023.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ozler (2023)</span>
<span class="ltx_bibblock">
Ozler. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.kaggle.com/datasets/kbulutozler/5k-turkish-tweets-with-incivil-content" title="" class="ltx_ref ltx_href">5k turkish tweets with incivil content</a>.

</span>
<span class="ltx_bibblock">Online; accessed 07-Aug-2023.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. (2017)</span>
<span class="ltx_bibblock">
Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng
Ji. 2017.

</span>
<span class="ltx_bibblock">Cross-lingual name tagging and linking for 282 languages.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1946–1958.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke et al. (2019)</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" title="" class="ltx_ref ltx_href">Pytorch: An imperative style, high-performance deep learning library</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 32</em>, pages
8024–8035. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pedregosa et al. (2011)</span>
<span class="ltx_bibblock">
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel,
M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011.

</span>
<span class="ltx_bibblock">Scikit-learn: Machine learning in Python.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 12:2825–2830.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2019)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1910.10683" title="" class="ltx_ref ltx_href">Exploring the limits of
transfer learning with a unified text-to-text transformer</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Řehůřek and Sojka (2010)</span>
<span class="ltx_bibblock">
Radim Řehůřek and Petr Sojka. 2010.

</span>
<span class="ltx_bibblock">Software Framework for Topic Modelling with Large Corpora.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the LREC 2010 Workshop on New Challenges for
NLP Frameworks</em>, pages 45–50, Valletta, Malta. ELRA.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://is.muni.cz/publication/884893/en" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://is.muni.cz/publication/884893/en</a>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Safaya et al. (2022)</span>
<span class="ltx_bibblock">
Ali Safaya, Emirhan Kurtuluş, Arda Goktogan, and Deniz Yuret. 2022.

</span>
<span class="ltx_bibblock">Mukayese: Turkish nlp strikes back.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
ACL 2022</em>, pages 846–863.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sahin et al. (2017)</span>
<span class="ltx_bibblock">
H Bahadir Sahin, Caglar Tirkaz, Eray Yildiz, Mustafa Tolga Eren, and Ozan
Sonmez. 2017.

</span>
<span class="ltx_bibblock">Automatically annotated turkish corpus for named entity recognition
and text categorization using large-scale gazetteers.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1702.02363</em>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saif et al. (2014)</span>
<span class="ltx_bibblock">
Hassan Saif, Miriam Fernandez, and Harith Alani. 2014.

</span>
<span class="ltx_bibblock">On stopwords, filtering and data sparsity for sentiment analysis of
twitter.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 9th International Language Resources and
Evaluation Conference (LREC’14)</em>, pages 810–817.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarigil (2023)</span>
<span class="ltx_bibblock">
Sarigil. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.kaggle.com/datasets/seymasa/turkish-sales-comments" title="" class="ltx_ref ltx_href">Turkish
sales comments</a>.

</span>
<span class="ltx_bibblock">Online; accessed 07-Aug-2023.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Satopaa et al. (2011)</span>
<span class="ltx_bibblock">
Ville Satopaa, Jeannie Albrecht, David Irwin, and Barath Raghavan. 2011.

</span>
<span class="ltx_bibblock">Finding a" kneedle" in a haystack: Detecting knee points in system
behavior.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">2011 31st international conference on distributed computing
systems workshops</em>, pages 166–171. IEEE.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sevinc (2023)</span>
<span class="ltx_bibblock">
Sevinc. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/emres/turkish-deasciifier" title="" class="ltx_ref ltx_href">turkish-deasciifier: Turkish deasciifier</a>.

</span>
<span class="ltx_bibblock">Online; accessed 07-Aug-2023.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2016)</span>
<span class="ltx_bibblock">
Qinlan Shen, Daniel Clothiaux, Emily Tagtow, Patrick Littell, and Chris Dyer.
2016.

</span>
<span class="ltx_bibblock">The role of context in neural morphological disambiguation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of COLING 2016, the 26th International
Conference on Computational Linguistics: Technical Papers</em>, pages 181–191.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siarohin et al. (2021)</span>
<span class="ltx_bibblock">
Aliaksandr Siarohin, Oliver J. Woodford, Jian Ren, Menglei Chai, and Sergey
Tulyakov. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2104.11280" title="" class="ltx_ref ltx_href">Motion representations for
articulated animation</a>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Subasi (2023)</span>
<span class="ltx_bibblock">
Subasi. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/ezgisubasi/turkish-tweets-sentiment-analysis" title="" class="ltx_ref ltx_href">turkish-tweets-sentiment-analysis</a>.

</span>
<span class="ltx_bibblock">Online; accessed 07-Aug-2023.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teghub (2023)</span>
<span class="ltx_bibblock">
Teghub. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/teghub/TurkishNER-BERT/tree/master/TurkishNERdata3Labels" title="" class="ltx_ref ltx_href">Turkishnerdata3labels</a>.

</span>
<span class="ltx_bibblock">Online; accessed 07-Aug-2023.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tür et al. (2003)</span>
<span class="ltx_bibblock">
Gökhan Tür, Dilek Hakkani-Tür, and Kemal Oflazer. 2003.

</span>
<span class="ltx_bibblock">A statistical information extraction system for turkish.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Natural Language Engineering</em>, 9(2):181–210.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Uskudarli et al. (2023)</span>
<span class="ltx_bibblock">
Susan Uskudarli, Muhammet Şen, Furkan Akkurt, Merve Gürbüz, Onur
Güngör, Arzucan Özgür, and Tunga Güngör. 2023.

</span>
<span class="ltx_bibblock">Tulap-an accessible and sustainable platform for turkish natural
language processing resources.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th Conference of the European Chapter
of the Association for Computational Linguistics: System Demonstrations</em>,
pages 219–227.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2019)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
et al. 2019.

</span>
<span class="ltx_bibblock">Huggingface’s transformers: State-of-the-art natural language
processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.03771</em>.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yildiz (2023)</span>
<span class="ltx_bibblock">
Yildiz. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/erayyildiz/LookupAnalyzerDisambiguator" title="" class="ltx_ref ltx_href">Lookupanalyzerdisambiguator</a>.

</span>
<span class="ltx_bibblock">Online; accessed 07-Aug-2023.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yildiz et al. (2016)</span>
<span class="ltx_bibblock">
Eray Yildiz, Caglar Tirkaz, H. Sahin, Mustafa Eren, and Omer Sonmez. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1609/aaai.v30i1.10355" title="" class="ltx_ref ltx_href">A morphology-aware
network for morphological disambiguation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>,
30(1).

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yilmaz (2023)</span>
<span class="ltx_bibblock">
Yilmaz. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.kaggle.com/code/baharyilmaz/turkish-sentiment-analysis/input" title="" class="ltx_ref ltx_href">Turkish sentiment analysis</a>.

</span>
<span class="ltx_bibblock">Online; accessed 07-Aug-2023.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yilmaz (2018)</span>
<span class="ltx_bibblock">
Selim Fırat Yilmaz. 2018.

</span>
<span class="ltx_bibblock">Bilkent turkish writings dataset.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://web.archive.org/web/20201020041933/https://github.com/selimfirat/bilkent-turkish-writings-dataset" title="" class="ltx_ref ltx_href">https://github.com/selimfirat/bilkent-turkish-writings-dataset</a>.

</span>
<span class="ltx_bibblock">Archived: 20-Oct-2020.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuret and Ture (2006)</span>
<span class="ltx_bibblock">
Deniz Yuret and Ferhan Ture. 2006.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://www.aclweb.org/anthology/N/N06/N06-1042" title="" class="ltx_ref ltx_href">Learning
morphological disambiguation rules for turkish</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Human Language Technology Conference of
the NAACL, Main Conference</em>, pages 328–334, New York City, USA. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2004.11867" title="" class="ltx_ref ltx_href">Improving massively
multilingual neural machine translation and zero-shot translation</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.01308" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.01309" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.01309">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.01309" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.01310" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 15:25:08 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
