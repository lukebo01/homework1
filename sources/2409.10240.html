<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.10240] RoboVox Far Field Speaker Recognition: A Novel Data Augmentation Approach with Pretrained Models</title><meta property="og:description" content="In this study, we address the challenge of speaker recognition using a novel data augmentation technique of adding noise to enrollment files. This technique efficiently aligns the sources of test and enrollment files, …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RoboVox Far Field Speaker Recognition: A Novel Data Augmentation Approach with Pretrained Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="RoboVox Far Field Speaker Recognition: A Novel Data Augmentation Approach with Pretrained Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.10240">

<!--Generated on Sat Oct  5 18:59:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">RoboVox Far Field Speaker Recognition: A Novel Data Augmentation Approach with Pretrained Models</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">In this study, we address the challenge of speaker recognition using a novel data augmentation technique of adding noise to enrollment files. This technique efficiently aligns the sources of test and enrollment files, enhancing comparability. Various pre-trained models were employed, with the resnet model achieving the highest DCF of 0.84 and an EER of 13.44. The augmentation technique notably improved these results to 0.75 DCF and 12.79 EER for the resnet model. Comparative analysis revealed the superiority of resnet over models such as ECPA, Mel-spectrogram, Payonnet, and Titanet large. Results, along with different augmentation schemes, contribute to the success of RoboVox far field speaker recognition in this paper.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">— </span></span>
speech augmentation, far-field speaker recognition, pre-trained model</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Speaker recognition systems are extensively utilized in applications related to home customization, authentication, and security. It is a biometric technology that helps to identify the system whether a pair of utterances correlate to the same speaker or not. The speaker verification generally contains a speaker pulling out embedding and a scoring process. During the embedding extraction process, audio of varying lengths is transformed into one fixed-dimensional vector representation known as a speaker embedding. This embedding is intended to include information about the speaker. For the scoring method, cosine similarity or Euclidean distance can be used. In recent years, with the development of computing power, deep learning techniques have been popular for the speaker verification process. However, the effectiveness significantly decreases when the speech is obtained in natural, uncontrolled environments such as far-field noisy environments with variable distance and reverberation. There are some benchmarks for these problems are VoiCes and FFSVC.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> However, they don’t include the internal noise of the device and the angle between the device and the speakers.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Usually, voice samples collected from distances are rather small and not enough to build high-quality speaker verification models without any prior training. Consequently, near-field datasets are generally utilized for training to enhance the classification performance of speaker verification systems. There are different kinds of transfer learning methods used to address this domain mismatch. Data augmentation is the most used technique to address this domain mismatch and train a robust neural network. Simulated reverberation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, additive noise, and Specaugment<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> are effective methods for augmenting data in speaker verification. These techniques can expand the variety of acoustic environments that may be encountered during real-life scenarios.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The challenge of SPCUP-2024 is Robovox: far-field speaker recognition by a mobile robot. We started the quest to solve this challenge by extracting embeddings of raw signals through neural networks. After going through several experiments we have that the system works well with pre-trained models and even better if we augment the near clean train data with some artificial noise. Finally, we have developed a novel augmentation method that helped us to attain optimal performance in both the EER and DCF assessment criteria. Our noise and reverberation augmentation techniques for real-life scenarios surpass our different experimental approaches. The rest of the papers are organized as follows. Section 2 represents the methodology of our work with different augmentation techniques. The results of our experiment are shown in section 3, while 4 discusses the results based on our experiments. The conclusion is added in section 5.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2409.10240/assets/Untitled-2.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="559" height="206" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fig. 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">An overall representation of our implemented framework. It starts by taking audio files from channel 5 and mixing them with noise extracted from audio files related to channel 4. The augmented enrollment signal is used to calculate the vector embedding set with the help of a large deep-learning model. Simultaneously the audio files from the test set are also fetched to calculate the embedding. Both of these are compared with the cosine dissimilarity evaluation metric</span></figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Dataset Description</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">This competition leverages the Robovox dataset. This dataset adds a novel benchmark in the research of far-field single-channel and multi-channel speaker verification. A robot is equipped with three microphones positioned at the angle of the robot (channels 1 to 3). The fourth microphone (channel 4) is placed inside the robot. Another microphone (channel 5) used as a ground truth microphone is placed close to the speaker. The dataset comprises 2,219 conversations spoken by 78 individuals. Each conversation is composed of an average of 5 dialogues, resulting in a total of around 11,000 dialogues. The average recording of the dialogue is 3.5 seconds. The dataset was recorded at different distances of 1m, 2m, and 3m from the speakers. To emulate real-life scenarios the session is recorded in different room environments in the hall, open space, and small and medium rooms while placing the robot at the wall, center, and corner. The dataset contains two parts for single-channel and multichannel tracks. For this competition, a single channel is utilized. For the enrollment files channel 5 and the test files, channel 4 is chosen, containing 225 and 10,332 files consecutively.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Preprocessing</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In the conventional machine learning approach, the data the model is trained on and evaluated needs to come from the same source. If the audio enrollment that is used for the learning algorithm is from one source and then the data is evaluated from data of another source raises difficulty. In this benchmark, the enrollment files are recorded with channel 5, which is the best channel. On the other hand, the test dataset is recorded with channel 4 which is the most challenging channel. The enrollment audio files are less noisy and ambient than test audio files as they were recorded closer to the speaker. The test audio is not only noisy but contains multiple variabilities with that noise such as reverberations, angles etc. Thus, the signal-to-noise (SNR) ratio from those two sources contains massive dissimilarity. Another problem was, that there was no voice activity in some audio files provided such as spk_6-6_11_0_0_d4_ch5 and all the files for spk_21 and some other files as well in enrollment and test dataset. Assigning subject labels to these data and then feeding it into the learning algorithm alongside other data risks misleading the algorithm and preventing it from learning correctly. Thus, instead of focusing on improving the learning algorithms itself, we focused more on how the data set could be improved which can be later used for feature extraction. We applied two different schemes that can reduce the mismatch between these two sources and improve proximity. In our first scheme, we focused on reducing noise from the test data upon simulating the noise reduction on enrollment data. In our second scheme, we focused on augmenting the enrollment files with similar and equivalent noise to the test dataset with our developed approach.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Noise Reduction</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">In this scheme, we tried to reduce the noise from the test dataset before it was fetched into a deep learning model for embedding calculation. We used the noisereduce library <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> which is a common library to reduce noise for stationary and non-stationary signals. We set the parameter of the proportion to reduce the noise by 100% and threshold for non-stationary noise reduction 1 considering the test files containing different variabilities.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Data augmentation with noise samples</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">In our second scheme of preprocessing, we utilized data augmentation by adding noises to our enrollment audio files. In general, two approaches can be used. First, noise such as Gaussian noise, pink noise, etc. can be simulated by different available libraries namely numpy, pudub etc. Secondly, background noises can collected from different resources and datasets such as AudioSet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">We hypothesize that reducing the mismatch between two sources may lead to reducing the cosine dissimilarity for the same speaker and increasing dissimilarity between different speakers. Instead of generating noise from different resources we simulated the noises from audio files which were recorded with channel 4 and used them to augment the enrollment dataset which was recorded with channel 5. We start by setting a threshold by manually inspecting the audio files and then detect the voice activity intervals where the amplitude in decibels is above the threshold. The intervals are used to create a binary masking which is multiplied by the corresponding sample audio signal to extract noise samples shown in figure <a href="#S2.F2" title="Figure 2 ‣ 2.2.2 Data augmentation with noise samples ‣ 2.2 Preprocessing ‣ 2 Methodology ‣ RoboVox Far Field Speaker Recognition: A Novel Data Augmentation Approach with Pretrained Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> which were used for augmenting audio files from channel 5. The step-by-step process is shown in the algorithm table.</p>
</div>
<figure id="S2.SS2.SSS2.tab1" class="ltx_table">
<table id="S2.SS2.SSS2.tab1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.SS2.SSS2.tab1.1.1.1" class="ltx_tr">
<th id="S2.SS2.SSS2.tab1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="4">
<table id="S2.SS2.SSS2.tab1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.SS2.SSS2.tab1.1.1.1.1.1.1" class="ltx_tr">
<td id="S2.SS2.SSS2.tab1.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S2.SS2.SSS2.tab1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Algorithm: Noise extraction from audio files for</span></td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.1.1.1.1.2" class="ltx_tr">
<td id="S2.SS2.SSS2.tab1.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><span id="S2.SS2.SSS2.tab1.1.1.1.1.1.2.1.1" class="ltx_text ltx_font_bold">augmentation</span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.SS2.SSS2.tab1.1.2.1" class="ltx_tr">
<th id="S2.SS2.SSS2.tab1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">1</th>
<td id="S2.SS2.SSS2.tab1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">Start</td>
<td id="S2.SS2.SSS2.tab1.1.2.1.3" class="ltx_td ltx_border_t"></td>
<td id="S2.SS2.SSS2.tab1.1.2.1.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.3.2" class="ltx_tr">
<th id="S2.SS2.SSS2.tab1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2</th>
<td id="S2.SS2.SSS2.tab1.1.3.2.2" class="ltx_td ltx_align_left" colspan="3">Input: Audio signal, Threshold in dB</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.4.3" class="ltx_tr">
<th id="S2.SS2.SSS2.tab1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">3</th>
<td id="S2.SS2.SSS2.tab1.1.4.3.2" class="ltx_td ltx_align_left" colspan="3">For each sample in an audio signal:</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.5.4" class="ltx_tr">
<th id="S2.SS2.SSS2.tab1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">4</th>
<td id="S2.SS2.SSS2.tab1.1.5.4.2" class="ltx_td"></td>
<td id="S2.SS2.SSS2.tab1.1.5.4.3" class="ltx_td ltx_align_left" colspan="2">if sample in dB &lt;threshold:</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.6.5" class="ltx_tr">
<th id="S2.SS2.SSS2.tab1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">5</th>
<td id="S2.SS2.SSS2.tab1.1.6.5.2" class="ltx_td"></td>
<td id="S2.SS2.SSS2.tab1.1.6.5.3" class="ltx_td"></td>
<td id="S2.SS2.SSS2.tab1.1.6.5.4" class="ltx_td ltx_align_left">Start of non-silent period</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.7.6" class="ltx_tr">
<th id="S2.SS2.SSS2.tab1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">6</th>
<td id="S2.SS2.SSS2.tab1.1.7.6.2" class="ltx_td"></td>
<td id="S2.SS2.SSS2.tab1.1.7.6.3" class="ltx_td ltx_align_left" colspan="2">Else</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.8.7" class="ltx_tr">
<th id="S2.SS2.SSS2.tab1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">7</th>
<td id="S2.SS2.SSS2.tab1.1.8.7.2" class="ltx_td"></td>
<td id="S2.SS2.SSS2.tab1.1.8.7.3" class="ltx_td"></td>
<td id="S2.SS2.SSS2.tab1.1.8.7.4" class="ltx_td ltx_align_left">End of non-silent period</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.9.8" class="ltx_tr">
<th id="S2.SS2.SSS2.tab1.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">8</th>
<td id="S2.SS2.SSS2.tab1.1.9.8.2" class="ltx_td ltx_align_left" colspan="3">
<table id="S2.SS2.SSS2.tab1.1.9.8.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.SS2.SSS2.tab1.1.9.8.2.1.1" class="ltx_tr">
<td id="S2.SS2.SSS2.tab1.1.9.8.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Make a list of ranges of non-silent periods</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.9.8.2.1.2" class="ltx_tr">
<td id="S2.SS2.SSS2.tab1.1.9.8.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">in an audio signal.</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.10.9" class="ltx_tr">
<th id="S2.SS2.SSS2.tab1.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">9</th>
<td id="S2.SS2.SSS2.tab1.1.10.9.2" class="ltx_td ltx_align_left" colspan="3">
<table id="S2.SS2.SSS2.tab1.1.10.9.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.SS2.SSS2.tab1.1.10.9.2.1.1" class="ltx_tr">
<td id="S2.SS2.SSS2.tab1.1.10.9.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Initialize an empty list to store</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.10.9.2.1.2" class="ltx_tr">
<td id="S2.SS2.SSS2.tab1.1.10.9.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">expanded non-silent indices</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.11.10" class="ltx_tr">
<th id="S2.SS2.SSS2.tab1.1.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">10</th>
<td id="S2.SS2.SSS2.tab1.1.11.10.2" class="ltx_td ltx_align_left" colspan="3">For each interval in non-silent intervals:</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.12.11" class="ltx_tr">
<th id="S2.SS2.SSS2.tab1.1.12.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">11</th>
<td id="S2.SS2.SSS2.tab1.1.12.11.2" class="ltx_td"></td>
<td id="S2.SS2.SSS2.tab1.1.12.11.3" class="ltx_td ltx_align_left" colspan="2">Expand the interval to individual indices.</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.13.12" class="ltx_tr">
<th id="S2.SS2.SSS2.tab1.1.13.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">12</th>
<td id="S2.SS2.SSS2.tab1.1.13.12.2" class="ltx_td"></td>
<td id="S2.SS2.SSS2.tab1.1.13.12.3" class="ltx_td ltx_align_left" colspan="2">
<table id="S2.SS2.SSS2.tab1.1.13.12.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.SS2.SSS2.tab1.1.13.12.3.1.1" class="ltx_tr">
<td id="S2.SS2.SSS2.tab1.1.13.12.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Add the expanded indices to the list</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.13.12.3.1.2" class="ltx_tr">
<td id="S2.SS2.SSS2.tab1.1.13.12.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">of non-silent indices</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.14.13" class="ltx_tr">
<th id="S2.SS2.SSS2.tab1.1.14.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">13</th>
<td id="S2.SS2.SSS2.tab1.1.14.13.2" class="ltx_td ltx_align_left" colspan="3">For each index in the list of non-silent indices:</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.15.14" class="ltx_tr">
<th id="S2.SS2.SSS2.tab1.1.15.14.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">14</th>
<td id="S2.SS2.SSS2.tab1.1.15.14.2" class="ltx_td"></td>
<td id="S2.SS2.SSS2.tab1.1.15.14.3" class="ltx_td ltx_align_left" colspan="2">Set the corresponding index in the mask to 0</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.16.15" class="ltx_tr">
<th id="S2.SS2.SSS2.tab1.1.16.15.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">15</th>
<td id="S2.SS2.SSS2.tab1.1.16.15.2" class="ltx_td ltx_align_left" colspan="3">
<table id="S2.SS2.SSS2.tab1.1.16.15.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.SS2.SSS2.tab1.1.16.15.2.1.1" class="ltx_tr">
<td id="S2.SS2.SSS2.tab1.1.16.15.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Multiply the mask with the original</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.16.15.2.1.2" class="ltx_tr">
<td id="S2.SS2.SSS2.tab1.1.16.15.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">signal (element-wise multiplication)</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.17.16" class="ltx_tr">
<th id="S2.SS2.SSS2.tab1.1.17.16.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">16</th>
<td id="S2.SS2.SSS2.tab1.1.17.16.2" class="ltx_td ltx_align_left" colspan="3">
<table id="S2.SS2.SSS2.tab1.1.17.16.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.SS2.SSS2.tab1.1.17.16.2.1.1" class="ltx_tr">
<td id="S2.SS2.SSS2.tab1.1.17.16.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">The result is the noise-only signal,</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.17.16.2.1.2" class="ltx_tr">
<td id="S2.SS2.SSS2.tab1.1.17.16.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">where non-silent parts are nullified</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.18.17" class="ltx_tr">
<th id="S2.SS2.SSS2.tab1.1.18.17.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">17</th>
<td id="S2.SS2.SSS2.tab1.1.18.17.2" class="ltx_td ltx_align_left" colspan="3">Return the noise-only signal</td>
</tr>
<tr id="S2.SS2.SSS2.tab1.1.19.18" class="ltx_tr">
<th id="S2.SS2.SSS2.tab1.1.19.18.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b">18</th>
<td id="S2.SS2.SSS2.tab1.1.19.18.2" class="ltx_td ltx_align_left ltx_border_b" colspan="3">End</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S2.F2" class="ltx_figure ltx_align_center"><img src="/html/2409.10240/assets/asasas.png" id="S2.F2.1.g1" class="ltx_graphics ltx_img_square" width="699" height="590" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Fig. 2</span>: </span><span id="S2.F2.4.2" class="ltx_text" style="font-size:90%;">The step-by-step response for the implemented noise extraction algorithm. Where a) An audio file recorded with microphone 4, b) Binary mask obtained by determining voice activity intervals c) Result of multiplication binary mask with the audio signal d) Keeping non-zero segments i.e. noise.</span></figcaption>
</figure>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.3.2" class="ltx_text" style="font-size:90%;">Comparison of results before augmentation</span></figcaption>
<table id="S2.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.4.1.1" class="ltx_tr">
<th id="S2.T1.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S2.T1.4.1.1.1.1" class="ltx_text ltx_font_bold">Models</span></th>
<th id="S2.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S2.T1.4.1.1.2.1" class="ltx_text ltx_font_bold">EER</span></th>
<th id="S2.T1.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S2.T1.4.1.1.3.1" class="ltx_text ltx_font_bold">DCF</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.4.2.1" class="ltx_tr">
<td id="S2.T1.4.2.1.1" class="ltx_td ltx_align_center ltx_border_t">Pyannote</td>
<td id="S2.T1.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t">19.84</td>
<td id="S2.T1.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.99</td>
</tr>
<tr id="S2.T1.4.3.2" class="ltx_tr">
<td id="S2.T1.4.3.2.1" class="ltx_td ltx_align_center">ECAPA-TDNN</td>
<td id="S2.T1.4.3.2.2" class="ltx_td ltx_align_center">15.59</td>
<td id="S2.T1.4.3.2.3" class="ltx_td ltx_align_center">0.89</td>
</tr>
<tr id="S2.T1.4.4.3" class="ltx_tr">
<td id="S2.T1.4.4.3.1" class="ltx_td ltx_align_center">Titanet Large</td>
<td id="S2.T1.4.4.3.2" class="ltx_td ltx_align_center">15.75</td>
<td id="S2.T1.4.4.3.3" class="ltx_td ltx_align_center">0.88</td>
</tr>
<tr id="S2.T1.4.5.4" class="ltx_tr">
<td id="S2.T1.4.5.4.1" class="ltx_td ltx_align_center">Mel-Spec + ECAPA-TDNN</td>
<td id="S2.T1.4.5.4.2" class="ltx_td ltx_align_center">15.05</td>
<td id="S2.T1.4.5.4.3" class="ltx_td ltx_align_center">0.88</td>
</tr>
<tr id="S2.T1.4.6.5" class="ltx_tr">
<td id="S2.T1.4.6.5.1" class="ltx_td ltx_align_center ltx_border_b">ResNet-TDNN</td>
<td id="S2.T1.4.6.5.2" class="ltx_td ltx_align_center ltx_border_b">13.44</td>
<td id="S2.T1.4.6.5.3" class="ltx_td ltx_align_center ltx_border_b">0.84</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T2.2.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Table 2</span>: </span><span id="S2.T2.3.2" class="ltx_text" style="font-size:90%;">Comparison of results after noise reduction and augmentation. Three augmentation schemas have been used with different levels of signal-to-noise ratio (SNR).</span></figcaption>
<table id="S2.T2.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T2.4.1.1" class="ltx_tr">
<td id="S2.T2.4.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="3"><span id="S2.T2.4.1.1.1.1" class="ltx_text">Feature extractor models</span></td>
<td id="S2.T2.4.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" rowspan="2"><span id="S2.T2.4.1.1.2.1" class="ltx_text">Noise Reduction</span></td>
<td id="S2.T2.4.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="6">Augmentation</td>
</tr>
<tr id="S2.T2.4.2.2" class="ltx_tr">
<td id="S2.T2.4.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">
<table id="S2.T2.4.2.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.4.2.2.1.1.1" class="ltx_tr">
<td id="S2.T2.4.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Augmentation 1</td>
</tr>
<tr id="S2.T2.4.2.2.1.1.2" class="ltx_tr">
<td id="S2.T2.4.2.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">SNR(-10dB to -4dB)</td>
</tr>
</table>
</td>
<td id="S2.T2.4.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">
<table id="S2.T2.4.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.4.2.2.2.1.1" class="ltx_tr">
<td id="S2.T2.4.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Augmentation 2</td>
</tr>
<tr id="S2.T2.4.2.2.2.1.2" class="ltx_tr">
<td id="S2.T2.4.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">SNR(-7dB to -4dB</td>
</tr>
</table>
</td>
<td id="S2.T2.4.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">
<table id="S2.T2.4.2.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.4.2.2.3.1.1" class="ltx_tr">
<td id="S2.T2.4.2.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Augmentation 3</td>
</tr>
<tr id="S2.T2.4.2.2.3.1.2" class="ltx_tr">
<td id="S2.T2.4.2.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">SNR(-7dB to +3dB)</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T2.4.3.3" class="ltx_tr">
<td id="S2.T2.4.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">EER</td>
<td id="S2.T2.4.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DCF</td>
<td id="S2.T2.4.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">EER</td>
<td id="S2.T2.4.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DCF</td>
<td id="S2.T2.4.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">EER</td>
<td id="S2.T2.4.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DCF</td>
<td id="S2.T2.4.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">EER</td>
<td id="S2.T2.4.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DCF</td>
</tr>
<tr id="S2.T2.4.4.4" class="ltx_tr">
<td id="S2.T2.4.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">ECAPA-TDNN</td>
<td id="S2.T2.4.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">19.53</td>
<td id="S2.T2.4.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.96</td>
<td id="S2.T2.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">23.25</td>
<td id="S2.T2.4.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.9</td>
<td id="S2.T2.4.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">24.02</td>
<td id="S2.T2.4.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.91</td>
<td id="S2.T2.4.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">25.57</td>
<td id="S2.T2.4.4.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.97</td>
</tr>
<tr id="S2.T2.4.5.5" class="ltx_tr">
<td id="S2.T2.4.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Mel-Spectrogram + ECAPA-TDNN</td>
<td id="S2.T2.4.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">18.56</td>
<td id="S2.T2.4.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.94</td>
<td id="S2.T2.4.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.47</td>
<td id="S2.T2.4.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.85</td>
<td id="S2.T2.4.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.66</td>
<td id="S2.T2.4.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.83</td>
<td id="S2.T2.4.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">14.47</td>
<td id="S2.T2.4.5.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.85</td>
</tr>
<tr id="S2.T2.4.6.6" class="ltx_tr">
<td id="S2.T2.4.6.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Resnet-TDNN</td>
<td id="S2.T2.4.6.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">18.15</td>
<td id="S2.T2.4.6.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.93</td>
<td id="S2.T2.4.6.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T2.4.6.6.4.1" class="ltx_text ltx_font_bold">12.72</span></td>
<td id="S2.T2.4.6.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T2.4.6.6.5.1" class="ltx_text ltx_font_bold">0.75</span></td>
<td id="S2.T2.4.6.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">12.99</td>
<td id="S2.T2.4.6.6.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">77</td>
<td id="S2.T2.4.6.6.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">13.63</td>
<td id="S2.T2.4.6.6.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.77</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Extracting the embedding</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">After the preprocessing, we extracted the embedding using 4 pre-trained models of different architectures. The ECAPA-TDNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is a Time Delay Neural Network (TDNN) based model built upon x-vector architecture and using multi-scale Res2Net features. This speaker-embedding extractor model emphasizes Channel Attention, Propagation, and Aggregation. The pre-trained model used in our implementation is trained with Voxceleb1<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and Voxceleb2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> training data. We have also tested the model with mel-spectrogram as input instead of using direct raw audio. The Resnet-Tdnn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> model is based on 34 layered residual networks. While the pyannote audio toolbox<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> has used a cannocial x-vector-based TDNN-based architecture. Also, SincNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> features have been used in this pre-trained model. Finally, the titanet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> has used Squeeze-and-Excitation layers followed by channel attention-based statistics pooling layer. All of the models were used from the speechbrain package <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Result</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">For speaker embedding calculation five different pretrained models (Paynnote, ECPA, Titanet, Melspectrogram, and ResNet) were employed. To address the challenge that, multiple files associated with a single speaker, the average embedding was computed to derive the final representation of each speaker. The results of each model are summarized in Table <a href="#S2.T1" title="Table 1 ‣ 2.2.2 Data augmentation with noise samples ‣ 2.2 Preprocessing ‣ 2 Methodology ‣ RoboVox Far Field Speaker Recognition: A Novel Data Augmentation Approach with Pretrained Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Paynnote exhibited a DCF of 0.99 and an EER of 19.85, while ECAPA outperformed Paynnote with a DCF of 0.89 and an EER of 15.59. Titanet and Melspectrogram demonstrated further improvements, achieving DCF values of 0.88, with corresponding EER results of 15.75 and 15.8, respectively. The ResNet model attained the best results, with a DCF of 0.84 and an EER of 13.44.
The evaluation extended to the impact of noise reduction of test files and different enrollment file augmentation processes on the performance of ECPA, ResNet and Melspectrogram models, detailed in Table 2. The noise reduction applied to the test files appeared to have limited effectiveness, as the models yielded higher DCF and EER values compared to the original test file’s dissimilarity. Specifically, ECPA, Melspectrogram, and ResNet attained DCF values of 0.96, 0.94, and 0.93, respectively. Employing three distinct frequency ranges for data augmentation (-3 dB to -17 dB, -7 dB to -4 dB, and -10 dB to -4 dB), we observed notable enhancements. In the first augmentation process (Aug3), both ResNet and Melspectrogram outperformed the models using only original enrollment files. Specifically, Melspectrogram improved from a DCF of 0.88 to 0.85, and ResNet improved from 0.84 to 0.77. The second frequency range (-7 dB to -4 dB) proved effective for both models, resulting in improved DCF values of 0.83 and 0.77 for Melspectrogram and ResNet, respectively. The third augmentation process, based on the frequency range of -10 dB to -4 dB, emerged as the optimal solution for both Melspectrogram and ResNet, achieving DCF values of 0.82 and 0.75, respectively. Conversely, the ECAPA model exhibited DCF values of 0.99, 0.91, and 0.90 for these three augmentation processes, indicating a sensitivity to variations in the augmentation parameters.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">These findings contribute substantial insights to the field of speaker verification, specifically within the context of the Robovox far field speaker recognition dataset. The nuanced performance variations observed across pretrained models underscore the critical importance of carefully selecting model architectures tailored to the characteristics of the dataset and specific application requirements. Notably, the effectiveness of ResNet, especially when coupled with different augmentation strategies, suggests its robustness in capturing the unique speaker characteristics prevalent in the Robovox far field dataset. Conversely, the observed sensitivity of the ECAPA model to augmentation parameters emphasizes the need for model-specific optimization, particularly when applied to this distinct dataset.
Furthermore, the study shows the potential of data augmentation technique by incorporating noise in elevating the overall performance of speaker verification systems, specifically in the context of the Robovox far field dataset. The discernible improvements in DCF values and EER across different augmentation processes underscore the paramount importance of tailoring augmentation strategies to the intricacies of the Robovox far field dataset and the pretrained models employed.
The study not only advances our understanding of pretrained model performances but also highlights the significance of dataset-specific considerations and augmentation strategies in optimizing speaker verification outcomes within the unique characteristics of the Robovox far field dataset.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, a novel data augmentation technique, noise addition to enrollment files was employed and, the resnet pretrained model’s notable effectiveness, achieving a DCF of 0.75 and an EER of 12.79 was observed. The application of our data augmentation technique significantly improved the model’s performance, reducing the DCF rating from 0.84 to 0.75. This indicates the efficacy of the proposed approach, tailored to the characteristics of the Robovox far field speaker dataset, positioning the data augmentation technique as a valuable tool for addressing speaker verification challenges. The study establishes a concise yet impactful strategy for enhanced speaker recognition outcomes, contributing to advancements in far field speaker verification system.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Nagrani, J. S. Chung, W. Xie, and A. Zisserman, “Voxceleb: Large-scale speaker verification in the wild,” <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Computer Science and Language</span>, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. S. Chung, A. Nagrani, and A. Zisserman, “Voxceleb2: Deep speaker recognition,” in <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">INTERSPEECH</span>, 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
M. Liu, K. A. Lee, L. Wang, H. Zhang, C. Zeng, and J. Dang, “Cross-modal audio-visual co-learning for text-independent speaker verification,” in <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pp. 1–5, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, “Specaugment: A simple data augmentation method for automatic speech recognition,” in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Interspeech 2019</span>, interspeech2019, ISCA, Sept. 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
T. Sainburg, M. Thielk, and T. Q. Gentner, “Finding, visualizing, and quantifying latent structure across diverse animal vocal repertoires,” <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">PLoS computational biology</span>, vol. 16, no. 10, p. e1008228, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and human-labeled dataset for audio events,” in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proc. IEEE ICASSP 2017</span>, (New Orleans, LA), 2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
B. Desplanques, J. Thienpondt, and K. Demuynck, “Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification,” in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Interspeech 2020</span> (H. Meng, B. Xu, and T. F. Zheng, eds.), pp. 3830–3834, ISCA, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. Villalba, N. Chen, D. Snyder, D. Garcia-Romero, A. McCree, G. Sell, J. Borgstrom, L. P. García-Perera, F. Richardson, R. Dehak, P. A. Torres-Carrasquillo, and N. Dehak, “State-of-the-art speaker recognition with neural network embeddings in nist sre18 and speakers in the wild evaluations,” <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Computer Speech &amp; Language</span>, vol. 60, p. 101026, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H. Bredin, R. Yin, J. M. Coria, G. Gelly, P. Korshunov, M. Lavechin, D. Fustes, H. Titeux, W. Bouaziz, and M.-P. Gill, “pyannote.audio: neural building blocks for speaker diarization,” in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">ICASSP 2020, IEEE International Conference on Acoustics, Speech, and Signal Processing</span>, (Barcelona, Spain), May 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M. Ravanelli and Y. Bengio, “Speaker recognition from raw waveform with sincnet,” in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">2018 IEEE Spoken Language Technology Workshop (SLT)</span>, pp. 1021–1028, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
N. R. Koluguri, T. Park, and B. Ginsburg, “Titanet: Neural model for speaker representation with 1d depth-wise separable convolutions and global context,” 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
M. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. Cornell, L. Lugosch, C. Subakan, N. Dawalatabad, A. Heba, J. Zhong, J.-C. Chou, S.-L. Yeh, S.-W. Fu, C.-F. Liao, E. Rastorgueva, F. Grondin, W. Aris, H. Na, Y. Gao, R. D. Mori, and Y. Bengio, “SpeechBrain: A general-purpose speech toolkit,” 2021.

</span>
<span class="ltx_bibblock">arXiv:2106.04624.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.10238" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.10240" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.10240">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.10240" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.10241" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 18:59:46 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
