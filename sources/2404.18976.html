<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.18976] Foundations of Multisensory Artificial Intelligence</title><meta property="og:description" content="Building multisensory artificial intelligence systems that learn from multiple sensory inputs such as text, speech, video, real-world sensors, wearable devices, and medical data holds great promise for impact in many s…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Foundations of Multisensory Artificial Intelligence">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Foundations of Multisensory Artificial Intelligence">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.18976">

<!--Generated on Sun May  5 21:25:52 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on May 2024.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\Year</span>
<p id="p1.2" class="ltx_p">2024
<span id="p1.2.1" class="ltx_ERROR undefined">\trnumber</span>CMU-ML-24-103
<span id="p1.2.2" class="ltx_ERROR undefined">\committee</span>
Louis-Philippe Morency, Co-chair 
<br class="ltx_break">Ruslan Salakhutdinov, Co-chair 
<br class="ltx_break">Manuel Blum 
<br class="ltx_break">Lenore Blum (UC Berkeley) 
<br class="ltx_break">Trevor Darrell (UC Berkeley)</p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\support</span>
<p id="p2.2" class="ltx_p">This research was funded by: National Science Foundation awards IIS1722822 and IIS1750439; National Institutes of Health awards R01MH096951 and U01MH116923; graduate fellowships from Meta Platforms and Siebel Scholars; and grants from Meta Platforms, Nippon Telegraph and Telephone Corporation, Oculus VR, and Samsung Electronics.</p>
</div>
<h1 class="ltx_title ltx_title_document"> <span id="id1.id1" class="ltx_text ltx_font_bold">Foundations of Multisensory Artificial Intelligence</span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Paul Pu Liang
</span></span>
</div>
<div class="ltx_dates">(May 2024)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Building multisensory artificial intelligence systems that learn from multiple sensory inputs such as text, speech, video, real-world sensors, wearable devices, and medical data holds great promise for impact in many scientific areas with practical benefits, such as in supporting human health and well-being, enabling multimedia content processing, and enhancing real-world autonomous agents.</p>
<p id="id3.id2" class="ltx_p">However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a range of theoretical frameworks and application domains, this thesis aims to advance the foundations of multimodal machine learning. We start by defining three key principles of modality <span id="id3.id2.1" class="ltx_text ltx_font_italic">heterogeneity</span>, <span id="id3.id2.2" class="ltx_text ltx_font_italic">connections</span>, and <span id="id3.id2.3" class="ltx_text ltx_font_italic">interactions</span> often present in multimodal problems <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">liang2022foundations</span>]</cite>. Using these principles as a foundation, we propose a taxonomy of six core challenges in multimodal research: <span id="id3.id2.4" class="ltx_text ltx_font_italic">representation</span>, <span id="id3.id2.5" class="ltx_text ltx_font_italic">alignment</span>, <span id="id3.id2.6" class="ltx_text ltx_font_italic">reasoning</span>, <span id="id3.id2.7" class="ltx_text ltx_font_italic">generation</span>, <span id="id3.id2.8" class="ltx_text ltx_font_italic">transference</span>, and <span id="id3.id2.9" class="ltx_text ltx_font_italic">quantification</span>. Recent technical achievements will be presented through this taxonomy, allowing researchers to understand the similarities and differences across approaches, and identifying open problems for future research.</p>
<p id="id4.id3" class="ltx_p">The bulk of the thesis covers our recent progress towards tackling two key problems in multimodal learning: the machine learning foundations of multimodal interactions, as well as practical methods for building multisensory foundation models that generalize to many modalities and tasks in the real world.</p>
<p id="id5.id4" class="ltx_p">In the first part, we study the foundations of multimodal interactions: the basic principle of how modalities combine to give rise to new information for a task. We present a theoretical framework formalizing how <span id="id5.id4.1" class="ltx_text ltx_font_italic">modalities interact</span> with each other to give rise to new information for a task, such as sarcasm identified from the incongruity between spoken words and vocal expressions <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">liang2023quantifying</span>]</cite>.
Using this theoretical framework, we propose two practical estimators to quantify the interactions in real-world datasets. Quantifying the types of interactions a multimodal task requires enables researchers to decide which modality to collect <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">liang2023multimodal</span>]</cite>, design suitable approaches to learn these interactions <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">liang2023factorized</span>]</cite>, and analyze whether their model has succeeded in learning <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">liang2023multiviz</span>]</cite>.</p>
<p id="id6.id5" class="ltx_p">In the second part, we study the design of practical multimodal foundation models that generalize over many modalities and tasks, which presents a step toward grounding large language models to real-world sensory modalities.
We first introduce <span id="id6.id5.1" class="ltx_text ltx_font_smallcaps">MultiBench</span>, a unified large-scale benchmark across a wide range of modalities, tasks, and research areas <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">liang2021multibench</span>]</cite>.
We will also present the <span id="id6.id5.2" class="ltx_text ltx_font_italic">cross-modal attention</span> <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">liang2018multimodal</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">chen2017multimodal</span>]</cite> and <span id="id6.id5.3" class="ltx_text ltx_font_italic">multimodal transformer</span> <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">tsai2019multimodal</span>]</cite> architectures that now underpin many of today’s multimodal foundation models.
Scaling these architectures on <span id="id6.id5.4" class="ltx_text ltx_font_smallcaps">MultiBench</span> enables the creation of general-purpose multimodal multitask models across a variety of tasks, and we have collaborated broadly with practitioners to apply these models for real-world impact on affective computing, mental health, and cancer prognosis.</p>
<p id="id7.id6" class="ltx_p">We conclude this thesis by discussing how future work can leverage these ideas toward more general, interactive, and safe multimodal artificial intelligence.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Multimodal Machine Learning, Multisensory Artificial Intelligence, Deep Learning, Information Theory, Quantification, Generalization, Affective Computing, AI and Healthcare
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>

I owe my greatest acknowledgments to my advisors, mentors, and thesis committee members for their invaluable guidance during my PhD. To Louis-Philippe Morency and Ruslan Salakutdinov, who have closely guided my research and personal development at every stage over the past 5 years. LP has mentored me closely in all aspects of research - brainstorming ideas, idea execution, and written and oral presentations. Some of the best memories I’ve had during my PhD have been whiteboard brainstorming sessions, coming up with good names for problems and models, and collaboratively figuring out the best ways to visually depict technical concepts. Russ’s incredibly sharp insight and keen eye for impactful problems has shaped my thinking and forced me to work on problems that matter in practice, and I’ve thoroughly enjoyed our recent push towards interactive multimodal agents with other folks in the group and at CMU. Thank you LP and Russ for additionally giving me the opportunity to co-instruct and guest lecture CMU courses multimodal ML, deep learning, and socially intelligent AI. I also had the pleasure of working closely with Manuel Blum and Lenore Blum during the senior years of my PhD. I have learned a lot from our discussions at the intersection of artificial intelligence, consciousness, and neuroscience, which have changed how I look at long-term problems and approach them. Manuel and Lenore have also inspired me to think big and make broad impact across CS and beyond, giving me many opportunities to communicate my ideas and contributions to a wide audience in neuroscience, psychology, and more. Finally, Trevor Darrell has been a source of inspiration as a senior faculty and has given me great advice for my PhD research and broadly for my research career. Some of his early works in multimodal machine learning and multimodal interaction are still some of my favorite works in this space.
Beyond my committee members, I would like to acknowledge other CMU faculty and students with whom I’ve had fruitful collaborations, discussions, and received helpful feedback on ideas, paper drafts, and presentations. Fantastic students in LP and Russ’s research groups: Hubert Tsai, Amir Zadeh, Chaitanya Ahuja, Volkan Cirik, Torsten Wortwein, Martin Ma, Alex Wilf, Leena Mathur, Victoria Lin, Yousouf Kebe, Devendra Chaplot, Bhuwan Dhingra, Lisa Lee, Shrimai Prabhumoye, Ben Eysenbach, Jing Yu Koh, Minji Yoon, Brandon Trabucco, Murtaza Dalal, Yue Wu, and Kelly He. In addition, Hai Pham, Shaojie Bai, and their advisors Barnabas Poczos, and Zico Kolter with whom I did some early work in multimodal representation learning. Yonatan Bisk, Daniel Fried, Albert Gu, Zack Lipton, Tom Mitchell, Graham Neubig, Mayank Goel, and Haiyi Zhu who have given me a lot of advice (both personal and professional) over the years. Roni Rosenfeld, Ryan Tibshirani, and Tai Sing Lee who mentored me on undergraduate research projects at CMU. Finally, CMU Machine Learning Department and Language Technologies Institute are some of the best places to do AI research, and this could not be possible without the fantastic support from staff like Diane Stidle, Dorothy Holland-Minkley, and John Friday.
Some folks outside CMU I would like to thank include: Faisal Mahmood’s group at Harvard Medical School, especially students Richard Chen, Guillaume Jaume, and Anurag Vadiya for a series of fruitful collaborations regarding multimodal computational pathology; David Brent at UPMC, Nicholas Allen at University of Oregon, and Randy Auerbach at Columbia University for collaborations on daily mood assessment, markers of suicide ideation, and mobile health; Liangqiong Qu, Yuyin Zhou, Daniel Rubin, and James Zou at Stanford for investigations into multimodal and federated learning for biomedical applications; and most recently Jack Hessel, Yejin Choi, and Jae Sung Park at University of Washington/AI2 for many discussions regarding research and projects on vision-language commonsense reasoning.
I was also lucky to be mentored by several fantastic researchers in industry labs during my internships. To Manzil Zaheer at Google, you have made me a more mature researcher by reminding me to focus deeply on problems rather than jumping around during my junior researcher days. Our close collaborations have also strengthened my expertise in both fundamental and practical machine learning. To Yuke Zhu, Anima Anandkumar, and Sanja Fidler at Nvidia, you have done a great job setting up a vibrant and flexible research environment at Nvidia and I have learned a lot about the latest progress in multisensor robotics, AI for science, and vision-language models from our collaborations. To Makoto Yamada and Qibin Zhao at Riken AIP, where I learned more about tensors and kernels for multimodal learning. To Brandon Amos, Tim Rocktäschel, and Ed Grefenstette at Facebook AI, where I learned a lot about optimization, control, and reinforcement learning. And finally, to Dani Yogatama, Lisa Anne Hendricks and Aida Nematzadeh at DeepMind, where I gained practical experience training large-scale multimodal foundation models.
The most personally rewarding part of my PhD was definitely the many undergraduate, masters, and PhD students I have had the pleasure of advising - both at CMU and around the world: Adejuwon Fasanya, Akshay Goindani, Aviv Bick, Arav Agarwal, Chengfeng Mao, Chiyu Wu, Dong Won Lee, Edmund Tong, Gunjan Chhablani, Haofei Yu, Haoli Yin, Holmes Wu, Irene Li, Jiewen Hu, Jingyi Zhang, Jivat Neet, Katrina Jiao, Marian Qian, Peter Wu, Rana Shahroz, Richard Zhu, Rohan Pandey, Rulin Shao, Samuael Adnew, Samuel Yu, Seong Hyeon Park, Shentong Mo, Siyuan Wu, Talha Chafekar, Terrance Liu, Xiang Fan, Xiangru Tang, Yao Chong Lim, Ying Shen, Yiwei Lyu, Yudong Liu, Yun Cheng, Yuxin Xiao, Zhun Liu, Zihao Deng, Ziyin Liu. All of you have taught me so much and become experts in your own fields. I’m delighted to see all of you make great strides in PhD studies and industry, and look forward to hearing about your successes in the future.
Finally, I could not have done all this without the close support of my family and friends, especially from my mom, dad, sister, and grandparents, Jane, Truffle, and Tigger, Jane’s family, close friends Chun Kai Ling, Yue Niu, Raahul Sriram, Dylan Sam, Rattana Pukdee, Jennifer Hsia, Clara Na, Cindy Wu, Pratyush Maini, Ananye Agarwal, Yiding Jiang, Sam Sokota, Alex Wilf, Leena Mathur, Yiwei Lyu, Chirag Gupta, Tom Yan, Helen Zhou, Manzil Zaheer, and many more.

</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>

<div class="ltx_document"><div class="ltx_para"><div class="ltx_p"><span class="ltx_ERROR">
Conversion to HTML had a Fatal error and exited abruptly. This document may be truncated or damaged.
</span></div></div></div>
</article>
<div class="ar5iv-footer"><a href="/html/2404.18974" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.18976" class="ar5iv-text-button ar5iv-severity-fatal">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.18976">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.18976" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.18977" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 21:25:52 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
