<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.06702] What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions</title><meta property="og:description" content="There is increasing interest in the use of the LEArnable Front-end (LEAF) in a variety of speech processing systems. However, there is a dearth of analyses of what is actually learnt and the relative importance of traiâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.06702">

<!--Generated on Sun May  5 19:15:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">Hanyu Meng, Vidhyasaharan Sethu, Eliathamby Ambikairajah


</p>
</div>
<h1 class="ltx_title ltx_title_document">What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">There is increasing interest in the use of the LEArnable Front-end (LEAF) in a variety of speech processing systems. However, there is a dearth of analyses of what is actually learnt and the relative importance of training the different components of the front-end. In this paper, we investigate this question on keyword spotting, speech-based emotion recognition and language identification tasks and find that the filters for spectral decomposition and the low pass filter used to estimate spectral energy variations exhibit no learning and the per-channel energy normalisation (PCEN) is the key component that is learnt. Following this, we explore the potential of adapting only the PCEN layer with a small amount of noisy data to enable it to learn appropriate dynamic range compression that better suits the noise conditions. This in turn enables a system trained on clean speech to work more accurately on noisy test data as demonstrated by the experimental results reported in this paper.</p>
</div>
<div id="p2" class="ltx_para ltx_noindent">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text ltx_font_bold">Index Terms</span>: learnable audio front-end, adaptive front-end, pre-channel energy normalization, speech signal classification</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The speech front-end is a crucial component in speech signal classification systems, and has been the focus of research for many decades. The Mel filterbankÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> leads to representations inspired by our understanding of human perception, and is arguably the most widely used front-end across a range of different speech applications. With the advent of deep learning systems, there has been an interest in end-to-end systems that attempt to learn the optimal transformations to extract information from speech waveforms for any target applicationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. More recently, focus has shifted to learnable front-ends that constraints the architecture of the front-end but allows the model parameters to be learnt in conjunction with the back-endÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. They have been employed in a wide range of applications, including but not limited to speaker verificationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, spoofing detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, and emotion recognitionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">A number of general-purposed learnable front-ends such as Time-Domain Filterbank (TD-Fbank)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, SincNetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, CGCNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and LEArnable Front-end (LEAF)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> have been developed recently. Among these, LEAF stands out as having fewer parameters and higher reported accuracy in tasks such as audio events classification. Its universal representation generated from raw speech signals has also made it applicable in other audio-related tasks, such as medical acoustic signal feature learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, analog acoustic recognitionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, bird activity detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, speaker verificationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, and limited-vocabulary speech recognition tasksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Despite these successes, little is known about what is exactly learnt by LEAF from speech signals.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Analyses of the LEAF model have suggested that one of its components, Per-Channel Energy Normalization (PCEN) plays an important role in effectively compensating for the impact of environmental noise on speech intelligibilityÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. PCEN has also been widely applied in acoustic scene classificationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and long-distance bioacoustic event detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. However, beyond this, there have been limited analyses and insights in the operation of LEAF. To address this shortcoming, we investigate which components of LEAF learn during model training and to what extent.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we demonstrate that only the PCEN layer of the broader LEAF model learns during training and there is no observable change in the characteristics of any of the other components away from their initial values. Following this, we leverage our findings to develop a noise adaptation strategy whereby only the PCEN layer of LEAF is adapted using a small amount of noisy data to enable the LEAF model to be used under noisy conditions.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>What is learnt by LEAF?</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>LEAF</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.5" class="ltx_p">LEAF is a general-purpose audio front-end designed for audio event classificationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. It mainly comprises three learnable and one non-learnable elements: Spectral Decomposition, Energy Estimation (non-learnable), Smoothing, and Dynamic Range Compression. As depicted in FigureÂ <a href="#S2.F1" title="Figure 1 â€£ 2.1 LEAF â€£ 2 What is learnt by LEAF? â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, a frame of speech with <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">M</annotation></semantics></math> samples passes through a parallel filterbank of <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">N</annotation></semantics></math> Gabor filtersÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, which is initialised to be equally spaced along the mel-frequency scale. During training, both the centre frequencies <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="(f_{i})" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mrow id="S2.SS1.p1.3.m3.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p1.3.m3.1.1.1.2" xref="S2.SS1.p1.3.m3.1.1.1.1.cmml">(</mo><msub id="S2.SS1.p1.3.m3.1.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.1.cmml"><mi id="S2.SS1.p1.3.m3.1.1.1.1.2" xref="S2.SS1.p1.3.m3.1.1.1.1.2.cmml">f</mi><mi id="S2.SS1.p1.3.m3.1.1.1.1.3" xref="S2.SS1.p1.3.m3.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.SS1.p1.3.m3.1.1.1.3" xref="S2.SS1.p1.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.3.m3.1.1.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.1.1.2">ğ‘“</ci><ci id="S2.SS1.p1.3.m3.1.1.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">(f_{i})</annotation></semantics></math> and bandwidths <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="(BW_{i})" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mrow id="S2.SS1.p1.4.m4.1.1.1" xref="S2.SS1.p1.4.m4.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p1.4.m4.1.1.1.2" xref="S2.SS1.p1.4.m4.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.p1.4.m4.1.1.1.1" xref="S2.SS1.p1.4.m4.1.1.1.1.cmml"><mi id="S2.SS1.p1.4.m4.1.1.1.1.2" xref="S2.SS1.p1.4.m4.1.1.1.1.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.4.m4.1.1.1.1.1" xref="S2.SS1.p1.4.m4.1.1.1.1.1.cmml">â€‹</mo><msub id="S2.SS1.p1.4.m4.1.1.1.1.3" xref="S2.SS1.p1.4.m4.1.1.1.1.3.cmml"><mi id="S2.SS1.p1.4.m4.1.1.1.1.3.2" xref="S2.SS1.p1.4.m4.1.1.1.1.3.2.cmml">W</mi><mi id="S2.SS1.p1.4.m4.1.1.1.1.3.3" xref="S2.SS1.p1.4.m4.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S2.SS1.p1.4.m4.1.1.1.3" xref="S2.SS1.p1.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1.1"><times id="S2.SS1.p1.4.m4.1.1.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1.1.1.1"></times><ci id="S2.SS1.p1.4.m4.1.1.1.1.2.cmml" xref="S2.SS1.p1.4.m4.1.1.1.1.2">ğµ</ci><apply id="S2.SS1.p1.4.m4.1.1.1.1.3.cmml" xref="S2.SS1.p1.4.m4.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.1.1.3.1.cmml" xref="S2.SS1.p1.4.m4.1.1.1.1.3">subscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.1.1.3.2.cmml" xref="S2.SS1.p1.4.m4.1.1.1.1.3.2">ğ‘Š</ci><ci id="S2.SS1.p1.4.m4.1.1.1.1.3.3.cmml" xref="S2.SS1.p1.4.m4.1.1.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">(BW_{i})</annotation></semantics></math> of all filters can be learnt. The filterbank is followed by energy estimation implemented by a sample-wise squaring operation, which in turn is smoothed using a low pass filter. The low pass filtering is implemented as a pooling operation comprising of a Gaussian Low-pass Filters (LPF), an approach that has been shown to be effective with 2D featuresÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. The learnable parameter is the standard deviation <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="\sigma_{i}" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><msub id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.p1.5.m5.1.1.2" xref="S2.SS1.p1.5.m5.1.1.2.cmml">Ïƒ</mi><mi id="S2.SS1.p1.5.m5.1.1.3" xref="S2.SS1.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><apply id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.p1.5.m5.1.1.2">ğœ</ci><ci id="S2.SS1.p1.5.m5.1.1.3.cmml" xref="S2.SS1.p1.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">\sigma_{i}</annotation></semantics></math> (equivalently its bandwidth) of the Gaussian LPF in each frequency channel.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.2" class="ltx_p">Finally, the per channel energy compression (PCEN) acts as an input energy level-dependent gain controller that computes appropriate gains to enhance or attenuate signals in each frequency channelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. The PCEN layer for the <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><msup id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">i</mi><mrow id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml"><mi id="S2.SS1.p2.1.m1.1.1.3.2" xref="S2.SS1.p2.1.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.1.m1.1.1.3.1" xref="S2.SS1.p2.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS1.p2.1.m1.1.1.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">superscript</csymbol><ci id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">ğ‘–</ci><apply id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3"><times id="S2.SS1.p2.1.m1.1.1.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3.1"></times><ci id="S2.SS1.p2.1.m1.1.1.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.2">ğ‘¡</ci><ci id="S2.SS1.p2.1.m1.1.1.3.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">i^{th}</annotation></semantics></math> frequency channel takes as input the smoothed energy estimates produced by the low pass filter layer, <math id="S2.SS1.p2.2.m2.2" class="ltx_Math" alttext="E[n,i]" display="inline"><semantics id="S2.SS1.p2.2.m2.2a"><mrow id="S2.SS1.p2.2.m2.2.3" xref="S2.SS1.p2.2.m2.2.3.cmml"><mi id="S2.SS1.p2.2.m2.2.3.2" xref="S2.SS1.p2.2.m2.2.3.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.2.m2.2.3.1" xref="S2.SS1.p2.2.m2.2.3.1.cmml">â€‹</mo><mrow id="S2.SS1.p2.2.m2.2.3.3.2" xref="S2.SS1.p2.2.m2.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS1.p2.2.m2.2.3.3.2.1" xref="S2.SS1.p2.2.m2.2.3.3.1.cmml">[</mo><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">n</mi><mo id="S2.SS1.p2.2.m2.2.3.3.2.2" xref="S2.SS1.p2.2.m2.2.3.3.1.cmml">,</mo><mi id="S2.SS1.p2.2.m2.2.2" xref="S2.SS1.p2.2.m2.2.2.cmml">i</mi><mo stretchy="false" id="S2.SS1.p2.2.m2.2.3.3.2.3" xref="S2.SS1.p2.2.m2.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.2b"><apply id="S2.SS1.p2.2.m2.2.3.cmml" xref="S2.SS1.p2.2.m2.2.3"><times id="S2.SS1.p2.2.m2.2.3.1.cmml" xref="S2.SS1.p2.2.m2.2.3.1"></times><ci id="S2.SS1.p2.2.m2.2.3.2.cmml" xref="S2.SS1.p2.2.m2.2.3.2">ğ¸</ci><interval closure="closed" id="S2.SS1.p2.2.m2.2.3.3.1.cmml" xref="S2.SS1.p2.2.m2.2.3.3.2"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">ğ‘›</ci><ci id="S2.SS1.p2.2.m2.2.2.cmml" xref="S2.SS1.p2.2.m2.2.2">ğ‘–</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.2c">E[n,i]</annotation></semantics></math>, and is formulated as:</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.8" class="ltx_Math" alttext="PCEN[n,i]=\left(\frac{E[n,i]}{\left(M[n,i]+\epsilon\right)^{\alpha_{i}}}+\delta_{i}\right)^{\gamma_{i}}-\delta_{i}^{\gamma_{i}}," display="block"><semantics id="S2.E1.m1.8a"><mrow id="S2.E1.m1.8.8.1" xref="S2.E1.m1.8.8.1.1.cmml"><mrow id="S2.E1.m1.8.8.1.1" xref="S2.E1.m1.8.8.1.1.cmml"><mrow id="S2.E1.m1.8.8.1.1.3" xref="S2.E1.m1.8.8.1.1.3.cmml"><mi id="S2.E1.m1.8.8.1.1.3.2" xref="S2.E1.m1.8.8.1.1.3.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.8.8.1.1.3.1" xref="S2.E1.m1.8.8.1.1.3.1.cmml">â€‹</mo><mi id="S2.E1.m1.8.8.1.1.3.3" xref="S2.E1.m1.8.8.1.1.3.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.8.8.1.1.3.1a" xref="S2.E1.m1.8.8.1.1.3.1.cmml">â€‹</mo><mi id="S2.E1.m1.8.8.1.1.3.4" xref="S2.E1.m1.8.8.1.1.3.4.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.8.8.1.1.3.1b" xref="S2.E1.m1.8.8.1.1.3.1.cmml">â€‹</mo><mi id="S2.E1.m1.8.8.1.1.3.5" xref="S2.E1.m1.8.8.1.1.3.5.cmml">N</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.8.8.1.1.3.1c" xref="S2.E1.m1.8.8.1.1.3.1.cmml">â€‹</mo><mrow id="S2.E1.m1.8.8.1.1.3.6.2" xref="S2.E1.m1.8.8.1.1.3.6.1.cmml"><mo stretchy="false" id="S2.E1.m1.8.8.1.1.3.6.2.1" xref="S2.E1.m1.8.8.1.1.3.6.1.cmml">[</mo><mi id="S2.E1.m1.6.6" xref="S2.E1.m1.6.6.cmml">n</mi><mo id="S2.E1.m1.8.8.1.1.3.6.2.2" xref="S2.E1.m1.8.8.1.1.3.6.1.cmml">,</mo><mi id="S2.E1.m1.7.7" xref="S2.E1.m1.7.7.cmml">i</mi><mo stretchy="false" id="S2.E1.m1.8.8.1.1.3.6.2.3" xref="S2.E1.m1.8.8.1.1.3.6.1.cmml">]</mo></mrow></mrow><mo id="S2.E1.m1.8.8.1.1.2" xref="S2.E1.m1.8.8.1.1.2.cmml">=</mo><mrow id="S2.E1.m1.8.8.1.1.1" xref="S2.E1.m1.8.8.1.1.1.cmml"><msup id="S2.E1.m1.8.8.1.1.1.1" xref="S2.E1.m1.8.8.1.1.1.1.cmml"><mrow id="S2.E1.m1.8.8.1.1.1.1.1.1" xref="S2.E1.m1.8.8.1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.8.8.1.1.1.1.1.1.2" xref="S2.E1.m1.8.8.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.8.8.1.1.1.1.1.1.1" xref="S2.E1.m1.8.8.1.1.1.1.1.1.1.cmml"><mfrac id="S2.E1.m1.5.5" xref="S2.E1.m1.5.5.cmml"><mrow id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.2.cmml"><mi id="S2.E1.m1.2.2.2.4" xref="S2.E1.m1.2.2.2.4.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.3" xref="S2.E1.m1.2.2.2.3.cmml">â€‹</mo><mrow id="S2.E1.m1.2.2.2.5.2" xref="S2.E1.m1.2.2.2.5.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.2.5.2.1" xref="S2.E1.m1.2.2.2.5.1.cmml">[</mo><mi id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml">n</mi><mo id="S2.E1.m1.2.2.2.5.2.2" xref="S2.E1.m1.2.2.2.5.1.cmml">,</mo><mi id="S2.E1.m1.2.2.2.2" xref="S2.E1.m1.2.2.2.2.cmml">i</mi><mo stretchy="false" id="S2.E1.m1.2.2.2.5.2.3" xref="S2.E1.m1.2.2.2.5.1.cmml">]</mo></mrow></mrow><msup id="S2.E1.m1.5.5.5" xref="S2.E1.m1.5.5.5.cmml"><mrow id="S2.E1.m1.5.5.5.3.1" xref="S2.E1.m1.5.5.5.3.1.1.cmml"><mo id="S2.E1.m1.5.5.5.3.1.2" xref="S2.E1.m1.5.5.5.3.1.1.cmml">(</mo><mrow id="S2.E1.m1.5.5.5.3.1.1" xref="S2.E1.m1.5.5.5.3.1.1.cmml"><mrow id="S2.E1.m1.5.5.5.3.1.1.2" xref="S2.E1.m1.5.5.5.3.1.1.2.cmml"><mi id="S2.E1.m1.5.5.5.3.1.1.2.2" xref="S2.E1.m1.5.5.5.3.1.1.2.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.5.5.5.3.1.1.2.1" xref="S2.E1.m1.5.5.5.3.1.1.2.1.cmml">â€‹</mo><mrow id="S2.E1.m1.5.5.5.3.1.1.2.3.2" xref="S2.E1.m1.5.5.5.3.1.1.2.3.1.cmml"><mo stretchy="false" id="S2.E1.m1.5.5.5.3.1.1.2.3.2.1" xref="S2.E1.m1.5.5.5.3.1.1.2.3.1.cmml">[</mo><mi id="S2.E1.m1.3.3.3.1" xref="S2.E1.m1.3.3.3.1.cmml">n</mi><mo id="S2.E1.m1.5.5.5.3.1.1.2.3.2.2" xref="S2.E1.m1.5.5.5.3.1.1.2.3.1.cmml">,</mo><mi id="S2.E1.m1.4.4.4.2" xref="S2.E1.m1.4.4.4.2.cmml">i</mi><mo stretchy="false" id="S2.E1.m1.5.5.5.3.1.1.2.3.2.3" xref="S2.E1.m1.5.5.5.3.1.1.2.3.1.cmml">]</mo></mrow></mrow><mo id="S2.E1.m1.5.5.5.3.1.1.1" xref="S2.E1.m1.5.5.5.3.1.1.1.cmml">+</mo><mi id="S2.E1.m1.5.5.5.3.1.1.3" xref="S2.E1.m1.5.5.5.3.1.1.3.cmml">Ïµ</mi></mrow><mo id="S2.E1.m1.5.5.5.3.1.3" xref="S2.E1.m1.5.5.5.3.1.1.cmml">)</mo></mrow><msub id="S2.E1.m1.5.5.5.5" xref="S2.E1.m1.5.5.5.5.cmml"><mi id="S2.E1.m1.5.5.5.5.2" xref="S2.E1.m1.5.5.5.5.2.cmml">Î±</mi><mi id="S2.E1.m1.5.5.5.5.3" xref="S2.E1.m1.5.5.5.5.3.cmml">i</mi></msub></msup></mfrac><mo id="S2.E1.m1.8.8.1.1.1.1.1.1.1.1" xref="S2.E1.m1.8.8.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S2.E1.m1.8.8.1.1.1.1.1.1.1.2" xref="S2.E1.m1.8.8.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E1.m1.8.8.1.1.1.1.1.1.1.2.2" xref="S2.E1.m1.8.8.1.1.1.1.1.1.1.2.2.cmml">Î´</mi><mi id="S2.E1.m1.8.8.1.1.1.1.1.1.1.2.3" xref="S2.E1.m1.8.8.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub></mrow><mo id="S2.E1.m1.8.8.1.1.1.1.1.1.3" xref="S2.E1.m1.8.8.1.1.1.1.1.1.1.cmml">)</mo></mrow><msub id="S2.E1.m1.8.8.1.1.1.1.3" xref="S2.E1.m1.8.8.1.1.1.1.3.cmml"><mi id="S2.E1.m1.8.8.1.1.1.1.3.2" xref="S2.E1.m1.8.8.1.1.1.1.3.2.cmml">Î³</mi><mi id="S2.E1.m1.8.8.1.1.1.1.3.3" xref="S2.E1.m1.8.8.1.1.1.1.3.3.cmml">i</mi></msub></msup><mo id="S2.E1.m1.8.8.1.1.1.2" xref="S2.E1.m1.8.8.1.1.1.2.cmml">âˆ’</mo><msubsup id="S2.E1.m1.8.8.1.1.1.3" xref="S2.E1.m1.8.8.1.1.1.3.cmml"><mi id="S2.E1.m1.8.8.1.1.1.3.2.2" xref="S2.E1.m1.8.8.1.1.1.3.2.2.cmml">Î´</mi><mi id="S2.E1.m1.8.8.1.1.1.3.2.3" xref="S2.E1.m1.8.8.1.1.1.3.2.3.cmml">i</mi><msub id="S2.E1.m1.8.8.1.1.1.3.3" xref="S2.E1.m1.8.8.1.1.1.3.3.cmml"><mi id="S2.E1.m1.8.8.1.1.1.3.3.2" xref="S2.E1.m1.8.8.1.1.1.3.3.2.cmml">Î³</mi><mi id="S2.E1.m1.8.8.1.1.1.3.3.3" xref="S2.E1.m1.8.8.1.1.1.3.3.3.cmml">i</mi></msub></msubsup></mrow></mrow><mo id="S2.E1.m1.8.8.1.2" xref="S2.E1.m1.8.8.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.8b"><apply id="S2.E1.m1.8.8.1.1.cmml" xref="S2.E1.m1.8.8.1"><eq id="S2.E1.m1.8.8.1.1.2.cmml" xref="S2.E1.m1.8.8.1.1.2"></eq><apply id="S2.E1.m1.8.8.1.1.3.cmml" xref="S2.E1.m1.8.8.1.1.3"><times id="S2.E1.m1.8.8.1.1.3.1.cmml" xref="S2.E1.m1.8.8.1.1.3.1"></times><ci id="S2.E1.m1.8.8.1.1.3.2.cmml" xref="S2.E1.m1.8.8.1.1.3.2">ğ‘ƒ</ci><ci id="S2.E1.m1.8.8.1.1.3.3.cmml" xref="S2.E1.m1.8.8.1.1.3.3">ğ¶</ci><ci id="S2.E1.m1.8.8.1.1.3.4.cmml" xref="S2.E1.m1.8.8.1.1.3.4">ğ¸</ci><ci id="S2.E1.m1.8.8.1.1.3.5.cmml" xref="S2.E1.m1.8.8.1.1.3.5">ğ‘</ci><interval closure="closed" id="S2.E1.m1.8.8.1.1.3.6.1.cmml" xref="S2.E1.m1.8.8.1.1.3.6.2"><ci id="S2.E1.m1.6.6.cmml" xref="S2.E1.m1.6.6">ğ‘›</ci><ci id="S2.E1.m1.7.7.cmml" xref="S2.E1.m1.7.7">ğ‘–</ci></interval></apply><apply id="S2.E1.m1.8.8.1.1.1.cmml" xref="S2.E1.m1.8.8.1.1.1"><minus id="S2.E1.m1.8.8.1.1.1.2.cmml" xref="S2.E1.m1.8.8.1.1.1.2"></minus><apply id="S2.E1.m1.8.8.1.1.1.1.cmml" xref="S2.E1.m1.8.8.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.8.8.1.1.1.1.2.cmml" xref="S2.E1.m1.8.8.1.1.1.1">superscript</csymbol><apply id="S2.E1.m1.8.8.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.8.8.1.1.1.1.1.1"><plus id="S2.E1.m1.8.8.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.8.8.1.1.1.1.1.1.1.1"></plus><apply id="S2.E1.m1.5.5.cmml" xref="S2.E1.m1.5.5"><divide id="S2.E1.m1.5.5.6.cmml" xref="S2.E1.m1.5.5"></divide><apply id="S2.E1.m1.2.2.2.cmml" xref="S2.E1.m1.2.2.2"><times id="S2.E1.m1.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.3"></times><ci id="S2.E1.m1.2.2.2.4.cmml" xref="S2.E1.m1.2.2.2.4">ğ¸</ci><interval closure="closed" id="S2.E1.m1.2.2.2.5.1.cmml" xref="S2.E1.m1.2.2.2.5.2"><ci id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1">ğ‘›</ci><ci id="S2.E1.m1.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2">ğ‘–</ci></interval></apply><apply id="S2.E1.m1.5.5.5.cmml" xref="S2.E1.m1.5.5.5"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.5.4.cmml" xref="S2.E1.m1.5.5.5">superscript</csymbol><apply id="S2.E1.m1.5.5.5.3.1.1.cmml" xref="S2.E1.m1.5.5.5.3.1"><plus id="S2.E1.m1.5.5.5.3.1.1.1.cmml" xref="S2.E1.m1.5.5.5.3.1.1.1"></plus><apply id="S2.E1.m1.5.5.5.3.1.1.2.cmml" xref="S2.E1.m1.5.5.5.3.1.1.2"><times id="S2.E1.m1.5.5.5.3.1.1.2.1.cmml" xref="S2.E1.m1.5.5.5.3.1.1.2.1"></times><ci id="S2.E1.m1.5.5.5.3.1.1.2.2.cmml" xref="S2.E1.m1.5.5.5.3.1.1.2.2">ğ‘€</ci><interval closure="closed" id="S2.E1.m1.5.5.5.3.1.1.2.3.1.cmml" xref="S2.E1.m1.5.5.5.3.1.1.2.3.2"><ci id="S2.E1.m1.3.3.3.1.cmml" xref="S2.E1.m1.3.3.3.1">ğ‘›</ci><ci id="S2.E1.m1.4.4.4.2.cmml" xref="S2.E1.m1.4.4.4.2">ğ‘–</ci></interval></apply><ci id="S2.E1.m1.5.5.5.3.1.1.3.cmml" xref="S2.E1.m1.5.5.5.3.1.1.3">italic-Ïµ</ci></apply><apply id="S2.E1.m1.5.5.5.5.cmml" xref="S2.E1.m1.5.5.5.5"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.5.5.1.cmml" xref="S2.E1.m1.5.5.5.5">subscript</csymbol><ci id="S2.E1.m1.5.5.5.5.2.cmml" xref="S2.E1.m1.5.5.5.5.2">ğ›¼</ci><ci id="S2.E1.m1.5.5.5.5.3.cmml" xref="S2.E1.m1.5.5.5.5.3">ğ‘–</ci></apply></apply></apply><apply id="S2.E1.m1.8.8.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.8.8.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.8.8.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.8.8.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E1.m1.8.8.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.8.8.1.1.1.1.1.1.1.2.2">ğ›¿</ci><ci id="S2.E1.m1.8.8.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.8.8.1.1.1.1.1.1.1.2.3">ğ‘–</ci></apply></apply><apply id="S2.E1.m1.8.8.1.1.1.1.3.cmml" xref="S2.E1.m1.8.8.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.8.8.1.1.1.1.3.1.cmml" xref="S2.E1.m1.8.8.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.8.8.1.1.1.1.3.2.cmml" xref="S2.E1.m1.8.8.1.1.1.1.3.2">ğ›¾</ci><ci id="S2.E1.m1.8.8.1.1.1.1.3.3.cmml" xref="S2.E1.m1.8.8.1.1.1.1.3.3">ğ‘–</ci></apply></apply><apply id="S2.E1.m1.8.8.1.1.1.3.cmml" xref="S2.E1.m1.8.8.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.8.8.1.1.1.3.1.cmml" xref="S2.E1.m1.8.8.1.1.1.3">superscript</csymbol><apply id="S2.E1.m1.8.8.1.1.1.3.2.cmml" xref="S2.E1.m1.8.8.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.8.8.1.1.1.3.2.1.cmml" xref="S2.E1.m1.8.8.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.8.8.1.1.1.3.2.2.cmml" xref="S2.E1.m1.8.8.1.1.1.3.2.2">ğ›¿</ci><ci id="S2.E1.m1.8.8.1.1.1.3.2.3.cmml" xref="S2.E1.m1.8.8.1.1.1.3.2.3">ğ‘–</ci></apply><apply id="S2.E1.m1.8.8.1.1.1.3.3.cmml" xref="S2.E1.m1.8.8.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.8.8.1.1.1.3.3.1.cmml" xref="S2.E1.m1.8.8.1.1.1.3.3">subscript</csymbol><ci id="S2.E1.m1.8.8.1.1.1.3.3.2.cmml" xref="S2.E1.m1.8.8.1.1.1.3.3.2">ğ›¾</ci><ci id="S2.E1.m1.8.8.1.1.1.3.3.3.cmml" xref="S2.E1.m1.8.8.1.1.1.3.3.3">ğ‘–</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.8c">PCEN[n,i]=\left(\frac{E[n,i]}{\left(M[n,i]+\epsilon\right)^{\alpha_{i}}}+\delta_{i}\right)^{\gamma_{i}}-\delta_{i}^{\gamma_{i}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p3.2" class="ltx_p">where <math id="S2.SS1.p3.1.m1.2" class="ltx_Math" alttext="M[n,i]" display="inline"><semantics id="S2.SS1.p3.1.m1.2a"><mrow id="S2.SS1.p3.1.m1.2.3" xref="S2.SS1.p3.1.m1.2.3.cmml"><mi id="S2.SS1.p3.1.m1.2.3.2" xref="S2.SS1.p3.1.m1.2.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.1.m1.2.3.1" xref="S2.SS1.p3.1.m1.2.3.1.cmml">â€‹</mo><mrow id="S2.SS1.p3.1.m1.2.3.3.2" xref="S2.SS1.p3.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS1.p3.1.m1.2.3.3.2.1" xref="S2.SS1.p3.1.m1.2.3.3.1.cmml">[</mo><mi id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">n</mi><mo id="S2.SS1.p3.1.m1.2.3.3.2.2" xref="S2.SS1.p3.1.m1.2.3.3.1.cmml">,</mo><mi id="S2.SS1.p3.1.m1.2.2" xref="S2.SS1.p3.1.m1.2.2.cmml">i</mi><mo stretchy="false" id="S2.SS1.p3.1.m1.2.3.3.2.3" xref="S2.SS1.p3.1.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.2b"><apply id="S2.SS1.p3.1.m1.2.3.cmml" xref="S2.SS1.p3.1.m1.2.3"><times id="S2.SS1.p3.1.m1.2.3.1.cmml" xref="S2.SS1.p3.1.m1.2.3.1"></times><ci id="S2.SS1.p3.1.m1.2.3.2.cmml" xref="S2.SS1.p3.1.m1.2.3.2">ğ‘€</ci><interval closure="closed" id="S2.SS1.p3.1.m1.2.3.3.1.cmml" xref="S2.SS1.p3.1.m1.2.3.3.2"><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">ğ‘›</ci><ci id="S2.SS1.p3.1.m1.2.2.cmml" xref="S2.SS1.p3.1.m1.2.2">ğ‘–</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.2c">M[n,i]</annotation></semantics></math> is the smoothed version of the input representation achieved by a first-order infinite impulse response (IIR) filter as expressed in EquationÂ <a href="#S2.E2" title="In 2.1 LEAF â€£ 2 What is learnt by LEAF? â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> with a learnable smoothing factor, <math id="S2.SS1.p3.2.m2.1" class="ltx_Math" alttext="s_{i}" display="inline"><semantics id="S2.SS1.p3.2.m2.1a"><msub id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml"><mi id="S2.SS1.p3.2.m2.1.1.2" xref="S2.SS1.p3.2.m2.1.1.2.cmml">s</mi><mi id="S2.SS1.p3.2.m2.1.1.3" xref="S2.SS1.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.1b"><apply id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.1.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p3.2.m2.1.1.2.cmml" xref="S2.SS1.p3.2.m2.1.1.2">ğ‘ </ci><ci id="S2.SS1.p3.2.m2.1.1.3.cmml" xref="S2.SS1.p3.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.1c">s_{i}</annotation></semantics></math>.</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.6" class="ltx_Math" alttext="M[n,i]=s_{i}E[n,i]+(1-s_{i})M[n-1,i]." display="block"><semantics id="S2.E2.m1.6a"><mrow id="S2.E2.m1.6.6.1" xref="S2.E2.m1.6.6.1.1.cmml"><mrow id="S2.E2.m1.6.6.1.1" xref="S2.E2.m1.6.6.1.1.cmml"><mrow id="S2.E2.m1.6.6.1.1.4" xref="S2.E2.m1.6.6.1.1.4.cmml"><mi id="S2.E2.m1.6.6.1.1.4.2" xref="S2.E2.m1.6.6.1.1.4.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.6.6.1.1.4.1" xref="S2.E2.m1.6.6.1.1.4.1.cmml">â€‹</mo><mrow id="S2.E2.m1.6.6.1.1.4.3.2" xref="S2.E2.m1.6.6.1.1.4.3.1.cmml"><mo stretchy="false" id="S2.E2.m1.6.6.1.1.4.3.2.1" xref="S2.E2.m1.6.6.1.1.4.3.1.cmml">[</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">n</mi><mo id="S2.E2.m1.6.6.1.1.4.3.2.2" xref="S2.E2.m1.6.6.1.1.4.3.1.cmml">,</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">i</mi><mo stretchy="false" id="S2.E2.m1.6.6.1.1.4.3.2.3" xref="S2.E2.m1.6.6.1.1.4.3.1.cmml">]</mo></mrow></mrow><mo id="S2.E2.m1.6.6.1.1.3" xref="S2.E2.m1.6.6.1.1.3.cmml">=</mo><mrow id="S2.E2.m1.6.6.1.1.2" xref="S2.E2.m1.6.6.1.1.2.cmml"><mrow id="S2.E2.m1.6.6.1.1.2.4" xref="S2.E2.m1.6.6.1.1.2.4.cmml"><msub id="S2.E2.m1.6.6.1.1.2.4.2" xref="S2.E2.m1.6.6.1.1.2.4.2.cmml"><mi id="S2.E2.m1.6.6.1.1.2.4.2.2" xref="S2.E2.m1.6.6.1.1.2.4.2.2.cmml">s</mi><mi id="S2.E2.m1.6.6.1.1.2.4.2.3" xref="S2.E2.m1.6.6.1.1.2.4.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.6.6.1.1.2.4.1" xref="S2.E2.m1.6.6.1.1.2.4.1.cmml">â€‹</mo><mi id="S2.E2.m1.6.6.1.1.2.4.3" xref="S2.E2.m1.6.6.1.1.2.4.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.6.6.1.1.2.4.1a" xref="S2.E2.m1.6.6.1.1.2.4.1.cmml">â€‹</mo><mrow id="S2.E2.m1.6.6.1.1.2.4.4.2" xref="S2.E2.m1.6.6.1.1.2.4.4.1.cmml"><mo stretchy="false" id="S2.E2.m1.6.6.1.1.2.4.4.2.1" xref="S2.E2.m1.6.6.1.1.2.4.4.1.cmml">[</mo><mi id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml">n</mi><mo id="S2.E2.m1.6.6.1.1.2.4.4.2.2" xref="S2.E2.m1.6.6.1.1.2.4.4.1.cmml">,</mo><mi id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml">i</mi><mo stretchy="false" id="S2.E2.m1.6.6.1.1.2.4.4.2.3" xref="S2.E2.m1.6.6.1.1.2.4.4.1.cmml">]</mo></mrow></mrow><mo id="S2.E2.m1.6.6.1.1.2.3" xref="S2.E2.m1.6.6.1.1.2.3.cmml">+</mo><mrow id="S2.E2.m1.6.6.1.1.2.2" xref="S2.E2.m1.6.6.1.1.2.2.cmml"><mrow id="S2.E2.m1.6.6.1.1.1.1.1.1" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.6.6.1.1.1.1.1.1.2" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.6.6.1.1.1.1.1.1.1" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.cmml"><mn id="S2.E2.m1.6.6.1.1.1.1.1.1.1.2" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S2.E2.m1.6.6.1.1.1.1.1.1.1.1" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S2.E2.m1.6.6.1.1.1.1.1.1.1.3" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.6.6.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.3.2.cmml">s</mi><mi id="S2.E2.m1.6.6.1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S2.E2.m1.6.6.1.1.1.1.1.1.3" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E2.m1.6.6.1.1.2.2.3" xref="S2.E2.m1.6.6.1.1.2.2.3.cmml">â€‹</mo><mi id="S2.E2.m1.6.6.1.1.2.2.4" xref="S2.E2.m1.6.6.1.1.2.2.4.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.6.6.1.1.2.2.3a" xref="S2.E2.m1.6.6.1.1.2.2.3.cmml">â€‹</mo><mrow id="S2.E2.m1.6.6.1.1.2.2.2.1" xref="S2.E2.m1.6.6.1.1.2.2.2.2.cmml"><mo stretchy="false" id="S2.E2.m1.6.6.1.1.2.2.2.1.2" xref="S2.E2.m1.6.6.1.1.2.2.2.2.cmml">[</mo><mrow id="S2.E2.m1.6.6.1.1.2.2.2.1.1" xref="S2.E2.m1.6.6.1.1.2.2.2.1.1.cmml"><mi id="S2.E2.m1.6.6.1.1.2.2.2.1.1.2" xref="S2.E2.m1.6.6.1.1.2.2.2.1.1.2.cmml">n</mi><mo id="S2.E2.m1.6.6.1.1.2.2.2.1.1.1" xref="S2.E2.m1.6.6.1.1.2.2.2.1.1.1.cmml">âˆ’</mo><mn id="S2.E2.m1.6.6.1.1.2.2.2.1.1.3" xref="S2.E2.m1.6.6.1.1.2.2.2.1.1.3.cmml">1</mn></mrow><mo id="S2.E2.m1.6.6.1.1.2.2.2.1.3" xref="S2.E2.m1.6.6.1.1.2.2.2.2.cmml">,</mo><mi id="S2.E2.m1.5.5" xref="S2.E2.m1.5.5.cmml">i</mi><mo stretchy="false" id="S2.E2.m1.6.6.1.1.2.2.2.1.4" xref="S2.E2.m1.6.6.1.1.2.2.2.2.cmml">]</mo></mrow></mrow></mrow></mrow><mo lspace="0em" id="S2.E2.m1.6.6.1.2" xref="S2.E2.m1.6.6.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.6b"><apply id="S2.E2.m1.6.6.1.1.cmml" xref="S2.E2.m1.6.6.1"><eq id="S2.E2.m1.6.6.1.1.3.cmml" xref="S2.E2.m1.6.6.1.1.3"></eq><apply id="S2.E2.m1.6.6.1.1.4.cmml" xref="S2.E2.m1.6.6.1.1.4"><times id="S2.E2.m1.6.6.1.1.4.1.cmml" xref="S2.E2.m1.6.6.1.1.4.1"></times><ci id="S2.E2.m1.6.6.1.1.4.2.cmml" xref="S2.E2.m1.6.6.1.1.4.2">ğ‘€</ci><interval closure="closed" id="S2.E2.m1.6.6.1.1.4.3.1.cmml" xref="S2.E2.m1.6.6.1.1.4.3.2"><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">ğ‘›</ci><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">ğ‘–</ci></interval></apply><apply id="S2.E2.m1.6.6.1.1.2.cmml" xref="S2.E2.m1.6.6.1.1.2"><plus id="S2.E2.m1.6.6.1.1.2.3.cmml" xref="S2.E2.m1.6.6.1.1.2.3"></plus><apply id="S2.E2.m1.6.6.1.1.2.4.cmml" xref="S2.E2.m1.6.6.1.1.2.4"><times id="S2.E2.m1.6.6.1.1.2.4.1.cmml" xref="S2.E2.m1.6.6.1.1.2.4.1"></times><apply id="S2.E2.m1.6.6.1.1.2.4.2.cmml" xref="S2.E2.m1.6.6.1.1.2.4.2"><csymbol cd="ambiguous" id="S2.E2.m1.6.6.1.1.2.4.2.1.cmml" xref="S2.E2.m1.6.6.1.1.2.4.2">subscript</csymbol><ci id="S2.E2.m1.6.6.1.1.2.4.2.2.cmml" xref="S2.E2.m1.6.6.1.1.2.4.2.2">ğ‘ </ci><ci id="S2.E2.m1.6.6.1.1.2.4.2.3.cmml" xref="S2.E2.m1.6.6.1.1.2.4.2.3">ğ‘–</ci></apply><ci id="S2.E2.m1.6.6.1.1.2.4.3.cmml" xref="S2.E2.m1.6.6.1.1.2.4.3">ğ¸</ci><interval closure="closed" id="S2.E2.m1.6.6.1.1.2.4.4.1.cmml" xref="S2.E2.m1.6.6.1.1.2.4.4.2"><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">ğ‘›</ci><ci id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.4">ğ‘–</ci></interval></apply><apply id="S2.E2.m1.6.6.1.1.2.2.cmml" xref="S2.E2.m1.6.6.1.1.2.2"><times id="S2.E2.m1.6.6.1.1.2.2.3.cmml" xref="S2.E2.m1.6.6.1.1.2.2.3"></times><apply id="S2.E2.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.6.6.1.1.1.1.1.1"><minus id="S2.E2.m1.6.6.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S2.E2.m1.6.6.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.2">1</cn><apply id="S2.E2.m1.6.6.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.6.6.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.6.6.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.3.2">ğ‘ </ci><ci id="S2.E2.m1.6.6.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.6.6.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply><ci id="S2.E2.m1.6.6.1.1.2.2.4.cmml" xref="S2.E2.m1.6.6.1.1.2.2.4">ğ‘€</ci><interval closure="closed" id="S2.E2.m1.6.6.1.1.2.2.2.2.cmml" xref="S2.E2.m1.6.6.1.1.2.2.2.1"><apply id="S2.E2.m1.6.6.1.1.2.2.2.1.1.cmml" xref="S2.E2.m1.6.6.1.1.2.2.2.1.1"><minus id="S2.E2.m1.6.6.1.1.2.2.2.1.1.1.cmml" xref="S2.E2.m1.6.6.1.1.2.2.2.1.1.1"></minus><ci id="S2.E2.m1.6.6.1.1.2.2.2.1.1.2.cmml" xref="S2.E2.m1.6.6.1.1.2.2.2.1.1.2">ğ‘›</ci><cn type="integer" id="S2.E2.m1.6.6.1.1.2.2.2.1.1.3.cmml" xref="S2.E2.m1.6.6.1.1.2.2.2.1.1.3">1</cn></apply><ci id="S2.E2.m1.5.5.cmml" xref="S2.E2.m1.5.5">ğ‘–</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.6c">M[n,i]=s_{i}E[n,i]+(1-s_{i})M[n-1,i].</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p3.3" class="ltx_p">As illustrated in FigureÂ <a href="#S2.F1" title="Figure 1 â€£ 2.1 LEAF â€£ 2 What is learnt by LEAF? â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the PCEN layer has four learnable parameters <math id="S2.SS1.p3.3.m1.4" class="ltx_Math" alttext="(s_{i},\alpha_{i},\delta_{i},\gamma_{i})" display="inline"><semantics id="S2.SS1.p3.3.m1.4a"><mrow id="S2.SS1.p3.3.m1.4.4.4" xref="S2.SS1.p3.3.m1.4.4.5.cmml"><mo stretchy="false" id="S2.SS1.p3.3.m1.4.4.4.5" xref="S2.SS1.p3.3.m1.4.4.5.cmml">(</mo><msub id="S2.SS1.p3.3.m1.1.1.1.1" xref="S2.SS1.p3.3.m1.1.1.1.1.cmml"><mi id="S2.SS1.p3.3.m1.1.1.1.1.2" xref="S2.SS1.p3.3.m1.1.1.1.1.2.cmml">s</mi><mi id="S2.SS1.p3.3.m1.1.1.1.1.3" xref="S2.SS1.p3.3.m1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.SS1.p3.3.m1.4.4.4.6" xref="S2.SS1.p3.3.m1.4.4.5.cmml">,</mo><msub id="S2.SS1.p3.3.m1.2.2.2.2" xref="S2.SS1.p3.3.m1.2.2.2.2.cmml"><mi id="S2.SS1.p3.3.m1.2.2.2.2.2" xref="S2.SS1.p3.3.m1.2.2.2.2.2.cmml">Î±</mi><mi id="S2.SS1.p3.3.m1.2.2.2.2.3" xref="S2.SS1.p3.3.m1.2.2.2.2.3.cmml">i</mi></msub><mo id="S2.SS1.p3.3.m1.4.4.4.7" xref="S2.SS1.p3.3.m1.4.4.5.cmml">,</mo><msub id="S2.SS1.p3.3.m1.3.3.3.3" xref="S2.SS1.p3.3.m1.3.3.3.3.cmml"><mi id="S2.SS1.p3.3.m1.3.3.3.3.2" xref="S2.SS1.p3.3.m1.3.3.3.3.2.cmml">Î´</mi><mi id="S2.SS1.p3.3.m1.3.3.3.3.3" xref="S2.SS1.p3.3.m1.3.3.3.3.3.cmml">i</mi></msub><mo id="S2.SS1.p3.3.m1.4.4.4.8" xref="S2.SS1.p3.3.m1.4.4.5.cmml">,</mo><msub id="S2.SS1.p3.3.m1.4.4.4.4" xref="S2.SS1.p3.3.m1.4.4.4.4.cmml"><mi id="S2.SS1.p3.3.m1.4.4.4.4.2" xref="S2.SS1.p3.3.m1.4.4.4.4.2.cmml">Î³</mi><mi id="S2.SS1.p3.3.m1.4.4.4.4.3" xref="S2.SS1.p3.3.m1.4.4.4.4.3.cmml">i</mi></msub><mo stretchy="false" id="S2.SS1.p3.3.m1.4.4.4.9" xref="S2.SS1.p3.3.m1.4.4.5.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.m1.4b"><vector id="S2.SS1.p3.3.m1.4.4.5.cmml" xref="S2.SS1.p3.3.m1.4.4.4"><apply id="S2.SS1.p3.3.m1.1.1.1.1.cmml" xref="S2.SS1.p3.3.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.3.m1.1.1.1.1.1.cmml" xref="S2.SS1.p3.3.m1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p3.3.m1.1.1.1.1.2.cmml" xref="S2.SS1.p3.3.m1.1.1.1.1.2">ğ‘ </ci><ci id="S2.SS1.p3.3.m1.1.1.1.1.3.cmml" xref="S2.SS1.p3.3.m1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S2.SS1.p3.3.m1.2.2.2.2.cmml" xref="S2.SS1.p3.3.m1.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p3.3.m1.2.2.2.2.1.cmml" xref="S2.SS1.p3.3.m1.2.2.2.2">subscript</csymbol><ci id="S2.SS1.p3.3.m1.2.2.2.2.2.cmml" xref="S2.SS1.p3.3.m1.2.2.2.2.2">ğ›¼</ci><ci id="S2.SS1.p3.3.m1.2.2.2.2.3.cmml" xref="S2.SS1.p3.3.m1.2.2.2.2.3">ğ‘–</ci></apply><apply id="S2.SS1.p3.3.m1.3.3.3.3.cmml" xref="S2.SS1.p3.3.m1.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p3.3.m1.3.3.3.3.1.cmml" xref="S2.SS1.p3.3.m1.3.3.3.3">subscript</csymbol><ci id="S2.SS1.p3.3.m1.3.3.3.3.2.cmml" xref="S2.SS1.p3.3.m1.3.3.3.3.2">ğ›¿</ci><ci id="S2.SS1.p3.3.m1.3.3.3.3.3.cmml" xref="S2.SS1.p3.3.m1.3.3.3.3.3">ğ‘–</ci></apply><apply id="S2.SS1.p3.3.m1.4.4.4.4.cmml" xref="S2.SS1.p3.3.m1.4.4.4.4"><csymbol cd="ambiguous" id="S2.SS1.p3.3.m1.4.4.4.4.1.cmml" xref="S2.SS1.p3.3.m1.4.4.4.4">subscript</csymbol><ci id="S2.SS1.p3.3.m1.4.4.4.4.2.cmml" xref="S2.SS1.p3.3.m1.4.4.4.4.2">ğ›¾</ci><ci id="S2.SS1.p3.3.m1.4.4.4.4.3.cmml" xref="S2.SS1.p3.3.m1.4.4.4.4.3">ğ‘–</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.m1.4c">(s_{i},\alpha_{i},\delta_{i},\gamma_{i})</annotation></semantics></math> per frequency channel.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2404.06702/assets/Images/leaf_structure.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="374" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An overview of LEAF (the input speech frame contains <math id="S2.F1.2.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.F1.2.m1.1b"><mi id="S2.F1.2.m1.1.1" xref="S2.F1.2.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.F1.2.m1.1c"><ci id="S2.F1.2.m1.1.1.cmml" xref="S2.F1.2.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.2.m1.1d">M</annotation></semantics></math> samples).</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Analysis of LEAF Parameters: Experimental Setup</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">To investigate what is learnt by LEAF, we train LEAF on three of the tasks where its performance has been perviously reported: emotion recognition, keyword spotting, and language identificationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. The tasks were chosen to represent a range of speech processing applications as well as prediction accuracies ranging from quite high to somewhat low (refer Table Â <a href="#S2.T1" title="Table 1 â€£ 2.2 Analysis of LEAF Parameters: Experimental Setup â€£ 2 What is learnt by LEAF? â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Following this, we compare the spectral and compression characteristics of the three learnable components of LEAF before and after training.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">For these analyses, we replicate the dataset settings reported inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. The keyword spotting task is trained on SpeechCommandsV2 datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, emotion recognition experiments were carried out on CREMA-D datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, and we use VoxforgeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> for language identification. All three datasets are sampled at 16kHz. For all three tasks, we utilise EfficentNetB0Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> as the back-end, which is a lightweight Convolution Neural Network (CNN) based classification network.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.4" class="ltx_p">Prior to training, the LEAF model was initialised as described inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Specifically, we initialised the 40 Gabor filters using the mel-scale and set the input window size to 401 samples with a hop size of 160 samples, corresponding to 25ms for audio sampled at 16kHz. The Gaussian LPFs were initialised with a standard deviation of 0.4 for all channels. The initial values of the PCEN layer parameters in each frequency channel were set to <math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="\alpha=0.96" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><mrow id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml"><mi id="S2.SS2.p3.1.m1.1.1.2" xref="S2.SS2.p3.1.m1.1.1.2.cmml">Î±</mi><mo id="S2.SS2.p3.1.m1.1.1.1" xref="S2.SS2.p3.1.m1.1.1.1.cmml">=</mo><mn id="S2.SS2.p3.1.m1.1.1.3" xref="S2.SS2.p3.1.m1.1.1.3.cmml">0.96</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><apply id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1"><eq id="S2.SS2.p3.1.m1.1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1.1"></eq><ci id="S2.SS2.p3.1.m1.1.1.2.cmml" xref="S2.SS2.p3.1.m1.1.1.2">ğ›¼</ci><cn type="float" id="S2.SS2.p3.1.m1.1.1.3.cmml" xref="S2.SS2.p3.1.m1.1.1.3">0.96</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">\alpha=0.96</annotation></semantics></math>, <math id="S2.SS2.p3.2.m2.1" class="ltx_Math" alttext="\gamma=2" display="inline"><semantics id="S2.SS2.p3.2.m2.1a"><mrow id="S2.SS2.p3.2.m2.1.1" xref="S2.SS2.p3.2.m2.1.1.cmml"><mi id="S2.SS2.p3.2.m2.1.1.2" xref="S2.SS2.p3.2.m2.1.1.2.cmml">Î³</mi><mo id="S2.SS2.p3.2.m2.1.1.1" xref="S2.SS2.p3.2.m2.1.1.1.cmml">=</mo><mn id="S2.SS2.p3.2.m2.1.1.3" xref="S2.SS2.p3.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m2.1b"><apply id="S2.SS2.p3.2.m2.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1"><eq id="S2.SS2.p3.2.m2.1.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1.1"></eq><ci id="S2.SS2.p3.2.m2.1.1.2.cmml" xref="S2.SS2.p3.2.m2.1.1.2">ğ›¾</ci><cn type="integer" id="S2.SS2.p3.2.m2.1.1.3.cmml" xref="S2.SS2.p3.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m2.1c">\gamma=2</annotation></semantics></math>, <math id="S2.SS2.p3.3.m3.1" class="ltx_Math" alttext="\delta=2" display="inline"><semantics id="S2.SS2.p3.3.m3.1a"><mrow id="S2.SS2.p3.3.m3.1.1" xref="S2.SS2.p3.3.m3.1.1.cmml"><mi id="S2.SS2.p3.3.m3.1.1.2" xref="S2.SS2.p3.3.m3.1.1.2.cmml">Î´</mi><mo id="S2.SS2.p3.3.m3.1.1.1" xref="S2.SS2.p3.3.m3.1.1.1.cmml">=</mo><mn id="S2.SS2.p3.3.m3.1.1.3" xref="S2.SS2.p3.3.m3.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.3.m3.1b"><apply id="S2.SS2.p3.3.m3.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1"><eq id="S2.SS2.p3.3.m3.1.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1.1"></eq><ci id="S2.SS2.p3.3.m3.1.1.2.cmml" xref="S2.SS2.p3.3.m3.1.1.2">ğ›¿</ci><cn type="integer" id="S2.SS2.p3.3.m3.1.1.3.cmml" xref="S2.SS2.p3.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.3.m3.1c">\delta=2</annotation></semantics></math>, and <math id="S2.SS2.p3.4.m4.1" class="ltx_Math" alttext="s=0.04" display="inline"><semantics id="S2.SS2.p3.4.m4.1a"><mrow id="S2.SS2.p3.4.m4.1.1" xref="S2.SS2.p3.4.m4.1.1.cmml"><mi id="S2.SS2.p3.4.m4.1.1.2" xref="S2.SS2.p3.4.m4.1.1.2.cmml">s</mi><mo id="S2.SS2.p3.4.m4.1.1.1" xref="S2.SS2.p3.4.m4.1.1.1.cmml">=</mo><mn id="S2.SS2.p3.4.m4.1.1.3" xref="S2.SS2.p3.4.m4.1.1.3.cmml">0.04</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.4.m4.1b"><apply id="S2.SS2.p3.4.m4.1.1.cmml" xref="S2.SS2.p3.4.m4.1.1"><eq id="S2.SS2.p3.4.m4.1.1.1.cmml" xref="S2.SS2.p3.4.m4.1.1.1"></eq><ci id="S2.SS2.p3.4.m4.1.1.2.cmml" xref="S2.SS2.p3.4.m4.1.1.2">ğ‘ </ci><cn type="float" id="S2.SS2.p3.4.m4.1.1.3.cmml" xref="S2.SS2.p3.4.m4.1.1.3">0.04</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.4.m4.1c">s=0.04</annotation></semantics></math>.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">For training the models, we utilised the ADAM optimiser with a fixed learning rate of <math id="S2.SS2.p4.1.m1.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S2.SS2.p4.1.m1.1a"><msup id="S2.SS2.p4.1.m1.1.1" xref="S2.SS2.p4.1.m1.1.1.cmml"><mn id="S2.SS2.p4.1.m1.1.1.2" xref="S2.SS2.p4.1.m1.1.1.2.cmml">10</mn><mrow id="S2.SS2.p4.1.m1.1.1.3" xref="S2.SS2.p4.1.m1.1.1.3.cmml"><mo id="S2.SS2.p4.1.m1.1.1.3a" xref="S2.SS2.p4.1.m1.1.1.3.cmml">âˆ’</mo><mn id="S2.SS2.p4.1.m1.1.1.3.2" xref="S2.SS2.p4.1.m1.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.1.m1.1b"><apply id="S2.SS2.p4.1.m1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p4.1.m1.1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1">superscript</csymbol><cn type="integer" id="S2.SS2.p4.1.m1.1.1.2.cmml" xref="S2.SS2.p4.1.m1.1.1.2">10</cn><apply id="S2.SS2.p4.1.m1.1.1.3.cmml" xref="S2.SS2.p4.1.m1.1.1.3"><minus id="S2.SS2.p4.1.m1.1.1.3.1.cmml" xref="S2.SS2.p4.1.m1.1.1.3"></minus><cn type="integer" id="S2.SS2.p4.1.m1.1.1.3.2.cmml" xref="S2.SS2.p4.1.m1.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.1.m1.1c">10^{-4}</annotation></semantics></math>, and employed mini-batches of size 256. We set the input sequence length for SpeechCommandsV2 and Voxforge to 1 second, and for CREMA-D as 3 seconds, based on the audio files durations in each dataset. To ensure consistent loudness range across different recordings, we rescaled the raw speech signals to a range between 15 dB Sound Pressure Level (SPL) and 30 dB SPL. In the test phase, we adopted the approach in Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, and compute predictions for non-overlapping one-second segments and averaging the logits across the entire recording.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">TableÂ <a href="#S2.T1" title="Table 1 â€£ 2.2 Analysis of LEAF Parameters: Experimental Setup â€£ 2 What is learnt by LEAF? â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the test accuracy for all tasks with four different LEAF model settings. The ``Untrained'' settings indicate that all parts of LEAF were frozen and the initial values were not updated during training. The ``Filter Trained'' settings indicate that Gabor filters and Gaussian low pass filters were set to be trainable, and their parameters would be updated during model training (but the PCEN layer parameters would not be updated). Conversly, the ``PCEN Trained'' settings refer to the condition where only the PCEN layer parameters were trained and the Gabor filter and Gaussian low pass filter parameters were kept unchanged from their initial values. Finally, ``Fully Trained'' settings refer to the standard setting where all parameters are trainable.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p">From the table, the first interesting observation that stands out is that there is little difference between the various trained and untrained versions of LEAF across all three tasks. It is worth noting that for the emotion recognition task, where the untrained LEAF shows the highest accuracy, we used the data partitioning reported inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, whereby the data was partitioned by shuffling speaker groups and segmenting the data to ensure speaker independence in each partition. For the other two speech tasks, we used the data partitions as per the original dataset release. These results prompt the question <em id="S2.SS2.p6.1.1" class="ltx_emph ltx_font_italic">``What is learnt by the LEAF model?''</em>. Specifically, through our analyses, we aim to answer the question: Which element of the model has undergone the most significant changes as a result of the training?</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Classification accuracy of LEAF models (mean <math id="S2.T1.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S2.T1.2.m1.1b"><mo id="S2.T1.2.m1.1.1" xref="S2.T1.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.m1.1c"><csymbol cd="latexml" id="S2.T1.2.m1.1.1.cmml" xref="S2.T1.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.m1.1d">\pm</annotation></semantics></math> std.dev, over three runs).</figcaption>
<table id="S2.T1.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.3.1.1" class="ltx_tr">
<th id="S2.T1.3.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S2.T1.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T1.3.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.3.1.1.2.1.1" class="ltx_tr">
<td id="S2.T1.3.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Keyword</td>
</tr>
<tr id="S2.T1.3.1.1.2.1.2" class="ltx_tr">
<td id="S2.T1.3.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Spotting</td>
</tr>
</table>
</th>
<th id="S2.T1.3.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T1.3.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.3.1.1.3.1.1" class="ltx_tr">
<td id="S2.T1.3.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Emotion</td>
</tr>
<tr id="S2.T1.3.1.1.3.1.2" class="ltx_tr">
<td id="S2.T1.3.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Recognition</td>
</tr>
</table>
</th>
<th id="S2.T1.3.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T1.3.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.3.1.1.4.1.1" class="ltx_tr">
<td id="S2.T1.3.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Language</td>
</tr>
<tr id="S2.T1.3.1.1.4.1.2" class="ltx_tr">
<td id="S2.T1.3.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Identification</td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.3.2.1" class="ltx_tr">
<th id="S2.T1.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<table id="S2.T1.3.2.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.3.2.1.1.1.1" class="ltx_tr">
<td id="S2.T1.3.2.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Untrained</td>
</tr>
</table>
</th>
<td id="S2.T1.3.2.1.2" class="ltx_td ltx_align_left ltx_border_t">94.78 Â± 0.1</td>
<td id="S2.T1.3.2.1.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S2.T1.3.2.1.3.1" class="ltx_text ltx_font_bold">44.01Â±8.6</span></td>
<td id="S2.T1.3.2.1.4" class="ltx_td ltx_align_left ltx_border_t">91.73Â±0.4</td>
</tr>
<tr id="S2.T1.3.3.2" class="ltx_tr">
<th id="S2.T1.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<table id="S2.T1.3.3.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.3.3.2.1.1.1" class="ltx_tr">
<td id="S2.T1.3.3.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">PCEN Trained</td>
</tr>
</table>
</th>
<td id="S2.T1.3.3.2.2" class="ltx_td ltx_align_left">95.07 Â± 0.2</td>
<td id="S2.T1.3.3.2.3" class="ltx_td ltx_align_left">39.66Â±1.5</td>
<td id="S2.T1.3.3.2.4" class="ltx_td ltx_align_left">89.95Â±2.1</td>
</tr>
<tr id="S2.T1.3.4.3" class="ltx_tr">
<th id="S2.T1.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<table id="S2.T1.3.4.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.3.4.3.1.1.1" class="ltx_tr">
<td id="S2.T1.3.4.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Filters Trained</td>
</tr>
</table>
</th>
<td id="S2.T1.3.4.3.2" class="ltx_td ltx_align_left">94.62 Â± 0.1</td>
<td id="S2.T1.3.4.3.3" class="ltx_td ltx_align_left">40.11Â±1.3</td>
<td id="S2.T1.3.4.3.4" class="ltx_td ltx_align_left"><span id="S2.T1.3.4.3.4.1" class="ltx_text ltx_font_bold">95.13Â±1.4</span></td>
</tr>
<tr id="S2.T1.3.5.4" class="ltx_tr">
<th id="S2.T1.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">
<table id="S2.T1.3.5.4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.3.5.4.1.1.1" class="ltx_tr">
<td id="S2.T1.3.5.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Fully Trained</td>
</tr>
</table>
</th>
<td id="S2.T1.3.5.4.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="S2.T1.3.5.4.2.1" class="ltx_text ltx_font_bold">95.18 Â± 0.3</span></td>
<td id="S2.T1.3.5.4.3" class="ltx_td ltx_align_left ltx_border_bb">41.10Â±1.4</td>
<td id="S2.T1.3.5.4.4" class="ltx_td ltx_align_left ltx_border_bb">91.03Â±0.6</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.06702/assets/Images/Filterbank_learned_fc.png" id="S2.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="192" height="134" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S2.F2.sf1.3.2" class="ltx_text" style="font-size:80%;">Gabor filters' centre frequency changes</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.06702/assets/Images/Filterbank_learned_bw.png" id="S2.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="192" height="136" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S2.F2.sf2.3.2" class="ltx_text" style="font-size:80%;">Gabor filters' bandwidth changes</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.06702/assets/Images/Gaussian_Learned.png" id="S2.F2.sf3.g1" class="ltx_graphics ltx_img_landscape" width="192" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf3.2.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="S2.F2.sf3.3.2" class="ltx_text" style="font-size:80%;">Gaussian frequency response changes</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.06702/assets/Images/pcen_gain_keyword.png" id="S2.F2.sf4.g1" class="ltx_graphics ltx_img_landscape" width="192" height="137" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf4.2.1.1" class="ltx_text" style="font-size:80%;">(d)</span> </span><span id="S2.F2.sf4.3.2" class="ltx_text" style="font-size:80%;">PCEN gain changes (Keyword Spotting)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F2.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.06702/assets/Images/pcen_gain_emotion.png" id="S2.F2.sf5.g1" class="ltx_graphics ltx_img_landscape" width="192" height="137" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf5.2.1.1" class="ltx_text" style="font-size:80%;">(e)</span> </span><span id="S2.F2.sf5.3.2" class="ltx_text" style="font-size:80%;">PCEN gain changes (Emotion Recognition)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S2.F2.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.06702/assets/Images/pcen_gain_language.png" id="S2.F2.sf6.g1" class="ltx_graphics ltx_img_landscape" width="192" height="137" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf6.2.1.1" class="ltx_text" style="font-size:80%;">(f)</span> </span><span id="S2.F2.sf6.3.2" class="ltx_text" style="font-size:80%;">PCEN gain changes (Language ID)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Visualising learnt information of LEAF across three tasks.</figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Analysis of LEAF Parameters: Results</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Extracting the weights from the fully trained and untrained LEAF models, we compute the centre frequencies and bandwidths of the Gabor filters as well as magnitude responses of the Gaussian low pass filters and compare them to each other (trained vs untrained across all three tasks). We also reconstruct the compression function of the PCEN layers and compare them to each other. These comparisons are shown in FigureÂ <a href="#S2.F2.sf1" title="In Figure 2 â€£ 2.2 Analysis of LEAF Parameters: Experimental Setup â€£ 2 What is learnt by LEAF? â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a>-<a href="#S2.F2.sf6" title="In Figure 2 â€£ 2.2 Analysis of LEAF Parameters: Experimental Setup â€£ 2 What is learnt by LEAF? â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(f)</span></a>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p"><span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_bold">a) Gabor Filterbank:</span> FiguresÂ <a href="#S2.F2.sf1" title="In Figure 2 â€£ 2.2 Analysis of LEAF Parameters: Experimental Setup â€£ 2 What is learnt by LEAF? â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a> andÂ <a href="#S2.F2.sf2" title="In Figure 2 â€£ 2.2 Analysis of LEAF Parameters: Experimental Setup â€£ 2 What is learnt by LEAF? â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a> represent the changes in centre frequencies and bandwidths of each filter of the 40 Gabor filters across three tasks as well as the initial values. No appreciable deviations from the intial values can be observed for any of the trained models in any of the three tasks. These results strongly indicate that the initial Gabor filters may be optimal and learning does not help.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p"><span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_bold">b) Gaussian Lowpass Filterbank:</span> In FigureÂ <a href="#S2.F2.sf3" title="In Figure 2 â€£ 2.2 Analysis of LEAF Parameters: Experimental Setup â€£ 2 What is learnt by LEAF? â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(c)</span></a>, we plot the frequency response of all <math id="S2.SS3.p3.1.m1.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S2.SS3.p3.1.m1.1a"><mn id="S2.SS3.p3.1.m1.1.1" xref="S2.SS3.p3.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.1.m1.1b"><cn type="integer" id="S2.SS3.p3.1.m1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.1.m1.1c">40</annotation></semantics></math> Gaussian lowpass filters for all three tasks. Once again, there appears to be no appreciable deviation from the initial values in the pass band of the low pass filters. This again suggests there may be no benefit to learning these filters.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.3" class="ltx_p"><span id="S2.SS3.p4.3.1" class="ltx_text ltx_font_bold">c) PCEN Range Compression:</span> In FiguresÂ <a href="#S2.F2.sf5" title="In Figure 2 â€£ 2.2 Analysis of LEAF Parameters: Experimental Setup â€£ 2 What is learnt by LEAF? â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(e)</span></a> toÂ <a href="#S2.F2.sf6" title="In Figure 2 â€£ 2.2 Analysis of LEAF Parameters: Experimental Setup â€£ 2 What is learnt by LEAF? â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(f)</span></a>, we present input level dependent gain imparted by the PCEN layer across all frequency channels in each of the three tasks. To visualise this, we reconstruct the trained and untrained PCEN functions from EquationsÂ <a href="#S2.E1" title="In 2.1 LEAF â€£ 2 What is learnt by LEAF? â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> andÂ <a href="#S2.E2" title="In 2.1 LEAF â€£ 2 What is learnt by LEAF? â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and plot the PCEN gains as a function of input energy <math id="S2.SS3.p4.1.m1.2" class="ltx_Math" alttext="E[n,i]" display="inline"><semantics id="S2.SS3.p4.1.m1.2a"><mrow id="S2.SS3.p4.1.m1.2.3" xref="S2.SS3.p4.1.m1.2.3.cmml"><mi id="S2.SS3.p4.1.m1.2.3.2" xref="S2.SS3.p4.1.m1.2.3.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p4.1.m1.2.3.1" xref="S2.SS3.p4.1.m1.2.3.1.cmml">â€‹</mo><mrow id="S2.SS3.p4.1.m1.2.3.3.2" xref="S2.SS3.p4.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS3.p4.1.m1.2.3.3.2.1" xref="S2.SS3.p4.1.m1.2.3.3.1.cmml">[</mo><mi id="S2.SS3.p4.1.m1.1.1" xref="S2.SS3.p4.1.m1.1.1.cmml">n</mi><mo id="S2.SS3.p4.1.m1.2.3.3.2.2" xref="S2.SS3.p4.1.m1.2.3.3.1.cmml">,</mo><mi id="S2.SS3.p4.1.m1.2.2" xref="S2.SS3.p4.1.m1.2.2.cmml">i</mi><mo stretchy="false" id="S2.SS3.p4.1.m1.2.3.3.2.3" xref="S2.SS3.p4.1.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.1.m1.2b"><apply id="S2.SS3.p4.1.m1.2.3.cmml" xref="S2.SS3.p4.1.m1.2.3"><times id="S2.SS3.p4.1.m1.2.3.1.cmml" xref="S2.SS3.p4.1.m1.2.3.1"></times><ci id="S2.SS3.p4.1.m1.2.3.2.cmml" xref="S2.SS3.p4.1.m1.2.3.2">ğ¸</ci><interval closure="closed" id="S2.SS3.p4.1.m1.2.3.3.1.cmml" xref="S2.SS3.p4.1.m1.2.3.3.2"><ci id="S2.SS3.p4.1.m1.1.1.cmml" xref="S2.SS3.p4.1.m1.1.1">ğ‘›</ci><ci id="S2.SS3.p4.1.m1.2.2.cmml" xref="S2.SS3.p4.1.m1.2.2">ğ‘–</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.1.m1.2c">E[n,i]</annotation></semantics></math>. The curves in FiguresÂ <a href="#S2.F2.sf5" title="In Figure 2 â€£ 2.2 Analysis of LEAF Parameters: Experimental Setup â€£ 2 What is learnt by LEAF? â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(e)</span></a> toÂ <a href="#S2.F2.sf6" title="In Figure 2 â€£ 2.2 Analysis of LEAF Parameters: Experimental Setup â€£ 2 What is learnt by LEAF? â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(f)</span></a> transition from light green (for the <math id="S2.SS3.p4.2.m2.1" class="ltx_Math" alttext="1^{st}" display="inline"><semantics id="S2.SS3.p4.2.m2.1a"><msup id="S2.SS3.p4.2.m2.1.1" xref="S2.SS3.p4.2.m2.1.1.cmml"><mn id="S2.SS3.p4.2.m2.1.1.2" xref="S2.SS3.p4.2.m2.1.1.2.cmml">1</mn><mrow id="S2.SS3.p4.2.m2.1.1.3" xref="S2.SS3.p4.2.m2.1.1.3.cmml"><mi id="S2.SS3.p4.2.m2.1.1.3.2" xref="S2.SS3.p4.2.m2.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p4.2.m2.1.1.3.1" xref="S2.SS3.p4.2.m2.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS3.p4.2.m2.1.1.3.3" xref="S2.SS3.p4.2.m2.1.1.3.3.cmml">t</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.2.m2.1b"><apply id="S2.SS3.p4.2.m2.1.1.cmml" xref="S2.SS3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.p4.2.m2.1.1.1.cmml" xref="S2.SS3.p4.2.m2.1.1">superscript</csymbol><cn type="integer" id="S2.SS3.p4.2.m2.1.1.2.cmml" xref="S2.SS3.p4.2.m2.1.1.2">1</cn><apply id="S2.SS3.p4.2.m2.1.1.3.cmml" xref="S2.SS3.p4.2.m2.1.1.3"><times id="S2.SS3.p4.2.m2.1.1.3.1.cmml" xref="S2.SS3.p4.2.m2.1.1.3.1"></times><ci id="S2.SS3.p4.2.m2.1.1.3.2.cmml" xref="S2.SS3.p4.2.m2.1.1.3.2">ğ‘ </ci><ci id="S2.SS3.p4.2.m2.1.1.3.3.cmml" xref="S2.SS3.p4.2.m2.1.1.3.3">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.2.m2.1c">1^{st}</annotation></semantics></math> channel) to dark purple (for the <math id="S2.SS3.p4.3.m3.1" class="ltx_Math" alttext="40^{th}" display="inline"><semantics id="S2.SS3.p4.3.m3.1a"><msup id="S2.SS3.p4.3.m3.1.1" xref="S2.SS3.p4.3.m3.1.1.cmml"><mn id="S2.SS3.p4.3.m3.1.1.2" xref="S2.SS3.p4.3.m3.1.1.2.cmml">40</mn><mrow id="S2.SS3.p4.3.m3.1.1.3" xref="S2.SS3.p4.3.m3.1.1.3.cmml"><mi id="S2.SS3.p4.3.m3.1.1.3.2" xref="S2.SS3.p4.3.m3.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p4.3.m3.1.1.3.1" xref="S2.SS3.p4.3.m3.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS3.p4.3.m3.1.1.3.3" xref="S2.SS3.p4.3.m3.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.3.m3.1b"><apply id="S2.SS3.p4.3.m3.1.1.cmml" xref="S2.SS3.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS3.p4.3.m3.1.1.1.cmml" xref="S2.SS3.p4.3.m3.1.1">superscript</csymbol><cn type="integer" id="S2.SS3.p4.3.m3.1.1.2.cmml" xref="S2.SS3.p4.3.m3.1.1.2">40</cn><apply id="S2.SS3.p4.3.m3.1.1.3.cmml" xref="S2.SS3.p4.3.m3.1.1.3"><times id="S2.SS3.p4.3.m3.1.1.3.1.cmml" xref="S2.SS3.p4.3.m3.1.1.3.1"></times><ci id="S2.SS3.p4.3.m3.1.1.3.2.cmml" xref="S2.SS3.p4.3.m3.1.1.3.2">ğ‘¡</ci><ci id="S2.SS3.p4.3.m3.1.1.3.3.cmml" xref="S2.SS3.p4.3.m3.1.1.3.3">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.3.m3.1c">40^{th}</annotation></semantics></math> channel), represent the learnt gain characteristics for all 40 channels across the three tasks. It can be seen that the gain curves for the learnt models differ from that of the untrained model across all three tasks.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p">Taken together these results suggest that of three potentially learnable components of the LEAF model, the Gabor filterbank for spectral decomposition, the Gaussian low pass filter for energy smoothing and the PCEN which offers dynamic range compression, only the PCEN layers appear to be actually learning anything. Consequently, constraining the learning to only this layer of the LEAF model would significantly cut down the number of trainable parameters in the model, leading to more efficient learning. This in turn suggests a noise adaptation scheme involving a small set of noisy speech samples might enable a LEAF model trained on clean speech to be employed in noisy conditions. This hypothesis is explored in the rest of the paper.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Adapting PCEN to Noisy Environments</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We explore the hypothesis that adapting or tuning the PCEN layer in LEAF can enhance the accuracy of speech classification in noisy environments using a limited amount of noisy adaptation data. To test this hypothesis, we compare the performance of a system with a PCEN-adapted LEAF model to that of one trained on clean speech. For reference we also include a model trained entirely on a large amount of noisy data in the comparison<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/Hanyu-Meng/Adapting-LEAF</span></span></span>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experiment Setups</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Dataset and Partition</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Of the three speech procesing tasks, we choose the emotion recognition task to test the proposed PCEN adaptation scheme since it had the lowest accuracy. The CREMA-D dataset for speech emotion recognition consists of 91 speakers and 6 emotionsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, with each speaker having an almost equal number of utterances. When partitioning CREMA-D for this set of experiments, we use different partitions from those used in section 2.2. Specifically, we split the data based on sentences to ensure that each partition contained utterances from each speaker in order to minimise the impact of speaker variability on the results. As illustrated in FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3.1.2 Experimental Setup â€£ 3.1 Experiment Setups â€£ 3 Adapting PCEN to Noisy Environments â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we used 9 sentences for training, 1 sentence for validation, and the remaining 2 sentences for testing. Consequently, the training set contained 5811 recordings, validation set contained 545, and the test set contained 1086. For adaptation, we selected one sentence from the training set (comprising of 546 recordings) as the adaptation data to which we add different types of noise (white noise and babble noise) at different Signal-to-Noise Ratios (SNRs).</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Experimental Setup</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">For this experiment, we compared four models that had the same structure as the system used in the experiments reported in section 2.2, but with varying training setups as illustrated in FigureÂ <a href="#S3.F4" title="Figure 4 â€£ 3.1.2 Experimental Setup â€£ 3.1 Experiment Setups â€£ 3 Adapting PCEN to Noisy Environments â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. These models are:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Clean Trained</span>: Trained on the entire noise-free training set and it serves as a baseline.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Noisy Trained</span>: Trained on a noisy version of the entire training data.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Before Adaptation (BA)</span>: Trained on the noise-free training set without including adaptation data. This provides a reference level for performance prior to adaptation (see FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3.1.2 Experimental Setup â€£ 3.1 Experiment Setups â€£ 3 Adapting PCEN to Noisy Environments â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">PCEN Adaptation (PA)</span>: This is the BA model with the PCEN layer adapted using the noisy adaptation data.</p>
</div>
</li>
</ul>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2404.06702/assets/Images/data_partition.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="130" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Data partitions in the CREMA-D dataset for PCEN adaptation experiments.</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2404.06702/assets/Images/experiment_process_leaf.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="267" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Overview of the PCEN adaptation experimental setup.</figcaption>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Results and Analysis</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To verify the hypothesis proposed at the beginning of Section 3, we tested the adaptation of PCEN using both stationary and non-stationary noise.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Gaussian Noise Adaptation</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">The left pane in FigureÂ <a href="#S3.F5" title="Figure 5 â€£ 3.2.2 Babble Noise Adaptation â€£ 3.2 Results and Analysis â€£ 3 Adapting PCEN to Noisy Environments â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the classification accuracy for all four models tested by adding different levels of Gaussian noise to the clean test data in order to obtain SNR in the range of 0 dB to 20 dB. The results suggest that training the model with Gaussian noise helps the model learn the pattern of noise and improves its robustness. Further, the models trained on only clean data perform poorly when exposed to Gaussian noise. However, after adapting the PCEN layer with a small amount of noisy data, the impact of noise on accuracy can be somewhat mitigated as can be seen by comparing the performance Before Adaptation (BA model) to that after adaptation (PA model).</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Babble Noise Adaptation</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">We also repeated the above experiment using babble noise instead of white noise. To simulate babble noise, we followed the data augmentation approach used inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and used the MUSAN dataset, which contains 60 hours of speech from 12 languagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Specifically, we randomly selected three speech recordings from MUSAN, mixed them together, and added them to the clean signal to simulate babble noise with SNR ranging from 0 dB to 20 dB. The right pane in FigureÂ <a href="#S3.F5" title="Figure 5 â€£ 3.2.2 Babble Noise Adaptation â€£ 3.2 Results and Analysis â€£ 3 Adapting PCEN to Noisy Environments â€£ What is Learnt by the LEArnable Front-end (LEAF)? Adapting Per-Channel Energy Normalisation (PCEN) to Noisy Conditions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows how the accuracy of the four models changes under different levels of babble noise.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p">The results suggest that training with noisy data is not as effective under babble noise conditions. This might be due to the greater similarity between noise and speech in this case. However, we can observe from the graph that adapting PCEN with babble noise may be quite effective in allowing the model to be used under noisy conditions.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2404.06702/assets/Images/noise_result.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="410" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Accuracy of PCEN adaptation experiments on the CREMA-D dataset for different levels of Gaussian noise (left) and babble noise (right).</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this study, we sought to answer the question <em id="S4.p1.1.1" class="ltx_emph ltx_font_italic">'What is learnt by LEAF?'</em>, by comparing the extent of change in the characteristics of the different learnable components of the LEAF model from their initial values prior to training. This analysis was repeated on multiple speech processing tasks, and consistently our analyses revealed that only the PCEN layer changes in response to training. The filterbank and low pass filters employed for spectral decomposition and spectral energy smoothing remained unchanged for all three tasks. This suggests that the actual learning in the LEArnable Front-end occurs within a much lower dimensional subspace of the parameter space of the model. Following this, we developed a model adaptation scheme constrained to this subspace (PCEN layer only) using a small amount of noisy training data to adapt a LEAF trained on clean speech to operate more effectively in noisy conditions.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Acknowledgements</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This work was funded by ARC Discovery Grant DP210101228. The authors would also like to thank UNSW, Sydney, Australia for providing PhD scholarship support.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S.Â S. Stevens and J.Â Volkmann, ``The relation of pitch to frequency: A revised scale,'' <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">The American Journal of Psychology</em>, vol.Â 53, no.Â 3, pp. 329â€“353, 1940.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Y.Â Luo and N.Â Mesgarani, ``Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation,'' <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM transactions on audio, speech, and language processing</em>, vol.Â 27, no.Â 8, pp. 1256â€“1266, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J.Â Li, Y.Â Wu, Y.Â Gaur, C.Â Wang, R.Â Zhao, and S.Â Liu, ``On the comparison of popular end-to-end models for large scale speech recognition,'' <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2005.14327</em>, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A.Â Graves and N.Â Jaitly, ``Towards end-to-end speech recognition with recurrent neural networks,'' in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.Â Â Â PMLR, 2014, pp. 1764â€“1772.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
C.Â Zeng, X.Â Miao, X.Â Wang, E.Â Cooper, and J.Â Yamagishi, ``Joint speaker encoder and neural back-end model for fully end-to-end automatic speaker verification with multiple enrollment utterances,'' <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.00485</em>, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
N.Â Zeghidour, O.Â Teboul, F.Â d.Â C. Quitry, and M.Â Tagliasacchi, ``Leaf: A learnable frontend for audio classification,'' <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.08596</em>, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J.Â Peng, R.Â Gu, L.Â MoÅ¡ner, O.Â Plchot, L.Â Burget, and J.Â ÄŒernocká»³, ``Learnable sparse filterbank for speaker verification,'' <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2022</em>, pp. 5110â€“5114, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S.Â Dutta and S.Â Ganapathy, ``Multimodal transformer with learnable frontend and self attention for emotion recognition,'' in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2022, pp. 6917â€“6921.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Q.Â Fu, Z.Â Teng, J.Â White, M.Â E. Powell, and D.Â C. Schmidt, ``Fastaudio: A learnable audio front-end for spoof speech detection,'' in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2022, pp. 3693â€“3697.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M.Â Ravanelli and Y.Â Bengio, ``Speaker recognition from raw waveform with sincnet,'' in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">2018 IEEE Spoken Language Technology Workshop (SLT)</em>.Â Â Â IEEE, 2018, pp. 1021â€“1028.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
P.-G. NoÃ©, T.Â Parcollet, and M.Â Morchid, ``Cgcnn: Complex gabor convolutional neural network on raw speech,'' in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2020, pp. 7724â€“7728.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
N.Â Zeghidour, N.Â Usunier, I.Â Kokkinos, T.Â Schaiz, G.Â Synnaeve, and E.Â Dupoux, ``Learning filterbanks from raw speech for phone recognition,'' in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2018 IEEE international conference on acoustics, speech and signal Processing (ICASSP)</em>.Â Â Â IEEE, 2018, pp. 5509â€“5513.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A.Â M. PoirÃ¨, F.Â Simonetta, and S.Â Ntalampiras, ``Deep feature learning for medical acoustics,'' in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Artificial Neural Networks and Machine Learningâ€“ICANN 2022: 31st International Conference on Artificial Neural Networks, Bristol, UK, September 6â€“9, 2022, Proceedings; Part IV</em>.Â Â Â Springer, 2022, pp. 39â€“50.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
B.Â Bergsma, M.Â Yang, and M.Â Cernak, ``Peaf: Learnable power efficient analog acoustic features for audio recognition,'' <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.03715</em>, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M.Â Anderson and N.Â Harte, ``Learnable acoustic frontends in bird activity detection,'' <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.00889</em>, 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J.Â Li, Y.Â Tian, and T.Â Lee, ``Learnable frequency filters for speech feature extraction in speaker verification,'' <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2206.07563</em>, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
H.Â PeicÂ Tukuljac, B.Â Ricaud, N.Â Aspert, and L.Â Colbois, ``Learnable filter-banks for cnn-based audio applications,'' in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Northern Lights Deep Learning Workshop 2022</em>, no. CONF, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
V.Â Lostanlen, J.Â Salamon, M.Â Cartwright, B.Â McFee, A.Â Farnsworth, S.Â Kelling, and J.Â P. Bello, ``Per-channel energy normalization: Why and how,'' <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Letters</em>, vol.Â 26, no.Â 1, pp. 39â€“43, 2018.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
C.Â Ick and B.Â McFee, ``Sound event detection in urban audio with single and multi-rate pcen,'' in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2021, pp. 880â€“884.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
H.-S. Choi, S.Â Park, J.Â H. Lee, H.Â Heo, D.Â Jeon, and K.Â Lee, ``Real-time denoising and dereverberation wtih tiny recurrent u-net,'' in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2021, pp. 5789â€“5793.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J.Â Millet and N.Â Zeghidour, ``Learning to detect dysarthria from raw speech,'' in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2019, pp. 5831â€“5835.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
B.Â Kim, S.Â Yang, J.Â Kim, and S.Â Chang, ``Domain generalization on efficient acoustic scene classification using residual normalization,'' <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.06531</em>, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
V.Â Lostanlen, K.Â Palmer, E.Â Knight, C.Â Clark, H.Â Klinck, A.Â Farnsworth, T.Â Wong, J.Â Cramer, and J.Â P. Bello, ``Long-distance detection of bioacoustic events with per-channel energy normalization,'' <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.00417</em>, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
D.Â Gabor, ``Theory of communication. part 1: The analysis of information,'' <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Journal of the Institution of Electrical Engineers-part III: radio and communication engineering</em>, vol.Â 93, no.Â 26, pp. 429â€“441, 1946.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
R.Â Zhang, ``Making convolutional networks shift-invariant again,'' in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.Â Â Â PMLR, 2019, pp. 7324â€“7334.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Y.Â Wang, P.Â Getreuer, T.Â Hughes, R.Â F. Lyon, and R.Â A. Saurous, ``Trainable frontend for robust and far-field keyword spotting,'' in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2017, pp. 5670â€“5674.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J.Â SchlÃ¼ter and G.Â Gutenbrunner, ``Efficientleaf: A faster learnable audio frontend of questionable use,'' in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">2022 30th European Signal Processing Conference (EUSIPCO)</em>.Â Â Â IEEE, 2022, pp. 205â€“208.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
P.Â Warden, ``Speech commands: A dataset for limited-vocabulary speech recognition,'' <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1804.03209</em>, 2018.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
H.Â Cao, D.Â G. Cooper, M.Â K. Keutmann, R.Â C. Gur, A.Â Nenkova, and R.Â Verma, ``Crema-d: Crowd-sourced emotional multimodal actors dataset,'' <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on affective computing</em>, vol.Â 5, no.Â 4, pp. 377â€“390, 2014.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
S.Â Revay and M.Â Teschke, ``Multiclass language identification using deep learning on spectral images of audio signals,'' <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.04348</em>, 2019.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
M.Â Tan and Q.Â Le, ``Efficientnet: Rethinking model scaling for convolutional neural networks,'' in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.Â Â Â PMLR, 2019, pp. 6105â€“6114.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
D.Â Snyder, D.Â Garcia-Romero, G.Â Sell, D.Â Povey, and S.Â Khudanpur, ``X-vectors: Robust dnn embeddings for speaker recognition,'' in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>.Â Â Â IEEE, 2018, pp. 5329â€“5333.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
D.Â Snyder, G.Â Chen, and D.Â Povey, ``Musan: A music, speech, and noise corpus,'' <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1510.08484</em>, 2015.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:creator" content="{Author name(s) withheld}"></div>
<div class="ltx_rdf" about="" property="dcterms:title" content="{Submitted to INTERSPEECH}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.06701" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.06702" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.06702">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.06702" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.06703" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 19:15:22 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
