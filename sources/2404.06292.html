<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.06292] nEMO: Dataset of Emotional Speech in Polish</title><meta property="og:description" content="Speech emotion recognition has become increasingly important in recent years due to its potential applications in healthcare, customer service, and personalization of dialogue systems. However, a major issue in this fi…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="nEMO: Dataset of Emotional Speech in Polish">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="nEMO: Dataset of Emotional Speech in Polish">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.06292">

<!--Generated on Sun May  5 20:58:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">nEMO: Dataset of Emotional Speech in Polish</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Speech emotion recognition has become increasingly important in recent years due to its potential applications in healthcare, customer service, and personalization of dialogue systems. However, a major issue in this field is the lack of datasets that adequately represent basic emotional states across various language families. As datasets covering Slavic languages are rare, there is a need to address this research gap. This paper presents the development of nEMO, a novel corpus of emotional speech in Polish. The dataset comprises over 3 hours of samples recorded with the participation of nine actors portraying six emotional states: anger, fear, happiness, sadness, surprise, and a neutral state. The text material used was carefully selected to represent the phonetics of the Polish language adequately. The corpus is freely available under the terms of a Creative Commons license (CC BY-NC-SA 4.0).

<br class="ltx_break">
<br class="ltx_break">
<span id="id2.id1.1" class="ltx_text ltx_font_bold">Keywords: </span>emotional speech, speech corpus, Polish speech</p>
</div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\NAT@set@cites</span>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text"></span></p>
</div>
<div id="id1" class="ltx_logical-block">
<div id="id1.p1" class="ltx_para">
<p id="id1.p1.1" class="ltx_p ltx_align_center"><span id="id1.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">nEMO: Dataset of Emotional Speech in Polish</span></p>
<br class="ltx_break ltx_centering">
<table id="id1.p1.2" class="ltx_tabular ltx_centering ltx_align_top">
<tbody class="ltx_tbody">
<tr id="id1.p1.2.1.1" class="ltx_tr">
<td id="id1.p1.2.1.1.1" class="ltx_td ltx_align_center"><span id="id1.p1.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:120%;">Iwona Christop</span></td>
</tr>
<tr id="id1.p1.2.2.2" class="ltx_tr">
<td id="id1.p1.2.2.2.1" class="ltx_td ltx_align_center">Adam Mickiewicz University</td>
</tr>
<tr id="id1.p1.2.3.3" class="ltx_tr">
<td id="id1.p1.2.3.3.1" class="ltx_td ltx_align_center">Faculty of Mathematics and Computer Science</td>
</tr>
<tr id="id1.p1.2.4.4" class="ltx_tr">
<td id="id1.p1.2.4.4.1" class="ltx_td ltx_align_center">Uniwersytetu Poznańskiego 4, 61-614 Poznań, Poland</td>
</tr>
<tr id="id1.p1.2.5.5" class="ltx_tr">
<td id="id1.p1.2.5.5.1" class="ltx_td ltx_align_center">christop@wmi.amu.edu.pl</td>
</tr>
</tbody>
</table>
<p id="id1.p1.3" class="ltx_p ltx_align_center"><span id="id1.p1.3.1" class="ltx_text ltx_font_italic">Abstract content</span></p>
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.   Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Automatic speech recognition is used in various aspects of life, ranging from customer service systems to virtual assistants and chatbots. As speech is the most natural form of communication for humans, human-computer interaction systems strive to minimize written interfaces.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Incorporating speech emotion recognition in dialogue systems may not seem like an obvious next step. However, this development could significantly refine the personalization of virtual assistants, enabling them to respond appropriately to the emotional state of the user. The impersonal nature of current dialogue systems often results in user discouragement <cite class="ltx_cite ltx_citemacro_citep">(Schuller, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The potential applications of emotion recognition extend beyond customer service. For example, in the context of emergency calls, the ability to distinguish emotions such as fear or sadness could be crucial in assessing the authenticity of the call. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Abbaschian et al. (<a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite> demonstrated its utility within therapy sessions, where the analysis of a patient’s emotions could offer insights into their psychological state.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Although speech emotion recognition is acknowledged as a critical area of research, the field still faces challenges, most notably the scarcity of high-quality, diverse datasets. Most available corpora contain simulated emotional expressions that do not accurately reflect human emotions <cite class="ltx_cite ltx_citemacro_citep">(Abbaschian et al., <a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite>. Additionally, the development of resources for various natural language processing and speech processing tasks often overlooks low-resource languages.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Polish being the second most spoken language within the Slavic language family exemplifies a significant gap in resources for speech emotion recognition. This study aims to bridge this gap with the introduction and analysis of the nEMO dataset, a novel corpus of Polish emotional speech.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The following section provides an overview of existing emotional speech corpora. Section <a href="#S3" title="3. Methodology ‣ nEMO: Dataset of Emotional Speech in Polish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> describes the methodology used to create the nEMO dataset, including the selection of linguistic content, emotional states, and participant actors. Section <a href="#S4" title="4. Final Dataset ‣ nEMO: Dataset of Emotional Speech in Polish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> provides an overview of the final dataset. Section <a href="#S5" title="5. Evaluation ‣ nEMO: Dataset of Emotional Speech in Polish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> describes the evaluation of the dataset using various machine learning techniques. Finally, section <a href="#S6" title="6. Conclusions ‣ nEMO: Dataset of Emotional Speech in Polish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> provides conclusions and considerations for future work.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.   Related Work</h2>

<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></td>
<td id="S2.T1.1.1.1.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.1.1.1.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S2.T1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Number of</span></td>
<td id="S2.T1.1.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">Number of</span></td>
<td id="S2.T1.1.1.1.6" class="ltx_td ltx_align_top ltx_border_r ltx_border_t"></td>
</tr>
<tr id="S2.T1.1.2.2" class="ltx_tr">
<td id="S2.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span id="S2.T1.1.2.2.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S2.T1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.1.2.2.2.1" class="ltx_text ltx_font_bold">Language</span></td>
<td id="S2.T1.1.2.2.3" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.1.2.2.3.1" class="ltx_text ltx_font_bold">Type</span></td>
<td id="S2.T1.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.1.2.2.4.1" class="ltx_text ltx_font_bold">samples</span></td>
<td id="S2.T1.1.2.2.5" class="ltx_td ltx_align_left ltx_border_r"><span id="S2.T1.1.2.2.5.1" class="ltx_text ltx_font_bold">speakers</span></td>
<td id="S2.T1.1.2.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S2.T1.1.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.2.6.1.1" class="ltx_p" style="width:113.8pt;"><span id="S2.T1.1.2.2.6.1.1.1" class="ltx_text ltx_font_bold">Emotions</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.1.3.3" class="ltx_tr">
<td id="S2.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.1.3.3.1.1" class="ltx_text ltx_font_bold">CREMA-D</span></td>
<td id="S2.T1.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">English</td>
<td id="S2.T1.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">simulated</td>
<td id="S2.T1.1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">7,742</td>
<td id="S2.T1.1.3.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">91</td>
<td id="S2.T1.1.3.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.3.6.1.1" class="ltx_p" style="width:113.8pt;">anger, disgust, fear, happiness, neutral, sadness</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.4.4" class="ltx_tr">
<td id="S2.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.1.4.4.1.1" class="ltx_text ltx_font_bold">EMO-DB</span></td>
<td id="S2.T1.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">German</td>
<td id="S2.T1.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">simulated</td>
<td id="S2.T1.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">700</td>
<td id="S2.T1.1.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">10</td>
<td id="S2.T1.1.4.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.4.6.1.1" class="ltx_p" style="width:113.8pt;">anger, boredom, disgust, fear, happiness, neutral, sadness</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.5.5" class="ltx_tr">
<td id="S2.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.1.5.5.1.1" class="ltx_text ltx_font_bold">IEMOCAP</span></td>
<td id="S2.T1.1.5.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">English</td>
<td id="S2.T1.1.5.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">semi-natural</td>
<td id="S2.T1.1.5.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1,150</td>
<td id="S2.T1.1.5.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">10</td>
<td id="S2.T1.1.5.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.5.6.1.1" class="ltx_p" style="width:113.8pt;">anger, disgust, excitement, fear, frustration, happiness, neutral, sadness, surprise</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.6.6" class="ltx_tr">
<td id="S2.T1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.1.6.6.1.1" class="ltx_text ltx_font_bold">RAVDESS</span></td>
<td id="S2.T1.1.6.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">English</td>
<td id="S2.T1.1.6.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">simulated</td>
<td id="S2.T1.1.6.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2,496</td>
<td id="S2.T1.1.6.6.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">24</td>
<td id="S2.T1.1.6.6.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.6.6.6.1.1" class="ltx_p" style="width:113.8pt;">anger, calm, disgust, fear, happiness, neutral, sadness, surprise</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.7.7" class="ltx_tr">
<td id="S2.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.1.7.7.1.1" class="ltx_text ltx_font_bold">TESS</span></td>
<td id="S2.T1.1.7.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">English</td>
<td id="S2.T1.1.7.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">simulated</td>
<td id="S2.T1.1.7.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2,800</td>
<td id="S2.T1.1.7.7.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2</td>
<td id="S2.T1.1.7.7.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.7.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.7.7.6.1.1" class="ltx_p" style="width:113.8pt;">anger, disgust, fear, happiness, neutral, sadness, surprise</span>
</span>
</td>
</tr>
<tr id="S2.T1.1.8.8" class="ltx_tr">
<td id="S2.T1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.1.8.8.1.1" class="ltx_text ltx_font_bold">URDU</span></td>
<td id="S2.T1.1.8.8.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Urdu</td>
<td id="S2.T1.1.8.8.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">natural</td>
<td id="S2.T1.1.8.8.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">400</td>
<td id="S2.T1.1.8.8.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">38</td>
<td id="S2.T1.1.8.8.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S2.T1.1.8.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.8.8.6.1.1" class="ltx_p" style="width:113.8pt;">anger, happiness, neutral, sadness</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>An overview of popular emotional speech datasets, highlighting language, type, number of samples and speakers, and emotional states covered.</figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The reliability of speech emotion recognition systems depends on the quality of data used for training. Although finding a corpus for Slavic languages poses a challenge, there are valuable datasets available for other language families. This indicates a global interest and effort in the development of emotional speech datasets.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The datasets of emotional speech can generally be classified into three categories based on the method used to acquire recordings: natural, semi-natural, and simulated.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Natural datasets contain authentic emotional states instead of exaggerated expressions. These datasets are typically sourced from YouTube videos, television series, and call center recordings. While this approach prevents overfitting to artificial emotions, it comes with other complications. The continuity and dynamics of the recordings can complicate emotion detection, and multiple emotional states may be present simultaneously. Additionally, we have no control over the selection of presented emotions. Furthermore, the subjective nature of emotions requires manual annotation, which can vary between individuals. Finally, such content may be restricted by copyrights <cite class="ltx_cite ltx_citemacro_citep">(Abbaschian et al., <a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">In the case of semi-natural datasets, actors attempt to portray emotions based on a given script, relying on their own perception of the content. While this approach strives to align with natural speech, the expressions may seem unnatural due to the specified context. Similar to natural datasets, the interpretation and portrayal of emotional states are actor-dependent, and manual labeling is required <cite class="ltx_cite ltx_citemacro_citep">(Abbaschian et al., <a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">For simulated datasets, professional actors read the same text while portraying different emotions. This approach allows for the definition of an arbitrary number of discrete emotional states, simplifying comparative analysis across different corpora and ensuring full control over copyrights. However, a major limitation of such datasets is that the enacted emotions may seem unnatural and distorted from conversational speech <cite class="ltx_cite ltx_citemacro_citep">(Abbaschian et al., <a href="#bib.bib1" title="" class="ltx_ref">2021</a>; Schuller, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Although natural datasets may seem like the best choice due to their authentic representation of human emotions, the majority of existing resources utilize the simulated approach because of its vast advantages. The selection of the approach is usually followed by determining the spectrum of emotions to be included. Existing datasets consist of portrayals of various emotional states, including anger, fear, happiness, sadness, and neutral state. The curation of linguistic material that accurately reflects the phonetics of the targeted language is the final essential step.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">Table <a href="#S2.T1" title="Table 1 ‣ 2. Related Work ‣ nEMO: Dataset of Emotional Speech in Polish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents a comparative overview of several well-known emotional speech datasets: CREMA-D <cite class="ltx_cite ltx_citemacro_citep">(Cao et al., <a href="#bib.bib4" title="" class="ltx_ref">2014</a>)</cite>, EMO-DB <cite class="ltx_cite ltx_citemacro_citep">(Burkhardt et al., <a href="#bib.bib2" title="" class="ltx_ref">2005</a>)</cite>, IEMOCAP <cite class="ltx_cite ltx_citemacro_citep">(Busso et al., <a href="#bib.bib3" title="" class="ltx_ref">2008</a>)</cite>, RAVDESS <cite class="ltx_cite ltx_citemacro_citep">(Livingstone and Russo, <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>, TESS <cite class="ltx_cite ltx_citemacro_citep">(Pichora-Fuller and Dupuis, <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>, and URDU <cite class="ltx_cite ltx_citemacro_cite">Latif et al. (<a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite>. It summarizes each dataset by providing information on its language, category, total number of samples, number of contributing speakers, and the range of emotions covered. This comparison provides an understanding of the diversity of existing resources in the field of speech emotion recognition.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.   Methodology</h2>

<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Phoneme</span></td>
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Word</span></td>
<td id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Word in IPA</span></td>
<td id="S3.T2.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.4.1.1" class="ltx_p" style="width:104.7pt;"><span id="S3.T2.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Sentence in Polish</span></span>
</span>
</td>
<td id="S3.T2.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.5.1.1" class="ltx_p" style="width:104.7pt;"><span id="S3.T2.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Sentence in English</span></span>
</span>
</td>
</tr>
<tr id="S3.T2.1.2.2" class="ltx_tr">
<td id="S3.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T2.1.2.2.1.1" class="ltx_ERROR undefined">\tipaencoding</span>ĩ</td>
<td id="S3.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">zima</td>
<td id="S3.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">[<span id="S3.T2.1.2.2.3.1" class="ltx_ERROR undefined">\tipaencoding</span><span id="S3.T2.1.2.2.3.2" class="ltx_ERROR undefined">\textprimstress</span><span id="S3.T2.1.2.2.3.3" class="ltx_ERROR undefined">\textctz</span>ĩma]</td>
<td id="S3.T2.1.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.2.4.1.1" class="ltx_p" style="width:104.7pt;">Zima to czas jeżdżenia na sankach.</span>
</span>
</td>
<td id="S3.T2.1.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.2.5.1.1" class="ltx_p" style="width:104.7pt;">Winter is the time for sledding.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.3.3" class="ltx_tr">
<td id="S3.T2.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T2.1.3.3.1.1" class="ltx_ERROR undefined">\tipaencoding</span>b</td>
<td id="S3.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">baza</td>
<td id="S3.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">[<span id="S3.T2.1.3.3.3.1" class="ltx_ERROR undefined">\tipaencoding</span>baza]</td>
<td id="S3.T2.1.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.3.4.1.1" class="ltx_p" style="width:104.7pt;">Baza wojskowa jest strategicznym punktem dla armii.</span>
</span>
</td>
<td id="S3.T2.1.3.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.3.5.1.1" class="ltx_p" style="width:104.7pt;">The military base is a strategic point for the army.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.4.4" class="ltx_tr">
<td id="S3.T2.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T2.1.4.4.1.1" class="ltx_ERROR undefined">\tipaencoding</span><span id="S3.T2.1.4.4.1.2" class="ltx_ERROR undefined">\textroundcap</span><span id="S3.T2.1.4.4.1.3" class="ltx_ERROR undefined">\texttslig</span><sup id="S3.T2.1.4.4.1.4" class="ltx_sup">j</sup>
</td>
<td id="S3.T2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">racja</td>
<td id="S3.T2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">[<span id="S3.T2.1.4.4.3.1" class="ltx_ERROR undefined">\tipaencoding</span><span id="S3.T2.1.4.4.3.2" class="ltx_ERROR undefined">\textprimstress</span>ra<span id="S3.T2.1.4.4.3.3" class="ltx_ERROR undefined">\textroundcap</span><span id="S3.T2.1.4.4.3.4" class="ltx_ERROR undefined">\texttslig</span><sup id="S3.T2.1.4.4.3.5" class="ltx_sup">j</sup>ja]</td>
<td id="S3.T2.1.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.4.4.1.1" class="ltx_p" style="width:104.7pt;">Racja, ten pomysł jest <span id="S3.T2.1.4.4.4.1.1.1" class="ltx_text">najlepszy</span>.</span>
</span>
</td>
<td id="S3.T2.1.4.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.4.5.1.1" class="ltx_p" style="width:104.7pt;">Right, this idea is the best.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.5.5" class="ltx_tr">
<td id="S3.T2.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T2.1.5.5.1.1" class="ltx_ERROR undefined">\tipaencoding</span>s<sup id="S3.T2.1.5.5.1.2" class="ltx_sup">j</sup>
</td>
<td id="S3.T2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">pasja</td>
<td id="S3.T2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">[<span id="S3.T2.1.5.5.3.1" class="ltx_ERROR undefined">\tipaencoding</span>pas<sup id="S3.T2.1.5.5.3.2" class="ltx_sup">j</sup>ja]</td>
<td id="S3.T2.1.5.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.5.4.1.1" class="ltx_p" style="width:104.7pt;">Piłka nożna to moja pasja.</span>
</span>
</td>
<td id="S3.T2.1.5.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.5.5.1.1" class="ltx_p" style="width:104.7pt;">Soccer is my passion.</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.6.6" class="ltx_tr">
<td id="S3.T2.1.6.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T2.1.6.6.1.1" class="ltx_ERROR undefined">\tipaencoding</span>*̊l<sup id="S3.T2.1.6.6.1.2" class="ltx_sup">j</sup>
</td>
<td id="S3.T2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">rzemieślniczy</td>
<td id="S3.T2.1.6.6.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_t">[<span id="S3.T2.1.6.6.3.1" class="ltx_ERROR undefined">\tipaencoding</span><span id="S3.T2.1.6.6.3.2" class="ltx_ERROR undefined">\textsecstress</span><span id="S3.T2.1.6.6.3.3" class="ltx_ERROR undefined">\textyogh</span>\̃textepsilonm<sup id="S3.T2.1.6.6.3.4" class="ltx_sup">j</sup>j\̇textepsilon<span id="S3.T2.1.6.6.3.5" class="ltx_ERROR undefined">\textctc</span>*̊l<sup id="S3.T2.1.6.6.3.6" class="ltx_sup">j</sup><span id="S3.T2.1.6.6.3.7" class="ltx_ERROR undefined">\textprimstress</span><span id="S3.T2.1.6.6.3.8" class="ltx_ERROR undefined">\textltailn</span>i<span id="S3.T2.1.6.6.3.9" class="ltx_ERROR undefined">\textroundcap</span><span id="S3.T2.1.6.6.3.10" class="ltx_ERROR undefined">\textteshlig</span><span id="S3.T2.1.6.6.3.11" class="ltx_ERROR undefined">\textbari</span>]</td>
<td id="S3.T2.1.6.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T2.1.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.6.4.1.1" class="ltx_p" style="width:104.7pt;">Na targach rzemieślniczych można zobaczyć wiele unikatowych produktów.</span>
</span>
</td>
<td id="S3.T2.1.6.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T2.1.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.6.6.5.1.1" class="ltx_p" style="width:104.7pt;">At craft fairs, you can see many unique products.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Representative sentences from the nEMO datasets that demonstrate the phonetic diversity of Polish, along with their English translations.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.1.   Emotional States</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We chose the simulated approach for the development of the nEMO dataset due to its numerous advantages. This method allowed us to define emotions in a distinct and categorical manner, while also providing control over copyrights.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The nEMO dataset focuses on six basic emotions: anger, fear, happiness, sadness, surprise, and neutral state. These emotions were selected due to their differences in valence, arousal, and dominance, which facilitates their discrimination <cite class="ltx_cite ltx_citemacro_citep">(Plutchik and Kellerman, <a href="#bib.bib8" title="" class="ltx_ref">1980</a>)</cite>. Furthermore, these emotional states are frequently depicted in existing corpora, allowing for an increase in the number of samples for basic emotions.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.2.   Linguistic Content</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In the development of the nEMO dataset, a simulated approach was used where each actor was required to record the same set of utterances for each of the six different emotional states. To ensure optimal duration of the recording sessions, an effort was made to minimize the number of utterances as much as possible. Additionally, the linguistic content was prepared to sufficiently represent the phonetics of the Polish language.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The initial step in preparing linguistic material involved identifying 90 uncommon phonemes present in the Polish language. For each phoneme, a word in which it appears was selected and used in a sentence. This resulted in the creation of 90 sentences, each containing at least one uncommon phoneme.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">It should be noted that all phrases are semantically correct and could be used in everyday conversations. Furthermore, the emotional neutrality of the sentences was not emphasized.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Table <a href="#S3.T2" title="Table 2 ‣ 3. Methodology ‣ nEMO: Dataset of Emotional Speech in Polish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows examples of the sentences used in the recordings.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.3.   Actors</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Nine actors, four female and five male, were involved in the development of the nEMO dataset. They ranged in age from 20 to 30 and were all native speakers of Polish.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Three of the participants were qualified voice actors. To avoid exaggerated emotional expressions, which are common among theatrical performers, individuals without formal acting training were also included.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">The dataset was balanced by strategically incorporating non-professional speakers to reduce the potential for exaggerated emotional portrayals. Although concerns were raised about the adequacy of emotional expression by non-actors, analysis revealed no significant differences between professionals and amateurs, both audibly and in spectral data analysis. To maintain the dataset’s integrity, we excluded any recordings that raised doubts about their emotional authenticity through human evaluation.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">During the recording sessions, actors were given explicit instructions to focus on depicting a single emotional state at a time. Feedback was provided constantly to all participants, particularly aiding those without professional experience. This guidance often included encouragement to use non-verbal cues, such as facial expressions and gestures, to enhance the authenticity of emotional expression. For example, actors were instructed to smile when portraying happiness, in order to enhance the emotional quality of the recordings.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">All participants provided consent for the use and distribution of their voices in the form of audio recordings. This agreement covers the rights to use, preserve, process, and reproduce the audio content. It also includes provisions for unrestricted distribution and public availability under the terms of a Creative Commons license (CC BY-NC-SA 4.0), ensuring ethical and legal use of the dataset in subsequent research.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T3.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.1.1.1.1.1" class="ltx_p" style="width:91.0pt;"><span id="S3.T3.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Emotion</span></span>
</span>
</th>
<th id="S3.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Anger</span></th>
<th id="S3.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">Fear</span></th>
<th id="S3.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T3.1.1.1.4.1" class="ltx_text ltx_font_bold">Happiness</span></th>
<th id="S3.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T3.1.1.1.5.1" class="ltx_text ltx_font_bold">Neutral</span></th>
<th id="S3.T3.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T3.1.1.1.6.1" class="ltx_text ltx_font_bold">Sadness</span></th>
<th id="S3.T3.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T3.1.1.1.7.1" class="ltx_text ltx_font_bold">Surprise</span></th>
<th id="S3.T3.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T3.1.1.1.8.1" class="ltx_text ltx_font_bold">Total</span></th>
</tr>
<tr id="S3.T3.1.2.2" class="ltx_tr">
<th id="S3.T3.1.2.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T3.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.2.2.1.1.1" class="ltx_p" style="width:91.0pt;"><span id="S3.T3.1.2.2.1.1.1.1" class="ltx_text ltx_font_bold">Number of samples</span></span>
</span>
</th>
<th id="S3.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">749</th>
<th id="S3.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">736</th>
<th id="S3.T3.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">749</th>
<th id="S3.T3.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">809</th>
<th id="S3.T3.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">769</th>
<th id="S3.T3.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">669</th>
<th id="S3.T3.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">4,481</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.1.3.1" class="ltx_tr">
<th id="S3.T3.1.3.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_l ltx_border_r">
<span id="S3.T3.1.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.3.1.1.1.1" class="ltx_p" style="width:91.0pt;"><span id="S3.T3.1.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Average length [s]</span></span>
</span>
</th>
<td id="S3.T3.1.3.1.2" class="ltx_td ltx_align_center">2.26</td>
<td id="S3.T3.1.3.1.3" class="ltx_td ltx_align_center">2.49</td>
<td id="S3.T3.1.3.1.4" class="ltx_td ltx_align_center">2.34</td>
<td id="S3.T3.1.3.1.5" class="ltx_td ltx_align_center">2.56</td>
<td id="S3.T3.1.3.1.6" class="ltx_td ltx_align_center">2.69</td>
<td id="S3.T3.1.3.1.7" class="ltx_td ltx_align_center ltx_border_r">2.46</td>
<td id="S3.T3.1.3.1.8" class="ltx_td ltx_align_center ltx_border_r">2.47</td>
</tr>
<tr id="S3.T3.1.4.2" class="ltx_tr">
<th id="S3.T3.1.4.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">
<span id="S3.T3.1.4.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T3.1.4.2.1.1.1" class="ltx_p" style="width:91.0pt;"><span id="S3.T3.1.4.2.1.1.1.1" class="ltx_text ltx_font_bold">Total length [h]</span></span>
</span>
</th>
<td id="S3.T3.1.4.2.2" class="ltx_td ltx_align_center ltx_border_b">0.47</td>
<td id="S3.T3.1.4.2.3" class="ltx_td ltx_align_center ltx_border_b">0.52</td>
<td id="S3.T3.1.4.2.4" class="ltx_td ltx_align_center ltx_border_b">0.48</td>
<td id="S3.T3.1.4.2.5" class="ltx_td ltx_align_center ltx_border_b">0.57</td>
<td id="S3.T3.1.4.2.6" class="ltx_td ltx_align_center ltx_border_b">0.57</td>
<td id="S3.T3.1.4.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.45</td>
<td id="S3.T3.1.4.2.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">3.07</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Quantitative analysis of the nEMO dataset per emotion: number of samples, average sample duration, and total length.</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.4.   Recording Setup</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The recordings were conducted in a home setting to better reflect a natural environment. Each recording session involved one actor and lasted approximately two hours, excluding post-processing or sample evaluation.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">The utterances were captured using Mozos MKIT-900 Pro, a cardioid condenser microphone with a 192 kHz sampling rate, ~42 dB sensitivity, and frequency response from 100 Hz to 18 kHz. The recording equipment also included a sponge and pop filter to eliminate background noise and explosive consonant utterances. As a result, there was no need for any post-processing, as the quality of the recordings was not affected by any external noise. All samples were normalized to a peak amplitude of 0 dB and downsampled to a 24 kHz sampling rate with 16 bits per sample.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.   Final Dataset</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The nEMO dataset underwent human evaluation, and only recordings that accurately captured the intended emotional state were included. The resulting dataset contains 4,481 audio recordings, a total of more than three hours of speech. Table <a href="#S3.T3" title="Table 3 ‣ 3.3. Actors ‣ 3. Methodology ‣ nEMO: Dataset of Emotional Speech in Polish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the distribution of samples per emotional state.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The final version of the nEMO dataset includes a comprehensive set of attributes. These attributes consist of an audio sample, a label corresponding to the emotional state, and both original and normalized transcriptions of the utterance. Additionally, metadata related to the speaker’s ID, gender, and age are included.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">The nEMO dataset is available as a single entity and is not divided into predefined training and test splits. This approach allows researchers and developers to customize the splits according to their specific needs.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">The dataset’s diverse attributes make it a valuable asset for a broad spectrum of speech processing tasks. It was primarily designed to facilitate speech emotion recognition and contains recordings annotated with one of six discrete emotional states: anger, fear, happiness, sadness, surprise, or neutral. Additionally, the dataset includes detailed metadata on the speaker, making it suitable for various audio classification tasks. Furthermore, including both orthographic and normalized transcriptions for each sample enhances the dataset’s utility for automatic speech recognition tasks. The linguistic content has been carefully selected to showcase a broad spectrum of the phonetics of the Polish language. Additionally, the emotional speech recordings, complemented by transcriptions, lay the groundwork for the development of text-to-speech systems capable of generating emotional speech.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.   Evaluation</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">To ensure accurate representation of designated emotional states, three experimental evaluations were conducted using machine learning algorithms. The dataset was randomly divided into training and test sets at an 80:20 ratio. It is important to note that the splits were not speaker-independent.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The experiments utilized three classifiers: Support Vector Machine (<span id="S5.p2.1.1" class="ltx_text ltx_font_bold">SVM</span>), Logistic Regression, and Random Forest. These classifiers leveraged Mel-frequency cepstral coefficients (<span id="S5.p2.1.2" class="ltx_text ltx_font_bold">MFCCs</span>) as input features. From each audio recording, 20 MFCCs were computed and then averaged to obtain a one-dimensional feature vector. This approach, based on basic features and machine learning methods, was chosen to provide interpretable results of the experiments.</p>
</div>
<figure id="S5.F1" class="ltx_figure"><img src="/html/2404.06292/assets/x1.png" id="S5.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="299" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Performance metrics of SVM, Logistic Regression, and Random Forest classifiers on the nEMO dataset.</figcaption>
</figure>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2404.06292/assets/x2.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="484" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Comparison of confusion matrices for SVM, Logistic Regression, and Random Forest classifiers on the nEMO dataset.</figcaption>
</figure>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">The performance of the classifiers is shown in Figure <a href="#S5.F1" title="Figure 1 ‣ 5. Evaluation ‣ nEMO: Dataset of Emotional Speech in Polish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, indicating satisfactory outcomes. Random Forest achieved the highest accuracy at 83.95%. The confusion matrices for all classifiers are shown in Figure <a href="#S5.F2" title="Figure 2 ‣ 5. Evaluation ‣ nEMO: Dataset of Emotional Speech in Polish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">The SVM classifier achieved 59.64% accuracy and a well-balanced precision-recall ratio with a macro F1 score of 58.17%. However, the classifier exhibited a tendency for errors in recognizing the emotional state of <span id="S5.p4.1.1" class="ltx_text ltx_font_italic">surprise</span>. The notably lower recall rate for this emotional state suggests a need for reevaluation of these particular samples.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">The Logistic Regression model produced comparable results to the SVM in terms of accuracy and macro F1 score. It also had problems in classifying <span id="S5.p5.1.1" class="ltx_text ltx_font_italic">surprise</span> accurately.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">The errors made by both models were mainly related to misclassification between emotions with similar levels of arousal, highlighting the complexity of the dataset and the nuanced challenge it poses. In contrast, the Random Forest classifier achieved the highest accuracy and demonstrated consistent performance across all emotional states.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.   Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper presents the development and evaluation of the nEMO dataset, a novel Polish emotional speech corpus. This dataset, which contains recordings of nine actors delivering 90 sentences in six different emotional states, consists of 4,481 samples, representing over three hours of speech data. It includes a wide range of metadata fields, including raw audio recordings, emotional state labels, speaker gender and age, and utterance transcriptions, thereby extending its utility beyond its primary goal for various speech processing tasks.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">The robustness of the dataset was evaluated using three machine learning methods: SVM, Logistic Regression, and Random Forest. The satisfactory performance of these classifiers proves the effectiveness of the dataset in capturing the nuanced phonetics of Polish emotional speech, highlighting its potential as a valuable asset for speech emotion recognition research.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Given the availability of specialized corpora in the field of speech emotion recognition, the nEMO dataset is an important resource aimed at bridging this gap. Future work for the corpus includes extensive development, including further human evaluation and the inclusion of additional recordings from more actors to further increase its diversity and applicability.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">The nEMO dataset is available under a Creative Commons license (CC BY-NC-SA 4.0) on platforms such as the Hugging Face<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://huggingface.co/datasets/amu-cai/nEMO" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/datasets/amu-cai/nEMO</a></span></span></span> website and GitHub<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/amu-cai/nEMO" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/amu-cai/nEMO</a></span></span></span> to enable further research and development in the field of speech emotion recognition. By making nEMO publicly available, we hope to encourage progress in the field and the creation of effective emotion recognition systems.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">7.   Acknowledgments</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">The author would like to thank all the actors whose voices are the essence of the nEMO dataset. Their commitment and dedication were instrumental in capturing the diverse emotional states that this corpus seeks to represent. Their contribution is the foundation of this research.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">Special thanks are due to Dr. Marek Kubis, whose invaluable support and insightful guidance were crucial throughout the research and development of this project.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">8.   Bibliographical References</h2>

<div id="S8.p1" class="ltx_para">
<span id="S8.p1.1" class="ltx_ERROR undefined">\c@NAT@ctr</span>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography"></h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abbaschian et al. (2021)</span>
<span class="ltx_bibblock">
Babak Joze Abbaschian, Daniel Sierra-Sosa, and Adel Said Elmaghraby. 2021.

</span>
<span class="ltx_bibblock">Deep Learning Techniques for Speech Emotion Recognition, from Databases to Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Sensors (Basel, Switzerland)</em>, 21.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burkhardt et al. (2005)</span>
<span class="ltx_bibblock">
Felix Burkhardt, Astrid Paeschke, M. Rolfes, Walter Sendlmeier, and Benjamin Weiss. 2005.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.21437/Interspeech.2005-446" title="" class="ltx_ref ltx_href">A database of German emotional speech</a>.

</span>
<span class="ltx_bibblock">volume 5, pages 1517–1520.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Busso et al. (2008)</span>
<span class="ltx_bibblock">
Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower Provost, Samuel Kim, Jeannette Chang, Sungbok Lee, and Shrikanth Narayanan. 2008.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/s10579-008-9076-6" title="" class="ltx_ref ltx_href">IEMOCAP: Interactive emotional dyadic motion capture database</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Language Resources and Evaluation</em>, 42:335–359.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. (2014)</span>
<span class="ltx_bibblock">
Houwei Cao, David G. Cooper, Michael K. Keutmann, Ruben C. Gur, Ani Nenkova, and Ragini Verma. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TAFFC.2014.2336244" title="" class="ltx_ref ltx_href">CREMA-D: Crowd-Sourced Emotional Multimodal Actors Dataset</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em>, 5(4):377–390.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Latif et al. (2018)</span>
<span class="ltx_bibblock">
Siddique Latif, Adnan Qayyum, Muhammad Usman, and Junaid Qadir. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/FIT.2018.00023" title="" class="ltx_ref ltx_href">Cross Lingual Speech Emotion Recognition: Urdu vs. Western Languages</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">2018 International Conference on Frontiers of Information Technology (FIT)</em>, pages 88–93.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Livingstone and Russo (2018)</span>
<span class="ltx_bibblock">
Steven R. Livingstone and Frank A. Russo. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1371/journal.pone.0196391" title="" class="ltx_ref ltx_href">The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">PLOS ONE</em>, 13(5):1–35.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pichora-Fuller and Dupuis (2020)</span>
<span class="ltx_bibblock">
M. Kathleen Pichora-Fuller and Kate Dupuis. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.5683/SP2/E8H2MF" title="" class="ltx_ref ltx_href">Toronto emotional speech set (TESS)</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plutchik and Kellerman (1980)</span>
<span class="ltx_bibblock">
Robert Plutchik and Henry Kellerman. 1980.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Emotion: Theory, Research, and Experience</em>, volume 1, pages 4, 16, 89. Academic Press.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuller (2018)</span>
<span class="ltx_bibblock">
Björn W. Schuller. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3129340" title="" class="ltx_ref ltx_href">Speech Emotion Recognition: Two Decades in a Nutshell, Benchmarks, and Ongoing Trends</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Commun. ACM</em>, 61(5):90–99.

</span>
</li>
</ul>
</section>
<div id="p3" class="ltx_para">
<p id="p3.1" class="ltx_p"></p>
</div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.06291" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.06292" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.06292">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.06292" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.06293" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 20:58:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
