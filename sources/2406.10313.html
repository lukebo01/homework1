<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.10313] CNVSRC 2023: The First Chinese Continuous Visual Speech Recognition Challenge</title><meta property="og:description" content="The first Chinese Continuous Visual Speech Recognition Challenge aimed to probe the performance of Large Vocabulary Continuous Visual Speech Recognition (LVC-VSR) on two tasks: (1) Single-speaker VSR for a particular s…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CNVSRC 2023: The First Chinese Continuous Visual Speech Recognition Challenge">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="CNVSRC 2023: The First Chinese Continuous Visual Speech Recognition Challenge">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.10313">

<!--Generated on Fri Jul  5 20:14:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=1]ChenChen
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=2]ZehuaLiu
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=2]XiaolouLi
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>[affiliation=2]LantianLi
<span id="p1.3.4" class="ltx_ERROR undefined">\name</span>[affiliation=1]DongWang




</p>
</div>
<h1 class="ltx_title ltx_title_document">CNVSRC 2023: The First Chinese Continuous Visual Speech Recognition Challenge</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id6.id1" class="ltx_p">The first Chinese Continuous Visual Speech Recognition Challenge aimed to probe the performance of Large Vocabulary Continuous Visual Speech Recognition (LVC-VSR) on two tasks: (1) Single-speaker VSR for a particular speaker and (2) Multi-speaker VSR for a set of registered speakers.
The challenge yielded highly successful results, with the best submission significantly outperforming the baseline, particularly in the single-speaker task.
This paper comprehensively reviews the challenge, encompassing the data profile, task specifications, and baseline system construction.
It also summarises the representative techniques employed by the submitted systems, highlighting the most effective approaches. Additional information and resources about this challenge can be accessed through the official website at <em id="id6.id1.1" class="ltx_emph ltx_font_italic"><a target="_blank" href="http://cnceleb.org/competition" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://cnceleb.org/competition</a></em>.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>CNVSRC, Visual speech recognition, Lip reading, Chinese VSR
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Speech Recognition (VSR), commonly called lip reading, is a technology that utilizes lip movements to infer speech content.
It has broad applications, including public surveillance, support for elderly and disabled individuals, and fake video detection.
Traditionally, VSR has been primarily focused on recognizing isolated words or phrases.
For example, Martinez et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
developed a model that extracts visual features using 3D convolution, ResNet-18, and a multi-scale temporal convolutional network (MS-TCN). This was further enhanced by simple average pooling and a softmax layer for inferring word posteriors, resulting in commendable performance on the LRW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and LRW-1000 datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>,
which are the largest publicly available benchmark datasets for unconstrained isolated word lip-reading in English and Mandarin, respectively.
Ma et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> adopted a similar architecture but introduced a densely connected temporal convolutional network (DC-TCN) to achieve improved performance.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recently, research in lip reading has progressed beyond word and phrase recognition to focus on large vocabulary continuous visual speech recognition (LVC-VSR),
a more challenging task but with more realistic merit.
Significant advancements have been made in English benchmarks, partly attributable to the availability of large-scale English visual-speech datasets such as LRS2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, LRS3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, AVSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, and VoxCeleb <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
While earlier studies addressed this issue using hand-crafted features and sequential models like HMM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> or RNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>,
a significant breakthrough occurred with the end-to-end approach, which processes raw video frames and generates a word sequence. LipNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> is perhaps the first end-to-end model, integrating spatiotemporal convolution layers and bi-directional gated recurrent unit (BGRU), trained using the connectionist temporal classification(CTC) loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
The model underwent testing on GRID, a dataset with limited grammar and vocabulary  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. A similar architecture was adopted by Jeon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, with their approach involving the integration of multiple CNN-based streams into the feature extraction process.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">While promising, the GRID corpus is limited in its ability to capture real-world complexity due to the constrained grammar and vocabulary of the sentences.
A more challenging task involves large vocabulary continuous visual speech recognition based on datasets gathered from online media repositories like YouTube. A seminal work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>introduced the first comprehensive visual speech datasets LRS2 and LRS3, along with the first transformer-based system trained with either the CTC loss (TM-CTC) or the sequence-to-sequence loss (TM-seq2seq).
In a parallel endeavour, Google <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> developed an end-to-end model (CNN/BLSTM backbone and CTC loss) that transcribes videos into phone sequences and utilizes an FST-based decoder to obtain word sequences.
The research was expanded upon in a subsequent study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, which introduced a hybrid CTC/Attention model utilizing a ResNet/Conformer encoder and a Transformer-based language model (LM). This work was further developed by incorporating time-masking data augmentation and an auxiliary reconstruction loss in a subsequent publication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In addition to employing complex structures, a simple yet effective approach is increasing the training data volume. However, a major challenge arises from the dearth of well-labeled data. One promising strategy to address this issue involves utilizing a pre-trained automatic speech recognition (ASR) model to transcribe unlabelled videos. This methodology was extensively explored in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, wherein a ResNet/Conformer model was trained on both unlabelled datasets like
VoxCeleb2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and AVSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>,
as well as text-labelled datasets such as LRW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, LRS2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, and LRS3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
The study revealed significant performance enhancements with auto-labelled data, and further improvements were observed with increased incorporation of unlabelled data.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">For VSR on Chinese data, the research progress has been significantly impeded by the scarcity of data resources.
Despite the presence of LRW-1000 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> as the sole large-scale dataset in Chinese, it consists solely of isolated words.
In 2023, the release of the CN-CVS dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> marked the debut of the first large-scale continuous visual-speech dataset in Chinese, thereby presenting an opportunity to propel research in Chinese LVC-VSR.
This also motivated the first Chinese Continuous Visual Speech Recognition Challenge (CNVSRC 2023), hosted as a special session on NCMMSC 2023 to estimate the performance boundary of existing LVC-VSR techniques with Chinese data
and attracting research interest in this domain.
The CN-CVS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> dataset was used as the primary training data, supplemented by two additional datasets - CNVSRC-Single and CNVSRC-Multi - introduced by the organizers to facilitate system development and evaluation.
The organizers also published the model and code of the baseline systems so that the participants could use them as references when developing their systems.
This paper summarises the challenge, emphasizing the predominant findings gleaned from the submitted systems.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The structure of the rest of this paper is outlined as follows:
Section <a href="#S2" title="2 Tasks and Data ‣ CNVSRC 2023: The First Chinese Continuous Visual Speech Recognition Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> introduces the tasks and data of the challenge.
In Section <a href="#S3" title="3 Baseline System ‣ CNVSRC 2023: The First Chinese Continuous Visual Speech Recognition Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, a comprehensive description of the baseline system is provided, covering the model structure, training strategies, and performance evaluation.
Section <a href="#S4" title="4 CNVSRC 2023 Report ‣ CNVSRC 2023: The First Chinese Continuous Visual Speech Recognition Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> reports the challenge result and summarizes the representative technologies utilized by the participants.
Lastly, the paper is concluded in Section <a href="#S5" title="5 Conclusion ‣ CNVSRC 2023: The First Chinese Continuous Visual Speech Recognition Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Tasks and Data</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Tasks</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The CNVSRC 2023 challenge encompasses two distinct tasks: Single-speaker VSR (T1) and Multi-speaker VSR (T2).
Task T1 emphasizes the performance of large-scale tuning for a specific speaker,
whereas T2 focuses on the fundamental performance of the system for non-specific but <em id="S2.SS1.p1.1.1" class="ltx_emph ltx_font_italic">registered</em> speakers,
i.e., speakers seen in the data for system development.
In both tasks, the system is fed with silent facial videos featuring a single individual, and it is required to generate the spoken content in written form.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Each task is further categorized into a `fixed track' and an `open track'. The fixed track permits the use of data and additional resources that have been agreed upon by the organizing committee.
Conversely, the open track allows participants to employ any resources except the evaluation set.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Character Error Rate (CER) was used as the main metric to evaluate VSR performance, formulated as follows:</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_Math" alttext="CER=\frac{S+D+I}{N}" display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><mrow id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml"><mi id="S2.E1.m1.1.1.2.2" xref="S2.E1.m1.1.1.2.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.2.1" xref="S2.E1.m1.1.1.2.1.cmml">​</mo><mi id="S2.E1.m1.1.1.2.3" xref="S2.E1.m1.1.1.2.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.2.1a" xref="S2.E1.m1.1.1.2.1.cmml">​</mo><mi id="S2.E1.m1.1.1.2.4" xref="S2.E1.m1.1.1.2.4.cmml">R</mi></mrow><mo id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml">=</mo><mfrac id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><mrow id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml"><mi id="S2.E1.m1.1.1.3.2.2" xref="S2.E1.m1.1.1.3.2.2.cmml">S</mi><mo id="S2.E1.m1.1.1.3.2.1" xref="S2.E1.m1.1.1.3.2.1.cmml">+</mo><mi id="S2.E1.m1.1.1.3.2.3" xref="S2.E1.m1.1.1.3.2.3.cmml">D</mi><mo id="S2.E1.m1.1.1.3.2.1a" xref="S2.E1.m1.1.1.3.2.1.cmml">+</mo><mi id="S2.E1.m1.1.1.3.2.4" xref="S2.E1.m1.1.1.3.2.4.cmml">I</mi></mrow><mi id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml">N</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><eq id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"></eq><apply id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2"><times id="S2.E1.m1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.2.1"></times><ci id="S2.E1.m1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.2.2">𝐶</ci><ci id="S2.E1.m1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.2.3">𝐸</ci><ci id="S2.E1.m1.1.1.2.4.cmml" xref="S2.E1.m1.1.1.2.4">𝑅</ci></apply><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><divide id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3"></divide><apply id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2"><plus id="S2.E1.m1.1.1.3.2.1.cmml" xref="S2.E1.m1.1.1.3.2.1"></plus><ci id="S2.E1.m1.1.1.3.2.2.cmml" xref="S2.E1.m1.1.1.3.2.2">𝑆</ci><ci id="S2.E1.m1.1.1.3.2.3.cmml" xref="S2.E1.m1.1.1.3.2.3">𝐷</ci><ci id="S2.E1.m1.1.1.3.2.4.cmml" xref="S2.E1.m1.1.1.3.2.4">𝐼</ci></apply><ci id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">CER=\frac{S+D+I}{N}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS1.p5" class="ltx_para ltx_noindent">
<p id="S2.SS1.p5.4" class="ltx_p">where <math id="S2.SS1.p5.1.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.SS1.p5.1.m1.1a"><mi id="S2.SS1.p5.1.m1.1.1" xref="S2.SS1.p5.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.1.m1.1b"><ci id="S2.SS1.p5.1.m1.1.1.cmml" xref="S2.SS1.p5.1.m1.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.1.m1.1c">S</annotation></semantics></math>, <math id="S2.SS1.p5.2.m2.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S2.SS1.p5.2.m2.1a"><mi id="S2.SS1.p5.2.m2.1.1" xref="S2.SS1.p5.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.2.m2.1b"><ci id="S2.SS1.p5.2.m2.1.1.cmml" xref="S2.SS1.p5.2.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.2.m2.1c">D</annotation></semantics></math>, and <math id="S2.SS1.p5.3.m3.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S2.SS1.p5.3.m3.1a"><mi id="S2.SS1.p5.3.m3.1.1" xref="S2.SS1.p5.3.m3.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.3.m3.1b"><ci id="S2.SS1.p5.3.m3.1.1.cmml" xref="S2.SS1.p5.3.m3.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.3.m3.1c">I</annotation></semantics></math> represent the number of substitutions, deletions, and insertions in the output transcription, respectively.
<math id="S2.SS1.p5.4.m4.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p5.4.m4.1a"><mi id="S2.SS1.p5.4.m4.1.1" xref="S2.SS1.p5.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.4.m4.1b"><ci id="S2.SS1.p5.4.m4.1.1.cmml" xref="S2.SS1.p5.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.4.m4.1c">N</annotation></semantics></math> is the number of characters in the ground truth transcription.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data profile</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">CNVSRC 2023 utilized the CN-CVS dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> in addition to two supplementary datasets: CNVSRC-Single and CNVSRC-Multi, which served as the development and evaluation data for the single-speaker VSR task (T1) and multi-speaker VSR task (T2), respectively.
All the data was transcribed into text. Table <a href="#S2.T1" title="Table 1 ‣ 2.2 Data profile ‣ 2 Tasks and Data ‣ CNVSRC 2023: The First Chinese Continuous Visual Speech Recognition Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the data profile of the three datasets.
Note that both CNVSRC-Single and CNVSRC-Multi were split into a development set and an evaluation set.
The development data was transparent to the participants (including the video, audio, and text), while
the text and audio of the evaluation data were kept secret during the entire challenge process.
The participants could use the development data in any way,
e.g., freely splitting it into a subset for model fine-tuning and a subset for model validation/selection.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Data profile used in CNVSRC 2023.</figcaption>
<div id="S2.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:134.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(64.8pt,-22.4pt) scale(1.49714067954959,1.49714067954959) ;">
<table id="S2.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S2.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">CN-CVS</th>
<td id="S2.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2">CNVSRC-Single</td>
<td id="S2.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">CNVSRC-Multi</td>
</tr>
<tr id="S2.T1.1.1.2.2" class="ltx_tr">
<th id="S2.T1.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">DataSet</th>
<th id="S2.T1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Train</th>
<td id="S2.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">Dev</td>
<td id="S2.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Eval</td>
<td id="S2.T1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">Dev</td>
<td id="S2.T1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">Eval</td>
</tr>
<tr id="S2.T1.1.1.3.3" class="ltx_tr">
<th id="S2.T1.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"># Spks</th>
<th id="S2.T1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">2,557</th>
<td id="S2.T1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">1</td>
<td id="S2.T1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t" colspan="2">43</td>
</tr>
<tr id="S2.T1.1.1.4.4" class="ltx_tr">
<th id="S2.T1.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"># Videos</th>
<th id="S2.T1.1.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">206,261</th>
<td id="S2.T1.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">25,947</td>
<td id="S2.T1.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2,881</td>
<td id="S2.T1.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">20,450</td>
<td id="S2.T1.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t">10,269</td>
</tr>
<tr id="S2.T1.1.1.5.5" class="ltx_tr">
<th id="S2.T1.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"># Hours</th>
<th id="S2.T1.1.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">308.00</th>
<td id="S2.T1.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_bb">94.00</td>
<td id="S2.T1.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">8.41</td>
<td id="S2.T1.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_bb">29.24</td>
<td id="S2.T1.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_bb">14.49</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>CN-CVS</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">The CN-CVS dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> comprises visual-speech data from over 2,557 speakers, totalling more than 300 hours of videos.
It encompasses various scenarios, including news broadcasts and public speeches.
So far, CN-CVS is the largest open-source Chinese visual-speech dataset.
This dataset was used as the primary training data for the CNVSRC 2023 challenge.
Note that the original publication of CN-CVS is for the video-to-speech synthesis (VTS) task and thus does not involve text transcription.
To support the CNVSRC 2023 challenge, we labelled the video with a semi-automatic pipeline that involves ASR transcribing and human check,
as will be presented shortly.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>CNVSRC-Single</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">The CNVSRC-Single dataset, designed for a single-speaker VSR task (T1), is obtained from a broadcaster's online channel.
It includes over 800 speech videos of that broadcaster, with a cumulative duration of over 100 hours.
The pipeline used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> was employed for the collection and processing.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>CNVSRC-Multi</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">The CNVSRC-Multi dataset was designed as the development/evaluation data for the multi-speaker VSR task (T2).
It encompasses two scenarios: reading in a recording studio and speeches downloaded from the internet.</p>
</div>
<div id="S2.SS2.SSS3.p2" class="ltx_para">
<p id="S2.SS2.SSS3.p2.1" class="ltx_p">In the recording studio scenario, facial videos of speakers were captured from three different
camera angles (0°,30°,60°), while their speech audio was recorded using a high-quality microphone.
The speakers were prompted a sentence at each time via a computer screen and were asked to read the sentence clearly with a neutral emotion.
The video data from the <em id="S2.SS2.SSS3.p2.1.1" class="ltx_emph ltx_font_italic">front camera</em> was used in CNVSRC 2023,
which was transcoded to 25 frames per second (FPS) and scaled to an appropriate size.
A total of 23 speakers participated in the recording, each reading 1,000 sentences.
In the speeches from the internet scenario, videos of public speeches from 20 speakers
were collected from the internet, again following the same collection and processing pipeline as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
To ensure the integrity of the collected videos from the internet, a face recognition tool<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://pypi.org/project/face-recognition/</span></span></span> was employed to check whether there is only one face in each video frame
and if the face is the target face.
A manual check was then conducted to double-check that each extracted video only contained the target face.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Text annotation</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">To generate text transcriptions, a Paraformer-based ASR system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> was employed to transcribe the speech of all the videos.
Furthermore, the manual check was conducted to ensure that the CER of the transcriptions remains below 2%.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Baseline System</h2>

<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Training Details of the Pretraining (P1 &amp; P2) and Fine-tuning (FT) steps when constructing the baseline systems.</figcaption>
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:672.1pt;height:145.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(86.9pt,-18.8pt) scale(1.34901908611296,1.34901908611296) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Experiment</th>
<th id="S3.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">P1</th>
<th id="S3.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">P2</th>
<th id="S3.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">FT (Single-Speaker)</th>
<th id="S3.T2.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">FT (Multi-Speaker)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.2.1" class="ltx_tr">
<th id="S3.T2.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Initialize</th>
<td id="S3.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Random</td>
<td id="S3.T2.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">P1 Saved Model</td>
<td id="S3.T2.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">P2 Saved Model</td>
<td id="S3.T2.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">P2 Saved Model</td>
</tr>
<tr id="S3.T2.1.1.3.2" class="ltx_tr">
<th id="S3.T2.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Warmup Epochs</th>
<td id="S3.T2.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">5</td>
<td id="S3.T2.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">5</td>
<td id="S3.T2.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">2</td>
<td id="S3.T2.1.1.3.2.5" class="ltx_td ltx_align_center">2</td>
</tr>
<tr id="S3.T2.1.1.4.3" class="ltx_tr">
<th id="S3.T2.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Learning Rate</th>
<td id="S3.T2.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0.0002</td>
<td id="S3.T2.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">0.001</td>
<td id="S3.T2.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">0.0003</td>
<td id="S3.T2.1.1.4.3.5" class="ltx_td ltx_align_center">0.0002</td>
</tr>
<tr id="S3.T2.1.1.5.4" class="ltx_tr">
<th id="S3.T2.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Training Epochs</th>
<td id="S3.T2.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">75 + Early stop</td>
<td id="S3.T2.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">75</td>
<td id="S3.T2.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">80</td>
<td id="S3.T2.1.1.5.4.5" class="ltx_td ltx_align_center">80</td>
</tr>
<tr id="S3.T2.1.1.6.5" class="ltx_tr">
<th id="S3.T2.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Saved Model</th>
<td id="S3.T2.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Top 10 average</td>
<td id="S3.T2.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Last 10 epochs average</td>
<td id="S3.T2.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Last 5 epochs average</td>
<td id="S3.T2.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_bb">Last 5 epochs average</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Leveraging the state-of-the-art model used in Auto-AVSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, we trained two baseline systems,
one for the single-speaker VSR task (T1) and the other for the multi-speaker VSR task (T2).
Only the datasets provided in this challenge were used. In other words, these systems conform to the specifications of the fixed tracks.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Model structure</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The model structure is duplicated from Auto-AVSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
Specifically, it comprises three components: visual frontend, encoder, and decoder.
As in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, the visual frontend adopts ResNet18 as its backbone, except that the first 2D-CNN layer is replaced with a 3D-CNN layer to capture local spatiotemporal correlation.
The encoder adopts a Conformer structure with 12 layers, while the decoder employs a Transformer structure with 6 layers.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Upon receiving the input video data, the visual frontend performs initial local spatiotemporal feature extraction.
Subsequently, the Conformer encoder further extracts context-dependent features.
A projection layer and a transformer decoder are employed to predict the output class labels.
The entire model was trained with the joint CTC/Attention loss, where the CTC loss is back-propagated through the projection layer,
while the Attention loss is back-propagated through the decoder.
Refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> for details.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data pre-processing</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The provided datasets of the challenge include the faces of the target speakers,
so a pre-processing pipeline was designed to extract the lip region.
Note that the pipeline was equally applied to both the training data (CN-CVS)
and the development/evaluation data (CNVSRC-Single and CNVSRC-Multi).</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Initially, we utilized RetinaFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> to detect the facial regions in each frame.
Subsequently, FAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> was employed to extract facial landmarks,
with which each detected face was aligned with a mean reference face.
Finally, we extracted the lip region through the alignment for each video frame, which served as the input to the visual frontend.
The modelling units are subword tokens, and the tokenizer was trained using the SentencePiece <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> tool in the unigram mode. During the training of this tokenizer, only the text data from CN-CVS was utilized.
Subsequently, we employed this tokenizer to process the CN-CVS, CNVSRC-Single, and CNVSRC-Multi datasets,
obtaining the token sequences used to train the recognition models.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Training strategy</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We followed a two-step process to build the baseline systems.
Firstly, we performed pre-training with the CN-CVS dataset.
Subsequently, the development set was split into an `adaptation set' and a `validation set',
with a ratio of 8:1 for the single-speaker dataset and 3:1 for the multi-speaker dataset.
The adaptation set was used to fine-tune the pre-trained model, while the validation set was used to select the appropriate checkpoint.
The training process was summarized in Table <a href="#S3.T2" title="Table 2 ‣ 3 Baseline System ‣ CNVSRC 2023: The First Chinese Continuous Visual Speech Recognition Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, and the details are as follows.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The initial training phase (P1) selected CN-CVS videos with a duration of less than 4 seconds to train an `easy model'.
The maximum number of training epochs was 75, and early stopping was triggered if the model's performance on the validation set started to drop. Once the training stopped, we selected the
top 10 models based on their accuracy on the validation set, averaged their parameters to obtain the P1 model.
Next, a full pre-training phase (P2) was evoked using the complete CN-CVS dataset, and the training was conducted for 75 epochs.
Note that a warmup stage of 5 epochs was designed, by which the learning rate was gradually increased from 0 to 0.001. The average of the models of the last 10 epochs was used as the P2 model, i.e., the pre-trained model.
</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">The fine-tuning step started from the P2 model and ran 80 epochs, including 2 epochs of warmup.
This process is the same for the models trained for the single-speaker task and the multi-speaker task,
but there are indeed some differences.
Besides the learning rate (see Table <a href="#S3.T2" title="Table 2 ‣ 3 Baseline System ‣ CNVSRC 2023: The First Chinese Continuous Visual Speech Recognition Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), the most notable difference is that for the single-speaker model,
we randomized the parameters of the classification layer of the P2 model,
to provide sufficient space for the adaptation with the large amount of single-speaker data.
The average of the models of the last 5 epochs was used as the final model,
for both the single-speaker task and the multi-speaker task.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Performance</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The performance of the baseline models was evaluated on the respective validation set and evaluation set for both the single-speaker task and multi-speaker task, using the TorchMetrics tool<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://lightning.ai/docs/torchmetrics/stable/</span></span></span>.
The CER results are as shown in Table <a href="#S3.T3" title="Table 3 ‣ 3.4 Performance ‣ 3 Baseline System ‣ CNVSRC 2023: The First Chinese Continuous Visual Speech Recognition Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance of the baseline systems.</figcaption>
<div id="S3.T3.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:126.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(84.2pt,-27.3pt) scale(1.75860728843821,1.75860728843821) ;">
<table id="S3.T3.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.3.1.1.1" class="ltx_tr">
<th id="S3.T3.3.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S3.T3.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S3.T3.3.1.1.1.2.1" class="ltx_text" style="font-size:80%;">Character Error Rate</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.3.1.2.1" class="ltx_tr">
<th id="S3.T3.3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T3.3.1.2.1.1.1" class="ltx_text" style="font-size:80%;">Task</span></th>
<td id="S3.T3.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.3.1.2.1.2.1" class="ltx_text" style="font-size:80%;">T1: Single-speaker VSR</span></td>
<td id="S3.T3.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.3.1.2.1.3.1" class="ltx_text" style="font-size:80%;">T2: Multi-speaker VSR</span></td>
</tr>
<tr id="S3.T3.3.1.3.2" class="ltx_tr">
<th id="S3.T3.3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T3.3.1.3.2.1.1" class="ltx_text" style="font-size:80%;">Valid</span></th>
<td id="S3.T3.3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.3.1.3.2.2.1" class="ltx_text" style="font-size:80%;">48.57%</span></td>
<td id="S3.T3.3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.3.1.3.2.3.1" class="ltx_text" style="font-size:80%;">58.77%</span></td>
</tr>
<tr id="S3.T3.3.1.4.3" class="ltx_tr">
<th id="S3.T3.3.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S3.T3.3.1.4.3.1.1" class="ltx_text" style="font-size:80%;">Eval</span></th>
<td id="S3.T3.3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T3.3.1.4.3.2.1" class="ltx_text" style="font-size:80%;">48.60%</span></td>
<td id="S3.T3.3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T3.3.1.4.3.3.1" class="ltx_text" style="font-size:80%;">58.37%</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Leaderboard of CNVSRC2023. Team ID and CER are reported.</figcaption>
<table id="S3.T4.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T4.3.1.1" class="ltx_tr">
<th id="S3.T4.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S3.T4.3.1.1.1.1" class="ltx_text" style="font-size:80%;">Task</span></th>
<td id="S3.T4.3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="4"><span id="S3.T4.3.1.1.2.1" class="ltx_text" style="font-size:80%;">T1: Single-speaker VSR</span></td>
<td id="S3.T4.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S3.T4.3.1.1.3.1" class="ltx_text" style="font-size:80%;">T2: Multi-speaker VSR</span></td>
</tr>
<tr id="S3.T4.3.2.2" class="ltx_tr">
<th id="S3.T4.3.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T4.3.2.2.1.1" class="ltx_text" style="font-size:80%;">Track</span></th>
<td id="S3.T4.3.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S3.T4.3.2.2.2.1" class="ltx_text" style="font-size:80%;">Fixed Track</span></td>
<td id="S3.T4.3.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S3.T4.3.2.2.3.1" class="ltx_text" style="font-size:80%;">Open Track</span></td>
<td id="S3.T4.3.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S3.T4.3.2.2.4.1" class="ltx_text" style="font-size:80%;">Fixed Track</span></td>
<td id="S3.T4.3.2.2.5" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S3.T4.3.2.2.5.1" class="ltx_text" style="font-size:80%;">Open Track</span></td>
</tr>
<tr id="S3.T4.3.3.3" class="ltx_tr">
<th id="S3.T4.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T4.3.3.3.1.1" class="ltx_text" style="font-size:80%;">Baseline</span></th>
<td id="S3.T4.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S3.T4.3.3.3.2.1" class="ltx_text" style="font-size:80%;">48.60%</span></td>
<td id="S3.T4.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S3.T4.3.3.3.3.1" class="ltx_text" style="font-size:80%;">48.60%</span></td>
<td id="S3.T4.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S3.T4.3.3.3.4.1" class="ltx_text" style="font-size:80%;">58.37%</span></td>
<td id="S3.T4.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t" colspan="2"><span id="S3.T4.3.3.3.5.1" class="ltx_text" style="font-size:80%;">58.37%</span></td>
</tr>
<tr id="S3.T4.3.4.4" class="ltx_tr">
<th id="S3.T4.3.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T4.3.4.4.1.1" class="ltx_text" style="font-size:80%;">Rank 1</span></th>
<td id="S3.T4.3.4.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T4.3.4.4.2.1" class="ltx_text" style="font-size:80%;">T237</span></td>
<td id="S3.T4.3.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.3.4.4.3.1" class="ltx_text" style="font-size:80%;">34.76%</span></td>
<td id="S3.T4.3.4.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T4.3.4.4.4.1" class="ltx_text" style="font-size:80%;">T237</span></td>
<td id="S3.T4.3.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.3.4.4.5.1" class="ltx_text" style="font-size:80%;">34.76%</span></td>
<td id="S3.T4.3.4.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T4.3.4.4.6.1" class="ltx_text" style="font-size:80%;">T244</span></td>
<td id="S3.T4.3.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.3.4.4.7.1" class="ltx_text" style="font-size:80%;">53.68%</span></td>
<td id="S3.T4.3.4.4.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T4.3.4.4.8.1" class="ltx_text" style="font-size:80%;">T237</span></td>
<td id="S3.T4.3.4.4.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T4.3.4.4.9.1" class="ltx_text" style="font-size:80%;">41.05%</span></td>
</tr>
<tr id="S3.T4.3.5.5" class="ltx_tr">
<th id="S3.T4.3.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T4.3.5.5.1.1" class="ltx_text" style="font-size:80%;">Rank 2</span></th>
<td id="S3.T4.3.5.5.2" class="ltx_td ltx_align_center"><span id="S3.T4.3.5.5.2.1" class="ltx_text" style="font-size:80%;">T266</span></td>
<td id="S3.T4.3.5.5.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T4.3.5.5.3.1" class="ltx_text" style="font-size:80%;">38.09%</span></td>
<td id="S3.T4.3.5.5.4" class="ltx_td"></td>
<td id="S3.T4.3.5.5.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T4.3.5.5.6" class="ltx_td ltx_align_center"><span id="S3.T4.3.5.5.6.1" class="ltx_text" style="font-size:80%;">T267</span></td>
<td id="S3.T4.3.5.5.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T4.3.5.5.7.1" class="ltx_text" style="font-size:80%;">54.56%</span></td>
<td id="S3.T4.3.5.5.8" class="ltx_td ltx_align_center"><span id="S3.T4.3.5.5.8.1" class="ltx_text" style="font-size:80%;">T244</span></td>
<td id="S3.T4.3.5.5.9" class="ltx_td ltx_align_center"><span id="S3.T4.3.5.5.9.1" class="ltx_text" style="font-size:80%;">53.68%</span></td>
</tr>
<tr id="S3.T4.3.6.6" class="ltx_tr">
<th id="S3.T4.3.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T4.3.6.6.1.1" class="ltx_text" style="font-size:80%;">Rank 3</span></th>
<td id="S3.T4.3.6.6.2" class="ltx_td ltx_align_center"><span id="S3.T4.3.6.6.2.1" class="ltx_text" style="font-size:80%;">T290</span></td>
<td id="S3.T4.3.6.6.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T4.3.6.6.3.1" class="ltx_text" style="font-size:80%;">39.47%</span></td>
<td id="S3.T4.3.6.6.4" class="ltx_td"></td>
<td id="S3.T4.3.6.6.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T4.3.6.6.6" class="ltx_td"></td>
<td id="S3.T4.3.6.6.7" class="ltx_td ltx_border_r"></td>
<td id="S3.T4.3.6.6.8" class="ltx_td"></td>
<td id="S3.T4.3.6.6.9" class="ltx_td"></td>
</tr>
<tr id="S3.T4.3.7.7" class="ltx_tr">
<th id="S3.T4.3.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T4.3.7.7.1.1" class="ltx_text" style="font-size:80%;">Rank 4</span></th>
<td id="S3.T4.3.7.7.2" class="ltx_td ltx_align_center"><span id="S3.T4.3.7.7.2.1" class="ltx_text" style="font-size:80%;">T238</span></td>
<td id="S3.T4.3.7.7.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T4.3.7.7.3.1" class="ltx_text" style="font-size:80%;">40.52%</span></td>
<td id="S3.T4.3.7.7.4" class="ltx_td"></td>
<td id="S3.T4.3.7.7.5" class="ltx_td ltx_border_r"></td>
<td id="S3.T4.3.7.7.6" class="ltx_td"></td>
<td id="S3.T4.3.7.7.7" class="ltx_td ltx_border_r"></td>
<td id="S3.T4.3.7.7.8" class="ltx_td"></td>
<td id="S3.T4.3.7.7.9" class="ltx_td"></td>
</tr>
<tr id="S3.T4.3.8.8" class="ltx_tr">
<th id="S3.T4.3.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S3.T4.3.8.8.1.1" class="ltx_text" style="font-size:80%;">Rank 5</span></th>
<td id="S3.T4.3.8.8.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.3.8.8.2.1" class="ltx_text" style="font-size:80%;">T267</span></td>
<td id="S3.T4.3.8.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T4.3.8.8.3.1" class="ltx_text" style="font-size:80%;">41.62%</span></td>
<td id="S3.T4.3.8.8.4" class="ltx_td ltx_border_bb"></td>
<td id="S3.T4.3.8.8.5" class="ltx_td ltx_border_bb ltx_border_r"></td>
<td id="S3.T4.3.8.8.6" class="ltx_td ltx_border_bb"></td>
<td id="S3.T4.3.8.8.7" class="ltx_td ltx_border_bb ltx_border_r"></td>
<td id="S3.T4.3.8.8.8" class="ltx_td ltx_border_bb"></td>
<td id="S3.T4.3.8.8.9" class="ltx_td ltx_border_bb"></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>CNVSRC 2023 Report</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Leaderboard</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">CNVSRC 2023 received 10 valid submissions from 6 teams.
Most teams chose to submit their results to the single-speaker task,
suggesting that single-speaker VSR is more suitable for the current stage of technical development, and multi-speaker VSR is over-challenging. The leaderboard is reported in Table <a href="#S3.T4" title="Table 4 ‣ 3.4 Performance ‣ 3 Baseline System ‣ CNVSRC 2023: The First Chinese Continuous Visual Speech Recognition Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Overall, T237 achieved the best performance in 3/4 of the tasks &amp; tracks,
and their results outperformed the baseline systems by a large margin.
Their proposed system consists of a ResNet-3D visual frontend, an E-Branchformer encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, and a Transformer decoder.
The Chinese characters were used as the modelling units, and multiple data augmentation methods, including speed perturbation, random rotation, and horizontal flipping, were applied during training.
Additionally, a ROVER-based system fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> was performed during the inference procedure.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Technical summary</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We summarize here the promising techniques demonstrated by the results of the submissions, highlighting the most effective methods in <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">bold font</span>.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Data pre-processing</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">Many teams adhered to the baseline system for data pre-processing. A notable observation is that T237 extracted lip regions of varying sizes and resolutions as inputs to their model and discovered that <span id="S4.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_bold">larger regions (lip + mouth around)</span> yielded clear performance improvement.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Data augmentation</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">The participants extensively used data augmentation techniques, including random erase, random crop, random flip, and adaptive time masking.
Notably, <span id="S4.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_bold">speed perturbation</span> and <span id="S4.SS2.SSS2.p1.1.2" class="ltx_text ltx_font_bold">generative data augmentation</span> were reported to yield unexceptionally remarkable results.
Speed perturbation adjusts the speed of the original videos by a factor ranging from 0.9 to 1.1.
T237 and T238 observed notable enhancements in model performance (4.86% relative improvement) by applying speed perturbation.
Generative data augmentation involves generating speech-driven lip videos, hence producing extra video-text training pairs.
T238 utilized facial images from CNVSRC-Single and speech from CN-CVS and CNVSRC-Single to create an extra set of video-text pairs and reported a relative CER reduction of 6.98%.
</p>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Model structure</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">Most of the participating teams adopted the model architecture of the baseline systems, though some teams chose a more complicated backbone to pursue better performance.
For instance, T237 achieved superior results using a <span id="S4.SS2.SSS3.p1.1.1" class="ltx_text ltx_font_bold">ResNet3D</span> structure. Moreover, T237 employed two advanced encoder structures: Branchformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and <span id="S4.SS2.SSS3.p1.1.2" class="ltx_text ltx_font_bold">E-Branchformer</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. All these advanced structures lead to notable performance improvements.
T266 introduced an <span id="S4.SS2.SSS3.p1.1.3" class="ltx_text ltx_font_bold">inner CTC residual module</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> that resides in the Conformer block of the encoder.
This module back-propagated a CTC loss through the shallow layers thus facilitating more effective parameter updates for the shallow layers of the model.
Furthermore, taking inspiration from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, T266 utilized a <span id="S4.SS2.SSS3.p1.1.4" class="ltx_text ltx_font_bold">bi-transformer</span> structure to construct the decoder.
This modification enhances the model's ability to capture contextual information from both the past and future segments.</p>
</div>
</section>
<section id="S4.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4 </span>Modeling units</h4>

<div id="S4.SS2.SSS4.p1" class="ltx_para">
<p id="S4.SS2.SSS4.p1.1" class="ltx_p">Most participating teams used <span id="S4.SS2.SSS4.p1.1.1" class="ltx_text ltx_font_bold">Chinese characters as the modelling units</span>, and showed better performance than the subword tokens used by the baseline systems.
In addition to subword tokens, T238 used <span id="S4.SS2.SSS4.p1.1.2" class="ltx_text ltx_font_bold">phonemes as supplementary modelling units</span> and designed a separate decoder for phoneme recognition. This approach achieved performance improvement, for which a hypothesis is that phonemes contain less semantic variation and thus are more closely related to lip movement, which may stabilize the training, especially in the early stage.</p>
</div>
</section>
<section id="S4.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.5 </span>Cross-modality modeling</h4>

<div id="S4.SS2.SSS5.p1" class="ltx_para">
<p id="S4.SS2.SSS5.p1.1" class="ltx_p">Some teams designed various approaches to <span id="S4.SS2.SSS5.p1.1.1" class="ltx_text ltx_font_bold">leverage the cross-modal dependency</span>.
T290 invented an ASR-VSR joint system that forces the video representations to predict not only the text labels but also the speech representations produced by the middle layer of the ASR system.
Following the same inspiration, T244 trained an audio-visual recognition system where the ASR and VSR have their respective independent encoders and decoders, and an AVSR decoder is constructed on top of the fused ASR and VSR features.</p>
</div>
</section>
<section id="S4.SS2.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.6 </span>Decoding strategy</h4>

<div id="S4.SS2.SSS6.p1" class="ltx_para">
<p id="S4.SS2.SSS6.p1.1" class="ltx_p">Several teams integrated RNN-based or Transformer-based language models to enhance the decoder, and mild performance improvement was reported.
Moreover, system fusion was widely employed by teams to improve the performance of their systems.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This paper comprehensively details the inaugural Chinese Continuous Visual Speech Recognition Challenge (CNVSRC 2023).
A key motivation of the challenge is to investigate the performance bound of VSR under the present data resource, e.g., 300 hours of training data from 2,557 speakers. The overall results suggest that the performance is far from satisfactory, even for the single-speaker scenario where about 100 hours of video is available for one person. The poor performance is certainly attributed to the lack of data, but whether it is related to the special linguistic properties of Chinese, e.g., the ubiquitous homophones, is unknown.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Based on these technical reports from participants, we have summarized the key techniques that might be crucial in constructing Chinese VSR systems. The most effective methods, ordered by their merit in terms of CER reduction: <span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Chinese characters as modelling units</span>, <span id="S5.p2.1.2" class="ltx_text ltx_font_bold">rich data augmentation</span>, <span id="S5.p2.1.3" class="ltx_text ltx_font_bold">fully 3D-CNN visual frontend</span>, <span id="S5.p2.1.4" class="ltx_text ltx_font_bold">cross-modality modelling</span>, <span id="S5.p2.1.5" class="ltx_text ltx_font_bold">system fusion</span>.
Leveraging the technical insights provided by the participants, we have established a cutting-edge benchmark for Chinese LVC-VSR. We aspire that these resources will strengthen the burgeoning field of LVC-VSR research.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
B. Martinez, P. Ma, S. Petridis, and M. Pantic, ``Lipreading using temporal convolutional networks,'' in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>.   IEEE, 2020, pp. 6319–6323.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. S. Chung and A. Zisserman, ``Lip reading in the wild,'' in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Asian Conference on Computer Vision (ACCV)</em>.   Springer, 2017, pp. 87–103.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. Yang, Y. Zhang, D. Feng, M. Yang, C. Wang, J. Xiao, K. Long, S. Shan, and X. Chen, ``Lrw-1000: A naturally-distributed large-scale benchmark for lip reading in the wild,'' in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG)</em>.   IEEE, 2019, pp. 1–8.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
P. Ma, Y. Wang, J. Shen, S. Petridis, and M. Pantic, ``Lip-reading with densely connected temporal convolutional networks,'' in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2021, pp. 2857–2866.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
T. Afouras, J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman, ``Deep audio-visual speech recognition,'' <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 44, no. 12, pp. 8717–8727, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Hassidim, W. T. Freeman, and M. Rubinstein, ``Looking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation,'' <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Graphics</em>, vol. 37, no. 4, pp. 1–11, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Nagrani, J. S. Chung, and A. Zisserman, ``Voxceleb: a large-scale speaker identification dataset,'' in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>.   ISCA, 2017.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. Chung, A. Nagrani, and A. Zisserman, ``Voxceleb2: Deep speaker recognition,'' in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>.   ISCA, 2018.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A. J. Goldschen, O. N. Garcia, and E. D. Petajan, ``Continuous automatic speech recognition by lipreading,'' in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Motion-Based Recognition</em>.   Springer, 1997, pp. 321–343.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S. Petridis and M. Pantic, ``Deep complementary bottleneck features for visual speech recognition,'' in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>.   IEEE, 2016, pp. 2304–2308.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Y. M. Assael, B. Shillingford, S. Whiteson, and N. De Freitas, ``Lipnet: End-to-end sentence-level lipreading,'' <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1611.01599</em>, 2016.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber, ``Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,'' in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine learning (ICML)</em>, 2006, pp. 369–376.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M. Cooke, J. Barker, S. Cunningham, and X. Shao, ``An audio-visual corpus for speech perception and automatic speech recognition,'' <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">The Journal of the Acoustical Society of America</em>, vol. 120, no. 5, pp. 2421–2424, 2006.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
S. Jeon, A. Elsharkawy, and M. S. Kim, ``Lipreading architecture based on multiple convolutional neural networks for sentence-level visual speech recognition,'' <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, vol. 22, no. 1, p. 72, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
B. Shillingford, Y. Assael, M. W. Hoffman, T. Paine, C. Hughes, U. Prabhu, H. Liao, H. Sak, K. Rao, L. Bennett <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Large-scale visual speech recognition,'' <em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.05162</em>, 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
P. Ma, S. Petridis, and M. Pantic, ``End-to-end audio-visual speech recognition with conformers,'' in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>.   IEEE, 2021, pp. 7613–7617.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
P. Ma, S. Petridis, and M. Pantic, ``Visual speech recognition for multiple languages in the wild,'' <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Nature Machine Intelligence</em>, vol. 4, no. 11, pp. 930–939, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
P. Ma, A. Haliassos, A. Fernandez-Lopez, H. Chen, S. Petridis, and M. Pantic, ``Auto-avsr: Audio-visual speech recognition with automatic labels,'' in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
C. Chen, D. Wang, and T. F. Zheng, ``Cn-cvs: A mandarin audio-visual dataset for large vocabulary continuous visual to speech synthesis,'' in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Z. Gao, S. Zhang, I. McLoughlin, and Z. Yan, ``Paraformer: Fast and accurate parallel transformer for non-autoregressive end-to-end speech recognition,'' in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>.   ISCA, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J. Deng, J. Guo, Y. Zhou, J. Yu, I. Kotsia, and S. Zafeiriou, ``Retinaface: Single-stage dense face localisation in the wild,'' <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.00641</em>, 2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A. Bulat and G. Tzimiropoulos, ``How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks),'' in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Computer Vision (ICCV)</em>, 2017, pp. 1021–1030.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
T. Kudo and J. Richardson, ``Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,'' in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2018, pp. 66–71.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
K. Kim, F. Wu, Y. Peng, J. Pan, P. Sridhar, K. J. Han, and S. Watanabe, ``E-branchformer: Branchformer with enhanced merging for speech recognition,'' in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">IEEE Spoken Language Technology Workshop (SLT)</em>.   IEEE, 2023, pp. 84–91.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
J. G. Fiscus, ``A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (rover),'' in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">IEEE Workshop on Automatic Speech Recognition and Understanding Proceedings</em>.   IEEE, 1997, pp. 347–354.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Y. Peng, S. Dalmia, I. Lane, and S. Watanabe, ``Branchformer: Parallel mlp-attention architectures to capture local and global context for speech recognition and understanding,'' in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning (ICML)</em>.   PMLR, 2022, pp. 17 627–17 643.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J. Nozaki and T. Komatsu, ``Relaxing the Conditional Independence Assumption of CTC-Based ASR by Conditioning on Intermediate Predictions,'' in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, 2021, pp. 3735–3739.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
J. Lee and S. Watanabe, ``Intermediate loss regularization for ctc-based speech recognition,'' in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>.   IEEE, 2021, pp. 6224–6228.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
M. Burchi and R. Timofte, ``Audio-visual efficient conformer for robust speech recognition,'' in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2023, pp. 2258–2267.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
D. Wu, B. Zhang, C. Yang, Z. Peng, W. Xia, X. Chen, and X. Lei, ``U2++: Unified two-pass bidirectional end-to-end model for speech recognition,'' <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.05642</em>, 2021.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
B. Zhang, D. Wu, Z. Peng, X. Song, Z. Yao, H. Lv, L. Xie, C. Yang, F. Pan, and J. Niu, ``Wenet 2.0: More productive end-to-end speech recognition toolkit,'' <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.15455</em>, 2022.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.10311" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.10313" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.10313">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.10313" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.10316" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 20:14:56 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
