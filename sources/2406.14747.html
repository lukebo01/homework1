<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.14747] An Adapter-Based Unified Model for Multiple Spoken Language Processing Tasks</title><meta property="og:description" content="Self-supervised learning models have revolutionized the field of speech processing. However, the process of fine-tuning these models on downstream tasks requires substantial computational resources, particularly when d…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="An Adapter-Based Unified Model for Multiple Spoken Language Processing Tasks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="An Adapter-Based Unified Model for Multiple Spoken Language Processing Tasks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.14747">

<!--Generated on Fri Jul  5 20:54:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\affiliations</span>
<p id="p1.2" class="ltx_p"><sup id="p1.2.1" class="ltx_sup">1</sup>National University of Singapore, <sup id="p1.2.2" class="ltx_sup">2</sup>Naver Labs Europe
<span id="p1.2.3" class="ltx_ERROR undefined">\website</span>
<span id="p1.2.4" class="ltx_ERROR undefined">\websiteref</span>
<span id="p1.2.5" class="ltx_ERROR undefined">\contributions</span>
<span id="p1.2.6" class="ltx_ERROR undefined">\teasercaption</span></p>
</div>
<h1 class="ltx_title ltx_title_document">An Adapter-Based Unified Model for Multiple Spoken Language Processing Tasks</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Self-supervised learning models have revolutionized the field of speech processing. However, the process of fine-tuning these models on downstream tasks requires substantial computational resources, particularly when dealing with multiple speech-processing tasks. In this paper, we explore the potential of adapter-based fine-tuning in developing a unified model capable of effectively handling multiple spoken language processing tasks. The tasks we investigate are Automatic Speech Recognition, Phoneme Recognition, Intent Classification, Slot Filling, and Spoken Emotion Recognition. We validate our approach through a series of experiments on the SUPERB benchmark, and our results indicate that adapter-based fine-tuning enables a single encoder-decoder model to perform multiple speech processing tasks with an average improvement of 18.4 % across the five target tasks while staying efficient in terms of parameter updates.</p>
</div>
<section id="S1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The fine-tuning of self-supervised learning (SSL) models, such as wav2vec 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, has improved the performance of Spoken Language Processing (SLP) tasks. However, as the quality of representations generated by these models improves, there is a corresponding increase in their size, necessitating additional storage and computational resources. This issue becomes particularly pronounced when dealing with multiple speech-processing tasks, with each target task requiring separate model fine-tuning, further increasing the need for resources.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Modular architectures, such as adapters, have been widely used in NLP to tackle both parameter efficiency and multi-tasking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. While adapter-based fine-tuning has been utilized in speech-related tasks, such as speech translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and domain adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, its efficiency in developing a unified model capable of handling multiple Spoken Language Processing (SLP) tasks remains relatively unexplored. Existing attempts to model multiple SLP tasks with a single model utilises task-specific decoders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. However, this approach becomes less scalable as the number of tasks increases.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we aim to develop a scalable and parameter-efficient unified encoder-decoder model to effectively handle multiple spoken language processing (SLP) tasks. For this, we use adapters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, which allows new tasks to be added without the need to re-train the entire model and which also mitigates the need for dedicated decoders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Moreover, since adapters facilitate Multi-Task Learning (MTL), we investigate two approaches: Stacking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and Fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, in addition to single-task adapters. To evaluate our approach, we choose five speech-processing tasks from the SUPERB benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>: Automatic Speech Recognition (ASR), Phoneme Recognition (PR), Intent Classification (IC), Slot Filing (SF), and Spoken Emotion Recognition (ER). The detailed model description is provided in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ An Adapter-Based Unified Model for Multiple Spoken Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. From our experiments, we observed that adapter-based fine-tuning outperformed the SUPERB benchmark with an average improvement of 18.4 % achieved across 5 target tasks. We summarise our contributions below:</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2406.14747/assets/model.jpeg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.3.1" class="ltx_text ltx_font_bold">Left</span>: Overall model architecture with a unified encoder-decoder model with adapter-based task modules on each transformer layer. <span id="S1.F1.4.2" class="ltx_text ltx_font_bold">Right</span>: Three types of adapter-based task modules used.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We investigate the feasibility and efficiency of using adapters to build a unified encoder-decoder model that can tackle multiple spoken language processing tasks in a simple and scalable manner.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We explore multi-task learning within our unified framework with two methods: stacking and fusion, which combine adapters to enhance the performance of positively correlated tasks.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In the field of NLP, researchers have used a single model to handle multiple tasks and adapt them to different domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. In the speech domain, most approaches that deal with multiple tasks fall under multi-task models. They either focus on improving a primary task by using auxiliary tasks, like performing ASR to enhance Emotion Recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, or simultaneously perform multiple tasks —slot-filling and intent classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, ASR and speech translation
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> etc. However, these approaches are not easily scalable for new tasks and are mostly applied for tasks that are known to be positively correlated.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Some studies have aimed to create a unified model for multiple speech-processing tasks by training different modules and composing them to perform each task. These architectures comprise encoders and decoders trained to capture features from different modalities, such as text and speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, or speech characteristics like prosody <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> etc. In contrast, our approach focuses on constructing task-specific modular architectures. Furthermore, adding a task is straightforward as these task-specific modules are trained independently.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Our work is inspired by SUPERB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, where multiple tasks are modeled using pre-trained frozen encoders (such as wav2vec 2.0) and task-specific decoders, the task performance relying on the type of decoder used for the task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. However, this approach does not scale well as the number of tasks increases. Instead, in our work, we aim to develop a single encoder-decoder model and show that adapters on the decoder side help us adapt to different types of tasks (both classification and generative) without the need for dedicated decoders.</p>
</div>
</section>
<section id="S3" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Model Architecture</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Pre-trained Encoder-Decoder model</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We use wav2vec 2.0-large<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://huggingface.co/facebook/wav2vec2-large-lv60" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/facebook/wav2vec2-large-lv60</a></span></span></span> as encoder and a 6-layer transformer decoder which is randomly initialized. We fine-tuned this encoder-decoder model on the LibriSpeech 100-hr dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> for the ASR task using hybrid CTC/attention objective <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> as the base model for our unified model. We use SentencePiece (BPE) vocabulary of size 5000. This base model achieved a word error rate (WER) of 3.54 on the test-clean split of LibriSpeech which is comparable to the 3.1 WER reported in the SUPERB benchmark.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:66.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-138.4pt,23.5pt) scale(0.584967751240677,0.584967751240677) ;">
<table id="S3.T1.7.7" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.7.7.8.1" class="ltx_tr">
<th id="S3.T1.7.7.8.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t"></th>
<th id="S3.T1.7.7.8.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" colspan="2">LibriSpeech</th>
<th id="S3.T1.7.7.8.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">IEMOCAP</th>
<th id="S3.T1.7.7.8.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" colspan="2">SNIPS</th>
<th id="S3.T1.7.7.8.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">FSC</th>
<th id="S3.T1.7.7.8.1.6" class="ltx_td ltx_th ltx_th_column ltx_border_t"></th>
</tr>
<tr id="S3.T1.7.7.7" class="ltx_tr">
<th id="S3.T1.7.7.7.8" class="ltx_td ltx_th ltx_th_column ltx_border_r"></th>
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S3.T1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.1.1.1.1.1.2" class="ltx_tr">
<td id="S3.T1.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ASR</td>
</tr>
<tr id="S3.T1.1.1.1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(WER <math id="S3.T1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.1.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T1.1.1.1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</td>
</tr>
</table>
</th>
<th id="S3.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">
<table id="S3.T1.2.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.2.2.2.2.1.2" class="ltx_tr">
<td id="S3.T1.2.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">PR</td>
</tr>
<tr id="S3.T1.2.2.2.2.1.1" class="ltx_tr">
<td id="S3.T1.2.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(PER <math id="S3.T1.2.2.2.2.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.2.2.2.2.1.1.1.m1.1a"><mo stretchy="false" id="S3.T1.2.2.2.2.1.1.1.m1.1.1" xref="S3.T1.2.2.2.2.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.1.1.1.m1.1b"><ci id="S3.T1.2.2.2.2.1.1.1.m1.1.1.cmml" xref="S3.T1.2.2.2.2.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</td>
</tr>
</table>
</th>
<th id="S3.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">
<table id="S3.T1.3.3.3.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.3.3.3.3.1.2" class="ltx_tr">
<td id="S3.T1.3.3.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ER</td>
</tr>
<tr id="S3.T1.3.3.3.3.1.1" class="ltx_tr">
<td id="S3.T1.3.3.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(Acc % <math id="S3.T1.3.3.3.3.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.3.3.3.3.1.1.1.m1.1a"><mo stretchy="false" id="S3.T1.3.3.3.3.1.1.1.m1.1.1" xref="S3.T1.3.3.3.3.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.1.1.1.m1.1b"><ci id="S3.T1.3.3.3.3.1.1.1.m1.1.1.cmml" xref="S3.T1.3.3.3.3.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.1.1.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
</tr>
</table>
</th>
<th id="S3.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S3.T1.4.4.4.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.4.4.4.4.1.2" class="ltx_tr">
<td id="S3.T1.4.4.4.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">IC</td>
</tr>
<tr id="S3.T1.4.4.4.4.1.1" class="ltx_tr">
<td id="S3.T1.4.4.4.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(Acc % <math id="S3.T1.4.4.4.4.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.4.4.4.4.1.1.1.m1.1a"><mo stretchy="false" id="S3.T1.4.4.4.4.1.1.1.m1.1.1" xref="S3.T1.4.4.4.4.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.4.1.1.1.m1.1b"><ci id="S3.T1.4.4.4.4.1.1.1.m1.1.1.cmml" xref="S3.T1.4.4.4.4.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.4.1.1.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
</tr>
</table>
</th>
<th id="S3.T1.6.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">
<table id="S3.T1.6.6.6.6.2" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.6.6.6.6.2.3" class="ltx_tr">
<td id="S3.T1.6.6.6.6.2.3.1" class="ltx_td ltx_nopad_r ltx_align_center">SF</td>
</tr>
<tr id="S3.T1.6.6.6.6.2.2" class="ltx_tr">
<td id="S3.T1.6.6.6.6.2.2.2" class="ltx_td ltx_nopad_r ltx_align_center">(F1 <math id="S3.T1.5.5.5.5.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.5.5.5.5.1.1.1.m1.1a"><mo stretchy="false" id="S3.T1.5.5.5.5.1.1.1.m1.1.1" xref="S3.T1.5.5.5.5.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.5.1.1.1.m1.1b"><ci id="S3.T1.5.5.5.5.1.1.1.m1.1.1.cmml" xref="S3.T1.5.5.5.5.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.5.1.1.1.m1.1c">\uparrow</annotation></semantics></math>,CER <math id="S3.T1.6.6.6.6.2.2.2.m2.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.6.6.6.6.2.2.2.m2.1a"><mo stretchy="false" id="S3.T1.6.6.6.6.2.2.2.m2.1.1" xref="S3.T1.6.6.6.6.2.2.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.6.2.2.2.m2.1b"><ci id="S3.T1.6.6.6.6.2.2.2.m2.1.1.cmml" xref="S3.T1.6.6.6.6.2.2.2.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.6.2.2.2.m2.1c">\downarrow</annotation></semantics></math>)</td>
</tr>
</table>
</th>
<th id="S3.T1.7.7.7.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">
<table id="S3.T1.7.7.7.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.7.7.7.7.1.2" class="ltx_tr">
<td id="S3.T1.7.7.7.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">IC</td>
</tr>
<tr id="S3.T1.7.7.7.7.1.1" class="ltx_tr">
<td id="S3.T1.7.7.7.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(Acc % <math id="S3.T1.7.7.7.7.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.7.7.7.7.1.1.1.m1.1a"><mo stretchy="false" id="S3.T1.7.7.7.7.1.1.1.m1.1.1" xref="S3.T1.7.7.7.7.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.7.7.1.1.1.m1.1b"><ci id="S3.T1.7.7.7.7.1.1.1.m1.1.1.cmml" xref="S3.T1.7.7.7.7.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.7.7.1.1.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
</tr>
</table>
</th>
<th id="S3.T1.7.7.7.9" class="ltx_td ltx_align_center ltx_th ltx_th_column">Avg</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.7.7.9.1" class="ltx_tr">
<td id="S3.T1.7.7.9.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">WavLM large SUPERB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S3.T1.7.7.9.1.2" class="ltx_td ltx_align_center ltx_border_t">3.4</td>
<td id="S3.T1.7.7.9.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.1</td>
<td id="S3.T1.7.7.9.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.7.7.9.1.4.1" class="ltx_text ltx_font_bold">70.6</span></td>
<td id="S3.T1.7.7.9.1.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T1.7.7.9.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">92.2, 18.4</td>
<td id="S3.T1.7.7.9.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">99.0</td>
<td id="S3.T1.7.7.9.1.8" class="ltx_td ltx_align_center ltx_border_t">89.5</td>
</tr>
<tr id="S3.T1.7.7.10.2" class="ltx_tr">
<td id="S3.T1.7.7.10.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">wav2vec2.0 large SUPERB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S3.T1.7.7.10.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.7.7.10.2.2.1" class="ltx_text ltx_font_bold">3.1</span></td>
<td id="S3.T1.7.7.10.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.7</td>
<td id="S3.T1.7.7.10.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">65.6</td>
<td id="S3.T1.7.7.10.2.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T1.7.7.10.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">87.1, 27.3</td>
<td id="S3.T1.7.7.10.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">95.2</td>
<td id="S3.T1.7.7.10.2.8" class="ltx_td ltx_align_center ltx_border_t">85.5</td>
</tr>
<tr id="S3.T1.7.7.11.3" class="ltx_tr">
<td id="S3.T1.7.7.11.3.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">wav2vec2.0 large (Ours)</td>
<td id="S3.T1.7.7.11.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">3.5</td>
<td id="S3.T1.7.7.11.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T1.7.7.11.3.3.1" class="ltx_text ltx_font_bold">2.4</span></td>
<td id="S3.T1.7.7.11.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">68.2</td>
<td id="S3.T1.7.7.11.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S3.T1.7.7.11.3.5.1" class="ltx_text ltx_font_bold">99.1</span></td>
<td id="S3.T1.7.7.11.3.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T1.7.7.11.3.6.1" class="ltx_text ltx_font_bold">95.4, 11.8</span></td>
<td id="S3.T1.7.7.11.3.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T1.7.7.11.3.7.1" class="ltx_text ltx_font_bold">99.5</span></td>
<td id="S3.T1.7.7.11.3.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S3.T1.7.7.11.3.8.1" class="ltx_text ltx_font_bold">90.9</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance comparison in various speech processing tasks from the SUPERB benchmark. WavLM is currently ranked first in SUPERB’s leaderboard and we choose wav2vec2-large to compare multi-decoder (SUPERB) and single decoder (Ours) solutions. Models from SUPERB have different decoder implementations for each task (e.g. Bi-LSTM, CNNs, linear projections) on top of the chosen SSL model. Our approach is a single transformer encoder-decoder model capable of performing all six tasks using various adapters for each task and initialized on the encoder side with the chosen SSL model. The metrics are computed with the s3prl framework and Avg denotes the average performance across all the tasks.</figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<div id="S3.T2.9" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:132.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-117.1pt,35.7pt) scale(0.649400816839012,0.649400816839012) ;">
<table id="S3.T2.9.9" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.9.9.10.1" class="ltx_tr">
<th id="S3.T2.9.9.10.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t"></th>
<th id="S3.T2.9.9.10.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2">IEMOCAP</th>
<th id="S3.T2.9.9.10.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3">SNIPS</th>
<th id="S3.T2.9.9.10.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">FSC</th>
</tr>
<tr id="S3.T2.8.8.8" class="ltx_tr">
<th id="S3.T2.8.8.8.9" class="ltx_td ltx_th ltx_th_column ltx_border_r"></th>
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S3.T2.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.1.1.1.1.1.2" class="ltx_tr">
<td id="S3.T2.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ASR</td>
</tr>
<tr id="S3.T2.1.1.1.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(WER <math id="S3.T2.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T2.1.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T2.1.1.1.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</td>
</tr>
</table>
</th>
<th id="S3.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">
<table id="S3.T2.2.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.2.2.2.2.1.2" class="ltx_tr">
<td id="S3.T2.2.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ER</td>
</tr>
<tr id="S3.T2.2.2.2.2.1.1" class="ltx_tr">
<td id="S3.T2.2.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(Acc % <math id="S3.T2.2.2.2.2.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T2.2.2.2.2.1.1.1.m1.1a"><mo stretchy="false" id="S3.T2.2.2.2.2.1.1.1.m1.1.1" xref="S3.T2.2.2.2.2.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.2.1.1.1.m1.1b"><ci id="S3.T2.2.2.2.2.1.1.1.m1.1.1.cmml" xref="S3.T2.2.2.2.2.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.2.1.1.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
</tr>
</table>
</th>
<th id="S3.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S3.T2.3.3.3.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.3.3.3.3.1.2" class="ltx_tr">
<td id="S3.T2.3.3.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ASR</td>
</tr>
<tr id="S3.T2.3.3.3.3.1.1" class="ltx_tr">
<td id="S3.T2.3.3.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(WER <math id="S3.T2.3.3.3.3.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T2.3.3.3.3.1.1.1.m1.1a"><mo stretchy="false" id="S3.T2.3.3.3.3.1.1.1.m1.1.1" xref="S3.T2.3.3.3.3.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.3.3.1.1.1.m1.1b"><ci id="S3.T2.3.3.3.3.1.1.1.m1.1.1.cmml" xref="S3.T2.3.3.3.3.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.3.3.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</td>
</tr>
</table>
</th>
<th id="S3.T2.4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S3.T2.4.4.4.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.4.4.4.4.1.2" class="ltx_tr">
<td id="S3.T2.4.4.4.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">IC</td>
</tr>
<tr id="S3.T2.4.4.4.4.1.1" class="ltx_tr">
<td id="S3.T2.4.4.4.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(Acc % <math id="S3.T2.4.4.4.4.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T2.4.4.4.4.1.1.1.m1.1a"><mo stretchy="false" id="S3.T2.4.4.4.4.1.1.1.m1.1.1" xref="S3.T2.4.4.4.4.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.4.4.4.4.1.1.1.m1.1b"><ci id="S3.T2.4.4.4.4.1.1.1.m1.1.1.cmml" xref="S3.T2.4.4.4.4.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.4.4.4.1.1.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
</tr>
</table>
</th>
<th id="S3.T2.6.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">
<table id="S3.T2.6.6.6.6.2" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.6.6.6.6.2.3" class="ltx_tr">
<td id="S3.T2.6.6.6.6.2.3.1" class="ltx_td ltx_nopad_r ltx_align_center">SF</td>
</tr>
<tr id="S3.T2.6.6.6.6.2.2" class="ltx_tr">
<td id="S3.T2.6.6.6.6.2.2.2" class="ltx_td ltx_nopad_r ltx_align_center">(F1 <math id="S3.T2.5.5.5.5.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T2.5.5.5.5.1.1.1.m1.1a"><mo stretchy="false" id="S3.T2.5.5.5.5.1.1.1.m1.1.1" xref="S3.T2.5.5.5.5.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.5.5.5.5.1.1.1.m1.1b"><ci id="S3.T2.5.5.5.5.1.1.1.m1.1.1.cmml" xref="S3.T2.5.5.5.5.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.5.5.5.1.1.1.m1.1c">\uparrow</annotation></semantics></math>,CER <math id="S3.T2.6.6.6.6.2.2.2.m2.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T2.6.6.6.6.2.2.2.m2.1a"><mo stretchy="false" id="S3.T2.6.6.6.6.2.2.2.m2.1.1" xref="S3.T2.6.6.6.6.2.2.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T2.6.6.6.6.2.2.2.m2.1b"><ci id="S3.T2.6.6.6.6.2.2.2.m2.1.1.cmml" xref="S3.T2.6.6.6.6.2.2.2.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.6.6.6.2.2.2.m2.1c">\downarrow</annotation></semantics></math>)</td>
</tr>
</table>
</th>
<th id="S3.T2.7.7.7.7" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S3.T2.7.7.7.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.7.7.7.7.1.2" class="ltx_tr">
<td id="S3.T2.7.7.7.7.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ASR</td>
</tr>
<tr id="S3.T2.7.7.7.7.1.1" class="ltx_tr">
<td id="S3.T2.7.7.7.7.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(WER <math id="S3.T2.7.7.7.7.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T2.7.7.7.7.1.1.1.m1.1a"><mo stretchy="false" id="S3.T2.7.7.7.7.1.1.1.m1.1.1" xref="S3.T2.7.7.7.7.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T2.7.7.7.7.1.1.1.m1.1b"><ci id="S3.T2.7.7.7.7.1.1.1.m1.1.1.cmml" xref="S3.T2.7.7.7.7.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.7.7.7.7.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</td>
</tr>
</table>
</th>
<th id="S3.T2.8.8.8.8" class="ltx_td ltx_align_center ltx_th ltx_th_column">
<table id="S3.T2.8.8.8.8.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.8.8.8.8.1.2" class="ltx_tr">
<td id="S3.T2.8.8.8.8.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">IC</td>
</tr>
<tr id="S3.T2.8.8.8.8.1.1" class="ltx_tr">
<td id="S3.T2.8.8.8.8.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">(Acc % <math id="S3.T2.8.8.8.8.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T2.8.8.8.8.1.1.1.m1.1a"><mo stretchy="false" id="S3.T2.8.8.8.8.1.1.1.m1.1.1" xref="S3.T2.8.8.8.8.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.8.8.8.8.1.1.1.m1.1b"><ci id="S3.T2.8.8.8.8.1.1.1.m1.1.1.cmml" xref="S3.T2.8.8.8.8.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.8.8.8.8.1.1.1.m1.1c">\uparrow</annotation></semantics></math>)</td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.9.9.11.1" class="ltx_tr">
<td id="S3.T2.9.9.11.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">MTL: ESP-net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S3.T2.9.9.11.1.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T2.9.9.11.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">67.6</td>
<td id="S3.T2.9.9.11.1.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T2.9.9.11.1.5" class="ltx_td ltx_align_center ltx_border_t">91.7</td>
<td id="S3.T2.9.9.11.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S3.T2.9.9.11.1.7" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T2.9.9.11.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.9.9.11.1.8.1" class="ltx_text ltx_font_bold">99.6</span></td>
</tr>
<tr id="S3.T2.9.9.9" class="ltx_tr">
<td id="S3.T2.9.9.9.2" class="ltx_td ltx_align_left ltx_border_r">MTL: ASR+SER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S3.T2.9.9.9.3" class="ltx_td ltx_align_center">32.7</td>
<td id="S3.T2.9.9.9.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S3.T2.9.9.9.1.m1.1" class="ltx_Math" alttext="\text{63.4}^{*}" display="inline"><semantics id="S3.T2.9.9.9.1.m1.1a"><msup id="S3.T2.9.9.9.1.m1.1.1" xref="S3.T2.9.9.9.1.m1.1.1.cmml"><mtext id="S3.T2.9.9.9.1.m1.1.1.2" xref="S3.T2.9.9.9.1.m1.1.1.2a.cmml">63.4</mtext><mo id="S3.T2.9.9.9.1.m1.1.1.3" xref="S3.T2.9.9.9.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.T2.9.9.9.1.m1.1b"><apply id="S3.T2.9.9.9.1.m1.1.1.cmml" xref="S3.T2.9.9.9.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T2.9.9.9.1.m1.1.1.1.cmml" xref="S3.T2.9.9.9.1.m1.1.1">superscript</csymbol><ci id="S3.T2.9.9.9.1.m1.1.1.2a.cmml" xref="S3.T2.9.9.9.1.m1.1.1.2"><mtext id="S3.T2.9.9.9.1.m1.1.1.2.cmml" xref="S3.T2.9.9.9.1.m1.1.1.2">63.4</mtext></ci><times id="S3.T2.9.9.9.1.m1.1.1.3.cmml" xref="S3.T2.9.9.9.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.9.9.9.1.m1.1c">\text{63.4}^{*}</annotation></semantics></math></td>
<td id="S3.T2.9.9.9.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.9.9.9.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.9.9.9.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.9.9.9.7" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.9.9.9.8" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T2.9.9.12.2" class="ltx_tr">
<td id="S3.T2.9.9.12.2.1" class="ltx_td ltx_align_left ltx_border_r">MTL: ASR+IC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S3.T2.9.9.12.2.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.9.9.12.2.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.9.9.12.2.4" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.9.9.12.2.5" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.9.9.12.2.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.9.9.12.2.7" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.9.9.12.2.8" class="ltx_td ltx_align_center">98.2</td>
</tr>
<tr id="S3.T2.9.9.13.3" class="ltx_tr">
<td id="S3.T2.9.9.13.3.1" class="ltx_td ltx_align_left ltx_border_r">MTL: ASR+IC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</td>
<td id="S3.T2.9.9.13.3.2" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.9.9.13.3.3" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.9.9.13.3.4" class="ltx_td ltx_align_center">11.8</td>
<td id="S3.T2.9.9.13.3.5" class="ltx_td ltx_align_center">98.6</td>
<td id="S3.T2.9.9.13.3.6" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S3.T2.9.9.13.3.7" class="ltx_td ltx_align_center">-</td>
<td id="S3.T2.9.9.13.3.8" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T2.9.9.14.4" class="ltx_tr">
<td id="S3.T2.9.9.14.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">wav2vec2.0 large (Ours)</td>
<td id="S3.T2.9.9.14.4.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.9.9.14.4.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.9.9.14.4.4" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.9.9.14.4.5" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.9.9.14.4.6" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S3.T2.9.9.14.4.7" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.9.9.14.4.8" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T2.9.9.15.5" class="ltx_tr">
<td id="S3.T2.9.9.15.5.1" class="ltx_td ltx_align_left ltx_border_r">- Single task Adapter</td>
<td id="S3.T2.9.9.15.5.2" class="ltx_td ltx_align_center">22.3</td>
<td id="S3.T2.9.9.15.5.3" class="ltx_td ltx_align_center ltx_border_r">65.6</td>
<td id="S3.T2.9.9.15.5.4" class="ltx_td ltx_align_center">8.5</td>
<td id="S3.T2.9.9.15.5.5" class="ltx_td ltx_align_center">98.4</td>
<td id="S3.T2.9.9.15.5.6" class="ltx_td ltx_align_center ltx_border_r">94.7, 12.9</td>
<td id="S3.T2.9.9.15.5.7" class="ltx_td ltx_align_center">0.6</td>
<td id="S3.T2.9.9.15.5.8" class="ltx_td ltx_align_center">99.4</td>
</tr>
<tr id="S3.T2.9.9.16.6" class="ltx_tr">
<td id="S3.T2.9.9.16.6.1" class="ltx_td ltx_align_left ltx_border_r">- MTL: Stacked</td>
<td id="S3.T2.9.9.16.6.2" class="ltx_td ltx_align_center">24.2</td>
<td id="S3.T2.9.9.16.6.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T2.9.9.16.6.3.1" class="ltx_text ltx_font_bold">68.2</span></td>
<td id="S3.T2.9.9.16.6.4" class="ltx_td ltx_align_center">7.7</td>
<td id="S3.T2.9.9.16.6.5" class="ltx_td ltx_align_center">98.7</td>
<td id="S3.T2.9.9.16.6.6" class="ltx_td ltx_align_center ltx_border_r">94.4, 13.5</td>
<td id="S3.T2.9.9.16.6.7" class="ltx_td ltx_align_center">0.6</td>
<td id="S3.T2.9.9.16.6.8" class="ltx_td ltx_align_center">99.5</td>
</tr>
<tr id="S3.T2.9.9.17.7" class="ltx_tr">
<td id="S3.T2.9.9.17.7.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">- MTL: Fusion</td>
<td id="S3.T2.9.9.17.7.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T2.9.9.17.7.2.1" class="ltx_text ltx_font_bold">22.1</span></td>
<td id="S3.T2.9.9.17.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">65.4</td>
<td id="S3.T2.9.9.17.7.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T2.9.9.17.7.4.1" class="ltx_text ltx_font_bold">7.3</span></td>
<td id="S3.T2.9.9.17.7.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S3.T2.9.9.17.7.5.1" class="ltx_text ltx_font_bold">99.1</span></td>
<td id="S3.T2.9.9.17.7.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S3.T2.9.9.17.7.6.1" class="ltx_text ltx_font_bold">95.4, 11.8</span></td>
<td id="S3.T2.9.9.17.7.7" class="ltx_td ltx_align_center ltx_border_b">0.6</td>
<td id="S3.T2.9.9.17.7.8" class="ltx_td ltx_align_center ltx_border_b">99.3</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance comparison between various MTL implementations and our three different adapter-based architectures. *uses weighted accuracy</figcaption>
</figure>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"></th>
<th id="S3.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2"># of tasks</th>
</tr>
<tr id="S3.T3.1.2.2" class="ltx_tr">
<th id="S3.T3.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S3.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">6 tasks</th>
<th id="S3.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">9 tasks</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.1.3.1" class="ltx_tr">
<th id="S3.T3.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">SUPERB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</th>
<td id="S3.T3.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">126.6M</td>
<td id="S3.T3.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">252.8M</td>
</tr>
<tr id="S3.T3.1.4.2" class="ltx_tr">
<th id="S3.T3.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Ours</th>
<td id="S3.T3.1.4.2.2" class="ltx_td ltx_align_center"><span id="S3.T3.1.4.2.2.1" class="ltx_text ltx_font_bold">113.1M</span></td>
<td id="S3.T3.1.4.2.3" class="ltx_td ltx_align_center"><span id="S3.T3.1.4.2.3.1" class="ltx_text ltx_font_bold">135.6M</span></td>
</tr>
<tr id="S3.T3.1.5.3" class="ltx_tr">
<th id="S3.T3.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">Ratio</th>
<td id="S3.T3.1.5.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">89.3%</td>
<td id="S3.T3.1.5.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">53.6%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison between the total # of additional trainable parameters required to accommodate 6 tasks depicted in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Pre-trained Encoder-Decoder model ‣ 3 Model Architecture ‣ An Adapter-Based Unified Model for Multiple Spoken Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and 9 tasks which includes the additional ASR tasks in Table <a href="#S3.T2" title="Table 2 ‣ 3.1 Pre-trained Encoder-Decoder model ‣ 3 Model Architecture ‣ An Adapter-Based Unified Model for Multiple Spoken Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Adapter-based Task Modules</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To enable the above-mentioned pre-trained encoder-decoder model to perform multiple SLP tasks, we insert task-specific adapter modules into the transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> layers of both the encoder and the decoder. These modules are depicted in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ An Adapter-Based Unified Model for Multiple Spoken Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The remainder of the model is frozen.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We focus on three types of architecture: i) single adapter ii) adapter stacking, and iii) adapter fusion as shown on the right side of Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ An Adapter-Based Unified Model for Multiple Spoken Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. In the standard setting, a single adapter is trained for each task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. However, some SLP tasks are known to benefit from MTL, such as performing ASR and emotion recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and Intent classification and Slot-filling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. As adapters naturally support MTL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> in addition to using a single adapter per task, we use the adapter stacking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and adapter fusion settings (same as the fast fusion setting in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>) to perform positively correlated tasks together.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.6" class="ltx_p">To facilitate a unified model, we encompass both classification (e.g., emotion recognition) and generative (e.g., slot filling) SLP tasks into this single encoder-decoder model. To achieve this, we model the classification task as a generative task i.e., the classification labels are generated. To accommodate multiple tasks using a single decoder output, some task-specific tokens are allocated in the vocabulary—slot value for SF task, emotion labels for ER task, etc. These tokens are selected from the least frequently used tokens in the vocabulary. For MTL, we combine the ground truth of the tasks involved using a task separator token. For example, to perform ASR along with ER, we format the ground truth as <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mo id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><lt id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">&lt;</annotation></semantics></math>transcript<math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mo id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><gt id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">&gt;</annotation></semantics></math> <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><mo id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><lt id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">&lt;</annotation></semantics></math>task separator<math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><mo id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><gt id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">&gt;</annotation></semantics></math> <math id="S3.SS2.p3.5.m5.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS2.p3.5.m5.1a"><mo id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><lt id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">&lt;</annotation></semantics></math>emotion label<math id="S3.SS2.p3.6.m6.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.SS2.p3.6.m6.1a"><mo id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><gt id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">&gt;</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">For training the encoder-decoder model, we use a combination of losses depending on the adapter architecture and the task. The overall objective <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="\mathbf{L}" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">𝐋</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">𝐋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\mathbf{L}</annotation></semantics></math> can be written as,</p>
<table id="S6.EGx1" class="ltx_equationgroup ltx_eqn_gather ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\displaystyle L_{nll}=\sum_{task=1}^{N}\lambda_{task}\cdot L_{task}" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.2.2" xref="S3.E1.m1.1.1.2.2.cmml">L</mi><mrow id="S3.E1.m1.1.1.2.3" xref="S3.E1.m1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1.2.3.2" xref="S3.E1.m1.1.1.2.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.2.3.1" xref="S3.E1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.2.3.3" xref="S3.E1.m1.1.1.2.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.2.3.1a" xref="S3.E1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.2.3.4" xref="S3.E1.m1.1.1.2.3.4.cmml">l</mi></mrow></msub><mo rspace="0.111em" id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><munderover id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml"><mo movablelimits="false" id="S3.E1.m1.1.1.3.1.2.2" xref="S3.E1.m1.1.1.3.1.2.2.cmml">∑</mo><mrow id="S3.E1.m1.1.1.3.1.2.3" xref="S3.E1.m1.1.1.3.1.2.3.cmml"><mrow id="S3.E1.m1.1.1.3.1.2.3.2" xref="S3.E1.m1.1.1.3.1.2.3.2.cmml"><mi id="S3.E1.m1.1.1.3.1.2.3.2.2" xref="S3.E1.m1.1.1.3.1.2.3.2.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.1.2.3.2.1" xref="S3.E1.m1.1.1.3.1.2.3.2.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.1.2.3.2.3" xref="S3.E1.m1.1.1.3.1.2.3.2.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.1.2.3.2.1a" xref="S3.E1.m1.1.1.3.1.2.3.2.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.1.2.3.2.4" xref="S3.E1.m1.1.1.3.1.2.3.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.1.2.3.2.1b" xref="S3.E1.m1.1.1.3.1.2.3.2.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.1.2.3.2.5" xref="S3.E1.m1.1.1.3.1.2.3.2.5.cmml">k</mi></mrow><mo id="S3.E1.m1.1.1.3.1.2.3.1" xref="S3.E1.m1.1.1.3.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.1.1.3.1.2.3.3" xref="S3.E1.m1.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.1.1.3.1.3" xref="S3.E1.m1.1.1.3.1.3.cmml">N</mi></munderover><mrow id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><msub id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml"><mi id="S3.E1.m1.1.1.3.2.2.2" xref="S3.E1.m1.1.1.3.2.2.2.cmml">λ</mi><mrow id="S3.E1.m1.1.1.3.2.2.3" xref="S3.E1.m1.1.1.3.2.2.3.cmml"><mi id="S3.E1.m1.1.1.3.2.2.3.2" xref="S3.E1.m1.1.1.3.2.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.2.2.3.1" xref="S3.E1.m1.1.1.3.2.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.2.2.3.3" xref="S3.E1.m1.1.1.3.2.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.2.2.3.1a" xref="S3.E1.m1.1.1.3.2.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.2.2.3.4" xref="S3.E1.m1.1.1.3.2.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.2.2.3.1b" xref="S3.E1.m1.1.1.3.2.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.2.2.3.5" xref="S3.E1.m1.1.1.3.2.2.3.5.cmml">k</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.1.3.2.1" xref="S3.E1.m1.1.1.3.2.1.cmml">⋅</mo><msub id="S3.E1.m1.1.1.3.2.3" xref="S3.E1.m1.1.1.3.2.3.cmml"><mi id="S3.E1.m1.1.1.3.2.3.2" xref="S3.E1.m1.1.1.3.2.3.2.cmml">L</mi><mrow id="S3.E1.m1.1.1.3.2.3.3" xref="S3.E1.m1.1.1.3.2.3.3.cmml"><mi id="S3.E1.m1.1.1.3.2.3.3.2" xref="S3.E1.m1.1.1.3.2.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.2.3.3.1" xref="S3.E1.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.2.3.3.3" xref="S3.E1.m1.1.1.3.2.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.2.3.3.1a" xref="S3.E1.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.2.3.3.4" xref="S3.E1.m1.1.1.3.2.3.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.2.3.3.1b" xref="S3.E1.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.2.3.3.5" xref="S3.E1.m1.1.1.3.2.3.3.5.cmml">k</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"></eq><apply id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.2.2">𝐿</ci><apply id="S3.E1.m1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.2.3"><times id="S3.E1.m1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.2.3.1"></times><ci id="S3.E1.m1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.2.3.2">𝑛</ci><ci id="S3.E1.m1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.2.3.3">𝑙</ci><ci id="S3.E1.m1.1.1.2.3.4.cmml" xref="S3.E1.m1.1.1.2.3.4">𝑙</ci></apply></apply><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><apply id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.1.1.cmml" xref="S3.E1.m1.1.1.3.1">superscript</csymbol><apply id="S3.E1.m1.1.1.3.1.2.cmml" xref="S3.E1.m1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.1.2.1.cmml" xref="S3.E1.m1.1.1.3.1">subscript</csymbol><sum id="S3.E1.m1.1.1.3.1.2.2.cmml" xref="S3.E1.m1.1.1.3.1.2.2"></sum><apply id="S3.E1.m1.1.1.3.1.2.3.cmml" xref="S3.E1.m1.1.1.3.1.2.3"><eq id="S3.E1.m1.1.1.3.1.2.3.1.cmml" xref="S3.E1.m1.1.1.3.1.2.3.1"></eq><apply id="S3.E1.m1.1.1.3.1.2.3.2.cmml" xref="S3.E1.m1.1.1.3.1.2.3.2"><times id="S3.E1.m1.1.1.3.1.2.3.2.1.cmml" xref="S3.E1.m1.1.1.3.1.2.3.2.1"></times><ci id="S3.E1.m1.1.1.3.1.2.3.2.2.cmml" xref="S3.E1.m1.1.1.3.1.2.3.2.2">𝑡</ci><ci id="S3.E1.m1.1.1.3.1.2.3.2.3.cmml" xref="S3.E1.m1.1.1.3.1.2.3.2.3">𝑎</ci><ci id="S3.E1.m1.1.1.3.1.2.3.2.4.cmml" xref="S3.E1.m1.1.1.3.1.2.3.2.4">𝑠</ci><ci id="S3.E1.m1.1.1.3.1.2.3.2.5.cmml" xref="S3.E1.m1.1.1.3.1.2.3.2.5">𝑘</ci></apply><cn type="integer" id="S3.E1.m1.1.1.3.1.2.3.3.cmml" xref="S3.E1.m1.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.1.1.3.1.3.cmml" xref="S3.E1.m1.1.1.3.1.3">𝑁</ci></apply><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><ci id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2.1">⋅</ci><apply id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.2.1.cmml" xref="S3.E1.m1.1.1.3.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2.2">𝜆</ci><apply id="S3.E1.m1.1.1.3.2.2.3.cmml" xref="S3.E1.m1.1.1.3.2.2.3"><times id="S3.E1.m1.1.1.3.2.2.3.1.cmml" xref="S3.E1.m1.1.1.3.2.2.3.1"></times><ci id="S3.E1.m1.1.1.3.2.2.3.2.cmml" xref="S3.E1.m1.1.1.3.2.2.3.2">𝑡</ci><ci id="S3.E1.m1.1.1.3.2.2.3.3.cmml" xref="S3.E1.m1.1.1.3.2.2.3.3">𝑎</ci><ci id="S3.E1.m1.1.1.3.2.2.3.4.cmml" xref="S3.E1.m1.1.1.3.2.2.3.4">𝑠</ci><ci id="S3.E1.m1.1.1.3.2.2.3.5.cmml" xref="S3.E1.m1.1.1.3.2.2.3.5">𝑘</ci></apply></apply><apply id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.1.1.3.2.3.2">𝐿</ci><apply id="S3.E1.m1.1.1.3.2.3.3.cmml" xref="S3.E1.m1.1.1.3.2.3.3"><times id="S3.E1.m1.1.1.3.2.3.3.1.cmml" xref="S3.E1.m1.1.1.3.2.3.3.1"></times><ci id="S3.E1.m1.1.1.3.2.3.3.2.cmml" xref="S3.E1.m1.1.1.3.2.3.3.2">𝑡</ci><ci id="S3.E1.m1.1.1.3.2.3.3.3.cmml" xref="S3.E1.m1.1.1.3.2.3.3.3">𝑎</ci><ci id="S3.E1.m1.1.1.3.2.3.3.4.cmml" xref="S3.E1.m1.1.1.3.2.3.3.4">𝑠</ci><ci id="S3.E1.m1.1.1.3.2.3.3.5.cmml" xref="S3.E1.m1.1.1.3.2.3.3.5">𝑘</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle L_{nll}=\sum_{task=1}^{N}\lambda_{task}\cdot L_{task}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle\mathbf{L}=(1-\lambda_{ctc})\cdot L_{nll}+\lambda_{ctc}\cdot L_{ctc}+\mathbbm{1}_{ce}\cdot L_{ce}" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><mi id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml">𝐋</mi><mo id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mn id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.3.2.cmml">λ</mi><mrow id="S3.E2.m1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.3.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.3.3.1a" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.3.3.4" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3.4.cmml">c</mi></mrow></msub></mrow><mo rspace="0.055em" stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">⋅</mo><msub id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml">L</mi><mrow id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.3.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.3.1" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.1.3.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.3.1a" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.3.3.4" xref="S3.E2.m1.1.1.1.1.3.3.4.cmml">l</mi></mrow></msub></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml">+</mo><mrow id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3.cmml"><msub id="S3.E2.m1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.3.2.cmml"><mi id="S3.E2.m1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.3.2.2.cmml">λ</mi><mrow id="S3.E2.m1.1.1.1.3.2.3" xref="S3.E2.m1.1.1.1.3.2.3.cmml"><mi id="S3.E2.m1.1.1.1.3.2.3.2" xref="S3.E2.m1.1.1.1.3.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.3.2.3.1" xref="S3.E2.m1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.3.2.3.3" xref="S3.E2.m1.1.1.1.3.2.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.3.2.3.1a" xref="S3.E2.m1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.3.2.3.4" xref="S3.E2.m1.1.1.1.3.2.3.4.cmml">c</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.3.1.cmml">⋅</mo><msub id="S3.E2.m1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.3.3.2.cmml">L</mi><mrow id="S3.E2.m1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.3.3.3.cmml"><mi id="S3.E2.m1.1.1.1.3.3.3.2" xref="S3.E2.m1.1.1.1.3.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.3.3.3.1" xref="S3.E2.m1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.3.3.3.3" xref="S3.E2.m1.1.1.1.3.3.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.3.3.3.1a" xref="S3.E2.m1.1.1.1.3.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.3.3.3.4" xref="S3.E2.m1.1.1.1.3.3.3.4.cmml">c</mi></mrow></msub></mrow><mo id="S3.E2.m1.1.1.1.2a" xref="S3.E2.m1.1.1.1.2.cmml">+</mo><mrow id="S3.E2.m1.1.1.1.4" xref="S3.E2.m1.1.1.1.4.cmml"><msub id="S3.E2.m1.1.1.1.4.2" xref="S3.E2.m1.1.1.1.4.2.cmml"><mn id="S3.E2.m1.1.1.1.4.2.2" xref="S3.E2.m1.1.1.1.4.2.2.cmml">𝟙</mn><mrow id="S3.E2.m1.1.1.1.4.2.3" xref="S3.E2.m1.1.1.1.4.2.3.cmml"><mi id="S3.E2.m1.1.1.1.4.2.3.2" xref="S3.E2.m1.1.1.1.4.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.4.2.3.1" xref="S3.E2.m1.1.1.1.4.2.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.4.2.3.3" xref="S3.E2.m1.1.1.1.4.2.3.3.cmml">e</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.1.1.1.4.1" xref="S3.E2.m1.1.1.1.4.1.cmml">⋅</mo><msub id="S3.E2.m1.1.1.1.4.3" xref="S3.E2.m1.1.1.1.4.3.cmml"><mi id="S3.E2.m1.1.1.1.4.3.2" xref="S3.E2.m1.1.1.1.4.3.2.cmml">L</mi><mrow id="S3.E2.m1.1.1.1.4.3.3" xref="S3.E2.m1.1.1.1.4.3.3.cmml"><mi id="S3.E2.m1.1.1.1.4.3.3.2" xref="S3.E2.m1.1.1.1.4.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.4.3.3.1" xref="S3.E2.m1.1.1.1.4.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.4.3.3.3" xref="S3.E2.m1.1.1.1.4.3.3.3.cmml">e</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"></eq><ci id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3">𝐋</ci><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><plus id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2"></plus><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1"><ci id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2">⋅</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><minus id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"></minus><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">1</cn><apply id="S3.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.2">𝜆</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3"><times id="S3.E2.m1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3.1"></times><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3.2">𝑐</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3.3">𝑡</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.3.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.3.4">𝑐</ci></apply></apply></apply><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2">𝐿</ci><apply id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3"><times id="S3.E2.m1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1"></times><ci id="S3.E2.m1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2">𝑛</ci><ci id="S3.E2.m1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3">𝑙</ci><ci id="S3.E2.m1.1.1.1.1.3.3.4.cmml" xref="S3.E2.m1.1.1.1.1.3.3.4">𝑙</ci></apply></apply></apply><apply id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3"><ci id="S3.E2.m1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.3.1">⋅</ci><apply id="S3.E2.m1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.1.3.2.2">𝜆</ci><apply id="S3.E2.m1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.1.3.2.3"><times id="S3.E2.m1.1.1.1.3.2.3.1.cmml" xref="S3.E2.m1.1.1.1.3.2.3.1"></times><ci id="S3.E2.m1.1.1.1.3.2.3.2.cmml" xref="S3.E2.m1.1.1.1.3.2.3.2">𝑐</ci><ci id="S3.E2.m1.1.1.1.3.2.3.3.cmml" xref="S3.E2.m1.1.1.1.3.2.3.3">𝑡</ci><ci id="S3.E2.m1.1.1.1.3.2.3.4.cmml" xref="S3.E2.m1.1.1.1.3.2.3.4">𝑐</ci></apply></apply><apply id="S3.E2.m1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.3.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.3.3.2">𝐿</ci><apply id="S3.E2.m1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.3.3.3"><times id="S3.E2.m1.1.1.1.3.3.3.1.cmml" xref="S3.E2.m1.1.1.1.3.3.3.1"></times><ci id="S3.E2.m1.1.1.1.3.3.3.2.cmml" xref="S3.E2.m1.1.1.1.3.3.3.2">𝑐</ci><ci id="S3.E2.m1.1.1.1.3.3.3.3.cmml" xref="S3.E2.m1.1.1.1.3.3.3.3">𝑡</ci><ci id="S3.E2.m1.1.1.1.3.3.3.4.cmml" xref="S3.E2.m1.1.1.1.3.3.3.4">𝑐</ci></apply></apply></apply><apply id="S3.E2.m1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.4"><ci id="S3.E2.m1.1.1.1.4.1.cmml" xref="S3.E2.m1.1.1.1.4.1">⋅</ci><apply id="S3.E2.m1.1.1.1.4.2.cmml" xref="S3.E2.m1.1.1.1.4.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.4.2.1.cmml" xref="S3.E2.m1.1.1.1.4.2">subscript</csymbol><cn type="integer" id="S3.E2.m1.1.1.1.4.2.2.cmml" xref="S3.E2.m1.1.1.1.4.2.2">1</cn><apply id="S3.E2.m1.1.1.1.4.2.3.cmml" xref="S3.E2.m1.1.1.1.4.2.3"><times id="S3.E2.m1.1.1.1.4.2.3.1.cmml" xref="S3.E2.m1.1.1.1.4.2.3.1"></times><ci id="S3.E2.m1.1.1.1.4.2.3.2.cmml" xref="S3.E2.m1.1.1.1.4.2.3.2">𝑐</ci><ci id="S3.E2.m1.1.1.1.4.2.3.3.cmml" xref="S3.E2.m1.1.1.1.4.2.3.3">𝑒</ci></apply></apply><apply id="S3.E2.m1.1.1.1.4.3.cmml" xref="S3.E2.m1.1.1.1.4.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.4.3.1.cmml" xref="S3.E2.m1.1.1.1.4.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.4.3.2.cmml" xref="S3.E2.m1.1.1.1.4.3.2">𝐿</ci><apply id="S3.E2.m1.1.1.1.4.3.3.cmml" xref="S3.E2.m1.1.1.1.4.3.3"><times id="S3.E2.m1.1.1.1.4.3.3.1.cmml" xref="S3.E2.m1.1.1.1.4.3.3.1"></times><ci id="S3.E2.m1.1.1.1.4.3.3.2.cmml" xref="S3.E2.m1.1.1.1.4.3.3.2">𝑐</ci><ci id="S3.E2.m1.1.1.1.4.3.3.3.cmml" xref="S3.E2.m1.1.1.1.4.3.3.3">𝑒</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle\mathbf{L}=(1-\lambda_{ctc})\cdot L_{nll}+\lambda_{ctc}\cdot L_{ctc}+\mathbbm{1}_{ce}\cdot L_{ce}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p4.8" class="ltx_p">where <math id="S3.SS2.p4.2.m1.1" class="ltx_Math" alttext="L_{nll}" display="inline"><semantics id="S3.SS2.p4.2.m1.1a"><msub id="S3.SS2.p4.2.m1.1.1" xref="S3.SS2.p4.2.m1.1.1.cmml"><mi id="S3.SS2.p4.2.m1.1.1.2" xref="S3.SS2.p4.2.m1.1.1.2.cmml">L</mi><mrow id="S3.SS2.p4.2.m1.1.1.3" xref="S3.SS2.p4.2.m1.1.1.3.cmml"><mi id="S3.SS2.p4.2.m1.1.1.3.2" xref="S3.SS2.p4.2.m1.1.1.3.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.2.m1.1.1.3.1" xref="S3.SS2.p4.2.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.2.m1.1.1.3.3" xref="S3.SS2.p4.2.m1.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.2.m1.1.1.3.1a" xref="S3.SS2.p4.2.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.2.m1.1.1.3.4" xref="S3.SS2.p4.2.m1.1.1.3.4.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m1.1b"><apply id="S3.SS2.p4.2.m1.1.1.cmml" xref="S3.SS2.p4.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m1.1.1.1.cmml" xref="S3.SS2.p4.2.m1.1.1">subscript</csymbol><ci id="S3.SS2.p4.2.m1.1.1.2.cmml" xref="S3.SS2.p4.2.m1.1.1.2">𝐿</ci><apply id="S3.SS2.p4.2.m1.1.1.3.cmml" xref="S3.SS2.p4.2.m1.1.1.3"><times id="S3.SS2.p4.2.m1.1.1.3.1.cmml" xref="S3.SS2.p4.2.m1.1.1.3.1"></times><ci id="S3.SS2.p4.2.m1.1.1.3.2.cmml" xref="S3.SS2.p4.2.m1.1.1.3.2">𝑛</ci><ci id="S3.SS2.p4.2.m1.1.1.3.3.cmml" xref="S3.SS2.p4.2.m1.1.1.3.3">𝑙</ci><ci id="S3.SS2.p4.2.m1.1.1.3.4.cmml" xref="S3.SS2.p4.2.m1.1.1.3.4">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m1.1c">L_{nll}</annotation></semantics></math> denotes the
Negative Log-Likelihood loss at the decoder end. The output tokens during multi-task training comprise tokens from <math id="S3.SS2.p4.3.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p4.3.m2.1a"><mi id="S3.SS2.p4.3.m2.1.1" xref="S3.SS2.p4.3.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m2.1b"><ci id="S3.SS2.p4.3.m2.1.1.cmml" xref="S3.SS2.p4.3.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m2.1c">N</annotation></semantics></math> tasks which are weighted using the hyperparameter
<math id="S3.SS2.p4.4.m3.1" class="ltx_Math" alttext="\lambda_{task}" display="inline"><semantics id="S3.SS2.p4.4.m3.1a"><msub id="S3.SS2.p4.4.m3.1.1" xref="S3.SS2.p4.4.m3.1.1.cmml"><mi id="S3.SS2.p4.4.m3.1.1.2" xref="S3.SS2.p4.4.m3.1.1.2.cmml">λ</mi><mrow id="S3.SS2.p4.4.m3.1.1.3" xref="S3.SS2.p4.4.m3.1.1.3.cmml"><mi id="S3.SS2.p4.4.m3.1.1.3.2" xref="S3.SS2.p4.4.m3.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.4.m3.1.1.3.1" xref="S3.SS2.p4.4.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.4.m3.1.1.3.3" xref="S3.SS2.p4.4.m3.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.4.m3.1.1.3.1a" xref="S3.SS2.p4.4.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.4.m3.1.1.3.4" xref="S3.SS2.p4.4.m3.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.4.m3.1.1.3.1b" xref="S3.SS2.p4.4.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.4.m3.1.1.3.5" xref="S3.SS2.p4.4.m3.1.1.3.5.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m3.1b"><apply id="S3.SS2.p4.4.m3.1.1.cmml" xref="S3.SS2.p4.4.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.4.m3.1.1.1.cmml" xref="S3.SS2.p4.4.m3.1.1">subscript</csymbol><ci id="S3.SS2.p4.4.m3.1.1.2.cmml" xref="S3.SS2.p4.4.m3.1.1.2">𝜆</ci><apply id="S3.SS2.p4.4.m3.1.1.3.cmml" xref="S3.SS2.p4.4.m3.1.1.3"><times id="S3.SS2.p4.4.m3.1.1.3.1.cmml" xref="S3.SS2.p4.4.m3.1.1.3.1"></times><ci id="S3.SS2.p4.4.m3.1.1.3.2.cmml" xref="S3.SS2.p4.4.m3.1.1.3.2">𝑡</ci><ci id="S3.SS2.p4.4.m3.1.1.3.3.cmml" xref="S3.SS2.p4.4.m3.1.1.3.3">𝑎</ci><ci id="S3.SS2.p4.4.m3.1.1.3.4.cmml" xref="S3.SS2.p4.4.m3.1.1.3.4">𝑠</ci><ci id="S3.SS2.p4.4.m3.1.1.3.5.cmml" xref="S3.SS2.p4.4.m3.1.1.3.5">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m3.1c">\lambda_{task}</annotation></semantics></math>. <math id="S3.SS2.p4.5.m4.1" class="ltx_Math" alttext="L_{ctc}" display="inline"><semantics id="S3.SS2.p4.5.m4.1a"><msub id="S3.SS2.p4.5.m4.1.1" xref="S3.SS2.p4.5.m4.1.1.cmml"><mi id="S3.SS2.p4.5.m4.1.1.2" xref="S3.SS2.p4.5.m4.1.1.2.cmml">L</mi><mrow id="S3.SS2.p4.5.m4.1.1.3" xref="S3.SS2.p4.5.m4.1.1.3.cmml"><mi id="S3.SS2.p4.5.m4.1.1.3.2" xref="S3.SS2.p4.5.m4.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.5.m4.1.1.3.1" xref="S3.SS2.p4.5.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.5.m4.1.1.3.3" xref="S3.SS2.p4.5.m4.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.5.m4.1.1.3.1a" xref="S3.SS2.p4.5.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.5.m4.1.1.3.4" xref="S3.SS2.p4.5.m4.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m4.1b"><apply id="S3.SS2.p4.5.m4.1.1.cmml" xref="S3.SS2.p4.5.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.5.m4.1.1.1.cmml" xref="S3.SS2.p4.5.m4.1.1">subscript</csymbol><ci id="S3.SS2.p4.5.m4.1.1.2.cmml" xref="S3.SS2.p4.5.m4.1.1.2">𝐿</ci><apply id="S3.SS2.p4.5.m4.1.1.3.cmml" xref="S3.SS2.p4.5.m4.1.1.3"><times id="S3.SS2.p4.5.m4.1.1.3.1.cmml" xref="S3.SS2.p4.5.m4.1.1.3.1"></times><ci id="S3.SS2.p4.5.m4.1.1.3.2.cmml" xref="S3.SS2.p4.5.m4.1.1.3.2">𝑐</ci><ci id="S3.SS2.p4.5.m4.1.1.3.3.cmml" xref="S3.SS2.p4.5.m4.1.1.3.3">𝑡</ci><ci id="S3.SS2.p4.5.m4.1.1.3.4.cmml" xref="S3.SS2.p4.5.m4.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m4.1c">L_{ctc}</annotation></semantics></math> denotes CTC loss applied at the encoder end, similar to hybrid CTC/attention objective <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Hyperparameter <math id="S3.SS2.p4.6.m5.1" class="ltx_Math" alttext="\lambda_{ctc}" display="inline"><semantics id="S3.SS2.p4.6.m5.1a"><msub id="S3.SS2.p4.6.m5.1.1" xref="S3.SS2.p4.6.m5.1.1.cmml"><mi id="S3.SS2.p4.6.m5.1.1.2" xref="S3.SS2.p4.6.m5.1.1.2.cmml">λ</mi><mrow id="S3.SS2.p4.6.m5.1.1.3" xref="S3.SS2.p4.6.m5.1.1.3.cmml"><mi id="S3.SS2.p4.6.m5.1.1.3.2" xref="S3.SS2.p4.6.m5.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.6.m5.1.1.3.1" xref="S3.SS2.p4.6.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.6.m5.1.1.3.3" xref="S3.SS2.p4.6.m5.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.6.m5.1.1.3.1a" xref="S3.SS2.p4.6.m5.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.6.m5.1.1.3.4" xref="S3.SS2.p4.6.m5.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m5.1b"><apply id="S3.SS2.p4.6.m5.1.1.cmml" xref="S3.SS2.p4.6.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.6.m5.1.1.1.cmml" xref="S3.SS2.p4.6.m5.1.1">subscript</csymbol><ci id="S3.SS2.p4.6.m5.1.1.2.cmml" xref="S3.SS2.p4.6.m5.1.1.2">𝜆</ci><apply id="S3.SS2.p4.6.m5.1.1.3.cmml" xref="S3.SS2.p4.6.m5.1.1.3"><times id="S3.SS2.p4.6.m5.1.1.3.1.cmml" xref="S3.SS2.p4.6.m5.1.1.3.1"></times><ci id="S3.SS2.p4.6.m5.1.1.3.2.cmml" xref="S3.SS2.p4.6.m5.1.1.3.2">𝑐</ci><ci id="S3.SS2.p4.6.m5.1.1.3.3.cmml" xref="S3.SS2.p4.6.m5.1.1.3.3">𝑡</ci><ci id="S3.SS2.p4.6.m5.1.1.3.4.cmml" xref="S3.SS2.p4.6.m5.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m5.1c">\lambda_{ctc}</annotation></semantics></math> is used to weigh between the NLL and CTC loss. Finally, <math id="S3.SS2.p4.7.m6.1" class="ltx_Math" alttext="L_{ce}" display="inline"><semantics id="S3.SS2.p4.7.m6.1a"><msub id="S3.SS2.p4.7.m6.1.1" xref="S3.SS2.p4.7.m6.1.1.cmml"><mi id="S3.SS2.p4.7.m6.1.1.2" xref="S3.SS2.p4.7.m6.1.1.2.cmml">L</mi><mrow id="S3.SS2.p4.7.m6.1.1.3" xref="S3.SS2.p4.7.m6.1.1.3.cmml"><mi id="S3.SS2.p4.7.m6.1.1.3.2" xref="S3.SS2.p4.7.m6.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.7.m6.1.1.3.1" xref="S3.SS2.p4.7.m6.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.7.m6.1.1.3.3" xref="S3.SS2.p4.7.m6.1.1.3.3.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.7.m6.1b"><apply id="S3.SS2.p4.7.m6.1.1.cmml" xref="S3.SS2.p4.7.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.7.m6.1.1.1.cmml" xref="S3.SS2.p4.7.m6.1.1">subscript</csymbol><ci id="S3.SS2.p4.7.m6.1.1.2.cmml" xref="S3.SS2.p4.7.m6.1.1.2">𝐿</ci><apply id="S3.SS2.p4.7.m6.1.1.3.cmml" xref="S3.SS2.p4.7.m6.1.1.3"><times id="S3.SS2.p4.7.m6.1.1.3.1.cmml" xref="S3.SS2.p4.7.m6.1.1.3.1"></times><ci id="S3.SS2.p4.7.m6.1.1.3.2.cmml" xref="S3.SS2.p4.7.m6.1.1.3.2">𝑐</ci><ci id="S3.SS2.p4.7.m6.1.1.3.3.cmml" xref="S3.SS2.p4.7.m6.1.1.3.3">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.7.m6.1c">L_{ce}</annotation></semantics></math> denotes Cross Entropy Loss applied at the encoder end for classification tasks, and
hyperparameter <math id="S3.SS2.p4.8.m7.1" class="ltx_Math" alttext="\mathbbm{1}_{ce}" display="inline"><semantics id="S3.SS2.p4.8.m7.1a"><msub id="S3.SS2.p4.8.m7.1.1" xref="S3.SS2.p4.8.m7.1.1.cmml"><mn id="S3.SS2.p4.8.m7.1.1.2" xref="S3.SS2.p4.8.m7.1.1.2.cmml">𝟙</mn><mrow id="S3.SS2.p4.8.m7.1.1.3" xref="S3.SS2.p4.8.m7.1.1.3.cmml"><mi id="S3.SS2.p4.8.m7.1.1.3.2" xref="S3.SS2.p4.8.m7.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.8.m7.1.1.3.1" xref="S3.SS2.p4.8.m7.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p4.8.m7.1.1.3.3" xref="S3.SS2.p4.8.m7.1.1.3.3.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.8.m7.1b"><apply id="S3.SS2.p4.8.m7.1.1.cmml" xref="S3.SS2.p4.8.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.8.m7.1.1.1.cmml" xref="S3.SS2.p4.8.m7.1.1">subscript</csymbol><cn type="integer" id="S3.SS2.p4.8.m7.1.1.2.cmml" xref="S3.SS2.p4.8.m7.1.1.2">1</cn><apply id="S3.SS2.p4.8.m7.1.1.3.cmml" xref="S3.SS2.p4.8.m7.1.1.3"><times id="S3.SS2.p4.8.m7.1.1.3.1.cmml" xref="S3.SS2.p4.8.m7.1.1.3.1"></times><ci id="S3.SS2.p4.8.m7.1.1.3.2.cmml" xref="S3.SS2.p4.8.m7.1.1.3.2">𝑐</ci><ci id="S3.SS2.p4.8.m7.1.1.3.3.cmml" xref="S3.SS2.p4.8.m7.1.1.3.3">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.8.m7.1c">\mathbbm{1}_{ce}</annotation></semantics></math> is 1 when it’s a classification task.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We train adapters to perform five different SLP tasks (corresponding datasets are denoted in the brackets), 1) ASR (LibriSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>), 2) PR (LibriSpeech), 3) ER (IEMOCAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>), 4) IC (Fluent Speech Commands <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>), and 5) SF (SNIPS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>). We chose datasets used by the SUPERB benchmark<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/s3prl/s3prl" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/s3prl/s3prl</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> for the corresponding tasks. In addition, we also train ASR adapters specifically for each domain (IEMOCAP, SNIPS and FSC) which helps in MTL (e.g, ASR + ER) and IC adapter for SNIPS which helps when performed with SF.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.5" class="ltx_p">For evaluation, we follow the same setting as SUPERB. The adapter dimension was set to 128. For <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S4.p2.1.m1.1a"><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">\lambda</annotation></semantics></math> settings, in single-label classification tasks such as ER, <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="\lambda_{ctc}" display="inline"><semantics id="S4.p2.2.m2.1a"><msub id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mi id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">λ</mi><mrow id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml"><mi id="S4.p2.2.m2.1.1.3.2" xref="S4.p2.2.m2.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p2.2.m2.1.1.3.1" xref="S4.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.p2.2.m2.1.1.3.3" xref="S4.p2.2.m2.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p2.2.m2.1.1.3.1a" xref="S4.p2.2.m2.1.1.3.1.cmml">​</mo><mi id="S4.p2.2.m2.1.1.3.4" xref="S4.p2.2.m2.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1">subscript</csymbol><ci id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">𝜆</ci><apply id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3"><times id="S4.p2.2.m2.1.1.3.1.cmml" xref="S4.p2.2.m2.1.1.3.1"></times><ci id="S4.p2.2.m2.1.1.3.2.cmml" xref="S4.p2.2.m2.1.1.3.2">𝑐</ci><ci id="S4.p2.2.m2.1.1.3.3.cmml" xref="S4.p2.2.m2.1.1.3.3">𝑡</ci><ci id="S4.p2.2.m2.1.1.3.4.cmml" xref="S4.p2.2.m2.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">\lambda_{ctc}</annotation></semantics></math> is set to 0. For the rest, <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="\lambda_{ctc}" display="inline"><semantics id="S4.p2.3.m3.1a"><msub id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><mi id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml">λ</mi><mrow id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3.cmml"><mi id="S4.p2.3.m3.1.1.3.2" xref="S4.p2.3.m3.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p2.3.m3.1.1.3.1" xref="S4.p2.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.p2.3.m3.1.1.3.3" xref="S4.p2.3.m3.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p2.3.m3.1.1.3.1a" xref="S4.p2.3.m3.1.1.3.1.cmml">​</mo><mi id="S4.p2.3.m3.1.1.3.4" xref="S4.p2.3.m3.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1">subscript</csymbol><ci id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2">𝜆</ci><apply id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3"><times id="S4.p2.3.m3.1.1.3.1.cmml" xref="S4.p2.3.m3.1.1.3.1"></times><ci id="S4.p2.3.m3.1.1.3.2.cmml" xref="S4.p2.3.m3.1.1.3.2">𝑐</ci><ci id="S4.p2.3.m3.1.1.3.3.cmml" xref="S4.p2.3.m3.1.1.3.3">𝑡</ci><ci id="S4.p2.3.m3.1.1.3.4.cmml" xref="S4.p2.3.m3.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">\lambda_{ctc}</annotation></semantics></math> is set to 0.3 and in experiments where CTC loss was used, we combined the attention-based and CTC scores for joint decoding, assigning a weight of 0.4 to the CTC scores (following the SpeechBrain recipe). For MTL, in the stacked adapter setting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> only the additional adapter is trained, while the rest of the model, including the bottom adapter(s) remains frozen. Here, <math id="S4.p2.4.m4.1" class="ltx_Math" alttext="\lambda_{task}" display="inline"><semantics id="S4.p2.4.m4.1a"><msub id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml"><mi id="S4.p2.4.m4.1.1.2" xref="S4.p2.4.m4.1.1.2.cmml">λ</mi><mrow id="S4.p2.4.m4.1.1.3" xref="S4.p2.4.m4.1.1.3.cmml"><mi id="S4.p2.4.m4.1.1.3.2" xref="S4.p2.4.m4.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p2.4.m4.1.1.3.1" xref="S4.p2.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.p2.4.m4.1.1.3.3" xref="S4.p2.4.m4.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p2.4.m4.1.1.3.1a" xref="S4.p2.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.p2.4.m4.1.1.3.4" xref="S4.p2.4.m4.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.p2.4.m4.1.1.3.1b" xref="S4.p2.4.m4.1.1.3.1.cmml">​</mo><mi id="S4.p2.4.m4.1.1.3.5" xref="S4.p2.4.m4.1.1.3.5.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><apply id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p2.4.m4.1.1.1.cmml" xref="S4.p2.4.m4.1.1">subscript</csymbol><ci id="S4.p2.4.m4.1.1.2.cmml" xref="S4.p2.4.m4.1.1.2">𝜆</ci><apply id="S4.p2.4.m4.1.1.3.cmml" xref="S4.p2.4.m4.1.1.3"><times id="S4.p2.4.m4.1.1.3.1.cmml" xref="S4.p2.4.m4.1.1.3.1"></times><ci id="S4.p2.4.m4.1.1.3.2.cmml" xref="S4.p2.4.m4.1.1.3.2">𝑡</ci><ci id="S4.p2.4.m4.1.1.3.3.cmml" xref="S4.p2.4.m4.1.1.3.3">𝑎</ci><ci id="S4.p2.4.m4.1.1.3.4.cmml" xref="S4.p2.4.m4.1.1.3.4">𝑠</ci><ci id="S4.p2.4.m4.1.1.3.5.cmml" xref="S4.p2.4.m4.1.1.3.5">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">\lambda_{task}</annotation></semantics></math> assigns a higher weight to the tokens corresponding to the new task. In our experiments, this value was set to 0.9 for the new task and 0.1 for the tasks of the already-trained bottom adapters. In fusion, the adapters are already trained with respective tasks, so we experimented with two settings: first, <math id="S4.p2.5.m5.1" class="ltx_Math" alttext="\lambda_{task}" display="inline"><semantics id="S4.p2.5.m5.1a"><msub id="S4.p2.5.m5.1.1" xref="S4.p2.5.m5.1.1.cmml"><mi id="S4.p2.5.m5.1.1.2" xref="S4.p2.5.m5.1.1.2.cmml">λ</mi><mrow id="S4.p2.5.m5.1.1.3" xref="S4.p2.5.m5.1.1.3.cmml"><mi id="S4.p2.5.m5.1.1.3.2" xref="S4.p2.5.m5.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p2.5.m5.1.1.3.1" xref="S4.p2.5.m5.1.1.3.1.cmml">​</mo><mi id="S4.p2.5.m5.1.1.3.3" xref="S4.p2.5.m5.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p2.5.m5.1.1.3.1a" xref="S4.p2.5.m5.1.1.3.1.cmml">​</mo><mi id="S4.p2.5.m5.1.1.3.4" xref="S4.p2.5.m5.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.p2.5.m5.1.1.3.1b" xref="S4.p2.5.m5.1.1.3.1.cmml">​</mo><mi id="S4.p2.5.m5.1.1.3.5" xref="S4.p2.5.m5.1.1.3.5.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p2.5.m5.1b"><apply id="S4.p2.5.m5.1.1.cmml" xref="S4.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.p2.5.m5.1.1.1.cmml" xref="S4.p2.5.m5.1.1">subscript</csymbol><ci id="S4.p2.5.m5.1.1.2.cmml" xref="S4.p2.5.m5.1.1.2">𝜆</ci><apply id="S4.p2.5.m5.1.1.3.cmml" xref="S4.p2.5.m5.1.1.3"><times id="S4.p2.5.m5.1.1.3.1.cmml" xref="S4.p2.5.m5.1.1.3.1"></times><ci id="S4.p2.5.m5.1.1.3.2.cmml" xref="S4.p2.5.m5.1.1.3.2">𝑡</ci><ci id="S4.p2.5.m5.1.1.3.3.cmml" xref="S4.p2.5.m5.1.1.3.3">𝑎</ci><ci id="S4.p2.5.m5.1.1.3.4.cmml" xref="S4.p2.5.m5.1.1.3.4">𝑠</ci><ci id="S4.p2.5.m5.1.1.3.5.cmml" xref="S4.p2.5.m5.1.1.3.5">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.5.m5.1c">\lambda_{task}</annotation></semantics></math> is set to 1, and second, we set it to equal weights for all tasks and chose the best. We modified SpeechBrain <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/speechbrain/speechbrain" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/speechbrain/speechbrain</a></span></span></span> recipes for our implementation.</p>
</div>
</section>
<section id="S5" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Pre-trained Encoder-Decoder model ‣ 3 Model Architecture ‣ An Adapter-Based Unified Model for Multiple Spoken Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents our results, showing that our approach achieves better performance compared to the wav2vec2 SUPERB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> benchmark (+5.4) and actually also with the WavLM model (+1.4) also from SUPERB. This performance improvement can be attributed to our design choice of utilizing adapters that allows combining different tasks for improved multi-task learning. For example, for SF performance on SNIPS, the adapters where ASR, SF and IC are learned simultaneously allows an improvement of 8.3 F1 and 15.5 CER (see Table <a href="#S3.T2" title="Table 2 ‣ 3.1 Pre-trained Encoder-Decoder model ‣ 3 Model Architecture ‣ An Adapter-Based Unified Model for Multiple Spoken Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Detailed results regarding the performance of different adapter combinations are discussed in the Ablation Study. In contrast to having a frozen encoder and task-specific decoders, we incorporate task-specific adapters on a single encoder-decoder model to perform multiple SLP tasks which leads to both efficient utilization of encoder representations, and memory efficiency (see Table <a href="#S3.T3" title="Table 3 ‣ 3.1 Pre-trained Encoder-Decoder model ‣ 3 Model Architecture ‣ An Adapter-Based Unified Model for Multiple Spoken Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Ablation Study: Comparison between different types of adapter-based task modules</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Research has shown that certain tasks, like ASR and ER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, can benefit from simultaneous learning, enhancing each other’s performance. As adapters naturally enable MTL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, in addition to single-adapter task modules, we investigate two adapter-based MTL approaches: Stacking and Fusion. We hypothesize that performing MTL with adapters produces less increase in computational overhead compared to the performance improvement.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Adapter stacking: no change in the number of parameters and adapter Fusion introduces an additional 57M parameters.</span></span></span></p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Table <a href="#S3.T2" title="Table 2 ‣ 3.1 Pre-trained Encoder-Decoder model ‣ 3 Model Architecture ‣ An Adapter-Based Unified Model for Multiple Spoken Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> compares the performance amongst three different adapter settings and also with existing works that perform MTL. In the IEMOCAP dataset, the Adapter Stacking setting achieves the highest performance in Emotion Recognition. On the SNIPS dataset, the Adapter Fusion setting performs the best in SF and IC. Our performance is comparable to studies that use gold-text directly, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> with an SF-F1 score of 95.9 and IC-Acc of 98.8% on SNIPS. For FSC, there is minimal performance variation in the literature, since models already achieve above 99% accuracy. The WER is also comparable with existing works — Ours: 0.6, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>: 0.5. This performance improvement of adapter-based MTL architectures aligns with previous research indicating that MTL enhances task performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Furthermore, fine-tuning our ASR adapters for each dataset performs better than approaches that use generic ASR models, as previously demonstrated by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">In addition to the improvements in performance, our unified model shows multi-task capabilities in a parameter-efficient and scalable manner. Table <a href="#S3.T3" title="Table 3 ‣ 3.1 Pre-trained Encoder-Decoder model ‣ 3 Model Architecture ‣ An Adapter-Based Unified Model for Multiple Spoken Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the comparison between our approach and the SUPERB benchmark in terms of the number of trainable parameters needed for accommodating six tasks (as in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Pre-trained Encoder-Decoder model ‣ 3 Model Architecture ‣ An Adapter-Based Unified Model for Multiple Spoken Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) and nine tasks (including the additional ASR tasks from Table <a href="#S3.T2" title="Table 2 ‣ 3.1 Pre-trained Encoder-Decoder model ‣ 3 Model Architecture ‣ An Adapter-Based Unified Model for Multiple Spoken Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Notably, our approach requires fewer parameters, and more importantly, even as the number of tasks increases, the increase in the parameter count remains significantly lower with the ratio dropping to 53.6%.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Our work shows that adapter-based task modules effectively enable a unified encoder-decoder model for handling multiple speech-processing tasks. Our experiments show that we are able to achieve performance improvements compared to the SUPERB benchmark, while being more efficient in terms of parameters by eliminating the need for dedicated task-specific decoders. This work highlights the potential to develop simple and scalable model architectures that are capable of performing multiple SLP tasks within a unified model. In the future, our goals include evaluating our approach for different choices of SSL models such as HuBERT and WavLM and exploring different adapter architectures. Additionally, we also aim to broaden the scope of our approach to add the remaining tasks in the SUPERB benchmark such as Speaker Identification, Speaker Diarization, and other speech-processing tasks/datasets.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antonios et al. [2022]</span>
<span class="ltx_bibblock">
Anastasopoulos Antonios, Barrault Loc, Luisa Bentivogli, Marcely Zanon Boito, Bojar Ondřej, Roldano Cattoni, Currey Anna, Dinu Georgiana, Duh Kevin, Elbayad Maha, et al.

</span>
<span class="ltx_bibblock">Findings of the iwslt 2022 evaluation campaign.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proc. of the 19th Int. Conf. on Spoken Language Translation (IWSLT 2022)</em>. ACL, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ao et al. [2022]</span>
<span class="ltx_bibblock">
Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, et al.

</span>
<span class="ltx_bibblock">Speecht5: Unified-modal encoder-decoder pre-training for spoken language processing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proc. of the 60th Annual Meeting of the ACL</em>, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arora et al. [2022]</span>
<span class="ltx_bibblock">
Siddhant Arora, Siddharth Dalmia, Pavel Denisov, Xuankai Chang, Yushi Ueda, Yifan Peng, Yuekai Zhang, Sujay Kumar, Karthik Ganesan, Brian Yan, et al.

</span>
<span class="ltx_bibblock">Espnet-slu: Advancing spoken language understanding through espnet.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)</em>. IEEE, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et al. [2020]</span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.

</span>
<span class="ltx_bibblock">wav2vec 2.0: A framework for self-supervised learning of speech representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Busso et al. [2008]</span>
<span class="ltx_bibblock">
Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S Narayanan.

</span>
<span class="ltx_bibblock">Iemocap: Interactive emotional dyadic motion capture database.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Language resources and evaluation</em>, 42, 2008.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al. [2021]</span>
<span class="ltx_bibblock">
Xingyu Cai, Jiahong Yuan, Renjie Zheng, Liang Huang, and Kenneth Church.

</span>
<span class="ltx_bibblock">Speech emotion recognition with multi-task learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2021]</span>
<span class="ltx_bibblock">
Yi-Chen Chen, Po-Han Chi, Shu-wen Yang, Kai-Wei Chang, Jheng-hao Lin, Sung-Feng Huang, Da-Rong Liu, Chi-Liang Liu, Cheng-Kuang Lee, and Hung-yi Lee.

</span>
<span class="ltx_bibblock">Speechnet: A universal modularized model for speech processing tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.03070</em>, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coucke et al. [2018]</span>
<span class="ltx_bibblock">
Alice Coucke, Alaa Saade, Adrien Ball, Théodore Bluche, Alexandre Caulier, David Leroy, Clément Doumouro, Thibault Gisselbrecht, Francesco Caltagirone, Thibaut Lavril, Maël Primet, and Joseph Dureau.

</span>
<span class="ltx_bibblock">Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1805.10190, 2018.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al. [2020]</span>
<span class="ltx_bibblock">
Han Feng, Sei Ueno, and Tatsuya Kawahara.

</span>
<span class="ltx_bibblock">End-to-end speech emotion recognition combined with acoustic-to-word asr model.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. [2022]</span>
<span class="ltx_bibblock">
Xuandi Fu, Feng-Ju Chang, Martin Radfar, Kai Wei, Jing Liu, Grant P Strimel, and Kanthashree Mysore Sathyendra.

</span>
<span class="ltx_bibblock">Multi-task rnn-t with semantic decoder for streamable spoken language understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)</em>. IEEE, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gow-Smith et al. [2023]</span>
<span class="ltx_bibblock">
Edward Gow-Smith, Alexandre Berard, Marcely Zanon Boito, and Ioan Calapodescu.

</span>
<span class="ltx_bibblock">Naver labs europe’s multilingual speech translation systems for the iwslt 2023 low-resource track.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.07763</em>, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houlsby et al. [2019]</span>
<span class="ltx_bibblock">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.

</span>
<span class="ltx_bibblock">Parameter-efficient transfer learning for nlp.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Int. Conf. on Machine Learning</em>. PMLR, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al. [2020]</span>
<span class="ltx_bibblock">
Cheng-I Lai, Jin Cao, Sravan Bodapati, and Shang-Wen Li.

</span>
<span class="ltx_bibblock">Towards semi-supervised semantics understanding from speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.06195</em>, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le et al. [2021]</span>
<span class="ltx_bibblock">
Hang Le, Juan Pino, Changhan Wang, Jiatao Gu, Didier Schwab, and Laurent Besacier.

</span>
<span class="ltx_bibblock">Lightweight adapter tuning for multilingual speech translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proc. of the 59th Annual Meeting of the ACL and the 11th Int. Joint Conf. on Natural Language Processing</em>, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2018]</span>
<span class="ltx_bibblock">
Changliang Li, Cunliang Kong, and Yan Zhao.

</span>
<span class="ltx_bibblock">A joint multi-task learning framework for spoken language understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)</em>. IEEE, 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2022]</span>
<span class="ltx_bibblock">
Yuanchao Li, Peter Bell, and Catherine Lai.

</span>
<span class="ltx_bibblock">Fusing asr outputs in joint training for speech emotion recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)</em>. IEEE, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lugosch et al. [2019]</span>
<span class="ltx_bibblock">
Loren Lugosch, Mirco Ravanelli, Patrick Ignoto, Vikrant Singh Tomar, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Speech model pre-training for end-to-end spoken language understanding, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McCann et al. [2018]</span>
<span class="ltx_bibblock">
Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher.

</span>
<span class="ltx_bibblock">The natural language decathlon: Multitask learning as question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.08730</em>, 2018.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meeus et al. [2022]</span>
<span class="ltx_bibblock">
Quentin Meeus, Marie-Francine Moens, et al.

</span>
<span class="ltx_bibblock">Multitask learning for low resource spoken language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2022</em>, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panayotov et al. [2015]</span>
<span class="ltx_bibblock">
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur.

</span>
<span class="ltx_bibblock">Librispeech: an asr corpus based on public domain audio books.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Int. Conf. on acoustics, speech and signal processing (ICASSP)</em>. IEEE, 2015.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfeiffer et al. [2020]</span>
<span class="ltx_bibblock">
Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulić, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych.

</span>
<span class="ltx_bibblock">Adapterhub: A framework for adapting transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.07779</em>, 2020.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. [2021]</span>
<span class="ltx_bibblock">
Libo Qin, Tailu Liu, Wanxiang Che, Bingbing Kang, Sendong Zhao, and Ting Liu.

</span>
<span class="ltx_bibblock">A co-interactive transformer for joint slot filling and intent detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)</em>. IEEE, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2022]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.04356</em>, 2022.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2021]</span>
<span class="ltx_bibblock">
Yun Tang, Juan Pino, Changhan Wang, Xutai Ma, and Dmitriy Genzel.

</span>
<span class="ltx_bibblock">A general multi-task learning framework to leverage text data for speech to text tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)</em>. IEEE, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thomas et al. [2022]</span>
<span class="ltx_bibblock">
Bethan Thomas, Samuel Kessler, and Salah Karout.

</span>
<span class="ltx_bibblock">Efficient adapter transfer of self-supervised speech models for automatic speech recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)</em>. IEEE, 2022.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. [2017]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 2017.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Watanabe et al. [2017]</span>
<span class="ltx_bibblock">
Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R Hershey, and Tomoki Hayashi.

</span>
<span class="ltx_bibblock">Hybrid ctc/attention architecture for end-to-end speech recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, 11(8):1240–1253, 2017.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weld et al. [2022]</span>
<span class="ltx_bibblock">
Henry Weld, Xiaoqi Huang, Siqu Long, Josiah Poon, and Soyeon Caren Han.

</span>
<span class="ltx_bibblock">A survey of joint intent detection and slot filling models in natural language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, 55(8), 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2021]</span>
<span class="ltx_bibblock">
Shu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y Lin, Andy T Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, et al.

</span>
<span class="ltx_bibblock">Superb: Speech processing universal performance benchmark.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.01051</em>, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zaiem et al. [2023]</span>
<span class="ltx_bibblock">
Salah Zaiem, Youcef Kemiche, Titouan Parcollet, Slim Essid, and Mirco Ravanelli.

</span>
<span class="ltx_bibblock">Speech self-supervised representation benchmarking: Are we doing it right?

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.00452</em>, 2023.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao and Calapodescu [2022]</span>
<span class="ltx_bibblock">
Yuting Zhao and Ioan Calapodescu.

</span>
<span class="ltx_bibblock">Multimodal robustness for neural machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proc. of the 2022 Conf. on Empirical Methods in Natural Language Processing</em>, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.14746" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.14747" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.14747">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.14747" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.14748" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 20:54:16 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
