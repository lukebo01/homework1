<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.05750] A Toolkit for Joint Speaker Diarization and Identification with Application to Speaker-Attributed ASR</title><meta property="og:description" content="We present a modular toolkit to perform joint speaker diarization and speaker identification. The toolkit can leverage on multiple models and algorithms which are defined in a configuration file. Such flexibility allow…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Toolkit for Joint Speaker Diarization and Identification with Application to Speaker-Attributed ASR">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Toolkit for Joint Speaker Diarization and Identification with Application to Speaker-Attributed ASR">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.05750">

<!--Generated on Sun Oct  6 00:50:54 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=1]GiovanniMorrone
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=1]EnricoZovato
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=1]FabioBrugnara
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>[affiliation=1]EnricoSartori
<span id="p1.3.4" class="ltx_ERROR undefined">\name</span>[affiliation=1]LeonardoBadino




</p>
</div>
<h1 class="ltx_title ltx_title_document">A Toolkit for Joint Speaker Diarization and Identification with Application to Speaker-Attributed ASR</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">We present a modular toolkit to perform joint speaker diarization and speaker identification. The toolkit can leverage on multiple models and algorithms which are defined in a configuration file. Such flexibility allows our system to work properly in various conditions (e.g., multiple registered speakers' sets, acoustic conditions and languages) and across application domains (e.g. media monitoring, institutional, speech analytics). In this demonstration we show a practical use-case in which speaker-related information is used jointly with automatic speech recognition engines to generate speaker-attributed transcriptions. To achieve that, we employ a user-friendly web-based interface to process audio and video inputs with the chosen configuration.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>speaker diarization, speaker identification, speech recognition, robust speech processing
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Speaker identification consists in assigning real person identities to input audio segments. Speaker identity is a valuable information which is useful for many applications, such as speaker-attributed automatic speech recognition (ASR), speaker-based indexing, voice biometrics and more.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this demonstration we address the problem of joint speaker diarization (SD) and speaker identification (SI). Contrary to speaker verification, there is no a priori identity claim, and the system determines who the person is among a fixed set of registered speakers. Additionally, in the open-set case the system can decide that the person is unknown. We show a practical use-case in which both SD and SI information are exploited to generate speaker-attributed transcriptions.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Although several open-source and commercial products are available, they are intended for people with programming skills. Typically, only general purpose models and systems are available for end users. Such solutions sometimes fail for specific application domains, making them of limited use in real use-cases.
We aim at making these technologies available also for users without expertise in the field. We employ a user-friendly web-based application which allows users to choose among different setups that reflect various application domains (e.g. media monitoring, speech analytics, institutional). A setup defines SD and SI models, alongside with the ASR engine used for generating transcriptions.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Such flexibility increases overall system robustness across different conditions (e.g. speech styles, languages, signal quality, speakers' identities and characteristics), while maintaining an easy-to-use interface for the end user. These features are very relevant for the Interspeech 2024 theme.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.05750/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="193" height="183" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Block diagram of the overall system architecture.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>System Architecture</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A Toolkit for Joint Speaker Diarization and Identification with Application to Speaker-Attributed ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows a diagram of the proposed system architecture. It consists of three building blocks:</p>
</div>
<div id="S2.p2" class="ltx_para">
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Speaker Analysis Microservice (SAM): it provides the speaker-related functions (e.g., SD and SI).</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Audioma<sup id="S2.I1.i2.p1.1.1" class="ltx_sup">®</sup>: it controls job scheduling and orchestration. In particular it manages requests to the SAM and controls processing of ASR engines.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">FlyScribe<sup id="S2.I1.i3.p1.1.1" class="ltx_sup">®</sup>: graphical user interface for Audioma<sup id="S2.I1.i3.p1.1.2" class="ltx_sup">®</sup>.</p>
</div>
</li>
</ol>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Speaker Analysis Microservice (SAM)</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The SAM is based on the gRPC framework<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://grpc.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://grpc.io/</a></span></span></span> to efficiently provide speaker-related services to external servers and programs. It supports three functions: SD, voice activity detection (VAD) and SI.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Our SD models follow the end-to-end neural diarization (EEND)-vector clustering framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. It is a hybrid diarization approach which combines neural and clustering-based diarization. In addition to the original implementation, we have designed an improved version which reaches better results for long input signals with many speakers (e.g., <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="&gt;5" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml"></mi><mo id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">&gt;</mo><mn id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><gt id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">&gt;5</annotation></semantics></math>). In particular, it replaces the speaker embeddings generated by neural diarization with those obtained from an external pre-trained embedding extractor. We use speaker embedding extractors available in the Wespeaker toolkit<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/wenet-e2e/wespeaker/tree/master/examples/voxceleb/v2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/wenet-e2e/wespeaker/tree/master/examples/voxceleb/v2</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and trained on VoxCeleb2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">The SAM also allows to only use VAD derived from SD models. In that case, speaker-related information is discarded and only speech/non-speech segmentation is retained. Indeed, if SD is not needed then speaker embeddings extraction and clustering can be avoided to save computations.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p">Our SI system is based on speaker embedding extractors. As for SD, we employ the Wespeaker toolkit. Given an input segment, it determines whether it is uttered by a speaker belonging to a fixed set of registered speakers or by an unknown speaker. In a preliminary enrollment stage, a reference speaker embedding is computed for each registered speaker from the fixed set. During inference, a speaker embedding is extracted from the input segment and a similarity score is computed for each of the reference speaker embeddings of the fixed set. In the close-set case, the system returns the person's identity with the highest score. In the open-set case, the system can mark the input as unknown if all scores are below a similarity threshold. In this case, numeric labels are used to distinguish between different unknown speakers (e.g., Speaker1, …, SpeakerN). SI inference is performed for each speech segment detected by SD or VAD. Additionally, SI can leverage SD information about speaker identities and speech overlap to derive better speaker embeddings.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Audioma<sup id="S2.SS2.1.1" class="ltx_sup">®</sup>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Audioma<sup id="S2.SS2.p1.1.1" class="ltx_sup">®</sup> is an in-house product which provides several speech-related functions and features, including utilities for sending requests to other services (e.g., SAM), execution of ASR engines and job orchestration (i.e., submission, queuing, parallelization, processing). The whole process can be controlled by setting one or more options in specific configuration files. In particular, the most relevant options for this demonstration are the following:</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p">Preprocessing step: SD or VAD model.</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p">SI model and registered speakers' set selection.</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p id="S2.I2.i3.p1.1" class="ltx_p">ASR engine: multiple languages and domains are available.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Both general-purpose and domain-specific (e.g., media monitoring, speech analytics) modules are available. The best configuration often depends on the final application.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>FlyScribe<sup id="S2.SS3.1.1" class="ltx_sup">®</sup>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">FlyScribe<sup id="S2.SS3.p1.1.1" class="ltx_sup">®</sup> is a web application that provides a graphical user interface for Audioma<sup id="S2.SS3.p1.1.2" class="ltx_sup">®</sup> functions. Among others, it includes transcription, automatic subtitling and machine translation services.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">In particular, in this demonstration we use the transcription service. Three main steps are involved: configuration setup selection, input submission, results visualization. Two screen captures of the user interface are shown in Fig. <a href="#S3.F2" title="Figure 2 ‣ 3 Demo Application ‣ A Toolkit for Joint Speaker Diarization and Identification with Application to Speaker-Attributed ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Demo Application</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section we briefly describe what participants are expected to see during the demonstration.
</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Firstly, the user can choose between different pre-compiled configuration setups (see Figure <a href="#S3.F2.sf1" title="In Figure 2 ‣ 3 Demo Application ‣ A Toolkit for Joint Speaker Diarization and Identification with Application to Speaker-Attributed ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a>). We provide both general and domain-specific setups. For each setup a detailed description is provided to help users select the most suitable configuration for their use-cases. Then, the user can upload the audio or video and submit the request to the system.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">When the submitted job is completed the user can visualize the speaker-attributed transcriptions results in a dedicated web interface (see Figure <a href="#S3.F2.sf2" title="In Figure 2 ‣ 3 Demo Application ‣ A Toolkit for Joint Speaker Diarization and Identification with Application to Speaker-Attributed ASR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>). Additionally, an audio/video player with synchronized annotations is available. This feature is useful to allow users to navigate long recordings as users may only be interested in specific sections. The annotations can be manually edited, if needed, to guarantee human-level accuracy and readability. Finally, the results can be exported in standard formats (e.g., SRT for subtitle files).</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">We employ a Linux-based server with an Intel<sup id="S3.p4.1.1" class="ltx_sup">®</sup> Core<sup id="S3.p4.1.2" class="ltx_sup">™</sup> i7-9800X CPU @ 3.80GHz using one CPU thread without any GPU. We achieve a real-time factor of 0.18 to process a transcription job. That is acceptable for an interactive demonstration.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.05750/assets/x2.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_img_square" width="461" height="385" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">Submission</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.05750/assets/x3.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="210" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">Results visualization</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Screen captures of the demo graphical user interface.</span></figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusions</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Speaker diarization and identification technologies have reached outstanding performance in the last few years. However, the joint SD and SI task has not gained much attention. Moreover, such technologies are not easily available for end users with no programming skills. To bridge this gap, we develop a toolkit for joint SD and SI which efficiently combines the latest speaker-related technologies to guarantee both high accuracy and inference speed in different application domains. We propose an interactive demonstration in which the proposed system is used along with ASR engines to generate speaker-attributed transcriptions. Through an user-friendly interface users can choose the configuration setup most suitable to their needs, visualize results and export them in standard human-readable formats.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
K. Kinoshita, M. Delcroix, and N. Tawara, ``Advances in Integration of End-to-End Neural and Clustering-Based Diarization for Real Conversational Speech,'' in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proc. of Interspeech</em>.   ISCA, 2021, pp. 3565–3569.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
H. Wang, C. Liang, S. Wang, Z. Chen, B. Zhang, X. Xiang, Y. Deng, and Y. Qian, ``Wespeaker: A research and production oriented speaker embedding learning toolkit,'' in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proc. of International Conference on Acoustics, Speech and Signal Processing</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. S. Chung, A. Nagrani, and A. Zisserman, ``VoxCeleb2: Deep speaker recognition,'' in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proc. of Interspeech</em>.   ISCA, 2018.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.05749" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.05750" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.05750">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.05750" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.05751" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 00:50:54 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
