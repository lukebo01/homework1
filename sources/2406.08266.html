<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.08266] Refining Self-Supervised Learnt Speech Representation using Brain Activations* Corresponding author. This work is supported by the National Natural Science Foundation of China (62101523), Hefei Municipal Natural Science Foundation (2022012) and USTC Research Funds of the Double First-Class Initiative (YD2100002008).</title><meta property="og:description" content="It was shown in literature that speech representations extracted by self-supervised pre-trained models exhibit similarities with brain activations of human for speech perception and fine-tuning speech representation mo‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Refining Self-Supervised Learnt Speech Representation using Brain Activations* Corresponding author. This work is supported by the National Natural Science Foundation of China (62101523), Hefei Municipal Natural Science Foundation (2022012) and USTC Research Funds of the Double First-Class Initiative (YD2100002008).">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Refining Self-Supervised Learnt Speech Representation using Brain Activations* Corresponding author. This work is supported by the National Natural Science Foundation of China (62101523), Hefei Municipal Natural Science Foundation (2022012) and USTC Research Funds of the Double First-Class Initiative (YD2100002008).">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.08266">

<!--Generated on Fri Jul  5 21:08:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">HengyuLi
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>KangdiMei
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>ZhaociLiu
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>YangAi
<span id="p1.3.4" class="ltx_ERROR undefined">\name</span>LipingChen
<span id="p1.3.5" class="ltx_ERROR undefined">\name</span>JieZhang*
<span id="p1.3.6" class="ltx_ERROR undefined">\name</span>ZhenhuaLing</p>
</div>
<h1 class="ltx_title ltx_title_document">Refining Self-Supervised Learnt 
<br class="ltx_break">Speech Representation using Brain Activations<span id="id1.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_type">thanks: </span>* Corresponding author. This work is supported by the National Natural Science Foundation of China (62101523), Hefei Municipal Natural Science Foundation (2022012) and USTC Research Funds of the Double First-Class Initiative (YD2100002008).</span></span></span>
</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">It was shown in literature that speech representations extracted by self-supervised pre-trained models exhibit similarities with brain activations of human for speech perception and fine-tuning speech representation models on downstream tasks can further improve the similarity. However, it still remains unclear if this similarity can be used to optimize the pre-trained speech models. In this work, we therefore propose to use the brain activations recorded by fMRI to refine the often-used wav2vec2.0 model by aligning model representations toward human neural responses. Experimental results on SUPERB reveal that this operation is beneficial for several downstream tasks, e.g., speaker verification, automatic speech recognition, intent classification. One can then consider the proposed method as a new alternative to improve self-supervised speech models.
</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Pre-trained speech model, wav2vec2.0, brain activation, SUPERB
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Self-supervised learning has revolutionized the field of speech processing, where the resulting model can extract some efficient, robust and universal features from unlabeled samples through e.g., adversarial learning, clustering¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Given a small amount of labeled data, the model can be fine-tuned to further upgrade the universal features with representation capabilities into exclusive features for downstream tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, contrastive predictive coding (CPC) was proposed, which leverages adversarial learning to learn audio features. Schneider et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> followed this scheme and optimization objectives of CPC and found that using the pre-trained wav2vec model to extract features can effectively improve the performance of automatic speech recognition (ASR). Baevski et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> further proposed a quantization-based wav2vec, followed by a more advanced end-to-end self-supervised model, called wav2vec2.0¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, which is shown to be more promising for ASR. HuBERT was then proposed in¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, which provides a discretized target sequence containing acoustic and semantic information, enabling to learn richer multi-level speech features and resulting in a stronger generalization to various downstream speech processing tasks.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Due to the impressive efficacy of self-supervised speech pre-tained models¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, numerous studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> make efforts to establish connections between the general features extracted from these models and the information processing procedures of the human brain's nervous system¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. For example, Millet et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> conducted an alignment-matching study using wav2vec2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and functional magnetic resonance imaging (fMRI) brain activity records during speech listening. The result shows a correlation between the speech representation of pre-trained models and brain activations during speech perception. Similarly, Vaidya et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> validated that self-supervised speech models (e.g., APC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, wav2vec, wav2vec2.0, HuBERT) can effectively capture information levels that are related to different stages of human cortical speech processing. Oota et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> then evaluated the performance of different categories of speech models in encoding speech stimuli toward human neural activations. It was further indicated in¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> that using downstream tasks to fine-tune pre-trained models can improve the neural encoding performance.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Conversely, it is still unclear whether leveraging neural activations in the human brain can enhance the performance of pre-trained speech models on downstream tasks. For text processing, Toneva et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> suggested that updating a pre-trained language model to better predict the neural responses of human language processing can improve language understanding capacity. Schwartz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> demonstrated that using brain activity records to fine-tune BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> can enhance the ability to predict brain neural activity during language processing without compromising its performance on downstream tasks. However, there is currently no report on how to optimize the pre-trained speech models via neural encoding for downstream speech tasks. Hence, it is worthwhile to investigate similar approaches for enhancing self-supervised pre-trained speech models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, which are thus called <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">neuroscience-driven</span>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, in this work we make efforts to explore the potential of pre-trained speech models on downstream tasks by incorporating neural activations from human speech perception into model parameters. Specifically, the well-known wav2vec2.0 is selected as the examplary pre-trained speech model without loss of generality. The proposed method refines wav2vec2.0 with brain activations by adding convolutional and linear layers on top of the model. These layers aim to predict brain activations from speech signals, and the parameters of wav2vec2.0 are updated using the L2-regularized mean square error (MSE) loss. Additionally, historical audio information, based on the predictive coding theory <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, is utilized as the input for prediction. After fine-tuning, the refined model is evaluated using the general speech processing performance benchmark (SUPERB) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. The obtained results verify the efficacy of the proposed method on several downstream tasks.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2406.08266/assets/model.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="295" height="226" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Flowchart of refining wav2vec2.0 using brain signals.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Material and Pre-processing</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The ‚Äútunnel under the world‚Äù subset of the ‚ÄúNarratives" dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> is adopted throughout experiments in this paper, which comprises various fMRI data collected from human subjects when listening to natural oral stories.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The audio stimulus includes 3435 words at a length of 1534 seconds. Silences of 3 seconds and 23 seconds are inserted before and after the stimulus, respectively, resulting in a total fMRI scan duration of 1560 seconds. This scan consists of 1040 repetition times (TR) with 1.5 seconds per TR. During each TR, the scan outputs blood oxygen level dependent (BOLD) signals, which are widely-used for analyzing brain activation and functional connectivity. These signals are measured based on the magnetic changes between the oxygenated and deoxygenated states of hemoglobin in the blood. The fMRI data used in this study originate from the dataset published on datalad<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="http://datasets.datalad.org/?dir=/labs/hasson/narratives" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://datasets.datalad.org/?dir=/labs/hasson/narratives</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">We exclude subjects 004 and 013 as recommended in the dataset. The neural responses of the remaining 21 subjects are utilized in experiments. These fMRI data have to be pre-processed before being applied to refine the pre-trained speech model. Initially, the brain activations represented by BOLD signals at each TR are projected onto the surface space of `fsaverage6' <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, composed of 40962 voxels. Subsequently, only the voxels corresponding to the brain regions of interest (ROIs) related to auditory and language processing are selected. The Glasser atlas <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> is adopted for this purpose, partitioning each hemisphere into 180 ROIs. The selected voxels are from ROIs of the early auditory cortex (EAC), auditory association cortex (AAC) and inferior frontal gyrus (IFG). Finally, the pre-processed BOLD signals in each TR are represented by a vector of 5085 dimensions, corresponding to 2468 voxels in the right brain and 2617 voxels in the left brain.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Refining wav2vec2.0 by Predicting Brain Activations</h3>

<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Problem Description</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">The goal of this work is to construct a neural encoding model by adding additional layers on top of the pre-trained wav2vec2.0, which can predict subject's brain activations given the speech stimulus. Due to the data conditions in Section 2.1, the model output is the 5085-dimensional pre-processed BOLD vector at each TR, and the model input comprises 16kHz speech waveforms corresponding to the current and previous TRs. The wav2vec2.0 model is refined by optimizing the prediction error. It is thus interesting to investigate whether this refinement can enhance the performance of wav2vec2.0 on downstream tasks.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Vanilla wav2vec2.0 Model</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.4" class="ltx_p">In this work, as an example Wav2vec2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> is adopted as the self-supervised speech model. It integrates the Gumbel softmax quantization module from vq-wav2vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> in an end-to-end fashion. In wav2vec2.0, raw speech waveforms first pass through a series of convolutional layers to generate latent speech representations <span id="S2.SS2.SSS2.p1.4.1" class="ltx_text ltx_markedasmath ltx_font_bold">Z</span>, which are then masked and fed into the Transformer to obtain context representations <span id="S2.SS2.SSS2.p1.4.2" class="ltx_text ltx_markedasmath ltx_font_bold">C</span>. Simultaneously, quantized representations <span id="S2.SS2.SSS2.p1.4.3" class="ltx_text ltx_markedasmath ltx_font_bold">Q</span> are obtained via product quantization on <span id="S2.SS2.SSS2.p1.4.4" class="ltx_text ltx_markedasmath ltx_font_bold">Z</span>. The self-supervised loss function for pre-training comprises a contrastive loss and a diversity loss. The former discriminates positive samples from negative ones for the quantized representations at the masked positions. Meanwhile, the latter loss aims to enhance the representation ability of the quantization codebook by encouraging the model to utilize all codebook entries equally. After pre-training, the model can be applied to downstream tasks by cascading task-specific layers and fine-tuning the entire model using labeled data in combination with task-specific loss function(s).</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The results of vanilla and refined wav2vec2.0 models on SUPERB.
For ASR, the WER is evaluated without language models. The result better than vanilla-base for each task is highlighted in bold. The stars indicates a significant improvement (<math id="S2.T1.2.m1.1" class="ltx_Math" alttext="p&lt;0.05" display="inline"><semantics id="S2.T1.2.m1.1b"><mrow id="S2.T1.2.m1.1.1" xref="S2.T1.2.m1.1.1.cmml"><mi id="S2.T1.2.m1.1.1.2" xref="S2.T1.2.m1.1.1.2.cmml">p</mi><mo id="S2.T1.2.m1.1.1.1" xref="S2.T1.2.m1.1.1.1.cmml">&lt;</mo><mn id="S2.T1.2.m1.1.1.3" xref="S2.T1.2.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.2.m1.1c"><apply id="S2.T1.2.m1.1.1.cmml" xref="S2.T1.2.m1.1.1"><lt id="S2.T1.2.m1.1.1.1.cmml" xref="S2.T1.2.m1.1.1.1"></lt><ci id="S2.T1.2.m1.1.1.2.cmml" xref="S2.T1.2.m1.1.1.2">ùëù</ci><cn type="float" id="S2.T1.2.m1.1.1.3.cmml" xref="S2.T1.2.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.m1.1d">p&lt;0.05</annotation></semantics></math>) in one-tailed paired t-test between refined and vanilla models.</figcaption>
<table id="S2.T1.13" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.13.12.1" class="ltx_tr">
<td id="S2.T1.13.12.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S2.T1.13.12.1.1.1" class="ltx_text">Model</span></td>
<td id="S2.T1.13.12.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">PR</td>
<td id="S2.T1.13.12.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">KS</td>
<td id="S2.T1.13.12.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">IC</td>
<td id="S2.T1.13.12.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SID</td>
<td id="S2.T1.13.12.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ER</td>
<td id="S2.T1.13.12.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ASR</td>
<td id="S2.T1.13.12.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">SF</td>
<td id="S2.T1.13.12.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ASV</td>
<td id="S2.T1.13.12.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Overall</td>
</tr>
<tr id="S2.T1.13.11" class="ltx_tr">
<td id="S2.T1.3.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">PER <math id="S2.T1.3.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.3.1.1.m1.1a"><mo stretchy="false" id="S2.T1.3.1.1.m1.1.1" xref="S2.T1.3.1.1.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.1.1.m1.1b"><ci id="S2.T1.3.1.1.m1.1.1.cmml" xref="S2.T1.3.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S2.T1.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ACC <math id="S2.T1.4.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.4.2.2.m1.1a"><mo stretchy="false" id="S2.T1.4.2.2.m1.1.1" xref="S2.T1.4.2.2.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S2.T1.4.2.2.m1.1b"><ci id="S2.T1.4.2.2.m1.1.1.cmml" xref="S2.T1.4.2.2.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S2.T1.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ACC <math id="S2.T1.5.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.5.3.3.m1.1a"><mo stretchy="false" id="S2.T1.5.3.3.m1.1.1" xref="S2.T1.5.3.3.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S2.T1.5.3.3.m1.1b"><ci id="S2.T1.5.3.3.m1.1.1.cmml" xref="S2.T1.5.3.3.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.3.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S2.T1.6.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ACC <math id="S2.T1.6.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.6.4.4.m1.1a"><mo stretchy="false" id="S2.T1.6.4.4.m1.1.1" xref="S2.T1.6.4.4.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S2.T1.6.4.4.m1.1b"><ci id="S2.T1.6.4.4.m1.1.1.cmml" xref="S2.T1.6.4.4.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.4.4.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S2.T1.7.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ACC <math id="S2.T1.7.5.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.7.5.5.m1.1a"><mo stretchy="false" id="S2.T1.7.5.5.m1.1.1" xref="S2.T1.7.5.5.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S2.T1.7.5.5.m1.1b"><ci id="S2.T1.7.5.5.m1.1.1.cmml" xref="S2.T1.7.5.5.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.5.5.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S2.T1.8.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">WER <math id="S2.T1.8.6.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.8.6.6.m1.1a"><mo stretchy="false" id="S2.T1.8.6.6.m1.1.1" xref="S2.T1.8.6.6.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S2.T1.8.6.6.m1.1b"><ci id="S2.T1.8.6.6.m1.1.1.cmml" xref="S2.T1.8.6.6.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.8.6.6.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S2.T1.9.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">F1 <math id="S2.T1.9.7.7.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.9.7.7.m1.1a"><mo stretchy="false" id="S2.T1.9.7.7.m1.1.1" xref="S2.T1.9.7.7.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S2.T1.9.7.7.m1.1b"><ci id="S2.T1.9.7.7.m1.1.1.cmml" xref="S2.T1.9.7.7.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.9.7.7.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S2.T1.10.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CER <math id="S2.T1.10.8.8.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.10.8.8.m1.1a"><mo stretchy="false" id="S2.T1.10.8.8.m1.1.1" xref="S2.T1.10.8.8.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S2.T1.10.8.8.m1.1b"><ci id="S2.T1.10.8.8.m1.1.1.cmml" xref="S2.T1.10.8.8.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.10.8.8.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S2.T1.11.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">EER <math id="S2.T1.11.9.9.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.11.9.9.m1.1a"><mo stretchy="false" id="S2.T1.11.9.9.m1.1.1" xref="S2.T1.11.9.9.m1.1.1.cmml">‚Üì</mo><annotation-xml encoding="MathML-Content" id="S2.T1.11.9.9.m1.1b"><ci id="S2.T1.11.9.9.m1.1.1.cmml" xref="S2.T1.11.9.9.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.11.9.9.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S2.T1.13.11.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S2.T1.12.10.10.m1.1" class="ltx_Math" alttext="{\rm superb}_{s}" display="inline"><semantics id="S2.T1.12.10.10.m1.1a"><msub id="S2.T1.12.10.10.m1.1.1" xref="S2.T1.12.10.10.m1.1.1.cmml"><mi id="S2.T1.12.10.10.m1.1.1.2" xref="S2.T1.12.10.10.m1.1.1.2.cmml">superb</mi><mi id="S2.T1.12.10.10.m1.1.1.3" xref="S2.T1.12.10.10.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.T1.12.10.10.m1.1b"><apply id="S2.T1.12.10.10.m1.1.1.cmml" xref="S2.T1.12.10.10.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.12.10.10.m1.1.1.1.cmml" xref="S2.T1.12.10.10.m1.1.1">subscript</csymbol><ci id="S2.T1.12.10.10.m1.1.1.2.cmml" xref="S2.T1.12.10.10.m1.1.1.2">superb</ci><ci id="S2.T1.12.10.10.m1.1.1.3.cmml" xref="S2.T1.12.10.10.m1.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.12.10.10.m1.1c">{\rm superb}_{s}</annotation></semantics></math> <math id="S2.T1.13.11.11.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.13.11.11.m2.1a"><mo stretchy="false" id="S2.T1.13.11.11.m2.1.1" xref="S2.T1.13.11.11.m2.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S2.T1.13.11.11.m2.1b"><ci id="S2.T1.13.11.11.m2.1.1.cmml" xref="S2.T1.13.11.11.m2.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.13.11.11.m2.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S2.T1.13.13.2" class="ltx_tr">
<td id="S2.T1.13.13.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Vanilla-base</td>
<td id="S2.T1.13.13.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.03</td>
<td id="S2.T1.13.13.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">96.23</td>
<td id="S2.T1.13.13.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">92.57</td>
<td id="S2.T1.13.13.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">74.45</td>
<td id="S2.T1.13.13.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">62.53</td>
<td id="S2.T1.13.13.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.60</td>
<td id="S2.T1.13.13.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T1.13.13.2.8.1" class="ltx_text ltx_font_bold">87.65</span></td>
<td id="S2.T1.13.13.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">25.83</td>
<td id="S2.T1.13.13.2.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.98</td>
<td id="S2.T1.13.13.2.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
</tr>
<tr id="S2.T1.13.14.3" class="ltx_tr">
<td id="S2.T1.13.14.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Vanilla-large(LV-60)</td>
<td id="S2.T1.13.14.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.75</td>
<td id="S2.T1.13.14.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">96.66</td>
<td id="S2.T1.13.14.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">95.28</td>
<td id="S2.T1.13.14.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.14</td>
<td id="S2.T1.13.14.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">65.64</td>
<td id="S2.T1.13.14.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.75</td>
<td id="S2.T1.13.14.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">87.11</td>
<td id="S2.T1.13.14.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27.31</td>
<td id="S2.T1.13.14.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5.65</td>
<td id="S2.T1.13.14.3.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1000</td>
</tr>
<tr id="S2.T1.13.15.4" class="ltx_tr">
<td id="S2.T1.13.15.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Stimuli-pretrain</td>
<td id="S2.T1.13.15.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.22</td>
<td id="S2.T1.13.15.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">95.72</td>
<td id="S2.T1.13.15.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">91.93</td>
<td id="S2.T1.13.15.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">73.88</td>
<td id="S2.T1.13.15.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">62.23</td>
<td id="S2.T1.13.15.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.70</td>
<td id="S2.T1.13.15.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">86.25</td>
<td id="S2.T1.13.15.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27.07</td>
<td id="S2.T1.13.15.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.08</td>
<td id="S2.T1.13.15.4.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-293.43</td>
</tr>
<tr id="S2.T1.13.16.5" class="ltx_tr">
<td id="S2.T1.13.16.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Refined</td>
<td id="S2.T1.13.16.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.13.16.5.2.1" class="ltx_text ltx_font_bold">5.67*</span></td>
<td id="S2.T1.13.16.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">96.23</td>
<td id="S2.T1.13.16.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.13.16.5.4.1" class="ltx_text ltx_font_bold">93.78*</span></td>
<td id="S2.T1.13.16.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.13.16.5.5.1" class="ltx_text ltx_font_bold">75.28*</span></td>
<td id="S2.T1.13.16.5.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.13.16.5.6.1" class="ltx_text ltx_font_bold">63.72</span></td>
<td id="S2.T1.13.16.5.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.13.16.5.7.1" class="ltx_text ltx_font_bold">6.36*</span></td>
<td id="S2.T1.13.16.5.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">87.31</td>
<td id="S2.T1.13.16.5.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.13.16.5.9.1" class="ltx_text ltx_font_bold">25.15</span></td>
<td id="S2.T1.13.16.5.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.13.16.5.10.1" class="ltx_text ltx_font_bold">5.50*</span></td>
<td id="S2.T1.13.16.5.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S2.T1.13.16.5.11.1" class="ltx_text ltx_font_bold">388.59</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Model Architecture</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">The structure of the neural encoding model is shown in Fig. 1. For each TR, the speech waveforms of the current TR and the previous <math id="S2.SS2.SSS3.p1.1.m1.1" class="ltx_Math" alttext="n-1" display="inline"><semantics id="S2.SS2.SSS3.p1.1.m1.1a"><mrow id="S2.SS2.SSS3.p1.1.m1.1.1" xref="S2.SS2.SSS3.p1.1.m1.1.1.cmml"><mi id="S2.SS2.SSS3.p1.1.m1.1.1.2" xref="S2.SS2.SSS3.p1.1.m1.1.1.2.cmml">n</mi><mo id="S2.SS2.SSS3.p1.1.m1.1.1.1" xref="S2.SS2.SSS3.p1.1.m1.1.1.1.cmml">‚àí</mo><mn id="S2.SS2.SSS3.p1.1.m1.1.1.3" xref="S2.SS2.SSS3.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p1.1.m1.1b"><apply id="S2.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS3.p1.1.m1.1.1"><minus id="S2.SS2.SSS3.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS3.p1.1.m1.1.1.1"></minus><ci id="S2.SS2.SSS3.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS3.p1.1.m1.1.1.2">ùëõ</ci><cn type="integer" id="S2.SS2.SSS3.p1.1.m1.1.1.3.cmml" xref="S2.SS2.SSS3.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p1.1.m1.1c">n-1</annotation></semantics></math> TRs are utilized as the model input. The reason of using historical speech waveforms is twofold: 1) the asynchronous relationship between the speech stimulus and BOLD signals is considered, in line with the mechanism of BOLD acquisition. Due to the inertia of changes in blood oxygen levels, the BOLD response reaches its peak after a latency period of several seconds, making historical information relevant. 2) the predictive coding theory <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> suggests that the human brain can effectively utilize long-term information in speech. Therefore, incorporating historical information into the speech input may enhance the model's ability to perform neural encoding.</p>
</div>
<div id="S2.SS2.SSS3.p2" class="ltx_para">
<p id="S2.SS2.SSS3.p2.4" class="ltx_p">In Fig.1, the input speech waveforms of <math id="S2.SS2.SSS3.p2.1.m1.1" class="ltx_Math" alttext="1.5n" display="inline"><semantics id="S2.SS2.SSS3.p2.1.m1.1a"><mrow id="S2.SS2.SSS3.p2.1.m1.1.1" xref="S2.SS2.SSS3.p2.1.m1.1.1.cmml"><mn id="S2.SS2.SSS3.p2.1.m1.1.1.2" xref="S2.SS2.SSS3.p2.1.m1.1.1.2.cmml">1.5</mn><mo lspace="0em" rspace="0em" id="S2.SS2.SSS3.p2.1.m1.1.1.1" xref="S2.SS2.SSS3.p2.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S2.SS2.SSS3.p2.1.m1.1.1.3" xref="S2.SS2.SSS3.p2.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p2.1.m1.1b"><apply id="S2.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS3.p2.1.m1.1.1"><times id="S2.SS2.SSS3.p2.1.m1.1.1.1.cmml" xref="S2.SS2.SSS3.p2.1.m1.1.1.1"></times><cn type="float" id="S2.SS2.SSS3.p2.1.m1.1.1.2.cmml" xref="S2.SS2.SSS3.p2.1.m1.1.1.2">1.5</cn><ci id="S2.SS2.SSS3.p2.1.m1.1.1.3.cmml" xref="S2.SS2.SSS3.p2.1.m1.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p2.1.m1.1c">1.5n</annotation></semantics></math> seconds are initially processed by wav2vec2.0, and the computed context representations <span id="S2.SS2.SSS3.p2.4.1" class="ltx_text ltx_markedasmath ltx_font_bold">C</span> are employed to predict the BOLD vector at the current TR. In wav2vec2.0, the frame rate of context representations is 50Hz, and the input speech waveform consist of <math id="S2.SS2.SSS3.p2.3.m3.1" class="ltx_Math" alttext="75n" display="inline"><semantics id="S2.SS2.SSS3.p2.3.m3.1a"><mrow id="S2.SS2.SSS3.p2.3.m3.1.1" xref="S2.SS2.SSS3.p2.3.m3.1.1.cmml"><mn id="S2.SS2.SSS3.p2.3.m3.1.1.2" xref="S2.SS2.SSS3.p2.3.m3.1.1.2.cmml">75</mn><mo lspace="0em" rspace="0em" id="S2.SS2.SSS3.p2.3.m3.1.1.1" xref="S2.SS2.SSS3.p2.3.m3.1.1.1.cmml">‚Äã</mo><mi id="S2.SS2.SSS3.p2.3.m3.1.1.3" xref="S2.SS2.SSS3.p2.3.m3.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p2.3.m3.1b"><apply id="S2.SS2.SSS3.p2.3.m3.1.1.cmml" xref="S2.SS2.SSS3.p2.3.m3.1.1"><times id="S2.SS2.SSS3.p2.3.m3.1.1.1.cmml" xref="S2.SS2.SSS3.p2.3.m3.1.1.1"></times><cn type="integer" id="S2.SS2.SSS3.p2.3.m3.1.1.2.cmml" xref="S2.SS2.SSS3.p2.3.m3.1.1.2">75</cn><ci id="S2.SS2.SSS3.p2.3.m3.1.1.3.cmml" xref="S2.SS2.SSS3.p2.3.m3.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p2.3.m3.1c">75n</annotation></semantics></math> frames in total. To downsample the context representations, four convolutional layers with strides of <math id="S2.SS2.SSS3.p2.4.m4.4" class="ltx_Math" alttext="\{n,5,5,3\}" display="inline"><semantics id="S2.SS2.SSS3.p2.4.m4.4a"><mrow id="S2.SS2.SSS3.p2.4.m4.4.5.2" xref="S2.SS2.SSS3.p2.4.m4.4.5.1.cmml"><mo stretchy="false" id="S2.SS2.SSS3.p2.4.m4.4.5.2.1" xref="S2.SS2.SSS3.p2.4.m4.4.5.1.cmml">{</mo><mi id="S2.SS2.SSS3.p2.4.m4.1.1" xref="S2.SS2.SSS3.p2.4.m4.1.1.cmml">n</mi><mo id="S2.SS2.SSS3.p2.4.m4.4.5.2.2" xref="S2.SS2.SSS3.p2.4.m4.4.5.1.cmml">,</mo><mn id="S2.SS2.SSS3.p2.4.m4.2.2" xref="S2.SS2.SSS3.p2.4.m4.2.2.cmml">5</mn><mo id="S2.SS2.SSS3.p2.4.m4.4.5.2.3" xref="S2.SS2.SSS3.p2.4.m4.4.5.1.cmml">,</mo><mn id="S2.SS2.SSS3.p2.4.m4.3.3" xref="S2.SS2.SSS3.p2.4.m4.3.3.cmml">5</mn><mo id="S2.SS2.SSS3.p2.4.m4.4.5.2.4" xref="S2.SS2.SSS3.p2.4.m4.4.5.1.cmml">,</mo><mn id="S2.SS2.SSS3.p2.4.m4.4.4" xref="S2.SS2.SSS3.p2.4.m4.4.4.cmml">3</mn><mo stretchy="false" id="S2.SS2.SSS3.p2.4.m4.4.5.2.5" xref="S2.SS2.SSS3.p2.4.m4.4.5.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p2.4.m4.4b"><set id="S2.SS2.SSS3.p2.4.m4.4.5.1.cmml" xref="S2.SS2.SSS3.p2.4.m4.4.5.2"><ci id="S2.SS2.SSS3.p2.4.m4.1.1.cmml" xref="S2.SS2.SSS3.p2.4.m4.1.1">ùëõ</ci><cn type="integer" id="S2.SS2.SSS3.p2.4.m4.2.2.cmml" xref="S2.SS2.SSS3.p2.4.m4.2.2">5</cn><cn type="integer" id="S2.SS2.SSS3.p2.4.m4.3.3.cmml" xref="S2.SS2.SSS3.p2.4.m4.3.3">5</cn><cn type="integer" id="S2.SS2.SSS3.p2.4.m4.4.4.cmml" xref="S2.SS2.SSS3.p2.4.m4.4.4">3</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p2.4.m4.4c">\{n,5,5,3\}</annotation></semantics></math> are inserted. A z-score standardization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> is then applied to the output of the convolution layers. Following the standard neural encoding models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, a linear layer is added at the end of the model to predict the BOLD response at the current TR.</p>
</div>
<div id="S2.SS2.SSS3.p3" class="ltx_para">
<p id="S2.SS2.SSS3.p3.1" class="ltx_p">An L2-regularized MSE loss is used to train the neural encoding model. Considering the limited amount of fMRI data, a two-stage strategy is utilized. After pre-training wav2vec2.0 and randomly initializing the convolutional and linear output layers, the first stage only focuses on updating the linear layer while freezing other modules. Once the training of this stage converges, the linear layer is frozen, and the rest of the model, including wav2vec2.0, is updated at the second stage.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments and Results</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Implementation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p">The vanilla-base wav2vec2.0 is provided by fairseq, which was already pre-trained on Librispeech-960h dataset<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec</a></span></span></span>. The vanilla-large(LV-60) wav2vec 2.0 Large is the comparison model originated from fairseq, featuring a larger network and pre-trained on LibriLight-60kh dataset<sup id="S3.SS1.p1.2.1" class="ltx_sup"><a href="#footnote2" title="footnote 2 ‚Ä£ 3.1 Implementation ‚Ä£ 3 Experiments and Results ‚Ä£ Refining Self-Supervised Learnt Speech Representation using Brain Activations* Corresponding author. This work is supported by the National Natural Science Foundation of China (62101523), Hefei Municipal Natural Science Foundation (2022012) and USTC Research Funds of the Double First-Class Initiative (YD2100002008)." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></sup>. For the proposed method, the TRs are shuffled and divided into a training set, a validation set and a test set at a ratio of 8:1:1. The validation set is employed to tune the L2 regularization weight <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\lambda</annotation></semantics></math>, which varies between 1e-3 and 1e-1.
The four convolutional layers have strides of <math id="S3.SS1.p1.2.m2.4" class="ltx_Math" alttext="\{n,5,5,3\}" display="inline"><semantics id="S3.SS1.p1.2.m2.4a"><mrow id="S3.SS1.p1.2.m2.4.5.2" xref="S3.SS1.p1.2.m2.4.5.1.cmml"><mo stretchy="false" id="S3.SS1.p1.2.m2.4.5.2.1" xref="S3.SS1.p1.2.m2.4.5.1.cmml">{</mo><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">n</mi><mo id="S3.SS1.p1.2.m2.4.5.2.2" xref="S3.SS1.p1.2.m2.4.5.1.cmml">,</mo><mn id="S3.SS1.p1.2.m2.2.2" xref="S3.SS1.p1.2.m2.2.2.cmml">5</mn><mo id="S3.SS1.p1.2.m2.4.5.2.3" xref="S3.SS1.p1.2.m2.4.5.1.cmml">,</mo><mn id="S3.SS1.p1.2.m2.3.3" xref="S3.SS1.p1.2.m2.3.3.cmml">5</mn><mo id="S3.SS1.p1.2.m2.4.5.2.4" xref="S3.SS1.p1.2.m2.4.5.1.cmml">,</mo><mn id="S3.SS1.p1.2.m2.4.4" xref="S3.SS1.p1.2.m2.4.4.cmml">3</mn><mo stretchy="false" id="S3.SS1.p1.2.m2.4.5.2.5" xref="S3.SS1.p1.2.m2.4.5.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.4b"><set id="S3.SS1.p1.2.m2.4.5.1.cmml" xref="S3.SS1.p1.2.m2.4.5.2"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ùëõ</ci><cn type="integer" id="S3.SS1.p1.2.m2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2">5</cn><cn type="integer" id="S3.SS1.p1.2.m2.3.3.cmml" xref="S3.SS1.p1.2.m2.3.3">5</cn><cn type="integer" id="S3.SS1.p1.2.m2.4.4.cmml" xref="S3.SS1.p1.2.m2.4.4">3</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.4c">\{n,5,5,3\}</annotation></semantics></math> with a kernel size of 3, and the padding size of the first layer is 1. Each layer performs batch normalization and the activation function is ReLU.
At the first stage of the model training, 60 epochs are conducted using a basic learning rate of 3e-3, but the second stage uses 60 epochs at a learning rate of 3e-4. In the first 10% epochs of both stages, the learning rate linearly increases from 0 to the basic learning rate, and in the subsequent epochs it linearly decreases to 0.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">To observe the benefit of the alignment from additional training steps, we introduce a ‚Äústimuli-pretrain‚Äù model, which undergoes follow-up pre-training using the audio stimulus in the ‚ÄúNarratives‚Äù dataset with the same number of training steps as the refined training for comparison. The training is performed upon the same configuration file as in the vanilla-base model.
All the trainings are conducted on a server equipped with 4 Nvidia A100 GPUs, each with 80GB GPU RAM.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.2" class="ltx_p">In theory, the BOLD signals induced by the stimulus gradually rise after about 1-2s and reach the peak at 5-6s<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. This characteristic is roughly consistent in the motor, visual, and auditory fields¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, but the change in prefrontal cortex (which is related to cognition and emotion) is slower than that in the visual area by 4s¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Considering the differences in the prefrontal lobe, a relatively high level of BOLD signal will be achieved at a time offset of 9s, that is, <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">n</annotation></semantics></math> = 6. In other words, setting the input history waveforms to 9s may enable the model to learn brain activation information more effectively. Therefore, we will use <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">n</annotation></semantics></math> = 6 as an example in the subsequent analyses.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Evaluation Tasks for Pre-trained Models</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">SUPERB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> aims to directly use pre-trained speech models on various downstream tasks by establishing a framework that uses pre-trained models with frozen parameters and lightweight prediction heads tuned for each task. This framework reflects four categories of speech tasks: content, speaker, semantics and paralinguistics. We evaluate the performance on 3 content tasks including phoneme recognition (PR), ASR and keyword spotting (KS), 2 speaker tasks of speaker identification (SID) and automatic speaker verification (ASV), 2 semantic tasks including intent classification (IC) and slot filling (SF), and 1 paralinguistics task of emotional recognition (ER). When designing and training these prediction heads for downstream tasks, we follow the default configuration of SUPERB, except that the learning rate of 1e-3 is used for the SID task. The evaluation results of all compared models are obtained by applying them to downstream tasks following the SUPERB framework with our own implementation, except the results of vanilla-large which are published ones <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.5" class="ltx_p">In SUPERB, superb-score (<math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="{\rm superb}_{s}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">superb</mi><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">superb</ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">{\rm superb}_{s}</annotation></semantics></math>) is utilized to measure the overall performance of upstream models<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://superbbenchmark.org/challenge-slt2022/metrics" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://superbbenchmark.org/challenge-slt2022/metrics</a></span></span></span>.
For each downstream task <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">t</annotation></semantics></math>, we define a specific metric ranging from [0, 1000] to evaluate the performance of the refined model <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="u" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mi id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><ci id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">ùë¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">u</annotation></semantics></math>, following the definition of <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="{\rm superb}_{s}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><msub id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">superb</mi><mi id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">superb</ci><ci id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">{\rm superb}_{s}</annotation></semantics></math>. By weighting the metrics on all the <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mi id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">T</annotation></semantics></math> tasks, the overall performance can then be calculated, where the vanilla-large (LV-60) is used as the upper limit of the evaluation and vanilla-base as the lower bound, given by</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.10" class="ltx_Math" alttext="{\rm superb}_{s}=\frac{1}{|T|}\sum_{t\in\{1,\cdots,T\}}\frac{1000\left(s_{t}(u)-s_{t}(\text{base})\right)}{s_{t}(\text{large})-s_{t}(\text{base})}." display="block"><semantics id="S3.E1.m1.10a"><mrow id="S3.E1.m1.10.10.1" xref="S3.E1.m1.10.10.1.1.cmml"><mrow id="S3.E1.m1.10.10.1.1" xref="S3.E1.m1.10.10.1.1.cmml"><msub id="S3.E1.m1.10.10.1.1.2" xref="S3.E1.m1.10.10.1.1.2.cmml"><mi id="S3.E1.m1.10.10.1.1.2.2" xref="S3.E1.m1.10.10.1.1.2.2.cmml">superb</mi><mi id="S3.E1.m1.10.10.1.1.2.3" xref="S3.E1.m1.10.10.1.1.2.3.cmml">s</mi></msub><mo id="S3.E1.m1.10.10.1.1.1" xref="S3.E1.m1.10.10.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.10.10.1.1.3" xref="S3.E1.m1.10.10.1.1.3.cmml"><mfrac id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mn id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml">1</mn><mrow id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.2.1.cmml">|</mo><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">T</mi><mo stretchy="false" id="S3.E1.m1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="S3.E1.m1.10.10.1.1.3.1" xref="S3.E1.m1.10.10.1.1.3.1.cmml">‚Äã</mo><mrow id="S3.E1.m1.10.10.1.1.3.2" xref="S3.E1.m1.10.10.1.1.3.2.cmml"><munder id="S3.E1.m1.10.10.1.1.3.2.1" xref="S3.E1.m1.10.10.1.1.3.2.1.cmml"><mo movablelimits="false" id="S3.E1.m1.10.10.1.1.3.2.1.2" xref="S3.E1.m1.10.10.1.1.3.2.1.2.cmml">‚àë</mo><mrow id="S3.E1.m1.4.4.3" xref="S3.E1.m1.4.4.3.cmml"><mi id="S3.E1.m1.4.4.3.5" xref="S3.E1.m1.4.4.3.5.cmml">t</mi><mo id="S3.E1.m1.4.4.3.4" xref="S3.E1.m1.4.4.3.4.cmml">‚àà</mo><mrow id="S3.E1.m1.4.4.3.6.2" xref="S3.E1.m1.4.4.3.6.1.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.3.6.2.1" xref="S3.E1.m1.4.4.3.6.1.cmml">{</mo><mn id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml">1</mn><mo id="S3.E1.m1.4.4.3.6.2.2" xref="S3.E1.m1.4.4.3.6.1.cmml">,</mo><mi mathvariant="normal" id="S3.E1.m1.3.3.2.2" xref="S3.E1.m1.3.3.2.2.cmml">‚ãØ</mi><mo id="S3.E1.m1.4.4.3.6.2.3" xref="S3.E1.m1.4.4.3.6.1.cmml">,</mo><mi id="S3.E1.m1.4.4.3.3" xref="S3.E1.m1.4.4.3.3.cmml">T</mi><mo stretchy="false" id="S3.E1.m1.4.4.3.6.2.4" xref="S3.E1.m1.4.4.3.6.1.cmml">}</mo></mrow></mrow></munder><mfrac id="S3.E1.m1.9.9" xref="S3.E1.m1.9.9.cmml"><mrow id="S3.E1.m1.7.7.3" xref="S3.E1.m1.7.7.3.cmml"><mn id="S3.E1.m1.7.7.3.5" xref="S3.E1.m1.7.7.3.5.cmml">1000</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.3.4" xref="S3.E1.m1.7.7.3.4.cmml">‚Äã</mo><mrow id="S3.E1.m1.7.7.3.3.1" xref="S3.E1.m1.7.7.3.3.1.1.cmml"><mo id="S3.E1.m1.7.7.3.3.1.2" xref="S3.E1.m1.7.7.3.3.1.1.cmml">(</mo><mrow id="S3.E1.m1.7.7.3.3.1.1" xref="S3.E1.m1.7.7.3.3.1.1.cmml"><mrow id="S3.E1.m1.7.7.3.3.1.1.2" xref="S3.E1.m1.7.7.3.3.1.1.2.cmml"><msub id="S3.E1.m1.7.7.3.3.1.1.2.2" xref="S3.E1.m1.7.7.3.3.1.1.2.2.cmml"><mi id="S3.E1.m1.7.7.3.3.1.1.2.2.2" xref="S3.E1.m1.7.7.3.3.1.1.2.2.2.cmml">s</mi><mi id="S3.E1.m1.7.7.3.3.1.1.2.2.3" xref="S3.E1.m1.7.7.3.3.1.1.2.2.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.3.3.1.1.2.1" xref="S3.E1.m1.7.7.3.3.1.1.2.1.cmml">‚Äã</mo><mrow id="S3.E1.m1.7.7.3.3.1.1.2.3.2" xref="S3.E1.m1.7.7.3.3.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.3.3.1.1.2.3.2.1" xref="S3.E1.m1.7.7.3.3.1.1.2.cmml">(</mo><mi id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.cmml">u</mi><mo stretchy="false" id="S3.E1.m1.7.7.3.3.1.1.2.3.2.2" xref="S3.E1.m1.7.7.3.3.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.7.7.3.3.1.1.1" xref="S3.E1.m1.7.7.3.3.1.1.1.cmml">‚àí</mo><mrow id="S3.E1.m1.7.7.3.3.1.1.3" xref="S3.E1.m1.7.7.3.3.1.1.3.cmml"><msub id="S3.E1.m1.7.7.3.3.1.1.3.2" xref="S3.E1.m1.7.7.3.3.1.1.3.2.cmml"><mi id="S3.E1.m1.7.7.3.3.1.1.3.2.2" xref="S3.E1.m1.7.7.3.3.1.1.3.2.2.cmml">s</mi><mi id="S3.E1.m1.7.7.3.3.1.1.3.2.3" xref="S3.E1.m1.7.7.3.3.1.1.3.2.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.3.3.1.1.3.1" xref="S3.E1.m1.7.7.3.3.1.1.3.1.cmml">‚Äã</mo><mrow id="S3.E1.m1.7.7.3.3.1.1.3.3.2" xref="S3.E1.m1.6.6.2.2a.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.3.3.1.1.3.3.2.1" xref="S3.E1.m1.6.6.2.2a.cmml">(</mo><mtext id="S3.E1.m1.6.6.2.2" xref="S3.E1.m1.6.6.2.2.cmml">base</mtext><mo stretchy="false" id="S3.E1.m1.7.7.3.3.1.1.3.3.2.2" xref="S3.E1.m1.6.6.2.2a.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.7.7.3.3.1.3" xref="S3.E1.m1.7.7.3.3.1.1.cmml">)</mo></mrow></mrow><mrow id="S3.E1.m1.9.9.5" xref="S3.E1.m1.9.9.5.cmml"><mrow id="S3.E1.m1.9.9.5.4" xref="S3.E1.m1.9.9.5.4.cmml"><msub id="S3.E1.m1.9.9.5.4.2" xref="S3.E1.m1.9.9.5.4.2.cmml"><mi id="S3.E1.m1.9.9.5.4.2.2" xref="S3.E1.m1.9.9.5.4.2.2.cmml">s</mi><mi id="S3.E1.m1.9.9.5.4.2.3" xref="S3.E1.m1.9.9.5.4.2.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.9.9.5.4.1" xref="S3.E1.m1.9.9.5.4.1.cmml">‚Äã</mo><mrow id="S3.E1.m1.9.9.5.4.3.2" xref="S3.E1.m1.8.8.4.1a.cmml"><mo stretchy="false" id="S3.E1.m1.9.9.5.4.3.2.1" xref="S3.E1.m1.8.8.4.1a.cmml">(</mo><mtext id="S3.E1.m1.8.8.4.1" xref="S3.E1.m1.8.8.4.1.cmml">large</mtext><mo stretchy="false" id="S3.E1.m1.9.9.5.4.3.2.2" xref="S3.E1.m1.8.8.4.1a.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.9.9.5.3" xref="S3.E1.m1.9.9.5.3.cmml">‚àí</mo><mrow id="S3.E1.m1.9.9.5.5" xref="S3.E1.m1.9.9.5.5.cmml"><msub id="S3.E1.m1.9.9.5.5.2" xref="S3.E1.m1.9.9.5.5.2.cmml"><mi id="S3.E1.m1.9.9.5.5.2.2" xref="S3.E1.m1.9.9.5.5.2.2.cmml">s</mi><mi id="S3.E1.m1.9.9.5.5.2.3" xref="S3.E1.m1.9.9.5.5.2.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.9.9.5.5.1" xref="S3.E1.m1.9.9.5.5.1.cmml">‚Äã</mo><mrow id="S3.E1.m1.9.9.5.5.3.2" xref="S3.E1.m1.9.9.5.2a.cmml"><mo stretchy="false" id="S3.E1.m1.9.9.5.5.3.2.1" xref="S3.E1.m1.9.9.5.2a.cmml">(</mo><mtext id="S3.E1.m1.9.9.5.2" xref="S3.E1.m1.9.9.5.2.cmml">base</mtext><mo stretchy="false" id="S3.E1.m1.9.9.5.5.3.2.2" xref="S3.E1.m1.9.9.5.2a.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow></mrow></mrow><mo lspace="0em" id="S3.E1.m1.10.10.1.2" xref="S3.E1.m1.10.10.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.10b"><apply id="S3.E1.m1.10.10.1.1.cmml" xref="S3.E1.m1.10.10.1"><eq id="S3.E1.m1.10.10.1.1.1.cmml" xref="S3.E1.m1.10.10.1.1.1"></eq><apply id="S3.E1.m1.10.10.1.1.2.cmml" xref="S3.E1.m1.10.10.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.10.10.1.1.2.1.cmml" xref="S3.E1.m1.10.10.1.1.2">subscript</csymbol><ci id="S3.E1.m1.10.10.1.1.2.2.cmml" xref="S3.E1.m1.10.10.1.1.2.2">superb</ci><ci id="S3.E1.m1.10.10.1.1.2.3.cmml" xref="S3.E1.m1.10.10.1.1.2.3">ùë†</ci></apply><apply id="S3.E1.m1.10.10.1.1.3.cmml" xref="S3.E1.m1.10.10.1.1.3"><times id="S3.E1.m1.10.10.1.1.3.1.cmml" xref="S3.E1.m1.10.10.1.1.3.1"></times><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><divide id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1"></divide><cn type="integer" id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3">1</cn><apply id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.3"><abs id="S3.E1.m1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.3.1"></abs><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">ùëá</ci></apply></apply><apply id="S3.E1.m1.10.10.1.1.3.2.cmml" xref="S3.E1.m1.10.10.1.1.3.2"><apply id="S3.E1.m1.10.10.1.1.3.2.1.cmml" xref="S3.E1.m1.10.10.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.10.10.1.1.3.2.1.1.cmml" xref="S3.E1.m1.10.10.1.1.3.2.1">subscript</csymbol><sum id="S3.E1.m1.10.10.1.1.3.2.1.2.cmml" xref="S3.E1.m1.10.10.1.1.3.2.1.2"></sum><apply id="S3.E1.m1.4.4.3.cmml" xref="S3.E1.m1.4.4.3"><in id="S3.E1.m1.4.4.3.4.cmml" xref="S3.E1.m1.4.4.3.4"></in><ci id="S3.E1.m1.4.4.3.5.cmml" xref="S3.E1.m1.4.4.3.5">ùë°</ci><set id="S3.E1.m1.4.4.3.6.1.cmml" xref="S3.E1.m1.4.4.3.6.2"><cn type="integer" id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1">1</cn><ci id="S3.E1.m1.3.3.2.2.cmml" xref="S3.E1.m1.3.3.2.2">‚ãØ</ci><ci id="S3.E1.m1.4.4.3.3.cmml" xref="S3.E1.m1.4.4.3.3">ùëá</ci></set></apply></apply><apply id="S3.E1.m1.9.9.cmml" xref="S3.E1.m1.9.9"><divide id="S3.E1.m1.9.9.6.cmml" xref="S3.E1.m1.9.9"></divide><apply id="S3.E1.m1.7.7.3.cmml" xref="S3.E1.m1.7.7.3"><times id="S3.E1.m1.7.7.3.4.cmml" xref="S3.E1.m1.7.7.3.4"></times><cn type="integer" id="S3.E1.m1.7.7.3.5.cmml" xref="S3.E1.m1.7.7.3.5">1000</cn><apply id="S3.E1.m1.7.7.3.3.1.1.cmml" xref="S3.E1.m1.7.7.3.3.1"><minus id="S3.E1.m1.7.7.3.3.1.1.1.cmml" xref="S3.E1.m1.7.7.3.3.1.1.1"></minus><apply id="S3.E1.m1.7.7.3.3.1.1.2.cmml" xref="S3.E1.m1.7.7.3.3.1.1.2"><times id="S3.E1.m1.7.7.3.3.1.1.2.1.cmml" xref="S3.E1.m1.7.7.3.3.1.1.2.1"></times><apply id="S3.E1.m1.7.7.3.3.1.1.2.2.cmml" xref="S3.E1.m1.7.7.3.3.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.3.3.1.1.2.2.1.cmml" xref="S3.E1.m1.7.7.3.3.1.1.2.2">subscript</csymbol><ci id="S3.E1.m1.7.7.3.3.1.1.2.2.2.cmml" xref="S3.E1.m1.7.7.3.3.1.1.2.2.2">ùë†</ci><ci id="S3.E1.m1.7.7.3.3.1.1.2.2.3.cmml" xref="S3.E1.m1.7.7.3.3.1.1.2.2.3">ùë°</ci></apply><ci id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1.1">ùë¢</ci></apply><apply id="S3.E1.m1.7.7.3.3.1.1.3.cmml" xref="S3.E1.m1.7.7.3.3.1.1.3"><times id="S3.E1.m1.7.7.3.3.1.1.3.1.cmml" xref="S3.E1.m1.7.7.3.3.1.1.3.1"></times><apply id="S3.E1.m1.7.7.3.3.1.1.3.2.cmml" xref="S3.E1.m1.7.7.3.3.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.3.3.1.1.3.2.1.cmml" xref="S3.E1.m1.7.7.3.3.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.7.7.3.3.1.1.3.2.2.cmml" xref="S3.E1.m1.7.7.3.3.1.1.3.2.2">ùë†</ci><ci id="S3.E1.m1.7.7.3.3.1.1.3.2.3.cmml" xref="S3.E1.m1.7.7.3.3.1.1.3.2.3">ùë°</ci></apply><ci id="S3.E1.m1.6.6.2.2a.cmml" xref="S3.E1.m1.7.7.3.3.1.1.3.3.2"><mtext id="S3.E1.m1.6.6.2.2.cmml" xref="S3.E1.m1.6.6.2.2">base</mtext></ci></apply></apply></apply><apply id="S3.E1.m1.9.9.5.cmml" xref="S3.E1.m1.9.9.5"><minus id="S3.E1.m1.9.9.5.3.cmml" xref="S3.E1.m1.9.9.5.3"></minus><apply id="S3.E1.m1.9.9.5.4.cmml" xref="S3.E1.m1.9.9.5.4"><times id="S3.E1.m1.9.9.5.4.1.cmml" xref="S3.E1.m1.9.9.5.4.1"></times><apply id="S3.E1.m1.9.9.5.4.2.cmml" xref="S3.E1.m1.9.9.5.4.2"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.5.4.2.1.cmml" xref="S3.E1.m1.9.9.5.4.2">subscript</csymbol><ci id="S3.E1.m1.9.9.5.4.2.2.cmml" xref="S3.E1.m1.9.9.5.4.2.2">ùë†</ci><ci id="S3.E1.m1.9.9.5.4.2.3.cmml" xref="S3.E1.m1.9.9.5.4.2.3">ùë°</ci></apply><ci id="S3.E1.m1.8.8.4.1a.cmml" xref="S3.E1.m1.9.9.5.4.3.2"><mtext id="S3.E1.m1.8.8.4.1.cmml" xref="S3.E1.m1.8.8.4.1">large</mtext></ci></apply><apply id="S3.E1.m1.9.9.5.5.cmml" xref="S3.E1.m1.9.9.5.5"><times id="S3.E1.m1.9.9.5.5.1.cmml" xref="S3.E1.m1.9.9.5.5.1"></times><apply id="S3.E1.m1.9.9.5.5.2.cmml" xref="S3.E1.m1.9.9.5.5.2"><csymbol cd="ambiguous" id="S3.E1.m1.9.9.5.5.2.1.cmml" xref="S3.E1.m1.9.9.5.5.2">subscript</csymbol><ci id="S3.E1.m1.9.9.5.5.2.2.cmml" xref="S3.E1.m1.9.9.5.5.2.2">ùë†</ci><ci id="S3.E1.m1.9.9.5.5.2.3.cmml" xref="S3.E1.m1.9.9.5.5.2.3">ùë°</ci></apply><ci id="S3.E1.m1.9.9.5.2a.cmml" xref="S3.E1.m1.9.9.5.5.3.2"><mtext id="S3.E1.m1.9.9.5.2.cmml" xref="S3.E1.m1.9.9.5.2">base</mtext></ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.10c">{\rm superb}_{s}=\frac{1}{|T|}\sum_{t\in\{1,\cdots,T\}}\frac{1000\left(s_{t}(u)-s_{t}(\text{base})\right)}{s_{t}(\text{large})-s_{t}(\text{base})}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.6" class="ltx_p">Since the upper limit is worse than the lower bound on the SF task¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, the metric <math id="S3.SS2.p2.6.m1.1" class="ltx_Math" alttext="{\rm superb}_{s}" display="inline"><semantics id="S3.SS2.p2.6.m1.1a"><msub id="S3.SS2.p2.6.m1.1.1" xref="S3.SS2.p2.6.m1.1.1.cmml"><mi id="S3.SS2.p2.6.m1.1.1.2" xref="S3.SS2.p2.6.m1.1.1.2.cmml">superb</mi><mi id="S3.SS2.p2.6.m1.1.1.3" xref="S3.SS2.p2.6.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m1.1b"><apply id="S3.SS2.p2.6.m1.1.1.cmml" xref="S3.SS2.p2.6.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m1.1.1.1.cmml" xref="S3.SS2.p2.6.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.6.m1.1.1.2.cmml" xref="S3.SS2.p2.6.m1.1.1.2">superb</ci><ci id="S3.SS2.p2.6.m1.1.1.3.cmml" xref="S3.SS2.p2.6.m1.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m1.1c">{\rm superb}_{s}</annotation></semantics></math> is excluded for the SF task.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Experimental Results</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">First, we use a fixed <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">n</annotation></semantics></math> to study the impact of refinement on downstream tasks. The results of the refined and vanilla models are shown in Table 1. We assess the reliability of our results by applying one-tailed paired t-test on each audio in the SUPERB test set for each downstream task. It shows that using an appropriate amount of historical audio information in the neural encoding model, the refined wav2vec2.0 model can achieve a similar performance with the vanilla on ER, KS and SF tasks, and outperform the vanilla on PR, IC, SID, ASR and ASV.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">However, the downstream results are not significantly different from the vanilla wav2vec2 on some tasks. For the content tasks, we speculate that the proposed method is likely inhibited by the change in the audio domain on PR and ASR, both of which use the same domain as the vanilla-base. We thus evaluate the stimuli-pretrain model on same downstream tasks and the results are shown at the fourth row of Table 1. It can be found that the model will not benefit on downstream tasks from using additional data for pre-training, which can even degrade due to the domain shift. As the proposed refined model also experiences the domain migration through the alignment, the results of refining operations are thus promising for downstream tasks.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div id="S3.F2.1" class="ltx_block ltx_minipage ltx_align_bottom" style="width:433.6pt;">
<p id="S3.F2.1.1" class="ltx_p ltx_align_center"><span id="S3.F2.1.1.1" class="ltx_text"><img src="/html/2406.08266/assets/changed.png" id="S3.F2.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="295" height="221" alt="Refer to caption"></span></p>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The percentages of parameter changes after refining different parameter types at different model layers.
</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S3.F3.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:173.4pt;">
<p id="S3.F3.1.1" class="ltx_p ltx_align_center"><span id="S3.F3.1.1.1" class="ltx_text"><img src="/html/2406.08266/assets/history.png" id="S3.F3.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="124" alt="Refer to caption"></span></p>
<p id="S3.F3.1.2" class="ltx_p ltx_align_center">(a)</p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S3.F3.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:173.4pt;">
<p id="S3.F3.2.1" class="ltx_p ltx_align_center"><span id="S3.F3.2.1.1" class="ltx_text"><img src="/html/2406.08266/assets/untitled.png" id="S3.F3.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="124" alt="Refer to caption"></span></p>
<p id="S3.F3.2.2" class="ltx_p ltx_align_center">(b)</p>
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Analytical results of (a) the length of input history waveforms and (b) predicting brain activations.</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Experimental analysis</h3>

<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S3.F4.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" style="width:173.4pt;">
<p id="S3.F4.1.1" class="ltx_p ltx_align_center"><span id="S3.F4.1.1.1" class="ltx_text"><img src="/html/2406.08266/assets/refine.png" id="S3.F4.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="Refer to caption"></span></p>
<p id="S3.F4.1.2" class="ltx_p ltx_align_center">(a) Layer weights of the refined model on eight downstream tasks</p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S3.F4.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" style="width:173.4pt;">
<p id="S3.F4.2.1" class="ltx_p ltx_align_center"><span id="S3.F4.2.1.1" class="ltx_text"><img src="/html/2406.08266/assets/vanilla.png" id="S3.F4.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="118" alt="Refer to caption"></span></p>
<p id="S3.F4.2.2" class="ltx_p ltx_align_center">(b) Layer weights of the vanilla model on eight downstream tasks</p>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S3.F4.3" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:433.6pt;">
<p id="S3.F4.3.1" class="ltx_p ltx_align_center"><span id="S3.F4.3.1.1" class="ltx_text"><img src="/html/2406.08266/assets/weight_change.png" id="S3.F4.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="220" height="165" alt="Refer to caption"></span></p>
<p id="S3.F4.3.2" class="ltx_p ltx_align_center">(c) Layer weights change rates on four representative downstream tasks</p>
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
Analytical results of layer weights.
</figcaption>
</figure>
<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.4" class="ltx_p"><span id="S3.SS4.p1.4.1" class="ltx_text ltx_font_bold">Analysis of parameter changes after refining:</span>
Although we can conclude from aforementioned results that the refined model can extract better speech representations, an accurate illustration of how the speech representation is refined is not straightforward. For this, we examine how the parameters of self-attention in the Transformer layers change, which is shown in Fig.¬†2.
It can be observed that compared to vanilla, the bias of self-attention in the refined model has a significant change for <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">ùêæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">K</annotation></semantics></math>, while the biases of <math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><mi id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><ci id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">ùëÑ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">Q</annotation></semantics></math> and <math id="S3.SS4.p1.3.m3.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S3.SS4.p1.3.m3.1a"><mi id="S3.SS4.p1.3.m3.1.1" xref="S3.SS4.p1.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><ci id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">ùëâ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">V</annotation></semantics></math> have relatively small changes. As the depth of the layer increases, the amount of changes in <math id="S3.SS4.p1.4.m4.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS4.p1.4.m4.1a"><mi id="S3.SS4.p1.4.m4.1.1" xref="S3.SS4.p1.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><ci id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1">ùêæ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">K</annotation></semantics></math> tends to decrease. This will lead the refined model to focus on different parts of the features, resulting in different attention distributions and positive effects on some downstream tasks. Therefore, we speculate that such changes in attention distribution enable the model to learn relevant information on brain activation, thereby enabling a better understanding of speech.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.3" class="ltx_p"><span id="S3.SS4.p2.3.1" class="ltx_text ltx_font_bold">Analysis of the length of input history waveforms:</span>
In order to see the impact of the length of input history waveforms, we adjust the value of <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mi id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><ci id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">n</annotation></semantics></math> from 1 to 8 and the result are shown in Fig.¬†3(a), where <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><mi id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><ci id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">n</annotation></semantics></math> = 1(-context) means that no history speech waveforms are considered for predicting the BOLD response at each TR.
Observing the average performance in terms of the context length <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><mi id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><ci id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">n</annotation></semantics></math>, we find that increasing the length of input history waveform first improves the performance of the refined model but then degrades. It does not follow a monotonic behaviour, which is probably caused by the fact that the acoustic and semantic understanding of human brain relies on both short-term and long-term cues contained in speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. This is consistent with the results in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> that in the context of speech perception, human brains follow an information processing pipeline from word recognition, intention in sentences, emotion in stories, to speaker recognition.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p"><span id="S3.SS4.p3.1.1" class="ltx_text ltx_font_bold">Analysis of predicting brain activations:</span>
To confirm the neural encoding ability of the refined model, we follow the method in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and compare the Pearson correlation coefficient (PCC) between the true brain activations and that predicted by the speech representation models before and after refining. Specifically, a ridge regression is adopted to predict the brain activations
using the speech representations given by each layer of the vanilla or refined wav2vec2.0. Afterwards, the PCC is computed for each selected voxel, and the obtained results are shown in Fig.¬†3(b). It indicates that after refining the model, the PCCs between the predicted BOLD responses and true counterparts are generally higher. It is interesting that the maximum PCC is achieved in case of using the representations at the 9-th layer of wav2vec2.0 for prediction. This encourages to utilize such representations at middle layers to predict BOLD responses for model refinement in future works.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p"><span id="S3.SS4.p4.1.1" class="ltx_text ltx_font_bold">Analysis of the change rates of layer weights after refining:</span>
Finally, as the SUPERB framework learns a weighted sum of the layer outputs for each downstream task, we carry out a deeper analysis on comparing the layer weights of refined and vanilla models on different downstream tasks, and calculate their change rates. The analytical results are presented in Fig.¬†4.
We observe that the change rates of layer weights are roughly inversely proportional to their absolute values, that is, the layer that plays an important role in a downstream task has a small change on its weight after being refined. The main function of our refined method is therefore to influence other secondary layers to better adapt to downstream tasks.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we proposed a refinement-based wav2vec2.0 model by building a neural encoding model to predict brain activations using speech. It was validated that self-supervised speech pre-trained models can encode brain activation clues into model parameters and extract better speech representations, indicating feasibility of using the universal relation between speech representation and brain activation in self-supervised learning. It was also shown that using historical information to help predict the BOLD response at current TR and thus downstream tasks.The proposed refining operation fills the gap between neuroscience and self-supervised speech representation models. It should be insightful that more efforts can be made to optimize these models using multiple sources of auditory and linguistic information contained in the brain activities, particularly in low signal-to-noise ratio scenarios.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A.¬†Jaiswal, A.¬†R. Babu, M.¬†Z. Zadeh, D.¬†Banerjee, and F.¬†Makedon, ``A survey on contrastive self-supervised learning,'' <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Technologies</em>, vol.¬†9, no.¬†1, p.¬†2, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
X.¬†Liu, F.¬†Zhang, Z.¬†Hou, L.¬†Mian, Z.¬†Wang, J.¬†Zhang, and J.¬†Tang, ``Self-supervised learning: Generative or contrastive,'' <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on knowledge and data engineering</em>, vol.¬†35, no.¬†1, pp. 857‚Äì876, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A.¬†van¬†den Oord, Y.¬†Li, and O.¬†Vinyals, ``Representation learning with contrastive predictive coding,'' <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, vol. abs/1807.03748, 2018. [Online]. Available: <a target="_blank" href="http://arxiv.org/abs/1807.03748" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1807.03748</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S.¬†Schneider, A.¬†Baevski, R.¬†Collobert, and M.¬†Auli, ``wav2vec: Unsupervised pre-training for speech recognition,'' <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.05862</em>, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A.¬†Baevski, S.¬†Schneider, and M.¬†Auli, ``vq-wav2vec: Self-supervised learning of discrete speech representations,'' <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.05453</em>, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A.¬†Baevski, Y.¬†Zhou, A.¬†Mohamed, and M.¬†Auli, ``wav2vec 2.0: A framework for self-supervised learning of speech representations,'' <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol.¬†33, pp. 12‚Äâ449‚Äì12‚Äâ460, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
W.-N. Hsu, B.¬†Bolte, Y.-H.¬†H. Tsai, K.¬†Lakhotia, R.¬†Salakhutdinov, and A.¬†Mohamed, ``Hubert: Self-supervised speech representation learning by masked prediction of hidden units,'' <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.¬†29, pp. 3451‚Äì3460, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A.¬†Mehrish, N.¬†Majumder, R.¬†Bharadwaj, R.¬†Mihalcea, and S.¬†Poria, ``A review of deep learning techniques for speech processing,'' <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Information Fusion</em>, p. 101869, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
R.¬†Krishnan, P.¬†Rajpurkar, and E.¬†J. Topol, ``Self-supervised learning in medicine and healthcare,'' <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Nature Biomedical Engineering</em>, vol.¬†6, no.¬†12, pp. 1346‚Äì1352, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J.¬†Zhang, Q.-T. Xu, Q.-S. Zhu, and Z.-H. Ling, ``Basen: Time-domain brain-assisted speech enhancement network with convolutional cross attention in multi-talker conditions,'' <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.09994</em>, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J.¬†Zhang, Q.-T. Xu, and Z.-H. Ling, ``Sparsity-driven eeg channel selection for brain-assisted speech enhancement,'' <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.13436</em>, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
N.¬†Kanwisher, M.¬†Khosla, and K.¬†Dobs, ``Using artificial neural networks to ask ‚Äòwhy‚Äôquestions of minds and brains,'' <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Trends in Neurosciences</em>, vol.¬†46, no.¬†3, pp. 240‚Äì254, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J.¬†Millet, C.¬†Caucheteux, Y.¬†Boubenec, A.¬†Gramfort, E.¬†Dunbar, C.¬†Pallier, J.-R. King <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ``Toward a realistic model of speech processing in the brain with self-supervised learning,'' <em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol.¬†35, pp. 33‚Äâ428‚Äì33‚Äâ443, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A.¬†R. Vaidya, S.¬†Jain, and A.¬†G. Huth, ``Self-supervised models of audio effectively explain human cortical responses to speech,'' <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.14252</em>, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Y.-A. Chung, H.¬†Tang, and J.¬†Glass, ``Vector-quantized autoregressive predictive coding,'' <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2005.08392</em>, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S.¬†R. Oota, K.¬†Pahwa, M.¬†Marreddy, M.¬†Gupta, and B.¬†S. Raju, ``Neural architecture of speech,'' in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.¬†¬†¬†IEEE, 2023, pp. 1‚Äì5.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
S.¬†R. Oota, A.¬†Veeral, M.¬†Mounika, G.¬†Manish, and R.¬†S. Bapi, ``Speech taskonomy: Which speech tasks are the most predictive of fmri brain activity?'' in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">24th INTERSPEECH Conference</em>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
M.¬†Toneva and L.¬†Wehbe, ``Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain),'' <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol.¬†32, 2019.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
D.¬†Schwartz, M.¬†Toneva, and L.¬†Wehbe, ``Inducing brain-relevant bias in natural language processing models,'' <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol.¬†32, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
J.¬†D. M.-W.¬†C. Kenton and L.¬†K. Toutanova, ``BERT: Pre-training of deep bidirectional transformers for language understanding,'' in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of naacL-HLT</em>, vol.¬†1, 2019, p.¬†2.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
G.¬†Tuckute, J.¬†Feather, D.¬†Boebinger, and J.¬†H. McDermott, ``Many but not all deep neural network audio models capture brain responses and exhibit hierarchical region correspondence,'' <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">bioRxiv</em>, pp. 2022‚Äì09, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
C.¬†Caucheteux, A.¬†Gramfort, and J.-R. King, ``Evidence of a predictive coding hierarchy in the human brain listening to speech,'' <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Nature human behaviour</em>, vol.¬†7, no.¬†3, pp. 430‚Äì441, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S.-w. Yang, P.-H. Chi, Y.-S. Chuang, C.-I.¬†J. Lai, K.¬†Lakhotia, Y.¬†Y. Lin, A.¬†T. Liu, J.¬†Shi, X.¬†Chang, G.-T. Lin <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ``Superb: Speech processing universal performance benchmark,'' <em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.01051</em>, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
S.¬†A. Nastase, Y.-F. Liu, H.¬†Hillman, A.¬†Zadbood, L.¬†Hasenfratz, N.¬†Keshavarzian, J.¬†Chen, C.¬†J. Honey, Y.¬†Yeshurun, M.¬†Regev <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ``The ‚Äúnarratives‚Äù fmri dataset for evaluating models of naturalistic language comprehension,'' <em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic">Scientific data</em>, vol.¬†8, no.¬†1, p. 250, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
B.¬†Fischl, ``Freesurfer,'' <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Neuroimage</em>, vol.¬†62, no.¬†2, pp. 774‚Äì781, 2012.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
M.¬†F. Glasser, T.¬†S. Coalson, E.¬†C. Robinson, C.¬†D. Hacker, J.¬†Harwell, E.¬†Yacoub, K.¬†Ugurbil, J.¬†Andersson, C.¬†F. Beckmann, M.¬†Jenkinson <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ``A multi-modal parcellation of human cerebral cortex,'' <em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic">Nature</em>, vol. 536, no. 7615, pp. 171‚Äì178, 2016.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
P.¬†Schober, E.¬†J. Mascha, and T.¬†R. Vetter, ``Statistics from a (agreement) to z (z score): a guide to interpreting common measures of association, agreement, diagnostic accuracy, effect size, heterogeneity, and reliability in medical research,'' <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Anesthesia &amp; Analgesia</em>, vol. 133, no.¬†6, pp. 1633‚Äì1641, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
C.¬†Caucheteux, A.¬†Gramfort, and J.-R. King, ``Model-based analysis of brain activity reveals the hierarchy of language in 305 subjects,'' <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.06078</em>, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A.¬†M. Dale and R.¬†L. Buckner, ``Selective averaging of rapidly presented individual trials using fmri,'' <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Human brain mapping</em>, vol.¬†5, no.¬†5, pp. 329‚Äì340, 1997.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
K.¬†J. Friston, P.¬†Fletcher, O.¬†Josephs, A.¬†Holmes, M.¬†Rugg, and R.¬†Turner, ``Event-related fmri: characterizing differential responses,'' <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Neuroimage</em>, vol.¬†7, no.¬†1, pp. 30‚Äì40, 1998.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
D.¬†L. Schacter, R.¬†L. Buckner, W.¬†Koutstaal, A.¬†M. Dale, and B.¬†R. Rosen, ``Late onset of anterior prefrontal activity during true and false recognition: an event-related fmri study,'' <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Neuroimage</em>, vol.¬†6, no.¬†4, pp. 259‚Äì269, 1997.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.08265" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.08266" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.08266">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.08266" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.08267" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 21:08:01 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
