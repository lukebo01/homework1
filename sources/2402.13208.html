<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.13208] How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena</title><meta property="og:description" content="The attention mechanism, a cornerstone of state-of-the-art neural models, faces computational hurdles in processing long sequences due to its quadratic complexity.
Consequently, research efforts in the last few years fâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.13208">

<!--Generated on Tue Mar  5 17:56:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">How do Hyenas deal with Human Speech? 
<br class="ltx_break">Speech Recognition and Translation with ConfHyena</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.1" class="ltx_p">The attention mechanism, a cornerstone of state-of-the-art neural models, faces computational hurdles in processing long sequences due to its quadratic complexity.
Consequently, research efforts in the last few years focused on finding more efficient alternatives.
Among them, Hyena <cite class="ltx_cite ltx_citemacro_citep">(Poli etÂ al., <a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite> stands out for achieving competitive results in both language modeling and image classification, while offering sub-quadratic memory and computational complexity.
Building on these promising results, we propose ConfHyena, a Conformer whose encoder self-attentions are replaced with an adaptation of Hyena for speech processing, where the long input sequences cause high computational costs.
Through experiments in automatic speech recognition (for English) and translation (from English into 8 target languages), we show that our best ConfHyena model significantly reduces the training time by 27%, at the cost of minimal quality degradation (<math id="id1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><csymbol cd="latexml" id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\sim</annotation></semantics></math>1%), which, in most cases, is not statistically significant.

<br class="ltx_break">
<br class="ltx_break">
<span id="id1.1.1" class="ltx_text ltx_font_bold">Keywords:â€‰</span>speech, recognition, translation, ASR, ST, Hyena, operator, convolutions, complexity</p>
</div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\NAT@set@cites</span>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text"></span></p>
</div>
<div id="id9" class="ltx_logical-block">
<div id="id9.p1" class="ltx_para">
<p id="id9.p1.1" class="ltx_p ltx_align_center"><span id="id9.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">How do Hyenas deal with Human Speech? 
<br class="ltx_break">Speech Recognition and Translation with ConfHyena</span></p>
<br class="ltx_break ltx_centering">
<table id="id8.7" class="ltx_tabular ltx_centering ltx_align_top">
<tbody class="ltx_tbody">
<tr id="id7.6.6" class="ltx_tr">
<td id="id7.6.6.6" class="ltx_td ltx_align_center"><span id="id7.6.6.6.6" class="ltx_text ltx_font_bold" style="font-size:120%;">Marco Gaido<img src="/html/2402.13208/assets/spotted_hyena96.png" id="id2.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="13" height="13" alt="[Uncaptioned image]"><img src="/html/2402.13208/assets/7139-yeenbean.png" id="id3.2.2.2.2.g2" class="ltx_graphics ltx_img_square" width="12" height="12" alt="[Uncaptioned image]">, Sara Papi<img src="/html/2402.13208/assets/spotted_hyena96.png" id="id4.3.3.3.3.g3" class="ltx_graphics ltx_img_square" width="13" height="13" alt="[Uncaptioned image]"><img src="/html/2402.13208/assets/7139-yeenbean.png" id="id5.4.4.4.4.g4" class="ltx_graphics ltx_img_square" width="12" height="12" alt="[Uncaptioned image]">, Matteo Negri<img src="/html/2402.13208/assets/7139-yeenbean.png" id="id6.5.5.5.5.g5" class="ltx_graphics ltx_img_square" width="12" height="12" alt="[Uncaptioned image]">, Luisa Bentivogli<img src="/html/2402.13208/assets/7139-yeenbean.png" id="id7.6.6.6.6.g6" class="ltx_graphics ltx_img_square" width="12" height="12" alt="[Uncaptioned image]"></span></td>
</tr>
<tr id="id8.7.7" class="ltx_tr">
<td id="id8.7.7.1" class="ltx_td ltx_align_center">
<img src="/html/2402.13208/assets/7139-yeenbean.png" id="id8.7.7.1.g1" class="ltx_graphics ltx_img_square" width="12" height="12" alt="[Uncaptioned image]"> Fondazione Bruno Kessler, Italy</td>
</tr>
<tr id="id8.7.8.1" class="ltx_tr">
<td id="id8.7.8.1.1" class="ltx_td ltx_align_center"><span id="id8.7.8.1.1.1" class="ltx_text ltx_font_typewriter">{mgaido,spapi,negri,bentivo}@fbk.eu</span></td>
</tr>
</tbody>
</table>
<p id="id9.p1.2" class="ltx_p ltx_align_center"><span id="id9.p1.2.1" class="ltx_text ltx_font_italic">Abstract content</span></p>
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.Â Â Â Introduction</h2>
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><img src="/html/2402.13208/assets/spotted_hyena96.png" id="footnote1.g1" class="ltx_graphics ltx_img_square" width="13" height="13" alt="[Uncaptioned image]"> The authors contributed equally.</span></span></span>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The attention mechanism <cite class="ltx_cite ltx_citemacro_citep">(Bahdanau etÂ al., <a href="#bib.bib2" title="" class="ltx_ref">2016a</a>)</cite> is the core of todayâ€™s neural architectures in many
AI fields <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a href="#bib.bib26" title="" class="ltx_ref">2022</a>)</cite>, including speech processing <cite class="ltx_cite ltx_citemacro_citep">(Latif etÂ al., <a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite>.
However, attention is known to be computationally expensive due to its quadratic computational and memory complexity with respect to the input length, which hinders the adoption of attention-based models in use cases that entail long input sequences.
This has complicated their application in speech tasks like automatic speech recognition (ASR) and speech translation (ST), where audio sequences are typically <math id="S1.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S1.p1.1.m1.1a"><mo id="S1.p1.1.m1.1.1" xref="S1.p1.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.1b"><csymbol cd="latexml" id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.1c">\sim</annotation></semantics></math>8-10 times longer than the equivalent text.
In fact, current models have to downsample the input sequence by a factor of 4 with strided convolutions before applying attention-based layers <cite class="ltx_cite ltx_citemacro_cite">BÃ©rard etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2018</a>); DiÂ Gangi etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite>.
Besides causing information loss <cite class="ltx_cite ltx_citemacro_citep">(Salesky etÂ al., <a href="#bib.bib40" title="" class="ltx_ref">2019</a>; Papi etÂ al., <a href="#bib.bib31" title="" class="ltx_ref">2021</a>)</cite>, this workaround does not entirely solve the problem. The sequences can indeed remain very long, thereby resulting in high training costs with important social and environmental negative impact <cite class="ltx_cite ltx_citemacro_citep">(Strubell etÂ al., <a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While attention alternatives with sub-quadratic complexity have been proposed in other fields <cite class="ltx_cite ltx_citemacro_citep">(Tay etÂ al., <a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite>, none of them have achieved widespread adoption, as their efficiency comes at the cost of non-negligible performance degradation in many tasks, including ASR and ST <cite class="ltx_cite ltx_citemacro_citep">(Alastruey etÂ al., <a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite>.
More recently, <cite class="ltx_cite ltx_citemacro_citet">Poli etÂ al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite> introduced the Hyena operator.
Based on a recurrence of implicitly parametrized long convolutions and data-controlled gating, Hyena has been the first attention alternative to claim comparable results (in language modeling and image classification).
Building upon these promising results, in this paper, we explore the adaptation and application of Hyena to ASR and ST, two tasks where training state-of-the-art models <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a href="#bib.bib49" title="" class="ltx_ref">2023</a>; Radford etÂ al., <a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite> is extremely demanding in terms of computational resources, hardware, and time.<span id="footnote1a" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Attempts to build models similar to Whisper <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>, even on a smaller training dataset (less than 25% of Whisper), require more than 15,000 GPU/hours on NVIDIA A100 GPUs <cite class="ltx_cite ltx_citemacro_citep">(Peng etÂ al., <a href="#bib.bib34" title="" class="ltx_ref">2023b</a>)</cite>.</span></span></span></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Along this line of work, our contributions are:</p>
</div>
<div id="S1.p4" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We adapt the original Hyena operator, which is <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">causal</span> (i.e., it prevents accessing future information when encoding each element of a sequence), into a <span id="S1.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">non-causal</span> version, allowing for richer representations by accessing information across the entire input sequence;</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We present two models, ConfHyena and Hybrid ConfHyena,<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span id="footnote2.1" class="ltx_text" style="color:#000000;">Code and pre-trained models are released open-source under Apache 2.0 license at: <a target="_blank" href="https://github.com/hlt-mt/FBK-fairseq" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/hlt-mt/FBK-fairseq</a>.</span></span></span></span> based on the state-of-the-art Conformer <cite class="ltx_cite ltx_citemacro_citep">(Gulati etÂ al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite> architecture, which replace the self-attention with our adapted Hyena operator in <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">all</span> encoder layers and <span id="S1.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">only</span> in initial layers, respectively;</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.2" class="ltx_p">We show that on ASR (en) and ST (en<math id="S1.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S1.I1.i3.p1.1.m1.1a"><mo stretchy="false" id="S1.I1.i3.p1.1.m1.1.1" xref="S1.I1.i3.p1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i3.p1.1.m1.1b"><ci id="S1.I1.i3.p1.1.m1.1.1.cmml" xref="S1.I1.i3.p1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i3.p1.1.m1.1c">\rightarrow</annotation></semantics></math>{de, es, fr, it, nl, pt, ro, ru}), our Hybrid ConfHyena reduces training time by 27%, at the cost of a minimal (<math id="S1.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S1.I1.i3.p1.2.m2.1a"><mo id="S1.I1.i3.p1.2.m2.1.1" xref="S1.I1.i3.p1.2.m2.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i3.p1.2.m2.1b"><csymbol cd="latexml" id="S1.I1.i3.p1.2.m2.1.1.cmml" xref="S1.I1.i3.p1.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i3.p1.2.m2.1c">\sim</annotation></semantics></math>1%) and mostly not statistically significant quality degradation.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.Â Â Â Background</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">2.1.Â Â Â Self-Attention</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.12" class="ltx_p">Attention <cite class="ltx_cite ltx_citemacro_citep"><span id="S2.SS1.p1.12.1.1" class="ltx_text" style="color:#000000;">(</span>Bahdanau etÂ al.<span id="S2.SS1.p1.12.2.2.1.1" class="ltx_text" style="color:#000000;">, </span><a href="#bib.bib3" title="" class="ltx_ref">2016b</a><span id="S2.SS1.p1.12.3.3" class="ltx_text" style="color:#000000;">)</span></cite> is a function that maps a query matrix <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">Q</annotation></semantics></math> and a pair of key-value matrices <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">K</annotation></semantics></math> and <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">V</annotation></semantics></math> to an output sequence <math id="S2.SS1.p1.4.m4.3" class="ltx_Math" alttext="A(Q,K,V)" display="inline"><semantics id="S2.SS1.p1.4.m4.3a"><mrow id="S2.SS1.p1.4.m4.3.4" xref="S2.SS1.p1.4.m4.3.4.cmml"><mi id="S2.SS1.p1.4.m4.3.4.2" xref="S2.SS1.p1.4.m4.3.4.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.4.m4.3.4.1" xref="S2.SS1.p1.4.m4.3.4.1.cmml">â€‹</mo><mrow id="S2.SS1.p1.4.m4.3.4.3.2" xref="S2.SS1.p1.4.m4.3.4.3.1.cmml"><mo stretchy="false" id="S2.SS1.p1.4.m4.3.4.3.2.1" xref="S2.SS1.p1.4.m4.3.4.3.1.cmml">(</mo><mi id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">Q</mi><mo id="S2.SS1.p1.4.m4.3.4.3.2.2" xref="S2.SS1.p1.4.m4.3.4.3.1.cmml">,</mo><mi id="S2.SS1.p1.4.m4.2.2" xref="S2.SS1.p1.4.m4.2.2.cmml">K</mi><mo id="S2.SS1.p1.4.m4.3.4.3.2.3" xref="S2.SS1.p1.4.m4.3.4.3.1.cmml">,</mo><mi id="S2.SS1.p1.4.m4.3.3" xref="S2.SS1.p1.4.m4.3.3.cmml">V</mi><mo stretchy="false" id="S2.SS1.p1.4.m4.3.4.3.2.4" xref="S2.SS1.p1.4.m4.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.3b"><apply id="S2.SS1.p1.4.m4.3.4.cmml" xref="S2.SS1.p1.4.m4.3.4"><times id="S2.SS1.p1.4.m4.3.4.1.cmml" xref="S2.SS1.p1.4.m4.3.4.1"></times><ci id="S2.SS1.p1.4.m4.3.4.2.cmml" xref="S2.SS1.p1.4.m4.3.4.2">ğ´</ci><vector id="S2.SS1.p1.4.m4.3.4.3.1.cmml" xref="S2.SS1.p1.4.m4.3.4.3.2"><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">ğ‘„</ci><ci id="S2.SS1.p1.4.m4.2.2.cmml" xref="S2.SS1.p1.4.m4.2.2">ğ¾</ci><ci id="S2.SS1.p1.4.m4.3.3.cmml" xref="S2.SS1.p1.4.m4.3.3">ğ‘‰</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.3c">A(Q,K,V)</annotation></semantics></math>.
In the case of self-attention, <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><mi id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">Q</annotation></semantics></math>, <math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><mi id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><ci id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">K</annotation></semantics></math>, and <math id="S2.SS1.p1.7.m7.1" class="ltx_Math" alttext="V" display="inline"><semantics id="S2.SS1.p1.7.m7.1a"><mi id="S2.SS1.p1.7.m7.1.1" xref="S2.SS1.p1.7.m7.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><ci id="S2.SS1.p1.7.m7.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">V</annotation></semantics></math> are computed from the same input sequence <math id="S2.SS1.p1.8.m8.1" class="ltx_Math" alttext="x\in\mathbb{R}^{L\times d}" display="inline"><semantics id="S2.SS1.p1.8.m8.1a"><mrow id="S2.SS1.p1.8.m8.1.1" xref="S2.SS1.p1.8.m8.1.1.cmml"><mi id="S2.SS1.p1.8.m8.1.1.2" xref="S2.SS1.p1.8.m8.1.1.2.cmml">x</mi><mo id="S2.SS1.p1.8.m8.1.1.1" xref="S2.SS1.p1.8.m8.1.1.1.cmml">âˆˆ</mo><msup id="S2.SS1.p1.8.m8.1.1.3" xref="S2.SS1.p1.8.m8.1.1.3.cmml"><mi id="S2.SS1.p1.8.m8.1.1.3.2" xref="S2.SS1.p1.8.m8.1.1.3.2.cmml">â„</mi><mrow id="S2.SS1.p1.8.m8.1.1.3.3" xref="S2.SS1.p1.8.m8.1.1.3.3.cmml"><mi id="S2.SS1.p1.8.m8.1.1.3.3.2" xref="S2.SS1.p1.8.m8.1.1.3.3.2.cmml">L</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p1.8.m8.1.1.3.3.1" xref="S2.SS1.p1.8.m8.1.1.3.3.1.cmml">Ã—</mo><mi id="S2.SS1.p1.8.m8.1.1.3.3.3" xref="S2.SS1.p1.8.m8.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><apply id="S2.SS1.p1.8.m8.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1"><in id="S2.SS1.p1.8.m8.1.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1.1"></in><ci id="S2.SS1.p1.8.m8.1.1.2.cmml" xref="S2.SS1.p1.8.m8.1.1.2">ğ‘¥</ci><apply id="S2.SS1.p1.8.m8.1.1.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.8.m8.1.1.3.1.cmml" xref="S2.SS1.p1.8.m8.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.8.m8.1.1.3.2.cmml" xref="S2.SS1.p1.8.m8.1.1.3.2">â„</ci><apply id="S2.SS1.p1.8.m8.1.1.3.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3.3"><times id="S2.SS1.p1.8.m8.1.1.3.3.1.cmml" xref="S2.SS1.p1.8.m8.1.1.3.3.1"></times><ci id="S2.SS1.p1.8.m8.1.1.3.3.2.cmml" xref="S2.SS1.p1.8.m8.1.1.3.3.2">ğ¿</ci><ci id="S2.SS1.p1.8.m8.1.1.3.3.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3.3.3">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">x\in\mathbb{R}^{L\times d}</annotation></semantics></math> through three different trainable matrices <math id="S2.SS1.p1.9.m9.3" class="ltx_Math" alttext="W_{q},W_{k},W_{v}\in\mathbb{R}^{d\times d}" display="inline"><semantics id="S2.SS1.p1.9.m9.3a"><mrow id="S2.SS1.p1.9.m9.3.3" xref="S2.SS1.p1.9.m9.3.3.cmml"><mrow id="S2.SS1.p1.9.m9.3.3.3.3" xref="S2.SS1.p1.9.m9.3.3.3.4.cmml"><msub id="S2.SS1.p1.9.m9.1.1.1.1.1" xref="S2.SS1.p1.9.m9.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.9.m9.1.1.1.1.1.2" xref="S2.SS1.p1.9.m9.1.1.1.1.1.2.cmml">W</mi><mi id="S2.SS1.p1.9.m9.1.1.1.1.1.3" xref="S2.SS1.p1.9.m9.1.1.1.1.1.3.cmml">q</mi></msub><mo id="S2.SS1.p1.9.m9.3.3.3.3.4" xref="S2.SS1.p1.9.m9.3.3.3.4.cmml">,</mo><msub id="S2.SS1.p1.9.m9.2.2.2.2.2" xref="S2.SS1.p1.9.m9.2.2.2.2.2.cmml"><mi id="S2.SS1.p1.9.m9.2.2.2.2.2.2" xref="S2.SS1.p1.9.m9.2.2.2.2.2.2.cmml">W</mi><mi id="S2.SS1.p1.9.m9.2.2.2.2.2.3" xref="S2.SS1.p1.9.m9.2.2.2.2.2.3.cmml">k</mi></msub><mo id="S2.SS1.p1.9.m9.3.3.3.3.5" xref="S2.SS1.p1.9.m9.3.3.3.4.cmml">,</mo><msub id="S2.SS1.p1.9.m9.3.3.3.3.3" xref="S2.SS1.p1.9.m9.3.3.3.3.3.cmml"><mi id="S2.SS1.p1.9.m9.3.3.3.3.3.2" xref="S2.SS1.p1.9.m9.3.3.3.3.3.2.cmml">W</mi><mi id="S2.SS1.p1.9.m9.3.3.3.3.3.3" xref="S2.SS1.p1.9.m9.3.3.3.3.3.3.cmml">v</mi></msub></mrow><mo id="S2.SS1.p1.9.m9.3.3.4" xref="S2.SS1.p1.9.m9.3.3.4.cmml">âˆˆ</mo><msup id="S2.SS1.p1.9.m9.3.3.5" xref="S2.SS1.p1.9.m9.3.3.5.cmml"><mi id="S2.SS1.p1.9.m9.3.3.5.2" xref="S2.SS1.p1.9.m9.3.3.5.2.cmml">â„</mi><mrow id="S2.SS1.p1.9.m9.3.3.5.3" xref="S2.SS1.p1.9.m9.3.3.5.3.cmml"><mi id="S2.SS1.p1.9.m9.3.3.5.3.2" xref="S2.SS1.p1.9.m9.3.3.5.3.2.cmml">d</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p1.9.m9.3.3.5.3.1" xref="S2.SS1.p1.9.m9.3.3.5.3.1.cmml">Ã—</mo><mi id="S2.SS1.p1.9.m9.3.3.5.3.3" xref="S2.SS1.p1.9.m9.3.3.5.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.9.m9.3b"><apply id="S2.SS1.p1.9.m9.3.3.cmml" xref="S2.SS1.p1.9.m9.3.3"><in id="S2.SS1.p1.9.m9.3.3.4.cmml" xref="S2.SS1.p1.9.m9.3.3.4"></in><list id="S2.SS1.p1.9.m9.3.3.3.4.cmml" xref="S2.SS1.p1.9.m9.3.3.3.3"><apply id="S2.SS1.p1.9.m9.1.1.1.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.9.m9.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.9.m9.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.9.m9.1.1.1.1.1.2">ğ‘Š</ci><ci id="S2.SS1.p1.9.m9.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.9.m9.1.1.1.1.1.3">ğ‘</ci></apply><apply id="S2.SS1.p1.9.m9.2.2.2.2.2.cmml" xref="S2.SS1.p1.9.m9.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.9.m9.2.2.2.2.2.1.cmml" xref="S2.SS1.p1.9.m9.2.2.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.9.m9.2.2.2.2.2.2.cmml" xref="S2.SS1.p1.9.m9.2.2.2.2.2.2">ğ‘Š</ci><ci id="S2.SS1.p1.9.m9.2.2.2.2.2.3.cmml" xref="S2.SS1.p1.9.m9.2.2.2.2.2.3">ğ‘˜</ci></apply><apply id="S2.SS1.p1.9.m9.3.3.3.3.3.cmml" xref="S2.SS1.p1.9.m9.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.9.m9.3.3.3.3.3.1.cmml" xref="S2.SS1.p1.9.m9.3.3.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.9.m9.3.3.3.3.3.2.cmml" xref="S2.SS1.p1.9.m9.3.3.3.3.3.2">ğ‘Š</ci><ci id="S2.SS1.p1.9.m9.3.3.3.3.3.3.cmml" xref="S2.SS1.p1.9.m9.3.3.3.3.3.3">ğ‘£</ci></apply></list><apply id="S2.SS1.p1.9.m9.3.3.5.cmml" xref="S2.SS1.p1.9.m9.3.3.5"><csymbol cd="ambiguous" id="S2.SS1.p1.9.m9.3.3.5.1.cmml" xref="S2.SS1.p1.9.m9.3.3.5">superscript</csymbol><ci id="S2.SS1.p1.9.m9.3.3.5.2.cmml" xref="S2.SS1.p1.9.m9.3.3.5.2">â„</ci><apply id="S2.SS1.p1.9.m9.3.3.5.3.cmml" xref="S2.SS1.p1.9.m9.3.3.5.3"><times id="S2.SS1.p1.9.m9.3.3.5.3.1.cmml" xref="S2.SS1.p1.9.m9.3.3.5.3.1"></times><ci id="S2.SS1.p1.9.m9.3.3.5.3.2.cmml" xref="S2.SS1.p1.9.m9.3.3.5.3.2">ğ‘‘</ci><ci id="S2.SS1.p1.9.m9.3.3.5.3.3.cmml" xref="S2.SS1.p1.9.m9.3.3.5.3.3">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.9.m9.3c">W_{q},W_{k},W_{v}\in\mathbb{R}^{d\times d}</annotation></semantics></math> as <math id="S2.SS1.p1.10.m10.1" class="ltx_Math" alttext="Q=xW_{q}" display="inline"><semantics id="S2.SS1.p1.10.m10.1a"><mrow id="S2.SS1.p1.10.m10.1.1" xref="S2.SS1.p1.10.m10.1.1.cmml"><mi id="S2.SS1.p1.10.m10.1.1.2" xref="S2.SS1.p1.10.m10.1.1.2.cmml">Q</mi><mo id="S2.SS1.p1.10.m10.1.1.1" xref="S2.SS1.p1.10.m10.1.1.1.cmml">=</mo><mrow id="S2.SS1.p1.10.m10.1.1.3" xref="S2.SS1.p1.10.m10.1.1.3.cmml"><mi id="S2.SS1.p1.10.m10.1.1.3.2" xref="S2.SS1.p1.10.m10.1.1.3.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.10.m10.1.1.3.1" xref="S2.SS1.p1.10.m10.1.1.3.1.cmml">â€‹</mo><msub id="S2.SS1.p1.10.m10.1.1.3.3" xref="S2.SS1.p1.10.m10.1.1.3.3.cmml"><mi id="S2.SS1.p1.10.m10.1.1.3.3.2" xref="S2.SS1.p1.10.m10.1.1.3.3.2.cmml">W</mi><mi id="S2.SS1.p1.10.m10.1.1.3.3.3" xref="S2.SS1.p1.10.m10.1.1.3.3.3.cmml">q</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.10.m10.1b"><apply id="S2.SS1.p1.10.m10.1.1.cmml" xref="S2.SS1.p1.10.m10.1.1"><eq id="S2.SS1.p1.10.m10.1.1.1.cmml" xref="S2.SS1.p1.10.m10.1.1.1"></eq><ci id="S2.SS1.p1.10.m10.1.1.2.cmml" xref="S2.SS1.p1.10.m10.1.1.2">ğ‘„</ci><apply id="S2.SS1.p1.10.m10.1.1.3.cmml" xref="S2.SS1.p1.10.m10.1.1.3"><times id="S2.SS1.p1.10.m10.1.1.3.1.cmml" xref="S2.SS1.p1.10.m10.1.1.3.1"></times><ci id="S2.SS1.p1.10.m10.1.1.3.2.cmml" xref="S2.SS1.p1.10.m10.1.1.3.2">ğ‘¥</ci><apply id="S2.SS1.p1.10.m10.1.1.3.3.cmml" xref="S2.SS1.p1.10.m10.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m10.1.1.3.3.1.cmml" xref="S2.SS1.p1.10.m10.1.1.3.3">subscript</csymbol><ci id="S2.SS1.p1.10.m10.1.1.3.3.2.cmml" xref="S2.SS1.p1.10.m10.1.1.3.3.2">ğ‘Š</ci><ci id="S2.SS1.p1.10.m10.1.1.3.3.3.cmml" xref="S2.SS1.p1.10.m10.1.1.3.3.3">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.10.m10.1c">Q=xW_{q}</annotation></semantics></math>, <math id="S2.SS1.p1.11.m11.1" class="ltx_Math" alttext="K=xW_{k}" display="inline"><semantics id="S2.SS1.p1.11.m11.1a"><mrow id="S2.SS1.p1.11.m11.1.1" xref="S2.SS1.p1.11.m11.1.1.cmml"><mi id="S2.SS1.p1.11.m11.1.1.2" xref="S2.SS1.p1.11.m11.1.1.2.cmml">K</mi><mo id="S2.SS1.p1.11.m11.1.1.1" xref="S2.SS1.p1.11.m11.1.1.1.cmml">=</mo><mrow id="S2.SS1.p1.11.m11.1.1.3" xref="S2.SS1.p1.11.m11.1.1.3.cmml"><mi id="S2.SS1.p1.11.m11.1.1.3.2" xref="S2.SS1.p1.11.m11.1.1.3.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.11.m11.1.1.3.1" xref="S2.SS1.p1.11.m11.1.1.3.1.cmml">â€‹</mo><msub id="S2.SS1.p1.11.m11.1.1.3.3" xref="S2.SS1.p1.11.m11.1.1.3.3.cmml"><mi id="S2.SS1.p1.11.m11.1.1.3.3.2" xref="S2.SS1.p1.11.m11.1.1.3.3.2.cmml">W</mi><mi id="S2.SS1.p1.11.m11.1.1.3.3.3" xref="S2.SS1.p1.11.m11.1.1.3.3.3.cmml">k</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.11.m11.1b"><apply id="S2.SS1.p1.11.m11.1.1.cmml" xref="S2.SS1.p1.11.m11.1.1"><eq id="S2.SS1.p1.11.m11.1.1.1.cmml" xref="S2.SS1.p1.11.m11.1.1.1"></eq><ci id="S2.SS1.p1.11.m11.1.1.2.cmml" xref="S2.SS1.p1.11.m11.1.1.2">ğ¾</ci><apply id="S2.SS1.p1.11.m11.1.1.3.cmml" xref="S2.SS1.p1.11.m11.1.1.3"><times id="S2.SS1.p1.11.m11.1.1.3.1.cmml" xref="S2.SS1.p1.11.m11.1.1.3.1"></times><ci id="S2.SS1.p1.11.m11.1.1.3.2.cmml" xref="S2.SS1.p1.11.m11.1.1.3.2">ğ‘¥</ci><apply id="S2.SS1.p1.11.m11.1.1.3.3.cmml" xref="S2.SS1.p1.11.m11.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.11.m11.1.1.3.3.1.cmml" xref="S2.SS1.p1.11.m11.1.1.3.3">subscript</csymbol><ci id="S2.SS1.p1.11.m11.1.1.3.3.2.cmml" xref="S2.SS1.p1.11.m11.1.1.3.3.2">ğ‘Š</ci><ci id="S2.SS1.p1.11.m11.1.1.3.3.3.cmml" xref="S2.SS1.p1.11.m11.1.1.3.3.3">ğ‘˜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.11.m11.1c">K=xW_{k}</annotation></semantics></math>, and <math id="S2.SS1.p1.12.m12.1" class="ltx_Math" alttext="V=xW_{v}" display="inline"><semantics id="S2.SS1.p1.12.m12.1a"><mrow id="S2.SS1.p1.12.m12.1.1" xref="S2.SS1.p1.12.m12.1.1.cmml"><mi id="S2.SS1.p1.12.m12.1.1.2" xref="S2.SS1.p1.12.m12.1.1.2.cmml">V</mi><mo id="S2.SS1.p1.12.m12.1.1.1" xref="S2.SS1.p1.12.m12.1.1.1.cmml">=</mo><mrow id="S2.SS1.p1.12.m12.1.1.3" xref="S2.SS1.p1.12.m12.1.1.3.cmml"><mi id="S2.SS1.p1.12.m12.1.1.3.2" xref="S2.SS1.p1.12.m12.1.1.3.2.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.12.m12.1.1.3.1" xref="S2.SS1.p1.12.m12.1.1.3.1.cmml">â€‹</mo><msub id="S2.SS1.p1.12.m12.1.1.3.3" xref="S2.SS1.p1.12.m12.1.1.3.3.cmml"><mi id="S2.SS1.p1.12.m12.1.1.3.3.2" xref="S2.SS1.p1.12.m12.1.1.3.3.2.cmml">W</mi><mi id="S2.SS1.p1.12.m12.1.1.3.3.3" xref="S2.SS1.p1.12.m12.1.1.3.3.3.cmml">v</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.12.m12.1b"><apply id="S2.SS1.p1.12.m12.1.1.cmml" xref="S2.SS1.p1.12.m12.1.1"><eq id="S2.SS1.p1.12.m12.1.1.1.cmml" xref="S2.SS1.p1.12.m12.1.1.1"></eq><ci id="S2.SS1.p1.12.m12.1.1.2.cmml" xref="S2.SS1.p1.12.m12.1.1.2">ğ‘‰</ci><apply id="S2.SS1.p1.12.m12.1.1.3.cmml" xref="S2.SS1.p1.12.m12.1.1.3"><times id="S2.SS1.p1.12.m12.1.1.3.1.cmml" xref="S2.SS1.p1.12.m12.1.1.3.1"></times><ci id="S2.SS1.p1.12.m12.1.1.3.2.cmml" xref="S2.SS1.p1.12.m12.1.1.3.2">ğ‘¥</ci><apply id="S2.SS1.p1.12.m12.1.1.3.3.cmml" xref="S2.SS1.p1.12.m12.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.12.m12.1.1.3.3.1.cmml" xref="S2.SS1.p1.12.m12.1.1.3.3">subscript</csymbol><ci id="S2.SS1.p1.12.m12.1.1.3.3.2.cmml" xref="S2.SS1.p1.12.m12.1.1.3.3.2">ğ‘Š</ci><ci id="S2.SS1.p1.12.m12.1.1.3.3.3.cmml" xref="S2.SS1.p1.12.m12.1.1.3.3.3">ğ‘£</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.12.m12.1c">V=xW_{v}</annotation></semantics></math>.
The scaled dot-product attention <cite class="ltx_cite ltx_citemacro_citep"><span id="S2.SS1.p1.12.4.1" class="ltx_text" style="color:#000000;">(</span>Chan etÂ al.<span id="S2.SS1.p1.12.5.2.1.1" class="ltx_text" style="color:#000000;">, </span><a href="#bib.bib7" title="" class="ltx_ref">2016</a><span id="S2.SS1.p1.12.6.3" class="ltx_text" style="color:#000000;">)</span></cite> used in the Transformer <cite class="ltx_cite ltx_citemacro_citep">(Vaswani etÂ al., <a href="#bib.bib44" title="" class="ltx_ref">2017</a>)</cite> is then obtained by:</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<table id="S2.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex1.m1.4" class="ltx_Math" alttext="A(Q,K,V)=softmax\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V" display="block"><semantics id="S2.Ex1.m1.4a"><mrow id="S2.Ex1.m1.4.5" xref="S2.Ex1.m1.4.5.cmml"><mrow id="S2.Ex1.m1.4.5.2" xref="S2.Ex1.m1.4.5.2.cmml"><mi id="S2.Ex1.m1.4.5.2.2" xref="S2.Ex1.m1.4.5.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.5.2.1" xref="S2.Ex1.m1.4.5.2.1.cmml">â€‹</mo><mrow id="S2.Ex1.m1.4.5.2.3.2" xref="S2.Ex1.m1.4.5.2.3.1.cmml"><mo stretchy="false" id="S2.Ex1.m1.4.5.2.3.2.1" xref="S2.Ex1.m1.4.5.2.3.1.cmml">(</mo><mi id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml">Q</mi><mo id="S2.Ex1.m1.4.5.2.3.2.2" xref="S2.Ex1.m1.4.5.2.3.1.cmml">,</mo><mi id="S2.Ex1.m1.2.2" xref="S2.Ex1.m1.2.2.cmml">K</mi><mo id="S2.Ex1.m1.4.5.2.3.2.3" xref="S2.Ex1.m1.4.5.2.3.1.cmml">,</mo><mi id="S2.Ex1.m1.3.3" xref="S2.Ex1.m1.3.3.cmml">V</mi><mo stretchy="false" id="S2.Ex1.m1.4.5.2.3.2.4" xref="S2.Ex1.m1.4.5.2.3.1.cmml">)</mo></mrow></mrow><mo id="S2.Ex1.m1.4.5.1" xref="S2.Ex1.m1.4.5.1.cmml">=</mo><mrow id="S2.Ex1.m1.4.5.3" xref="S2.Ex1.m1.4.5.3.cmml"><mi id="S2.Ex1.m1.4.5.3.2" xref="S2.Ex1.m1.4.5.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.5.3.1" xref="S2.Ex1.m1.4.5.3.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.5.3.3" xref="S2.Ex1.m1.4.5.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.5.3.1a" xref="S2.Ex1.m1.4.5.3.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.5.3.4" xref="S2.Ex1.m1.4.5.3.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.5.3.1b" xref="S2.Ex1.m1.4.5.3.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.5.3.5" xref="S2.Ex1.m1.4.5.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.5.3.1c" xref="S2.Ex1.m1.4.5.3.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.5.3.6" xref="S2.Ex1.m1.4.5.3.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.5.3.1d" xref="S2.Ex1.m1.4.5.3.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.5.3.7" xref="S2.Ex1.m1.4.5.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.5.3.1e" xref="S2.Ex1.m1.4.5.3.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.5.3.8" xref="S2.Ex1.m1.4.5.3.8.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.5.3.1f" xref="S2.Ex1.m1.4.5.3.1.cmml">â€‹</mo><mrow id="S2.Ex1.m1.4.5.3.9.2" xref="S2.Ex1.m1.4.4.cmml"><mo id="S2.Ex1.m1.4.5.3.9.2.1" xref="S2.Ex1.m1.4.4.cmml">(</mo><mfrac id="S2.Ex1.m1.4.4" xref="S2.Ex1.m1.4.4.cmml"><mrow id="S2.Ex1.m1.4.4.2" xref="S2.Ex1.m1.4.4.2.cmml"><mi id="S2.Ex1.m1.4.4.2.2" xref="S2.Ex1.m1.4.4.2.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.4.2.1" xref="S2.Ex1.m1.4.4.2.1.cmml">â€‹</mo><msup id="S2.Ex1.m1.4.4.2.3" xref="S2.Ex1.m1.4.4.2.3.cmml"><mi id="S2.Ex1.m1.4.4.2.3.2" xref="S2.Ex1.m1.4.4.2.3.2.cmml">K</mi><mi id="S2.Ex1.m1.4.4.2.3.3" xref="S2.Ex1.m1.4.4.2.3.3.cmml">T</mi></msup></mrow><msqrt id="S2.Ex1.m1.4.4.3" xref="S2.Ex1.m1.4.4.3.cmml"><msub id="S2.Ex1.m1.4.4.3.2" xref="S2.Ex1.m1.4.4.3.2.cmml"><mi id="S2.Ex1.m1.4.4.3.2.2" xref="S2.Ex1.m1.4.4.3.2.2.cmml">d</mi><mi id="S2.Ex1.m1.4.4.3.2.3" xref="S2.Ex1.m1.4.4.3.2.3.cmml">k</mi></msub></msqrt></mfrac><mo id="S2.Ex1.m1.4.5.3.9.2.2" xref="S2.Ex1.m1.4.4.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.5.3.1g" xref="S2.Ex1.m1.4.5.3.1.cmml">â€‹</mo><mi id="S2.Ex1.m1.4.5.3.10" xref="S2.Ex1.m1.4.5.3.10.cmml">V</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.4b"><apply id="S2.Ex1.m1.4.5.cmml" xref="S2.Ex1.m1.4.5"><eq id="S2.Ex1.m1.4.5.1.cmml" xref="S2.Ex1.m1.4.5.1"></eq><apply id="S2.Ex1.m1.4.5.2.cmml" xref="S2.Ex1.m1.4.5.2"><times id="S2.Ex1.m1.4.5.2.1.cmml" xref="S2.Ex1.m1.4.5.2.1"></times><ci id="S2.Ex1.m1.4.5.2.2.cmml" xref="S2.Ex1.m1.4.5.2.2">ğ´</ci><vector id="S2.Ex1.m1.4.5.2.3.1.cmml" xref="S2.Ex1.m1.4.5.2.3.2"><ci id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.1.1">ğ‘„</ci><ci id="S2.Ex1.m1.2.2.cmml" xref="S2.Ex1.m1.2.2">ğ¾</ci><ci id="S2.Ex1.m1.3.3.cmml" xref="S2.Ex1.m1.3.3">ğ‘‰</ci></vector></apply><apply id="S2.Ex1.m1.4.5.3.cmml" xref="S2.Ex1.m1.4.5.3"><times id="S2.Ex1.m1.4.5.3.1.cmml" xref="S2.Ex1.m1.4.5.3.1"></times><ci id="S2.Ex1.m1.4.5.3.2.cmml" xref="S2.Ex1.m1.4.5.3.2">ğ‘ </ci><ci id="S2.Ex1.m1.4.5.3.3.cmml" xref="S2.Ex1.m1.4.5.3.3">ğ‘œ</ci><ci id="S2.Ex1.m1.4.5.3.4.cmml" xref="S2.Ex1.m1.4.5.3.4">ğ‘“</ci><ci id="S2.Ex1.m1.4.5.3.5.cmml" xref="S2.Ex1.m1.4.5.3.5">ğ‘¡</ci><ci id="S2.Ex1.m1.4.5.3.6.cmml" xref="S2.Ex1.m1.4.5.3.6">ğ‘š</ci><ci id="S2.Ex1.m1.4.5.3.7.cmml" xref="S2.Ex1.m1.4.5.3.7">ğ‘</ci><ci id="S2.Ex1.m1.4.5.3.8.cmml" xref="S2.Ex1.m1.4.5.3.8">ğ‘¥</ci><apply id="S2.Ex1.m1.4.4.cmml" xref="S2.Ex1.m1.4.5.3.9.2"><divide id="S2.Ex1.m1.4.4.1.cmml" xref="S2.Ex1.m1.4.5.3.9.2"></divide><apply id="S2.Ex1.m1.4.4.2.cmml" xref="S2.Ex1.m1.4.4.2"><times id="S2.Ex1.m1.4.4.2.1.cmml" xref="S2.Ex1.m1.4.4.2.1"></times><ci id="S2.Ex1.m1.4.4.2.2.cmml" xref="S2.Ex1.m1.4.4.2.2">ğ‘„</ci><apply id="S2.Ex1.m1.4.4.2.3.cmml" xref="S2.Ex1.m1.4.4.2.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.4.4.2.3.1.cmml" xref="S2.Ex1.m1.4.4.2.3">superscript</csymbol><ci id="S2.Ex1.m1.4.4.2.3.2.cmml" xref="S2.Ex1.m1.4.4.2.3.2">ğ¾</ci><ci id="S2.Ex1.m1.4.4.2.3.3.cmml" xref="S2.Ex1.m1.4.4.2.3.3">ğ‘‡</ci></apply></apply><apply id="S2.Ex1.m1.4.4.3.cmml" xref="S2.Ex1.m1.4.4.3"><root id="S2.Ex1.m1.4.4.3a.cmml" xref="S2.Ex1.m1.4.4.3"></root><apply id="S2.Ex1.m1.4.4.3.2.cmml" xref="S2.Ex1.m1.4.4.3.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.4.4.3.2.1.cmml" xref="S2.Ex1.m1.4.4.3.2">subscript</csymbol><ci id="S2.Ex1.m1.4.4.3.2.2.cmml" xref="S2.Ex1.m1.4.4.3.2.2">ğ‘‘</ci><ci id="S2.Ex1.m1.4.4.3.2.3.cmml" xref="S2.Ex1.m1.4.4.3.2.3">ğ‘˜</ci></apply></apply></apply><ci id="S2.Ex1.m1.4.5.3.10.cmml" xref="S2.Ex1.m1.4.5.3.10">ğ‘‰</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.4c">A(Q,K,V)=softmax\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right)V</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS1.p2.4" class="ltx_p">Since the matrix multiplication between <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="Q\in\mathbb{R}^{L\times d}" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">Q</mi><mo id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml"><mi id="S2.SS1.p2.1.m1.1.1.3.2" xref="S2.SS1.p2.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S2.SS1.p2.1.m1.1.1.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.cmml"><mi id="S2.SS1.p2.1.m1.1.1.3.3.2" xref="S2.SS1.p2.1.m1.1.1.3.3.2.cmml">L</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p2.1.m1.1.1.3.3.1" xref="S2.SS1.p2.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S2.SS1.p2.1.m1.1.1.3.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><in id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1"></in><ci id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">ğ‘„</ci><apply id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.1.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3">superscript</csymbol><ci id="S2.SS1.p2.1.m1.1.1.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.2">â„</ci><apply id="S2.SS1.p2.1.m1.1.1.3.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3"><times id="S2.SS1.p2.1.m1.1.1.3.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3.1"></times><ci id="S2.SS1.p2.1.m1.1.1.3.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3.2">ğ¿</ci><ci id="S2.SS1.p2.1.m1.1.1.3.3.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3.3">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">Q\in\mathbb{R}^{L\times d}</annotation></semantics></math> and <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="K^{T}\in\mathbb{R}^{d\times L}" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mrow id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml"><msup id="S2.SS1.p2.2.m2.1.1.2" xref="S2.SS1.p2.2.m2.1.1.2.cmml"><mi id="S2.SS1.p2.2.m2.1.1.2.2" xref="S2.SS1.p2.2.m2.1.1.2.2.cmml">K</mi><mi id="S2.SS1.p2.2.m2.1.1.2.3" xref="S2.SS1.p2.2.m2.1.1.2.3.cmml">T</mi></msup><mo id="S2.SS1.p2.2.m2.1.1.1" xref="S2.SS1.p2.2.m2.1.1.1.cmml">âˆˆ</mo><msup id="S2.SS1.p2.2.m2.1.1.3" xref="S2.SS1.p2.2.m2.1.1.3.cmml"><mi id="S2.SS1.p2.2.m2.1.1.3.2" xref="S2.SS1.p2.2.m2.1.1.3.2.cmml">â„</mi><mrow id="S2.SS1.p2.2.m2.1.1.3.3" xref="S2.SS1.p2.2.m2.1.1.3.3.cmml"><mi id="S2.SS1.p2.2.m2.1.1.3.3.2" xref="S2.SS1.p2.2.m2.1.1.3.3.2.cmml">d</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p2.2.m2.1.1.3.3.1" xref="S2.SS1.p2.2.m2.1.1.3.3.1.cmml">Ã—</mo><mi id="S2.SS1.p2.2.m2.1.1.3.3.3" xref="S2.SS1.p2.2.m2.1.1.3.3.3.cmml">L</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1"><in id="S2.SS1.p2.2.m2.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1.1"></in><apply id="S2.SS1.p2.2.m2.1.1.2.cmml" xref="S2.SS1.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.1.1.2.1.cmml" xref="S2.SS1.p2.2.m2.1.1.2">superscript</csymbol><ci id="S2.SS1.p2.2.m2.1.1.2.2.cmml" xref="S2.SS1.p2.2.m2.1.1.2.2">ğ¾</ci><ci id="S2.SS1.p2.2.m2.1.1.2.3.cmml" xref="S2.SS1.p2.2.m2.1.1.2.3">ğ‘‡</ci></apply><apply id="S2.SS1.p2.2.m2.1.1.3.cmml" xref="S2.SS1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.1.1.3.1.cmml" xref="S2.SS1.p2.2.m2.1.1.3">superscript</csymbol><ci id="S2.SS1.p2.2.m2.1.1.3.2.cmml" xref="S2.SS1.p2.2.m2.1.1.3.2">â„</ci><apply id="S2.SS1.p2.2.m2.1.1.3.3.cmml" xref="S2.SS1.p2.2.m2.1.1.3.3"><times id="S2.SS1.p2.2.m2.1.1.3.3.1.cmml" xref="S2.SS1.p2.2.m2.1.1.3.3.1"></times><ci id="S2.SS1.p2.2.m2.1.1.3.3.2.cmml" xref="S2.SS1.p2.2.m2.1.1.3.3.2">ğ‘‘</ci><ci id="S2.SS1.p2.2.m2.1.1.3.3.3.cmml" xref="S2.SS1.p2.2.m2.1.1.3.3.3">ğ¿</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">K^{T}\in\mathbb{R}^{d\times L}</annotation></semantics></math> results in a <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="\mathbb{R}^{L\times L}" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><msup id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml"><mi id="S2.SS1.p2.3.m3.1.1.2" xref="S2.SS1.p2.3.m3.1.1.2.cmml">â„</mi><mrow id="S2.SS1.p2.3.m3.1.1.3" xref="S2.SS1.p2.3.m3.1.1.3.cmml"><mi id="S2.SS1.p2.3.m3.1.1.3.2" xref="S2.SS1.p2.3.m3.1.1.3.2.cmml">L</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p2.3.m3.1.1.3.1" xref="S2.SS1.p2.3.m3.1.1.3.1.cmml">Ã—</mo><mi id="S2.SS1.p2.3.m3.1.1.3.3" xref="S2.SS1.p2.3.m3.1.1.3.3.cmml">L</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><apply id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.3.m3.1.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">superscript</csymbol><ci id="S2.SS1.p2.3.m3.1.1.2.cmml" xref="S2.SS1.p2.3.m3.1.1.2">â„</ci><apply id="S2.SS1.p2.3.m3.1.1.3.cmml" xref="S2.SS1.p2.3.m3.1.1.3"><times id="S2.SS1.p2.3.m3.1.1.3.1.cmml" xref="S2.SS1.p2.3.m3.1.1.3.1"></times><ci id="S2.SS1.p2.3.m3.1.1.3.2.cmml" xref="S2.SS1.p2.3.m3.1.1.3.2">ğ¿</ci><ci id="S2.SS1.p2.3.m3.1.1.3.3.cmml" xref="S2.SS1.p2.3.m3.1.1.3.3">ğ¿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">\mathbb{R}^{L\times L}</annotation></semantics></math> matrix, its overall memory and time complexity is <math id="S2.SS1.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{O}(L^{2})" display="inline"><semantics id="S2.SS1.p2.4.m4.1a"><mrow id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p2.4.m4.1.1.3" xref="S2.SS1.p2.4.m4.1.1.3.cmml">ğ’ª</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.4.m4.1.1.2" xref="S2.SS1.p2.4.m4.1.1.2.cmml">â€‹</mo><mrow id="S2.SS1.p2.4.m4.1.1.1.1" xref="S2.SS1.p2.4.m4.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p2.4.m4.1.1.1.1.2" xref="S2.SS1.p2.4.m4.1.1.1.1.1.cmml">(</mo><msup id="S2.SS1.p2.4.m4.1.1.1.1.1" xref="S2.SS1.p2.4.m4.1.1.1.1.1.cmml"><mi id="S2.SS1.p2.4.m4.1.1.1.1.1.2" xref="S2.SS1.p2.4.m4.1.1.1.1.1.2.cmml">L</mi><mn id="S2.SS1.p2.4.m4.1.1.1.1.1.3" xref="S2.SS1.p2.4.m4.1.1.1.1.1.3.cmml">2</mn></msup><mo stretchy="false" id="S2.SS1.p2.4.m4.1.1.1.1.3" xref="S2.SS1.p2.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><apply id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1"><times id="S2.SS1.p2.4.m4.1.1.2.cmml" xref="S2.SS1.p2.4.m4.1.1.2"></times><ci id="S2.SS1.p2.4.m4.1.1.3.cmml" xref="S2.SS1.p2.4.m4.1.1.3">ğ’ª</ci><apply id="S2.SS1.p2.4.m4.1.1.1.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.4.m4.1.1.1.1.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1.1.1">superscript</csymbol><ci id="S2.SS1.p2.4.m4.1.1.1.1.1.2.cmml" xref="S2.SS1.p2.4.m4.1.1.1.1.1.2">ğ¿</ci><cn type="integer" id="S2.SS1.p2.4.m4.1.1.1.1.1.3.cmml" xref="S2.SS1.p2.4.m4.1.1.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">\mathcal{O}(L^{2})</annotation></semantics></math>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">2.2.Â Â Â <span id="S2.SS2.1.1" class="ltx_text ltx_align_left" style="color:#000000;">Conformer</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p"><span id="S2.SS2.p1.1.1" class="ltx_text" style="color:#000000;">The self-attention is a cornerstone not only of the Transformer but also of the more recent Conformer <cite class="ltx_cite ltx_citemacro_citep">(Gulati etÂ al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>, an architecture tailored for speech processing that significantly outperformed the Transformer in both ASR and ST <cite class="ltx_cite ltx_citemacro_citep">(Inaguma etÂ al., <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite>. The Conformer modifies the encoder layer structure of the Transformer by
implementing three
key modifications.
First, it adds relative sinusoidal positional encodings <cite class="ltx_cite ltx_citemacro_citep">(Dai etÂ al., <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite> into the self-attention computation, eliminating the absolute positional embeddings added to the input in the Transformer. Second, it replaces the Transformer feed-forward network (FFN) with a Macaron-Net <cite class="ltx_cite ltx_citemacro_citep">(Lu etÂ al., <a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite>
consisting of two FFNs that wrap the module. Lastly, it introduces a new convolution module after the self-attention. This module is made of a pointwise convolution, a Gated Linear Unit (GLU) activation functionÂ <cite class="ltx_cite ltx_citemacro_citep">(Dauphin etÂ al., <a href="#bib.bib11" title="" class="ltx_ref">2017</a>)</cite>, a depthwise convolution, a batch normalization <cite class="ltx_cite ltx_citemacro_citep">(Ioffe and Szegedy, <a href="#bib.bib20" title="" class="ltx_ref">2015</a>)</cite>, a Swish activation function <cite class="ltx_cite ltx_citemacro_citep">(Ramachandran etÂ al., <a href="#bib.bib38" title="" class="ltx_ref">2017</a>)</cite>, and another pointwise convolution.</span></p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">2.3.Â Â Â Hyena</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Hyena <cite class="ltx_cite ltx_citemacro_citep"><span id="S2.SS3.p1.1.1.1" class="ltx_text" style="color:#000000;">(</span>Poli etÂ al.<span id="S2.SS3.p1.1.2.2.1.1" class="ltx_text" style="color:#000000;">, </span><a href="#bib.bib35" title="" class="ltx_ref">2023</a><span id="S2.SS3.p1.1.3.3" class="ltx_text" style="color:#000000;">)</span></cite> has been introduced as an alternative to self-attention with the goal of achieving the same representativeness (i.e., the ability to model dependencies between time steps of the input, regardless of their distance) while avoiding the quadratic complexity w.r.t. the input length.
The core of this operator is a recurrence of two operations: an implicitly parametrized long convolution (i.e., a convolution where the kernel has the same size as the input), and an element-wise multiplicative gating (i.e., an element-wise multiplication with a matrix of learned parameters). Hyena can be formalized as:</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<table id="S2.Ex2" class="ltx_equationgroup ltx_eqn_table">

<tbody id="S2.Ex2X"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.Ex2X.2.1.1.m1.5" class="ltx_Math" alttext="\displaystyle u_{0,...,N},z_{0}" display="inline"><semantics id="S2.Ex2X.2.1.1.m1.5a"><mrow id="S2.Ex2X.2.1.1.m1.5.5.2" xref="S2.Ex2X.2.1.1.m1.5.5.3.cmml"><msub id="S2.Ex2X.2.1.1.m1.4.4.1.1" xref="S2.Ex2X.2.1.1.m1.4.4.1.1.cmml"><mi id="S2.Ex2X.2.1.1.m1.4.4.1.1.2" xref="S2.Ex2X.2.1.1.m1.4.4.1.1.2.cmml">u</mi><mrow id="S2.Ex2X.2.1.1.m1.3.3.3.5" xref="S2.Ex2X.2.1.1.m1.3.3.3.4.cmml"><mn id="S2.Ex2X.2.1.1.m1.1.1.1.1" xref="S2.Ex2X.2.1.1.m1.1.1.1.1.cmml">0</mn><mo id="S2.Ex2X.2.1.1.m1.3.3.3.5.1" xref="S2.Ex2X.2.1.1.m1.3.3.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.Ex2X.2.1.1.m1.2.2.2.2" xref="S2.Ex2X.2.1.1.m1.2.2.2.2.cmml">â€¦</mi><mo id="S2.Ex2X.2.1.1.m1.3.3.3.5.2" xref="S2.Ex2X.2.1.1.m1.3.3.3.4.cmml">,</mo><mi id="S2.Ex2X.2.1.1.m1.3.3.3.3" xref="S2.Ex2X.2.1.1.m1.3.3.3.3.cmml">N</mi></mrow></msub><mo id="S2.Ex2X.2.1.1.m1.5.5.2.3" xref="S2.Ex2X.2.1.1.m1.5.5.3.cmml">,</mo><msub id="S2.Ex2X.2.1.1.m1.5.5.2.2" xref="S2.Ex2X.2.1.1.m1.5.5.2.2.cmml"><mi id="S2.Ex2X.2.1.1.m1.5.5.2.2.2" xref="S2.Ex2X.2.1.1.m1.5.5.2.2.2.cmml">z</mi><mn id="S2.Ex2X.2.1.1.m1.5.5.2.2.3" xref="S2.Ex2X.2.1.1.m1.5.5.2.2.3.cmml">0</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2X.2.1.1.m1.5b"><list id="S2.Ex2X.2.1.1.m1.5.5.3.cmml" xref="S2.Ex2X.2.1.1.m1.5.5.2"><apply id="S2.Ex2X.2.1.1.m1.4.4.1.1.cmml" xref="S2.Ex2X.2.1.1.m1.4.4.1.1"><csymbol cd="ambiguous" id="S2.Ex2X.2.1.1.m1.4.4.1.1.1.cmml" xref="S2.Ex2X.2.1.1.m1.4.4.1.1">subscript</csymbol><ci id="S2.Ex2X.2.1.1.m1.4.4.1.1.2.cmml" xref="S2.Ex2X.2.1.1.m1.4.4.1.1.2">ğ‘¢</ci><list id="S2.Ex2X.2.1.1.m1.3.3.3.4.cmml" xref="S2.Ex2X.2.1.1.m1.3.3.3.5"><cn type="integer" id="S2.Ex2X.2.1.1.m1.1.1.1.1.cmml" xref="S2.Ex2X.2.1.1.m1.1.1.1.1">0</cn><ci id="S2.Ex2X.2.1.1.m1.2.2.2.2.cmml" xref="S2.Ex2X.2.1.1.m1.2.2.2.2">â€¦</ci><ci id="S2.Ex2X.2.1.1.m1.3.3.3.3.cmml" xref="S2.Ex2X.2.1.1.m1.3.3.3.3">ğ‘</ci></list></apply><apply id="S2.Ex2X.2.1.1.m1.5.5.2.2.cmml" xref="S2.Ex2X.2.1.1.m1.5.5.2.2"><csymbol cd="ambiguous" id="S2.Ex2X.2.1.1.m1.5.5.2.2.1.cmml" xref="S2.Ex2X.2.1.1.m1.5.5.2.2">subscript</csymbol><ci id="S2.Ex2X.2.1.1.m1.5.5.2.2.2.cmml" xref="S2.Ex2X.2.1.1.m1.5.5.2.2.2">ğ‘§</ci><cn type="integer" id="S2.Ex2X.2.1.1.m1.5.5.2.2.3.cmml" xref="S2.Ex2X.2.1.1.m1.5.5.2.2.3">0</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2X.2.1.1.m1.5c">\displaystyle u_{0,...,N},z_{0}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex2X.3.2.2.m1.4" class="ltx_Math" alttext="\displaystyle=\text{ShortConv}(W_{0,...,N+1}x)" display="inline"><semantics id="S2.Ex2X.3.2.2.m1.4a"><mrow id="S2.Ex2X.3.2.2.m1.4.4" xref="S2.Ex2X.3.2.2.m1.4.4.cmml"><mi id="S2.Ex2X.3.2.2.m1.4.4.3" xref="S2.Ex2X.3.2.2.m1.4.4.3.cmml"></mi><mo id="S2.Ex2X.3.2.2.m1.4.4.2" xref="S2.Ex2X.3.2.2.m1.4.4.2.cmml">=</mo><mrow id="S2.Ex2X.3.2.2.m1.4.4.1" xref="S2.Ex2X.3.2.2.m1.4.4.1.cmml"><mtext id="S2.Ex2X.3.2.2.m1.4.4.1.3" xref="S2.Ex2X.3.2.2.m1.4.4.1.3a.cmml">ShortConv</mtext><mo lspace="0em" rspace="0em" id="S2.Ex2X.3.2.2.m1.4.4.1.2" xref="S2.Ex2X.3.2.2.m1.4.4.1.2.cmml">â€‹</mo><mrow id="S2.Ex2X.3.2.2.m1.4.4.1.1.1" xref="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.cmml"><mo stretchy="false" id="S2.Ex2X.3.2.2.m1.4.4.1.1.1.2" xref="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.cmml">(</mo><mrow id="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1" xref="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.cmml"><msub id="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.2" xref="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.2.cmml"><mi id="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.2.2" xref="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.2.2.cmml">W</mi><mrow id="S2.Ex2X.3.2.2.m1.3.3.3.3" xref="S2.Ex2X.3.2.2.m1.3.3.3.4.cmml"><mn id="S2.Ex2X.3.2.2.m1.1.1.1.1" xref="S2.Ex2X.3.2.2.m1.1.1.1.1.cmml">0</mn><mo id="S2.Ex2X.3.2.2.m1.3.3.3.3.2" xref="S2.Ex2X.3.2.2.m1.3.3.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.Ex2X.3.2.2.m1.2.2.2.2" xref="S2.Ex2X.3.2.2.m1.2.2.2.2.cmml">â€¦</mi><mo id="S2.Ex2X.3.2.2.m1.3.3.3.3.3" xref="S2.Ex2X.3.2.2.m1.3.3.3.4.cmml">,</mo><mrow id="S2.Ex2X.3.2.2.m1.3.3.3.3.1" xref="S2.Ex2X.3.2.2.m1.3.3.3.3.1.cmml"><mi id="S2.Ex2X.3.2.2.m1.3.3.3.3.1.2" xref="S2.Ex2X.3.2.2.m1.3.3.3.3.1.2.cmml">N</mi><mo id="S2.Ex2X.3.2.2.m1.3.3.3.3.1.1" xref="S2.Ex2X.3.2.2.m1.3.3.3.3.1.1.cmml">+</mo><mn id="S2.Ex2X.3.2.2.m1.3.3.3.3.1.3" xref="S2.Ex2X.3.2.2.m1.3.3.3.3.1.3.cmml">1</mn></mrow></mrow></msub><mo lspace="0em" rspace="0em" id="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.1" xref="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.1.cmml">â€‹</mo><mi id="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.3" xref="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S2.Ex2X.3.2.2.m1.4.4.1.1.1.3" xref="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2X.3.2.2.m1.4b"><apply id="S2.Ex2X.3.2.2.m1.4.4.cmml" xref="S2.Ex2X.3.2.2.m1.4.4"><eq id="S2.Ex2X.3.2.2.m1.4.4.2.cmml" xref="S2.Ex2X.3.2.2.m1.4.4.2"></eq><csymbol cd="latexml" id="S2.Ex2X.3.2.2.m1.4.4.3.cmml" xref="S2.Ex2X.3.2.2.m1.4.4.3">absent</csymbol><apply id="S2.Ex2X.3.2.2.m1.4.4.1.cmml" xref="S2.Ex2X.3.2.2.m1.4.4.1"><times id="S2.Ex2X.3.2.2.m1.4.4.1.2.cmml" xref="S2.Ex2X.3.2.2.m1.4.4.1.2"></times><ci id="S2.Ex2X.3.2.2.m1.4.4.1.3a.cmml" xref="S2.Ex2X.3.2.2.m1.4.4.1.3"><mtext id="S2.Ex2X.3.2.2.m1.4.4.1.3.cmml" xref="S2.Ex2X.3.2.2.m1.4.4.1.3">ShortConv</mtext></ci><apply id="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.cmml" xref="S2.Ex2X.3.2.2.m1.4.4.1.1.1"><times id="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.1.cmml" xref="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.1"></times><apply id="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.2.cmml" xref="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.2.1.cmml" xref="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.2">subscript</csymbol><ci id="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.2.2.cmml" xref="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.2.2">ğ‘Š</ci><list id="S2.Ex2X.3.2.2.m1.3.3.3.4.cmml" xref="S2.Ex2X.3.2.2.m1.3.3.3.3"><cn type="integer" id="S2.Ex2X.3.2.2.m1.1.1.1.1.cmml" xref="S2.Ex2X.3.2.2.m1.1.1.1.1">0</cn><ci id="S2.Ex2X.3.2.2.m1.2.2.2.2.cmml" xref="S2.Ex2X.3.2.2.m1.2.2.2.2">â€¦</ci><apply id="S2.Ex2X.3.2.2.m1.3.3.3.3.1.cmml" xref="S2.Ex2X.3.2.2.m1.3.3.3.3.1"><plus id="S2.Ex2X.3.2.2.m1.3.3.3.3.1.1.cmml" xref="S2.Ex2X.3.2.2.m1.3.3.3.3.1.1"></plus><ci id="S2.Ex2X.3.2.2.m1.3.3.3.3.1.2.cmml" xref="S2.Ex2X.3.2.2.m1.3.3.3.3.1.2">ğ‘</ci><cn type="integer" id="S2.Ex2X.3.2.2.m1.3.3.3.3.1.3.cmml" xref="S2.Ex2X.3.2.2.m1.3.3.3.3.1.3">1</cn></apply></list></apply><ci id="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.3.cmml" xref="S2.Ex2X.3.2.2.m1.4.4.1.1.1.1.3">ğ‘¥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2X.3.2.2.m1.4c">\displaystyle=\text{ShortConv}(W_{0,...,N+1}x)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex2Xa"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.Ex2Xa.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle z_{i+1}" display="inline"><semantics id="S2.Ex2Xa.2.1.1.m1.1a"><msub id="S2.Ex2Xa.2.1.1.m1.1.1" xref="S2.Ex2Xa.2.1.1.m1.1.1.cmml"><mi id="S2.Ex2Xa.2.1.1.m1.1.1.2" xref="S2.Ex2Xa.2.1.1.m1.1.1.2.cmml">z</mi><mrow id="S2.Ex2Xa.2.1.1.m1.1.1.3" xref="S2.Ex2Xa.2.1.1.m1.1.1.3.cmml"><mi id="S2.Ex2Xa.2.1.1.m1.1.1.3.2" xref="S2.Ex2Xa.2.1.1.m1.1.1.3.2.cmml">i</mi><mo id="S2.Ex2Xa.2.1.1.m1.1.1.3.1" xref="S2.Ex2Xa.2.1.1.m1.1.1.3.1.cmml">+</mo><mn id="S2.Ex2Xa.2.1.1.m1.1.1.3.3" xref="S2.Ex2Xa.2.1.1.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.Ex2Xa.2.1.1.m1.1b"><apply id="S2.Ex2Xa.2.1.1.m1.1.1.cmml" xref="S2.Ex2Xa.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.Ex2Xa.2.1.1.m1.1.1.1.cmml" xref="S2.Ex2Xa.2.1.1.m1.1.1">subscript</csymbol><ci id="S2.Ex2Xa.2.1.1.m1.1.1.2.cmml" xref="S2.Ex2Xa.2.1.1.m1.1.1.2">ğ‘§</ci><apply id="S2.Ex2Xa.2.1.1.m1.1.1.3.cmml" xref="S2.Ex2Xa.2.1.1.m1.1.1.3"><plus id="S2.Ex2Xa.2.1.1.m1.1.1.3.1.cmml" xref="S2.Ex2Xa.2.1.1.m1.1.1.3.1"></plus><ci id="S2.Ex2Xa.2.1.1.m1.1.1.3.2.cmml" xref="S2.Ex2Xa.2.1.1.m1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S2.Ex2Xa.2.1.1.m1.1.1.3.3.cmml" xref="S2.Ex2Xa.2.1.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2Xa.2.1.1.m1.1c">\displaystyle z_{i+1}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex2Xa.3.2.2.m1.4" class="ltx_Math" alttext="\displaystyle=u_{i}\odot\text{LongConv}_{i}(z_{i}),i\in[0,N)" display="inline"><semantics id="S2.Ex2Xa.3.2.2.m1.4a"><mrow id="S2.Ex2Xa.3.2.2.m1.4.4.2" xref="S2.Ex2Xa.3.2.2.m1.4.4.3.cmml"><mrow id="S2.Ex2Xa.3.2.2.m1.3.3.1.1" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.cmml"><mi id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.3" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.3.cmml"></mi><mo id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.2" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.2.cmml">=</mo><mrow id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.cmml"><mrow id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.cmml"><msub id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.2" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.2.cmml"><mi id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.2.2" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.2.2.cmml">u</mi><mi id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.2.3" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.1" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.1.cmml">âŠ™</mo><msub id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.3" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.3.cmml"><mtext id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.3.2" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.3.2a.cmml">LongConv</mtext><mi id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.3.3" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.3.3.cmml">i</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.2" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.2.cmml">â€‹</mo><mrow id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1.2" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1.1.cmml">(</mo><msub id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1.1" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1.1.2" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1.1.2.cmml">z</mi><mi id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1.1.3" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1.3" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S2.Ex2Xa.3.2.2.m1.4.4.2.3" xref="S2.Ex2Xa.3.2.2.m1.4.4.3a.cmml">,</mo><mrow id="S2.Ex2Xa.3.2.2.m1.4.4.2.2" xref="S2.Ex2Xa.3.2.2.m1.4.4.2.2.cmml"><mi id="S2.Ex2Xa.3.2.2.m1.4.4.2.2.2" xref="S2.Ex2Xa.3.2.2.m1.4.4.2.2.2.cmml">i</mi><mo id="S2.Ex2Xa.3.2.2.m1.4.4.2.2.1" xref="S2.Ex2Xa.3.2.2.m1.4.4.2.2.1.cmml">âˆˆ</mo><mrow id="S2.Ex2Xa.3.2.2.m1.4.4.2.2.3.2" xref="S2.Ex2Xa.3.2.2.m1.4.4.2.2.3.1.cmml"><mo stretchy="false" id="S2.Ex2Xa.3.2.2.m1.4.4.2.2.3.2.1" xref="S2.Ex2Xa.3.2.2.m1.4.4.2.2.3.1.cmml">[</mo><mn id="S2.Ex2Xa.3.2.2.m1.1.1" xref="S2.Ex2Xa.3.2.2.m1.1.1.cmml">0</mn><mo id="S2.Ex2Xa.3.2.2.m1.4.4.2.2.3.2.2" xref="S2.Ex2Xa.3.2.2.m1.4.4.2.2.3.1.cmml">,</mo><mi id="S2.Ex2Xa.3.2.2.m1.2.2" xref="S2.Ex2Xa.3.2.2.m1.2.2.cmml">N</mi><mo stretchy="false" id="S2.Ex2Xa.3.2.2.m1.4.4.2.2.3.2.3" xref="S2.Ex2Xa.3.2.2.m1.4.4.2.2.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2Xa.3.2.2.m1.4b"><apply id="S2.Ex2Xa.3.2.2.m1.4.4.3.cmml" xref="S2.Ex2Xa.3.2.2.m1.4.4.2"><csymbol cd="ambiguous" id="S2.Ex2Xa.3.2.2.m1.4.4.3a.cmml" xref="S2.Ex2Xa.3.2.2.m1.4.4.2.3">formulae-sequence</csymbol><apply id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1"><eq id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.2.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.2"></eq><csymbol cd="latexml" id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.3.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.3">absent</csymbol><apply id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1"><times id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.2.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.2"></times><apply id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3"><csymbol cd="latexml" id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.1.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.1">direct-product</csymbol><apply id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.2.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.2.1.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.2">subscript</csymbol><ci id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.2.2.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.2.2">ğ‘¢</ci><ci id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.2.3.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.2.3">ğ‘–</ci></apply><apply id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.3.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.3.1.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.3">subscript</csymbol><ci id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.3.2a.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.3.2"><mtext id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.3.2.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.3.2">LongConv</mtext></ci><ci id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.3.3.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.3.3.3">ğ‘–</ci></apply></apply><apply id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1">subscript</csymbol><ci id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1.1.2">ğ‘§</ci><ci id="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S2.Ex2Xa.3.2.2.m1.3.3.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply><apply id="S2.Ex2Xa.3.2.2.m1.4.4.2.2.cmml" xref="S2.Ex2Xa.3.2.2.m1.4.4.2.2"><in id="S2.Ex2Xa.3.2.2.m1.4.4.2.2.1.cmml" xref="S2.Ex2Xa.3.2.2.m1.4.4.2.2.1"></in><ci id="S2.Ex2Xa.3.2.2.m1.4.4.2.2.2.cmml" xref="S2.Ex2Xa.3.2.2.m1.4.4.2.2.2">ğ‘–</ci><interval closure="closed-open" id="S2.Ex2Xa.3.2.2.m1.4.4.2.2.3.1.cmml" xref="S2.Ex2Xa.3.2.2.m1.4.4.2.2.3.2"><cn type="integer" id="S2.Ex2Xa.3.2.2.m1.1.1.cmml" xref="S2.Ex2Xa.3.2.2.m1.1.1">0</cn><ci id="S2.Ex2Xa.3.2.2.m1.2.2.cmml" xref="S2.Ex2Xa.3.2.2.m1.2.2">ğ‘</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2Xa.3.2.2.m1.4c">\displaystyle=u_{i}\odot\text{LongConv}_{i}(z_{i}),i\in[0,N)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex2Xb"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.Ex2Xb.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle y" display="inline"><semantics id="S2.Ex2Xb.2.1.1.m1.1a"><mi id="S2.Ex2Xb.2.1.1.m1.1.1" xref="S2.Ex2Xb.2.1.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.Ex2Xb.2.1.1.m1.1b"><ci id="S2.Ex2Xb.2.1.1.m1.1.1.cmml" xref="S2.Ex2Xb.2.1.1.m1.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2Xb.2.1.1.m1.1c">\displaystyle y</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex2Xb.3.2.2.m1.1" class="ltx_Math" alttext="\displaystyle=z_{N}" display="inline"><semantics id="S2.Ex2Xb.3.2.2.m1.1a"><mrow id="S2.Ex2Xb.3.2.2.m1.1.1" xref="S2.Ex2Xb.3.2.2.m1.1.1.cmml"><mi id="S2.Ex2Xb.3.2.2.m1.1.1.2" xref="S2.Ex2Xb.3.2.2.m1.1.1.2.cmml"></mi><mo id="S2.Ex2Xb.3.2.2.m1.1.1.1" xref="S2.Ex2Xb.3.2.2.m1.1.1.1.cmml">=</mo><msub id="S2.Ex2Xb.3.2.2.m1.1.1.3" xref="S2.Ex2Xb.3.2.2.m1.1.1.3.cmml"><mi id="S2.Ex2Xb.3.2.2.m1.1.1.3.2" xref="S2.Ex2Xb.3.2.2.m1.1.1.3.2.cmml">z</mi><mi id="S2.Ex2Xb.3.2.2.m1.1.1.3.3" xref="S2.Ex2Xb.3.2.2.m1.1.1.3.3.cmml">N</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2Xb.3.2.2.m1.1b"><apply id="S2.Ex2Xb.3.2.2.m1.1.1.cmml" xref="S2.Ex2Xb.3.2.2.m1.1.1"><eq id="S2.Ex2Xb.3.2.2.m1.1.1.1.cmml" xref="S2.Ex2Xb.3.2.2.m1.1.1.1"></eq><csymbol cd="latexml" id="S2.Ex2Xb.3.2.2.m1.1.1.2.cmml" xref="S2.Ex2Xb.3.2.2.m1.1.1.2">absent</csymbol><apply id="S2.Ex2Xb.3.2.2.m1.1.1.3.cmml" xref="S2.Ex2Xb.3.2.2.m1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex2Xb.3.2.2.m1.1.1.3.1.cmml" xref="S2.Ex2Xb.3.2.2.m1.1.1.3">subscript</csymbol><ci id="S2.Ex2Xb.3.2.2.m1.1.1.3.2.cmml" xref="S2.Ex2Xb.3.2.2.m1.1.1.3.2">ğ‘§</ci><ci id="S2.Ex2Xb.3.2.2.m1.1.1.3.3.cmml" xref="S2.Ex2Xb.3.2.2.m1.1.1.3.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2Xb.3.2.2.m1.1c">\displaystyle=z_{N}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.p3.3" class="ltx_p">where <math id="S2.SS3.p3.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS3.p3.1.m1.1a"><mi id="S2.SS3.p3.1.m1.1.1" xref="S2.SS3.p3.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.1.m1.1b"><ci id="S2.SS3.p3.1.m1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.1.m1.1c">x</annotation></semantics></math> is the input sequence, <math id="S2.SS3.p3.2.m2.3" class="ltx_Math" alttext="W_{0,...,N+1}" display="inline"><semantics id="S2.SS3.p3.2.m2.3a"><msub id="S2.SS3.p3.2.m2.3.4" xref="S2.SS3.p3.2.m2.3.4.cmml"><mi id="S2.SS3.p3.2.m2.3.4.2" xref="S2.SS3.p3.2.m2.3.4.2.cmml">W</mi><mrow id="S2.SS3.p3.2.m2.3.3.3.3" xref="S2.SS3.p3.2.m2.3.3.3.4.cmml"><mn id="S2.SS3.p3.2.m2.1.1.1.1" xref="S2.SS3.p3.2.m2.1.1.1.1.cmml">0</mn><mo id="S2.SS3.p3.2.m2.3.3.3.3.2" xref="S2.SS3.p3.2.m2.3.3.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS3.p3.2.m2.2.2.2.2" xref="S2.SS3.p3.2.m2.2.2.2.2.cmml">â€¦</mi><mo id="S2.SS3.p3.2.m2.3.3.3.3.3" xref="S2.SS3.p3.2.m2.3.3.3.4.cmml">,</mo><mrow id="S2.SS3.p3.2.m2.3.3.3.3.1" xref="S2.SS3.p3.2.m2.3.3.3.3.1.cmml"><mi id="S2.SS3.p3.2.m2.3.3.3.3.1.2" xref="S2.SS3.p3.2.m2.3.3.3.3.1.2.cmml">N</mi><mo id="S2.SS3.p3.2.m2.3.3.3.3.1.1" xref="S2.SS3.p3.2.m2.3.3.3.3.1.1.cmml">+</mo><mn id="S2.SS3.p3.2.m2.3.3.3.3.1.3" xref="S2.SS3.p3.2.m2.3.3.3.3.1.3.cmml">1</mn></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.2.m2.3b"><apply id="S2.SS3.p3.2.m2.3.4.cmml" xref="S2.SS3.p3.2.m2.3.4"><csymbol cd="ambiguous" id="S2.SS3.p3.2.m2.3.4.1.cmml" xref="S2.SS3.p3.2.m2.3.4">subscript</csymbol><ci id="S2.SS3.p3.2.m2.3.4.2.cmml" xref="S2.SS3.p3.2.m2.3.4.2">ğ‘Š</ci><list id="S2.SS3.p3.2.m2.3.3.3.4.cmml" xref="S2.SS3.p3.2.m2.3.3.3.3"><cn type="integer" id="S2.SS3.p3.2.m2.1.1.1.1.cmml" xref="S2.SS3.p3.2.m2.1.1.1.1">0</cn><ci id="S2.SS3.p3.2.m2.2.2.2.2.cmml" xref="S2.SS3.p3.2.m2.2.2.2.2">â€¦</ci><apply id="S2.SS3.p3.2.m2.3.3.3.3.1.cmml" xref="S2.SS3.p3.2.m2.3.3.3.3.1"><plus id="S2.SS3.p3.2.m2.3.3.3.3.1.1.cmml" xref="S2.SS3.p3.2.m2.3.3.3.3.1.1"></plus><ci id="S2.SS3.p3.2.m2.3.3.3.3.1.2.cmml" xref="S2.SS3.p3.2.m2.3.3.3.3.1.2">ğ‘</ci><cn type="integer" id="S2.SS3.p3.2.m2.3.3.3.3.1.3.cmml" xref="S2.SS3.p3.2.m2.3.3.3.3.1.3">1</cn></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.2.m2.3c">W_{0,...,N+1}</annotation></semantics></math> are learned weights and <math id="S2.SS3.p3.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS3.p3.3.m3.1a"><mi id="S2.SS3.p3.3.m3.1.1" xref="S2.SS3.p3.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.3.m3.1b"><ci id="S2.SS3.p3.3.m3.1.1.cmml" xref="S2.SS3.p3.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.3.m3.1c">N</annotation></semantics></math> is a hyperparameter named <span id="S2.SS3.p3.3.1" class="ltx_text ltx_font_italic">order</span>, which controls the depth of the recurrence and is usually set to 2.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.7" class="ltx_p">The short convolution (ShortConv) models short-term dependencies and is a Conv1D with kernel size 3 and stride 1, which does not alter the shape of the input.
The long convolution (LongConv) models long-range dependencies and its efficient implementation is the key to the sub-quadratic complexity of the Hyena operator.
A naive implementation of the LongConv would require learning a kernel of the same size as the input, which slides over the padded input <math id="S2.SS3.p4.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S2.SS3.p4.1.m1.1a"><mi id="S2.SS3.p4.1.m1.1.1" xref="S2.SS3.p4.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.1.m1.1b"><ci id="S2.SS3.p4.1.m1.1.1.cmml" xref="S2.SS3.p4.1.m1.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.1.m1.1c">L</annotation></semantics></math> times. This approach would result in a quadratic complexity, akin to self-attention.
In Hyena, instead, the kernel is implicitly learned by applying a
<span id="S2.SS3.p4.7.1" class="ltx_text" style="color:#000000;">FFN</span>
over complex exponential positional embeddings <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a href="#bib.bib45" title="" class="ltx_ref">2020a</a>)</cite>, making the number of the kernel parameters
independent from the input length.
Furthermore, Hyena bases its LongConv implementation on two principles:
<span id="S2.SS3.p4.7.2" class="ltx_text ltx_font_italic">i)</span> the convolution theorem <cite class="ltx_cite ltx_citemacro_citep">(Bracewell and Kahn, <a href="#bib.bib5" title="" class="ltx_ref">1966</a>)</cite>, which states that a circular convolution corresponds to an element-wise multiplication in the discrete Fourier domain <cite class="ltx_cite ltx_citemacro_citep">(Oppenheim and Schafer, <a href="#bib.bib30" title="" class="ltx_ref">2009</a>)</cite>, and <span id="S2.SS3.p4.7.3" class="ltx_text ltx_font_italic">ii)</span> the equivalence between a linear convolution over a sequence of length <math id="S2.SS3.p4.2.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S2.SS3.p4.2.m2.1a"><mi id="S2.SS3.p4.2.m2.1.1" xref="S2.SS3.p4.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.2.m2.1b"><ci id="S2.SS3.p4.2.m2.1.1.cmml" xref="S2.SS3.p4.2.m2.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.2.m2.1c">L</annotation></semantics></math> with a kernel <math id="S2.SS3.p4.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS3.p4.3.m3.1a"><mi id="S2.SS3.p4.3.m3.1.1" xref="S2.SS3.p4.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.3.m3.1b"><ci id="S2.SS3.p4.3.m3.1.1.cmml" xref="S2.SS3.p4.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.3.m3.1c">k</annotation></semantics></math> of length <math id="S2.SS3.p4.4.m4.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.SS3.p4.4.m4.1a"><mi id="S2.SS3.p4.4.m4.1.1" xref="S2.SS3.p4.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.4.m4.1b"><ci id="S2.SS3.p4.4.m4.1.1.cmml" xref="S2.SS3.p4.4.m4.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.4.m4.1c">K</annotation></semantics></math> and a circular convolution with a kernel <math id="S2.SS3.p4.5.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS3.p4.5.m5.1a"><mi id="S2.SS3.p4.5.m5.1.1" xref="S2.SS3.p4.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.5.m5.1b"><ci id="S2.SS3.p4.5.m5.1.1.cmml" xref="S2.SS3.p4.5.m5.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.5.m5.1c">k</annotation></semantics></math> zero-padded to length <math id="S2.SS3.p4.6.m6.1" class="ltx_Math" alttext="K+L" display="inline"><semantics id="S2.SS3.p4.6.m6.1a"><mrow id="S2.SS3.p4.6.m6.1.1" xref="S2.SS3.p4.6.m6.1.1.cmml"><mi id="S2.SS3.p4.6.m6.1.1.2" xref="S2.SS3.p4.6.m6.1.1.2.cmml">K</mi><mo id="S2.SS3.p4.6.m6.1.1.1" xref="S2.SS3.p4.6.m6.1.1.1.cmml">+</mo><mi id="S2.SS3.p4.6.m6.1.1.3" xref="S2.SS3.p4.6.m6.1.1.3.cmml">L</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.6.m6.1b"><apply id="S2.SS3.p4.6.m6.1.1.cmml" xref="S2.SS3.p4.6.m6.1.1"><plus id="S2.SS3.p4.6.m6.1.1.1.cmml" xref="S2.SS3.p4.6.m6.1.1.1"></plus><ci id="S2.SS3.p4.6.m6.1.1.2.cmml" xref="S2.SS3.p4.6.m6.1.1.2">ğ¾</ci><ci id="S2.SS3.p4.6.m6.1.1.3.cmml" xref="S2.SS3.p4.6.m6.1.1.3">ğ¿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.6.m6.1c">K+L</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Oppenheim and Schafer, <a href="#bib.bib29" title="" class="ltx_ref">1975</a>)</cite>. Building on them, Hyena computes the <math id="S2.SS3.p4.7.m7.1" class="ltx_Math" alttext="\text{LongConv}_{i}(x)" display="inline"><semantics id="S2.SS3.p4.7.m7.1a"><mrow id="S2.SS3.p4.7.m7.1.2" xref="S2.SS3.p4.7.m7.1.2.cmml"><msub id="S2.SS3.p4.7.m7.1.2.2" xref="S2.SS3.p4.7.m7.1.2.2.cmml"><mtext id="S2.SS3.p4.7.m7.1.2.2.2" xref="S2.SS3.p4.7.m7.1.2.2.2a.cmml">LongConv</mtext><mi id="S2.SS3.p4.7.m7.1.2.2.3" xref="S2.SS3.p4.7.m7.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS3.p4.7.m7.1.2.1" xref="S2.SS3.p4.7.m7.1.2.1.cmml">â€‹</mo><mrow id="S2.SS3.p4.7.m7.1.2.3.2" xref="S2.SS3.p4.7.m7.1.2.cmml"><mo stretchy="false" id="S2.SS3.p4.7.m7.1.2.3.2.1" xref="S2.SS3.p4.7.m7.1.2.cmml">(</mo><mi id="S2.SS3.p4.7.m7.1.1" xref="S2.SS3.p4.7.m7.1.1.cmml">x</mi><mo stretchy="false" id="S2.SS3.p4.7.m7.1.2.3.2.2" xref="S2.SS3.p4.7.m7.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.7.m7.1b"><apply id="S2.SS3.p4.7.m7.1.2.cmml" xref="S2.SS3.p4.7.m7.1.2"><times id="S2.SS3.p4.7.m7.1.2.1.cmml" xref="S2.SS3.p4.7.m7.1.2.1"></times><apply id="S2.SS3.p4.7.m7.1.2.2.cmml" xref="S2.SS3.p4.7.m7.1.2.2"><csymbol cd="ambiguous" id="S2.SS3.p4.7.m7.1.2.2.1.cmml" xref="S2.SS3.p4.7.m7.1.2.2">subscript</csymbol><ci id="S2.SS3.p4.7.m7.1.2.2.2a.cmml" xref="S2.SS3.p4.7.m7.1.2.2.2"><mtext id="S2.SS3.p4.7.m7.1.2.2.2.cmml" xref="S2.SS3.p4.7.m7.1.2.2.2">LongConv</mtext></ci><ci id="S2.SS3.p4.7.m7.1.2.2.3.cmml" xref="S2.SS3.p4.7.m7.1.2.2.3">ğ‘–</ci></apply><ci id="S2.SS3.p4.7.m7.1.1.cmml" xref="S2.SS3.p4.7.m7.1.1">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.7.m7.1c">\text{LongConv}_{i}(x)</annotation></semantics></math> as:</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<table id="S2.E1" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S2.E1X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle z" display="inline"><semantics id="S2.E1X.2.1.1.m1.1a"><mi id="S2.E1X.2.1.1.m1.1.1" xref="S2.E1X.2.1.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S2.E1X.2.1.1.m1.1b"><ci id="S2.E1X.2.1.1.m1.1.1.cmml" xref="S2.E1X.2.1.1.m1.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.E1X.2.1.1.m1.1c">\displaystyle z</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E1X.3.2.2.m1.1" class="ltx_math_unparsed" alttext="\displaystyle=\text{iFFT}(\text{FFT}(\text{pad}(x))\odot\text{FFT}(\text{pad}(k_{i}))" display="inline"><semantics id="S2.E1X.3.2.2.m1.1a"><mrow id="S2.E1X.3.2.2.m1.1b"><mo id="S2.E1X.3.2.2.m1.1.2">=</mo><mtext id="S2.E1X.3.2.2.m1.1.3">iFFT</mtext><mrow id="S2.E1X.3.2.2.m1.1.4"><mo stretchy="false" id="S2.E1X.3.2.2.m1.1.4.1">(</mo><mtext id="S2.E1X.3.2.2.m1.1.4.2">FFT</mtext><mrow id="S2.E1X.3.2.2.m1.1.4.3"><mo stretchy="false" id="S2.E1X.3.2.2.m1.1.4.3.1">(</mo><mtext id="S2.E1X.3.2.2.m1.1.4.3.2">pad</mtext><mrow id="S2.E1X.3.2.2.m1.1.4.3.3"><mo stretchy="false" id="S2.E1X.3.2.2.m1.1.4.3.3.1">(</mo><mi id="S2.E1X.3.2.2.m1.1.1">x</mi><mo stretchy="false" id="S2.E1X.3.2.2.m1.1.4.3.3.2">)</mo></mrow><mo rspace="0.055em" stretchy="false" id="S2.E1X.3.2.2.m1.1.4.3.4">)</mo></mrow><mo rspace="0.222em" id="S2.E1X.3.2.2.m1.1.4.4">âŠ™</mo><mtext id="S2.E1X.3.2.2.m1.1.4.5">FFT</mtext><mrow id="S2.E1X.3.2.2.m1.1.4.6"><mo stretchy="false" id="S2.E1X.3.2.2.m1.1.4.6.1">(</mo><mtext id="S2.E1X.3.2.2.m1.1.4.6.2">pad</mtext><mrow id="S2.E1X.3.2.2.m1.1.4.6.3"><mo stretchy="false" id="S2.E1X.3.2.2.m1.1.4.6.3.1">(</mo><msub id="S2.E1X.3.2.2.m1.1.4.6.3.2"><mi id="S2.E1X.3.2.2.m1.1.4.6.3.2.2">k</mi><mi id="S2.E1X.3.2.2.m1.1.4.6.3.2.3">i</mi></msub><mo stretchy="false" id="S2.E1X.3.2.2.m1.1.4.6.3.3">)</mo></mrow><mo stretchy="false" id="S2.E1X.3.2.2.m1.1.4.6.4">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex" id="S2.E1X.3.2.2.m1.1c">\displaystyle=\text{iFFT}(\text{FFT}(\text{pad}(x))\odot\text{FFT}(\text{pad}(k_{i}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
<tr id="S2.E1Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1Xa.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle y" display="inline"><semantics id="S2.E1Xa.2.1.1.m1.1a"><mi id="S2.E1Xa.2.1.1.m1.1.1" xref="S2.E1Xa.2.1.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.E1Xa.2.1.1.m1.1b"><ci id="S2.E1Xa.2.1.1.m1.1.1.cmml" xref="S2.E1Xa.2.1.1.m1.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.E1Xa.2.1.1.m1.1c">\displaystyle y</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E1Xa.3.2.2.m1.1" class="ltx_math_unparsed" alttext="\displaystyle=z[:L]" display="inline"><semantics id="S2.E1Xa.3.2.2.m1.1a"><mrow id="S2.E1Xa.3.2.2.m1.1b"><mo id="S2.E1Xa.3.2.2.m1.1.1">=</mo><mi id="S2.E1Xa.3.2.2.m1.1.2">z</mi><mrow id="S2.E1Xa.3.2.2.m1.1.3"><mo stretchy="false" id="S2.E1Xa.3.2.2.m1.1.3.1">[</mo><mo rspace="0.278em" id="S2.E1Xa.3.2.2.m1.1.3.2">:</mo><mi id="S2.E1Xa.3.2.2.m1.1.3.3">L</mi><mo stretchy="false" id="S2.E1Xa.3.2.2.m1.1.3.4">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.E1Xa.3.2.2.m1.1c">\displaystyle=z[:L]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
<div id="S2.SS3.p6" class="ltx_para ltx_noindent">
<p id="S2.SS3.p6.4" class="ltx_p">where FFT is the Fast Fourier Transform <cite class="ltx_cite ltx_citemacro_citep">(Selesnick and Burrus, <a href="#bib.bib41" title="" class="ltx_ref">1997</a>)</cite>, iFFT is the inverse FFT and <math id="S2.SS3.p6.1.m1.1" class="ltx_math_unparsed" alttext="\text{pad}(.)" display="inline"><semantics id="S2.SS3.p6.1.m1.1a"><mrow id="S2.SS3.p6.1.m1.1b"><mtext id="S2.SS3.p6.1.m1.1.1">pad</mtext><mrow id="S2.SS3.p6.1.m1.1.2"><mo stretchy="false" id="S2.SS3.p6.1.m1.1.2.1">(</mo><mo lspace="0em" rspace="0.167em" id="S2.SS3.p6.1.m1.1.2.2">.</mo><mo stretchy="false" id="S2.SS3.p6.1.m1.1.2.3">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.SS3.p6.1.m1.1c">\text{pad}(.)</annotation></semantics></math> pads the input with 0s up to a <math id="S2.SS3.p6.2.m2.1" class="ltx_Math" alttext="2L" display="inline"><semantics id="S2.SS3.p6.2.m2.1a"><mrow id="S2.SS3.p6.2.m2.1.1" xref="S2.SS3.p6.2.m2.1.1.cmml"><mn id="S2.SS3.p6.2.m2.1.1.2" xref="S2.SS3.p6.2.m2.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S2.SS3.p6.2.m2.1.1.1" xref="S2.SS3.p6.2.m2.1.1.1.cmml">â€‹</mo><mi id="S2.SS3.p6.2.m2.1.1.3" xref="S2.SS3.p6.2.m2.1.1.3.cmml">L</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p6.2.m2.1b"><apply id="S2.SS3.p6.2.m2.1.1.cmml" xref="S2.SS3.p6.2.m2.1.1"><times id="S2.SS3.p6.2.m2.1.1.1.cmml" xref="S2.SS3.p6.2.m2.1.1.1"></times><cn type="integer" id="S2.SS3.p6.2.m2.1.1.2.cmml" xref="S2.SS3.p6.2.m2.1.1.2">2</cn><ci id="S2.SS3.p6.2.m2.1.1.3.cmml" xref="S2.SS3.p6.2.m2.1.1.3">ğ¿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p6.2.m2.1c">2L</annotation></semantics></math> length.
The output selection <math id="S2.SS3.p6.3.m3.1" class="ltx_math_unparsed" alttext="z[:L]" display="inline"><semantics id="S2.SS3.p6.3.m3.1a"><mrow id="S2.SS3.p6.3.m3.1b"><mi id="S2.SS3.p6.3.m3.1.1">z</mi><mrow id="S2.SS3.p6.3.m3.1.2"><mo stretchy="false" id="S2.SS3.p6.3.m3.1.2.1">[</mo><mo rspace="0.278em" id="S2.SS3.p6.3.m3.1.2.2">:</mo><mi id="S2.SS3.p6.3.m3.1.2.3">L</mi><mo stretchy="false" id="S2.SS3.p6.3.m3.1.2.4">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.SS3.p6.3.m3.1c">z[:L]</annotation></semantics></math> preserves <span id="S2.SS3.p6.4.1" class="ltx_text ltx_font_italic">causality</span> (i.e., the output at a given position depends only on the past), which is necessary for autoregressive models such as Transformer decoders.
The resulting LongConv and, in turn, Hyena implementation has a sub-quadratic computational complexity of <math id="S2.SS3.p6.4.m4.1" class="ltx_Math" alttext="\mathcal{O}(L\log_{2}L)" display="inline"><semantics id="S2.SS3.p6.4.m4.1a"><mrow id="S2.SS3.p6.4.m4.1.1" xref="S2.SS3.p6.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p6.4.m4.1.1.3" xref="S2.SS3.p6.4.m4.1.1.3.cmml">ğ’ª</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p6.4.m4.1.1.2" xref="S2.SS3.p6.4.m4.1.1.2.cmml">â€‹</mo><mrow id="S2.SS3.p6.4.m4.1.1.1.1" xref="S2.SS3.p6.4.m4.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS3.p6.4.m4.1.1.1.1.2" xref="S2.SS3.p6.4.m4.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS3.p6.4.m4.1.1.1.1.1" xref="S2.SS3.p6.4.m4.1.1.1.1.1.cmml"><mi id="S2.SS3.p6.4.m4.1.1.1.1.1.2" xref="S2.SS3.p6.4.m4.1.1.1.1.1.2.cmml">L</mi><mo lspace="0.167em" rspace="0em" id="S2.SS3.p6.4.m4.1.1.1.1.1.1" xref="S2.SS3.p6.4.m4.1.1.1.1.1.1.cmml">â€‹</mo><mrow id="S2.SS3.p6.4.m4.1.1.1.1.1.3" xref="S2.SS3.p6.4.m4.1.1.1.1.1.3.cmml"><msub id="S2.SS3.p6.4.m4.1.1.1.1.1.3.1" xref="S2.SS3.p6.4.m4.1.1.1.1.1.3.1.cmml"><mi id="S2.SS3.p6.4.m4.1.1.1.1.1.3.1.2" xref="S2.SS3.p6.4.m4.1.1.1.1.1.3.1.2.cmml">log</mi><mn id="S2.SS3.p6.4.m4.1.1.1.1.1.3.1.3" xref="S2.SS3.p6.4.m4.1.1.1.1.1.3.1.3.cmml">2</mn></msub><mo lspace="0.167em" id="S2.SS3.p6.4.m4.1.1.1.1.1.3a" xref="S2.SS3.p6.4.m4.1.1.1.1.1.3.cmml">â¡</mo><mi id="S2.SS3.p6.4.m4.1.1.1.1.1.3.2" xref="S2.SS3.p6.4.m4.1.1.1.1.1.3.2.cmml">L</mi></mrow></mrow><mo stretchy="false" id="S2.SS3.p6.4.m4.1.1.1.1.3" xref="S2.SS3.p6.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p6.4.m4.1b"><apply id="S2.SS3.p6.4.m4.1.1.cmml" xref="S2.SS3.p6.4.m4.1.1"><times id="S2.SS3.p6.4.m4.1.1.2.cmml" xref="S2.SS3.p6.4.m4.1.1.2"></times><ci id="S2.SS3.p6.4.m4.1.1.3.cmml" xref="S2.SS3.p6.4.m4.1.1.3">ğ’ª</ci><apply id="S2.SS3.p6.4.m4.1.1.1.1.1.cmml" xref="S2.SS3.p6.4.m4.1.1.1.1"><times id="S2.SS3.p6.4.m4.1.1.1.1.1.1.cmml" xref="S2.SS3.p6.4.m4.1.1.1.1.1.1"></times><ci id="S2.SS3.p6.4.m4.1.1.1.1.1.2.cmml" xref="S2.SS3.p6.4.m4.1.1.1.1.1.2">ğ¿</ci><apply id="S2.SS3.p6.4.m4.1.1.1.1.1.3.cmml" xref="S2.SS3.p6.4.m4.1.1.1.1.1.3"><apply id="S2.SS3.p6.4.m4.1.1.1.1.1.3.1.cmml" xref="S2.SS3.p6.4.m4.1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S2.SS3.p6.4.m4.1.1.1.1.1.3.1.1.cmml" xref="S2.SS3.p6.4.m4.1.1.1.1.1.3.1">subscript</csymbol><log id="S2.SS3.p6.4.m4.1.1.1.1.1.3.1.2.cmml" xref="S2.SS3.p6.4.m4.1.1.1.1.1.3.1.2"></log><cn type="integer" id="S2.SS3.p6.4.m4.1.1.1.1.1.3.1.3.cmml" xref="S2.SS3.p6.4.m4.1.1.1.1.1.3.1.3">2</cn></apply><ci id="S2.SS3.p6.4.m4.1.1.1.1.1.3.2.cmml" xref="S2.SS3.p6.4.m4.1.1.1.1.1.3.2">ğ¿</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p6.4.m4.1c">\mathcal{O}(L\log_{2}L)</annotation></semantics></math><span id="S2.SS3.p6.4.2" class="ltx_text" style="color:#000000;">. While replacing the self-attention with sub-quadratic solutions based on the Fourier transform is not a new proposal <cite class="ltx_cite ltx_citemacro_citep">(Lee-Thorp etÂ al., <a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite>, Hyena has been the first to claim no performance loss</span>, which makes it a promising alternative to self-attention with quadratic complexity.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.5.1.1" class="ltx_tr">
<th id="S2.T1.5.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;">Â </span><span id="S2.T1.5.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Model</span>
</th>
<th id="S2.T1.5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">ASR (en)</span></th>
<th id="S2.T1.5.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">en-de</span></th>
<th id="S2.T1.5.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">en-es</span></th>
<th id="S2.T1.5.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">en-fr</span></th>
<th id="S2.T1.5.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">en-it</span></th>
<th id="S2.T1.5.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.1.1.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">en-nl</span></th>
<th id="S2.T1.5.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.1.1.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">en-pt</span></th>
<th id="S2.T1.5.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.1.1.9.1" class="ltx_text ltx_font_bold" style="font-size:90%;">en-ro</span></th>
<th id="S2.T1.5.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.1.1.10.1" class="ltx_text ltx_font_bold" style="font-size:90%;">en-ru</span></th>
<th id="S2.T1.5.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.1.1.11.1" class="ltx_text ltx_font_bold" style="font-size:90%;">ST avg</span></th>
</tr>
<tr id="S2.T1.5.2.2" class="ltx_tr">
<th id="S2.T1.5.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_rr ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.2.2.1.1" class="ltx_text" style="font-size:90%;">Conformer</span></th>
<th id="S2.T1.5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">10.64</span></th>
<th id="S2.T1.5.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.2.2.3.1" class="ltx_text" style="font-size:90%;">24.97</span></th>
<th id="S2.T1.5.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.2.2.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">30.48</span></th>
<th id="S2.T1.5.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.2.2.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">36.43</span></th>
<th id="S2.T1.5.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.2.2.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">26.25</span></th>
<th id="S2.T1.5.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.2.2.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">30.31</span></th>
<th id="S2.T1.5.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.2.2.8.1" class="ltx_text" style="font-size:90%;">30.09</span></th>
<th id="S2.T1.5.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.2.2.9.1" class="ltx_text ltx_font_bold" style="font-size:90%;">24.67</span></th>
<th id="S2.T1.5.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.2.2.10.1" class="ltx_text ltx_font_bold" style="font-size:90%;">17.35</span></th>
<th id="S2.T1.5.2.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.2.2.11.1" class="ltx_text ltx_font_bold" style="font-size:90%;">27.57</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.5.3.1" class="ltx_tr">
<th id="S2.T1.5.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.3.1.1.1" class="ltx_text" style="font-size:90%;">ConfHyena</span></th>
<th id="S2.T1.5.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.3.1.2.1" class="ltx_text" style="font-size:90%;">10.88</span></th>
<td id="S2.T1.5.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.3.1.3.1" class="ltx_text" style="font-size:90%;">24.90</span></td>
<td id="S2.T1.5.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.3.1.4.1" class="ltx_text" style="font-size:90%;">30.28</span></td>
<td id="S2.T1.5.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.3.1.5.1" class="ltx_text" style="font-size:90%;">35.71*</span></td>
<td id="S2.T1.5.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.3.1.6.1" class="ltx_text" style="font-size:90%;">25.69*</span></td>
<td id="S2.T1.5.3.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.3.1.7.1" class="ltx_text" style="font-size:90%;">29.49*</span></td>
<td id="S2.T1.5.3.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.3.1.8.1" class="ltx_text" style="font-size:90%;">30.51*</span></td>
<td id="S2.T1.5.3.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.3.1.9.1" class="ltx_text" style="font-size:90%;">23.93*</span></td>
<td id="S2.T1.5.3.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.3.1.10.1" class="ltx_text" style="font-size:90%;">17.34</span></td>
<td id="S2.T1.5.3.1.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.3.1.11.1" class="ltx_text" style="font-size:90%;">27.23</span></td>
</tr>
<tr id="S2.T1.5.4.2" class="ltx_tr">
<th id="S2.T1.5.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.4.2.1.1" class="ltx_text" style="font-size:90%;">- non-causal Hyena</span></th>
<th id="S2.T1.5.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.4.2.2.1" class="ltx_text" style="font-size:90%;">10.99*</span></th>
<td id="S2.T1.5.4.2.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.4.2.3.1" class="ltx_text" style="font-size:90%;">24.28</span></td>
<td id="S2.T1.5.4.2.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.4.2.4.1" class="ltx_text" style="font-size:90%;">29.42*</span></td>
<td id="S2.T1.5.4.2.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.4.2.5.1" class="ltx_text" style="font-size:90%;">35.57*</span></td>
<td id="S2.T1.5.4.2.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.4.2.6.1" class="ltx_text" style="font-size:90%;">25.38*</span></td>
<td id="S2.T1.5.4.2.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.4.2.7.1" class="ltx_text" style="font-size:90%;">29.55*</span></td>
<td id="S2.T1.5.4.2.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.4.2.8.1" class="ltx_text" style="font-size:90%;">29.99</span></td>
<td id="S2.T1.5.4.2.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.4.2.9.1" class="ltx_text" style="font-size:90%;">23.92*</span></td>
<td id="S2.T1.5.4.2.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.4.2.10.1" class="ltx_text" style="font-size:90%;">16.99*</span></td>
<td id="S2.T1.5.4.2.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.4.2.11.1" class="ltx_text" style="font-size:90%;">26.89</span></td>
</tr>
<tr id="S2.T1.5.5.3" class="ltx_tr">
<th id="S2.T1.5.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.5.3.1.1" class="ltx_text" style="font-size:90%;">Hybrid ConfHyena</span></th>
<th id="S2.T1.5.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.5.3.2.1" class="ltx_text" style="font-size:90%;">10.75</span></th>
<td id="S2.T1.5.5.3.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.5.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">25.22</span></td>
<td id="S2.T1.5.5.3.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.5.3.4.1" class="ltx_text" style="font-size:90%;">30.15*</span></td>
<td id="S2.T1.5.5.3.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.5.3.5.1" class="ltx_text" style="font-size:90%;">36.19</span></td>
<td id="S2.T1.5.5.3.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.5.3.6.1" class="ltx_text" style="font-size:90%;">26.04</span></td>
<td id="S2.T1.5.5.3.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.5.3.7.1" class="ltx_text" style="font-size:90%;">29.76*</span></td>
<td id="S2.T1.5.5.3.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.5.3.8.1" class="ltx_text" style="font-size:90%;">30.43</span></td>
<td id="S2.T1.5.5.3.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.5.3.9.1" class="ltx_text" style="font-size:90%;">23.78*</span></td>
<td id="S2.T1.5.5.3.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.5.3.10.1" class="ltx_text" style="font-size:90%;">17.06</span></td>
<td id="S2.T1.5.5.3.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.5.3.11.1" class="ltx_text" style="font-size:90%;">27.33</span></td>
</tr>
<tr id="S2.T1.5.6.4" class="ltx_tr">
<th id="S2.T1.5.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.6.4.1.1" class="ltx_text" style="font-size:90%;">- non-causal Hyena</span></th>
<th id="S2.T1.5.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.6.4.2.1" class="ltx_text" style="font-size:90%;">11.27*</span></th>
<td id="S2.T1.5.6.4.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.6.4.3.1" class="ltx_text" style="font-size:90%;">24.84</span></td>
<td id="S2.T1.5.6.4.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.6.4.4.1" class="ltx_text" style="font-size:90%;">30.28</span></td>
<td id="S2.T1.5.6.4.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.6.4.5.1" class="ltx_text" style="font-size:90%;">36.09</span></td>
<td id="S2.T1.5.6.4.6" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.6.4.6.1" class="ltx_text" style="font-size:90%;">25.81*</span></td>
<td id="S2.T1.5.6.4.7" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.6.4.7.1" class="ltx_text" style="font-size:90%;">29.53*</span></td>
<td id="S2.T1.5.6.4.8" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S2.T1.5.6.4.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">30.69</span><span id="S2.T1.5.6.4.8.2" class="ltx_text" style="font-size:90%;">*</span>
</td>
<td id="S2.T1.5.6.4.9" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.6.4.9.1" class="ltx_text" style="font-size:90%;">23.96*</span></td>
<td id="S2.T1.5.6.4.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.6.4.10.1" class="ltx_text" style="font-size:90%;">16.98*</span></td>
<td id="S2.T1.5.6.4.11" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S2.T1.5.6.4.11.1" class="ltx_text" style="font-size:90%;">27.27</span></td>
</tr>
<tr id="S2.T1.5.7.5" class="ltx_tr">
<th id="S2.T1.5.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;">Â </span></th>
<th id="S2.T1.5.7.5.2" class="ltx_td ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<td id="S2.T1.5.7.5.3" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S2.T1.5.7.5.4" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S2.T1.5.7.5.5" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S2.T1.5.7.5.6" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S2.T1.5.7.5.7" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S2.T1.5.7.5.8" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S2.T1.5.7.5.9" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S2.T1.5.7.5.10" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td id="S2.T1.5.7.5.11" class="ltx_td" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>ConfHyena and Hybrid ConfHyena results on MuST-C v1.0 tst-COMMON for all language pairs. ASR scores are WER<math id="S2.T1.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.3.m1.1b"><mo stretchy="false" id="S2.T1.3.m1.1.1" xref="S2.T1.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.m1.1c"><ci id="S2.T1.3.m1.1.1.cmml" xref="S2.T1.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.m1.1d">\downarrow</annotation></semantics></math> while ST scores are BLEU<math id="S2.T1.4.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.4.m2.1b"><mo stretchy="false" id="S2.T1.4.m2.1.1" xref="S2.T1.4.m2.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S2.T1.4.m2.1c"><ci id="S2.T1.4.m2.1.1.cmml" xref="S2.T1.4.m2.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.m2.1d">\uparrow</annotation></semantics></math>. * means that the difference with the baseline (Conformer) is statistically significant as per bootstrap resampling <cite class="ltx_cite ltx_citemacro_citep">(Koehn, <a href="#bib.bib21" title="" class="ltx_ref">2004</a>)</cite> with 95% CI.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.Â Â Â Hyena for Speech Processing</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">As just seen, the original Hyena operator is built to preserve the causality property.
However, in the context of speech processing, constraining the encoder to access only past elements could reduce performance, as it is typically designed to look at the entire sequence to create context-aware encodings from the complete input representation <cite class="ltx_cite ltx_citemacro_citep">(Chorowski etÂ al., <a href="#bib.bib8" title="" class="ltx_ref">2015</a>)</cite>.
For this reason, we introduce <span id="S3.p1.1.1" class="ltx_text ltx_font_bold">ConfHyena</span>, a Conformer-based model <span id="S3.p1.1.2" class="ltx_text" style="color:#000000;">(Â§<a href="#S2.SS2" title="2.2. Conformer â€£ 2. Background â€£ How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>)</span> where
encoder self-attentions are replaced by a non-causal version of the Hyena operator
<span id="S3.p1.1.3" class="ltx_text" style="color:#000000;">that</span> can access the entire input sequence.
The differences with the causal version of Hyena are two:</p>
</div>
<div id="S3.p2" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.2" class="ltx_p">the ShortConv operator processes the current, preceding, and following time frames at each step, rather than the current and the two preceding frames.
This is realized by setting the left padding to <math id="S3.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="\lceil\frac{K}{2}\rceil" display="inline"><semantics id="S3.I1.i1.p1.1.m1.1a"><mrow id="S3.I1.i1.p1.1.m1.1.2.2" xref="S3.I1.i1.p1.1.m1.1.2.1.cmml"><mo stretchy="false" id="S3.I1.i1.p1.1.m1.1.2.2.1" xref="S3.I1.i1.p1.1.m1.1.2.1.1.cmml">âŒˆ</mo><mfrac id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml"><mi id="S3.I1.i1.p1.1.m1.1.1.2" xref="S3.I1.i1.p1.1.m1.1.1.2.cmml">K</mi><mn id="S3.I1.i1.p1.1.m1.1.1.3" xref="S3.I1.i1.p1.1.m1.1.1.3.cmml">2</mn></mfrac><mo stretchy="false" id="S3.I1.i1.p1.1.m1.1.2.2.2" xref="S3.I1.i1.p1.1.m1.1.2.1.1.cmml">âŒ‰</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><apply id="S3.I1.i1.p1.1.m1.1.2.1.cmml" xref="S3.I1.i1.p1.1.m1.1.2.2"><ceiling id="S3.I1.i1.p1.1.m1.1.2.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.2.2.1"></ceiling><apply id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1"><divide id="S3.I1.i1.p1.1.m1.1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1"></divide><ci id="S3.I1.i1.p1.1.m1.1.1.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2">ğ¾</ci><cn type="integer" id="S3.I1.i1.p1.1.m1.1.1.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">\lceil\frac{K}{2}\rceil</annotation></semantics></math> instead of <math id="S3.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="K-1" display="inline"><semantics id="S3.I1.i1.p1.2.m2.1a"><mrow id="S3.I1.i1.p1.2.m2.1.1" xref="S3.I1.i1.p1.2.m2.1.1.cmml"><mi id="S3.I1.i1.p1.2.m2.1.1.2" xref="S3.I1.i1.p1.2.m2.1.1.2.cmml">K</mi><mo id="S3.I1.i1.p1.2.m2.1.1.1" xref="S3.I1.i1.p1.2.m2.1.1.1.cmml">âˆ’</mo><mn id="S3.I1.i1.p1.2.m2.1.1.3" xref="S3.I1.i1.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m2.1b"><apply id="S3.I1.i1.p1.2.m2.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1"><minus id="S3.I1.i1.p1.2.m2.1.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1.1"></minus><ci id="S3.I1.i1.p1.2.m2.1.1.2.cmml" xref="S3.I1.i1.p1.2.m2.1.1.2">ğ¾</ci><cn type="integer" id="S3.I1.i1.p1.2.m2.1.1.3.cmml" xref="S3.I1.i1.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m2.1c">K-1</annotation></semantics></math>;</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">the LongConv operator is similarly adjusted, with the non-causal version implemented by altering the selection in Eq. <a href="#S2.E1" title="In 2.3. Hyena â€£ 2. Background â€£ How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> to <math id="S3.I1.i2.p1.1.m1.1" class="ltx_math_unparsed" alttext="y=z[L/2:-L/2]" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><mrow id="S3.I1.i2.p1.1.m1.1b"><mi id="S3.I1.i2.p1.1.m1.1.1">y</mi><mo id="S3.I1.i2.p1.1.m1.1.2">=</mo><mi id="S3.I1.i2.p1.1.m1.1.3">z</mi><mrow id="S3.I1.i2.p1.1.m1.1.4"><mo stretchy="false" id="S3.I1.i2.p1.1.m1.1.4.1">[</mo><mi id="S3.I1.i2.p1.1.m1.1.4.2">L</mi><mo id="S3.I1.i2.p1.1.m1.1.4.3">/</mo><mn id="S3.I1.i2.p1.1.m1.1.4.4">2</mn><mo lspace="0.278em" rspace="0em" id="S3.I1.i2.p1.1.m1.1.4.5">:</mo><mo lspace="0em" id="S3.I1.i2.p1.1.m1.1.4.6">âˆ’</mo><mi id="S3.I1.i2.p1.1.m1.1.4.7">L</mi><mo id="S3.I1.i2.p1.1.m1.1.4.8">/</mo><mn id="S3.I1.i2.p1.1.m1.1.4.9">2</mn><mo stretchy="false" id="S3.I1.i2.p1.1.m1.1.4.10">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">y=z[L/2:-L/2]</annotation></semantics></math> to select the portion of the circular convolution that corresponds to a linear convolution with <span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">same</span> padding <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow etÂ al., <a href="#bib.bib16" title="" class="ltx_ref">2016</a>)</cite>.</p>
</div>
</li>
</ol>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">With such modifications, ConfHyena has access to the same information as the self-attention in speech-processing models, while retaining a lower computational and memory complexity.
The lower <span id="S3.p3.1.1" class="ltx_text ltx_font_italic">computational</span> complexity entails faster computation of long input sequences.
The lower <span id="S3.p3.1.2" class="ltx_text ltx_font_italic">memory</span> complexity, instead, enables ConfHyena to process longer inputs compared to attention-based models without incurring out-of-memory issues and to run on GPUs with less VRAM.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">In addition, we integrate a CTC-compression module <cite class="ltx_cite ltx_citemacro_citep">(Gaido etÂ al., <a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite> into the middle of the encoder of all our systems.
This module <span id="S3.p4.1.1" class="ltx_text" style="color:#000000;">is based on the connectionist temporal classification or CTC <cite class="ltx_cite ltx_citemacro_citep">(Graves etÂ al., <a href="#bib.bib17" title="" class="ltx_ref">2006</a>)</cite> and</span> collapses intermediate context-aware encodings to reduce their redundancy, making them more effectively encoded by successive
layers, with benefits in terms of both output quality and efficiency <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a href="#bib.bib27" title="" class="ltx_ref">2020</a>; Zhang etÂ al., <a href="#bib.bib48" title="" class="ltx_ref">2020</a>; Zhao etÂ al., <a href="#bib.bib50" title="" class="ltx_ref">2022</a>)</cite>.
As the sequences resulting from this module are significantly shorter, we posit that their length is sufficiently small so that they can be efficiently processed by self-attention.
As such, we propose <span id="S3.p4.1.2" class="ltx_text ltx_font_bold">Hybrid ConfHyena</span>, an architecture where the non-causal Hyena operator is introduced only in the layers <span id="S3.p4.1.3" class="ltx_text" style="color:#000000;">before</span> the CTC compression while preserving the self-attention in the subsequent ones.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.Â Â Â Experimental Settings</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">For both ASR and ST, we use the MuST-C v1.0 dataset <cite class="ltx_cite ltx_citemacro_citep">(Cattoni etÂ al., <a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite>, comprising <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">(audio, transcription, translation)</span> triplets in the TED-talks domain.
Audios and transcriptions are in English while translations cover 8 languages (de, es, fr, it, nl, pt, ro, ru).
For ASR, we use the en-es section.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The input is represented by 80-dimensional log mel-filterbank features (on 25ms windows sliding every 10ms) that are processed by 2 layers of CNN with stride 2, realizing a total downsampling of 4.
For all models, we use 12 encoder layers, 6 decoder layers, 8 heads, an embedding size of 512, and 2048 FFN width.
The kernel size of both point- and depth-wise convolutions in Conformer layers is set to 31.
In Hyena operators, the order is set to 2, the width is 3 times the embedding size, and the filter FFN has 4 layers with 64 neurons and sine activations.
Dropout is set to 0.1 for all models.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text" style="color:#000000;">To ensure the reliability of our findings, we used a Conformer implementation that is padding-safe (i.e., it does not change the output according to the amount of padding) and we tested our Hyena implementation with <span id="S4.p3.1.1.1" class="ltx_text ltx_font_typewriter">pangolinn</span> to ensure the same property <cite class="ltx_cite ltx_citemacro_citep">(Papi etÂ al., <a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite>.</span></p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">We train with Adam optimizer, label-smoothed cross-entropy loss (smoothing factor 0.1), and CTC loss <cite class="ltx_cite ltx_citemacro_citep">(Graves etÂ al., <a href="#bib.bib17" title="" class="ltx_ref">2006</a>)</cite> with weight factor 0.5 to ease convergence. We apply CTC compression after the 8<sup id="S4.p4.1.1" class="ltx_sup">th</sup> encoder layer.
The learning rate is 2e-3 with Noam scheduler <cite class="ltx_cite ltx_citemacro_cite">Vaswani etÂ al. (<a href="#bib.bib44" title="" class="ltx_ref">2017</a>)</cite> and 25,000 warm-up steps. SentencePiece unigram vocabularies <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite> are used with size 5,000 for English
and 8,000 for all target languages.
We early stop the training after 10 epochs without improvement on the dev loss and average 5 checkpoints around the best.
We train on 2 NVIDIA A40 40GB VRAM GPUs with 40k tokens per batch and 4 as update frequency
and generate with 1 NVIDIA K80 16GB VRAM GPU.
All other settings are the default of Fairseq-ST <cite class="ltx_cite ltx_citemacro_cite">Wang etÂ al. (<a href="#bib.bib46" title="" class="ltx_ref">2020b</a>)</cite>.
Evaluation is performed using WER for ASR and sacreBLEU <cite class="ltx_cite ltx_citemacro_citep">(Post, <a href="#bib.bib36" title="" class="ltx_ref">2018</a>)</cite><span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>BLEUâ€”c:mixedâ€”e:noâ€”tok:13aâ€”s:expâ€”v:2.0.0</span></span></span> for ST.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.Â Â Â Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">For a comprehensive evaluation, in this section, we first analyze the output quality of ConfHyena and Hybrid ConfHyena (Â§<a href="#S5.SS1" title="5.1. Output Quality â€£ 5. Results â€£ How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>), and then we study their efficiency (Â§<a href="#S5.SS2" title="5.2. Training and Inference Efficiency â€£ 5. Results â€£ How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>), as both aspects are critical in determining the success of an architecture.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">5.1.Â Â Â Output Quality</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Table <a href="#S2.T1" title="Table 1 â€£ 2.3. Hyena â€£ 2. Background â€£ How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> compares our ConfHyena and Hybrid ConfHyena models with Conformer in terms of transcription (ASR) and translation (ST) quality.
Through ablation tests, we also report the results of the proposed architectures with the original causal Hyena operator (<span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_italic">- non-causal Hyena</span>).</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Focusing on the effect of the causality property, the results confirm its negative impact on model performance.
Indeed, the causal versions of both ConfHyena and Hybrid ConfHyena are consistently outperformed by those equipped with the non-causal operator both in ASR and ST.
The difference is more pronounced in ASR, where causality produces statistically significant degradations up to 0.52 WER.
This holds also for ST, albeit with less significant gaps (on average over the 8 language pairs, -0.34 and -0.06 BLEU for ConfHyena and Hybrid ConfHyena, respectively).
These results confirm the usefulness of accessing the entire input sequence to maximize
transcription and translation quality and, therefore, <span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">the superiority of the non-causal
Hyena</span> in speech encoders.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Comparing ConfHyena and Hybrid ConfHyena, the latter is superior both in terms of transcription and translation quality.
The gap between the two models, however, is marginal (-0.13 WER, +0.10 BLEU), showing that <span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold">the Hybrid variant has similar quality as the full ConfHyena encoder</span>.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">Comparing Hybrid ConfHyena with Conformer, we observe that Conformer achieves the best results in most cases (ASR, and 6 out of 8 ST directions, with en-de and en-pt being the exceptions).
However, although Conformer consistently yields the best average score of 27.57 BLEU in ST, it is worth remarking that the overall margin over Hybrid ConfHyena is very narrow (-0.11 WER and +0.24 BLEU on average), corresponding to a <math id="S5.SS1.p4.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.SS1.p4.1.m1.1a"><mo id="S5.SS1.p4.1.m1.1.1" xref="S5.SS1.p4.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.1.m1.1b"><csymbol cd="latexml" id="S5.SS1.p4.1.m1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.1c">\sim</annotation></semantics></math>1% relative difference. Moreover, the difference is statistically significant only in 3 ST directions.
We can therefore conclude that, in terms of mere output quality, <span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold">Hybrid ConfHyena performs closely to the attention-based Conformer model.</span></p>
</div>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.2.2" class="ltx_tr">
<th id="S5.T2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="4">
<span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;">Â </span><span id="S5.T2.2.2.3.1" class="ltx_text" style="font-size:90%;">â€„</span><span id="S5.T2.2.2.3.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Model</span><span id="S5.T2.2.2.3.3" class="ltx_text" style="font-size:90%;"></span>
</th>
<th id="S5.T2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.2.2.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;"># param.</span></th>
<th id="S5.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">train time<math id="S5.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T2.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T2.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></th>
<th id="S5.T2.2.2.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">inf time<math id="S5.T2.2.2.2.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T2.2.2.2.1.m1.1a"><mo stretchy="false" id="S5.T2.2.2.2.1.m1.1.1" xref="S5.T2.2.2.2.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.1.m1.1b"><ci id="S5.T2.2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.2.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.1.m1.1c">\downarrow</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.4.4" class="ltx_tr">
<td id="S5.T2.4.4.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.4.4.3.1" class="ltx_text" style="font-size:90%;">Conformer</span></td>
<td id="S5.T2.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.4.4.4.1" class="ltx_text" style="font-size:90%;">114.9M</span></td>
<td id="S5.T2.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<math id="S5.T2.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.3.3.1.m1.1a"><mo mathsize="90%" id="S5.T2.3.3.1.m1.1.1" xref="S5.T2.3.3.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.1.m1.1b"><times id="S5.T2.3.3.1.m1.1.1.cmml" xref="S5.T2.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.1.m1.1c">\times</annotation></semantics></math><span id="S5.T2.3.3.1.1" class="ltx_text" style="font-size:90%;">1.00</span>
</td>
<td id="S5.T2.4.4.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">
<math id="S5.T2.4.4.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.4.4.2.m1.1a"><mo mathsize="90%" id="S5.T2.4.4.2.m1.1.1" xref="S5.T2.4.4.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.2.m1.1b"><times id="S5.T2.4.4.2.m1.1.1.cmml" xref="S5.T2.4.4.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.2.m1.1c">\times</annotation></semantics></math><span id="S5.T2.4.4.2.1" class="ltx_text" style="font-size:90%;">1.00</span>
</td>
</tr>
<tr id="S5.T2.6.6" class="ltx_tr">
<td id="S5.T2.6.6.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.6.6.3.1" class="ltx_text" style="font-size:90%;">ConfHyena</span></td>
<td id="S5.T2.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.6.6.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">112.0M</span></td>
<td id="S5.T2.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<math id="S5.T2.5.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.5.5.1.m1.1a"><mo mathsize="90%" id="S5.T2.5.5.1.m1.1.1" xref="S5.T2.5.5.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.1.m1.1b"><times id="S5.T2.5.5.1.m1.1.1.cmml" xref="S5.T2.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.1.m1.1c">\times</annotation></semantics></math><span id="S5.T2.5.5.1.1" class="ltx_text" style="font-size:90%;">1.04</span>
</td>
<td id="S5.T2.6.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<math id="S5.T2.6.6.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.6.6.2.m1.1a"><mo mathsize="90%" id="S5.T2.6.6.2.m1.1.1" xref="S5.T2.6.6.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.2.m1.1b"><times id="S5.T2.6.6.2.m1.1.1.cmml" xref="S5.T2.6.6.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.2.m1.1c">\times</annotation></semantics></math><span id="S5.T2.6.6.2.1" class="ltx_text" style="font-size:90%;">0.95</span>
</td>
</tr>
<tr id="S5.T2.8.8" class="ltx_tr">
<td id="S5.T2.8.8.3" class="ltx_td ltx_align_left ltx_border_rr" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.8.8.3.1" class="ltx_text" style="font-size:90%;">Hybrid ConfHyena</span></td>
<td id="S5.T2.8.8.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T2.8.8.4.1" class="ltx_text" style="font-size:90%;">112.9M</span></td>
<td id="S5.T2.7.7.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<math id="S5.T2.7.7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.7.7.1.m1.1a"><mo mathsize="90%" id="S5.T2.7.7.1.m1.1.1" xref="S5.T2.7.7.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.7.7.1.m1.1b"><times id="S5.T2.7.7.1.m1.1.1.cmml" xref="S5.T2.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.7.1.m1.1c">\times</annotation></semantics></math><span id="S5.T2.7.7.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.73</span>
</td>
<td id="S5.T2.8.8.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">
<math id="S5.T2.8.8.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T2.8.8.2.m1.1a"><mo mathsize="90%" id="S5.T2.8.8.2.m1.1.1" xref="S5.T2.8.8.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S5.T2.8.8.2.m1.1b"><times id="S5.T2.8.8.2.m1.1.1.cmml" xref="S5.T2.8.8.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.8.8.2.m1.1c">\times</annotation></semantics></math><span id="S5.T2.8.8.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.93</span>
</td>
</tr>
<tr id="S5.T2.8.9.1" class="ltx_tr">
<td id="S5.T2.8.9.1.1" class="ltx_td ltx_align_left ltx_border_rr" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;">Â </span></td>
<td id="S5.T2.8.9.1.2" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S5.T2.8.9.1.3" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S5.T2.8.9.1.4" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Relative inference and training time averaged over all languages and tasks of Conformer, ConfHyena, and Hybrid ConfHyena.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">5.2.Â Â Â Training and Inference Efficiency</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">After establishing that the output quality of the Conformer and ConfHyena models is similar, we now turn to assess their efficiency.
Looking at Table <a href="#S5.T2" title="Table 2 â€£ 5.1. Output Quality â€£ 5. Results â€£ How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we notice that, in these terms, the differences are instead significant.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Notably, Hybrid ConfHyena emerges as the most efficient architecture by a large margin, reducing both training time by 27% and inference time by 7% in comparison to the Conformer model.
The greater savings in training time can be attributed to the autoregressive nature of the models.
In fact, while during training the number of forward (and backward) passes on the encoder and the decoder are the same (one per batch), during inference the encoder performs a single forward pass while most of the time is taken up by the multiple forward passes on the autoregressive decoder.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">The training and inference time of ConfHyena is, instead, comparable to that of the Conformer, and much higher than that of Hybrid ConfHyena.
Although this may seem counterintuitive, the explanation is straightforward: the sub-quadratic complexity of Hyena makes it more efficient when sequences are long, while for shorter sequences, such as those obtained after the CTC compression, attention is faster.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">In summary, we can conclude that <span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_bold">Hybrid ConfHyena substantially reduces computational costs compared to Conformer and ConfHyena</span>.
Drawing on this and the comparable output quality of the three models, we can state that <span id="S5.SS2.p4.1.2" class="ltx_text ltx_font_bold">Hybrid ConfHyena offers the most favorable balance between quality and efficiency</span>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.Â Â Â Reducing Downsampling</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The lower memory complexity of Hyena (as discussed in Â§<a href="#S2.SS3" title="2.3. Hyena â€£ 2. Background â€£ How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>) opens up the possibility of reducing the initial downsampling performed by the two initial convolutions in speech encoders without incurring out-of-memory issues.
A potential advantage of this operation would lie in mitigating the information loss caused by the context-uninformed compression of the input speech sequence. which is inherent to downsampling (see Â§<a href="#S1" title="1. Introduction â€£ How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
For this reason, we conclude this work by investigating the potential advantage of halving downsampling from a factor of 4 to a factor of 2.
In practice, this is obtained using stride 1 in the first convolution, while keeping stride 2 in the second.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">The results in Table <a href="#S6.T3" title="Table 3 â€£ 6. Reducing Downsampling â€£ How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> show that there is no advantage in reducing the initial downsampling: in fact, halving it does not increase the translation ability of our models. Rather, this operation inflates both training and inference time by up to 50% and 35%, respectively. In light of this, we believe that future works should pursue other directions (e.g., different hyperparameters) to close the small quality gap without losing the efficiency gains.</p>
</div>
<figure id="S6.T3" class="ltx_table">
<table id="S6.T3.11" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T3.3.3" class="ltx_tr">
<th id="S6.T3.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;" colspan="4">
<span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;">Â </span><span id="S6.T3.3.3.4.1" class="ltx_text" style="font-size:90%;">â€„</span><span id="S6.T3.3.3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Model</span><span id="S6.T3.3.3.4.3" class="ltx_text" style="font-size:90%;"></span>
</th>
<th id="S6.T3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S6.T3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">BLEU<math id="S6.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S6.T3.1.1.1.1.m1.1a"><mo stretchy="false" id="S6.T3.1.1.1.1.m1.1.1" xref="S6.T3.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S6.T3.1.1.1.1.m1.1b"><ci id="S6.T3.1.1.1.1.m1.1.1.cmml" xref="S6.T3.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
<th id="S6.T3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S6.T3.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">train time<math id="S6.T3.2.2.2.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S6.T3.2.2.2.1.m1.1a"><mo stretchy="false" id="S6.T3.2.2.2.1.m1.1.1" xref="S6.T3.2.2.2.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S6.T3.2.2.2.1.m1.1b"><ci id="S6.T3.2.2.2.1.m1.1.1.cmml" xref="S6.T3.2.2.2.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.2.2.2.1.m1.1c">\downarrow</annotation></semantics></math></span></th>
<th id="S6.T3.3.3.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S6.T3.3.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">inf time<math id="S6.T3.3.3.3.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S6.T3.3.3.3.1.m1.1a"><mo stretchy="false" id="S6.T3.3.3.3.1.m1.1.1" xref="S6.T3.3.3.3.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S6.T3.3.3.3.1.m1.1b"><ci id="S6.T3.3.3.3.1.m1.1.1.cmml" xref="S6.T3.3.3.3.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.3.3.3.1.m1.1c">\downarrow</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T3.5.5" class="ltx_tr">
<td id="S6.T3.5.5.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S6.T3.5.5.3.1" class="ltx_text" style="font-size:90%;">ConfHyena</span></td>
<td id="S6.T3.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S6.T3.5.5.4.1" class="ltx_text" style="font-size:90%;">24.90</span></td>
<td id="S6.T3.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;">
<math id="S6.T3.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T3.4.4.1.m1.1a"><mo mathsize="90%" id="S6.T3.4.4.1.m1.1.1" xref="S6.T3.4.4.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S6.T3.4.4.1.m1.1b"><times id="S6.T3.4.4.1.m1.1.1.cmml" xref="S6.T3.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.4.4.1.m1.1c">\times</annotation></semantics></math><span id="S6.T3.4.4.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">1.00</span>
</td>
<td id="S6.T3.5.5.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;">
<math id="S6.T3.5.5.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T3.5.5.2.m1.1a"><mo mathsize="90%" id="S6.T3.5.5.2.m1.1.1" xref="S6.T3.5.5.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S6.T3.5.5.2.m1.1b"><times id="S6.T3.5.5.2.m1.1.1.cmml" xref="S6.T3.5.5.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.5.5.2.m1.1c">\times</annotation></semantics></math><span id="S6.T3.5.5.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">1.00</span>
</td>
</tr>
<tr id="S6.T3.7.7" class="ltx_tr">
<td id="S6.T3.7.7.3" class="ltx_td ltx_align_left ltx_border_rr" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S6.T3.7.7.3.1" class="ltx_text" style="font-size:90%;">+ downsample 2</span></td>
<td id="S6.T3.7.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S6.T3.7.7.4.1" class="ltx_text" style="font-size:90%;">24.79</span></td>
<td id="S6.T3.6.6.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.5pt;padding-right:3.5pt;">
<math id="S6.T3.6.6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T3.6.6.1.m1.1a"><mo mathsize="90%" id="S6.T3.6.6.1.m1.1.1" xref="S6.T3.6.6.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S6.T3.6.6.1.m1.1b"><times id="S6.T3.6.6.1.m1.1.1.cmml" xref="S6.T3.6.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.6.6.1.m1.1c">\times</annotation></semantics></math><span id="S6.T3.6.6.1.1" class="ltx_text" style="font-size:90%;">1.39</span>
</td>
<td id="S6.T3.7.7.2" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">
<math id="S6.T3.7.7.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T3.7.7.2.m1.1a"><mo mathsize="90%" id="S6.T3.7.7.2.m1.1.1" xref="S6.T3.7.7.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S6.T3.7.7.2.m1.1b"><times id="S6.T3.7.7.2.m1.1.1.cmml" xref="S6.T3.7.7.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.7.7.2.m1.1c">\times</annotation></semantics></math><span id="S6.T3.7.7.2.1" class="ltx_text" style="font-size:90%;">1.35</span>
</td>
</tr>
<tr id="S6.T3.9.9" class="ltx_tr">
<td id="S6.T3.9.9.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S6.T3.9.9.3.1" class="ltx_text" style="font-size:90%;">Hybrid ConfHyena</span></td>
<td id="S6.T3.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S6.T3.9.9.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">25.22</span></td>
<td id="S6.T3.8.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">
<math id="S6.T3.8.8.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T3.8.8.1.m1.1a"><mo mathsize="90%" id="S6.T3.8.8.1.m1.1.1" xref="S6.T3.8.8.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S6.T3.8.8.1.m1.1b"><times id="S6.T3.8.8.1.m1.1.1.cmml" xref="S6.T3.8.8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.8.8.1.m1.1c">\times</annotation></semantics></math><span id="S6.T3.8.8.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">1.00</span>
</td>
<td id="S6.T3.9.9.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">
<math id="S6.T3.9.9.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T3.9.9.2.m1.1a"><mo mathsize="90%" id="S6.T3.9.9.2.m1.1.1" xref="S6.T3.9.9.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S6.T3.9.9.2.m1.1b"><times id="S6.T3.9.9.2.m1.1.1.cmml" xref="S6.T3.9.9.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.9.9.2.m1.1c">\times</annotation></semantics></math><span id="S6.T3.9.9.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">1.00</span>
</td>
</tr>
<tr id="S6.T3.11.11" class="ltx_tr">
<td id="S6.T3.11.11.3" class="ltx_td ltx_align_left ltx_border_rr" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S6.T3.11.11.3.1" class="ltx_text" style="font-size:90%;">+ downsample 2</span></td>
<td id="S6.T3.11.11.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S6.T3.11.11.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">25.22</span></td>
<td id="S6.T3.10.10.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.5pt;padding-right:3.5pt;">
<math id="S6.T3.10.10.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T3.10.10.1.m1.1a"><mo mathsize="90%" id="S6.T3.10.10.1.m1.1.1" xref="S6.T3.10.10.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S6.T3.10.10.1.m1.1b"><times id="S6.T3.10.10.1.m1.1.1.cmml" xref="S6.T3.10.10.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.10.10.1.m1.1c">\times</annotation></semantics></math><span id="S6.T3.10.10.1.1" class="ltx_text" style="font-size:90%;">1.50</span>
</td>
<td id="S6.T3.11.11.2" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">
<math id="S6.T3.11.11.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T3.11.11.2.m1.1a"><mo mathsize="90%" id="S6.T3.11.11.2.m1.1.1" xref="S6.T3.11.11.2.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S6.T3.11.11.2.m1.1b"><times id="S6.T3.11.11.2.m1.1.1.cmml" xref="S6.T3.11.11.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.11.11.2.m1.1c">\times</annotation></semantics></math><span id="S6.T3.11.11.2.1" class="ltx_text" style="font-size:90%;">1.13</span>
</td>
</tr>
<tr id="S6.T3.11.12.1" class="ltx_tr">
<td id="S6.T3.11.12.1.1" class="ltx_td ltx_align_left ltx_border_rr" style="padding-left:3.5pt;padding-right:3.5pt;"><span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;">Â </span></td>
<td id="S6.T3.11.12.1.2" class="ltx_td" style="padding-left:3.5pt;padding-right:3.5pt;"></td>
<td id="S6.T3.11.12.1.3" class="ltx_td" style="padding-left:3.5pt;padding-right:3.5pt;"></td>
<td id="S6.T3.11.12.1.4" class="ltx_td" style="padding-left:3.5pt;padding-right:3.5pt;"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>BLEU score and relative training/inference time of ConfHyena and Hybrid ConfHyena with downsample 2 on MuST-C en-de.</figcaption>
</figure>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">7.Â Â Â Conclusions</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In order to reduce the high computational costs of ASR and ST models, in this paper we proposed ConfHyena, a Confomer-based model that replaces self-attentions with non-causal Hyena operators, and a Hybrid version that mixes ConfHyena and Conformer layers. Our experiments on English ASR and 8 ST directions demonstrated that Hybrid ConfHyena has the best quality/efficiency trade-off, as it significantly reduces training time by 27% with a minimal quality degradation of <math id="S7.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S7.p1.1.m1.1a"><mo id="S7.p1.1.m1.1.1" xref="S7.p1.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S7.p1.1.m1.1b"><csymbol cd="latexml" id="S7.p1.1.m1.1.1.cmml" xref="S7.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.1.m1.1c">\sim</annotation></semantics></math>1% compared to the Conformer model.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">Limitations</h2>

<section id="Sx1.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">The <span id="Sx1.SS0.SSS0.Px1.1.1" class="ltx_text ltx_inline-quote ltx_outerquote">â€œgood resultsâ€</span> conundrum.</h4>

<div id="Sx1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="Sx1.SS0.SSS0.Px1.p1.1" class="ltx_p">By proposing and experimenting with Hybrid ConfHyena, we introduced an alternative architecture that improves model efficiency without substantial performance degradation. Admittedly, in our experiments, we recognize that Hybrid ConfHyena exhibits a gap, albeit limited, with respect to the current, state-of-the-art Conformer model. However, we refrain from interpreting our results as inherently <span id="Sx1.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_inline-quote ltx_outerquote">â€œnegativeâ€</span> or from considering a minor performance gap as an invalidating limitation. Indeed, we echo the recent criticism of purely leaderboard-based evaluation of new systems <cite class="ltx_cite ltx_citemacro_citep">(Ethayarajh and Jurafsky, <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite>, advocating for a more comprehensive perspective that considers a wider range of factors, including efficiency, ethics, and environmental sustainability <cite class="ltx_cite ltx_citemacro_citep">(Wynsberghe, <a href="#bib.bib47" title="" class="ltx_ref">2021</a>)</cite>. As such, we believe that achieving higher scores should not be the sole objective of research efforts, especially if it comes at the cost of resource-intensive training procedures <cite class="ltx_cite ltx_citemacro_citep">(Ligozat etÂ al., <a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite> that sacrifice efficiency <cite class="ltx_cite ltx_citemacro_citep">(Peng etÂ al., <a href="#bib.bib33" title="" class="ltx_ref">2023a</a>)</cite>. This approach also aligns with best practices recently adopted by major corporations that prioritize cost-effectiveness, environmental sustainability <cite class="ltx_cite ltx_citemacro_citep">(Rolnick etÂ al., <a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite>, and accessibility in model design <cite class="ltx_cite ltx_citemacro_citep">(deÂ Laat, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite> <span id="Sx1.SS0.SSS0.Px1.p1.1.2" class="ltx_text" style="color:#000000;">to</span> reduce their impact and democratize their use.
Therefore, we underscore that the notable reduction in complexity and, in turn, in training time achieved by Hybrid ConfHyena should definitely be accounted as a positive advancement, regardless of the minimal quality degradation it implies.</p>
</div>
</section>
<section id="Sx1.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Portability and scalability.</h4>

<div id="Sx1.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="Sx1.SS0.SSS0.Px2.p1.1" class="ltx_p">The validity of our findings should be confirmed across a wider range of datasets since our experiments only focused on the MuST-C corpus, which is the typical resource used in recent ST research.
The robustness of our results across different domains (MuST-C is composed of TED talks) and source languages (limited to English) have to be further verified, although there is no reason to believe that other settings may be more or less favorable to our proposed architecture.
Furthermore, we did not scale our experiments to large training data, such as those employed in the training of state-of-the-art models like Whisper <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>, which are <math id="Sx1.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="Sx1.SS0.SSS0.Px2.p1.1.m1.1a"><mo id="Sx1.SS0.SSS0.Px2.p1.1.m1.1.1" xref="Sx1.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="Sx1.SS0.SSS0.Px2.p1.1.m1.1b"><csymbol cd="latexml" id="Sx1.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="Sx1.SS0.SSS0.Px2.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx1.SS0.SSS0.Px2.p1.1.m1.1c">\sim</annotation></semantics></math>100-1,000 times larger than MuST-C.
Unfortunately, conducting such experiments is extremely expensive and demands access to high-performance hardware.
Evaluating the savings achievable by our proposed model in such a scenario is an interesting future step for this research, as well as confirming that the performance gap it suffers from remains limited.</p>
</div>
</section>
<section id="Sx1.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="color:#000000;">Efficient Attention Implementation.</h4>

<div id="Sx1.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="Sx1.SS0.SSS0.Px3.p1.1" class="ltx_p"><span id="Sx1.SS0.SSS0.Px3.p1.1.1" class="ltx_text" style="color:#000000;">In our work we did not use efficient attention implementations, such as Flash Attention <cite class="ltx_cite ltx_citemacro_citep">(Dao etÂ al., <a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite>. While
these engineering optimizations can significantly reduce resource requirements and speed up computation, they are hardware-specific. For instance, the Flash Attention is not supported by the K80 GPUs used in our experimental settings. For this reason, we address the efficiency of the attention mechanism from a theoretical perspective, whose benefits are maintained regardless of the hardware employed.</span></p>
</div>
</section>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">Acknowledgements</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">We acknowledge the support of the PNRR project
FAIR - Future AI Research (PE00000013), under
the NRRP MUR program funded by the NextGenerationEU.</p>
</div>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">Bibliographical References</h2>

<div id="Sx3.p1" class="ltx_para">
<span id="Sx3.p1.1" class="ltx_ERROR undefined">\c@NAT@ctr</span>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography"></h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alastruey etÂ al. (2021)</span>
<span class="ltx_bibblock">
Belen Alastruey, GerardÂ I. GÃ¡llego, and MartaÂ R. Costa-jussÃ . 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2107.03069" title="" class="ltx_ref ltx_href">Efficient transformer for
direct speech translation</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau etÂ al. (2016a)</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, PhilÃ©mon Brakel, and Yoshua
Bengio. 2016a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP.2016.7472618" title="" class="ltx_ref ltx_href">End-to-end
attention-based large vocabulary speech recognition</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">2016 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP)</em>, pages 4945â€“4949.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau etÂ al. (2016b)</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, PhilÃ©mon Brakel, and Yoshua
Bengio. 2016b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP.2016.7472618" title="" class="ltx_ref ltx_href">End-to-end
attention-based large vocabulary speech recognition</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">2016 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP)</em>, pages 4945â€“4949.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BÃ©rard etÂ al. (2018)</span>
<span class="ltx_bibblock">
Alexandre BÃ©rard, Laurent Besacier, AliÂ Can Kocabiyikoglu, and Olivier
Pietquin. 2018.

</span>
<span class="ltx_bibblock">End-to-End Automatic Speech Translation of Audiobooks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of ICASSP 2018 - IEEE International Conference
on Acoustics, Speech and Signal Processing</em>, Calgary, Alberta, Canada.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bracewell and Kahn (1966)</span>
<span class="ltx_bibblock">
Ron Bracewell and PeterÂ B. Kahn. 1966.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1119/1.1973431" title="" class="ltx_ref ltx_href">The Fourier Transform and
Its Applications</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">American Journal of Physics</em>, 34(8):712â€“712.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cattoni etÂ al. (2021)</span>
<span class="ltx_bibblock">
Roldano Cattoni, MattiaÂ Antonino Di Gangi, Luisa Bentivogli, Matteo Negri,
and Marco Turchi. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1016/j.csl.2020.101155" title="" class="ltx_ref ltx_href">Must-c: A multilingual corpus for end-to-end speech translation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Computer Speech &amp; Language</em>, 66:101155.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan etÂ al. (2016)</span>
<span class="ltx_bibblock">
William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP.2016.7472621" title="" class="ltx_ref ltx_href">Listen, attend
and spell: A neural network for large vocabulary conversational speech
recognition</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">2016 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP)</em>, pages 4960â€“4964.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chorowski etÂ al. (2015)</span>
<span class="ltx_bibblock">
JanÂ K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua
Bengio. 2015.

</span>
<span class="ltx_bibblock">Attention-based models for speech recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 28.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai etÂ al. (2019)</span>
<span class="ltx_bibblock">
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan
Salakhutdinov. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P19-1285" title="" class="ltx_ref ltx_href">Transformer-XL:
Attentive language models beyond a fixed-length context</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 2978â€“2988, Florence, Italy.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao etÂ al. (2022)</span>
<span class="ltx_bibblock">
Tri Dao, DanielÂ Y. Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. 2022.

</span>
<span class="ltx_bibblock">FlashAttention: Fast and memory-efficient exact attention with
IO-awareness.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dauphin etÂ al. (2017)</span>
<span class="ltx_bibblock">
YannÂ N. Dauphin, Angela Fan, Michael Auli, and David Grangier. 2017.

</span>
<span class="ltx_bibblock">Language modeling with gated convolutional networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 34th International Conference on Machine
Learning - Volume 70</em>, ICMLâ€™17, page 933â€“941. JMLR.org.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">deÂ Laat (2021)</span>
<span class="ltx_bibblock">
Paul deÂ Laat. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/s13347-021-00474-3" title="" class="ltx_ref ltx_href">Companies
committed to responsible ai: From principles towards implementation and
regulation?</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Philosophy &amp; Technology</em>, 34.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DiÂ Gangi etÂ al. (2019)</span>
<span class="ltx_bibblock">
MattiaÂ A. DiÂ Gangi, Matteo Negri, Roldano Cattoni, Roberto Dessi, and Marco
Turchi. 2019.

</span>
<span class="ltx_bibblock">Enhancing Transformer for End-to-end Speech-to-Text Translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Translation Summit XVII Volume 1:
Research Track</em>, pages 21â€“31, Dublin, Ireland. European Association for
Machine Translation.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ethayarajh and Jurafsky (2020)</span>
<span class="ltx_bibblock">
Kawin Ethayarajh and Dan Jurafsky. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.393" title="" class="ltx_ref ltx_href">Utility is
in the eye of the user: A critique of NLP leaderboards</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 4846â€“4853, Online. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gaido etÂ al. (2021)</span>
<span class="ltx_bibblock">
Marco Gaido, Mauro Cettolo, Matteo Negri, and Marco Turchi. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.eacl-main.57" title="" class="ltx_ref ltx_href">CTC-based
compression for direct speech translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th Conference of the European Chapter
of the Association for Computational Linguistics: Main Volume</em>, pages
690â€“696, Online.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow etÂ al. (2016)</span>
<span class="ltx_bibblock">
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016.

</span>
<span class="ltx_bibblock">Convolutional networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Deep learning</em>, chapterÂ 9, page 330â€“372. MIT Press,
Cambridge, MA, USA.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves etÂ al. (2006)</span>
<span class="ltx_bibblock">
Alex Graves, Santiago FernÃ¡ndez, FaustinoÂ J. Gomez, and JÃ¼rgen
Schmidhuber. 2006.

</span>
<span class="ltx_bibblock">Connectionist Temporal Classification: Labelling Unsegmented
Sequence Data with Recurrent Neural Networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd international conference on Machine
learning (ICML)</em>, pages 369â€“376, Pittsburgh, Pennsylvania.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gulati etÂ al. (2020)</span>
<span class="ltx_bibblock">
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, YuÂ Zhang, Jiahui Yu,
Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.21437/Interspeech.2020-3015" title="" class="ltx_ref ltx_href">Conformer:
Convolution-augmented Transformer for Speech Recognition</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2020</em>, pages 5036â€“5040.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Inaguma etÂ al. (2021)</span>
<span class="ltx_bibblock">
Hirofumi Inaguma, Yosuke Higuchi, Kevin Duh, Tatsuya Kawahara, and Shinji
Watanabe. 2021.

</span>
<span class="ltx_bibblock">Non-autoregressive end-to-end speech translation with parallel
autoregressive rescoring.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.04411</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ioffe and Szegedy (2015)</span>
<span class="ltx_bibblock">
Sergey Ioffe and Christian Szegedy. 2015.

</span>
<span class="ltx_bibblock">Batch normalization: Accelerating deep network training by reducing
internal covariate shift.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd International Conference on
International Conference on Machine Learning - Volume 37</em>, ICMLâ€™15, page
448â€“456. JMLR.org.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn (2004)</span>
<span class="ltx_bibblock">
Philipp Koehn. 2004.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/W04-3250" title="" class="ltx_ref ltx_href">Statistical significance
tests for machine translation evaluation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2004 Conference on Empirical Methods in
Natural Language Processing</em>, pages 388â€“395, Barcelona, Spain. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D18-2012" title="" class="ltx_ref ltx_href">SentencePiece: A
simple and language independent subword tokenizer and detokenizer for neural
text processing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</em>, pages 66â€“71, Brussels,
Belgium.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Latif etÂ al. (2023)</span>
<span class="ltx_bibblock">
Siddique Latif, Aun Zaidi, Heriberto Cuayahuitl, Fahad Shamshad, Moazzam
Shoukat, and Junaid Qadir. 2023.

</span>
<span class="ltx_bibblock">Transformers in speech processing: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.11607</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee-Thorp etÂ al. (2022)</span>
<span class="ltx_bibblock">
James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.naacl-main.319" title="" class="ltx_ref ltx_href">FNet:
Mixing tokens with Fourier transforms</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 4296â€“4313, Seattle, United States. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ligozat etÂ al. (2021)</span>
<span class="ltx_bibblock">
Anne-Laure Ligozat, Julien LefÃ¨vre, AurÃ©lie Bugeau, and Jacques Combaz.
2021.

</span>
<span class="ltx_bibblock">Unraveling the hidden environmental impacts of ai solutions for
environment.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.11822</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2022)</span>
<span class="ltx_bibblock">
Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. 2022.

</span>
<span class="ltx_bibblock">A survey of transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">AI Open</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Yuchen Liu, Junnan Zhu, Jiajun Zhang, and Chengqing Zong. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2010.14920" title="" class="ltx_ref ltx_href">Bridging the modality gap
for speech-to-text translation</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al. (2019)</span>
<span class="ltx_bibblock">
Yiping Lu, Zhuohan Li, DiÂ He, Zhiqing Sun, Bin Dong, Tao Qin, Liwei Wang, and
Tie-Yan Liu. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.1906.02762" title="" class="ltx_ref ltx_href">Understanding and
improving transformer from a multi-particle dynamic system point of view</a>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oppenheim and Schafer (1975)</span>
<span class="ltx_bibblock">
AlanÂ V. Oppenheim and RonaldÂ W. Schafer. 1975.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://books.google.it/books?id=vSzuVLBbp6cC" title="" class="ltx_ref ltx_href"><em id="bib.bib29.1.1.1" class="ltx_emph ltx_font_italic">Digital
Signal Processing</em></a>.

</span>
<span class="ltx_bibblock">Prentice Hall international editions. Prentice-Hall.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oppenheim and Schafer (2009)</span>
<span class="ltx_bibblock">
AlanÂ V. Oppenheim and RonaldÂ W. Schafer. 2009.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Discrete-Time Signal Processing</em>, 3rd edition.

</span>
<span class="ltx_bibblock">Prentice Hall Press, USA.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papi etÂ al. (2021)</span>
<span class="ltx_bibblock">
Sara Papi, Marco Gaido, Matteo Negri, and Marco Turchi. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.emnlp-main.127" title="" class="ltx_ref ltx_href">Speechformer: Reducing information loss in direct speech translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</em>, pages 1698â€“1706, Online and Punta Cana,
Dominican Republic.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papi etÂ al. (2023)</span>
<span class="ltx_bibblock">
Sara Papi, Marco Gaido, Andrea Pilzer, and Matteo Negri. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2303.16166" title="" class="ltx_ref ltx_href">When Good and Reproducible
Results are a Giant with Feet of Clay: The Importance of Software Quality in
NLP</a>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Hao Peng, Qingqing Cao, Jesse Dodge, MatthewÂ E. Peters, Jared Fernandez, Tom
Sherborne, Kyle Lo, Sam Skjonsberg, Emma Strubell, Darrell Plessas,
IzÂ Beltagy, EvanÂ Pete Walsh, NoahÂ A. Smith, and Hannaneh Hajishirzi.
2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2307.09701" title="" class="ltx_ref ltx_href">Efficiency Pentathlon: A
Standardized Arena for Efficiency Evaluation</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li,
Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou Zhang, Yui
Sudo, Muhammad Shakeel, Jee weon Jung, Soumi Maiti, and Shinji Watanabe.
2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2309.13876" title="" class="ltx_ref ltx_href">Reproducing whisper-style
training using an open-source toolkit and publicly available data</a>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poli etÂ al. (2023)</span>
<span class="ltx_bibblock">
Michael Poli, Stefano Massaroli, EricÂ Q. Nguyen, DanielÂ Y. Fu, Tri Dao,
StephenÂ A. Baccus, Yoshua Bengio, Stefano Ermon, and Christopher RÃ©.
2023.

</span>
<span class="ltx_bibblock">Hyena hierarchy: Towards larger convolutional language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.aclweb.org/anthology/W18-6319" title="" class="ltx_ref ltx_href">A Call for
Clarity in Reporting BLEU Scores</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Third Conference on Machine Translation:
Research Papers</em>, pages 186â€“191, Belgium, Brussels.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2023)</span>
<span class="ltx_bibblock">
Alec Radford, JongÂ Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and
Ilya Sutskever. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v202/radford23a.html" title="" class="ltx_ref ltx_href">Robust
Speech Recognition via Large-Scale Weak Supervision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine
Learning</em>, volume 202 of <em id="bib.bib37.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>,
pages 28492â€“28518. PMLR.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramachandran etÂ al. (2017)</span>
<span class="ltx_bibblock">
Prajit Ramachandran, Barret Zoph, and QuocÂ V. Le. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.1710.05941" title="" class="ltx_ref ltx_href">Searching for
activation functions</a>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rolnick etÂ al. (2022)</span>
<span class="ltx_bibblock">
David Rolnick, PriyaÂ L Donti, LynnÂ H Kaack, Kelly Kochanski, Alexandre Lacoste,
Kris Sankaran, AndrewÂ Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques,
Anna Waldman-Brown, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Tackling climate change with machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em>, 55(2):1â€“96.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salesky etÂ al. (2019)</span>
<span class="ltx_bibblock">
Elizabeth Salesky, Matthias Sperber, and AlanÂ W. Black. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P19-1179" title="" class="ltx_ref ltx_href">Exploring
Phoneme-Level Speech Representations for End-to-End Speech Translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 1835â€“1841, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Selesnick and Burrus (1997)</span>
<span class="ltx_bibblock">
IvanÂ W Selesnick and CÂ Sidney Burrus. 1997.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Fast Convolution and Filtering</em>. CRC Press.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Strubell etÂ al. (2019)</span>
<span class="ltx_bibblock">
Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P19-1355" title="" class="ltx_ref ltx_href">Energy and policy
considerations for deep learning in NLP</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 3645â€“3650, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay etÂ al. (2022)</span>
<span class="ltx_bibblock">
YiÂ Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3530811" title="" class="ltx_ref ltx_href">Efficient transformers: A
survey</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">ACM Comput. Surv.</em>, 55(6).

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
AidanÂ N Gomez, ÅÂ ukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="" class="ltx_ref ltx_href">Attention is all you need</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volumeÂ 30. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2020a)</span>
<span class="ltx_bibblock">
Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and
JakobÂ Grue Simonsen. 2020a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=Hke-WTVtwr" title="" class="ltx_ref ltx_href">Encoding word
order in complex embeddings</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2020b)</span>
<span class="ltx_bibblock">
Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, and Juan Pino.
2020b.

</span>
<span class="ltx_bibblock">fairseq s2t: Fast speech-to-text modeling with fairseq.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference of the Asian Chapter of
the Association for Computational Linguistics (AACL): System Demonstrations</em>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wynsberghe (2021)</span>
<span class="ltx_bibblock">
Aimee Wynsberghe. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/s43681-021-00043-6" title="" class="ltx_ref ltx_href">Sustainable ai:
Ai for sustainability and the sustainability of ai</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">AI and Ethics</em>, 1.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2020)</span>
<span class="ltx_bibblock">
Biao Zhang, Ivan Titov, Barry Haddow, and Rico Sennrich. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.findings-emnlp.230" title="" class="ltx_ref ltx_href">Adaptive
feature selection for end-to-end speech translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2020</em>, pages 2533â€“2544, Online. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023)</span>
<span class="ltx_bibblock">
YuÂ Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin
Chen, BoÂ Li, Vera Axelrod, Gary Wang, etÂ al. 2023.

</span>
<span class="ltx_bibblock">Google usm: Scaling automatic speech recognition beyond 100
languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.01037</em>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jinming Zhao, Hao Yang, Gholamreza Haffari, and Ehsan Shareghi. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.findings-emnlp.142" title="" class="ltx_ref ltx_href">RedApt: An adaptor for wav2vec 2 encodingfaster and smaller speech
translation without quality compromise</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2022</em>, pages 1960â€“1967, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2402.13207" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2402.13208" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2402.13208">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.13208" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2402.13209" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 17:56:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
