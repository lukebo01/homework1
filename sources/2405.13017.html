<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.13017] A Systematic Analysis on the Temporal Generalization of Language Models in Social Media</title><meta property="og:description" content="In machine learning, temporal shifts occur when there are differences between training and test splits in terms of time. For streaming data such as news or social media, models are commonly trained on a fixed corpus fr…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Systematic Analysis on the Temporal Generalization of Language Models in Social Media">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Systematic Analysis on the Temporal Generalization of Language Models in Social Media">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.13017">

<!--Generated on Wed Jun  5 16:46:26 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">A Systematic Analysis on the Temporal Generalization 
<br class="ltx_break">of Language Models in Social Media</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Asahi Ushio
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jose Camacho-Collados 
<br class="ltx_break">Cardiff NLP, Cardiff University, UK 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">{ushioa,camachocolladosj}@cardiff.ac.uk 
<br class="ltx_break"></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">In machine learning, temporal shifts occur when there are differences between training and test splits in terms of time. For streaming data such as news or social media, models are commonly trained on a fixed corpus from a certain period of time, and they can become obsolete due to the dynamism and evolving nature of online content. This paper focuses on temporal shifts in social media and, in particular, Twitter. We propose a unified evaluation scheme to assess the performance of language models (LMs) under temporal shift on standard social media tasks. LMs are tested on five diverse social media NLP tasks under different temporal settings, which revealed two important findings: (i) the decrease in performance under temporal shift is consistent across different models for entity-focused tasks such as named entity recognition or disambiguation, and hate speech detection, but not significant in the other tasks analysed (i.e., topic and sentiment classification); and (ii) continuous pre-training on the test period does not improve the temporal adaptability of LMs.</p>
</div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\useunder</span>
<p id="p1.2" class="ltx_p"><span id="p1.2.1" class="ltx_text ltx_ulem_uline"></span><span id="p1.2.2" class="ltx_text ltx_framed ltx_framed_underline"></span>


</p>
</div>
<div id="p2" class="ltx_para ltx_noindent">
<div id="p2.1" class="ltx_block ltx_align_bottom">
<p id="p2.1.1" class="ltx_p"><span id="p2.1.1.1" class="ltx_text ltx_font_bold">A Systematic Analysis on the Temporal Generalization 
<br class="ltx_break">of Language Models in Social Media</span></p>
<br class="ltx_break ltx_centering">
<p id="p2.1.2" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p2.1.2.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p2.1.2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_top">
<span class="ltx_thead">
<span id="p2.1.2.1.1.1.1" class="ltx_tr">
<span id="p2.1.2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="p2.1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">
Asahi Ushio  </span>and<span id="p2.1.2.1.1.1.1.1.2" class="ltx_text ltx_font_bold"> Jose Camacho-Collados</span></span></span>
</span>
<span class="ltx_tbody">
<span id="p2.1.2.1.1.2.1" class="ltx_tr">
<span id="p2.1.2.1.1.2.1.1" class="ltx_td ltx_align_center">Cardiff NLP, Cardiff University, UK</span></span>
<span id="p2.1.2.1.1.3.2" class="ltx_tr">
<span id="p2.1.2.1.1.3.2.1" class="ltx_td ltx_align_center"><span id="p2.1.2.1.1.3.2.1.1" class="ltx_text ltx_font_typewriter">{ushioa,camachocolladosj}@cardiff.ac.uk</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Modern natural language processing (NLP) is centered on language models (LMs) <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib11" title="" class="ltx_ref">2019</a>); Radford et al. (<a href="#bib.bib35" title="" class="ltx_ref">2019</a>); Liu et al. (<a href="#bib.bib26" title="" class="ltx_ref">2019</a>); Min et al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>. The versatility of LMs has enabled many real world applications, including chatbot<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://openai.com/blog/chatgpt" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/blog/chatgpt</a></span></span></span>, text-guided image generation <cite class="ltx_cite ltx_citemacro_cite">Aditya et al. (<a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite>, and text-to-speech <cite class="ltx_cite ltx_citemacro_cite">Paul K. et al. (<a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite>. One of the well-known issues of LMs, however, is that the capabilities of LMs can not be fully analyzed due to their blackbox nature. To overcome such limitations to understand LMs’ true capability, methodologies and datasets to inspect LMs have been proposed in the context of model probing study, which uncovered various features such as syntax <cite class="ltx_cite ltx_citemacro_cite">Hewitt and Manning (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>); Goldberg (<a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite>, factual knowledge <cite class="ltx_cite ltx_citemacro_cite">Petroni et al. (<a href="#bib.bib34" title="" class="ltx_ref">2019</a>); Ushio et al. (<a href="#bib.bib43" title="" class="ltx_ref">2021</a>)</cite>, semantics <cite class="ltx_cite ltx_citemacro_cite">Ettinger (<a href="#bib.bib14" title="" class="ltx_ref">2020</a>); Tenney et al. (<a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>, and emergent ability <cite class="ltx_cite ltx_citemacro_cite">Jason et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Besides such studies of LM probing, there is another line of research that focuses on the adaptability of LMs under settings incurring changing conditions, including <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">temporal shifts</em> <cite class="ltx_cite ltx_citemacro_cite">Lazaridou et al. (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>); Loureiro et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022a</a>)</cite>. In this paper, we refer to temporal shifts when discussing settings in which the time period of the test set is different from that of the training set (with the test set period being generally <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">after</span>, reassembling real-world settings.). These settings have been empirically known to lead a non-trivial decrease in performance on some tasks <cite class="ltx_cite ltx_citemacro_cite">Liska et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>); Jungo et al. (<a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite>. Needless to say, temporal shifts are more important in more dynamic streaming data with frequent meaning changes and evolving entities, such as social media <cite class="ltx_cite ltx_citemacro_cite">Antypas et al. (<a href="#bib.bib7" title="" class="ltx_ref">2022</a>); Ushio et al. (<a href="#bib.bib42" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we focus on temporal shifts on Twitter, one of the major social media platforms, and propose a unified evaluation scheme to assess the adaptability of LMs toward temporal shift on Twitter. In particular, we are interested in answering the following two research questions:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">RQ1.</span> Are temporal shifts in social media detrimental for LM performance in NLP tasks?</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">RQ2.</span> If so, what are the causes of this temporal shift and can it be mitigated (by e.g. using LMs pre-trained on recent data)?</p>
</div>
</li>
</ul>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">For the evaluation, we selected five diverse social media NLP tasks for which there are datasets with temporal information available: hate speech detection, topic classification, sentiment classification, named entity disambiguation (NED), and named entity recognition (NER) ranging over different time periods. The temporal shifts considered are relatively short compared to those studied in other sources of streaming data such as news and scientific papers. We test both LMs specialized on social media and other general-purpose trained on encyclopedic and web-crawled corpus.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our study shows that tasks driven by named entities or events (i.e., hate speech, NED, and NER) present consistent decrease across model under temporal shift settings, while it is less prominent in the other tasks. Crucially, our results show that the decrease caused by temporal shift cannot be mitigated by considering a more recent corpus to the pre-training dataset. Finally, qualitative analysis highlights that the common mistakes made by LMs are indeed instances that require to understand the named entities in the tweet.
All the datasets and the scripts to reproduce our experiments are made publicly available online<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://huggingface.co/datasets/tweettemposhift/tweet_temporal_shift" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/datasets/tweettemposhift/tweet_temporal_shift</a></span></span></span>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">LMs on Social Media.</h5>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Major LMs are commonly pre-trained on encyclopedic and web-crawled corpora <cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a href="#bib.bib24" title="" class="ltx_ref">2020</a>); Raffel et al. (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>); Aakanksha et al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>); Rohan et al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>); Hugo et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib19" title="" class="ltx_ref">a</a>); Tom B. et al. (<a href="#bib.bib41" title="" class="ltx_ref">2020</a>)</cite>, while the adaptation of such LMs to social media has led new LMs pre-trained on corpus curated over social media <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a href="#bib.bib32" title="" class="ltx_ref">2020</a>); Loureiro et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022a</a>); DeLucia et al. (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>); Barbieri et al. (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite>, which present better performance on social media NLP tasks than standard LMs <cite class="ltx_cite ltx_citemacro_cite">Barbieri et al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>); Antypas et al. (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite>. However, such studies on NLP tasks in social media mainly focus on static datasets without temporal shift. A few of them associate timestamps to the dataset and provide basic temporal analysis <cite class="ltx_cite ltx_citemacro_cite">Antypas et al. (<a href="#bib.bib7" title="" class="ltx_ref">2022</a>); Ushio et al. (<a href="#bib.bib42" title="" class="ltx_ref">2022</a>)</cite>, but these are limited to a single task. Finally, related to the temporal aspect of this work, short-term meaning shift has also been studied in the context of social media and LMs <cite class="ltx_cite ltx_citemacro_cite">Loureiro et al. (<a href="#bib.bib28" title="" class="ltx_ref">2022b</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Temporal Generalization.</h5>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Importantly, this work aligns to the research on the temporal or diachronic generalization of LMs. In this area, however, most previous works focus on relatively long term (over 10 years) <cite class="ltx_cite ltx_citemacro_cite">Lazaridou et al. (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite> or formal source of text such as news and scientific papers <cite class="ltx_cite ltx_citemacro_cite">Liska et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>); Jungo et al. (<a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite>. In the context of short-term temporal analysis, there are three studies that are most similar to ours. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib29" title="" class="ltx_ref">Luu et al.</a></cite> (<cite class="ltx_cite ltx_citemacro_citeyear"><a href="#bib.bib29" title="" class="ltx_ref">2022</a></cite>) analyse the temporal performance degradation of LMs in NLP tasks in relatively short time periods. While social media is included as one of the domains, the evaluation is limited to the classification task and to general-domain models. <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib3" title="" class="ltx_ref">Agarwal and Nenkova</a></cite> (<cite class="ltx_cite ltx_citemacro_citeyear"><a href="#bib.bib3" title="" class="ltx_ref">2022</a></cite>) performed a similar general analysis for different tasks, while also analysing the effect of self-labeling as a mitigation to temporal misalignment, which we do not analyse in this work. The main difference between these works in ours is our focus on social media, where we carry out a targeted comprehensive analysis on short-term temporal effects. When it comes to social media, temporal shifts are especially relevant given the real-time nature of the platform and their focus on current events. In the context of Italian Twitter, <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib15" title="" class="ltx_ref">Florio et al.</a></cite> (<cite class="ltx_cite ltx_citemacro_citeyear"><a href="#bib.bib15" title="" class="ltx_ref">2020</a></cite>) analysed the temporal sensitivity of models for hate speech detection, which is one of the tasks included in this paper.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Temporal-aware LMs.</h5>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">To enhance adaptability of LMs for temporal shift, there are a few works that explicitly ingests the temporal information to the model by specific attention mechanism <cite class="ltx_cite ltx_citemacro_cite">Rosin and Radinsky (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite>, augmenting the input with timestamp <cite class="ltx_cite ltx_citemacro_cite">Rosin et al. (<a href="#bib.bib38" title="" class="ltx_ref">2022</a>)</cite>, joint modeling of temporal information <cite class="ltx_cite ltx_citemacro_cite">Dhingra et al. (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>, and self-labeling <cite class="ltx_cite ltx_citemacro_cite">Agarwal and Nenkova (<a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite>. In this paper, we do not include any temporal-aware LMs, because we are interested in analysing the adaptability of plain LMs to temporal shifts.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setting</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we describe our experimental setting to investigate the effect of temporal shifts in LMs.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Evaluation Methodology</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.5" class="ltx_p">Let us define <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{{\rm train}}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">𝒟</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">train</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝒟</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">train</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathcal{D}_{{\rm train}}</annotation></semantics></math> and
<math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{D}_{{\rm test}}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">𝒟</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">test</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝒟</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">test</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mathcal{D}_{{\rm test}}</annotation></semantics></math> as the training and test splits of a dataset <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\mathcal{D}</annotation></semantics></math> for a single downstream task (e.g. sentiment classification), where each dataset contains pairs of a text input and associated labels. Importantly, <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{D}_{{\rm train}}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">𝒟</mi><mi id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">train</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">𝒟</ci><ci id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">train</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">\mathcal{D}_{{\rm train}}</annotation></semantics></math> is taken from the period prior to <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{D}_{{\rm test}}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">𝒟</mi><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">test</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">𝒟</ci><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">test</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\mathcal{D}_{{\rm test}}</annotation></semantics></math> without any temporal overlap. Given such dataset with temporal split, we consider the following two settings of out-of-time (OOT) and in-time (IT).</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Out-of-Time (OOT).</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.2" class="ltx_p">In the first setting, we simply train the models on <math id="S3.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{{\rm train}}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.1.m1.1a"><msub id="S3.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">𝒟</mi><mi id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">train</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2">𝒟</ci><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3">train</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.1.m1.1c">\mathcal{D}_{{\rm train}}</annotation></semantics></math> and evaluate them on <math id="S3.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{D}_{{\rm test}}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.2.m2.1a"><msub id="S3.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml">𝒟</mi><mi id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml">test</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.2">𝒟</ci><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.3">test</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.2.m2.1c">\mathcal{D}_{{\rm test}}</annotation></semantics></math>. Noticeably, models have no access to the instances from the test period at the training phase in this setting, so we refer the setting as <em id="S3.SS1.SSS0.Px1.p1.2.1" class="ltx_emph ltx_font_italic">out-of-time</em> (OOT) as an analogy to the out-of-domain (OOD).</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2405.13017/assets/figures/oot_2.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="479" height="417" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An illustrative example of the conceptual differences between the sampling time periods of the OOT and IT settings.</figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:422.8pt;height:238.9pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-52.9pt,29.8pt) scale(0.8,0.8) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S3.T1.1.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Split</th>
<th id="S3.T1.1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Size</th>
<th id="S3.T1.1.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Date</th>
<th id="S3.T1.1.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.1.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.1.5.1.1" class="ltx_p" style="width:300.0pt;">Examples</span>
</span>
</th>
</tr>
<tr id="S3.T1.1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="3"><span id="S3.T1.1.1.2.2.1.1" class="ltx_text">
<span id="S3.T1.1.1.2.2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:20.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:20.8pt;transform:translate(-7pt,-7pt) rotate(-90deg) ;">
<span id="S3.T1.1.1.2.2.1.1.1.1" class="ltx_p">Hate</span>
</span></span></span></td>
<td id="S3.T1.1.1.2.2.2" class="ltx_td ltx_align_right ltx_border_t">Train</td>
<td id="S3.T1.1.1.2.2.3" class="ltx_td ltx_align_right ltx_border_t">2,318</td>
<td id="S3.T1.1.1.2.2.4" class="ltx_td ltx_align_right ltx_border_t">2013-09-23 / 2015-03-03</td>
<td id="S3.T1.1.1.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.1.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.2.2.5.1.1" class="ltx_p" style="width:300.0pt;"><em id="S3.T1.1.1.2.2.5.1.1.1" class="ltx_emph ltx_font_italic">Zebra undies #MKR chic in pink dress.</em> (Hate)</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.3.3" class="ltx_tr">
<td id="S3.T1.1.1.3.3.1" class="ltx_td ltx_align_right">Valid</td>
<td id="S3.T1.1.1.3.3.2" class="ltx_td ltx_align_right">579</td>
<td id="S3.T1.1.1.3.3.3" class="ltx_td ltx_align_right">2013-09-23 / 2015-03-03</td>
<td id="S3.T1.1.1.3.3.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.1.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.3.3.4.1.1" class="ltx_p" style="width:300.0pt;"><em id="S3.T1.1.1.3.3.4.1.1.1" class="ltx_emph ltx_font_italic">OMG fashion parade time #mkr.</em> (non-Hate)</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.4.4" class="ltx_tr">
<td id="S3.T1.1.1.4.4.1" class="ltx_td ltx_align_right">Test</td>
<td id="S3.T1.1.1.4.4.2" class="ltx_td ltx_align_right">1,475</td>
<td id="S3.T1.1.1.4.4.3" class="ltx_td ltx_align_right">2015-03-04 / 2015-03-14</td>
<td id="S3.T1.1.1.4.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.4.4.4.1.1" class="ltx_p" style="width:300.0pt;"><em id="S3.T1.1.1.4.4.4.1.1.1" class="ltx_emph ltx_font_italic">female football commentators just don’t work.</em> (Hate)</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.5.5" class="ltx_tr">
<td id="S3.T1.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="3"><span id="S3.T1.1.1.5.5.1.1" class="ltx_text">
<span id="S3.T1.1.1.5.5.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:24.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:24.2pt;transform:translate(-7.69pt,-6.72pt) rotate(-90deg) ;">
<span id="S3.T1.1.1.5.5.1.1.1.1" class="ltx_p">Topic</span>
</span></span></span></td>
<td id="S3.T1.1.1.5.5.2" class="ltx_td ltx_align_right ltx_border_t">Train</td>
<td id="S3.T1.1.1.5.5.3" class="ltx_td ltx_align_right ltx_border_t">4,585</td>
<td id="S3.T1.1.1.5.5.4" class="ltx_td ltx_align_right ltx_border_t">2019-09-08 / 2020-08-30</td>
<td id="S3.T1.1.1.5.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.1.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.5.5.5.1.1" class="ltx_p" style="width:300.0pt;"><em id="S3.T1.1.1.5.5.5.1.1.1" class="ltx_emph ltx_font_italic">So, when I can listen to watermelon sugar live in Jakarta Harry?</em></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.6.6" class="ltx_tr">
<td id="S3.T1.1.1.6.6.1" class="ltx_td ltx_align_right">Valid</td>
<td id="S3.T1.1.1.6.6.2" class="ltx_td ltx_align_right">573</td>
<td id="S3.T1.1.1.6.6.3" class="ltx_td ltx_align_right">2019-09-08 / 2020-08-30</td>
<td id="S3.T1.1.1.6.6.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.1.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.6.6.4.1.1" class="ltx_p" style="width:300.0pt;"><em id="S3.T1.1.1.6.6.4.1.1.1" class="ltx_emph ltx_font_italic">@Harry_Styles</em> (celebrity, music)</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.7.7" class="ltx_tr">
<td id="S3.T1.1.1.7.7.1" class="ltx_td ltx_align_right">Test</td>
<td id="S3.T1.1.1.7.7.2" class="ltx_td ltx_align_right">1,679</td>
<td id="S3.T1.1.1.7.7.3" class="ltx_td ltx_align_right">2020-09-06 / 2021-08-29</td>
<td id="S3.T1.1.1.7.7.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.1.7.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.7.7.4.1.1" class="ltx_p" style="width:300.0pt;"><em id="S3.T1.1.1.7.7.4.1.1.1" class="ltx_emph ltx_font_italic">Glad to see the Chiefs crushed the Texans</em> (sports)</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.8.8" class="ltx_tr">
<td id="S3.T1.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="3"><span id="S3.T1.1.1.8.8.1.1" class="ltx_text">
<span id="S3.T1.1.1.8.8.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:21.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:21.9pt;transform:translate(-7.56pt,-7.56pt) rotate(-90deg) ;">
<span id="S3.T1.1.1.8.8.1.1.1.1" class="ltx_p">Sent.</span>
</span></span></span></td>
<td id="S3.T1.1.1.8.8.2" class="ltx_td ltx_align_right ltx_border_t">Train</td>
<td id="S3.T1.1.1.8.8.3" class="ltx_td ltx_align_right ltx_border_t">5,000</td>
<td id="S3.T1.1.1.8.8.4" class="ltx_td ltx_align_right ltx_border_t">2014-02-06 / 2016-12-31</td>
<td id="S3.T1.1.1.8.8.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.1.8.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.8.8.5.1.1" class="ltx_p" style="width:300.0pt;"><em id="S3.T1.1.1.8.8.5.1.1.1" class="ltx_emph ltx_font_italic">I think I’m in love</em> (positive)</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.9.9" class="ltx_tr">
<td id="S3.T1.1.1.9.9.1" class="ltx_td ltx_align_right">Valid</td>
<td id="S3.T1.1.1.9.9.2" class="ltx_td ltx_align_right">1,344</td>
<td id="S3.T1.1.1.9.9.3" class="ltx_td ltx_align_right">2016-01-01 / 2016-12-31</td>
<td id="S3.T1.1.1.9.9.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.1.9.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.9.9.4.1.1" class="ltx_p" style="width:300.0pt;"><em id="S3.T1.1.1.9.9.4.1.1.1" class="ltx_emph ltx_font_italic">@user is making me very upset</em> (negative)</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.10.10" class="ltx_tr">
<td id="S3.T1.1.1.10.10.1" class="ltx_td ltx_align_right">Test</td>
<td id="S3.T1.1.1.10.10.2" class="ltx_td ltx_align_right">1,344</td>
<td id="S3.T1.1.1.10.10.3" class="ltx_td ltx_align_right">2018-01-01 / 2019-01-01</td>
<td id="S3.T1.1.1.10.10.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.1.10.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.10.10.4.1.1" class="ltx_p" style="width:300.0pt;"><em id="S3.T1.1.1.10.10.4.1.1.1" class="ltx_emph ltx_font_italic">Shoutout to @MENTION for donating to poor</em> (positive)</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.11.11" class="ltx_tr">
<td id="S3.T1.1.1.11.11.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="3"><span id="S3.T1.1.1.11.11.1.1" class="ltx_text">
<span id="S3.T1.1.1.11.11.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:21.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:21.9pt;transform:translate(-7.56pt,-7.56pt) rotate(-90deg) ;">
<span id="S3.T1.1.1.11.11.1.1.1.1" class="ltx_p">NED</span>
</span></span></span></td>
<td id="S3.T1.1.1.11.11.2" class="ltx_td ltx_align_right ltx_border_t">Train</td>
<td id="S3.T1.1.1.11.11.3" class="ltx_td ltx_align_right ltx_border_t">18,469</td>
<td id="S3.T1.1.1.11.11.4" class="ltx_td ltx_align_right ltx_border_t">2020-02-26 / 2021-08-27</td>
<td id="S3.T1.1.1.11.11.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.1.11.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.11.11.5.1.1" class="ltx_p" style="width:300.0pt;"><em id="S3.T1.1.1.11.11.5.1.1.1" class="ltx_emph ltx_font_italic">Every concert I’ve seen announce lately, they are steering clear of Detroit</em> (Target: Detroit, Definition: Art museum, Label: False)</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.12.12" class="ltx_tr">
<td id="S3.T1.1.1.12.12.1" class="ltx_td ltx_align_right">Valid</td>
<td id="S3.T1.1.1.12.12.2" class="ltx_td ltx_align_right">4,617</td>
<td id="S3.T1.1.1.12.12.3" class="ltx_td ltx_align_right">2020-02-27 / 2021-08-27</td>
<td id="S3.T1.1.1.12.12.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.1.12.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.12.12.4.1.1" class="ltx_p" style="width:300.0pt;"><em id="S3.T1.1.1.12.12.4.1.1.1" class="ltx_emph ltx_font_italic">Me on stream: Happy Friday!, Australia: It’s Saturday</em></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.13.13" class="ltx_tr">
<td id="S3.T1.1.1.13.13.1" class="ltx_td ltx_align_right">Test</td>
<td id="S3.T1.1.1.13.13.2" class="ltx_td ltx_align_right">21,253</td>
<td id="S3.T1.1.1.13.13.3" class="ltx_td ltx_align_right">2021-08-28 / 2021-11-28</td>
<td id="S3.T1.1.1.13.13.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.1.13.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.13.13.4.1.1" class="ltx_p" style="width:300.0pt;">(Target: Australia, Definition: country, Label: True)</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.14.14" class="ltx_tr">
<td id="S3.T1.1.1.14.14.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="3"><span id="S3.T1.1.1.14.14.1.1" class="ltx_text">
<span id="S3.T1.1.1.14.14.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:21.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:21.7pt;transform:translate(-7.42pt,-7.42pt) rotate(-90deg) ;">
<span id="S3.T1.1.1.14.14.1.1.1.1" class="ltx_p">NER</span>
</span></span></span></td>
<td id="S3.T1.1.1.14.14.2" class="ltx_td ltx_align_right ltx_border_t">Train</td>
<td id="S3.T1.1.1.14.14.3" class="ltx_td ltx_align_right ltx_border_t">4,616</td>
<td id="S3.T1.1.1.14.14.4" class="ltx_td ltx_align_right ltx_border_t">2019-09-08 / 2020-08-30</td>
<td id="S3.T1.1.1.14.14.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.1.14.14.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.14.14.5.1.1" class="ltx_p" style="width:300.0pt;"><em id="S3.T1.1.1.14.14.5.1.1.1" class="ltx_emph ltx_font_italic">UFC 245: Looking at the three title fights on tap at T-Mobile Arena</em></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.15.15" class="ltx_tr">
<td id="S3.T1.1.1.15.15.1" class="ltx_td ltx_align_right">Valid</td>
<td id="S3.T1.1.1.15.15.2" class="ltx_td ltx_align_right">576</td>
<td id="S3.T1.1.1.15.15.3" class="ltx_td ltx_align_right">2019-09-08 / 2020-08-30</td>
<td id="S3.T1.1.1.15.15.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T1.1.1.15.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.15.15.4.1.1" class="ltx_p" style="width:300.0pt;">(UFC 245: corporation, T-Mobile Arena: location)</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.1.16.16" class="ltx_tr">
<td id="S3.T1.1.1.16.16.1" class="ltx_td ltx_align_right ltx_border_bb">Test</td>
<td id="S3.T1.1.1.16.16.2" class="ltx_td ltx_align_right ltx_border_bb">2,807</td>
<td id="S3.T1.1.1.16.16.3" class="ltx_td ltx_align_right ltx_border_bb">2020-09-05 / 2021-08-31</td>
<td id="S3.T1.1.1.16.16.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T1.1.1.16.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.16.16.4.1.1" class="ltx_p" style="width:300.0pt;"><em id="S3.T1.1.1.16.16.4.1.1.1" class="ltx_emph ltx_font_italic">Glad the Chiefs crushed the Texans</em> (Chiefs: group, Texans: group)</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The number of tweets and the period with examples of each dataset.</figcaption>
</figure>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">In-Time (IT).</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.9" class="ltx_p">As a comparison to OOT, we consider the second experimental setting, which is designed to assess the effect of training instances from the test period.
The test set is randomly split into non-overlapped four chunks (<math id="S3.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{{\rm test}}=\bigcup_{i=1}^{4}\mathcal{D}_{{\rm test}}^{i}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.1.m1.1a"><mrow id="S3.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml"><msub id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2.2" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2.2.cmml">𝒟</mi><mi id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2.3" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2.3.cmml">test</mi></msub><mo rspace="0.111em" id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml"><msubsup id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.cmml"><mo id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.2" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.2.cmml">⋃</mo><mrow id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.3" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.3.cmml"><mi id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.3.2" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.3.2.cmml">i</mi><mo id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.3.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.3.1.cmml">=</mo><mn id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.3.3" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.3.3.cmml">1</mn></mrow><mn id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.3" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.3.cmml">4</mn></msubsup><msubsup id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.2.2" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.2.2.cmml">𝒟</mi><mi id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.2.3" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.2.3.cmml">test</mi><mi id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.3" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.3.cmml">i</mi></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1"><eq id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1"></eq><apply id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2.2">𝒟</ci><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2.3">test</ci></apply><apply id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3"><apply id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1">subscript</csymbol><union id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.2"></union><apply id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.3"><eq id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.3.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.3.1"></eq><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.3.2.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.3.2">𝑖</ci><cn type="integer" id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.3.3.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.2.3.3">1</cn></apply></apply><cn type="integer" id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.3">4</cn></apply><apply id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.2.2">𝒟</ci><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.2.3">test</ci></apply><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.1.m1.1c">\mathcal{D}_{{\rm test}}=\bigcup_{i=1}^{4}\mathcal{D}_{{\rm test}}^{i}</annotation></semantics></math>) for cross validation, where models trained on <math id="S3.SS1.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{D}_{{\rm train}}\bigcup\mathcal{D}_{{\rm test}}\backslash\mathcal{D}_{{\rm test}}^{i}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.2.m2.1a"><mrow id="S3.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.cmml"><msub id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2.2" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.cmml">𝒟</mi><mi id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2.3" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.cmml">train</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml">​</mo><mrow id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml"><mo id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.1" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.1.cmml">⋃</mo><mrow id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.cmml"><msub id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.2" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.2.2" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.2.2.cmml">𝒟</mi><mi id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.2.3" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.2.3.cmml">test</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.1" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.1.cmml">\</mo><msubsup id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3.2.2" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3.2.2.cmml">𝒟</mi><mi id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3.2.3" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3.2.3.cmml">test</mi><mi id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3.3" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3.3.cmml">i</mi></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.2.m2.1b"><apply id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1"><times id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1"></times><apply id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2.2">𝒟</ci><ci id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2.3">train</ci></apply><apply id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3"><union id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.1"></union><apply id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2"><ci id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.1">\</ci><apply id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.2.2">𝒟</ci><ci id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.2.3">test</ci></apply><apply id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3.2.2">𝒟</ci><ci id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3.2.3">test</ci></apply><ci id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3.3.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.2.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.2.m2.1c">\mathcal{D}_{{\rm train}}\bigcup\mathcal{D}_{{\rm test}}\backslash\mathcal{D}_{{\rm test}}^{i}</annotation></semantics></math> are evaluated on <math id="S3.SS1.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{D}_{{\rm test}}^{i}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.3.m3.1a"><msubsup id="S3.SS1.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.2" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.2.cmml">𝒟</mi><mi id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.3" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.3.cmml">test</mi><mi id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.3.m3.1b"><apply id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.2">𝒟</ci><ci id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.3">test</ci></apply><ci id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.3.m3.1c">\mathcal{D}_{{\rm test}}^{i}</annotation></semantics></math>.
For each chunk of the test set, we downsample the IT training set to the same size as <math id="S3.SS1.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{D}_{{\rm train}}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.4.m4.1a"><msub id="S3.SS1.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2.cmml">𝒟</mi><mi id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.cmml">train</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.4.m4.1b"><apply id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2">𝒟</ci><ci id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3">train</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.4.m4.1c">\mathcal{D}_{{\rm train}}</annotation></semantics></math> with three random seeds and report the averaged metrics over the runs. To be precise, we consider a function <math id="S3.SS1.SSS0.Px2.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{F}_{s}(\mathcal{D})" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.5.m5.1a"><mrow id="S3.SS1.SSS0.Px2.p1.5.m5.1.2" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.2.cmml"><msub id="S3.SS1.SSS0.Px2.p1.5.m5.1.2.2" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px2.p1.5.m5.1.2.2.2" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.2.2.2.cmml">ℱ</mi><mi id="S3.SS1.SSS0.Px2.p1.5.m5.1.2.2.3" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.2.2.3.cmml">s</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px2.p1.5.m5.1.2.1" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.2.1.cmml">​</mo><mrow id="S3.SS1.SSS0.Px2.p1.5.m5.1.2.3.2" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px2.p1.5.m5.1.2.3.2.1" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.2.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.cmml">𝒟</mi><mo stretchy="false" id="S3.SS1.SSS0.Px2.p1.5.m5.1.2.3.2.2" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.5.m5.1b"><apply id="S3.SS1.SSS0.Px2.p1.5.m5.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.2"><times id="S3.SS1.SSS0.Px2.p1.5.m5.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.2.1"></times><apply id="S3.SS1.SSS0.Px2.p1.5.m5.1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.5.m5.1.2.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.5.m5.1.2.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.2.2.2">ℱ</ci><ci id="S3.SS1.SSS0.Px2.p1.5.m5.1.2.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.2.2.3">𝑠</ci></apply><ci id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1">𝒟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.5.m5.1c">\mathcal{F}_{s}(\mathcal{D})</annotation></semantics></math> that randomly samples <math id="S3.SS1.SSS0.Px2.p1.6.m6.1" class="ltx_Math" alttext="|\mathcal{D}_{\rm train}|" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.6.m6.1a"><mrow id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1.2" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1.2.1.cmml">|</mo><msub id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1.1" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1.1.2" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1.1.2.cmml">𝒟</mi><mi id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1.1.3" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1.1.3.cmml">train</mi></msub><mo stretchy="false" id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1.3" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.6.m6.1b"><apply id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1"><abs id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1.2"></abs><apply id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1.1.2">𝒟</ci><ci id="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.6.m6.1.1.1.1.3">train</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.6.m6.1c">|\mathcal{D}_{\rm train}|</annotation></semantics></math> instances from <math id="S3.SS1.SSS0.Px2.p1.7.m7.1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.7.m7.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px2.p1.7.m7.1.1" xref="S3.SS1.SSS0.Px2.p1.7.m7.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.7.m7.1b"><ci id="S3.SS1.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.7.m7.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.7.m7.1c">\mathcal{D}</annotation></semantics></math>, and we independently train models on <math id="S3.SS1.SSS0.Px2.p1.8.m8.1" class="ltx_Math" alttext="\mathcal{F}_{s}(\mathcal{D}_{{\rm train}}\bigcup\mathcal{D}_{{\rm test}}\backslash\mathcal{D}_{{\rm test}}^{i})" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.8.m8.1a"><mrow id="S3.SS1.SSS0.Px2.p1.8.m8.1.1" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.cmml"><msub id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.3" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.3.2" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.3.2.cmml">ℱ</mi><mi id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.3.3" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.3.3.cmml">s</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.2" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.2.cmml">​</mo><mrow id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.2" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.cmml"><msub id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.2" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.2.2" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.2.2.cmml">𝒟</mi><mi id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.2.3" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.2.3.cmml">train</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.1" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.1.cmml">​</mo><mrow id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.cmml"><mo id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.1" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.1.cmml">⋃</mo><mrow id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.cmml"><msub id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.2" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.2.2" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.2.2.cmml">𝒟</mi><mi id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.2.3" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.2.3.cmml">test</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.1" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.1.cmml">\</mo><msubsup id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3.2.2" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3.2.2.cmml">𝒟</mi><mi id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3.2.3" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3.2.3.cmml">test</mi><mi id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3.3" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3.3.cmml">i</mi></msubsup></mrow></mrow></mrow><mo stretchy="false" id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.3" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.8.m8.1b"><apply id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1"><times id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.2"></times><apply id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.3.2">ℱ</ci><ci id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.3.3">𝑠</ci></apply><apply id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1"><times id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.1"></times><apply id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.2.2">𝒟</ci><ci id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.2.3">train</ci></apply><apply id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3"><union id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.1"></union><apply id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2"><ci id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.1">\</ci><apply id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.2.2">𝒟</ci><ci id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.2.3">test</ci></apply><apply id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3.1.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3.2.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3.2.2">𝒟</ci><ci id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3.2.3">test</ci></apply><ci id="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3.3.cmml" xref="S3.SS1.SSS0.Px2.p1.8.m8.1.1.1.1.1.3.2.3.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.8.m8.1c">\mathcal{F}_{s}(\mathcal{D}_{{\rm train}}\bigcup\mathcal{D}_{{\rm test}}\backslash\mathcal{D}_{{\rm test}}^{i})</annotation></semantics></math> for
<math id="S3.SS1.SSS0.Px2.p1.9.m9.3" class="ltx_Math" alttext="s=0,1,2" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.9.m9.3a"><mrow id="S3.SS1.SSS0.Px2.p1.9.m9.3.4" xref="S3.SS1.SSS0.Px2.p1.9.m9.3.4.cmml"><mi id="S3.SS1.SSS0.Px2.p1.9.m9.3.4.2" xref="S3.SS1.SSS0.Px2.p1.9.m9.3.4.2.cmml">s</mi><mo id="S3.SS1.SSS0.Px2.p1.9.m9.3.4.1" xref="S3.SS1.SSS0.Px2.p1.9.m9.3.4.1.cmml">=</mo><mrow id="S3.SS1.SSS0.Px2.p1.9.m9.3.4.3.2" xref="S3.SS1.SSS0.Px2.p1.9.m9.3.4.3.1.cmml"><mn id="S3.SS1.SSS0.Px2.p1.9.m9.1.1" xref="S3.SS1.SSS0.Px2.p1.9.m9.1.1.cmml">0</mn><mo id="S3.SS1.SSS0.Px2.p1.9.m9.3.4.3.2.1" xref="S3.SS1.SSS0.Px2.p1.9.m9.3.4.3.1.cmml">,</mo><mn id="S3.SS1.SSS0.Px2.p1.9.m9.2.2" xref="S3.SS1.SSS0.Px2.p1.9.m9.2.2.cmml">1</mn><mo id="S3.SS1.SSS0.Px2.p1.9.m9.3.4.3.2.2" xref="S3.SS1.SSS0.Px2.p1.9.m9.3.4.3.1.cmml">,</mo><mn id="S3.SS1.SSS0.Px2.p1.9.m9.3.3" xref="S3.SS1.SSS0.Px2.p1.9.m9.3.3.cmml">2</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.9.m9.3b"><apply id="S3.SS1.SSS0.Px2.p1.9.m9.3.4.cmml" xref="S3.SS1.SSS0.Px2.p1.9.m9.3.4"><eq id="S3.SS1.SSS0.Px2.p1.9.m9.3.4.1.cmml" xref="S3.SS1.SSS0.Px2.p1.9.m9.3.4.1"></eq><ci id="S3.SS1.SSS0.Px2.p1.9.m9.3.4.2.cmml" xref="S3.SS1.SSS0.Px2.p1.9.m9.3.4.2">𝑠</ci><list id="S3.SS1.SSS0.Px2.p1.9.m9.3.4.3.1.cmml" xref="S3.SS1.SSS0.Px2.p1.9.m9.3.4.3.2"><cn type="integer" id="S3.SS1.SSS0.Px2.p1.9.m9.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.9.m9.1.1">0</cn><cn type="integer" id="S3.SS1.SSS0.Px2.p1.9.m9.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.9.m9.2.2">1</cn><cn type="integer" id="S3.SS1.SSS0.Px2.p1.9.m9.3.3.cmml" xref="S3.SS1.SSS0.Px2.p1.9.m9.3.3">2</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.9.m9.3c">s=0,1,2</annotation></semantics></math>. In contrast to OOT, we refer this setting as <em id="S3.SS1.SSS0.Px2.p1.9.1" class="ltx_emph ltx_font_italic">in-time</em> (IT) setting.</p>
</div>
<div id="S3.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p2.1" class="ltx_p">Figure <a href="#S3.F1" title="Figure 1 ‣ Out-of-Time (OOT). ‣ 3.1 Evaluation Methodology ‣ 3 Experimental Setting ‣ A Systematic Analysis on the Temporal Generalization of Language Models in Social Media" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents an example overview of the differences between IT and OOT settings from the perspective of data sampling periods (data from 2018 to 2022 in the example).</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Tasks &amp; Datasets</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We consider the following five diverse social media NLP tasks: hate speech detection, topic classification, sentiment classification, named entity disambiguation (NED), and named entity recognition (NER). For each task, we employ a public dataset for English and leverage its original temporal splits, unless there is overlap between the periods of training and test sets.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Hate Speech Detection.</h5>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">Hate speech detection in Twitter consists of identifying whether a tweet contains hateful content. We use the dataset proposed by Waseem and Hovy <cite class="ltx_cite ltx_citemacro_cite">Waseem and Hovy (<a href="#bib.bib44" title="" class="ltx_ref">2016</a>)</cite> framed as binary classification as the dataset to create the training and test splits based on the timestamp. The first half is used for training and the rest for test split. The training split is further randomly split into 2:8 for validation:training. We use accuracy to evaluate the hate speech detection models.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Topic Classification.</h5>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">Topic classification is a task that consists of associating an input text with one or more labels from a fixed label set. For this evaluation, we rely on TweetTopic <cite class="ltx_cite ltx_citemacro_cite">Antypas et al. (<a href="#bib.bib7" title="" class="ltx_ref">2022</a>)</cite>, a multi-label topic classification dataset with 19 topics such as <span id="S3.SS2.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">sports</span> or <span id="S3.SS2.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">music</span>. As evaluation metric, we use micro-F1 score to measure the performance of topic classification models.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Sentiment Analysis.</h5>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.1" class="ltx_p">Sentiment analysis is a standard social media task consisting of associating each post with its sentiment. In particular, we use the dataset from the task 2: LongEval-Classification from CLEF-2023 <cite class="ltx_cite ltx_citemacro_cite">Alkhalifa et al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite> in which the task is framed as binary classification with positive and negative labels. The original training split contains around 50k instances while 1k test split, which is highly imbalance and the effect of the IT sample can be very limited. To balance the training and test splits, we randomly sample 2.5k instances from each label, amounting 5k new training instance. We use accuracy to evaluate the sentiment classification models.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Named Entity Disambiguation (NED).</h5>

<div id="S3.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px4.p1.1" class="ltx_p">NED is a a binary classification that consists of identifying if the meaning of a given target entity in context is the same as the one provided. We use the TweetNERD <cite class="ltx_cite ltx_citemacro_cite">Mishra et al. (<a href="#bib.bib31" title="" class="ltx_ref">2022</a>)</cite>
dataset and reformulated into NED following SuperTweetEval  <cite class="ltx_cite ltx_citemacro_cite">Dimosthenis et al. (<a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>. Then, we create the train, validation, and test splits in the same way as the hate speech detection. We use accuracy to evaluate the NED models.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Named Entity Recognition (NER).</h5>

<div id="S3.SS2.SSS0.Px5.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px5.p1.1" class="ltx_p">NER is a sequence labelling task to predict a single named-entity type on each token on the input text. We rely on TweetNER7 <cite class="ltx_cite ltx_citemacro_cite">Ushio et al. (<a href="#bib.bib42" title="" class="ltx_ref">2022</a>)</cite>, a NER dataset on Twitter that contains seven named entity types. We use span F1 score to evaluate NER models.</p>
</div>
</section>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Data Statistics</h4>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2405.13017/assets/figures/data_time_period.no_emoji.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="290" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Quarterly breakdown of the number of tweets ratio (%) in each dataset. For example, a ratio of 5% in 13-Q3 for Dataset X would mean that 5% of all tweets in Dataset X belong to the third quarter (July-September) of 2013.</figcaption>
</figure>
<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ Out-of-Time (OOT). ‣ 3.1 Evaluation Methodology ‣ 3 Experimental Setting ‣ A Systematic Analysis on the Temporal Generalization of Language Models in Social Media" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the size and the period of the training and the test sets for each dataset, and Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2.1 Data Statistics ‣ 3.2 Tasks &amp; Datasets ‣ 3 Experimental Setting ‣ A Systematic Analysis on the Temporal Generalization of Language Models in Social Media" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> displays the number of tweets per quarter for each task.
Topic classification and NER use the same tweets, which are sampled uniformly from each month, while NED and hate speech detection have the majority of the tweets in the latest quarter. Sentiment analysis covers the longest period in the dataset that spans over four years.
Figures <a href="#S3.F3" title="Figure 3 ‣ 3.2.1 Data Statistics ‣ 3.2 Tasks &amp; Datasets ‣ 3 Experimental Setting ‣ A Systematic Analysis on the Temporal Generalization of Language Models in Social Media" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S3.F4" title="Figure 4 ‣ 3.2.1 Data Statistics ‣ 3.2 Tasks &amp; Datasets ‣ 3 Experimental Setting ‣ A Systematic Analysis on the Temporal Generalization of Language Models in Social Media" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> show the comparisons of the label distribution of the binary (i.e., hate speech, sentiment classification, and NED) and multi-classification tasks (i.e., NER and topic classification), respectively. As can be observed, hate speech detection has fewer positive labels in OOT than in IT, while the other two tasks have the same ratio of the positive labels between OOT and IT. The same pattern can be observed for topic classification and NED, for which the label distribution does not substantially change.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2405.13017/assets/figures/label_dist.binary.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="392" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Comparisons of ratio (%) of positive labels in the training split of each task between OOT and IT.</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.13017/assets/figures/label_dist.ner.png" id="S3.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="392" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Ratio of entities in NER.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.13017/assets/figures/label_dist.topic.png" id="S3.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Ratio of labels in topic classification.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparisons of label distributions between OOT and IT settings.</figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<div id="S3.T2.1" class="ltx_inline-block ltx_transformed_outer" style="width:537.4pt;height:159.2pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-67.2pt,19.8pt) scale(0.8,0.8) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Model</td>
<td id="S3.T2.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_tt">Parameters</td>
<td id="S3.T2.1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_tt">HF Name</td>
<td id="S3.T2.1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_tt">Citation</td>
</tr>
<tr id="S3.T2.1.1.2.2" class="ltx_tr">
<td id="S3.T2.1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">RoBERTa<sub id="S3.T2.1.1.2.2.1.1" class="ltx_sub">BASE</sub>
</td>
<td id="S3.T2.1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">123M</td>
<td id="S3.T2.1.1.2.2.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.1.2.2.3.1" class="ltx_text ltx_font_typewriter">roberta-base</span></td>
<td id="S3.T2.1.1.2.2.4" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="S3.T2.1.1.2.2.4.1" class="ltx_text"><cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite></span></td>
</tr>
<tr id="S3.T2.1.1.3.3" class="ltx_tr">
<td id="S3.T2.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_t">RoBERTa<sub id="S3.T2.1.1.3.3.1.1" class="ltx_sub">LARGE</sub>
</td>
<td id="S3.T2.1.1.3.3.2" class="ltx_td ltx_align_left ltx_border_t">354M</td>
<td id="S3.T2.1.1.3.3.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.1.3.3.3.1" class="ltx_text ltx_font_typewriter">roberta-large</span></td>
</tr>
<tr id="S3.T2.1.1.4.4" class="ltx_tr">
<td id="S3.T2.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_t">BERTweet<sub id="S3.T2.1.1.4.4.1.1" class="ltx_sub">BASE</sub>
</td>
<td id="S3.T2.1.1.4.4.2" class="ltx_td ltx_align_left ltx_border_t">123M</td>
<td id="S3.T2.1.1.4.4.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.1.4.4.3.1" class="ltx_text ltx_font_typewriter">vinai/bertweet-base</span></td>
<td id="S3.T2.1.1.4.4.4" class="ltx_td ltx_align_left ltx_border_t" rowspan="2"><span id="S3.T2.1.1.4.4.4.1" class="ltx_text"><cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite></span></td>
</tr>
<tr id="S3.T2.1.1.5.5" class="ltx_tr">
<td id="S3.T2.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_t">BERTweet<sub id="S3.T2.1.1.5.5.1.1" class="ltx_sub">LARGE</sub>
</td>
<td id="S3.T2.1.1.5.5.2" class="ltx_td ltx_align_left ltx_border_t">354M</td>
<td id="S3.T2.1.1.5.5.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.1.5.5.3.1" class="ltx_text ltx_font_typewriter">vinai/bertweet-large</span></td>
</tr>
<tr id="S3.T2.1.1.6.6" class="ltx_tr">
<td id="S3.T2.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_t">TimeLM2019<sub id="S3.T2.1.1.6.6.1.1" class="ltx_sub">BASE</sub>
</td>
<td id="S3.T2.1.1.6.6.2" class="ltx_td ltx_align_left ltx_border_t">123M</td>
<td id="S3.T2.1.1.6.6.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.1.6.6.3.1" class="ltx_text ltx_font_typewriter">cardiffnlp/twitter-roberta-base-2019-90m</span></td>
<td id="S3.T2.1.1.6.6.4" class="ltx_td ltx_align_left ltx_border_t" rowspan="5"><span id="S3.T2.1.1.6.6.4.1" class="ltx_text"><cite class="ltx_cite ltx_citemacro_cite">Loureiro et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022a</a>)</cite></span></td>
</tr>
<tr id="S3.T2.1.1.7.7" class="ltx_tr">
<td id="S3.T2.1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_t">TimeLM2020<sub id="S3.T2.1.1.7.7.1.1" class="ltx_sub">BASE</sub>
</td>
<td id="S3.T2.1.1.7.7.2" class="ltx_td ltx_align_left ltx_border_t">123M</td>
<td id="S3.T2.1.1.7.7.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.1.7.7.3.1" class="ltx_text ltx_font_typewriter">cardiffnlp/twitter-roberta-base-dec2020</span></td>
</tr>
<tr id="S3.T2.1.1.8.8" class="ltx_tr">
<td id="S3.T2.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_t">TimeLM2021<sub id="S3.T2.1.1.8.8.1.1" class="ltx_sub">BASE</sub>
</td>
<td id="S3.T2.1.1.8.8.2" class="ltx_td ltx_align_left ltx_border_t">123M</td>
<td id="S3.T2.1.1.8.8.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.1.8.8.3.1" class="ltx_text ltx_font_typewriter">cardiffnlp/twitter-roberta-base-2021-124m</span></td>
</tr>
<tr id="S3.T2.1.1.9.9" class="ltx_tr">
<td id="S3.T2.1.1.9.9.1" class="ltx_td ltx_align_left ltx_border_t">TimeLM2022<sub id="S3.T2.1.1.9.9.1.1" class="ltx_sub">BASE</sub>
</td>
<td id="S3.T2.1.1.9.9.2" class="ltx_td ltx_align_left ltx_border_t">354M</td>
<td id="S3.T2.1.1.9.9.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.1.9.9.3.1" class="ltx_text ltx_font_typewriter">cardiffnlp/twitter-roberta-base-2022-154m</span></td>
</tr>
<tr id="S3.T2.1.1.10.10" class="ltx_tr">
<td id="S3.T2.1.1.10.10.1" class="ltx_td ltx_align_left ltx_border_t">TimeLM2022<sub id="S3.T2.1.1.10.10.1.1" class="ltx_sub">LARGE</sub>
</td>
<td id="S3.T2.1.1.10.10.2" class="ltx_td ltx_align_left ltx_border_t">354M</td>
<td id="S3.T2.1.1.10.10.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.1.10.10.3.1" class="ltx_text ltx_font_typewriter">cardiffnlp/twitter-roberta-large-2022-154m</span></td>
</tr>
<tr id="S3.T2.1.1.11.11" class="ltx_tr">
<td id="S3.T2.1.1.11.11.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">BERNICE</td>
<td id="S3.T2.1.1.11.11.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">278M</td>
<td id="S3.T2.1.1.11.11.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="S3.T2.1.1.11.11.3.1" class="ltx_text ltx_font_typewriter">jhu-clsp/bernice</span></td>
<td id="S3.T2.1.1.11.11.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">DeLucia et al. (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Language models used in the paper with the number of parameters and model aliases on Hugging Face.</figcaption>
</figure>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Models</h3>

<figure id="S3.T3" class="ltx_table">
<div id="S3.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:228.0pt;height:129.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-12.7pt,7.2pt) scale(0.9,0.9) ;">
<table id="S3.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.1.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S3.T3.1.1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt">Hate</th>
<th id="S3.T3.1.1.1.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt">Topic</th>
<th id="S3.T3.1.1.1.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt">Sentiment</th>
<th id="S3.T3.1.1.1.1.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt">NED</th>
<th id="S3.T3.1.1.1.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">NER</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.1.1.2.1" class="ltx_tr">
<th id="S3.T3.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">RoBERTa</th>
<td id="S3.T3.1.1.2.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">✓</td>
<td id="S3.T3.1.1.2.1.3" class="ltx_td ltx_nopad_l ltx_border_t"></td>
<td id="S3.T3.1.1.2.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">✓</td>
<td id="S3.T3.1.1.2.1.5" class="ltx_td ltx_nopad_l ltx_border_t"></td>
<td id="S3.T3.1.1.2.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t"></td>
</tr>
<tr id="S3.T3.1.1.3.2" class="ltx_tr">
<th id="S3.T3.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BERTweet</th>
<td id="S3.T3.1.1.3.2.2" class="ltx_td ltx_nopad_l ltx_align_center">✓</td>
<td id="S3.T3.1.1.3.2.3" class="ltx_td ltx_nopad_l"></td>
<td id="S3.T3.1.1.3.2.4" class="ltx_td ltx_nopad_l ltx_align_center">✓</td>
<td id="S3.T3.1.1.3.2.5" class="ltx_td ltx_nopad_l"></td>
<td id="S3.T3.1.1.3.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r"></td>
</tr>
<tr id="S3.T3.1.1.4.3" class="ltx_tr">
<th id="S3.T3.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BERNICE</th>
<td id="S3.T3.1.1.4.3.2" class="ltx_td ltx_nopad_l ltx_align_center">✓</td>
<td id="S3.T3.1.1.4.3.3" class="ltx_td ltx_nopad_l ltx_align_center">✓</td>
<td id="S3.T3.1.1.4.3.4" class="ltx_td ltx_nopad_l ltx_align_center">✓</td>
<td id="S3.T3.1.1.4.3.5" class="ltx_td ltx_nopad_l ltx_align_center">✓</td>
<td id="S3.T3.1.1.4.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">✓</td>
</tr>
<tr id="S3.T3.1.1.5.4" class="ltx_tr">
<th id="S3.T3.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TimeLM2019</th>
<td id="S3.T3.1.1.5.4.2" class="ltx_td ltx_nopad_l ltx_align_center">✓</td>
<td id="S3.T3.1.1.5.4.3" class="ltx_td ltx_nopad_l"></td>
<td id="S3.T3.1.1.5.4.4" class="ltx_td ltx_nopad_l ltx_align_center">✓</td>
<td id="S3.T3.1.1.5.4.5" class="ltx_td ltx_nopad_l"></td>
<td id="S3.T3.1.1.5.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r"></td>
</tr>
<tr id="S3.T3.1.1.6.5" class="ltx_tr">
<th id="S3.T3.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TimeLM2020</th>
<td id="S3.T3.1.1.6.5.2" class="ltx_td ltx_nopad_l ltx_align_center">✓</td>
<td id="S3.T3.1.1.6.5.3" class="ltx_td ltx_nopad_l"></td>
<td id="S3.T3.1.1.6.5.4" class="ltx_td ltx_nopad_l ltx_align_center">✓</td>
<td id="S3.T3.1.1.6.5.5" class="ltx_td ltx_nopad_l"></td>
<td id="S3.T3.1.1.6.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r"></td>
</tr>
<tr id="S3.T3.1.1.7.6" class="ltx_tr">
<th id="S3.T3.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TimeLM2021</th>
<td id="S3.T3.1.1.7.6.2" class="ltx_td ltx_nopad_l ltx_align_center">✓</td>
<td id="S3.T3.1.1.7.6.3" class="ltx_td ltx_nopad_l ltx_align_center">✓</td>
<td id="S3.T3.1.1.7.6.4" class="ltx_td ltx_nopad_l ltx_align_center">✓</td>
<td id="S3.T3.1.1.7.6.5" class="ltx_td ltx_nopad_l ltx_align_center">✓</td>
<td id="S3.T3.1.1.7.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center">✓</td>
</tr>
<tr id="S3.T3.1.1.8.7" class="ltx_tr">
<th id="S3.T3.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">TimeLM2022</th>
<td id="S3.T3.1.1.8.7.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">✓</td>
<td id="S3.T3.1.1.8.7.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">✓</td>
<td id="S3.T3.1.1.8.7.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">✓</td>
<td id="S3.T3.1.1.8.7.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">✓</td>
<td id="S3.T3.1.1.8.7.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb">✓</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The overlap between the test period and the pre-trained corpus of each LM (✓indicates that the LM is pre-trained on the corpus including the test period of the task).</figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We investigate an established general-purpose LM, RoBERTa <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite> as well as other LMs pre-trained on tweets including BERTweet <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite>, TimeLM <cite class="ltx_cite ltx_citemacro_cite">Loureiro et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022a</a>)</cite>, and BERNICE <cite class="ltx_cite ltx_citemacro_cite">DeLucia et al. (<a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite>. For RoBERTa and BERTweet, we consider the base and the large models, referred as RoBERTa (B), RoBERTa (L), BERTweet (B), and BERTweet (L). For TimeLM, we consider the base models trained on the tweets up to 2019, 2020, 2021 and 2022, referred as TimeLM2019 (B), TimeLM2020 (B), TimeLM2021 (B) and TimeLM2022 (B), and the large model trained upto 2022, referred as TimeLM2022 (L). The end date of the pre-trained corpus for each model is 2019-02 (RoBERTa), 2019-08 (BERTweet), 2019-12 (TimeLM2019), 2020-12 (TimeLM2020), 2021-12 (TimeLM2021 and BERNICE), and 2022-12 (TimeLM2022).
All the model weights are taken from the transformers model hub <cite class="ltx_cite ltx_citemacro_cite">Wolf et al. (<a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite> and Table <a href="#S3.T2" title="Table 2 ‣ 3.2.1 Data Statistics ‣ 3.2 Tasks &amp; Datasets ‣ 3 Experimental Setting ‣ A Systematic Analysis on the Temporal Generalization of Language Models in Social Media" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the details of models we used in the paper.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Note that for this analysis we are not interested in the performance of zero-shot LLMs such as GPT-4, but rather on the effect of fine-tuned LMs.</span></span></span>
Table <a href="#S3.T3" title="Table 3 ‣ 3.3 Models ‣ 3 Experimental Setting ‣ A Systematic Analysis on the Temporal Generalization of Language Models in Social Media" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the overlap between the period of the pre-trained corpus and the test set for each task, which will be relevant for the analysis on the effect of pre-training in Section <a href="#S5.SS1" title="5.1 Effect of Pre-Training ‣ 5 Analysis ‣ A Systematic Analysis on the Temporal Generalization of Language Models in Social Media" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>.
These models are then fine-tuned in the datasets presented in the previous section, in both OOT and IT settings. For model fine tuning, we run hyperparameter search with Optuna <cite class="ltx_cite ltx_citemacro_cite">Akiba et al. (<a href="#bib.bib4" title="" class="ltx_ref">2019</a>)</cite> with the default search space.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<figure id="S4.F5" class="ltx_figure"><img src="/html/2405.13017/assets/figures/hate.bar.4.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="443" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparisons of IT and OOT performance (accuracy) for hate speech detection.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2405.13017/assets/figures/nerd.bar.4.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="443" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Comparisons of IT and OOT performance (accuracy) for NED.</figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2405.13017/assets/figures/ner.bar.4.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="443" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Comparisons of IT and OOT performance (F1 score) for NER.</figcaption>
</figure>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2405.13017/assets/figures/topic.bar.4.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="443" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Comparisons of IT and OOT performance (F1 score) for topic classification.</figcaption>
</figure>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2405.13017/assets/figures/sentiment.bar.4.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="443" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Comparisons of IT and OOT performance (accuracy) for sentiment classification.</figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Figures <a href="#S4.F5" title="Figure 5 ‣ 4 Results ‣ A Systematic Analysis on the Temporal Generalization of Language Models in Social Media" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> to <a href="#S4.F9" title="Figure 9 ‣ 4 Results ‣ A Systematic Analysis on the Temporal Generalization of Language Models in Social Media" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> show the comparisons of IT and OOT in hate speech detection, NED, NER, topic classification and sentiment analysis. As can be observed, hate speech detection, NED and NER present inconsistencies in both settings, decreasing the performance from IT to OOT.
In contrast, this cannot be observed for both sentiment analysis, and especially topic classification. The average decrease of OOT performance for each of the tasks is 4.5, 2.4, 1.7, 0.8 and -0.1 for hate speech detection, NER, NED topic classification and sentiment analysis.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">One of the main differences of those two groups of tasks (i.e. hate/NED/NER v.s. topic/sentiment) entity-centric or event-driven nature of the former. NER and NED are clearly related to named entities. Hate speech detection does not relate to named entities explicitly, but since the tweets for hate speech detection are collected by querying specific events, they are often about events or celebrities which peak around the sampled timestamp <cite class="ltx_cite ltx_citemacro_cite">Gómez et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite>.
On the other hand, events or named entities are not as important in sentiment analysis, as the sentiment can be estimated from the context in most cases. Topic classification depends on the topic, with some of them related to entities (e.g. those related to celebrities or TV) and others not (e.g., daily life, family or food), but in the main clearly identifiable by the context. Through the lens of entity relevancy, this result may suggest that the temporal shift can be caused by named entities, which includes meaning drift of existing named entities or emerging new named entities. Topic classification can be seen as a mixture of entity-related instances and not, which results in not fully consistent gain from OOT, but still significant in the average.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section focuses on the second research question (RQ2) and analyses the main causes behind temporal shift performance degradation of LMs.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Effect of Pre-Training</h3>

<figure id="S5.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F10.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.13017/assets/figures/topic.4.png" id="S5.F10.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="444" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Topic classification.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F10.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.13017/assets/figures/nerd.4.png" id="S5.F10.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="444" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>NED.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F10.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.13017/assets/figures/ner.4.png" id="S5.F10.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="444" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>NER.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Relative improvement (%) from OOT to IT for each task (topic, NED and NER). LMs with pre-training corpus including the test period are in blue, and those without temporal overlap in red.</figcaption>
</figure>
<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">A possible direction to mitigate the temporal shift is to pre-train the LMs on the text from the test period, which does not require any labeling. Figure <a href="#S5.F10" title="Figure 10 ‣ 5.1 Effect of Pre-Training ‣ 5 Analysis ‣ A Systematic Analysis on the Temporal Generalization of Language Models in Social Media" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> visualizes the performance and relative IT improvement of LMs with/without pre-training corpus covering the test period of each task for topic classification/NED/NER<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>The test periods of hate speech detection and sentiment classification are covered by all the LMs we considered in the experiment.</span></span></span>. At a glance, we cannot observe see any relationship between the pre-training corpus and the performance.
The averaged relative gains of the metrics from OOT within the LMs pre-trained on the test period and the others are 2.0 and 0.6 (topic classification), 3.5 and 3.8 (NER), and 2.1 and 1.9 (NED) respectively. Therefore, all models are affected by the temporal shift irrespective of the pre-training corpus date. This implies that the temporal shift cannot be robustly resolved by only adding data from the test period to the pre-training corpus, a conclusion that was also reached by <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib29" title="" class="ltx_ref">Luu et al.</a></cite> (<cite class="ltx_cite ltx_citemacro_citeyear"><a href="#bib.bib29" title="" class="ltx_ref">2022</a></cite>).</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Effect of Label Distribution</h3>

<figure id="S5.T4" class="ltx_table">
<div id="S5.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:184.8pt;height:205.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-4.9pt,5.4pt) scale(0.95,0.95) ;">
<table id="S5.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S5.T4.1.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Original</th>
<th id="S5.T4.1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Balanced</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.2.1" class="ltx_tr">
<th id="S5.T4.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">RoBERTa (B)</th>
<td id="S5.T4.1.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">7.25</td>
<td id="S5.T4.1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">5.19</td>
</tr>
<tr id="S5.T4.1.1.3.2" class="ltx_tr">
<th id="S5.T4.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">RoBERTa (L)</th>
<td id="S5.T4.1.1.3.2.2" class="ltx_td ltx_align_right">5.96</td>
<td id="S5.T4.1.1.3.2.3" class="ltx_td ltx_align_right">-0.95</td>
</tr>
<tr id="S5.T4.1.1.4.3" class="ltx_tr">
<th id="S5.T4.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BERTweet (B)</th>
<td id="S5.T4.1.1.4.3.2" class="ltx_td ltx_align_right">5.91</td>
<td id="S5.T4.1.1.4.3.3" class="ltx_td ltx_align_right">4.84</td>
</tr>
<tr id="S5.T4.1.1.5.4" class="ltx_tr">
<th id="S5.T4.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BERTweet (L)</th>
<td id="S5.T4.1.1.5.4.2" class="ltx_td ltx_align_right">2.88</td>
<td id="S5.T4.1.1.5.4.3" class="ltx_td ltx_align_right">-0.30</td>
</tr>
<tr id="S5.T4.1.1.6.5" class="ltx_tr">
<th id="S5.T4.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BERNICE</th>
<td id="S5.T4.1.1.6.5.2" class="ltx_td ltx_align_right">5.04</td>
<td id="S5.T4.1.1.6.5.3" class="ltx_td ltx_align_right">4.72</td>
</tr>
<tr id="S5.T4.1.1.7.6" class="ltx_tr">
<th id="S5.T4.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TimeLM2019 (B)</th>
<td id="S5.T4.1.1.7.6.2" class="ltx_td ltx_align_right">4.80</td>
<td id="S5.T4.1.1.7.6.3" class="ltx_td ltx_align_right">4.16</td>
</tr>
<tr id="S5.T4.1.1.8.7" class="ltx_tr">
<th id="S5.T4.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TimeLM2020 (B)</th>
<td id="S5.T4.1.1.8.7.2" class="ltx_td ltx_align_right">5.71</td>
<td id="S5.T4.1.1.8.7.3" class="ltx_td ltx_align_right">5.39</td>
</tr>
<tr id="S5.T4.1.1.9.8" class="ltx_tr">
<th id="S5.T4.1.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TimeLM2021 (B)</th>
<td id="S5.T4.1.1.9.8.2" class="ltx_td ltx_align_right">4.51</td>
<td id="S5.T4.1.1.9.8.3" class="ltx_td ltx_align_right">5.52</td>
</tr>
<tr id="S5.T4.1.1.10.9" class="ltx_tr">
<th id="S5.T4.1.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TimeLM2022 (B)</th>
<td id="S5.T4.1.1.10.9.2" class="ltx_td ltx_align_right">4.97</td>
<td id="S5.T4.1.1.10.9.3" class="ltx_td ltx_align_right">0.15</td>
</tr>
<tr id="S5.T4.1.1.11.10" class="ltx_tr">
<th id="S5.T4.1.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TimeLM2022 (L)</th>
<td id="S5.T4.1.1.11.10.2" class="ltx_td ltx_align_right">4.94</td>
<td id="S5.T4.1.1.11.10.3" class="ltx_td ltx_align_right">1.89</td>
</tr>
<tr id="S5.T4.1.1.12.11" class="ltx_tr">
<th id="S5.T4.1.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Average</th>
<td id="S5.T4.1.1.12.11.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">5.20</td>
<td id="S5.T4.1.1.12.11.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">3.06</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparisons of relative accuracy gain from OOT to IT between original (unbalanced) and balanced label distributions for hate speech detection.</figcaption>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In supervised machine learning label distribution, the distribution of the binary label over the test instances, shifts can affect a model’s performance. In this section, we analyse this potential effect when it comes to temporal shifts. For this, we rely on hate speech detection, which presents the largest decrease in performance from IT to OOT, with a different label distribution between training and test (see Figure <a href="#S3.F3" title="Figure 3 ‣ 3.2.1 Data Statistics ‣ 3.2 Tasks &amp; Datasets ‣ 3 Experimental Setting ‣ A Systematic Analysis on the Temporal Generalization of Language Models in Social Media" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). For the other tasks, the label distribution appears to be largely similar. To separate the effect of label distributional shift between IT and OOT from the temporal shift, we conduct a controlled experiment by balancing the label distribution of each IT training split to be the same as OOT training split. This is achieved by undersampling the size of the training set. Table <a href="#S5.T4" title="Table 4 ‣ 5.2 Effect of Label Distribution ‣ 5 Analysis ‣ A Systematic Analysis on the Temporal Generalization of Language Models in Social Media" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the results, where the average relative gain is still positive, although it becomes less dominant in balanced experiment. This highlights how label distribution may change over time and this itself have an effect in model performance. A similar finding was already discussed by <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib29" title="" class="ltx_ref">Luu et al.</a></cite> (<cite class="ltx_cite ltx_citemacro_citeyear"><a href="#bib.bib29" title="" class="ltx_ref">2022</a></cite>).</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Qualitative Analysis</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">In this analysis, we have a closer look on the test instances that are incorrect in OOT, turning to be correct in IT. To be precise, we sort the test instance in a single task based on the number of models where the error in OOT setting has been corrected in IT setting over all the random seeds. In other words, given a test instance, we check whether a model prediction is incorrect in OOT, but correct in the IT setting. This particular instance is counted as a correction. In total, we have 10 models with 3 independent runs with different random seed to construct the training data, so 30 would be the maximum number of corrections. For sentiment classification, hate speech detection, topic classification and NED, we simply count instance-level corrections. Given the complex nature of NER evaluation, we decided to only focus on the entity type predictions for this analysis.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<table id="S5.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.1.1.1" class="ltx_tr">
<th id="S5.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Task</th>
<th id="S5.T5.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Top corrected</th>
<th id="S5.T5.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Avg Top 10</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.1.2.1" class="ltx_tr">
<td id="S5.T5.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">NED</td>
<td id="S5.T5.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">30/30 (100%)</td>
<td id="S5.T5.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">30.0</td>
</tr>
<tr id="S5.T5.1.3.2" class="ltx_tr">
<td id="S5.T5.1.3.2.1" class="ltx_td ltx_align_left">Hate</td>
<td id="S5.T5.1.3.2.2" class="ltx_td ltx_align_left">30/30 (100%)</td>
<td id="S5.T5.1.3.2.3" class="ltx_td ltx_align_right">30.0</td>
</tr>
<tr id="S5.T5.1.4.3" class="ltx_tr">
<td id="S5.T5.1.4.3.1" class="ltx_td ltx_align_left">NER</td>
<td id="S5.T5.1.4.3.2" class="ltx_td ltx_align_left">28/30 (93.3%)</td>
<td id="S5.T5.1.4.3.3" class="ltx_td ltx_align_right">24.0</td>
</tr>
<tr id="S5.T5.1.5.4" class="ltx_tr">
<td id="S5.T5.1.5.4.1" class="ltx_td ltx_align_left">Sentiment</td>
<td id="S5.T5.1.5.4.2" class="ltx_td ltx_align_left">19/30 (63.3%)</td>
<td id="S5.T5.1.5.4.3" class="ltx_td ltx_align_right">13.6</td>
</tr>
<tr id="S5.T5.1.6.5" class="ltx_tr">
<td id="S5.T5.1.6.5.1" class="ltx_td ltx_align_left ltx_border_bb">Topic</td>
<td id="S5.T5.1.6.5.2" class="ltx_td ltx_align_left ltx_border_bb">16/30 (53.3%)</td>
<td id="S5.T5.1.6.5.3" class="ltx_td ltx_align_right ltx_border_bb">12.5</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Top instances in terms of number of predictions corrected with an IT split. The second column indicates the top 10 average.</figcaption>
</figure>
<figure id="S5.T6" class="ltx_table">
<div id="S5.T6.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:206.9pt;height:203.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.9pt,25.4pt) scale(0.8,0.8) ;">
<table id="S5.T6.4.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T6.4.4.5.1" class="ltx_tr">
<th id="S5.T6.4.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Task</th>
<th id="S5.T6.4.4.5.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T6.4.4.5.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.4.4.5.1.2.1.1" class="ltx_p" style="width:90.0pt;">Instance</span>
</span>
</th>
<th id="S5.T6.4.4.5.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Gold</th>
<th id="S5.T6.4.4.5.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Times corrected</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T6.2.2.2" class="ltx_tr">
<td id="S5.T6.2.2.2.3" class="ltx_td ltx_align_left ltx_border_t">NED</td>
<td id="S5.T6.2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T6.2.2.2.2.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.2.2.2.2.2.2" class="ltx_p" style="width:90.0pt;">so cute how <math id="S5.T6.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.T6.1.1.1.1.1.1.m1.1a"><mo id="S5.T6.1.1.1.1.1.1.m1.1.1" xref="S5.T6.1.1.1.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.T6.1.1.1.1.1.1.m1.1b"><lt id="S5.T6.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T6.1.1.1.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.1.1.1.1.1.1.m1.1c">&lt;</annotation></semantics></math>Aoki<math id="S5.T6.2.2.2.2.2.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S5.T6.2.2.2.2.2.2.m2.1a"><mo id="S5.T6.2.2.2.2.2.2.m2.1.1" xref="S5.T6.2.2.2.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S5.T6.2.2.2.2.2.2.m2.1b"><gt id="S5.T6.2.2.2.2.2.2.m2.1.1.cmml" xref="S5.T6.2.2.2.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.2.2.2.2.2.m2.1c">&gt;</annotation></semantics></math> describes Ida. "thinks about things seriously" <span id="S5.T6.2.2.2.2.2.2.1" class="ltx_text ltx_font_italic">(Japanese manga series)</span></span>
</span>
</td>
<td id="S5.T6.2.2.2.4" class="ltx_td ltx_align_right ltx_border_t">False</td>
<td id="S5.T6.2.2.2.5" class="ltx_td ltx_align_right ltx_border_t">30/30 (100%)</td>
</tr>
<tr id="S5.T6.4.4.4" class="ltx_tr">
<td id="S5.T6.4.4.4.3" class="ltx_td"></td>
<td id="S5.T6.4.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T6.4.4.4.2.2" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.4.4.4.2.2.2" class="ltx_p" style="width:90.0pt;">Will Ram &amp; <math id="S5.T6.3.3.3.1.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.T6.3.3.3.1.1.1.m1.1a"><mo id="S5.T6.3.3.3.1.1.1.m1.1.1" xref="S5.T6.3.3.3.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.T6.3.3.3.1.1.1.m1.1b"><lt id="S5.T6.3.3.3.1.1.1.m1.1.1.cmml" xref="S5.T6.3.3.3.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.3.3.3.1.1.1.m1.1c">&lt;</annotation></semantics></math>Priya<math id="S5.T6.4.4.4.2.2.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S5.T6.4.4.4.2.2.2.m2.1a"><mo id="S5.T6.4.4.4.2.2.2.m2.1.1" xref="S5.T6.4.4.4.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S5.T6.4.4.4.2.2.2.m2.1b"><gt id="S5.T6.4.4.4.2.2.2.m2.1.1.cmml" xref="S5.T6.4.4.4.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.4.4.4.2.2.2.m2.1c">&gt;</annotation></semantics></math> go on a honeymoon it’ll be a nice break for them (…) #BadeAchheLagteHain2 <span id="S5.T6.4.4.4.2.2.2.1" class="ltx_text ltx_font_italic">(Indian actress)</span></span>
</span>
</td>
<td id="S5.T6.4.4.4.4" class="ltx_td ltx_align_right ltx_border_t">False</td>
<td id="S5.T6.4.4.4.5" class="ltx_td ltx_align_right ltx_border_t">30/30 (100%)</td>
</tr>
<tr id="S5.T6.4.4.6.1" class="ltx_tr">
<td id="S5.T6.4.4.6.1.1" class="ltx_td ltx_align_left ltx_border_t">Hate</td>
<td id="S5.T6.4.4.6.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T6.4.4.6.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.4.4.6.1.2.1.1" class="ltx_p" style="width:90.0pt;">#MKR God Kat you are awful awful person. Oh you are humiliated? GOOD.</span>
</span>
</td>
<td id="S5.T6.4.4.6.1.3" class="ltx_td ltx_align_right ltx_border_t">False</td>
<td id="S5.T6.4.4.6.1.4" class="ltx_td ltx_align_right ltx_border_t">30/30 (100%)</td>
</tr>
<tr id="S5.T6.4.4.7.2" class="ltx_tr">
<td id="S5.T6.4.4.7.2.1" class="ltx_td ltx_border_bb"></td>
<td id="S5.T6.4.4.7.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S5.T6.4.4.7.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.4.4.7.2.2.1.1" class="ltx_p" style="width:90.0pt;">#katandandre gaaaaah I just want to slap her back to WA #MKR</span>
</span>
</td>
<td id="S5.T6.4.4.7.2.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">False</td>
<td id="S5.T6.4.4.7.2.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">30/30 (100%)</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Two examples from the NED and hate speech detection datasets in which the prediction was corrected 100% of the times with an IT split. For NED, the definition is provided in parenthesis and target word indicated between <math id="S5.T6.7.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.T6.7.m1.1b"><mo id="S5.T6.7.m1.1.1" xref="S5.T6.7.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.T6.7.m1.1c"><lt id="S5.T6.7.m1.1.1.cmml" xref="S5.T6.7.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.7.m1.1d">&lt;</annotation></semantics></math> and <math id="S5.T6.8.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S5.T6.8.m2.1b"><mo id="S5.T6.8.m2.1.1" xref="S5.T6.8.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S5.T6.8.m2.1c"><gt id="S5.T6.8.m2.1.1.cmml" xref="S5.T6.8.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.8.m2.1d">&gt;</annotation></semantics></math>.</figcaption>
</figure>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Table <a href="#S5.T5" title="Table 5 ‣ 5.3 Qualitative Analysis ‣ 5 Analysis ‣ A Systematic Analysis on the Temporal Generalization of Language Models in Social Media" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the top instances in terms of IT corrections for each of the task. We can observed the marked differences across tasks, with NED and hate speech detection including instances which were corrected 100% in the OOT setting. In fact, there are respectively 44 and 15 instances for which this is the case in these two tasks. Similarly for NER, the number of corrections is high. This is correlated with the main results of the paper (see Section <a href="#S4" title="4 Results ‣ A Systematic Analysis on the Temporal Generalization of Language Models in Social Media" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) which showed clear improvements for these tasks in the IT setting, but not for sentiment and and topic classification.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">Finally, Table <a href="#S5.T6" title="Table 6 ‣ 5.3 Qualitative Analysis ‣ 5 Analysis ‣ A Systematic Analysis on the Temporal Generalization of Language Models in Social Media" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows some of these instances for NED and hate speech detection. In the case of NED, the tweets relate to two new TV series that were on air at test time (Japanese <span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_italic">Kieta Hatsukoi</span> in the first example and Indian B<span id="S5.SS3.p3.1.2" class="ltx_text ltx_font_italic">ade Achhe Lagte Hain</span> in the second, both from 2021). This is similar to the hate speech detection in which the examples belong to the <span id="S5.SS3.p3.1.3" class="ltx_text ltx_font_italic">My Kitchen Rules TV</span> show. This highlights the event-driven nature of social media, and the importance of acquiring the background context for the specific task.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We proposed an evaluation method to assess the adaptability of LMs for temporal shifts on social media with five diverse downstream tasks including sentiment classification, NER, NED, hate speech detection, and topic classification. We have tested diverse LMs trained on Twitter under different temporal settings. The experimental results indicate that the adaptability gets consistently worse on entity or event-driven tasks (NED, NER, and hate speech detection) while the effect is limited in the other tasks. This conclusion was similar to previous work in more general domains, which observed a variation across different types of task when it comes to temporal degradation <cite class="ltx_cite ltx_citemacro_cite">Luu et al. (<a href="#bib.bib29" title="" class="ltx_ref">2022</a>); Agarwal and Nenkova (<a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite>. Finally, our analysis shows that pre-training on a corpus from the test period is not enough to solve the temporal shift issue, with performance still being degraded in comparison to models fine-tuned on the labeled dataset from the test period.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations.</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Regardless of some similarities between Twitter and other streaming data such as news and other social media platforms being real-time and trend-driven, they can have different characteristics, and the results of our study may apply to Twitter exclusively. For our evaluation we rely on a single dataset for each of the tasks. Of course, these datasets are not a faithfully representation of the task and may contain their own biases. Therefore, even for the same task, the findings in this paper may differ if using a different dataset.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Ethical Statement.</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">The datasets we used in the experiments are all from Twitter. Data has been anonymized (only information about legacy-verified users is kept) so that they do not contain any personal identifiable information (PII). We do not gather information from individual accounts but rely on aggregated information and metrics only. Please note that the text may contain sensitive content due to the nature of social media and the task, in particular hate speech detection.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aakanksha et al. (2023)</span>
<span class="ltx_bibblock">
Chowdhery Aakanksha, Narang Sharan, Devlin Jacob, et al. 2023.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 24(240):1–113.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aditya et al. (2021)</span>
<span class="ltx_bibblock">
Ramesh Aditya, Pavlov Mikhail, Goh Gabriel, et al. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2102.12092" title="" class="ltx_ref ltx_href">Zero-shot text-to-image generation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2102.12092.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agarwal and Nenkova (2022)</span>
<span class="ltx_bibblock">
Oshin Agarwal and Ani Nenkova. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00497" title="" class="ltx_ref ltx_href">Temporal effects on pre-trained models for language processing tasks</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Transactions of the ACL</em>, 10:904–921.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akiba et al. (2019)</span>
<span class="ltx_bibblock">
Takuya Akiba, Shotaro Sano, Yanase, et al. 2019.

</span>
<span class="ltx_bibblock">Optuna: A next-generation hyperparameter optimization framework.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>, pages 2623–2631.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alkhalifa et al. (2023)</span>
<span class="ltx_bibblock">
Rabab Alkhalifa, Iman Bilal, Hsuvas Borkakoty, et al. 2023.

</span>
<span class="ltx_bibblock">Overview of the clef-2023 longeval lab on longitudinal evaluation of model performance.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">International Conference of the Cross-Language Evaluation Forum for European Languages</em>, pages 440–458. Springer.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antypas et al. (2023)</span>
<span class="ltx_bibblock">
Dimosthenis Antypas, Asahi Ushio, Francesco Barbieri, et al. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-emnlp.838" title="" class="ltx_ref ltx_href">SuperTweetEval: A challenging, unified and heterogeneous benchmark for social media NLP research</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Findings of EMNLP 2023</em>, pages 12590–12607, Singapore.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antypas et al. (2022)</span>
<span class="ltx_bibblock">
Dimosthenis Antypas, Asahi Ushio, Jose Camacho-Collados, et al. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.coling-1.299" title="" class="ltx_ref ltx_href">Twitter topic classification</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of COLING</em>, pages 3386–3400, Gyeongju, Republic of Korea.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barbieri et al. (2020)</span>
<span class="ltx_bibblock">
Francesco Barbieri, Jose Camacho-Collados, Luis Espinosa Anke, and Leonardo Neves. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.findings-emnlp.148" title="" class="ltx_ref ltx_href">TweetEval: Unified benchmark and comparative evaluation for tweet classification</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Findings of the ACL: EMNLP 2020</em>, pages 1644–1650, Online. ACL.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barbieri et al. (2022)</span>
<span class="ltx_bibblock">
Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.lrec-1.27" title="" class="ltx_ref ltx_href">XLM-T: Multilingual language models in Twitter for sentiment analysis and beyond</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of LREC</em>, pages 258–266, Marseille, France.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeLucia et al. (2022)</span>
<span class="ltx_bibblock">
Alexandra DeLucia, Shijie Wu, Aaron Mueller, Carlos Aguirre, Philip Resnik, and Mark Dredze. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.emnlp-main.415" title="" class="ltx_ref ltx_href">Bernice: A multilingual pre-trained encoder for Twitter</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on EMNLP</em>, pages 6191–6205, Abu Dhabi, United Arab Emirates. ACL.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, et al. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N19-1423" title="" class="ltx_ref ltx_href">BERT: Pre-training of deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of NAACL</em>, pages 4171–4186, Minneapolis, Minnesota.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhingra et al. (2022)</span>
<span class="ltx_bibblock">
Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W. Cohen. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00459" title="" class="ltx_ref ltx_href">Time-aware language models as temporal knowledge bases</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Transactions of the ACL</em>, 10:257–273.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dimosthenis et al. (2023)</span>
<span class="ltx_bibblock">
Antypas Dimosthenis, Ushio Asahi, Barbieri Francesco, et al. 2023.

</span>
<span class="ltx_bibblock">Supertweeteval: A challenging, unified and heterogeneous benchmark for social media nlp research.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Findings of EMNLP 2023</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ettinger (2020)</span>
<span class="ltx_bibblock">
Allyson Ettinger. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00298" title="" class="ltx_ref ltx_href">What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Transactions of the ACL</em>, 8:34–48.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Florio et al. (2020)</span>
<span class="ltx_bibblock">
Komal Florio, Valerio Basile, Marco Polignano, Pierpaolo Basile, and Viviana Patti. 2020.

</span>
<span class="ltx_bibblock">Time of your hate: The challenge of time in hate speech detection on social media.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, 10(12):4180.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldberg (2019)</span>
<span class="ltx_bibblock">
Yoav Goldberg. 2019.

</span>
<span class="ltx_bibblock">Assessing bert’s syntactic abilities.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.05287</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gómez et al. (2023)</span>
<span class="ltx_bibblock">
Jesús Gómez, Alberto Matilla-Molina, Ma Pilar Amado, Dimosthenis Antypas, Jose Camacho-Collados, Carlos J Máñez, Tomás Fernández-Villazala, Alicia Méndez-Sanchís, and Javier López. 2023.

</span>
<span class="ltx_bibblock">The interaction between offensive and hate speech on twitter and relevant social events in spain.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">News Media and Hate Speech Promotion in Mediterranean Countries</em>, pages 81–109. IGI Global.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hewitt and Manning (2019)</span>
<span class="ltx_bibblock">
John Hewitt and Christopher D. Manning. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N19-1419" title="" class="ltx_ref ltx_href">A structural probe for finding syntax in word representations</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the ACL: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 4129–4138, Minneapolis, Minnesota. ACL.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hugo et al. (2023a)</span>
<span class="ltx_bibblock">
Touvron Hugo, Martin Louis, Stone Kevin, et al. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2307.09288" title="" class="ltx_ref ltx_href">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hugo et al. (2023b)</span>
<span class="ltx_bibblock">
Touvron Hugo, Lavril Thibaut, Izacard Gautier, et al. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2302.13971" title="" class="ltx_ref ltx_href">Llama: Open and efficient foundation language models</a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jason et al. (2022)</span>
<span class="ltx_bibblock">
Wei Jason, Tay Yi, Bommasani Rishi, et al. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=yzkSU5zdwD" title="" class="ltx_ref ltx_href">Emergent abilities of large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>.

</span>
<span class="ltx_bibblock">Survey Certification.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jungo et al. (2022)</span>
<span class="ltx_bibblock">
Kasai Jungo, Sakaguchi Keisuke, Takahashi Yoichi, et al. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2207.13332" title="" class="ltx_ref ltx_href">Realtime qa: What’s the answer right now?</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2207.13332.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lazaridou et al. (2021)</span>
<span class="ltx_bibblock">
Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya, et al. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2021/file/f5bf0ba0a17ef18f9607774722f5698c-Paper.pdf" title="" class="ltx_ref ltx_href">Mind the gap: Assessing temporal generalization in neural language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 34, pages 29348–29363.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, et al. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.703" title="" class="ltx_ref ltx_href">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of ACL</em>, pages 7871–7880, Online.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liska et al. (2022)</span>
<span class="ltx_bibblock">
Adam Liska, Tomas Kocisky, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal, D’Autume Cyprien De Masson, Tim Scholtes, Manzil Zaheer, Susannah Young, et al. 2022.

</span>
<span class="ltx_bibblock">Streamingqa: A benchmark for adaptation to new knowledge over time in question answering models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages 13604–13622. PMLR.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, et al. 2019.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loureiro et al. (2022a)</span>
<span class="ltx_bibblock">
Daniel Loureiro, Francesco Barbieri, Leonardo Neves, et al. 2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-demo.25" title="" class="ltx_ref ltx_href">TimeLMs: Diachronic language models from Twitter</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of ACL: System Demonstrations</em>, pages 251–260, Dublin, Ireland.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loureiro et al. (2022b)</span>
<span class="ltx_bibblock">
Daniel Loureiro, Aminette D’Souza, Areej Nasser Muhajab, et al. 2022b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.coling-1.296" title="" class="ltx_ref ltx_href">TempoWiC: An evaluation benchmark for detecting meaning shift in social media</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of COLING</em>, pages 3353–3359, Gyeongju, Republic of Korea.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luu et al. (2022)</span>
<span class="ltx_bibblock">
Kelvin Luu, Daniel Khashabi, Suchin Gururangan, Karishma Mandyam, and Noah A. Smith. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.naacl-main.435" title="" class="ltx_ref ltx_href">Time waits for no one! analysis and challenges of temporal misalignment</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 5944–5958, Seattle, United States.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min et al. (2023)</span>
<span class="ltx_bibblock">
Bonan Min, Hayley Ross, Elior Sulem, et al. 2023.

</span>
<span class="ltx_bibblock">Recent advances in natural language processing via large pre-trained language models: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys</em>, 56(2):1–40.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et al. (2022)</span>
<span class="ltx_bibblock">
Shubhanshu Mishra, Aman Saini, Raheleh Makki, et al. 2022.

</span>
<span class="ltx_bibblock">Tweetnerd–end to end entity linking benchmark for tweets.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.08129</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2020)</span>
<span class="ltx_bibblock">
Dat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-demos.2" title="" class="ltx_ref ltx_href">BERTweet: A pre-trained language model for English tweets</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on EMNLP: System Demonstrations</em>, pages 9–14, Online. ACL.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paul K. et al. (2023)</span>
<span class="ltx_bibblock">
Rubenstein Paul K., Asawaroengchai Chulayuth, Dung Nguyen Duc, et al. 2023.

</span>
<span class="ltx_bibblock">Audiopalm: A large language model that can speak and listen.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.12925</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni et al. (2019)</span>
<span class="ltx_bibblock">
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, et al. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1250" title="" class="ltx_ref ltx_href">Language models as knowledge bases?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of EMNLP-IJCNLP</em>, pages 2463–2473, Hong Kong, China.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, et al. 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">OpenAI blog</em>, 1(8):9.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, et al. 2020.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">J. Mach. Learn. Res.</em>, 21(1).

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rohan et al. (2023)</span>
<span class="ltx_bibblock">
Anil Rohan, Dai Andrew M., Firat Orhan, et al. 2023.

</span>
<span class="ltx_bibblock">Palm 2 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.10403</em>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rosin et al. (2022)</span>
<span class="ltx_bibblock">
Guy D. Rosin, Ido Guy, and Kira Radinsky. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3488560.3498529" title="" class="ltx_ref ltx_href">Time masking for temporal language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining</em>, WSDM ’22, page 833–841, New York, NY, USA.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rosin and Radinsky (2022)</span>
<span class="ltx_bibblock">
Guy D. Rosin and Kira Radinsky. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.findings-naacl.112" title="" class="ltx_ref ltx_href">Temporal attention for language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Findings of the ACL: NAACL 2022</em>, pages 1498–1508, Seattle, United States. ACL.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tenney et al. (2019)</span>
<span class="ltx_bibblock">
Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P19-1452" title="" class="ltx_ref ltx_href">BERT rediscovers the classical NLP pipeline</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the ACL</em>, pages 4593–4601, Florence, Italy. ACL.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tom B. et al. (2020)</span>
<span class="ltx_bibblock">
Brown Tom B., Mann Benjamin, Ryder Nick, et al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:1877–1901.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ushio et al. (2022)</span>
<span class="ltx_bibblock">
Asahi Ushio, Francesco Barbieri, Vitor Sousa, et al. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.aacl-main.25" title="" class="ltx_ref ltx_href">Named entity recognition in Twitter: A dataset and analysis on short-term temporal shifts</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2nd Conference of AACL</em>, pages 309–319, Online only.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ushio et al. (2021)</span>
<span class="ltx_bibblock">
Asahi Ushio, Luis Espinosa Anke, Steven Schockaert, et al. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.acl-long.280" title="" class="ltx_ref ltx_href">BERT is to NLP what AlexNet is to CV: Can pre-trained language models identify analogies?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of ACL</em>, pages 3609–3624, Online.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Waseem and Hovy (2016)</span>
<span class="ltx_bibblock">
Zeerak Waseem and Dirk Hovy. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N16-2013" title="" class="ltx_ref ltx_href">Hateful symbols or hateful people? predictive features for hate speech detection on Twitter</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the NAACL Student Research Workshop</em>, pages 88–93, San Diego, California. ACL.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2020)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, et al. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-demos.6" title="" class="ltx_ref ltx_href">Transformers: State-of-the-art natural language processing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on EMNLP: System Demonstrations</em>, pages 38–45, Online.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.13016" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.13017" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.13017">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.13017" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.13018" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 16:46:26 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
