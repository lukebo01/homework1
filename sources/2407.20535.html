<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.20535] DeepSpeech models show Human-like Performance and Processing of Cochlear Implant Inputs</title><meta property="og:description" content="Cochlear implants(CIs) are arguably the most successful neural implant, having restored hearing to over one million people worldwide. While CI research has focused on modeling the cochlear activations in response to lo…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DeepSpeech models show Human-like Performance and Processing of Cochlear Implant Inputs">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="DeepSpeech models show Human-like Performance and Processing of Cochlear Implant Inputs">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.20535">

<!--Generated on Mon Aug  5 16:16:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">DeepSpeech models show Human-like Performance and Processing of Cochlear Implant Inputs</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Cynthia R. Steinhardt 
<br class="ltx_break">Center for Theoretical Neuroscience 
<br class="ltx_break">Zuckerman Mind Brain Behavior Institute
<br class="ltx_break">Columbia University
<br class="ltx_break">New York, NY 10027 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">cs4248@columbia.edu</span> 
<br class="ltx_break">&amp;Menoua Keshishian 
<br class="ltx_break">Department of Electrical Engineering 
<br class="ltx_break">Zuckerman Mind Brain Behavior Institute 
<br class="ltx_break">Columbia University 
<br class="ltx_break">New York, NY 10027 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">mk4011@columbia.edu</span> 
<br class="ltx_break">&amp;Nima Mesgarani 
<br class="ltx_break">Department of Electrical Engineering 
<br class="ltx_break">Zuckerman Mind Brain Behavior Institute 
<br class="ltx_break">Columbia University 
<br class="ltx_break">New York, NY 10027 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">nima@ee.columbia.edu</span> 
<br class="ltx_break">&amp;Kimberly Stachenfeld 
<br class="ltx_break">Google DeepMind 
<br class="ltx_break">Columbia University 
<br class="ltx_break">New York, NY
<br class="ltx_break"><span id="id4.4.id4" class="ltx_text ltx_font_typewriter">stachenfeld@deepmind.com</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes">corresponding author. Simons Society of Fellows Junior Fellow</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Cochlear implants(CIs) are arguably the most successful neural implant, having restored hearing to over one million people worldwide. While CI research has focused on modeling the cochlear activations in response to low-level acoustic features, we hypothesize that the success of these implants is due in large part to the role of the upstream network in extracting useful features from a degraded signal and learned statistics of language to resolve the signal. In this work, we use the deep neural network (DNN) DeepSpeech2, which processes audio inputs causally to perform phoneme prediction from spoken sentences, and use it as a paradigm to investigate how natural input and cochlear implant-based inputs are processed over time. We generate naturalistic and cochlear implant-like inputs from spoken sentences and test the similarity of model performance to human performance on analogous phoneme recognition tests. Our model reproduces error patterns in reaction time and phoneme confusion patterns under noise conditions in normal hearing and CI participant studies. We then use interpretability techniques to determine where and when confusions arise when processing naturalistic and CI-like inputs. We find that dynamics over time in each layer are affected by context as well as input type. Dynamics of all phonemes diverge during confusion and comprehension within the same time window, which is temporally shifted backward in each layer of the network. There is a shift and reduction in amplitude of this signal during processing of CI inputs compared to natural inputs which resembles the timing and changes of EEG signals in the auditory stream. This reduction likely relates to the reduction of encoded phoneme identity and indicates similarity in representation of natural and CI inputs. These findings suggest that we have a viable model in which to explore the loss of speech-related information in time and that we can use it to find population-level encoding signals to target when optimizing cochlear implant inputs to improve encoding of essential speech-related information and improve perception.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction </h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Deep Neural Networks have emerged as modeling framework capable of complex and human-like behaviors <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and recently have made particular strides in performing text-based and auditory language tasks <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. As they have gained human-like capabilities they have been increasingly compared to the representation and processing of the human brain <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. While they omit certain biophysical details, these models are uniquely capable of capturing complex perception processes. Speech-to-text models (i.e. DeepSpeech2 <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>) in particular have been applied to model naturalistic speech perception <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. In this work, we investigate their applicability for modeling how speech perception is altered in the hearing-impaired patients with cochlear implants (CIs).
CIs have restored hearing to over one million people around the world <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. They encode sound with a limited number of electrodes (16 in the Advanced Bionic implant simulated in this study) which deliver pulses of current over time with amplitude modulated proportional to power in the spectra band being encoded per channel. This strategy enables an informative but limited audio channel compared to the full spectrum experienced by normal hearing subjects. Much of the efforts to address deficits of cochlear implants have focused on detailed biophysical modeling of voltage-driven activations of single-neurons in the ear itself. While this work has enabled CIs to better approximate the effect of sound on the lowest levels of auditory processing, these simulations are not able to model the entire hierarchy of auditory processing (from sound to phonemes to words to sentences), nor how auditory processing is altered over time and across regions of the brain <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this work, we aim to create a model system in which to investigate how electrical encoding of speech-related information at the cochlea affects speech comprehension at the word and phoneme level. Our specific contributions are as follows:</p>
</div>
<div id="S1.p3" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We develop a model of natural speech comprehension for patients with cochlear implants. Our approach is to combine (1) a “vocoder” model designed to mimic how a CI distorts the acoustics of an auditory signal and (2) a DeepSpeech2 model trained to convert speech to phonemes. The latter is a novel variant on the speech-to-text model, DeepSpeech2, which we dub Phoneme DeepSpeech2 (PhoDe).</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We validate this model, showing the model captures different aspects of error patterns in CI versus NH subjects in the types of errors made, the effect of background noise on error types, and phoneme confusion rates.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We find that the model shows similar characteristics of temporal processing on words and phonemes, in particular replicating delays in reaction time with CI inputs, background noise, consonant vs. vowel, and correct vs. error.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We replicate findings of an auditory hierarchy and find that dynamics across layers of the model recapitulate key features of neural dynamics across processing levels in the brain as measured with EEG in CI vs. NH subjects.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2407.20535/assets/fin_figures/fig_1_small.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="273" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Auditory System Model and Input Generation. A. Phoneme DeepSpeech 2 (PhoDe) Network with 5 LSTM layers followed by a fully-connected layer was trained to process spectrograms of sentences from various speakers in LibriSpeech. B. Cochlear implant versions of inputs were made by running the audio through the front-end processing algorithm of an Advanced Bionics cochlear implant, then transforming the electrodogram via a biophysical model and filterbanks into a vocoded version of the speech. C. During testing, the output predicted sequence of phonemes was aligned to the (target) true phoneme utterances over time with a Levenshtein’s algorithm. The number of substitutions or confusions, omissions, and additions could be determined to find the phoneme confusion matrix. D. Example 3-D projection of activation during confusion (red) and non-confusion (green) of phonemes in network layers: Layer 2 (top) and Layer 11 (bottom). </figcaption>
</figure>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Related Work</h3>

<section id="S1.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.1.1 </span>Cochlear Implants</h4>

<div id="S1.SS1.SSS1.p1" class="ltx_para">
<p id="S1.SS1.SSS1.p1.1" class="ltx_p">Since the first use of electrical stimulation in a cochlear implant(CI) to restore hearing in 1957, CIs have proliferated and restored hearing to over one million people around the world <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> The success of CIs inspired a migration of the invasive electrode hardware and pulsatile stimulation algorithm over to a variety of devices <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, including retinal and vestibular implants for sensory restoration <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, spinal cord stimulators for pain, and deep brain stimulators for treatment of motor and psychiatric disorders <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. While all these devices successfully aid in a range of restorative treatments, patient recovery remains limited compared to normal function in each case <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Deficits are often attributed to significant differences in the spatial targeting of neurons due to current spread from the electrodes <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, unnatural temporal synchrony of neurons due to pulse-locked activations <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, or the limited bandwidth of the signal delivered, due to hardware limitations, especially in CI uses <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. With the largest patient population and over 60 years of use in real-world situations, the deficits of CIs in different types of noise, for speech tasks at the phonetic- <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, word-, and sentence-level <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> have been carefully characterized <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, as has the psychophysics of the normal auditory system for equivalent tasks <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. This combination of understanding the natural system and behavioral and recording data makes CIs an excellent test-bed for understanding differences in processing of information encoded with electrical stimulation compared to natural inputs.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2407.20535/assets/fin_figures/fig_4_small.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="356" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A. Human <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> NH consonant and vowel performance in -6 dB versus low noise simulation condition pattern of confusion. B. Human<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> and simulation comparison of CI listening in quiet for consonants and vowels. Shown at 5,40,70, and 90% thresholding compared to normalized maximum prediction probability per phoneme. C. Diagonal correlation, non-diagonal correlation, and KL divergence between human and simulation confusion matrices for original matrices(blue) versus shuffling of the simulation matrix for 500 shuffles (red) at each noise level. D.Statistics for CI data. </figcaption>
</figure>
</section>
<section id="S1.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.1.2 </span>Local Biophysical Modeling of Electrical Stimulation</h4>

<div id="S1.SS1.SSS2.p1" class="ltx_para">
<p id="S1.SS1.SSS2.p1.1" class="ltx_p">Much of the efforts to address deficits of neural implants have focused on determining optimal stimulation parameters and hardware configurations for targeting desired local neuron populations using detailed biophysical modeling of voltage-driven activations of single-neurons in the complex three-dimensional geometries of the body <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. While this body of work has improved the ability to tune parameters to optimize patient-specific outcomes, modeling neural responses over time locally and across brain regions <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> is intractable in these let alone the complex behaviors and deficits observed when using neural implants.</p>
</div>
</section>
<section id="S1.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.1.3 </span> Deep Neural Networks as a Comparative Model System to Human Auditory Stream </h4>

<div id="S1.SS1.SSS3.p1" class="ltx_para">
<p id="S1.SS1.SSS3.p1.1" class="ltx_p">Auditory DNNs, specifically text-based large language models(LLMs) have been a particularly popular point of comparison since the success of transformer architectures at language task <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Often, comparisons have addressed representational similarities between LLMs and the brain <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. Each of these studies has revealed shared representational features between DNNs and the human auditory system, but they have also been limited particularly in addressing temporal processing similarities. Some lacked temporal precision in comparisons due to use of functional MRI <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. Comparisons are often made on text-based LLM which differ significantly, especially in earlier processing stages from listening to spoken words <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. Additionally, models process inputs in biologically implausible manners that lack causality <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> in how inputs are integrated to perform speech comprehension. As a result, certain model architectures share more representational similarity to the human auditory stream <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. We choose to focus our efforts on a DeepSpeech2 model that has been shown to share the temporal processing hierarchy of phonetic and semantic content <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and causality with the human auditory system <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>.</p>
</div>
<div id="S1.SS1.SSS3.p2" class="ltx_para">
<p id="S1.SS1.SSS3.p2.1" class="ltx_p">In this work, we aim to create a model system in which we can investigate how electrical encoding of speech-related information in the cochlea affects speech comprehension at the word and phoneme-level.
Previous recording <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and simulation studies <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> indicate pulsatile stimulation produces different encoding patterns than natural inputs in higher-order auditory cortex <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. However, we hypothesize non-identical inputs could also produce similar deep layer responses <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> and therefore better speech encoding without producing identical cochlear activity to the healthy cochlea which is intractable with current technologies <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>.</p>
</div>
<figure id="S1.F3" class="ltx_figure"><img src="/html/2407.20535/assets/fin_figures/fig_2_SMALL.png" id="S1.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="138" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> Error Rate Comparison. Errors types were Substitution(blue), Addition(yellow),Omission(green),Failed(red), Sub-Om (purple), Sub-Add(teal),Om-Add(grey),S-O-A(all three,pink). Percent of each error made per word by simulation(left) versus humans (right) in A. NH condition and B. the CI condition. C. The percent of correctly identified phonemes at all noise levels by (top) the network and (bottom) human subjects. All comparisons were made to data from <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. </figcaption>
</figure>
<figure id="S1.F4" class="ltx_figure"><img src="/html/2407.20535/assets/fin_figures/fig3_v2_small.png" id="S1.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="185" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Reaction Time Comparison. A. CDF of reaction times for all phonemes for CI(red), NH(blue), confused (dashed), non-confused(solid). B.D. from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. B Time to fixate to image of heard word for NH than CI. C. Reaction time for confused (left) versus non-confused(right) phonemes in CI(black) and NH(grey) conditions for the simulations with increasing noise level. D. Time to fixate image for foils -cohort (words with a similar starting phoneme e.g. wizard/whistle) and rhyme for NH and CI subjects. E. CI subject reaction time for certain and uncertain word predictions in quiet(black) and noise(red) from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. F. Reaction time of model for vowels(red) versus consonants(blue) for non-confused phonemes in quiet(dark) and medium noise(light) G. Reaction time for NH humans for vowel or consonant identification in quiet and noise (4-talker babble) from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. </figcaption>
</figure>
<figure id="S1.F5" class="ltx_figure"><img src="/html/2407.20535/assets/fin_figures/fig5_fin.png" id="S1.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="480" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Differences in Dynamics during Confusion and Non-confusion. A. Model activity can be parsed into time from phoneme onset to phoneme prediction per phoneme. B. Numbered layers in the network as referenced in C-G. C. Layer 11 activations for non-confusion of ‘NG’(green) (left) and confusion with ‘N’ (red). Other phoneme-related activations shown in various colors with thinner lines. The no prediction signal(black) dips during phoneme onset (green circle) and phoneme prediction(red circle). D. Raw activation in Layer 11 (left) versus utterance windows interpolated to the same length (40 model time points/400 ms). E. Z-scored distance in PC space between dynamics when phonemes are NC colored purple to light green by depth of layer in the model for NH inputs. F. Distance in PC space between dynamics during processing of NH (top) and CI (bottom) inputs during utterances that were C-P(red),C-NP(blue), NC-NP(green), and NC-P(yellow). G. Change in amplitude and latency of the peak response time with increase in noise (quiet-sand, low-peach,medium-purple) for Layer 2 and Layer 10 which have different average latencies of response. H. ERPs from whole-brain human EEG to words in quiet, stationary noise, or modulated noise in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. N1 and P2 times were found at about 130 and 250 ms delays. I. Amplitude and latency of ERP peak response under quiet and noise conditions for NH and CI listeners.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Phoneme DeepSpeech2 Model (PhoDe)</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">We used a DeepSpeech2 architecture trained end-to-end to convert spectrograms to English phonemes using CTC Loss <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>(Figure 1A). Our model consisted of five causal LSTM layers, a fully-connected layer, and a softmax layer with batch normalization following each LSTM layer(Figure 1A, Supp. Table 1).</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Constructing Cochlear Implant-like Inputs</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Vocoders have been developed as a research tool for imitating the distortion of audio experienced by CI users due to the limitations of CI hardware, such as limited electrode count, frequency shift, and distortions due to current spread <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. For the CI version of each input, audio was processed using the Advanced Bionic Generic Toolbox front-end processing to produce the stimulation per electrode channel (electrodogram)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. Then, a biophysical model followed by a filterbank-based vocoder was used to reconstruct an equivalent CI audio, and the same spectrogram procedure was used to make an equivalent CI version of each test audio sequence in the 10,028 sentence set(Figure 1B).</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Dataset &amp; Training</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Training and evaluation data consisted of 64-channel spectrograms created from natural speech from the LibriSpeech <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> dataset. LibriSpeech includes 1000 hours of speech read by male and female speakers. The model was trained with supervision to predict phonemes from 280,000 sentences. The DeepSpeech2 model was trained only on normal hearing spectrograms; thus, recapitulating behavior on vocoded CI spectrograms requires a degree of out-of-distribution generalization. Speech recordings were either in quiet (unaugmented) or augmented using the Sound eXchange (SoX) backend from torchaudio with background noise, reverberations, frequency masking and stretching and pitch shifting inputs. To create different noise levels (low, medium, and high), we increase the range of parameters of each of these augmentations (Supp. Table 2). To improve model generalization, the model was trained with quiet and low-level augmentations.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Alignment of Model Predictions and Utterance Window Isolation</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">The alignment of spoken and predicted phonemes was important for two reasons. Patterns of error confusion were used as a metric of comparison to human data, and, after alignment, the window between phoneme speech onset and prediction time of the phoneme or the confused phoneme was used to analyze differences in dynamics during neural processing of the phoneme. We use the spoken phoneme order without spaces as a target sequence and the output of our model without spaces or blanks as the predicted sequence. The Levenshtein algorithm <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> was used to add insertions to both sequences to best align them based on the chosen phoneme.
The utterance window, the time between phoneme onset from the audio segmentation and prediction of a phoneme was recursively isolated for each phoneme to use for analysis of dynamics for phonemes not paired with an insertion. Due to a combination of hallucinations, omissions, and confusions, alignment by the Levenshtein algorithm alone did not recapitulate the spoken and predicted pairings. Thus, a secondary correction algorithm was used to ensure the spoken phoneme from the alignment precedes the predicted phoneme, which moved insertion locations if this was not the case. In an increasing number of sentences, as noise was introduced, predictions began to proceed target times. Sentences in which this occurred for the NH or CI condition were excluded from further analyses.</p>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span> Latency and Amplitude of Dynamics </h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">To assess for dynamical signatures of preserved encoding, we analyze dynamics per layer of the model in the time between phoneme onset and prediction, as this is the window in which information must be encoded and interpreted by the network.
Evoked response potentials(ERPs), changes in electroencephalogram (EEG) after a stimulus onset have been characterized for a variety of sensory processing tasks and cognitive disorders; changes in amplitude and latency of response have been linked to differences in sensory processing <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. Changes in ERPs are thought to reflect synchronous changes in postsynaptic potentials that occur within a large population of local pyramidal neurons <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> and therefore reflect local processing. To get equivalent ERPs for comparisons, studies are typically designed with identical duration stimuli. Here, we time-lock to phoneme onset and interpolate the response to each phoneme to get an equivalent window. Distance between dynamics in PC space over time is used as a measure of the local change in activity. The time when the distance is the smallest is considered the time of peak responses. Then, latency and amplitude of the maximum change in response compared to the first 3 time points of the utterance window (baseline) is calculated in each later. These measures are compared to human EEG response data <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We conduct experiments on PhoDe to determine whether its performance on naturalistic and CI-like inputs is similar to normal hearing(NH) subjects and CI users performing analogous experiments. We find that our model can recapitulate a similar proportion of errors, changes in reaction time, and confusion patterns with NH versus CI inputs and as the noise level of the input increases. Moreover, we inspect this model to unpack temporal processing per network layer. We find a time-locked response in each model layer that shares similarities to human ERPs. This input changes with context before the phoneme, confusion of phoneme, and the input type and noise level. We find increases in latency and reductions in amplitude of responses in the network activity, like those observed in human EEG, when subjects respond to CI inputs with increasing noise levels compared to naturalistic inputs in quiet. We hypothesize these network signals indicate the correct encoding of phoneme identity and conclude that we can use them to optimize cochlear implant encoding strategies for future improvement of stimulation algorithms.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Comparison to Human Speech Perception</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">A unique feature of PhoDe is that it not only is capable of processing speech inputs in the form of spectrograms like prior DNNs<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> but also that due to its more biophysically realistic architecture, it is constrained to processes inputs like human listeners, causally overtime. Thus, the model is expected to show contextual effects processing effects similar to humans. Training PhoDe to predict phonemes allows us to compare errors to human studies at the phoneme-level, often considered the smallest unit of speech, up to the word-, sentence-, and semantic-level. Here, we compare model performance to three aspects of human performance: (1) the pattern of phonemes confusions, (2) the frequency and types of errors when processing words, and (3) reaction time.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Similarity in Phoneme Confusion Pattern with Noise and under CI Conditions</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">Given PhoDe predicts speech at the phoneme-level, the most direct comparison that can be made is phoneme confusion across all sentences. The target, the sequence of phonemes spoken, was aligned to the predicted phonemes over time, as described in Methods 2.4. Then, phoneme confusion across the same test sentences could be measured in the NH and CI conditions with increasing noise levels. We compared our results for NH conditions against Cutler et al. (2004) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. For the CI condition, we compare vowel confusion to Munson and Nelson(2003)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> and consonant confusion to Incerti et al. (2011)<cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> (Figure 2, Supplemental Figure 1). Across NH human studies, there is a baseline level of variability due to differences in tests and individual variability; differences only increase in CI test conditions where there is an added variability due to differences in experience with the implant, CI coding strategies, and hearing impairment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. Thus, we aim to determine whether prominent confusions are shared with prominent ones in population confusion matrices, although those may also differ across studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">We find some of the main confusions are present and the effect of a CI input and noise resembles human data. less confusion of ‘S’,’R’,’K’, and ‘T’ occurs, and ‘K’/’T’,‘L’/’M’, ‘M’/’N’, and ‘P’,’T’ confusion patterns are common, like human data<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. ‘AH’ and ‘UH’ confusions are also common in our model like in human data<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. We see a bias towards the prediction of ‘T’,’S’, and ‘AH’ that is not apparent in human data. This may derive from learned statistics of language. Like human performance, added noise does not change prominent confusion patterns; it only amplifies existing confusions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> (Supp. Fig. 1). With CI inputs, like human data, the most amplified confusions are ‘TH’/’DH’/’SH’ and /’L’/’M’/’N’/’NG’ for consonants and ‘AA’/’AH’/’AO’ for vowels which are also present in human data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>; ‘AH’ and ‘UH’ confusions also become more prominent in the CI case like human data (Figure 2A-B, Supp. Fig. 1).</p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p">We find statistically significant similarity between human and simulated confusion patterns across NH and CI simulations and with increasing noise levels(Figure 2C-D, Supp. Fig. 3). Correlations between 0.2 to 0.45 of the diagonals of the normalized confusion matrices indicate similar relative confusion of phonemes (shuffle statistic, p &lt;0.001, Figure 2C-D). KL divergence of the off-diagonals per phoneme and correlation of the off-diagonals were also statistically significant, indicating similar pattern of confusion with other phonemes per phoneme in NH and CI conditions (shuffle statistic, p &lt;0.001, Figure 2C-D).</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Capturing Error Patterns During Phoneme Recognition in Words </h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">Because the model is trained to produce sequences of phonemes, we can evaluate errors at the sentence- and word-level. We compare the pattern of errors to those from the Chun et al.(2015) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> experiment, which measured the number of phoneme errors per individual tri-phoneme monosyllables in increasing levels of noise in NH and CI conditions. (Figure 2A).</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">We find similar patterns of errors by PhoDe for tri-phoneme words in test sentences. The main type of error was substitution(blue) for NH, like in the human data; as noise increased, the relative amount of omission (green), failed(red) and sub-om(purple) errors increased, while the percent of additions(yellow) and substitutions(blue) decreased; a relatively smaller number of Sub-Adds(teal) also increased with noise (Figure 3A).
For CI inputs, our model showed a relatively lower portion of substitutions than NH inputs, like the data, and the number of failed errors increased with noise, while the number of sub-om, and additions decreased with noise. Additions also made up a relatively smaller percentage of the errors overall for CI inputs (Figure 3B).Overall, the percentage of errors increases with noise for both CI and NH inputs, and errors in the CI case were substantially higher than NH errors across all noise levels (Figure 3C).</p>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.1" class="ltx_p">Potentially, because our model processes words in continuous speech instead of individual words, we see a higher level of mixed errors even in quiet than in the human study. Our model overall also showed more omissions than human data, which may contribute to how low substitution errors are relative to other errors. We note that the percentage of NH and CI errors in the model was consistent with Finke et al.(2016)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> where word error per sentence in continuous speech was measured (Supp. Fig. 2A).</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Capturing Change in Reaction Time with Noise and under Cochlear Implant Conditions </h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">One unique benefit of PhoDe is how it is constrained to process inputs causally, like humans. So, we hypothesize that behavioral metrics, such as reaction time, which have been shown to increase with confusion and difficulty of task in humans may also increase in our model. We operationalize reaction time(RT) as the time from phoneme onset to prediction of a phoneme for each phoneme in the test set for our model. We can then measure RT in the CI(red) and NH(blue) condition and during confusion(dashed) versus non-confusion(solid)(Figure 4A, Supp. Fig. 2B).</p>
</div>
<div id="S3.SS1.SSS3.p2" class="ltx_para">
<p id="S3.SS1.SSS3.p2.1" class="ltx_p">The CDF of RT is compared to human time to fixation of a visual image of a spoken word from McMurray et al. (2017)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> (Figure 4B). Like human data, we see faster RT to NH inputs than CI inputs. During confusion(C) and non-confusion(NC), RT to NH inputs is faster than CI inputs, and RT to NC inputs is faster than C inputs(Figure 4C). As noise increases, RT increases for NH and CI conditions (Figure 4A,C). This emulates data from McMurray at al. that shows for foils (confused target) RT is faster for NH than CI (Figure 4C, left). In Winn and Teece (2002)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, CI users also performed word recognition in sentences in quiet and noise, and RT significantly increased with noise and was slower when subjects were uncertain or confused in both conditions. This is also consistent with model performance. Note, in a similar study, Finke et al.(2016), CI RTs were significantly higher but the increase in RT with noise was not significant(Supp. Fig. 2), so the strength of the noise effect varies with the task. Finally, we determine if vowel and consonant RT differences are present in the model. Human studies show RT to consonants is more affected by noise, and RTs for consonants are higher than for vowels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> (Figure 4F bottom). We see both of these effects when looking at the five vowels and consonants with the fastest RTs(Figure 4F top). In the full phoneme set, consonant RT were still more affected by noise than vowel RTs, but some baseline vowel RTs were higher than consonant RTs(Supp. Fig. 2 C,E). Overall, these results show PhoDe shares phoneme-specific RT effects with humans under NH and CI conditions (faster RT in NC, faster RT for NH inputs, and relatively stronger effects of noise on consonant RTs), indicating many similarities in processing of phonemes over time.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Identifying ERP-like Dynamical Signatures of Phoneme Recognition</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.5" class="ltx_p">PhoDe shares several performance effects with human subjects and is processing inputs that share many features of natural and CI inputs to the auditory system. Thus, we assess whether there is activity within the model that indicates successful encoding of speech information with artificial inputs. To do so, we isolate the time between phoneme speech onset, as determined by the speech aligner, and the time of phoneme prediction, which we call the utterance window during confusion(C) and non-confusion/comprehension (NC) of phonemes (Figure 5A-C). All utterance windows were interpolated to a fixed length and z-scored to account for differences in layer size(Figure 5D). Then, we could assess for delays and amplitude changes in fixed response windows. Dynamics in each layer become most similar, as measured by distance in PC space, at distinct times after phoneme onset. The time at which dynamics became most similar was <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="t_{peak}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">t</mi><mrow id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.3.1" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.3.1a" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.1.m1.1.1.3.4" xref="S3.SS2.p1.1.m1.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.1.3.1b" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.1.m1.1.1.3.5" xref="S3.SS2.p1.1.m1.1.1.3.5.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">𝑡</ci><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><times id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3.1"></times><ci id="S3.SS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p1.1.m1.1.1.3.2">𝑝</ci><ci id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3">𝑒</ci><ci id="S3.SS2.p1.1.m1.1.1.3.4.cmml" xref="S3.SS2.p1.1.m1.1.1.3.4">𝑎</ci><ci id="S3.SS2.p1.1.m1.1.1.3.5.cmml" xref="S3.SS2.p1.1.m1.1.1.3.5">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">t_{peak}</annotation></semantics></math>. <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="t_{peak}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">t</mi><mrow id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.p1.2.m2.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.3.1" xref="S3.SS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.2.m2.1.1.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.3.1a" xref="S3.SS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.2.m2.1.1.3.4" xref="S3.SS2.p1.2.m2.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.3.1b" xref="S3.SS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.2.m2.1.1.3.5" xref="S3.SS2.p1.2.m2.1.1.3.5.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝑡</ci><apply id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3"><times id="S3.SS2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3.1"></times><ci id="S3.SS2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.2">𝑝</ci><ci id="S3.SS2.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3">𝑒</ci><ci id="S3.SS2.p1.2.m2.1.1.3.4.cmml" xref="S3.SS2.p1.2.m2.1.1.3.4">𝑎</ci><ci id="S3.SS2.p1.2.m2.1.1.3.5.cmml" xref="S3.SS2.p1.2.m2.1.1.3.5">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">t_{peak}</annotation></semantics></math> shifts back in time with the depth in the network (Figure 5E). Additionally, we find that <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="t_{peak}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">t</mi><mrow id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.p1.3.m3.1.1.3.2" xref="S3.SS2.p1.3.m3.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.1.3.1" xref="S3.SS2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.3.m3.1.1.3.3" xref="S3.SS2.p1.3.m3.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.1.3.1a" xref="S3.SS2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.3.m3.1.1.3.4" xref="S3.SS2.p1.3.m3.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.1.3.1b" xref="S3.SS2.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.3.m3.1.1.3.5" xref="S3.SS2.p1.3.m3.1.1.3.5.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">𝑡</ci><apply id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3"><times id="S3.SS2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.p1.3.m3.1.1.3.1"></times><ci id="S3.SS2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.p1.3.m3.1.1.3.2">𝑝</ci><ci id="S3.SS2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3">𝑒</ci><ci id="S3.SS2.p1.3.m3.1.1.3.4.cmml" xref="S3.SS2.p1.3.m3.1.1.3.4">𝑎</ci><ci id="S3.SS2.p1.3.m3.1.1.3.5.cmml" xref="S3.SS2.p1.3.m3.1.1.3.5">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">t_{peak}</annotation></semantics></math> is modulated by whether the input was C or NC and contextually whether the phoneme was probable(P) or not probable(NP). P phonemes (yellow and red) converging in dynamics sooner and NC-P dynamics reaching minimal distance in all layers across utterances (Figure 5F, Supp. Fig. 3B). The latency of <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="t_{peak}" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><msub id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">t</mi><mrow id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.p1.4.m4.1.1.3.2" xref="S3.SS2.p1.4.m4.1.1.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.4.m4.1.1.3.1" xref="S3.SS2.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.4.m4.1.1.3.3" xref="S3.SS2.p1.4.m4.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.4.m4.1.1.3.1a" xref="S3.SS2.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.4.m4.1.1.3.4" xref="S3.SS2.p1.4.m4.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.4.m4.1.1.3.1b" xref="S3.SS2.p1.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.4.m4.1.1.3.5" xref="S3.SS2.p1.4.m4.1.1.3.5.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">𝑡</ci><apply id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3"><times id="S3.SS2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.p1.4.m4.1.1.3.1"></times><ci id="S3.SS2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.p1.4.m4.1.1.3.2">𝑝</ci><ci id="S3.SS2.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3.3">𝑒</ci><ci id="S3.SS2.p1.4.m4.1.1.3.4.cmml" xref="S3.SS2.p1.4.m4.1.1.3.4">𝑎</ci><ci id="S3.SS2.p1.4.m4.1.1.3.5.cmml" xref="S3.SS2.p1.4.m4.1.1.3.5">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">t_{peak}</annotation></semantics></math> and amplitude of deviance from baseline activity (<math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="t=0-30" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mrow id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">t</mi><mo id="S3.SS2.p1.5.m5.1.1.1" xref="S3.SS2.p1.5.m5.1.1.1.cmml">=</mo><mrow id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml"><mn id="S3.SS2.p1.5.m5.1.1.3.2" xref="S3.SS2.p1.5.m5.1.1.3.2.cmml">0</mn><mo id="S3.SS2.p1.5.m5.1.1.3.1" xref="S3.SS2.p1.5.m5.1.1.3.1.cmml">−</mo><mn id="S3.SS2.p1.5.m5.1.1.3.3" xref="S3.SS2.p1.5.m5.1.1.3.3.cmml">30</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><eq id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1.1"></eq><ci id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2">𝑡</ci><apply id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3"><minus id="S3.SS2.p1.5.m5.1.1.3.1.cmml" xref="S3.SS2.p1.5.m5.1.1.3.1"></minus><cn type="integer" id="S3.SS2.p1.5.m5.1.1.3.2.cmml" xref="S3.SS2.p1.5.m5.1.1.3.2">0</cn><cn type="integer" id="S3.SS2.p1.5.m5.1.1.3.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3.3">30</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">t=0-30</annotation></semantics></math> ms) are both modulated by increases in noise and whether the inputs in NH and CI (Figure 5G).</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">In human EEG studies, the ERP to words in continuous speech were recorded with increasing levels of noise (Figure 5H). The ERP showed differences in modulation of latency and amplitude of the N1(00) and P2(00) signal for NH and CI subject and with increasing noise. The changes closely resemble changes in activity at peak time in Layers 2 and 4 and Layers 8 and 10 respectively (Figure 5G, Supp. Fig. 3C). Both the N1 and P2 showed an increase in latency with noise and reduced amplitude; additionally latencies were longer for CI users across conditions and amplitudes were stronger. Differences in amplitude were more significant in P2 than in N1, and differences were more significant for N1(Figure 5H). These results are compatible with changes in PhoDe dynamics in Layers 2/4 and 8/10. The main difference we find is that differences in latencies are significant across conditions for our model(Supp. Fig. 3C). However, the magnitude of differences in latencies reduced in Layers 8/10, further reflecting the timing and modulation of human EEG activity during auditory processing.
These findings reveal a signature of confusion and non-confusion with similar timing for all phonemes. It shares similarities with human ERP N1s and P2s during NH and CI conditions. This may be usable as a marker of comprehension of spoken phonemes and therefore as an optimization target. We also find that differences arise in Layer 2 and propagate in the network(Supp. Fig. 3A and surrounding discussion) and find specific time windows for potential intervention in each layer that could be used for future work in optimizing inputs to maximize encoding of information.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Our findings support PhoDe as a potential clinical model for investigating how auditory information is processed over time in normal hearing and cochlear implant conditions. Having the ability to model electrical stimulation-based inputs throughout the auditory processing hierarchy over time in models performing complex tasks, such as speech recognition, may allow us to find neural signatures of speech comprehension that may be used to improve cochlear implant algorithm performance. Additionally, because of the similar encoding approach of cochlear implants and other devices, the understanding gained about network-level processing of these artificial inputs applies to other neural implants, such as retinal implants or deep brain stimulators.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">We have several limitations in our ability to model and compare to human experiments. There is variability in performance of NH subjects and CI users, especially in phoneme confusion, which has been attributed to differences in implant location, remaining inner ear health, age, and cognitive factors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. We also did not have a precise implant placement to replicate and pulled data from multiple studies of speech perception and the word and phoneme level that uses different types of noise. Thus, we cannot directly compare model performance. Additionally, our vocoder model is also not an accurate representation of CI inputs. In future work, we could replace the inputs with a more biophysical model of neural activations based on human CI placement maps. This paper aims to propose a model system to investigate differences in human processing of electrical and CI inputs and find signatures to use for optimizing population-level encoding with electrical inputs. We feel these limitations do not significantly affect our findings, although comparisons would likely improve with further biophysical accuracy.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments and Disclosure of Funding</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was supported by a grant from the Simons Foundation ( 965377 CRS). We thank Andrea Weber for sharing data from Cutler et al. (2004).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Dan Fink, Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy, and Babak Hodjat,
<span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Artificial Intelligence in the Age of Neural Networks and Brain Computing (Second Edition)</span>,
Part 3: Cutting-edge developments in deep learning and intelligent systems, 2024, DOI: 10.1016/b978-0-323-96104-2.00002-6, pages 269–287.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin,
<span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Attention Is All You Need</span>,
arXiv, 2017, DOI: 10.48550/arxiv.1706.03762, eprint: 1706.03762.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Guangyu Robert Yang and Xiao-Jing Wang,
<span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Artificial Neural Networks for Neuroscientists: A Primer</span>,
Neuron, 2020, ISSN: 0896-6273, DOI: 10.1016/j.neuron.2020.09.005, PMID: 32970997, pages 1048–1070.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Gavin Mischler, Yinghao Aaron Li, Stephan Bickel, Ashesh D. Mehta, and Nima Mesgarani,
<span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Contextual Feature Extraction Hierarchies Converge in Large Language Models and the Brain</span>,
arXiv, 2024, DOI: 10.48550/arxiv.2401.17671, eprint: 2401.17671.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Greta Tuckute, Jenelle Feather, Dana Boebinger, and Josh H. McDermott,
<span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Many but not all deep neural network audio models capture brain responses and exhibit correspondence between model stages and brain regions</span>,
PLOS Biology, 2023, ISSN: 1544-9173, DOI: 10.1371/journal.pbio.3002366, PMID: 38091351, PMCID: PMC10718467.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan, and Zhenyao Zhu,
<span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</span>,
arXiv, 2015, eprint: 1512.02595, archivePrefix: arXiv, primaryClass: cs.CL.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Menoua Keshishian, Sam V. Norman-Haignere, and Nima Mesgarani,
<span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Understanding Adaptive, Multiscale Temporal Integration In Deep Speech Recognition Systems</span>,
Advances in neural information processing systems, 2021, PMID: 38737583, PMCID: PMC11087060, pages 24455–24467.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Karthik Kumaravelu, Joseph Sombeck, Lee E. Miller, Sliman J. Bensmaia, and Warren M. Grill,
<span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Stoney vs. Histed: Quantifying the spatial effects of intracortical microstimulation</span>,
Brain Stimulation, 2022, volume 15, number 1, pages 141-151, DOI: 10.1016/j.brs.2021.11.015.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Boisvert, Isabelle and Reis, Mariana and Au, Agnes and Cowan, Robert and Dowell, Richard C.,
<span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Cochlear implantation outcomes in adults: A scoping review</span>,
PLoS ONE, 2020, volume 15, number 5, pages e0232421, DOI: 10.1371/journal.pone.0232421.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Zeng, Fan-Gang,
<span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Celebrating the one millionth cochlear implant</span>,
JASA Express Letters, 2022, volume 2, number 7, pages 077201, DOI: 10.1121/10.0012825.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Loeb, Gerald E.,
<span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Neural Prosthetics: A Review of Empirical vs. Systems Engineering Strategies</span>,
Applied Bionics and Biomechanics, 2018, volume 2018, pages 1435030, DOI: 10.1155/2018/1435030.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Corey J. Keller, Christopher J. Honey, Pierre Mégevand, Laszlo Entz, Istvan Ulbert, and Ashesh D. Mehta,
<span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Mapping human brain networks with cortico-cortical evoked potentials</span>,
Philosophical Transactions of the Royal Society B: Biological Sciences, 2014, volume 369, number 1653, pages 20130528, DOI: 10.1098/rstb.2013.0528, PMID: 25180306, PMCID: PMC4150303.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Cosetti, Maura K. and Waltzman, Susan B.,
<span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Cochlear implants: current status and future potential</span>,
Expert Review of Medical Devices, 2011, volume 8, number 3, pages 389-401, DOI: 10.1586/erd.11.12.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Nowik, Kamil and Langwińska-Wośko, Ewa and Skopiński, Piotr and Nowik, Katarzyna E. and Szaflik, Jacek P.,
<span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Bionic eye review – An update</span>,
Journal of Clinical Neuroscience, 2021, volume 78, pages 8-19, DOI: 10.1016/j.jocn.2020.05.041.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Boutros, Peter J. et al.,
<span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Continuous vestibular implant stimulation partially restores eye-stabilizing reflexes</span>,
JCI Insight, 2019, volume 4, number 22, DOI: 10.1172/jci.insight.128397.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Marquez-Chin, Cesar and Popovic, Milos R.,
<span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Functional electrical stimulation therapy for restoration of motor function after spinal cord injury and stroke: a review</span>,
BioMedical Engineering OnLine, 2020, volume 19, number 1, DOI: 10.1186/s12938-020-00773-4.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Khairuddin, Sharafuddin et al.,
<span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">A Decade of Progress in Deep Brain Stimulation of the Subcallosal Cingulate for the Treatment of Depression</span>,
Journal of Clinical Medicine, 2020, volume 9, number 10, pages 3260, DOI: 10.3390/jcm9103260.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Kalkman, Randy K. and Briaire, Jeroen J. and Frijns, Johan H. M.,
<span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Stimulation strategies and electrode design in computational models of the electrically stimulated cochlea: An overview of existing literature</span>,
Network: Computation in Neural Systems, 2016, volume 27, number 2-3, pages 107-134, DOI: 10.3109/0954898x.2016.1171412.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Mitchell, Diana E. et al.,
<span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Plasticity within non-cerebellar pathways rapidly shapes motor performance in vivo</span>,
Nature Communications, 2016, volume 7, number 1, DOI: 10.1038/ncomms11238.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Friesen, Lendra M. and Shannon, Robert V. and Baskent, Deniz and Wang, Xiaosong,
<span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Speech recognition in noise as a function of the number of spectral channels: Comparison of acoustic hearing and cochlear implants</span>,
The Journal of the Acoustical Society of America, 2001, volume 110, number 2, pages 1150-1163, DOI: 10.1121/1.1381538.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Fishman, Kim E. and Shannon, Robert V. and Slattery, William H.,
<span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Speech Recognition as a Function of the Number of Electrodes Used in the SPEAK Cochlear Implant Speech Processor</span>,
Journal of Speech, Language, and Hearing Research, 1997, volume 40, number 5, pages 1201-1215, DOI: 10.1044/jslhr.4005.1201.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Rødvik, Arne Kirkhorn and Torkildsen, Janne von Koss and Wie, Ona Bø and Storaker, Marit Aarvaag and Silvola, Juha Tapio,
<span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Consonant and Vowel Identification in Cochlear Implant Users Measured by Nonsense Words: A Systematic Review and Meta-Analysis</span>,
Journal of Speech, Language, and Hearing Research, 2018, volume 61, number 4, pages 1023-1050.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Philpott, Nikki and Philips, Birgit and Tromp, Kayla and Kramer, Sophia and Mylanus, Emmanuel and Huinck, Wendy,
<span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Phoneme Training for Adult Cochlear Implant Users: A Review of the Literature and Study Protocol</span>,
Journal of Speech, Language, and Hearing Research, 2023, volume 66, number 12, pages 5071-5086.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Holt, Colleen M. and Demuth, Katherine and Yuen, Ivan,
<span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">The Use of Prosodic Cues in Sentence Processing by Prelingually Deaf Users of Cochlear Implants</span>,
Ear and Hearing, 2016, volume 37, number 4, pages e256-e262, DOI: 10.1097/aud.0000000000000253.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Shannon, Robert V. and Cruz, Rachel J. and Galvin, John J.,
<span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Effect of Stimulation Rate on Cochlear Implant Users’ Phoneme, Word and Sentence Recognition in Quiet and in Noise</span>,
Audiology and Neurotology, 2011, volume 16, number 2, pages 113-123, DOI: 10.1159/000315115.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Hyungi Chun, Sunmi Ma, Woojae Han, and Youngmyoung Chun,
<span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Error Patterns Analysis of Hearing Aid and Cochlear Implant Users as a Function of Noise</span>,
Journal of Audiology &amp; Otology, 2015, ISSN: 2384-1621, DOI: 10.7874/jao.2015.19.3.144, PMID: 26771013, PMCID: PMC4704547, pages 144–153.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Cutler, Anne and Weber, Andrea and Smits, Roel and Cooper, Nicole,
<span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Patterns of English phoneme confusions by native and non-native listeners</span>,
The Journal of the Acoustical Society of America, 2004, volume 116, number 6, pages 3668-3678, DOI: 10.1121/1.1810292.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Meyer, Julien and Dentel, Laure and Meunier, Fanny,
<span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Speech Recognition in Natural Background Noise</span>,
PLoS ONE, 2013, volume 8, number 11, pages e79279, DOI: 10.1371/journal.pone.0079279.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Wang, Xianhui and Xu, Li,
<span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Speech perception in noise: Masking and unmasking</span>,
Journal of Otology, 2021, volume 16, number 2, pages 109-119, DOI: 10.1016/j.joto.2020.12.001.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Ng, Patrick R. and Bush, Alan and Vissani, Matteo and McIntyre, Cameron C. and Richardson, Robert Mark,
<span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Biophysical Principles and Computational Modeling of Deep Brain Stimulation</span>,
Neuromodulation: Technology at the Neural Interface, 2023, DOI: 10.1016/j.neurom.2023.04.471.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Lempka, Scott F. and Zander, Hans J. and Anaya, Carlos J. and Wyant, Alexandria and IV, John G. Ozinga and Machado, Andre G.,
<span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Patient-Specific Analysis of Neural Activation During Spinal Cord Stimulation for Pain</span>,
Neuromodulation: Technology at the Neural Interface, 2020, volume 23, number 5, pages 572-581, DOI: 10.1111/ner.13037.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Hayden, Russell and Sawyer, Stacia and Frey, Eric and Mori, Susumu and Migliaccio, Americo A. and Santina, Charles C. Della,
<span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Virtual labyrinth model of vestibular afferent excitation via implanted electrodes: validation and application to design of a multichannel vestibular prosthesis</span>,
Experimental Brain Research, 2011, volume 210, number 3-4, pages 623-640, DOI: 10.1007/s00221-011-2599-x.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Caucheteux, Charlotte and King, Jean-Rémi,
<span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Brains and algorithms partially converge in natural language processing</span>,
Communications Biology, 2022, volume 5, number 1, pages 134, DOI: 10.1038/s42003-022-03036-1.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-Rémi,
<span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Evidence of a predictive coding hierarchy in the human brain listening to speech</span>,
Nature Human Behaviour, 2023, volume 7, number 3, pages 430-441, DOI: 10.1038/s41562-022-01516-2.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Goldstein, Ariel and Zada, Zaid and Buchnik, Eliav and Schain, Mariano and Price, Amy and Aubrey, Bobbi and Nastase, Samuel A. and Feder, Amir and Emanuel, Dotan and Cohen, Alon and Jansen, Aren and Gazula, Harshvardhan and Choe, Gina and Rao, Aditi and Kim, Catherine and Casto, Colton and Fanda, Lora and Doyle, Werner and Friedman, Daniel and Dugan, Patricia and Melloni, Lucia and Reichart, Roi and Devore, Sasha and Norman, Kenneth A. and Devinsky, Orrin and Hasson, Uri,
<span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Shared computational principles for language processing in humans and deep language models</span>,
Nature Neuroscience, 2022, volume 25, number 3, pages 369-380, DOI: 10.1038/s41593-022-01026-4.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Li, Yuanning and Anumanchipalli, Gopala K. and Mohamed, Abdelrahman and Chen, Peili and Carney, Laurel H. and Lu, Junfeng and Wu, Jinsong and Chang, Edward F.,
<span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Dissecting neural computations in the human auditory pathway using deep neural networks for speech</span>,
Nature Neuroscience, 2023, volume 26, number 12, pages 2213-2225, DOI: 10.1038/s41593-023-01468-4.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Lerner, Yulia and Honey, Christopher J. and Silbert, Lauren J. and Hasson, Uri,
<span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Topographic Mapping of a Hierarchy of Temporal Receptive Windows Using a Narrated Story</span>,
The Journal of Neuroscience, 2011, volume 31, number 8, pages 2906-2915, DOI: 10.1523/jneurosci.3684-10.2011.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Johnson, Luke A. and Santina, Charles C. Della and Wang, Xiaoqin,
<span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Selective Neuronal Activation by Cochlear Implant Stimulation in Auditory Cortex of Awake Primate</span>,
The Journal of Neuroscience, 2016, volume 36, number 49, pages 12468-12484, DOI: 10.1523/jneurosci.1699-16.2016.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Adkisson, Paul W and Steinhardt, Cynthia R and Fridman, Gene Y,
<span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">Galvanic vs. pulsatile effects on decision-making networks: reshaping the neural activation landscape</span>,
Journal of Neural Engineering, 2024, volume 21, number 2, pages 026021, DOI: 10.1088/1741-2552/ad36e2.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Adkisson, Paul and Fridman, Gene Y. and Steinhardt, Cynthia R.,
<span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">Difference in Network Effects of Pulsatile and Galvanic Stimulation**Research supported by NIH R01NS110893 Grant.</span>,
2022 44th Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC), DOI: 10.1109/embc48229.2022.9871812.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Feather, Jenelle and Leclerc, Guillaume and Mądry, Aleksander and McDermott, Josh H.,
<span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">Model metamers reveal divergent invariances between biological and artificial neural networks</span>,
Nature Neuroscience, 2023, volume 26, number 11, pages 2017-2034, DOI: 10.1038/s41593-023-01442-0.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Chien, Hsiang-Yun Sherry and Honey, Christopher J.,
<span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">Constructing and Forgetting Temporal Context in the Human Cerebral Cortex</span>,
bioRxiv, 2019, DOI: 10.1101/761593.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Steinhardt, C.R., Mitchell, D.E., Cullen, K.E., et al.,
<span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">Pulsatile electrical stimulation creates predictable, correctable disruptions in neural firing</span>,
Nature Communications, 2024, volume 15, pages 5861,
DOI: 10.1038/s41467-024-49900-y.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
McMurray, Bob and Farris-Trimble, Ashley and Rigler, Hannah,
<span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">Waiting for lexical access: Cochlear implants or severely degraded input lead listeners to process speech less incrementally</span>,
Cognition, 2017, volume 169, pages 147-164, DOI: 10.1016/j.cognition.2017.08.013.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Winn, Matthew B and Teece, Katherine H,
<span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">Effortful listening despite correct responses: the cost of mental repair in sentence recognition by listeners with cochlear implants</span>,
Journal of Speech, Language, and Hearing Research, 2022, volume 65, number 10, pages 3966-3980.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Koerner, Tess K. and Zhang, Yang and Nelson, Peggy B. and Wang, Boxiang and Zou, Hui,
<span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">Neural indices of phonemic discrimination and sentence-level speech intelligibility in quiet and noise: A mismatch negativity study</span>,
Hearing Research, 2016, volume 339, pages 40-49, DOI: 10.1016/j.heares.2016.06.001.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev,
<span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">Librispeech: An ASR Corpus Based on Public Domain Audio Books</span>,
2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5206-5210, DOI: 10.1109/ICASSP.2015.7178964.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Karoui, Chadlia and James, Chris and Barone, Pascal and Bakhos, David and Marx, Mathieu and Macherey, Olivier,
<span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">Searching for the Sound of a Cochlear Implant: Evaluation of Different Vocoder Parameters by Cochlear Implant Users With Single-Sided Deafness</span>,
Trends in Hearing, volume 23, 2019, DOI: 10.1177/2331216519866029.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Jabeim,
<span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">AB-Generic-Python-Toolbox</span>,
GitHub repository, GitHub, 2024, <a target="_blank" href="https://github.com/jabeim/AB-Generic-Python-Toolbox" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/jabeim/AB-Generic-Python-Toolbox</a>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Finke, Mareike and Sandmann, Pascale and Bönitz, Hanna and Kral, Andrej and Büchner, Andreas,
<span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">Consequences of Stimulus Type on Higher-Order Processing in Single-Sided Deaf Cochlear Implant Users</span>,
Audiology and Neurotology, volume 21, number 5, pages 305-315, 2017, DOI: 10.1159/000452123.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Rødvik, Arne Kirkhorn and von Koss Torkildsen, Janne and Wie, Ona Bø and Storaker, Marit Aarvaag and Silvola, Juha Tapio,
<span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">Consonant and vowel identification in cochlear implant users measured by nonsense words: A systematic review and meta-analysis</span>,
Journal of Speech, Language, and Hearing Research, volume 61, number 4, pages 1023–1050, 2018.
DOI: <a href="10.1044/2018_jslhr-h-16-0463" title="" class="ltx_ref ltx_url ltx_font_typewriter">10.1044/2018_jslhr-h-16-0463</a>

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Valimaa, Taina T. and Maatta, Taisto K. and Lopponen, Heikki J. and Sorri, Martti J.,
<span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">Phoneme Recognition and Confusions With Multichannel Cochlear Implants</span>,
Journal of Speech, Language, and Hearing Research, volume 45, number 5, pages 1055–1069, 2002.
DOI: <a href="10.1044/1092-4388(2002/085)" title="" class="ltx_ref ltx_url ltx_font_typewriter">10.1044/1092-4388(2002/085)</a>

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Levenshtein, VI,
<span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">Binary codes capable of correcting deletions, insertions, and reversals</span>,
Soviet Physics Doklady, volume 10, number 8, pages 707-710, 1966.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Sur, Shravani and Sinha, V. K.,
<span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">Event-related potential: An overview</span>,
Industrial Psychiatry Journal, volume 18, number 1, pages 70-73, 2009.
DOI: <a href="10.4103/0972-6748.57865" title="" class="ltx_ref ltx_url ltx_font_typewriter">10.4103/0972-6748.57865</a>

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Nunez, PL and Srinivasan, R,
<span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">Electric Fields of the Brain</span>,
Oxford University Press, 2006.
DOI: <a href="10.1093/acprof:oso/9780195050387.001.0001" title="" class="ltx_ref ltx_url ltx_font_typewriter">10.1093/acprof:oso/9780195050387.001.0001</a>

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Munson, Benjamin and Donaldson, Gail S. and Allen, Shanna L. and Collison, Elizabeth A. and Nelson, David A.,
<span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">Patterns of phoneme perception errors by listeners with cochlear implants as a function of overall speech perception ability</span>,
Journal of the Acoustical Society of America, volume 113, number 3, pages 925–935, 2003.
DOI: <a href="10.1121/1.1536630" title="" class="ltx_ref ltx_url ltx_font_typewriter">10.1121/1.1536630</a>

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Incerti, Paola and Ching, Teresa and Hill, Amanda,
<span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">Consonant Perception by Adults with Bimodal Fitting</span>,
Seminars in Hearing, volume 32, number 1, pages 90–102, 2011.
DOI: <a href="10.1055/s-0031-1271950" title="" class="ltx_ref ltx_url ltx_font_typewriter">10.1055/s-0031-1271950</a>

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Peter Ladefoged and Keith Johnson, <span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">A Course in Phonetics</span>, 3rd ed., Thomson Wadsworth, Boston, MA, 2006.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Additional Model Details</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">All models were implemented in PyTorch on the training set of the LibriSpeech corpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. All model layers contained 500 neurons, except for the last layer which contained 41 units related to the 39 English phonemes, the blank, and space token predictions. We used the Adam optimizer (learning rate: 1.5e-4, weight decay: 1e-5) and a batch size of 64. Training was of all models were performed on NVIDIA A40 and L40 GPUs (one per training) at the internal cluster of our organization. Each epochs of training took approximately 2.5 hours for each model, totaling 30 epochs with 8787 steps per epoch.</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span> Evaluation of Prediction Performance</h3>

<section id="A1.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.1 </span> Error Metrics</h4>

<div id="A1.SS2.SSS1.p1" class="ltx_para">
<p id="A1.SS2.SSS1.p1.1" class="ltx_p">To compare to the Chun et al(2015) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> experiment, phoneme errors present after final alignment were re-attributed to the word they were uttered in using the spoken word and phoneme segmentation information (Figure 2). Error rate was counted at the word-level, like in the study, where if only a substitution occurred in a word, this was considered one substitution error in the total count, but if a substitution and omission occurred, this was considered only substitution-omission error. The experiment used tri-phoneme words, so we restrict this measure only to words containing three phonemes (Figure 2A-B). Categories are self-explanatory, except for failed, meaning three errors on these three phoneme words.</p>
</div>
</section>
<section id="A1.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.2 </span>Reaction Time</h4>

<div id="A1.SS2.SSS2.p1" class="ltx_para">
<p id="A1.SS2.SSS2.p1.1" class="ltx_p">Using the utterance windows, we operationalize reaction time for each phoneme as the time from phoneme onset to the prediction time of the output in millisecond. For vowel and consonant reaction times the average length of a vowel or consonant utterance was subtracted from this difference as the reaction time to prediction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>.</p>
</div>
</section>
<section id="A1.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.3 </span> Human Phoneme Confusion Data Comparisons</h4>

<div id="A1.SS2.SSS3.p1" class="ltx_para">
<p id="A1.SS2.SSS3.p1.1" class="ltx_p">In order to quantify the extent to which our model captures the same pattern of errors as humans, comparisons were made to four different studies of English phoneme perception. The confusion matrices from Cutler et al. (2004) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> for normal hearing subjects listening to consonants and vowels at three noise levels (+6 dB, 0 dB, -6 dB) are used to compare to simulations at low, medium, and high noise for natural inputs (Supp. Table 2). Two cochlear implant studies of confusion were used. We use vowel confusion data from Munson and Nelson (2003) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> of speech in quiet for better listeners as a comparison to the simulation in quiet conditions. We also use consonant confusion from Incerti et al. (2011) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> of speech in quiet and with 8-talker babble as a comparison to our model processing CI inputs at quiet and medium noise levels.</p>
</div>
</section>
<section id="A1.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.4 </span>Confusion Matrix Similar Metrics</h4>

<div id="A1.SS2.SSS4.p1" class="ltx_para">
<p id="A1.SS2.SSS4.p1.1" class="ltx_p">For all phonemes that did not have an insertion in the final alignment the pattern of confusion could be measured. For comparison of simulation confusion to human confusion in each study, only the phonemes present in that study were included in the matrix. We use several similarity metrics. The correlation between the diagonal elements of the matrices only were used as a measure of the similarity in relative confusability of each phoneme. The correction of the off-diagonal only and the KL-Divergence per row were used as a measure of similarity in pattern of confusion per phoneme. Overall correlation and Manhattan distance were also used to measure overall performance similarities. All comparisons were made on the row-normalized matrices. A shuffle comparison was made with 1028 shuffles of paired rows to evaluate significance.</p>
</div>
</section>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Additional Model Interpretability Methods</h3>

<section id="A1.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.3.1 </span>Decoder of Intact Encoding of Spoken Phonemes over Time</h4>

<div id="A1.SS3.SSS1.p1" class="ltx_para">
<p id="A1.SS3.SSS1.p1.1" class="ltx_p">To assess the level of preserved information about the original audio input per layer, we use a linear decoder(SVD) to decode the spoken phoneme at each time point from the activation outputs of the model. We consider the phoneme as being present at each time point between the phoneme onset and offset based on segmentation. Decoding was performed per layer for 100 concatenated sentences with 10 80-20 cross-validated splits of the data.</p>
</div>
</section>
</section>
<section id="A1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Determine Context Effects from Previous Phonemes</h3>

<div id="A1.SS4.p1" class="ltx_para">
<p id="A1.SS4.p1.1" class="ltx_p">A bi-gram model was made by using the full LibriSpeech dataset to directly count statistic of occurrence. A phoneme was considered probable (P) if it was in the top 10 % of phonemes followed by the proceeding phoneme and not probable (NP) if it was in the remaining 90 %.</p>
</div>
</section>
<section id="A1.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>Constructing Comparable Dynamics Windows</h3>

<div id="A1.SS5.p1" class="ltx_para">
<p id="A1.SS5.p1.1" class="ltx_p">The utterance of each phoneme varies speaker-to-speaker and sentence-by-sentence. To make utterances comparable, the time of prediction of the model was found based on the time of peak of the predicted phoneme-associated neuron activations in Layer 11, which predicts a phoneme by producing a sharp increase in activity of one of the output-associated neurons. Then, the utterance time series was interpolated to a fixed length from two time points between the phoneme onset time to 0.2*(utterance length) time points to 0.25*(utterance length) time points after prediction time to produce a fixed length of 40 model time steps or 400 ms in real-time (Figure 5D). Phoneme utterances were categorized as confused(C) or non-confused(NC) and probable(P) or not probable(NP). Up to 50 exemplars of each category (C-P,C-NP,NC-P,NC-NP) were collected for each phoneme in the test set. Phoneme comparisons were not made if there were not at least 2 exemplars in each category, leaving 34 phoneme comparisons.</p>
</div>
</section>
<section id="A1.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.6 </span>PC Space Visualization Details</h3>

<div id="A1.SS6.p1" class="ltx_para">
<p id="A1.SS6.p1.1" class="ltx_p">To visualize and measure differences in dynamics in the network a principal component analysis (PCA) was used to project activations over time into a shared space. For dynamics comparisons, distance was measured in the full PC space dimensionality. Distance metrics were compared in the projection into the shared PC space of all NH and CI input responses that were C or NC in the test set. For visualizations of the difference in dynamics in NH and CI conditions in Figure 5F and the Supplemental Figures 3 and 4, data were projected into a 3-D space of the PC space for NH responses.</p>
</div>
<div id="A1.SS6.p2" class="ltx_para">
<p id="A1.SS6.p2.1" class="ltx_p">All code used for training the model and these analyses is available at https://github.com/ANONYMOUS</p>
</div>
<div id="A1.SS6.p3" class="ltx_para">
<p id="A1.SS6.p3.1" class="ltx_p">All figures with errorbars show the mean and S.E.M of the data.</p>
</div>
<figure id="A1.SS6.1" class="ltx_figure"><img src="/html/2407.20535/assets/fin_figures/supp_conf_2.png" id="A1.SS6.1.g1" class="ltx_graphics ltx_img_square" width="598" height="557" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">Supplemental Figure 1. Comparison of human(left) and simulated(right) NH (top) and CI(bottom) confusion matrices for A. consonants and B. vowels. during increasing noise levels. C. Overall matrix correction and Manhattan distance between human and simulation confusion matrices for original matrices(blue) in A versus paired shuffling of the simulation matrix for 500 shuffles (red).</figcaption>
</figure>
<figure id="A1.SS6.2" class="ltx_figure"><img src="/html/2407.20535/assets/fin_figures/fig3_supp.png" id="A1.SS6.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="338" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">Supplemental Figure 2. A. Percent correct performance in spoken sentences of the PhoDe (left) versus humans (right) during phoneme and word recognition task respectively. B. Human non-confusion reaction time data from Finke et al. (2016) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> for NH and CI users with increasing noise. C. Reaction time for vowels(red) and consonants(blue) when confused and non-confused plotted against each other for (top) NH and (bottom) CI inputs in (left) quiet and (right) high noise. D. CDF of reaction time of model on sentences in quiet and high noise levels, showing confused (dashed) versus non-confused (solid) for NH (blue) and CI (red) inputs. Colors increase in darkness with noise intensity. (Right) all NH responses (left) and all CI responses (right). E. Model reaction time for vowels (red) versus consonants(blue) during confusion or non-confusion for NH(left) and CI(right) inputs. (top) Results for all phonemes in each category. (bottom) Results for the top five shortest reaction times of phonemes per category. </figcaption>
</figure>
<figure id="A1.SS6.3" class="ltx_figure"><img src="/html/2407.20535/assets/fin_figures/supp_fig6.png" id="A1.SS6.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="440" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">Supplemental Figure 3. A. Decodability of inputs per layer of the network compared to the original input. RNN layers (yellow) then linear layers (blue). Decoding performance per layer for NH and CI inputs at increasing noise levels. Note (*) differences in Layer 2. B. Distance in PC space between all traces within a category overtime during the utterance window averaged across all phonemes. Layer number increases going down the graphs. NH responses (left) versus CI responses (right) in the NH PC space for C-P(red), C-NP(blue), NC-NP(green), and NC-P(yellow) traces. C. The amplitude (top) and latency (bottom) of the maximal change in response (which the distance between traces is most similar) per layer (left to right) and with increasing levels of noise (quiet - sand, low-peach, mid-purple). Unpaired-test significance shown as p&lt;0.05*,p&lt;0.01**, p&lt;0.001***.</figcaption>
</figure>
<figure id="A1.SS6.6" class="ltx_figure"><img src="/html/2407.20535/assets/fin_figures/supp_pca.png" id="A1.SS6.4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="451" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">Supplemental Figure 4. Projection of dynamics during vowel (<span id="A1.SS6.6.3.1" class="ltx_text" style="color:#FF0000;">red</span>) and consonants (<span id="A1.SS6.6.4.2" class="ltx_text" style="color:#0000FF;">blue</span>) processing per layer during the time point <math id="A1.SS6.5.1.m1.1" class="ltx_Math" alttext="t_{\text{peak}-1}" display="inline"><semantics id="A1.SS6.5.1.m1.1b"><msub id="A1.SS6.5.1.m1.1.1" xref="A1.SS6.5.1.m1.1.1.cmml"><mi id="A1.SS6.5.1.m1.1.1.2" xref="A1.SS6.5.1.m1.1.1.2.cmml">t</mi><mrow id="A1.SS6.5.1.m1.1.1.3" xref="A1.SS6.5.1.m1.1.1.3.cmml"><mtext id="A1.SS6.5.1.m1.1.1.3.2" xref="A1.SS6.5.1.m1.1.1.3.2a.cmml">peak</mtext><mo id="A1.SS6.5.1.m1.1.1.3.1" xref="A1.SS6.5.1.m1.1.1.3.1.cmml">−</mo><mn id="A1.SS6.5.1.m1.1.1.3.3" xref="A1.SS6.5.1.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="A1.SS6.5.1.m1.1c"><apply id="A1.SS6.5.1.m1.1.1.cmml" xref="A1.SS6.5.1.m1.1.1"><csymbol cd="ambiguous" id="A1.SS6.5.1.m1.1.1.1.cmml" xref="A1.SS6.5.1.m1.1.1">subscript</csymbol><ci id="A1.SS6.5.1.m1.1.1.2.cmml" xref="A1.SS6.5.1.m1.1.1.2">𝑡</ci><apply id="A1.SS6.5.1.m1.1.1.3.cmml" xref="A1.SS6.5.1.m1.1.1.3"><minus id="A1.SS6.5.1.m1.1.1.3.1.cmml" xref="A1.SS6.5.1.m1.1.1.3.1"></minus><ci id="A1.SS6.5.1.m1.1.1.3.2a.cmml" xref="A1.SS6.5.1.m1.1.1.3.2"><mtext mathsize="70%" id="A1.SS6.5.1.m1.1.1.3.2.cmml" xref="A1.SS6.5.1.m1.1.1.3.2">peak</mtext></ci><cn type="integer" id="A1.SS6.5.1.m1.1.1.3.3.cmml" xref="A1.SS6.5.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS6.5.1.m1.1d">t_{\text{peak}-1}</annotation></semantics></math> to <math id="A1.SS6.6.2.m2.1" class="ltx_Math" alttext="t_{\text{peak}+1}" display="inline"><semantics id="A1.SS6.6.2.m2.1b"><msub id="A1.SS6.6.2.m2.1.1" xref="A1.SS6.6.2.m2.1.1.cmml"><mi id="A1.SS6.6.2.m2.1.1.2" xref="A1.SS6.6.2.m2.1.1.2.cmml">t</mi><mrow id="A1.SS6.6.2.m2.1.1.3" xref="A1.SS6.6.2.m2.1.1.3.cmml"><mtext id="A1.SS6.6.2.m2.1.1.3.2" xref="A1.SS6.6.2.m2.1.1.3.2a.cmml">peak</mtext><mo id="A1.SS6.6.2.m2.1.1.3.1" xref="A1.SS6.6.2.m2.1.1.3.1.cmml">+</mo><mn id="A1.SS6.6.2.m2.1.1.3.3" xref="A1.SS6.6.2.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="A1.SS6.6.2.m2.1c"><apply id="A1.SS6.6.2.m2.1.1.cmml" xref="A1.SS6.6.2.m2.1.1"><csymbol cd="ambiguous" id="A1.SS6.6.2.m2.1.1.1.cmml" xref="A1.SS6.6.2.m2.1.1">subscript</csymbol><ci id="A1.SS6.6.2.m2.1.1.2.cmml" xref="A1.SS6.6.2.m2.1.1.2">𝑡</ci><apply id="A1.SS6.6.2.m2.1.1.3.cmml" xref="A1.SS6.6.2.m2.1.1.3"><plus id="A1.SS6.6.2.m2.1.1.3.1.cmml" xref="A1.SS6.6.2.m2.1.1.3.1"></plus><ci id="A1.SS6.6.2.m2.1.1.3.2a.cmml" xref="A1.SS6.6.2.m2.1.1.3.2"><mtext mathsize="70%" id="A1.SS6.6.2.m2.1.1.3.2.cmml" xref="A1.SS6.6.2.m2.1.1.3.2">peak</mtext></ci><cn type="integer" id="A1.SS6.6.2.m2.1.1.3.3.cmml" xref="A1.SS6.6.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS6.6.2.m2.1d">t_{\text{peak}+1}</annotation></semantics></math> per layer. Location shown in quiet, low noise, and medium noise going left to right, and from Layer 2 to 10 going down the plot. NH representations (left) compared to CI representations (right) when projected onto the PC space of NH responses only.</figcaption>
</figure>
</section>
<section id="A1.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.7 </span>Additional Experiments</h3>

<div id="A1.SS7.p1" class="ltx_para">
<p id="A1.SS7.p1.1" class="ltx_p">Using a linear decoder shows that after the first LSTM (Layer 2), a significant reduction in decodability of the CI inputs occurs, and noise more negatively affects decodability for CI inputs which is not recovered during deeper layer processing (Supp. Fig. 3). We also observe that in all layers of the network there is a shift in representational space of the phonemes that reduces phoneme separation in NH PC space for CI input (Supp. Fig. 4).</p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Supplemental Tables</h2>

<figure id="A2.tab1" class="ltx_table">
<figcaption class="ltx_caption">Supplemental Table 1: Layer specifications and dimensionality of principal component space with added noise in NH, CI and shared NH+CI conditions</figcaption>
<div id="A2.tab1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:182pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-84.6pt,35.4pt) scale(0.71926896971007,0.71926896971007) ;">
<table id="A2.tab1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.tab1.1.1.1.1" class="ltx_tr">
<th id="A2.tab1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A2.tab1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Layer Num</span></th>
<th id="A2.tab1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A2.tab1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Layer Type</span></th>
<th id="A2.tab1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span id="A2.tab1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Quiet</span></th>
<th id="A2.tab1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span id="A2.tab1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Low</span></th>
<th id="A2.tab1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span id="A2.tab1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Medium</span></th>
<th id="A2.tab1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span id="A2.tab1.1.1.1.1.6.1" class="ltx_text ltx_font_bold">High</span></th>
<th id="A2.tab1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.tab1.1.1.1.1.7.1" class="ltx_text ltx_font_bold"># Outputs</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.tab1.1.1.2.1" class="ltx_tr">
<th id="A2.tab1.1.1.2.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="A2.tab1.1.1.2.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row"></th>
<th id="A2.tab1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.tab1.1.1.2.1.3.1" class="ltx_text ltx_font_bold">NH</span></th>
<th id="A2.tab1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.tab1.1.1.2.1.4.1" class="ltx_text ltx_font_bold">CI</span></th>
<th id="A2.tab1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.tab1.1.1.2.1.5.1" class="ltx_text ltx_font_bold">NH+CI</span></th>
<th id="A2.tab1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.tab1.1.1.2.1.6.1" class="ltx_text ltx_font_bold">NH</span></th>
<th id="A2.tab1.1.1.2.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.tab1.1.1.2.1.7.1" class="ltx_text ltx_font_bold">CI</span></th>
<th id="A2.tab1.1.1.2.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.tab1.1.1.2.1.8.1" class="ltx_text ltx_font_bold">NH+CI</span></th>
<th id="A2.tab1.1.1.2.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.tab1.1.1.2.1.9.1" class="ltx_text ltx_font_bold">NH</span></th>
<th id="A2.tab1.1.1.2.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.tab1.1.1.2.1.10.1" class="ltx_text ltx_font_bold">CI</span></th>
<th id="A2.tab1.1.1.2.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.tab1.1.1.2.1.11.1" class="ltx_text ltx_font_bold">NH+CI</span></th>
<th id="A2.tab1.1.1.2.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.tab1.1.1.2.1.12.1" class="ltx_text ltx_font_bold">NH</span></th>
<th id="A2.tab1.1.1.2.1.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.tab1.1.1.2.1.13.1" class="ltx_text ltx_font_bold">CI</span></th>
<th id="A2.tab1.1.1.2.1.14" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="A2.tab1.1.1.2.1.14.1" class="ltx_text ltx_font_bold">NH+CI</span></th>
<td id="A2.tab1.1.1.2.1.15" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="A2.tab1.1.1.3.2" class="ltx_tr">
<th id="A2.tab1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">0</th>
<th id="A2.tab1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Input</th>
<td id="A2.tab1.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">12</td>
<td id="A2.tab1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">6</td>
<td id="A2.tab1.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t">18</td>
<td id="A2.tab1.1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_t">16</td>
<td id="A2.tab1.1.1.3.2.7" class="ltx_td ltx_align_center ltx_border_t">7</td>
<td id="A2.tab1.1.1.3.2.8" class="ltx_td ltx_align_center ltx_border_t">23</td>
<td id="A2.tab1.1.1.3.2.9" class="ltx_td ltx_align_center ltx_border_t">14</td>
<td id="A2.tab1.1.1.3.2.10" class="ltx_td ltx_align_center ltx_border_t">7</td>
<td id="A2.tab1.1.1.3.2.11" class="ltx_td ltx_align_center ltx_border_t">21</td>
<td id="A2.tab1.1.1.3.2.12" class="ltx_td ltx_align_center ltx_border_t">12</td>
<td id="A2.tab1.1.1.3.2.13" class="ltx_td ltx_align_center ltx_border_t">7</td>
<td id="A2.tab1.1.1.3.2.14" class="ltx_td ltx_align_center ltx_border_t">19</td>
<td id="A2.tab1.1.1.3.2.15" class="ltx_td ltx_align_center ltx_border_t">64</td>
</tr>
<tr id="A2.tab1.1.1.4.3" class="ltx_tr">
<th id="A2.tab1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">1</th>
<th id="A2.tab1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">LSTM1</th>
<td id="A2.tab1.1.1.4.3.3" class="ltx_td" colspan="12"></td>
<td id="A2.tab1.1.1.4.3.4" class="ltx_td ltx_align_center">500</td>
</tr>
<tr id="A2.tab1.1.1.5.4" class="ltx_tr">
<th id="A2.tab1.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2</th>
<th id="A2.tab1.1.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">Batch Norm</th>
<td id="A2.tab1.1.1.5.4.3" class="ltx_td ltx_align_center">84</td>
<td id="A2.tab1.1.1.5.4.4" class="ltx_td ltx_align_center">40</td>
<td id="A2.tab1.1.1.5.4.5" class="ltx_td ltx_align_center">124</td>
<td id="A2.tab1.1.1.5.4.6" class="ltx_td ltx_align_center">92</td>
<td id="A2.tab1.1.1.5.4.7" class="ltx_td ltx_align_center">43</td>
<td id="A2.tab1.1.1.5.4.8" class="ltx_td ltx_align_center">135</td>
<td id="A2.tab1.1.1.5.4.9" class="ltx_td ltx_align_center">95</td>
<td id="A2.tab1.1.1.5.4.10" class="ltx_td ltx_align_center">45</td>
<td id="A2.tab1.1.1.5.4.11" class="ltx_td ltx_align_center">140</td>
<td id="A2.tab1.1.1.5.4.12" class="ltx_td ltx_align_center">98</td>
<td id="A2.tab1.1.1.5.4.13" class="ltx_td ltx_align_center">46</td>
<td id="A2.tab1.1.1.5.4.14" class="ltx_td ltx_align_center">144</td>
<td id="A2.tab1.1.1.5.4.15" class="ltx_td ltx_align_center">500</td>
</tr>
<tr id="A2.tab1.1.1.6.5" class="ltx_tr">
<th id="A2.tab1.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">3</th>
<th id="A2.tab1.1.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">LSTM2</th>
<td id="A2.tab1.1.1.6.5.3" class="ltx_td" colspan="12"></td>
<td id="A2.tab1.1.1.6.5.4" class="ltx_td ltx_align_center">500</td>
</tr>
<tr id="A2.tab1.1.1.7.6" class="ltx_tr">
<th id="A2.tab1.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">4</th>
<th id="A2.tab1.1.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">Batch Norm</th>
<td id="A2.tab1.1.1.7.6.3" class="ltx_td ltx_align_center">105</td>
<td id="A2.tab1.1.1.7.6.4" class="ltx_td ltx_align_center">54</td>
<td id="A2.tab1.1.1.7.6.5" class="ltx_td ltx_align_center">159</td>
<td id="A2.tab1.1.1.7.6.6" class="ltx_td ltx_align_center">113</td>
<td id="A2.tab1.1.1.7.6.7" class="ltx_td ltx_align_center">55</td>
<td id="A2.tab1.1.1.7.6.8" class="ltx_td ltx_align_center">168</td>
<td id="A2.tab1.1.1.7.6.9" class="ltx_td ltx_align_center">114</td>
<td id="A2.tab1.1.1.7.6.10" class="ltx_td ltx_align_center">57</td>
<td id="A2.tab1.1.1.7.6.11" class="ltx_td ltx_align_center">171</td>
<td id="A2.tab1.1.1.7.6.12" class="ltx_td ltx_align_center">116</td>
<td id="A2.tab1.1.1.7.6.13" class="ltx_td ltx_align_center">58</td>
<td id="A2.tab1.1.1.7.6.14" class="ltx_td ltx_align_center">174</td>
<td id="A2.tab1.1.1.7.6.15" class="ltx_td ltx_align_center">500</td>
</tr>
<tr id="A2.tab1.1.1.8.7" class="ltx_tr">
<th id="A2.tab1.1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">5</th>
<th id="A2.tab1.1.1.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">LSTM3</th>
<td id="A2.tab1.1.1.8.7.3" class="ltx_td" colspan="12"></td>
<td id="A2.tab1.1.1.8.7.4" class="ltx_td ltx_align_center">500</td>
</tr>
<tr id="A2.tab1.1.1.9.8" class="ltx_tr">
<th id="A2.tab1.1.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">6</th>
<th id="A2.tab1.1.1.9.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">Batch Norm</th>
<td id="A2.tab1.1.1.9.8.3" class="ltx_td ltx_align_center">145</td>
<td id="A2.tab1.1.1.9.8.4" class="ltx_td ltx_align_center">11</td>
<td id="A2.tab1.1.1.9.8.5" class="ltx_td ltx_align_center">156</td>
<td id="A2.tab1.1.1.9.8.6" class="ltx_td ltx_align_center">161</td>
<td id="A2.tab1.1.1.9.8.7" class="ltx_td ltx_align_center">112</td>
<td id="A2.tab1.1.1.9.8.8" class="ltx_td ltx_align_center">273</td>
<td id="A2.tab1.1.1.9.8.9" class="ltx_td ltx_align_center">165</td>
<td id="A2.tab1.1.1.9.8.10" class="ltx_td ltx_align_center">114</td>
<td id="A2.tab1.1.1.9.8.11" class="ltx_td ltx_align_center">279</td>
<td id="A2.tab1.1.1.9.8.12" class="ltx_td ltx_align_center">167</td>
<td id="A2.tab1.1.1.9.8.13" class="ltx_td ltx_align_center">117</td>
<td id="A2.tab1.1.1.9.8.14" class="ltx_td ltx_align_center">284</td>
<td id="A2.tab1.1.1.9.8.15" class="ltx_td ltx_align_center">500</td>
</tr>
<tr id="A2.tab1.1.1.10.9" class="ltx_tr">
<th id="A2.tab1.1.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">7</th>
<th id="A2.tab1.1.1.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">LSTM4</th>
<td id="A2.tab1.1.1.10.9.3" class="ltx_td" colspan="12"></td>
<td id="A2.tab1.1.1.10.9.4" class="ltx_td ltx_align_center">500</td>
</tr>
<tr id="A2.tab1.1.1.11.10" class="ltx_tr">
<th id="A2.tab1.1.1.11.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">8</th>
<th id="A2.tab1.1.1.11.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">Batch Norm</th>
<td id="A2.tab1.1.1.11.10.3" class="ltx_td ltx_align_center">163</td>
<td id="A2.tab1.1.1.11.10.4" class="ltx_td ltx_align_center">146</td>
<td id="A2.tab1.1.1.11.10.5" class="ltx_td ltx_align_center">309</td>
<td id="A2.tab1.1.1.11.10.6" class="ltx_td ltx_align_center">169</td>
<td id="A2.tab1.1.1.11.10.7" class="ltx_td ltx_align_center">142</td>
<td id="A2.tab1.1.1.11.10.8" class="ltx_td ltx_align_center">311</td>
<td id="A2.tab1.1.1.11.10.9" class="ltx_td ltx_align_center">171</td>
<td id="A2.tab1.1.1.11.10.10" class="ltx_td ltx_align_center">143</td>
<td id="A2.tab1.1.1.11.10.11" class="ltx_td ltx_align_center">314</td>
<td id="A2.tab1.1.1.11.10.12" class="ltx_td ltx_align_center">172</td>
<td id="A2.tab1.1.1.11.10.13" class="ltx_td ltx_align_center">145</td>
<td id="A2.tab1.1.1.11.10.14" class="ltx_td ltx_align_center">317</td>
<td id="A2.tab1.1.1.11.10.15" class="ltx_td ltx_align_center">500</td>
</tr>
<tr id="A2.tab1.1.1.12.11" class="ltx_tr">
<th id="A2.tab1.1.1.12.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">9</th>
<th id="A2.tab1.1.1.12.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">LSTM5</th>
<td id="A2.tab1.1.1.12.11.3" class="ltx_td" colspan="12"></td>
<td id="A2.tab1.1.1.12.11.4" class="ltx_td ltx_align_center">500</td>
</tr>
<tr id="A2.tab1.1.1.13.12" class="ltx_tr">
<th id="A2.tab1.1.1.13.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">10</th>
<th id="A2.tab1.1.1.13.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">Batch Norm</th>
<td id="A2.tab1.1.1.13.12.3" class="ltx_td ltx_align_center">221</td>
<td id="A2.tab1.1.1.13.12.4" class="ltx_td ltx_align_center">203</td>
<td id="A2.tab1.1.1.13.12.5" class="ltx_td ltx_align_center">424</td>
<td id="A2.tab1.1.1.13.12.6" class="ltx_td ltx_align_center">218</td>
<td id="A2.tab1.1.1.13.12.7" class="ltx_td ltx_align_center">193</td>
<td id="A2.tab1.1.1.13.12.8" class="ltx_td ltx_align_center">411</td>
<td id="A2.tab1.1.1.13.12.9" class="ltx_td ltx_align_center">217</td>
<td id="A2.tab1.1.1.13.12.10" class="ltx_td ltx_align_center">191</td>
<td id="A2.tab1.1.1.13.12.11" class="ltx_td ltx_align_center">408</td>
<td id="A2.tab1.1.1.13.12.12" class="ltx_td ltx_align_center">215</td>
<td id="A2.tab1.1.1.13.12.13" class="ltx_td ltx_align_center">191</td>
<td id="A2.tab1.1.1.13.12.14" class="ltx_td ltx_align_center">406</td>
<td id="A2.tab1.1.1.13.12.15" class="ltx_td ltx_align_center">500</td>
</tr>
<tr id="A2.tab1.1.1.14.13" class="ltx_tr">
<th id="A2.tab1.1.1.14.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">11</th>
<th id="A2.tab1.1.1.14.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">Fully-Connected</th>
<td id="A2.tab1.1.1.14.13.3" class="ltx_td ltx_align_center ltx_border_bb">22</td>
<td id="A2.tab1.1.1.14.13.4" class="ltx_td ltx_align_center ltx_border_bb">20</td>
<td id="A2.tab1.1.1.14.13.5" class="ltx_td ltx_align_center ltx_border_bb">42</td>
<td id="A2.tab1.1.1.14.13.6" class="ltx_td ltx_align_center ltx_border_bb">22</td>
<td id="A2.tab1.1.1.14.13.7" class="ltx_td ltx_align_center ltx_border_bb">20</td>
<td id="A2.tab1.1.1.14.13.8" class="ltx_td ltx_align_center ltx_border_bb">42</td>
<td id="A2.tab1.1.1.14.13.9" class="ltx_td ltx_align_center ltx_border_bb">21</td>
<td id="A2.tab1.1.1.14.13.10" class="ltx_td ltx_align_center ltx_border_bb">19</td>
<td id="A2.tab1.1.1.14.13.11" class="ltx_td ltx_align_center ltx_border_bb">40</td>
<td id="A2.tab1.1.1.14.13.12" class="ltx_td ltx_align_center ltx_border_bb">22</td>
<td id="A2.tab1.1.1.14.13.13" class="ltx_td ltx_align_center ltx_border_bb">20</td>
<td id="A2.tab1.1.1.14.13.14" class="ltx_td ltx_align_center ltx_border_bb">42</td>
<td id="A2.tab1.1.1.14.13.15" class="ltx_td ltx_align_center ltx_border_bb">41</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="A2.tab2" class="ltx_table">
<figcaption class="ltx_caption">Supplemental Table 2. Audio augmentation configuration parameters for each noise level</figcaption>
<table id="A2.tab2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.tab2.1.1.1" class="ltx_tr">
<th id="A2.tab2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A2.tab2.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Parameter</span></th>
<th id="A2.tab2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.tab2.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Low</span></th>
<th id="A2.tab2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.tab2.1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Mid</span></th>
<th id="A2.tab2.1.1.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.tab2.1.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">High</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.tab2.1.2.1" class="ltx_tr">
<th id="A2.tab2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="A2.tab2.1.2.1.1.1" class="ltx_text" style="font-size:80%;">Background SNR</span></th>
<td id="A2.tab2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.tab2.1.2.1.2.1" class="ltx_text" style="font-size:80%;">(10, 15)</span></td>
<td id="A2.tab2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.tab2.1.2.1.3.1" class="ltx_text" style="font-size:80%;">(0, 15)</span></td>
<td id="A2.tab2.1.2.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="A2.tab2.1.2.1.4.1" class="ltx_text" style="font-size:80%;">(-10, 15)</span></td>
</tr>
<tr id="A2.tab2.1.3.2" class="ltx_tr">
<th id="A2.tab2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.tab2.1.3.2.1.1" class="ltx_text" style="font-size:80%;">Pitch Shift</span></th>
<td id="A2.tab2.1.3.2.2" class="ltx_td ltx_align_center"><span id="A2.tab2.1.3.2.2.1" class="ltx_text" style="font-size:80%;">(-2, 2)</span></td>
<td id="A2.tab2.1.3.2.3" class="ltx_td ltx_align_center"><span id="A2.tab2.1.3.2.3.1" class="ltx_text" style="font-size:80%;">(-4, 4)</span></td>
<td id="A2.tab2.1.3.2.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.tab2.1.3.2.4.1" class="ltx_text" style="font-size:80%;">(-6, 6)</span></td>
</tr>
<tr id="A2.tab2.1.4.3" class="ltx_tr">
<th id="A2.tab2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.tab2.1.4.3.1.1" class="ltx_text" style="font-size:80%;">Speed Rate</span></th>
<td id="A2.tab2.1.4.3.2" class="ltx_td ltx_align_center"><span id="A2.tab2.1.4.3.2.1" class="ltx_text" style="font-size:80%;">(0.9, 1.1)</span></td>
<td id="A2.tab2.1.4.3.3" class="ltx_td ltx_align_center"><span id="A2.tab2.1.4.3.3.1" class="ltx_text" style="font-size:80%;">(0.7, 1.3)</span></td>
<td id="A2.tab2.1.4.3.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.tab2.1.4.3.4.1" class="ltx_text" style="font-size:80%;">(0.5, 1.5)</span></td>
</tr>
<tr id="A2.tab2.1.5.4" class="ltx_tr">
<th id="A2.tab2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.tab2.1.5.4.1.1" class="ltx_text" style="font-size:80%;">Tempo Rate</span></th>
<td id="A2.tab2.1.5.4.2" class="ltx_td ltx_align_center"><span id="A2.tab2.1.5.4.2.1" class="ltx_text" style="font-size:80%;">(0.9, 1.2)</span></td>
<td id="A2.tab2.1.5.4.3" class="ltx_td ltx_align_center"><span id="A2.tab2.1.5.4.3.1" class="ltx_text" style="font-size:80%;">(0.8, 1.4)</span></td>
<td id="A2.tab2.1.5.4.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.tab2.1.5.4.4.1" class="ltx_text" style="font-size:80%;">(0.7, 1.6)</span></td>
</tr>
<tr id="A2.tab2.1.6.5" class="ltx_tr">
<th id="A2.tab2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.tab2.1.6.5.1.1" class="ltx_text" style="font-size:80%;">Chorus N</span></th>
<td id="A2.tab2.1.6.5.2" class="ltx_td ltx_align_center"><span id="A2.tab2.1.6.5.2.1" class="ltx_text" style="font-size:80%;">(1, 3)</span></td>
<td id="A2.tab2.1.6.5.3" class="ltx_td ltx_align_center"><span id="A2.tab2.1.6.5.3.1" class="ltx_text" style="font-size:80%;">(1, 4)</span></td>
<td id="A2.tab2.1.6.5.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.tab2.1.6.5.4.1" class="ltx_text" style="font-size:80%;">(1, 6)</span></td>
</tr>
<tr id="A2.tab2.1.7.6" class="ltx_tr">
<th id="A2.tab2.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.tab2.1.7.6.1.1" class="ltx_text" style="font-size:80%;">Echo N</span></th>
<td id="A2.tab2.1.7.6.2" class="ltx_td ltx_align_center"><span id="A2.tab2.1.7.6.2.1" class="ltx_text" style="font-size:80%;">(1, 3)</span></td>
<td id="A2.tab2.1.7.6.3" class="ltx_td ltx_align_center"><span id="A2.tab2.1.7.6.3.1" class="ltx_text" style="font-size:80%;">(1, 4)</span></td>
<td id="A2.tab2.1.7.6.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.tab2.1.7.6.4.1" class="ltx_text" style="font-size:80%;">(1, 5)</span></td>
</tr>
<tr id="A2.tab2.1.8.7" class="ltx_tr">
<th id="A2.tab2.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.tab2.1.8.7.1.1" class="ltx_text" style="font-size:80%;">Reverb</span></th>
<td id="A2.tab2.1.8.7.2" class="ltx_td ltx_align_center"><span id="A2.tab2.1.8.7.2.1" class="ltx_text" style="font-size:80%;">(10, 40)</span></td>
<td id="A2.tab2.1.8.7.3" class="ltx_td ltx_align_center"><span id="A2.tab2.1.8.7.3.1" class="ltx_text" style="font-size:80%;">(20, 70)</span></td>
<td id="A2.tab2.1.8.7.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.tab2.1.8.7.4.1" class="ltx_text" style="font-size:80%;">(30, 100)</span></td>
</tr>
<tr id="A2.tab2.1.9.8" class="ltx_tr">
<th id="A2.tab2.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.tab2.1.9.8.1.1" class="ltx_text" style="font-size:80%;">Low-pass F</span></th>
<td id="A2.tab2.1.9.8.2" class="ltx_td ltx_align_center"><span id="A2.tab2.1.9.8.2.1" class="ltx_text" style="font-size:80%;">(6000, 7500)</span></td>
<td id="A2.tab2.1.9.8.3" class="ltx_td ltx_align_center"><span id="A2.tab2.1.9.8.3.1" class="ltx_text" style="font-size:80%;">(4000, 7000)</span></td>
<td id="A2.tab2.1.9.8.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.tab2.1.9.8.4.1" class="ltx_text" style="font-size:80%;">(2000, 6000)</span></td>
</tr>
<tr id="A2.tab2.1.10.9" class="ltx_tr">
<th id="A2.tab2.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.tab2.1.10.9.1.1" class="ltx_text" style="font-size:80%;">High-pass F</span></th>
<td id="A2.tab2.1.10.9.2" class="ltx_td ltx_align_center"><span id="A2.tab2.1.10.9.2.1" class="ltx_text" style="font-size:80%;">(100, 500)</span></td>
<td id="A2.tab2.1.10.9.3" class="ltx_td ltx_align_center"><span id="A2.tab2.1.10.9.3.1" class="ltx_text" style="font-size:80%;">(300, 1000)</span></td>
<td id="A2.tab2.1.10.9.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.tab2.1.10.9.4.1" class="ltx_text" style="font-size:80%;">(500, 2000)</span></td>
</tr>
<tr id="A2.tab2.1.11.10" class="ltx_tr">
<th id="A2.tab2.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.tab2.1.11.10.1.1" class="ltx_text" style="font-size:80%;">Band-pass F</span></th>
<td id="A2.tab2.1.11.10.2" class="ltx_td ltx_align_center"><span id="A2.tab2.1.11.10.2.1" class="ltx_text" style="font-size:80%;">(100, 500)</span></td>
<td id="A2.tab2.1.11.10.3" class="ltx_td ltx_align_center"><span id="A2.tab2.1.11.10.3.1" class="ltx_text" style="font-size:80%;">(200, 1000)</span></td>
<td id="A2.tab2.1.11.10.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.tab2.1.11.10.4.1" class="ltx_text" style="font-size:80%;">(300, 1500)</span></td>
</tr>
<tr id="A2.tab2.1.12.11" class="ltx_tr">
<th id="A2.tab2.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.tab2.1.12.11.1.1" class="ltx_text" style="font-size:80%;">Band-pass W</span></th>
<td id="A2.tab2.1.12.11.2" class="ltx_td ltx_align_center"><span id="A2.tab2.1.12.11.2.1" class="ltx_text" style="font-size:80%;">(12, 16)</span></td>
<td id="A2.tab2.1.12.11.3" class="ltx_td ltx_align_center"><span id="A2.tab2.1.12.11.3.1" class="ltx_text" style="font-size:80%;">(6, 8)</span></td>
<td id="A2.tab2.1.12.11.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.tab2.1.12.11.4.1" class="ltx_text" style="font-size:80%;">(3, 5)</span></td>
</tr>
<tr id="A2.tab2.1.13.12" class="ltx_tr">
<th id="A2.tab2.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.tab2.1.13.12.1.1" class="ltx_text" style="font-size:80%;">Band-stop F</span></th>
<td id="A2.tab2.1.13.12.2" class="ltx_td ltx_align_center"><span id="A2.tab2.1.13.12.2.1" class="ltx_text" style="font-size:80%;">(300, 4000)</span></td>
<td id="A2.tab2.1.13.12.3" class="ltx_td ltx_align_center"><span id="A2.tab2.1.13.12.3.1" class="ltx_text" style="font-size:80%;">(300, 2500)</span></td>
<td id="A2.tab2.1.13.12.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="A2.tab2.1.13.12.4.1" class="ltx_text" style="font-size:80%;">(300, 1500)</span></td>
</tr>
<tr id="A2.tab2.1.14.13" class="ltx_tr">
<th id="A2.tab2.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="A2.tab2.1.14.13.1.1" class="ltx_text" style="font-size:80%;">Band-stop W</span></th>
<td id="A2.tab2.1.14.13.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.tab2.1.14.13.2.1" class="ltx_text" style="font-size:80%;">(1, 2)</span></td>
<td id="A2.tab2.1.14.13.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.tab2.1.14.13.3.1" class="ltx_text" style="font-size:80%;">(2, 3)</span></td>
<td id="A2.tab2.1.14.13.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="A2.tab2.1.14.13.4.1" class="ltx_text" style="font-size:80%;">(3, 5)</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.20534" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.20535" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.20535">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.20535" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.20536" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 16:16:32 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
