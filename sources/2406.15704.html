<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.15704] video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models</title><meta property="og:description" content="Speech understanding as an element of the more generic video understanding using audio-visual large language models (av-LLMs) is a crucial yet understudied aspect.
This paper proposes video-SALMONN, a single end-to-end…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.15704">

<!--Generated on Fri Jul  5 21:39:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Machine Learning,  ICML">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Guangzhi Sun
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wenyi Yu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Changli Tang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xianzhao Chen
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tian Tan
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wei Li
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lu Lu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zejun Ma
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuxuan Wang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chao Zhang
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Speech understanding as an element of the more generic video understanding using audio-visual large language models (av-LLMs) is a crucial yet understudied aspect.
This paper proposes video-SALMONN, a single end-to-end av-LLM for video processing, which can understand not only visual frame sequences, audio events and music, but speech as well. To obtain fine-grained temporal information required by speech understanding, while keeping efficient for other video elements, this paper proposes a novel multi-resolution causal Q-Former (MRC Q-Former) structure to connect pre-trained audio-visual encoders and the backbone large language model. Moreover, dedicated training approaches including the diversity loss and the unpaired audio-visual mixed training scheme are proposed to avoid frames or modality dominance. On the introduced speech-audio-visual evaluation benchmark, video-SALMONN achieves more than 25% absolute accuracy improvements on the video-QA task
and over 30% absolute accuracy improvements on audio-visual QA tasks with human speech. In addition, video-SALMONN demonstrates remarkable video comprehension and reasoning abilities on tasks that are unprecedented by other av-LLMs. Our training code and model checkpoints are available at <a target="_blank" href="https://github.com/bytedance/SALMONN/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/bytedance/SALMONN/</a>.</p>
</div>
<div class="ltx_keywords">Machine Learning, ICML
</div>
<div id="p2" class="ltx_para">
<br class="ltx_break">
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Text-based large language models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>; Touvron et al., <a href="#bib.bib49" title="" class="ltx_ref">2023</a>; Chiang et al., <a href="#bib.bib9" title="" class="ltx_ref">2023</a>; Anil et al., <a href="#bib.bib2" title="" class="ltx_ref">2023</a>; Du et al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite> have demonstrated remarkable performance in many natural language processing tasks, especially achieving human-level capabilities in reasoning and comprehension <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite>. Meanwhile, instruction fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(Chung et al., <a href="#bib.bib10" title="" class="ltx_ref">2022</a>; Ouyang et al., <a href="#bib.bib34" title="" class="ltx_ref">2022</a>; Peng et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite>, where data is organised as paired user instructions (or prompts) and reference responses, has emerged as a training paradigm that enables LLMs to perform tasks by following open-ended natural language instructions from non-expert users. Recently, there has been a burgeoning research interest in equipping LLMs with visual and auditory perception abilities, resulting in a range of visual <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib24" title="" class="ltx_ref">2023a</a>; Alayrac et al., <a href="#bib.bib1" title="" class="ltx_ref">2022</a>; Dai et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>; Maaz et al., <a href="#bib.bib31" title="" class="ltx_ref">2023</a>; Chen et al., <a href="#bib.bib5" title="" class="ltx_ref">2023b</a>; Zhao et al., <a href="#bib.bib56" title="" class="ltx_ref">2022</a>; Zeng et al., <a href="#bib.bib53" title="" class="ltx_ref">2023</a>; Luo et al., <a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite>, audio <cite class="ltx_cite ltx_citemacro_citep">(Gong et al., <a href="#bib.bib18" title="" class="ltx_ref">2023</a>; Zhang et al., <a href="#bib.bib54" title="" class="ltx_ref">2023a</a>; Rubenstein et al., <a href="#bib.bib39" title="" class="ltx_ref">2023</a>; Tang et al., <a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite>,
and audio-visual LLMs (av-LLMs) <cite class="ltx_cite ltx_citemacro_citep">(Su et al., <a href="#bib.bib45" title="" class="ltx_ref">2023</a>; Zhang et al., <a href="#bib.bib55" title="" class="ltx_ref">2023b</a>; Lyu et al., <a href="#bib.bib30" title="" class="ltx_ref">2023</a>; Zhao et al., <a href="#bib.bib57" title="" class="ltx_ref">2023</a>; Chen et al., <a href="#bib.bib4" title="" class="ltx_ref">2023a</a>; Shu et al., <a href="#bib.bib42" title="" class="ltx_ref">2023b</a>; Piergiovanni et al., <a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Despite av-LLMs’ prosperity, speech, as a primary carrier of human language in videos, is considerably under-explored in these models. Complementary to non-speech audio events and natural images, speech provides direct and abundant linguistic and semantic information, making it indispensable for comprehensive video understanding. Speech signals also include rich paralinguistic information, such as the tone and pitch of voice, which is often hard to textualise precisely but necessary to understand the underlying meanings and emotions.
Additionally, there exist diverse speaker attributes in speech, which are tedious and difficult to transcribe using separate systems but essential for video understanding (see Fig. <a href="#A10.F16" title="Figure 16 ‣ Appendix J Case Studies ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>), including the speaker’s age, gender, accent and identity <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">etc</span>. To avoid building complex cascaded systems, it is desired to recognise and understand all of the aforementioned speech attributes in videos in a fully end-to-end and integrated way with av-LLMs.
Nevertheless, enhancing general-purposed av-LLMs with speech is very challenging, which requires temporally fine-grained modelling while intricately interacting with other modalities at both coarse (<span id="S1.p2.1.2" class="ltx_text ltx_font_italic">e.g.</span> video topics) and fine (<span id="S1.p2.1.3" class="ltx_text ltx_font_italic">e.g.</span> lip movements) time scales. This necessitates the design of specialised fine-grained multi-resolution approaches to address this challenge.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To this end, we propose video-SALMONN (<span id="S1.p3.1.1" class="ltx_text ltx_font_bold">s</span>peech <span id="S1.p3.1.2" class="ltx_text ltx_font_bold">a</span>udio <span id="S1.p3.1.3" class="ltx_text ltx_font_bold">l</span>anguage <span id="S1.p3.1.4" class="ltx_text ltx_font_bold">m</span>usic <span id="S1.p3.1.5" class="ltx_text ltx_font_bold">o</span>pen <span id="S1.p3.1.6" class="ltx_text ltx_font_bold">n</span>eural <span id="S1.p3.1.7" class="ltx_text ltx_font_bold">n</span>etwork), a speech-enhanced av-LLM for short video understanding. By resembling the audio encoder structure of the SALMONN <cite class="ltx_cite ltx_citemacro_citep">(Tang et al., <a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite> LLM with generic hearing abilities and incorporating an additional visual encoder, video-SALMONN enables video inputs with natural image, visual frame sequence, speech, audio events, and music elements, covering all basic elements in general video data.
The core of video-SALMONN is a multi-resolution causal (MRC) Q-Former structure aligning time-synchronised audio-visual input features with text representation space at three different temporal scales, which meets the requirements of tasks relying on different video elements.
To reinforce the temporal causal relations of events among successive video frames, a causal self-attention structure with a special causal mask is included in the MRC Q-Former.
Further, to avoid the dominance of a specific frame or a single modality in the video, video-SALMONN is trained using a proposed diversity loss together with a new unpaired audio-visual mixing strategy.
To our knowledge, video-SALMONN is the first av-LLM tailored to achieve general video understanding.
</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To comprehensively evaluate the general video understanding abilities, we introduce the speech-audio-visual evaluation (SAVE) benchmark containing six open-source representative single-modal tasks and four open-source audio-visual tasks. video-SALMONN is the only av-LLM
that can achieve tasks relying on speech elements, such as audio-visual speech recognition (AVSR) and speech-content-based QA.
On the single-modal tasks, video-SALMONN achieves a remarkably 25% accuracy improvement in Video QA, a question answering (QA) task
focusing on temporal causal reasoning compared to a strong InstructBLIP baseline <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>.
On audio-visual tasks, video-SALMONN has shown large performance improvements, <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">e.g.</span> over 30% absolute accuracy improvement on audio-visual QA dataset.
The main contributions are summarised as follows.</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose video-SALMONN, a speech-enhanced av-LLM. To our knowledge, video-SALMONN is the first single LLM-centric model that can handle video along with both speech and non-speech audio inputs.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose the MRC Q-Former structure as a multi-resolution modality aligner for video-SALMONN, which lays a solid foundation for the joint speech-audio-visual information extraction in videos.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We propose the diversity loss and mixed training scheme to achieve a better balance of features from different frames and modalities.

</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">video-SALMONN achieves superior performance on the SAVE benchmark, especially in audio-visual tasks requiring speech understanding and causal reasoning.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2406.15704/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="325" height="189" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The model structure of video-SALMONN using fine-grained audio-visual joint representations. Audio and visual input streams are encoded into sequences of features with individual encoders that are not updated during training, and the features are temporally synchronised and processed by the proposed multi-resolution causal (MRC) Q-Former operating at different time scales.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The work most closely related to video-SALMONN is Video-LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib55" title="" class="ltx_ref">2023b</a>)</cite>, Macaw-LLM <cite class="ltx_cite ltx_citemacro_citep">(Lyu et al., <a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>, X-LLM <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib4" title="" class="ltx_ref">2023a</a>)</cite> and also work proposed by <cite class="ltx_cite ltx_citemacro_citet">Shu et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023a</a>); Chen et al. (<a href="#bib.bib7" title="" class="ltx_ref">2023c</a>)</cite>, as all of them used LLMs for cross-modal understanding based on general non-silent video inputs (referred to as audio-visual sequence in this paper).
X-LLM supports video with Chinese speech inputs, but doesn’t support audio events and music.
Video-LLaMA employs an additional video Q-Former to encode features of several equally-spaced frames extracted using a BLIP2 <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib24" title="" class="ltx_ref">2023a</a>)</cite> image encoder. Macaw-LLM adopted a similar approach and used three separate encoders for image, video and non-speech audio events.
Both Video-LLaMA and Macaw-LLM consider only non-speech audio events, and the audio encoders in the two models are the ImageBind <cite class="ltx_cite ltx_citemacro_citep">(Girdhar et al., <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite> and Whisper <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite> model encoders respectively.
While both methods involve the fusion of audio and visual feature streams, the two streams are sparsely pooled and processed rather independently, which removes fine-grained audio-visual interactions at each time step.
Compared to Video-LLaMA and Macaw-LLM, video-SALMONN understands speech in a video and reserves fine-grained modality interactions that are common in general non-silent videos.
This leads to an emphasis on causal modality synchronisation across time and allows more content-based cross-modal interactions.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">As an alternative to include speech modelling in av-LLM, speech content can be extracted using an external automatic speech recognition (ASR) system and fed into the av-LLM as textual subtitle inputs <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib7" title="" class="ltx_ref">2023c</a>)</cite>.
However, this approach ignores the rich paralinguistic and speaker information embedded in speech, unless they are also extracted using external systems.
Rich transcription (RT) is a long-standing research problem targeting extracting abundant information from speech signals <cite class="ltx_cite ltx_citemacro_citep">(Garofolo et al., <a href="#bib.bib16" title="" class="ltx_ref">2004</a>; Fiscus et al., <a href="#bib.bib14" title="" class="ltx_ref">2006b</a>, <a href="#bib.bib13" title="" class="ltx_ref">a</a>, <a href="#bib.bib15" title="" class="ltx_ref">2007</a>)</cite> that used to be tackled as several separate tasks, such as ASR, speaker diarisation and emotion recognition <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">etc</span>.
In contrast, video-SALMONN unifies those hearing ability tasks together with visual perception abilities using a single end-to-end model.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Our work is based on the Q-Former structure to fuse the audio and visual modalities and to align with the text representation space <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib24" title="" class="ltx_ref">2023a</a>; Dai et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>. While Q-Former has been primarily proposed for visual information extraction, it also performs remarkably in extracting auditory features for generic audio understanding in SALMONN <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a href="#bib.bib52" title="" class="ltx_ref">2024</a>; Tang et al., <a href="#bib.bib48" title="" class="ltx_ref">2024</a>, <a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite>.
In addition, various types of modality aligners have been studied, such as the cross-attention mechanism <cite class="ltx_cite ltx_citemacro_citep">(Alayrac et al., <a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite>, pre-trained multimodal embeddings, <cite class="ltx_cite ltx_citemacro_citep">(Girdhar et al., <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite> and temporal and spatial pooling <cite class="ltx_cite ltx_citemacro_citep">(Maaz et al., <a href="#bib.bib31" title="" class="ltx_ref">2023</a>)</cite> <span id="S2.p3.1.1" class="ltx_text ltx_font_italic">etc</span>. Different from these approaches, our proposed MRC Q-Former used in video-SALMONN pays particular attention to the sequential nature of video and the multi-resolution information of the input feature streams suitable for the understanding of different video elements. This work is a revision of an unpublished work of ours <cite class="ltx_cite ltx_citemacro_citep">(Sun et al., <a href="#bib.bib46" title="" class="ltx_ref">2023</a>)</cite>, which is the first study to explore video understanding with general audio (audio event, speech and music <span id="S2.p3.1.2" class="ltx_text ltx_font_italic">etc.</span>).</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>video-SALMONN</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section introduces the structure and the training approach for video-SALMONN. As shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, key components include the synchronisation module and the MRC Q-Former. First, visual (image or video), speech and non-speech audio are encoded using corresponding pre-trained encoders. The visual encoder converts the input image into a certain number of vectors via the image encoder from InstructBLIP <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib24" title="" class="ltx_ref">2023a</a>)</cite>. When video input is given, the visual encoder encodes each video frame separately as a sequence of images at a 2 Hz frame rate, and the output image features are concatenated along the temporal dimension to form a sequence of visual frames. Following SALMONN <cite class="ltx_cite ltx_citemacro_citep">(Tang et al., <a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite>, Whisper <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite> encoder and BEATs <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib8" title="" class="ltx_ref">2023d</a>)</cite> encoder are adopted to encode speech and non-speech audio respectively from the same audio stream at 50 Hz spectrogram frame rate.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Temporal Fine-grained Synchronisation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.5" class="ltx_p">When both audio and visual inputs are present, the encoded feature sequences are sent to the temporal synchronisation module to obtain the time-synchronised feature sequences. Since video is sampled at a lower frame rate than audio, the audio and visual frames are synchronised at each video frame (<span id="S3.SS1.p1.5.1" class="ltx_text ltx_font_italic">i.e.</span> every 0.5 seconds), with zero padding to make both sequences have equal lengths.
Note that higher frequencies of visual frames are also supported which requires higher computation and storage costs.
<math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{h}^{\text{S}}_{t}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><msubsup id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2.2" xref="S3.SS1.p1.1.m1.1.1.2.2.cmml">𝐡</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">t</mi><mtext id="S3.SS1.p1.1.m1.1.1.2.3" xref="S3.SS1.p1.1.m1.1.1.2.3a.cmml">S</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><apply id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2.2">𝐡</ci><ci id="S3.SS1.p1.1.m1.1.1.2.3a.cmml" xref="S3.SS1.p1.1.m1.1.1.2.3"><mtext mathsize="70%" id="S3.SS1.p1.1.m1.1.1.2.3.cmml" xref="S3.SS1.p1.1.m1.1.1.2.3">S</mtext></ci></apply><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathbf{h}^{\text{S}}_{t}</annotation></semantics></math>, <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{h}^{\text{A}}_{t}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><msubsup id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2.2" xref="S3.SS1.p1.2.m2.1.1.2.2.cmml">𝐡</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">t</mi><mtext id="S3.SS1.p1.2.m2.1.1.2.3" xref="S3.SS1.p1.2.m2.1.1.2.3a.cmml">A</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><apply id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS1.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2.2">𝐡</ci><ci id="S3.SS1.p1.2.m2.1.1.2.3a.cmml" xref="S3.SS1.p1.2.m2.1.1.2.3"><mtext mathsize="70%" id="S3.SS1.p1.2.m2.1.1.2.3.cmml" xref="S3.SS1.p1.2.m2.1.1.2.3">A</mtext></ci></apply><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mathbf{h}^{\text{A}}_{t}</annotation></semantics></math> and <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{h}^{\text{V}}_{t}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><msubsup id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2.2" xref="S3.SS1.p1.3.m3.1.1.2.2.cmml">𝐡</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">t</mi><mtext id="S3.SS1.p1.3.m3.1.1.2.3" xref="S3.SS1.p1.3.m3.1.1.2.3a.cmml">V</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><apply id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS1.p1.3.m3.1.1">superscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2.2">𝐡</ci><ci id="S3.SS1.p1.3.m3.1.1.2.3a.cmml" xref="S3.SS1.p1.3.m3.1.1.2.3"><mtext mathsize="70%" id="S3.SS1.p1.3.m3.1.1.2.3.cmml" xref="S3.SS1.p1.3.m3.1.1.2.3">V</mtext></ci></apply><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\mathbf{h}^{\text{V}}_{t}</annotation></semantics></math>, the synchronised frame-level outputs at step <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">t</annotation></semantics></math> from the Whisper speech encoder, BEATs audio encoder and InstructBLIP video encoder, are concatenated along the feature dimension to obtain the combined representation <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="\mathbf{h}^{\text{SAV}}_{t}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><msubsup id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2.2" xref="S3.SS1.p1.5.m5.1.1.2.2.cmml">𝐡</mi><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">t</mi><mtext id="S3.SS1.p1.5.m5.1.1.2.3" xref="S3.SS1.p1.5.m5.1.1.2.3a.cmml">SAV</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><apply id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.2.1.cmml" xref="S3.SS1.p1.5.m5.1.1">superscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2.2">𝐡</ci><ci id="S3.SS1.p1.5.m5.1.1.2.3a.cmml" xref="S3.SS1.p1.5.m5.1.1.2.3"><mtext mathsize="70%" id="S3.SS1.p1.5.m5.1.1.2.3.cmml" xref="S3.SS1.p1.5.m5.1.1.2.3">SAV</mtext></ci></apply><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\mathbf{h}^{\text{SAV}}_{t}</annotation></semantics></math>. That is,</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\mathbf{h}^{\text{SAV}}_{t}=\text{Concat}(\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{v}_{t})," display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msubsup id="S3.E1.m1.1.1.1.1.5" xref="S3.E1.m1.1.1.1.1.5.cmml"><mi id="S3.E1.m1.1.1.1.1.5.2.2" xref="S3.E1.m1.1.1.1.1.5.2.2.cmml">𝐡</mi><mi id="S3.E1.m1.1.1.1.1.5.3" xref="S3.E1.m1.1.1.1.1.5.3.cmml">t</mi><mtext id="S3.E1.m1.1.1.1.1.5.2.3" xref="S3.E1.m1.1.1.1.1.5.2.3a.cmml">SAV</mtext></msubsup><mo id="S3.E1.m1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.4.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mtext id="S3.E1.m1.1.1.1.1.3.5" xref="S3.E1.m1.1.1.1.1.3.5a.cmml">Concat</mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.3.4" xref="S3.E1.m1.1.1.1.1.3.4.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.4.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.3.3.3.4" xref="S3.E1.m1.1.1.1.1.3.3.4.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml">𝐬</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.E1.m1.1.1.1.1.3.3.3.5" xref="S3.E1.m1.1.1.1.1.3.3.4.cmml">,</mo><msub id="S3.E1.m1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2.cmml">𝐚</mi><mi id="S3.E1.m1.1.1.1.1.2.2.2.2.3" xref="S3.E1.m1.1.1.1.1.2.2.2.2.3.cmml">t</mi></msub><mo id="S3.E1.m1.1.1.1.1.3.3.3.6" xref="S3.E1.m1.1.1.1.1.3.3.4.cmml">,</mo><msub id="S3.E1.m1.1.1.1.1.3.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.3.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.3.3.2.cmml">𝐯</mi><mi id="S3.E1.m1.1.1.1.1.3.3.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.3.3.cmml">t</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.3.3.3.7" xref="S3.E1.m1.1.1.1.1.3.3.4.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.4"></eq><apply id="S3.E1.m1.1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.5.1.cmml" xref="S3.E1.m1.1.1.1.1.5">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.5.2.cmml" xref="S3.E1.m1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.5.2.1.cmml" xref="S3.E1.m1.1.1.1.1.5">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.5.2.2.cmml" xref="S3.E1.m1.1.1.1.1.5.2.2">𝐡</ci><ci id="S3.E1.m1.1.1.1.1.5.2.3a.cmml" xref="S3.E1.m1.1.1.1.1.5.2.3"><mtext mathsize="70%" id="S3.E1.m1.1.1.1.1.5.2.3.cmml" xref="S3.E1.m1.1.1.1.1.5.2.3">SAV</mtext></ci></apply><ci id="S3.E1.m1.1.1.1.1.5.3.cmml" xref="S3.E1.m1.1.1.1.1.5.3">𝑡</ci></apply><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><times id="S3.E1.m1.1.1.1.1.3.4.cmml" xref="S3.E1.m1.1.1.1.1.3.4"></times><ci id="S3.E1.m1.1.1.1.1.3.5a.cmml" xref="S3.E1.m1.1.1.1.1.3.5"><mtext id="S3.E1.m1.1.1.1.1.3.5.cmml" xref="S3.E1.m1.1.1.1.1.3.5">Concat</mtext></ci><vector id="S3.E1.m1.1.1.1.1.3.3.4.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3"><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.2">𝐬</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.3">𝑡</ci></apply><apply id="S3.E1.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2">𝐚</ci><ci id="S3.E1.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2.3">𝑡</ci></apply><apply id="S3.E1.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.3.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.3.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.3.2">𝐯</ci><ci id="S3.E1.m1.1.1.1.1.3.3.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.3.3">𝑡</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\mathbf{h}^{\text{SAV}}_{t}=\text{Concat}(\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{v}_{t}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.9" class="ltx_p">where Concat<math id="S3.SS1.p1.6.m1.1" class="ltx_Math" alttext="(\cdot)" display="inline"><semantics id="S3.SS1.p1.6.m1.1a"><mrow id="S3.SS1.p1.6.m1.1.2.2"><mo stretchy="false" id="S3.SS1.p1.6.m1.1.2.2.1">(</mo><mo lspace="0em" rspace="0em" id="S3.SS1.p1.6.m1.1.1" xref="S3.SS1.p1.6.m1.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS1.p1.6.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m1.1b"><ci id="S3.SS1.p1.6.m1.1.1.cmml" xref="S3.SS1.p1.6.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m1.1c">(\cdot)</annotation></semantics></math> represents the concatenation along the feature dimension and <math id="S3.SS1.p1.7.m2.1" class="ltx_Math" alttext="\mathbf{W}" display="inline"><semantics id="S3.SS1.p1.7.m2.1a"><mi id="S3.SS1.p1.7.m2.1.1" xref="S3.SS1.p1.7.m2.1.1.cmml">𝐖</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m2.1b"><ci id="S3.SS1.p1.7.m2.1.1.cmml" xref="S3.SS1.p1.7.m2.1.1">𝐖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m2.1c">\mathbf{W}</annotation></semantics></math> is a projection weight matrix. Note that in cases when audio input is missing, <math id="S3.SS1.p1.8.m3.1" class="ltx_Math" alttext="\mathbf{s}_{t}" display="inline"><semantics id="S3.SS1.p1.8.m3.1a"><msub id="S3.SS1.p1.8.m3.1.1" xref="S3.SS1.p1.8.m3.1.1.cmml"><mi id="S3.SS1.p1.8.m3.1.1.2" xref="S3.SS1.p1.8.m3.1.1.2.cmml">𝐬</mi><mi id="S3.SS1.p1.8.m3.1.1.3" xref="S3.SS1.p1.8.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m3.1b"><apply id="S3.SS1.p1.8.m3.1.1.cmml" xref="S3.SS1.p1.8.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m3.1.1.1.cmml" xref="S3.SS1.p1.8.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m3.1.1.2.cmml" xref="S3.SS1.p1.8.m3.1.1.2">𝐬</ci><ci id="S3.SS1.p1.8.m3.1.1.3.cmml" xref="S3.SS1.p1.8.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m3.1c">\mathbf{s}_{t}</annotation></semantics></math> and <math id="S3.SS1.p1.9.m4.1" class="ltx_Math" alttext="\mathbf{a}_{t}" display="inline"><semantics id="S3.SS1.p1.9.m4.1a"><msub id="S3.SS1.p1.9.m4.1.1" xref="S3.SS1.p1.9.m4.1.1.cmml"><mi id="S3.SS1.p1.9.m4.1.1.2" xref="S3.SS1.p1.9.m4.1.1.2.cmml">𝐚</mi><mi id="S3.SS1.p1.9.m4.1.1.3" xref="S3.SS1.p1.9.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m4.1b"><apply id="S3.SS1.p1.9.m4.1.1.cmml" xref="S3.SS1.p1.9.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m4.1.1.1.cmml" xref="S3.SS1.p1.9.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.9.m4.1.1.2.cmml" xref="S3.SS1.p1.9.m4.1.1.2">𝐚</ci><ci id="S3.SS1.p1.9.m4.1.1.3.cmml" xref="S3.SS1.p1.9.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m4.1c">\mathbf{a}_{t}</annotation></semantics></math> are replaced with a sequence of zero padding of the same sequence length, and <span id="S3.SS1.p1.9.1" class="ltx_text ltx_font_italic">vice versa</span>.
While an image alone is treated as a single frame, when paired audio input exists, such as images with spoken captions <cite class="ltx_cite ltx_citemacro_citep">(Hsu et al., <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite>, each image is duplicated as if it were a video input with a matched length to the audio input.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>MRC Q-Former</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.3" class="ltx_p">The MRC Q-Former extracts audio-visual features from variable-length inputs at different temporal resolutions. The detailed structure is shown in Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.2 MRC Q-Former ‣ 3 video-SALMONN ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. First, the synchronised input stream is divided into fixed-length windows at multiple different resolutions, <span id="S3.SS2.p1.3.1" class="ltx_text ltx_font_italic">e.g.</span> spanning every 1, 5 or 10 seconds. Then, at each resolution level <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">r</annotation></semantics></math>, based on <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="N(r)" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.2" xref="S3.SS2.p1.2.m2.1.2.cmml"><mi id="S3.SS2.p1.2.m2.1.2.2" xref="S3.SS2.p1.2.m2.1.2.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.2.1" xref="S3.SS2.p1.2.m2.1.2.1.cmml">​</mo><mrow id="S3.SS2.p1.2.m2.1.2.3.2" xref="S3.SS2.p1.2.m2.1.2.cmml"><mo stretchy="false" id="S3.SS2.p1.2.m2.1.2.3.2.1" xref="S3.SS2.p1.2.m2.1.2.cmml">(</mo><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">r</mi><mo stretchy="false" id="S3.SS2.p1.2.m2.1.2.3.2.2" xref="S3.SS2.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.2.cmml" xref="S3.SS2.p1.2.m2.1.2"><times id="S3.SS2.p1.2.m2.1.2.1.cmml" xref="S3.SS2.p1.2.m2.1.2.1"></times><ci id="S3.SS2.p1.2.m2.1.2.2.cmml" xref="S3.SS2.p1.2.m2.1.2.2">𝑁</ci><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">N(r)</annotation></semantics></math> trainable input query vectors, the MRC Q-Former is applied to convert features in each sliding window into <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="N(r)\in\mathbb{N}^{+}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.2" xref="S3.SS2.p1.3.m3.1.2.cmml"><mrow id="S3.SS2.p1.3.m3.1.2.2" xref="S3.SS2.p1.3.m3.1.2.2.cmml"><mi id="S3.SS2.p1.3.m3.1.2.2.2" xref="S3.SS2.p1.3.m3.1.2.2.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.3.m3.1.2.2.1" xref="S3.SS2.p1.3.m3.1.2.2.1.cmml">​</mo><mrow id="S3.SS2.p1.3.m3.1.2.2.3.2" xref="S3.SS2.p1.3.m3.1.2.2.cmml"><mo stretchy="false" id="S3.SS2.p1.3.m3.1.2.2.3.2.1" xref="S3.SS2.p1.3.m3.1.2.2.cmml">(</mo><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">r</mi><mo stretchy="false" id="S3.SS2.p1.3.m3.1.2.2.3.2.2" xref="S3.SS2.p1.3.m3.1.2.2.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p1.3.m3.1.2.1" xref="S3.SS2.p1.3.m3.1.2.1.cmml">∈</mo><msup id="S3.SS2.p1.3.m3.1.2.3" xref="S3.SS2.p1.3.m3.1.2.3.cmml"><mi id="S3.SS2.p1.3.m3.1.2.3.2" xref="S3.SS2.p1.3.m3.1.2.3.2.cmml">ℕ</mi><mo id="S3.SS2.p1.3.m3.1.2.3.3" xref="S3.SS2.p1.3.m3.1.2.3.3.cmml">+</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.2.cmml" xref="S3.SS2.p1.3.m3.1.2"><in id="S3.SS2.p1.3.m3.1.2.1.cmml" xref="S3.SS2.p1.3.m3.1.2.1"></in><apply id="S3.SS2.p1.3.m3.1.2.2.cmml" xref="S3.SS2.p1.3.m3.1.2.2"><times id="S3.SS2.p1.3.m3.1.2.2.1.cmml" xref="S3.SS2.p1.3.m3.1.2.2.1"></times><ci id="S3.SS2.p1.3.m3.1.2.2.2.cmml" xref="S3.SS2.p1.3.m3.1.2.2.2">𝑁</ci><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝑟</ci></apply><apply id="S3.SS2.p1.3.m3.1.2.3.cmml" xref="S3.SS2.p1.3.m3.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.2.3.1.cmml" xref="S3.SS2.p1.3.m3.1.2.3">superscript</csymbol><ci id="S3.SS2.p1.3.m3.1.2.3.2.cmml" xref="S3.SS2.p1.3.m3.1.2.3.2">ℕ</ci><plus id="S3.SS2.p1.3.m3.1.2.3.3.cmml" xref="S3.SS2.p1.3.m3.1.2.3.3"></plus></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">N(r)\in\mathbb{N}^{+}</annotation></semantics></math> output query vectors carrying the audio-visual joint information. That is,</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.8" class="ltx_Math" alttext="\mathbf{h}^{(r)}_{w,1:N(r)}=\text{Q-Former}_{\text{MRC}}(\mathbf{h}^{\text{SAV}}_{t:t+k(r)};\mathbf{q}^{(r)}_{1:N(r)})," display="block"><semantics id="S3.E2.m1.8a"><mrow id="S3.E2.m1.8.8.1" xref="S3.E2.m1.8.8.1.1.cmml"><mrow id="S3.E2.m1.8.8.1.1" xref="S3.E2.m1.8.8.1.1.cmml"><msubsup id="S3.E2.m1.8.8.1.1.4" xref="S3.E2.m1.8.8.1.1.4.cmml"><mi id="S3.E2.m1.8.8.1.1.4.2.2" xref="S3.E2.m1.8.8.1.1.4.2.2.cmml">𝐡</mi><mrow id="S3.E2.m1.4.4.3" xref="S3.E2.m1.4.4.3.cmml"><mrow id="S3.E2.m1.4.4.3.5.2" xref="S3.E2.m1.4.4.3.5.1.cmml"><mi id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml">w</mi><mo id="S3.E2.m1.4.4.3.5.2.1" xref="S3.E2.m1.4.4.3.5.1.cmml">,</mo><mn id="S3.E2.m1.3.3.2.2" xref="S3.E2.m1.3.3.2.2.cmml">1</mn></mrow><mo lspace="0.278em" rspace="0.278em" id="S3.E2.m1.4.4.3.4" xref="S3.E2.m1.4.4.3.4.cmml">:</mo><mrow id="S3.E2.m1.4.4.3.6" xref="S3.E2.m1.4.4.3.6.cmml"><mi id="S3.E2.m1.4.4.3.6.2" xref="S3.E2.m1.4.4.3.6.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.3.6.1" xref="S3.E2.m1.4.4.3.6.1.cmml">​</mo><mrow id="S3.E2.m1.4.4.3.6.3.2" xref="S3.E2.m1.4.4.3.6.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.3.6.3.2.1" xref="S3.E2.m1.4.4.3.6.cmml">(</mo><mi id="S3.E2.m1.4.4.3.3" xref="S3.E2.m1.4.4.3.3.cmml">r</mi><mo stretchy="false" id="S3.E2.m1.4.4.3.6.3.2.2" xref="S3.E2.m1.4.4.3.6.cmml">)</mo></mrow></mrow></mrow><mrow id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.8.8.1.1.4.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.3.1" xref="S3.E2.m1.8.8.1.1.4.cmml">(</mo><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">r</mi><mo stretchy="false" id="S3.E2.m1.1.1.1.3.2" xref="S3.E2.m1.8.8.1.1.4.cmml">)</mo></mrow></msubsup><mo id="S3.E2.m1.8.8.1.1.3" xref="S3.E2.m1.8.8.1.1.3.cmml">=</mo><mrow id="S3.E2.m1.8.8.1.1.2" xref="S3.E2.m1.8.8.1.1.2.cmml"><msub id="S3.E2.m1.8.8.1.1.2.4" xref="S3.E2.m1.8.8.1.1.2.4.cmml"><mtext id="S3.E2.m1.8.8.1.1.2.4.2" xref="S3.E2.m1.8.8.1.1.2.4.2a.cmml">Q-Former</mtext><mtext id="S3.E2.m1.8.8.1.1.2.4.3" xref="S3.E2.m1.8.8.1.1.2.4.3a.cmml">MRC</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.8.8.1.1.2.3" xref="S3.E2.m1.8.8.1.1.2.3.cmml">​</mo><mrow id="S3.E2.m1.8.8.1.1.2.2.2" xref="S3.E2.m1.8.8.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.8.8.1.1.2.2.2.3" xref="S3.E2.m1.8.8.1.1.2.2.3.cmml">(</mo><msubsup id="S3.E2.m1.8.8.1.1.1.1.1.1" xref="S3.E2.m1.8.8.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.8.8.1.1.1.1.1.1.2.2" xref="S3.E2.m1.8.8.1.1.1.1.1.1.2.2.cmml">𝐡</mi><mrow id="S3.E2.m1.5.5.1" xref="S3.E2.m1.5.5.1.cmml"><mi id="S3.E2.m1.5.5.1.3" xref="S3.E2.m1.5.5.1.3.cmml">t</mi><mo lspace="0.278em" rspace="0.278em" id="S3.E2.m1.5.5.1.2" xref="S3.E2.m1.5.5.1.2.cmml">:</mo><mrow id="S3.E2.m1.5.5.1.4" xref="S3.E2.m1.5.5.1.4.cmml"><mi id="S3.E2.m1.5.5.1.4.2" xref="S3.E2.m1.5.5.1.4.2.cmml">t</mi><mo id="S3.E2.m1.5.5.1.4.1" xref="S3.E2.m1.5.5.1.4.1.cmml">+</mo><mrow id="S3.E2.m1.5.5.1.4.3" xref="S3.E2.m1.5.5.1.4.3.cmml"><mi id="S3.E2.m1.5.5.1.4.3.2" xref="S3.E2.m1.5.5.1.4.3.2.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.5.5.1.4.3.1" xref="S3.E2.m1.5.5.1.4.3.1.cmml">​</mo><mrow id="S3.E2.m1.5.5.1.4.3.3.2" xref="S3.E2.m1.5.5.1.4.3.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.1.4.3.3.2.1" xref="S3.E2.m1.5.5.1.4.3.cmml">(</mo><mi id="S3.E2.m1.5.5.1.1" xref="S3.E2.m1.5.5.1.1.cmml">r</mi><mo stretchy="false" id="S3.E2.m1.5.5.1.4.3.3.2.2" xref="S3.E2.m1.5.5.1.4.3.cmml">)</mo></mrow></mrow></mrow></mrow><mtext id="S3.E2.m1.8.8.1.1.1.1.1.1.2.3" xref="S3.E2.m1.8.8.1.1.1.1.1.1.2.3a.cmml">SAV</mtext></msubsup><mo id="S3.E2.m1.8.8.1.1.2.2.2.4" xref="S3.E2.m1.8.8.1.1.2.2.3.cmml">;</mo><msubsup id="S3.E2.m1.8.8.1.1.2.2.2.2" xref="S3.E2.m1.8.8.1.1.2.2.2.2.cmml"><mi id="S3.E2.m1.8.8.1.1.2.2.2.2.2.2" xref="S3.E2.m1.8.8.1.1.2.2.2.2.2.2.cmml">𝐪</mi><mrow id="S3.E2.m1.7.7.1" xref="S3.E2.m1.7.7.1.cmml"><mn id="S3.E2.m1.7.7.1.3" xref="S3.E2.m1.7.7.1.3.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.E2.m1.7.7.1.2" xref="S3.E2.m1.7.7.1.2.cmml">:</mo><mrow id="S3.E2.m1.7.7.1.4" xref="S3.E2.m1.7.7.1.4.cmml"><mi id="S3.E2.m1.7.7.1.4.2" xref="S3.E2.m1.7.7.1.4.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.7.7.1.4.1" xref="S3.E2.m1.7.7.1.4.1.cmml">​</mo><mrow id="S3.E2.m1.7.7.1.4.3.2" xref="S3.E2.m1.7.7.1.4.cmml"><mo stretchy="false" id="S3.E2.m1.7.7.1.4.3.2.1" xref="S3.E2.m1.7.7.1.4.cmml">(</mo><mi id="S3.E2.m1.7.7.1.1" xref="S3.E2.m1.7.7.1.1.cmml">r</mi><mo stretchy="false" id="S3.E2.m1.7.7.1.4.3.2.2" xref="S3.E2.m1.7.7.1.4.cmml">)</mo></mrow></mrow></mrow><mrow id="S3.E2.m1.6.6.1.3" xref="S3.E2.m1.8.8.1.1.2.2.2.2.cmml"><mo stretchy="false" id="S3.E2.m1.6.6.1.3.1" xref="S3.E2.m1.8.8.1.1.2.2.2.2.cmml">(</mo><mi id="S3.E2.m1.6.6.1.1" xref="S3.E2.m1.6.6.1.1.cmml">r</mi><mo stretchy="false" id="S3.E2.m1.6.6.1.3.2" xref="S3.E2.m1.8.8.1.1.2.2.2.2.cmml">)</mo></mrow></msubsup><mo stretchy="false" id="S3.E2.m1.8.8.1.1.2.2.2.5" xref="S3.E2.m1.8.8.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.8.8.1.2" xref="S3.E2.m1.8.8.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.8b"><apply id="S3.E2.m1.8.8.1.1.cmml" xref="S3.E2.m1.8.8.1"><eq id="S3.E2.m1.8.8.1.1.3.cmml" xref="S3.E2.m1.8.8.1.1.3"></eq><apply id="S3.E2.m1.8.8.1.1.4.cmml" xref="S3.E2.m1.8.8.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.1.1.4.1.cmml" xref="S3.E2.m1.8.8.1.1.4">subscript</csymbol><apply id="S3.E2.m1.8.8.1.1.4.2.cmml" xref="S3.E2.m1.8.8.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.1.1.4.2.1.cmml" xref="S3.E2.m1.8.8.1.1.4">superscript</csymbol><ci id="S3.E2.m1.8.8.1.1.4.2.2.cmml" xref="S3.E2.m1.8.8.1.1.4.2.2">𝐡</ci><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝑟</ci></apply><apply id="S3.E2.m1.4.4.3.cmml" xref="S3.E2.m1.4.4.3"><ci id="S3.E2.m1.4.4.3.4.cmml" xref="S3.E2.m1.4.4.3.4">:</ci><list id="S3.E2.m1.4.4.3.5.1.cmml" xref="S3.E2.m1.4.4.3.5.2"><ci id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1.1">𝑤</ci><cn type="integer" id="S3.E2.m1.3.3.2.2.cmml" xref="S3.E2.m1.3.3.2.2">1</cn></list><apply id="S3.E2.m1.4.4.3.6.cmml" xref="S3.E2.m1.4.4.3.6"><times id="S3.E2.m1.4.4.3.6.1.cmml" xref="S3.E2.m1.4.4.3.6.1"></times><ci id="S3.E2.m1.4.4.3.6.2.cmml" xref="S3.E2.m1.4.4.3.6.2">𝑁</ci><ci id="S3.E2.m1.4.4.3.3.cmml" xref="S3.E2.m1.4.4.3.3">𝑟</ci></apply></apply></apply><apply id="S3.E2.m1.8.8.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.2"><times id="S3.E2.m1.8.8.1.1.2.3.cmml" xref="S3.E2.m1.8.8.1.1.2.3"></times><apply id="S3.E2.m1.8.8.1.1.2.4.cmml" xref="S3.E2.m1.8.8.1.1.2.4"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.1.1.2.4.1.cmml" xref="S3.E2.m1.8.8.1.1.2.4">subscript</csymbol><ci id="S3.E2.m1.8.8.1.1.2.4.2a.cmml" xref="S3.E2.m1.8.8.1.1.2.4.2"><mtext id="S3.E2.m1.8.8.1.1.2.4.2.cmml" xref="S3.E2.m1.8.8.1.1.2.4.2">Q-Former</mtext></ci><ci id="S3.E2.m1.8.8.1.1.2.4.3a.cmml" xref="S3.E2.m1.8.8.1.1.2.4.3"><mtext mathsize="70%" id="S3.E2.m1.8.8.1.1.2.4.3.cmml" xref="S3.E2.m1.8.8.1.1.2.4.3">MRC</mtext></ci></apply><list id="S3.E2.m1.8.8.1.1.2.2.3.cmml" xref="S3.E2.m1.8.8.1.1.2.2.2"><apply id="S3.E2.m1.8.8.1.1.1.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E2.m1.8.8.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.8.8.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.2.2">𝐡</ci><ci id="S3.E2.m1.8.8.1.1.1.1.1.1.2.3a.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.2.3"><mtext mathsize="70%" id="S3.E2.m1.8.8.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.8.8.1.1.1.1.1.1.2.3">SAV</mtext></ci></apply><apply id="S3.E2.m1.5.5.1.cmml" xref="S3.E2.m1.5.5.1"><ci id="S3.E2.m1.5.5.1.2.cmml" xref="S3.E2.m1.5.5.1.2">:</ci><ci id="S3.E2.m1.5.5.1.3.cmml" xref="S3.E2.m1.5.5.1.3">𝑡</ci><apply id="S3.E2.m1.5.5.1.4.cmml" xref="S3.E2.m1.5.5.1.4"><plus id="S3.E2.m1.5.5.1.4.1.cmml" xref="S3.E2.m1.5.5.1.4.1"></plus><ci id="S3.E2.m1.5.5.1.4.2.cmml" xref="S3.E2.m1.5.5.1.4.2">𝑡</ci><apply id="S3.E2.m1.5.5.1.4.3.cmml" xref="S3.E2.m1.5.5.1.4.3"><times id="S3.E2.m1.5.5.1.4.3.1.cmml" xref="S3.E2.m1.5.5.1.4.3.1"></times><ci id="S3.E2.m1.5.5.1.4.3.2.cmml" xref="S3.E2.m1.5.5.1.4.3.2">𝑘</ci><ci id="S3.E2.m1.5.5.1.1.cmml" xref="S3.E2.m1.5.5.1.1">𝑟</ci></apply></apply></apply></apply><apply id="S3.E2.m1.8.8.1.1.2.2.2.2.cmml" xref="S3.E2.m1.8.8.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.1.1.2.2.2.2.1.cmml" xref="S3.E2.m1.8.8.1.1.2.2.2.2">subscript</csymbol><apply id="S3.E2.m1.8.8.1.1.2.2.2.2.2.cmml" xref="S3.E2.m1.8.8.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.8.8.1.1.2.2.2.2.2.1.cmml" xref="S3.E2.m1.8.8.1.1.2.2.2.2">superscript</csymbol><ci id="S3.E2.m1.8.8.1.1.2.2.2.2.2.2.cmml" xref="S3.E2.m1.8.8.1.1.2.2.2.2.2.2">𝐪</ci><ci id="S3.E2.m1.6.6.1.1.cmml" xref="S3.E2.m1.6.6.1.1">𝑟</ci></apply><apply id="S3.E2.m1.7.7.1.cmml" xref="S3.E2.m1.7.7.1"><ci id="S3.E2.m1.7.7.1.2.cmml" xref="S3.E2.m1.7.7.1.2">:</ci><cn type="integer" id="S3.E2.m1.7.7.1.3.cmml" xref="S3.E2.m1.7.7.1.3">1</cn><apply id="S3.E2.m1.7.7.1.4.cmml" xref="S3.E2.m1.7.7.1.4"><times id="S3.E2.m1.7.7.1.4.1.cmml" xref="S3.E2.m1.7.7.1.4.1"></times><ci id="S3.E2.m1.7.7.1.4.2.cmml" xref="S3.E2.m1.7.7.1.4.2">𝑁</ci><ci id="S3.E2.m1.7.7.1.1.cmml" xref="S3.E2.m1.7.7.1.1">𝑟</ci></apply></apply></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.8c">\mathbf{h}^{(r)}_{w,1:N(r)}=\text{Q-Former}_{\text{MRC}}(\mathbf{h}^{\text{SAV}}_{t:t+k(r)};\mathbf{q}^{(r)}_{1:N(r)}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.12" class="ltx_p">where <math id="S3.SS2.p1.4.m1.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S3.SS2.p1.4.m1.1a"><mi id="S3.SS2.p1.4.m1.1.1" xref="S3.SS2.p1.4.m1.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m1.1b"><ci id="S3.SS2.p1.4.m1.1.1.cmml" xref="S3.SS2.p1.4.m1.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m1.1c">w</annotation></semantics></math> is the window index and <math id="S3.SS2.p1.5.m2.1" class="ltx_Math" alttext="k(r)" display="inline"><semantics id="S3.SS2.p1.5.m2.1a"><mrow id="S3.SS2.p1.5.m2.1.2" xref="S3.SS2.p1.5.m2.1.2.cmml"><mi id="S3.SS2.p1.5.m2.1.2.2" xref="S3.SS2.p1.5.m2.1.2.2.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.5.m2.1.2.1" xref="S3.SS2.p1.5.m2.1.2.1.cmml">​</mo><mrow id="S3.SS2.p1.5.m2.1.2.3.2" xref="S3.SS2.p1.5.m2.1.2.cmml"><mo stretchy="false" id="S3.SS2.p1.5.m2.1.2.3.2.1" xref="S3.SS2.p1.5.m2.1.2.cmml">(</mo><mi id="S3.SS2.p1.5.m2.1.1" xref="S3.SS2.p1.5.m2.1.1.cmml">r</mi><mo stretchy="false" id="S3.SS2.p1.5.m2.1.2.3.2.2" xref="S3.SS2.p1.5.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m2.1b"><apply id="S3.SS2.p1.5.m2.1.2.cmml" xref="S3.SS2.p1.5.m2.1.2"><times id="S3.SS2.p1.5.m2.1.2.1.cmml" xref="S3.SS2.p1.5.m2.1.2.1"></times><ci id="S3.SS2.p1.5.m2.1.2.2.cmml" xref="S3.SS2.p1.5.m2.1.2.2">𝑘</ci><ci id="S3.SS2.p1.5.m2.1.1.cmml" xref="S3.SS2.p1.5.m2.1.1">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m2.1c">k(r)</annotation></semantics></math> is the number of input video frames in each window at resolution level <math id="S3.SS2.p1.6.m3.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS2.p1.6.m3.1a"><mi id="S3.SS2.p1.6.m3.1.1" xref="S3.SS2.p1.6.m3.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m3.1b"><ci id="S3.SS2.p1.6.m3.1.1.cmml" xref="S3.SS2.p1.6.m3.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m3.1c">r</annotation></semantics></math>, and <math id="S3.SS2.p1.7.m4.1" class="ltx_Math" alttext="\text{Q-Former}_{\text{MRC}}(\cdot)" display="inline"><semantics id="S3.SS2.p1.7.m4.1a"><mrow id="S3.SS2.p1.7.m4.1.2" xref="S3.SS2.p1.7.m4.1.2.cmml"><msub id="S3.SS2.p1.7.m4.1.2.2" xref="S3.SS2.p1.7.m4.1.2.2.cmml"><mtext id="S3.SS2.p1.7.m4.1.2.2.2" xref="S3.SS2.p1.7.m4.1.2.2.2a.cmml">Q-Former</mtext><mtext id="S3.SS2.p1.7.m4.1.2.2.3" xref="S3.SS2.p1.7.m4.1.2.2.3a.cmml">MRC</mtext></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p1.7.m4.1.2.1" xref="S3.SS2.p1.7.m4.1.2.1.cmml">​</mo><mrow id="S3.SS2.p1.7.m4.1.2.3.2" xref="S3.SS2.p1.7.m4.1.2.cmml"><mo stretchy="false" id="S3.SS2.p1.7.m4.1.2.3.2.1" xref="S3.SS2.p1.7.m4.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS2.p1.7.m4.1.1" xref="S3.SS2.p1.7.m4.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS2.p1.7.m4.1.2.3.2.2" xref="S3.SS2.p1.7.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m4.1b"><apply id="S3.SS2.p1.7.m4.1.2.cmml" xref="S3.SS2.p1.7.m4.1.2"><times id="S3.SS2.p1.7.m4.1.2.1.cmml" xref="S3.SS2.p1.7.m4.1.2.1"></times><apply id="S3.SS2.p1.7.m4.1.2.2.cmml" xref="S3.SS2.p1.7.m4.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.7.m4.1.2.2.1.cmml" xref="S3.SS2.p1.7.m4.1.2.2">subscript</csymbol><ci id="S3.SS2.p1.7.m4.1.2.2.2a.cmml" xref="S3.SS2.p1.7.m4.1.2.2.2"><mtext id="S3.SS2.p1.7.m4.1.2.2.2.cmml" xref="S3.SS2.p1.7.m4.1.2.2.2">Q-Former</mtext></ci><ci id="S3.SS2.p1.7.m4.1.2.2.3a.cmml" xref="S3.SS2.p1.7.m4.1.2.2.3"><mtext mathsize="70%" id="S3.SS2.p1.7.m4.1.2.2.3.cmml" xref="S3.SS2.p1.7.m4.1.2.2.3">MRC</mtext></ci></apply><ci id="S3.SS2.p1.7.m4.1.1.cmml" xref="S3.SS2.p1.7.m4.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m4.1c">\text{Q-Former}_{\text{MRC}}(\cdot)</annotation></semantics></math> denotes the Q-Former computation <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib24" title="" class="ltx_ref">2023a</a>)</cite>. The output query vectors are <math id="S3.SS2.p1.8.m5.4" class="ltx_Math" alttext="\mathbf{h}^{(r)}_{w,1:N(r)}" display="inline"><semantics id="S3.SS2.p1.8.m5.4a"><msubsup id="S3.SS2.p1.8.m5.4.5" xref="S3.SS2.p1.8.m5.4.5.cmml"><mi id="S3.SS2.p1.8.m5.4.5.2.2" xref="S3.SS2.p1.8.m5.4.5.2.2.cmml">𝐡</mi><mrow id="S3.SS2.p1.8.m5.4.4.3" xref="S3.SS2.p1.8.m5.4.4.3.cmml"><mrow id="S3.SS2.p1.8.m5.4.4.3.5.2" xref="S3.SS2.p1.8.m5.4.4.3.5.1.cmml"><mi id="S3.SS2.p1.8.m5.2.2.1.1" xref="S3.SS2.p1.8.m5.2.2.1.1.cmml">w</mi><mo id="S3.SS2.p1.8.m5.4.4.3.5.2.1" xref="S3.SS2.p1.8.m5.4.4.3.5.1.cmml">,</mo><mn id="S3.SS2.p1.8.m5.3.3.2.2" xref="S3.SS2.p1.8.m5.3.3.2.2.cmml">1</mn></mrow><mo lspace="0.278em" rspace="0.278em" id="S3.SS2.p1.8.m5.4.4.3.4" xref="S3.SS2.p1.8.m5.4.4.3.4.cmml">:</mo><mrow id="S3.SS2.p1.8.m5.4.4.3.6" xref="S3.SS2.p1.8.m5.4.4.3.6.cmml"><mi id="S3.SS2.p1.8.m5.4.4.3.6.2" xref="S3.SS2.p1.8.m5.4.4.3.6.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.8.m5.4.4.3.6.1" xref="S3.SS2.p1.8.m5.4.4.3.6.1.cmml">​</mo><mrow id="S3.SS2.p1.8.m5.4.4.3.6.3.2" xref="S3.SS2.p1.8.m5.4.4.3.6.cmml"><mo stretchy="false" id="S3.SS2.p1.8.m5.4.4.3.6.3.2.1" xref="S3.SS2.p1.8.m5.4.4.3.6.cmml">(</mo><mi id="S3.SS2.p1.8.m5.4.4.3.3" xref="S3.SS2.p1.8.m5.4.4.3.3.cmml">r</mi><mo stretchy="false" id="S3.SS2.p1.8.m5.4.4.3.6.3.2.2" xref="S3.SS2.p1.8.m5.4.4.3.6.cmml">)</mo></mrow></mrow></mrow><mrow id="S3.SS2.p1.8.m5.1.1.1.3" xref="S3.SS2.p1.8.m5.4.5.cmml"><mo stretchy="false" id="S3.SS2.p1.8.m5.1.1.1.3.1" xref="S3.SS2.p1.8.m5.4.5.cmml">(</mo><mi id="S3.SS2.p1.8.m5.1.1.1.1" xref="S3.SS2.p1.8.m5.1.1.1.1.cmml">r</mi><mo stretchy="false" id="S3.SS2.p1.8.m5.1.1.1.3.2" xref="S3.SS2.p1.8.m5.4.5.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m5.4b"><apply id="S3.SS2.p1.8.m5.4.5.cmml" xref="S3.SS2.p1.8.m5.4.5"><csymbol cd="ambiguous" id="S3.SS2.p1.8.m5.4.5.1.cmml" xref="S3.SS2.p1.8.m5.4.5">subscript</csymbol><apply id="S3.SS2.p1.8.m5.4.5.2.cmml" xref="S3.SS2.p1.8.m5.4.5"><csymbol cd="ambiguous" id="S3.SS2.p1.8.m5.4.5.2.1.cmml" xref="S3.SS2.p1.8.m5.4.5">superscript</csymbol><ci id="S3.SS2.p1.8.m5.4.5.2.2.cmml" xref="S3.SS2.p1.8.m5.4.5.2.2">𝐡</ci><ci id="S3.SS2.p1.8.m5.1.1.1.1.cmml" xref="S3.SS2.p1.8.m5.1.1.1.1">𝑟</ci></apply><apply id="S3.SS2.p1.8.m5.4.4.3.cmml" xref="S3.SS2.p1.8.m5.4.4.3"><ci id="S3.SS2.p1.8.m5.4.4.3.4.cmml" xref="S3.SS2.p1.8.m5.4.4.3.4">:</ci><list id="S3.SS2.p1.8.m5.4.4.3.5.1.cmml" xref="S3.SS2.p1.8.m5.4.4.3.5.2"><ci id="S3.SS2.p1.8.m5.2.2.1.1.cmml" xref="S3.SS2.p1.8.m5.2.2.1.1">𝑤</ci><cn type="integer" id="S3.SS2.p1.8.m5.3.3.2.2.cmml" xref="S3.SS2.p1.8.m5.3.3.2.2">1</cn></list><apply id="S3.SS2.p1.8.m5.4.4.3.6.cmml" xref="S3.SS2.p1.8.m5.4.4.3.6"><times id="S3.SS2.p1.8.m5.4.4.3.6.1.cmml" xref="S3.SS2.p1.8.m5.4.4.3.6.1"></times><ci id="S3.SS2.p1.8.m5.4.4.3.6.2.cmml" xref="S3.SS2.p1.8.m5.4.4.3.6.2">𝑁</ci><ci id="S3.SS2.p1.8.m5.4.4.3.3.cmml" xref="S3.SS2.p1.8.m5.4.4.3.3">𝑟</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m5.4c">\mathbf{h}^{(r)}_{w,1:N(r)}</annotation></semantics></math>. If the input sequence length of the MRC Q-Former is <math id="S3.SS2.p1.9.m6.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.p1.9.m6.1a"><mi id="S3.SS2.p1.9.m6.1.1" xref="S3.SS2.p1.9.m6.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m6.1b"><ci id="S3.SS2.p1.9.m6.1.1.cmml" xref="S3.SS2.p1.9.m6.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m6.1c">T</annotation></semantics></math>, the number of sliding windows <math id="S3.SS2.p1.10.m7.1" class="ltx_Math" alttext="W({r})\in\mathbb{N}^{+}" display="inline"><semantics id="S3.SS2.p1.10.m7.1a"><mrow id="S3.SS2.p1.10.m7.1.2" xref="S3.SS2.p1.10.m7.1.2.cmml"><mrow id="S3.SS2.p1.10.m7.1.2.2" xref="S3.SS2.p1.10.m7.1.2.2.cmml"><mi id="S3.SS2.p1.10.m7.1.2.2.2" xref="S3.SS2.p1.10.m7.1.2.2.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.10.m7.1.2.2.1" xref="S3.SS2.p1.10.m7.1.2.2.1.cmml">​</mo><mrow id="S3.SS2.p1.10.m7.1.2.2.3.2" xref="S3.SS2.p1.10.m7.1.2.2.cmml"><mo stretchy="false" id="S3.SS2.p1.10.m7.1.2.2.3.2.1" xref="S3.SS2.p1.10.m7.1.2.2.cmml">(</mo><mi id="S3.SS2.p1.10.m7.1.1" xref="S3.SS2.p1.10.m7.1.1.cmml">r</mi><mo stretchy="false" id="S3.SS2.p1.10.m7.1.2.2.3.2.2" xref="S3.SS2.p1.10.m7.1.2.2.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p1.10.m7.1.2.1" xref="S3.SS2.p1.10.m7.1.2.1.cmml">∈</mo><msup id="S3.SS2.p1.10.m7.1.2.3" xref="S3.SS2.p1.10.m7.1.2.3.cmml"><mi id="S3.SS2.p1.10.m7.1.2.3.2" xref="S3.SS2.p1.10.m7.1.2.3.2.cmml">ℕ</mi><mo id="S3.SS2.p1.10.m7.1.2.3.3" xref="S3.SS2.p1.10.m7.1.2.3.3.cmml">+</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m7.1b"><apply id="S3.SS2.p1.10.m7.1.2.cmml" xref="S3.SS2.p1.10.m7.1.2"><in id="S3.SS2.p1.10.m7.1.2.1.cmml" xref="S3.SS2.p1.10.m7.1.2.1"></in><apply id="S3.SS2.p1.10.m7.1.2.2.cmml" xref="S3.SS2.p1.10.m7.1.2.2"><times id="S3.SS2.p1.10.m7.1.2.2.1.cmml" xref="S3.SS2.p1.10.m7.1.2.2.1"></times><ci id="S3.SS2.p1.10.m7.1.2.2.2.cmml" xref="S3.SS2.p1.10.m7.1.2.2.2">𝑊</ci><ci id="S3.SS2.p1.10.m7.1.1.cmml" xref="S3.SS2.p1.10.m7.1.1">𝑟</ci></apply><apply id="S3.SS2.p1.10.m7.1.2.3.cmml" xref="S3.SS2.p1.10.m7.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.p1.10.m7.1.2.3.1.cmml" xref="S3.SS2.p1.10.m7.1.2.3">superscript</csymbol><ci id="S3.SS2.p1.10.m7.1.2.3.2.cmml" xref="S3.SS2.p1.10.m7.1.2.3.2">ℕ</ci><plus id="S3.SS2.p1.10.m7.1.2.3.3.cmml" xref="S3.SS2.p1.10.m7.1.2.3.3"></plus></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m7.1c">W({r})\in\mathbb{N}^{+}</annotation></semantics></math> becomes <math id="S3.SS2.p1.11.m8.2" class="ltx_Math" alttext="\lceil T/k(r)\rceil" display="inline"><semantics id="S3.SS2.p1.11.m8.2a"><mrow id="S3.SS2.p1.11.m8.2.2.1" xref="S3.SS2.p1.11.m8.2.2.2.cmml"><mo stretchy="false" id="S3.SS2.p1.11.m8.2.2.1.2" xref="S3.SS2.p1.11.m8.2.2.2.1.cmml">⌈</mo><mrow id="S3.SS2.p1.11.m8.2.2.1.1" xref="S3.SS2.p1.11.m8.2.2.1.1.cmml"><mrow id="S3.SS2.p1.11.m8.2.2.1.1.2" xref="S3.SS2.p1.11.m8.2.2.1.1.2.cmml"><mi id="S3.SS2.p1.11.m8.2.2.1.1.2.2" xref="S3.SS2.p1.11.m8.2.2.1.1.2.2.cmml">T</mi><mo id="S3.SS2.p1.11.m8.2.2.1.1.2.1" xref="S3.SS2.p1.11.m8.2.2.1.1.2.1.cmml">/</mo><mi id="S3.SS2.p1.11.m8.2.2.1.1.2.3" xref="S3.SS2.p1.11.m8.2.2.1.1.2.3.cmml">k</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.p1.11.m8.2.2.1.1.1" xref="S3.SS2.p1.11.m8.2.2.1.1.1.cmml">​</mo><mrow id="S3.SS2.p1.11.m8.2.2.1.1.3.2" xref="S3.SS2.p1.11.m8.2.2.1.1.cmml"><mo stretchy="false" id="S3.SS2.p1.11.m8.2.2.1.1.3.2.1" xref="S3.SS2.p1.11.m8.2.2.1.1.cmml">(</mo><mi id="S3.SS2.p1.11.m8.1.1" xref="S3.SS2.p1.11.m8.1.1.cmml">r</mi><mo stretchy="false" id="S3.SS2.p1.11.m8.2.2.1.1.3.2.2" xref="S3.SS2.p1.11.m8.2.2.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS2.p1.11.m8.2.2.1.3" xref="S3.SS2.p1.11.m8.2.2.2.1.cmml">⌉</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.11.m8.2b"><apply id="S3.SS2.p1.11.m8.2.2.2.cmml" xref="S3.SS2.p1.11.m8.2.2.1"><ceiling id="S3.SS2.p1.11.m8.2.2.2.1.cmml" xref="S3.SS2.p1.11.m8.2.2.1.2"></ceiling><apply id="S3.SS2.p1.11.m8.2.2.1.1.cmml" xref="S3.SS2.p1.11.m8.2.2.1.1"><times id="S3.SS2.p1.11.m8.2.2.1.1.1.cmml" xref="S3.SS2.p1.11.m8.2.2.1.1.1"></times><apply id="S3.SS2.p1.11.m8.2.2.1.1.2.cmml" xref="S3.SS2.p1.11.m8.2.2.1.1.2"><divide id="S3.SS2.p1.11.m8.2.2.1.1.2.1.cmml" xref="S3.SS2.p1.11.m8.2.2.1.1.2.1"></divide><ci id="S3.SS2.p1.11.m8.2.2.1.1.2.2.cmml" xref="S3.SS2.p1.11.m8.2.2.1.1.2.2">𝑇</ci><ci id="S3.SS2.p1.11.m8.2.2.1.1.2.3.cmml" xref="S3.SS2.p1.11.m8.2.2.1.1.2.3">𝑘</ci></apply><ci id="S3.SS2.p1.11.m8.1.1.cmml" xref="S3.SS2.p1.11.m8.1.1">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.11.m8.2c">\lceil T/k(r)\rceil</annotation></semantics></math>, and the overall output sequence length from the MRC Q-Former will be <math id="S3.SS2.p1.12.m9.2" class="ltx_Math" alttext="W(r)\times N(r)" display="inline"><semantics id="S3.SS2.p1.12.m9.2a"><mrow id="S3.SS2.p1.12.m9.2.3" xref="S3.SS2.p1.12.m9.2.3.cmml"><mrow id="S3.SS2.p1.12.m9.2.3.2" xref="S3.SS2.p1.12.m9.2.3.2.cmml"><mrow id="S3.SS2.p1.12.m9.2.3.2.2" xref="S3.SS2.p1.12.m9.2.3.2.2.cmml"><mi id="S3.SS2.p1.12.m9.2.3.2.2.2" xref="S3.SS2.p1.12.m9.2.3.2.2.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.12.m9.2.3.2.2.1" xref="S3.SS2.p1.12.m9.2.3.2.2.1.cmml">​</mo><mrow id="S3.SS2.p1.12.m9.2.3.2.2.3.2" xref="S3.SS2.p1.12.m9.2.3.2.2.cmml"><mo stretchy="false" id="S3.SS2.p1.12.m9.2.3.2.2.3.2.1" xref="S3.SS2.p1.12.m9.2.3.2.2.cmml">(</mo><mi id="S3.SS2.p1.12.m9.1.1" xref="S3.SS2.p1.12.m9.1.1.cmml">r</mi><mo rspace="0.055em" stretchy="false" id="S3.SS2.p1.12.m9.2.3.2.2.3.2.2" xref="S3.SS2.p1.12.m9.2.3.2.2.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S3.SS2.p1.12.m9.2.3.2.1" xref="S3.SS2.p1.12.m9.2.3.2.1.cmml">×</mo><mi id="S3.SS2.p1.12.m9.2.3.2.3" xref="S3.SS2.p1.12.m9.2.3.2.3.cmml">N</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.p1.12.m9.2.3.1" xref="S3.SS2.p1.12.m9.2.3.1.cmml">​</mo><mrow id="S3.SS2.p1.12.m9.2.3.3.2" xref="S3.SS2.p1.12.m9.2.3.cmml"><mo stretchy="false" id="S3.SS2.p1.12.m9.2.3.3.2.1" xref="S3.SS2.p1.12.m9.2.3.cmml">(</mo><mi id="S3.SS2.p1.12.m9.2.2" xref="S3.SS2.p1.12.m9.2.2.cmml">r</mi><mo stretchy="false" id="S3.SS2.p1.12.m9.2.3.3.2.2" xref="S3.SS2.p1.12.m9.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.12.m9.2b"><apply id="S3.SS2.p1.12.m9.2.3.cmml" xref="S3.SS2.p1.12.m9.2.3"><times id="S3.SS2.p1.12.m9.2.3.1.cmml" xref="S3.SS2.p1.12.m9.2.3.1"></times><apply id="S3.SS2.p1.12.m9.2.3.2.cmml" xref="S3.SS2.p1.12.m9.2.3.2"><times id="S3.SS2.p1.12.m9.2.3.2.1.cmml" xref="S3.SS2.p1.12.m9.2.3.2.1"></times><apply id="S3.SS2.p1.12.m9.2.3.2.2.cmml" xref="S3.SS2.p1.12.m9.2.3.2.2"><times id="S3.SS2.p1.12.m9.2.3.2.2.1.cmml" xref="S3.SS2.p1.12.m9.2.3.2.2.1"></times><ci id="S3.SS2.p1.12.m9.2.3.2.2.2.cmml" xref="S3.SS2.p1.12.m9.2.3.2.2.2">𝑊</ci><ci id="S3.SS2.p1.12.m9.1.1.cmml" xref="S3.SS2.p1.12.m9.1.1">𝑟</ci></apply><ci id="S3.SS2.p1.12.m9.2.3.2.3.cmml" xref="S3.SS2.p1.12.m9.2.3.2.3">𝑁</ci></apply><ci id="S3.SS2.p1.12.m9.2.2.cmml" xref="S3.SS2.p1.12.m9.2.2">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.12.m9.2c">W(r)\times N(r)</annotation></semantics></math>. The sliding window design enables the length of the input sequence to vary according to the input feature sequence lengths. It hence achieves a better balance between the degree of information reserved and the computation and storage costs than using a single Q-Former for the entire sequence.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2406.15704/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="178" height="82" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Illustration of the MRC Q-Former structure with two levels of temporal resolutions. The high-resolution sliding window covers <math id="S3.F2.3.m1.1" class="ltx_Math" alttext="k=5" display="inline"><semantics id="S3.F2.3.m1.1b"><mrow id="S3.F2.3.m1.1.1" xref="S3.F2.3.m1.1.1.cmml"><mi id="S3.F2.3.m1.1.1.2" xref="S3.F2.3.m1.1.1.2.cmml">k</mi><mo id="S3.F2.3.m1.1.1.1" xref="S3.F2.3.m1.1.1.1.cmml">=</mo><mn id="S3.F2.3.m1.1.1.3" xref="S3.F2.3.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.3.m1.1c"><apply id="S3.F2.3.m1.1.1.cmml" xref="S3.F2.3.m1.1.1"><eq id="S3.F2.3.m1.1.1.1.cmml" xref="S3.F2.3.m1.1.1.1"></eq><ci id="S3.F2.3.m1.1.1.2.cmml" xref="S3.F2.3.m1.1.1.2">𝑘</ci><cn type="integer" id="S3.F2.3.m1.1.1.3.cmml" xref="S3.F2.3.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.3.m1.1d">k=5</annotation></semantics></math> input features with two query vectors and the low-resolution Q-Former covers <math id="S3.F2.4.m2.1" class="ltx_Math" alttext="k=25" display="inline"><semantics id="S3.F2.4.m2.1b"><mrow id="S3.F2.4.m2.1.1" xref="S3.F2.4.m2.1.1.cmml"><mi id="S3.F2.4.m2.1.1.2" xref="S3.F2.4.m2.1.1.2.cmml">k</mi><mo id="S3.F2.4.m2.1.1.1" xref="S3.F2.4.m2.1.1.1.cmml">=</mo><mn id="S3.F2.4.m2.1.1.3" xref="S3.F2.4.m2.1.1.3.cmml">25</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.4.m2.1c"><apply id="S3.F2.4.m2.1.1.cmml" xref="S3.F2.4.m2.1.1"><eq id="S3.F2.4.m2.1.1.1.cmml" xref="S3.F2.4.m2.1.1.1"></eq><ci id="S3.F2.4.m2.1.1.2.cmml" xref="S3.F2.4.m2.1.1.2">𝑘</ci><cn type="integer" id="S3.F2.4.m2.1.1.3.cmml" xref="S3.F2.4.m2.1.1.3">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.4.m2.1d">k=25</annotation></semantics></math> with 10 query vectors.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">This operation is repeated for all resolution levels with the resolution-specific query vectors. We ensure that Q-Former output at different resolutions can be synchronised by enforcing Eqn. (<a href="#S3.E3" title="Equation 3 ‣ 3.2 MRC Q-Former ‣ 3 video-SALMONN ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), where <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">C</annotation></semantics></math> is a hyper-parameter representing the total number of output query vectors sent to the LLM:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.3" class="ltx_Math" alttext="W({r})\times N(r)=C." display="block"><semantics id="S3.E3.m1.3a"><mrow id="S3.E3.m1.3.3.1" xref="S3.E3.m1.3.3.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1" xref="S3.E3.m1.3.3.1.1.cmml"><mrow id="S3.E3.m1.3.3.1.1.2" xref="S3.E3.m1.3.3.1.1.2.cmml"><mrow id="S3.E3.m1.3.3.1.1.2.2" xref="S3.E3.m1.3.3.1.1.2.2.cmml"><mrow id="S3.E3.m1.3.3.1.1.2.2.2" xref="S3.E3.m1.3.3.1.1.2.2.2.cmml"><mi id="S3.E3.m1.3.3.1.1.2.2.2.2" xref="S3.E3.m1.3.3.1.1.2.2.2.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.2.2.2.1" xref="S3.E3.m1.3.3.1.1.2.2.2.1.cmml">​</mo><mrow id="S3.E3.m1.3.3.1.1.2.2.2.3.2" xref="S3.E3.m1.3.3.1.1.2.2.2.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.1.1.2.2.2.3.2.1" xref="S3.E3.m1.3.3.1.1.2.2.2.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">r</mi><mo rspace="0.055em" stretchy="false" id="S3.E3.m1.3.3.1.1.2.2.2.3.2.2" xref="S3.E3.m1.3.3.1.1.2.2.2.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S3.E3.m1.3.3.1.1.2.2.1" xref="S3.E3.m1.3.3.1.1.2.2.1.cmml">×</mo><mi id="S3.E3.m1.3.3.1.1.2.2.3" xref="S3.E3.m1.3.3.1.1.2.2.3.cmml">N</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.2.1" xref="S3.E3.m1.3.3.1.1.2.1.cmml">​</mo><mrow id="S3.E3.m1.3.3.1.1.2.3.2" xref="S3.E3.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.1.1.2.3.2.1" xref="S3.E3.m1.3.3.1.1.2.cmml">(</mo><mi id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml">r</mi><mo stretchy="false" id="S3.E3.m1.3.3.1.1.2.3.2.2" xref="S3.E3.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.3.3.1.1.1" xref="S3.E3.m1.3.3.1.1.1.cmml">=</mo><mi id="S3.E3.m1.3.3.1.1.3" xref="S3.E3.m1.3.3.1.1.3.cmml">C</mi></mrow><mo lspace="0em" id="S3.E3.m1.3.3.1.2" xref="S3.E3.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.3b"><apply id="S3.E3.m1.3.3.1.1.cmml" xref="S3.E3.m1.3.3.1"><eq id="S3.E3.m1.3.3.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1"></eq><apply id="S3.E3.m1.3.3.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.2"><times id="S3.E3.m1.3.3.1.1.2.1.cmml" xref="S3.E3.m1.3.3.1.1.2.1"></times><apply id="S3.E3.m1.3.3.1.1.2.2.cmml" xref="S3.E3.m1.3.3.1.1.2.2"><times id="S3.E3.m1.3.3.1.1.2.2.1.cmml" xref="S3.E3.m1.3.3.1.1.2.2.1"></times><apply id="S3.E3.m1.3.3.1.1.2.2.2.cmml" xref="S3.E3.m1.3.3.1.1.2.2.2"><times id="S3.E3.m1.3.3.1.1.2.2.2.1.cmml" xref="S3.E3.m1.3.3.1.1.2.2.2.1"></times><ci id="S3.E3.m1.3.3.1.1.2.2.2.2.cmml" xref="S3.E3.m1.3.3.1.1.2.2.2.2">𝑊</ci><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝑟</ci></apply><ci id="S3.E3.m1.3.3.1.1.2.2.3.cmml" xref="S3.E3.m1.3.3.1.1.2.2.3">𝑁</ci></apply><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">𝑟</ci></apply><ci id="S3.E3.m1.3.3.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.3c">W({r})\times N(r)=C.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.9" class="ltx_p">When applying smaller windows for finer time scales, a smaller number of query vectors is used for a reduced information capacity, and <span id="S3.SS2.p2.9.1" class="ltx_text ltx_font_italic">vice versa</span>. Note that while keeping the query vectors different for different resolutions, the rest of the MRC Q-Former parameters are shared across all resolution levels as the task of modality alignment is the same. Output query vectors at all resolution levels are combined using a projection layer before sending them to the LLM.</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.4" class="ltx_Math" alttext="\mathbf{H}=\mathbf{W}^{(1)}\mathbf{H}^{(1)}+\dots+\mathbf{W}^{(R)}\mathbf{H}^{(R)}" display="block"><semantics id="S3.E4.m1.4a"><mrow id="S3.E4.m1.4.5" xref="S3.E4.m1.4.5.cmml"><mi id="S3.E4.m1.4.5.2" xref="S3.E4.m1.4.5.2.cmml">𝐇</mi><mo id="S3.E4.m1.4.5.1" xref="S3.E4.m1.4.5.1.cmml">=</mo><mrow id="S3.E4.m1.4.5.3" xref="S3.E4.m1.4.5.3.cmml"><mrow id="S3.E4.m1.4.5.3.2" xref="S3.E4.m1.4.5.3.2.cmml"><msup id="S3.E4.m1.4.5.3.2.2" xref="S3.E4.m1.4.5.3.2.2.cmml"><mi id="S3.E4.m1.4.5.3.2.2.2" xref="S3.E4.m1.4.5.3.2.2.2.cmml">𝐖</mi><mrow id="S3.E4.m1.1.1.1.3" xref="S3.E4.m1.4.5.3.2.2.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.3.1" xref="S3.E4.m1.4.5.3.2.2.cmml">(</mo><mn id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml">1</mn><mo stretchy="false" id="S3.E4.m1.1.1.1.3.2" xref="S3.E4.m1.4.5.3.2.2.cmml">)</mo></mrow></msup><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.5.3.2.1" xref="S3.E4.m1.4.5.3.2.1.cmml">​</mo><msup id="S3.E4.m1.4.5.3.2.3" xref="S3.E4.m1.4.5.3.2.3.cmml"><mi id="S3.E4.m1.4.5.3.2.3.2" xref="S3.E4.m1.4.5.3.2.3.2.cmml">𝐇</mi><mrow id="S3.E4.m1.2.2.1.3" xref="S3.E4.m1.4.5.3.2.3.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.1.3.1" xref="S3.E4.m1.4.5.3.2.3.cmml">(</mo><mn id="S3.E4.m1.2.2.1.1" xref="S3.E4.m1.2.2.1.1.cmml">1</mn><mo stretchy="false" id="S3.E4.m1.2.2.1.3.2" xref="S3.E4.m1.4.5.3.2.3.cmml">)</mo></mrow></msup></mrow><mo id="S3.E4.m1.4.5.3.1" xref="S3.E4.m1.4.5.3.1.cmml">+</mo><mi mathvariant="normal" id="S3.E4.m1.4.5.3.3" xref="S3.E4.m1.4.5.3.3.cmml">⋯</mi><mo id="S3.E4.m1.4.5.3.1a" xref="S3.E4.m1.4.5.3.1.cmml">+</mo><mrow id="S3.E4.m1.4.5.3.4" xref="S3.E4.m1.4.5.3.4.cmml"><msup id="S3.E4.m1.4.5.3.4.2" xref="S3.E4.m1.4.5.3.4.2.cmml"><mi id="S3.E4.m1.4.5.3.4.2.2" xref="S3.E4.m1.4.5.3.4.2.2.cmml">𝐖</mi><mrow id="S3.E4.m1.3.3.1.3" xref="S3.E4.m1.4.5.3.4.2.cmml"><mo stretchy="false" id="S3.E4.m1.3.3.1.3.1" xref="S3.E4.m1.4.5.3.4.2.cmml">(</mo><mi id="S3.E4.m1.3.3.1.1" xref="S3.E4.m1.3.3.1.1.cmml">R</mi><mo stretchy="false" id="S3.E4.m1.3.3.1.3.2" xref="S3.E4.m1.4.5.3.4.2.cmml">)</mo></mrow></msup><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.5.3.4.1" xref="S3.E4.m1.4.5.3.4.1.cmml">​</mo><msup id="S3.E4.m1.4.5.3.4.3" xref="S3.E4.m1.4.5.3.4.3.cmml"><mi id="S3.E4.m1.4.5.3.4.3.2" xref="S3.E4.m1.4.5.3.4.3.2.cmml">𝐇</mi><mrow id="S3.E4.m1.4.4.1.3" xref="S3.E4.m1.4.5.3.4.3.cmml"><mo stretchy="false" id="S3.E4.m1.4.4.1.3.1" xref="S3.E4.m1.4.5.3.4.3.cmml">(</mo><mi id="S3.E4.m1.4.4.1.1" xref="S3.E4.m1.4.4.1.1.cmml">R</mi><mo stretchy="false" id="S3.E4.m1.4.4.1.3.2" xref="S3.E4.m1.4.5.3.4.3.cmml">)</mo></mrow></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.4b"><apply id="S3.E4.m1.4.5.cmml" xref="S3.E4.m1.4.5"><eq id="S3.E4.m1.4.5.1.cmml" xref="S3.E4.m1.4.5.1"></eq><ci id="S3.E4.m1.4.5.2.cmml" xref="S3.E4.m1.4.5.2">𝐇</ci><apply id="S3.E4.m1.4.5.3.cmml" xref="S3.E4.m1.4.5.3"><plus id="S3.E4.m1.4.5.3.1.cmml" xref="S3.E4.m1.4.5.3.1"></plus><apply id="S3.E4.m1.4.5.3.2.cmml" xref="S3.E4.m1.4.5.3.2"><times id="S3.E4.m1.4.5.3.2.1.cmml" xref="S3.E4.m1.4.5.3.2.1"></times><apply id="S3.E4.m1.4.5.3.2.2.cmml" xref="S3.E4.m1.4.5.3.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.4.5.3.2.2.1.cmml" xref="S3.E4.m1.4.5.3.2.2">superscript</csymbol><ci id="S3.E4.m1.4.5.3.2.2.2.cmml" xref="S3.E4.m1.4.5.3.2.2.2">𝐖</ci><cn type="integer" id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1">1</cn></apply><apply id="S3.E4.m1.4.5.3.2.3.cmml" xref="S3.E4.m1.4.5.3.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.4.5.3.2.3.1.cmml" xref="S3.E4.m1.4.5.3.2.3">superscript</csymbol><ci id="S3.E4.m1.4.5.3.2.3.2.cmml" xref="S3.E4.m1.4.5.3.2.3.2">𝐇</ci><cn type="integer" id="S3.E4.m1.2.2.1.1.cmml" xref="S3.E4.m1.2.2.1.1">1</cn></apply></apply><ci id="S3.E4.m1.4.5.3.3.cmml" xref="S3.E4.m1.4.5.3.3">⋯</ci><apply id="S3.E4.m1.4.5.3.4.cmml" xref="S3.E4.m1.4.5.3.4"><times id="S3.E4.m1.4.5.3.4.1.cmml" xref="S3.E4.m1.4.5.3.4.1"></times><apply id="S3.E4.m1.4.5.3.4.2.cmml" xref="S3.E4.m1.4.5.3.4.2"><csymbol cd="ambiguous" id="S3.E4.m1.4.5.3.4.2.1.cmml" xref="S3.E4.m1.4.5.3.4.2">superscript</csymbol><ci id="S3.E4.m1.4.5.3.4.2.2.cmml" xref="S3.E4.m1.4.5.3.4.2.2">𝐖</ci><ci id="S3.E4.m1.3.3.1.1.cmml" xref="S3.E4.m1.3.3.1.1">𝑅</ci></apply><apply id="S3.E4.m1.4.5.3.4.3.cmml" xref="S3.E4.m1.4.5.3.4.3"><csymbol cd="ambiguous" id="S3.E4.m1.4.5.3.4.3.1.cmml" xref="S3.E4.m1.4.5.3.4.3">superscript</csymbol><ci id="S3.E4.m1.4.5.3.4.3.2.cmml" xref="S3.E4.m1.4.5.3.4.3.2">𝐇</ci><ci id="S3.E4.m1.4.4.1.1.cmml" xref="S3.E4.m1.4.4.1.1">𝑅</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.4c">\mathbf{H}=\mathbf{W}^{(1)}\mathbf{H}^{(1)}+\dots+\mathbf{W}^{(R)}\mathbf{H}^{(R)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.8" class="ltx_p">where each <math id="S3.SS2.p2.2.m1.8" class="ltx_Math" alttext="\mathbf{H}^{(r)}=[\mathbf{h}^{(r)}_{w,1:N(r)}]^{\lceil T/k(r)\rceil}_{w=1}\in\mathbb{R}^{C\times D}" display="inline"><semantics id="S3.SS2.p2.2.m1.8a"><mrow id="S3.SS2.p2.2.m1.8.8" xref="S3.SS2.p2.2.m1.8.8.cmml"><msup id="S3.SS2.p2.2.m1.8.8.3" xref="S3.SS2.p2.2.m1.8.8.3.cmml"><mi id="S3.SS2.p2.2.m1.8.8.3.2" xref="S3.SS2.p2.2.m1.8.8.3.2.cmml">𝐇</mi><mrow id="S3.SS2.p2.2.m1.1.1.1.3" xref="S3.SS2.p2.2.m1.8.8.3.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m1.1.1.1.3.1" xref="S3.SS2.p2.2.m1.8.8.3.cmml">(</mo><mi id="S3.SS2.p2.2.m1.1.1.1.1" xref="S3.SS2.p2.2.m1.1.1.1.1.cmml">r</mi><mo stretchy="false" id="S3.SS2.p2.2.m1.1.1.1.3.2" xref="S3.SS2.p2.2.m1.8.8.3.cmml">)</mo></mrow></msup><mo id="S3.SS2.p2.2.m1.8.8.4" xref="S3.SS2.p2.2.m1.8.8.4.cmml">=</mo><msubsup id="S3.SS2.p2.2.m1.8.8.1" xref="S3.SS2.p2.2.m1.8.8.1.cmml"><mrow id="S3.SS2.p2.2.m1.8.8.1.1.1.1" xref="S3.SS2.p2.2.m1.8.8.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m1.8.8.1.1.1.1.2" xref="S3.SS2.p2.2.m1.8.8.1.1.1.2.1.cmml">[</mo><msubsup id="S3.SS2.p2.2.m1.8.8.1.1.1.1.1" xref="S3.SS2.p2.2.m1.8.8.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.2.m1.8.8.1.1.1.1.1.2.2" xref="S3.SS2.p2.2.m1.8.8.1.1.1.1.1.2.2.cmml">𝐡</mi><mrow id="S3.SS2.p2.2.m1.5.5.3" xref="S3.SS2.p2.2.m1.5.5.3.cmml"><mrow id="S3.SS2.p2.2.m1.5.5.3.5.2" xref="S3.SS2.p2.2.m1.5.5.3.5.1.cmml"><mi id="S3.SS2.p2.2.m1.3.3.1.1" xref="S3.SS2.p2.2.m1.3.3.1.1.cmml">w</mi><mo id="S3.SS2.p2.2.m1.5.5.3.5.2.1" xref="S3.SS2.p2.2.m1.5.5.3.5.1.cmml">,</mo><mn id="S3.SS2.p2.2.m1.4.4.2.2" xref="S3.SS2.p2.2.m1.4.4.2.2.cmml">1</mn></mrow><mo lspace="0.278em" rspace="0.278em" id="S3.SS2.p2.2.m1.5.5.3.4" xref="S3.SS2.p2.2.m1.5.5.3.4.cmml">:</mo><mrow id="S3.SS2.p2.2.m1.5.5.3.6" xref="S3.SS2.p2.2.m1.5.5.3.6.cmml"><mi id="S3.SS2.p2.2.m1.5.5.3.6.2" xref="S3.SS2.p2.2.m1.5.5.3.6.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m1.5.5.3.6.1" xref="S3.SS2.p2.2.m1.5.5.3.6.1.cmml">​</mo><mrow id="S3.SS2.p2.2.m1.5.5.3.6.3.2" xref="S3.SS2.p2.2.m1.5.5.3.6.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m1.5.5.3.6.3.2.1" xref="S3.SS2.p2.2.m1.5.5.3.6.cmml">(</mo><mi id="S3.SS2.p2.2.m1.5.5.3.3" xref="S3.SS2.p2.2.m1.5.5.3.3.cmml">r</mi><mo stretchy="false" id="S3.SS2.p2.2.m1.5.5.3.6.3.2.2" xref="S3.SS2.p2.2.m1.5.5.3.6.cmml">)</mo></mrow></mrow></mrow><mrow id="S3.SS2.p2.2.m1.2.2.1.3" xref="S3.SS2.p2.2.m1.8.8.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m1.2.2.1.3.1" xref="S3.SS2.p2.2.m1.8.8.1.1.1.1.1.cmml">(</mo><mi id="S3.SS2.p2.2.m1.2.2.1.1" xref="S3.SS2.p2.2.m1.2.2.1.1.cmml">r</mi><mo stretchy="false" id="S3.SS2.p2.2.m1.2.2.1.3.2" xref="S3.SS2.p2.2.m1.8.8.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo stretchy="false" id="S3.SS2.p2.2.m1.8.8.1.1.1.1.3" xref="S3.SS2.p2.2.m1.8.8.1.1.1.2.1.cmml">]</mo></mrow><mrow id="S3.SS2.p2.2.m1.8.8.1.3" xref="S3.SS2.p2.2.m1.8.8.1.3.cmml"><mi id="S3.SS2.p2.2.m1.8.8.1.3.2" xref="S3.SS2.p2.2.m1.8.8.1.3.2.cmml">w</mi><mo id="S3.SS2.p2.2.m1.8.8.1.3.1" xref="S3.SS2.p2.2.m1.8.8.1.3.1.cmml">=</mo><mn id="S3.SS2.p2.2.m1.8.8.1.3.3" xref="S3.SS2.p2.2.m1.8.8.1.3.3.cmml">1</mn></mrow><mrow id="S3.SS2.p2.2.m1.7.7.2.2" xref="S3.SS2.p2.2.m1.7.7.2.3.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m1.7.7.2.2.2" xref="S3.SS2.p2.2.m1.7.7.2.3.1.cmml">⌈</mo><mrow id="S3.SS2.p2.2.m1.7.7.2.2.1" xref="S3.SS2.p2.2.m1.7.7.2.2.1.cmml"><mrow id="S3.SS2.p2.2.m1.7.7.2.2.1.2" xref="S3.SS2.p2.2.m1.7.7.2.2.1.2.cmml"><mi id="S3.SS2.p2.2.m1.7.7.2.2.1.2.2" xref="S3.SS2.p2.2.m1.7.7.2.2.1.2.2.cmml">T</mi><mo id="S3.SS2.p2.2.m1.7.7.2.2.1.2.1" xref="S3.SS2.p2.2.m1.7.7.2.2.1.2.1.cmml">/</mo><mi id="S3.SS2.p2.2.m1.7.7.2.2.1.2.3" xref="S3.SS2.p2.2.m1.7.7.2.2.1.2.3.cmml">k</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m1.7.7.2.2.1.1" xref="S3.SS2.p2.2.m1.7.7.2.2.1.1.cmml">​</mo><mrow id="S3.SS2.p2.2.m1.7.7.2.2.1.3.2" xref="S3.SS2.p2.2.m1.7.7.2.2.1.cmml"><mo stretchy="false" id="S3.SS2.p2.2.m1.7.7.2.2.1.3.2.1" xref="S3.SS2.p2.2.m1.7.7.2.2.1.cmml">(</mo><mi id="S3.SS2.p2.2.m1.6.6.1.1" xref="S3.SS2.p2.2.m1.6.6.1.1.cmml">r</mi><mo stretchy="false" id="S3.SS2.p2.2.m1.7.7.2.2.1.3.2.2" xref="S3.SS2.p2.2.m1.7.7.2.2.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.SS2.p2.2.m1.7.7.2.2.3" xref="S3.SS2.p2.2.m1.7.7.2.3.1.cmml">⌉</mo></mrow></msubsup><mo id="S3.SS2.p2.2.m1.8.8.5" xref="S3.SS2.p2.2.m1.8.8.5.cmml">∈</mo><msup id="S3.SS2.p2.2.m1.8.8.6" xref="S3.SS2.p2.2.m1.8.8.6.cmml"><mi id="S3.SS2.p2.2.m1.8.8.6.2" xref="S3.SS2.p2.2.m1.8.8.6.2.cmml">ℝ</mi><mrow id="S3.SS2.p2.2.m1.8.8.6.3" xref="S3.SS2.p2.2.m1.8.8.6.3.cmml"><mi id="S3.SS2.p2.2.m1.8.8.6.3.2" xref="S3.SS2.p2.2.m1.8.8.6.3.2.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.2.m1.8.8.6.3.1" xref="S3.SS2.p2.2.m1.8.8.6.3.1.cmml">×</mo><mi id="S3.SS2.p2.2.m1.8.8.6.3.3" xref="S3.SS2.p2.2.m1.8.8.6.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m1.8b"><apply id="S3.SS2.p2.2.m1.8.8.cmml" xref="S3.SS2.p2.2.m1.8.8"><and id="S3.SS2.p2.2.m1.8.8a.cmml" xref="S3.SS2.p2.2.m1.8.8"></and><apply id="S3.SS2.p2.2.m1.8.8b.cmml" xref="S3.SS2.p2.2.m1.8.8"><eq id="S3.SS2.p2.2.m1.8.8.4.cmml" xref="S3.SS2.p2.2.m1.8.8.4"></eq><apply id="S3.SS2.p2.2.m1.8.8.3.cmml" xref="S3.SS2.p2.2.m1.8.8.3"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m1.8.8.3.1.cmml" xref="S3.SS2.p2.2.m1.8.8.3">superscript</csymbol><ci id="S3.SS2.p2.2.m1.8.8.3.2.cmml" xref="S3.SS2.p2.2.m1.8.8.3.2">𝐇</ci><ci id="S3.SS2.p2.2.m1.1.1.1.1.cmml" xref="S3.SS2.p2.2.m1.1.1.1.1">𝑟</ci></apply><apply id="S3.SS2.p2.2.m1.8.8.1.cmml" xref="S3.SS2.p2.2.m1.8.8.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m1.8.8.1.2.cmml" xref="S3.SS2.p2.2.m1.8.8.1">subscript</csymbol><apply id="S3.SS2.p2.2.m1.8.8.1.1.cmml" xref="S3.SS2.p2.2.m1.8.8.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m1.8.8.1.1.2.cmml" xref="S3.SS2.p2.2.m1.8.8.1">superscript</csymbol><apply id="S3.SS2.p2.2.m1.8.8.1.1.1.2.cmml" xref="S3.SS2.p2.2.m1.8.8.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.p2.2.m1.8.8.1.1.1.2.1.cmml" xref="S3.SS2.p2.2.m1.8.8.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS2.p2.2.m1.8.8.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.m1.8.8.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m1.8.8.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.m1.8.8.1.1.1.1.1">subscript</csymbol><apply id="S3.SS2.p2.2.m1.8.8.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.2.m1.8.8.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m1.8.8.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p2.2.m1.8.8.1.1.1.1.1">superscript</csymbol><ci id="S3.SS2.p2.2.m1.8.8.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p2.2.m1.8.8.1.1.1.1.1.2.2">𝐡</ci><ci id="S3.SS2.p2.2.m1.2.2.1.1.cmml" xref="S3.SS2.p2.2.m1.2.2.1.1">𝑟</ci></apply><apply id="S3.SS2.p2.2.m1.5.5.3.cmml" xref="S3.SS2.p2.2.m1.5.5.3"><ci id="S3.SS2.p2.2.m1.5.5.3.4.cmml" xref="S3.SS2.p2.2.m1.5.5.3.4">:</ci><list id="S3.SS2.p2.2.m1.5.5.3.5.1.cmml" xref="S3.SS2.p2.2.m1.5.5.3.5.2"><ci id="S3.SS2.p2.2.m1.3.3.1.1.cmml" xref="S3.SS2.p2.2.m1.3.3.1.1">𝑤</ci><cn type="integer" id="S3.SS2.p2.2.m1.4.4.2.2.cmml" xref="S3.SS2.p2.2.m1.4.4.2.2">1</cn></list><apply id="S3.SS2.p2.2.m1.5.5.3.6.cmml" xref="S3.SS2.p2.2.m1.5.5.3.6"><times id="S3.SS2.p2.2.m1.5.5.3.6.1.cmml" xref="S3.SS2.p2.2.m1.5.5.3.6.1"></times><ci id="S3.SS2.p2.2.m1.5.5.3.6.2.cmml" xref="S3.SS2.p2.2.m1.5.5.3.6.2">𝑁</ci><ci id="S3.SS2.p2.2.m1.5.5.3.3.cmml" xref="S3.SS2.p2.2.m1.5.5.3.3">𝑟</ci></apply></apply></apply></apply><apply id="S3.SS2.p2.2.m1.7.7.2.3.cmml" xref="S3.SS2.p2.2.m1.7.7.2.2"><ceiling id="S3.SS2.p2.2.m1.7.7.2.3.1.cmml" xref="S3.SS2.p2.2.m1.7.7.2.2.2"></ceiling><apply id="S3.SS2.p2.2.m1.7.7.2.2.1.cmml" xref="S3.SS2.p2.2.m1.7.7.2.2.1"><times id="S3.SS2.p2.2.m1.7.7.2.2.1.1.cmml" xref="S3.SS2.p2.2.m1.7.7.2.2.1.1"></times><apply id="S3.SS2.p2.2.m1.7.7.2.2.1.2.cmml" xref="S3.SS2.p2.2.m1.7.7.2.2.1.2"><divide id="S3.SS2.p2.2.m1.7.7.2.2.1.2.1.cmml" xref="S3.SS2.p2.2.m1.7.7.2.2.1.2.1"></divide><ci id="S3.SS2.p2.2.m1.7.7.2.2.1.2.2.cmml" xref="S3.SS2.p2.2.m1.7.7.2.2.1.2.2">𝑇</ci><ci id="S3.SS2.p2.2.m1.7.7.2.2.1.2.3.cmml" xref="S3.SS2.p2.2.m1.7.7.2.2.1.2.3">𝑘</ci></apply><ci id="S3.SS2.p2.2.m1.6.6.1.1.cmml" xref="S3.SS2.p2.2.m1.6.6.1.1">𝑟</ci></apply></apply></apply><apply id="S3.SS2.p2.2.m1.8.8.1.3.cmml" xref="S3.SS2.p2.2.m1.8.8.1.3"><eq id="S3.SS2.p2.2.m1.8.8.1.3.1.cmml" xref="S3.SS2.p2.2.m1.8.8.1.3.1"></eq><ci id="S3.SS2.p2.2.m1.8.8.1.3.2.cmml" xref="S3.SS2.p2.2.m1.8.8.1.3.2">𝑤</ci><cn type="integer" id="S3.SS2.p2.2.m1.8.8.1.3.3.cmml" xref="S3.SS2.p2.2.m1.8.8.1.3.3">1</cn></apply></apply></apply><apply id="S3.SS2.p2.2.m1.8.8c.cmml" xref="S3.SS2.p2.2.m1.8.8"><in id="S3.SS2.p2.2.m1.8.8.5.cmml" xref="S3.SS2.p2.2.m1.8.8.5"></in><share href="#S3.SS2.p2.2.m1.8.8.1.cmml" id="S3.SS2.p2.2.m1.8.8d.cmml" xref="S3.SS2.p2.2.m1.8.8"></share><apply id="S3.SS2.p2.2.m1.8.8.6.cmml" xref="S3.SS2.p2.2.m1.8.8.6"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m1.8.8.6.1.cmml" xref="S3.SS2.p2.2.m1.8.8.6">superscript</csymbol><ci id="S3.SS2.p2.2.m1.8.8.6.2.cmml" xref="S3.SS2.p2.2.m1.8.8.6.2">ℝ</ci><apply id="S3.SS2.p2.2.m1.8.8.6.3.cmml" xref="S3.SS2.p2.2.m1.8.8.6.3"><times id="S3.SS2.p2.2.m1.8.8.6.3.1.cmml" xref="S3.SS2.p2.2.m1.8.8.6.3.1"></times><ci id="S3.SS2.p2.2.m1.8.8.6.3.2.cmml" xref="S3.SS2.p2.2.m1.8.8.6.3.2">𝐶</ci><ci id="S3.SS2.p2.2.m1.8.8.6.3.3.cmml" xref="S3.SS2.p2.2.m1.8.8.6.3.3">𝐷</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m1.8c">\mathbf{H}^{(r)}=[\mathbf{h}^{(r)}_{w,1:N(r)}]^{\lceil T/k(r)\rceil}_{w=1}\in\mathbb{R}^{C\times D}</annotation></semantics></math> includes output query vectors at resolution level <math id="S3.SS2.p2.3.m2.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS2.p2.3.m2.1a"><mi id="S3.SS2.p2.3.m2.1.1" xref="S3.SS2.p2.3.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m2.1b"><ci id="S3.SS2.p2.3.m2.1.1.cmml" xref="S3.SS2.p2.3.m2.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m2.1c">r</annotation></semantics></math>, <math id="S3.SS2.p2.4.m3.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS2.p2.4.m3.1a"><mi id="S3.SS2.p2.4.m3.1.1" xref="S3.SS2.p2.4.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m3.1b"><ci id="S3.SS2.p2.4.m3.1.1.cmml" xref="S3.SS2.p2.4.m3.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m3.1c">D</annotation></semantics></math> is the dimension of output query vectors, and <math id="S3.SS2.p2.5.m4.1" class="ltx_Math" alttext="\mathbf{W}^{(r)}\in\mathbb{R}^{D\times E}" display="inline"><semantics id="S3.SS2.p2.5.m4.1a"><mrow id="S3.SS2.p2.5.m4.1.2" xref="S3.SS2.p2.5.m4.1.2.cmml"><msup id="S3.SS2.p2.5.m4.1.2.2" xref="S3.SS2.p2.5.m4.1.2.2.cmml"><mi id="S3.SS2.p2.5.m4.1.2.2.2" xref="S3.SS2.p2.5.m4.1.2.2.2.cmml">𝐖</mi><mrow id="S3.SS2.p2.5.m4.1.1.1.3" xref="S3.SS2.p2.5.m4.1.2.2.cmml"><mo stretchy="false" id="S3.SS2.p2.5.m4.1.1.1.3.1" xref="S3.SS2.p2.5.m4.1.2.2.cmml">(</mo><mi id="S3.SS2.p2.5.m4.1.1.1.1" xref="S3.SS2.p2.5.m4.1.1.1.1.cmml">r</mi><mo stretchy="false" id="S3.SS2.p2.5.m4.1.1.1.3.2" xref="S3.SS2.p2.5.m4.1.2.2.cmml">)</mo></mrow></msup><mo id="S3.SS2.p2.5.m4.1.2.1" xref="S3.SS2.p2.5.m4.1.2.1.cmml">∈</mo><msup id="S3.SS2.p2.5.m4.1.2.3" xref="S3.SS2.p2.5.m4.1.2.3.cmml"><mi id="S3.SS2.p2.5.m4.1.2.3.2" xref="S3.SS2.p2.5.m4.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS2.p2.5.m4.1.2.3.3" xref="S3.SS2.p2.5.m4.1.2.3.3.cmml"><mi id="S3.SS2.p2.5.m4.1.2.3.3.2" xref="S3.SS2.p2.5.m4.1.2.3.3.2.cmml">D</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.5.m4.1.2.3.3.1" xref="S3.SS2.p2.5.m4.1.2.3.3.1.cmml">×</mo><mi id="S3.SS2.p2.5.m4.1.2.3.3.3" xref="S3.SS2.p2.5.m4.1.2.3.3.3.cmml">E</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m4.1b"><apply id="S3.SS2.p2.5.m4.1.2.cmml" xref="S3.SS2.p2.5.m4.1.2"><in id="S3.SS2.p2.5.m4.1.2.1.cmml" xref="S3.SS2.p2.5.m4.1.2.1"></in><apply id="S3.SS2.p2.5.m4.1.2.2.cmml" xref="S3.SS2.p2.5.m4.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m4.1.2.2.1.cmml" xref="S3.SS2.p2.5.m4.1.2.2">superscript</csymbol><ci id="S3.SS2.p2.5.m4.1.2.2.2.cmml" xref="S3.SS2.p2.5.m4.1.2.2.2">𝐖</ci><ci id="S3.SS2.p2.5.m4.1.1.1.1.cmml" xref="S3.SS2.p2.5.m4.1.1.1.1">𝑟</ci></apply><apply id="S3.SS2.p2.5.m4.1.2.3.cmml" xref="S3.SS2.p2.5.m4.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m4.1.2.3.1.cmml" xref="S3.SS2.p2.5.m4.1.2.3">superscript</csymbol><ci id="S3.SS2.p2.5.m4.1.2.3.2.cmml" xref="S3.SS2.p2.5.m4.1.2.3.2">ℝ</ci><apply id="S3.SS2.p2.5.m4.1.2.3.3.cmml" xref="S3.SS2.p2.5.m4.1.2.3.3"><times id="S3.SS2.p2.5.m4.1.2.3.3.1.cmml" xref="S3.SS2.p2.5.m4.1.2.3.3.1"></times><ci id="S3.SS2.p2.5.m4.1.2.3.3.2.cmml" xref="S3.SS2.p2.5.m4.1.2.3.3.2">𝐷</ci><ci id="S3.SS2.p2.5.m4.1.2.3.3.3.cmml" xref="S3.SS2.p2.5.m4.1.2.3.3.3">𝐸</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m4.1c">\mathbf{W}^{(r)}\in\mathbb{R}^{D\times E}</annotation></semantics></math> projects output query vectors to the LLM input embedding dimension <math id="S3.SS2.p2.6.m5.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.SS2.p2.6.m5.1a"><mi id="S3.SS2.p2.6.m5.1.1" xref="S3.SS2.p2.6.m5.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m5.1b"><ci id="S3.SS2.p2.6.m5.1.1.cmml" xref="S3.SS2.p2.6.m5.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m5.1c">E</annotation></semantics></math>.
Finally, the LLM backbone generates output based on the projected query vectors <math id="S3.SS2.p2.7.m6.1" class="ltx_Math" alttext="\mathbf{H}" display="inline"><semantics id="S3.SS2.p2.7.m6.1a"><mi id="S3.SS2.p2.7.m6.1.1" xref="S3.SS2.p2.7.m6.1.1.cmml">𝐇</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m6.1b"><ci id="S3.SS2.p2.7.m6.1.1.cmml" xref="S3.SS2.p2.7.m6.1.1">𝐇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m6.1c">\mathbf{H}</annotation></semantics></math> and the content of the prompt <math id="S3.SS2.p2.8.m7.4" class="ltx_Math" alttext="\mathbf{c}_{1},\mathbf{c}_{2},\ldots,\mathbf{c}_{M}" display="inline"><semantics id="S3.SS2.p2.8.m7.4a"><mrow id="S3.SS2.p2.8.m7.4.4.3" xref="S3.SS2.p2.8.m7.4.4.4.cmml"><msub id="S3.SS2.p2.8.m7.2.2.1.1" xref="S3.SS2.p2.8.m7.2.2.1.1.cmml"><mi id="S3.SS2.p2.8.m7.2.2.1.1.2" xref="S3.SS2.p2.8.m7.2.2.1.1.2.cmml">𝐜</mi><mn id="S3.SS2.p2.8.m7.2.2.1.1.3" xref="S3.SS2.p2.8.m7.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.p2.8.m7.4.4.3.4" xref="S3.SS2.p2.8.m7.4.4.4.cmml">,</mo><msub id="S3.SS2.p2.8.m7.3.3.2.2" xref="S3.SS2.p2.8.m7.3.3.2.2.cmml"><mi id="S3.SS2.p2.8.m7.3.3.2.2.2" xref="S3.SS2.p2.8.m7.3.3.2.2.2.cmml">𝐜</mi><mn id="S3.SS2.p2.8.m7.3.3.2.2.3" xref="S3.SS2.p2.8.m7.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS2.p2.8.m7.4.4.3.5" xref="S3.SS2.p2.8.m7.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.p2.8.m7.1.1" xref="S3.SS2.p2.8.m7.1.1.cmml">…</mi><mo id="S3.SS2.p2.8.m7.4.4.3.6" xref="S3.SS2.p2.8.m7.4.4.4.cmml">,</mo><msub id="S3.SS2.p2.8.m7.4.4.3.3" xref="S3.SS2.p2.8.m7.4.4.3.3.cmml"><mi id="S3.SS2.p2.8.m7.4.4.3.3.2" xref="S3.SS2.p2.8.m7.4.4.3.3.2.cmml">𝐜</mi><mi id="S3.SS2.p2.8.m7.4.4.3.3.3" xref="S3.SS2.p2.8.m7.4.4.3.3.3.cmml">M</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m7.4b"><list id="S3.SS2.p2.8.m7.4.4.4.cmml" xref="S3.SS2.p2.8.m7.4.4.3"><apply id="S3.SS2.p2.8.m7.2.2.1.1.cmml" xref="S3.SS2.p2.8.m7.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m7.2.2.1.1.1.cmml" xref="S3.SS2.p2.8.m7.2.2.1.1">subscript</csymbol><ci id="S3.SS2.p2.8.m7.2.2.1.1.2.cmml" xref="S3.SS2.p2.8.m7.2.2.1.1.2">𝐜</ci><cn type="integer" id="S3.SS2.p2.8.m7.2.2.1.1.3.cmml" xref="S3.SS2.p2.8.m7.2.2.1.1.3">1</cn></apply><apply id="S3.SS2.p2.8.m7.3.3.2.2.cmml" xref="S3.SS2.p2.8.m7.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m7.3.3.2.2.1.cmml" xref="S3.SS2.p2.8.m7.3.3.2.2">subscript</csymbol><ci id="S3.SS2.p2.8.m7.3.3.2.2.2.cmml" xref="S3.SS2.p2.8.m7.3.3.2.2.2">𝐜</ci><cn type="integer" id="S3.SS2.p2.8.m7.3.3.2.2.3.cmml" xref="S3.SS2.p2.8.m7.3.3.2.2.3">2</cn></apply><ci id="S3.SS2.p2.8.m7.1.1.cmml" xref="S3.SS2.p2.8.m7.1.1">…</ci><apply id="S3.SS2.p2.8.m7.4.4.3.3.cmml" xref="S3.SS2.p2.8.m7.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.8.m7.4.4.3.3.1.cmml" xref="S3.SS2.p2.8.m7.4.4.3.3">subscript</csymbol><ci id="S3.SS2.p2.8.m7.4.4.3.3.2.cmml" xref="S3.SS2.p2.8.m7.4.4.3.3.2">𝐜</ci><ci id="S3.SS2.p2.8.m7.4.4.3.3.3.cmml" xref="S3.SS2.p2.8.m7.4.4.3.3.3">𝑀</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m7.4c">\mathbf{c}_{1},\mathbf{c}_{2},\ldots,\mathbf{c}_{M}</annotation></semantics></math> by</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.2" class="ltx_Math" alttext="\vspace{-0.3cm}\hat{\mathbf{Y}}=\operatorname*{argmax}_{\textbf{Y}}P(\mathbf{Y}|\mathbf{H},\mathbf{c}_{1:M})." display="block"><semantics id="S3.E5.m1.2a"><mrow id="S3.E5.m1.2.2.1" xref="S3.E5.m1.2.2.1.1.cmml"><mrow id="S3.E5.m1.2.2.1.1" xref="S3.E5.m1.2.2.1.1.cmml"><mover accent="true" id="S3.E5.m1.2.2.1.1.3" xref="S3.E5.m1.2.2.1.1.3.cmml"><mi id="S3.E5.m1.2.2.1.1.3.2" xref="S3.E5.m1.2.2.1.1.3.2.cmml">𝐘</mi><mo id="S3.E5.m1.2.2.1.1.3.1" xref="S3.E5.m1.2.2.1.1.3.1.cmml">^</mo></mover><mo rspace="0.1389em" id="S3.E5.m1.2.2.1.1.2" xref="S3.E5.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.2.2.1.1.1" xref="S3.E5.m1.2.2.1.1.1.cmml"><mrow id="S3.E5.m1.2.2.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.3.cmml"><munder id="S3.E5.m1.2.2.1.1.1.3.1" xref="S3.E5.m1.2.2.1.1.1.3.1.cmml"><mo lspace="0.1389em" rspace="0.167em" id="S3.E5.m1.2.2.1.1.1.3.1.2" xref="S3.E5.m1.2.2.1.1.1.3.1.2.cmml">argmax</mo><mtext class="ltx_mathvariant_bold" id="S3.E5.m1.2.2.1.1.1.3.1.3" xref="S3.E5.m1.2.2.1.1.1.3.1.3a.cmml">Y</mtext></munder><mi id="S3.E5.m1.2.2.1.1.1.3.2" xref="S3.E5.m1.2.2.1.1.1.3.2.cmml">P</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E5.m1.2.2.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E5.m1.2.2.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.cmml">𝐘</mi><mo fence="false" id="S3.E5.m1.2.2.1.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">𝐇</mi><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.cmml">,</mo><msub id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">𝐜</mi><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml"><mn id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3.1.cmml">:</mo><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.cmml">M</mi></mrow></msub></mrow></mrow><mo stretchy="false" id="S3.E5.m1.2.2.1.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E5.m1.2.2.1.2" xref="S3.E5.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.2b"><apply id="S3.E5.m1.2.2.1.1.cmml" xref="S3.E5.m1.2.2.1"><eq id="S3.E5.m1.2.2.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.2"></eq><apply id="S3.E5.m1.2.2.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.3"><ci id="S3.E5.m1.2.2.1.1.3.1.cmml" xref="S3.E5.m1.2.2.1.1.3.1">^</ci><ci id="S3.E5.m1.2.2.1.1.3.2.cmml" xref="S3.E5.m1.2.2.1.1.3.2">𝐘</ci></apply><apply id="S3.E5.m1.2.2.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1"><times id="S3.E5.m1.2.2.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.2"></times><apply id="S3.E5.m1.2.2.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.3"><apply id="S3.E5.m1.2.2.1.1.1.3.1.cmml" xref="S3.E5.m1.2.2.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.3.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.3.1">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.1.3.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.3.1.2">argmax</ci><ci id="S3.E5.m1.2.2.1.1.1.3.1.3a.cmml" xref="S3.E5.m1.2.2.1.1.1.3.1.3"><mtext class="ltx_mathvariant_bold" mathsize="70%" id="S3.E5.m1.2.2.1.1.1.3.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.3.1.3">Y</mtext></ci></apply><ci id="S3.E5.m1.2.2.1.1.1.3.2.cmml" xref="S3.E5.m1.2.2.1.1.1.3.2">𝑃</ci></apply><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1"><csymbol cd="latexml" id="S3.E5.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.2">conditional</csymbol><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3">𝐘</ci><list id="S3.E5.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1"><ci id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1">𝐇</ci><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.2">𝐜</ci><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3"><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3.1">:</ci><cn type="integer" id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3.2">1</cn><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.1.1.3.3">𝑀</ci></apply></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.2c">\vspace{-0.3cm}\hat{\mathbf{Y}}=\operatorname*{argmax}_{\textbf{Y}}P(\mathbf{Y}|\mathbf{H},\mathbf{c}_{1:M}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Causal Structure</h4>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2406.15704/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="173" height="62" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The causal attention module in the MRC Q-Former with a block-wise triangular causal mask (grey cells are masked). The number of features per frame here is two as an example.</figcaption>
</figure>
<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">The proposed MRC Q-Former adopts a causal structure as shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.2.1 Causal Structure ‣ 3.2 MRC Q-Former ‣ 3 video-SALMONN ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. To capture the causal temporal correlation among frames that are extracted independently, an additional causal self-attention module is added to the standard Q-Former structure, indicated by the red block in Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.2.1 Causal Structure ‣ 3.2 MRC Q-Former ‣ 3 video-SALMONN ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">With the causal attention module, the encoding of one specific frame also includes the information of all previous frames carried in an auto-regressive way.
This is particularly beneficial for causal reasoning questions, such as the “what happens next” questions <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a href="#bib.bib50" title="" class="ltx_ref">2021</a>)</cite>. Such questions are sometimes difficult to learn using only the positional embeddings.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>System Training</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.5" class="ltx_p">The training data of video tasks such as video QA usually only requires one or two keyframes, and the output queries tend to repeatedly capture the same information. Therefore, a novel diversity loss is proposed to encourage the MRC Q-Former to extract more diverse aspects of the input sequence. Specifically, the diversity loss is formulated as:</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.11" class="ltx_Math" alttext="\mathcal{L}_{\text{diverse}}=\sum_{r=2}^{R}\sum_{w=1}^{W(r)}\sum_{i=1}^{N}\sum_{j=1{,j\neq i}}^{N}\text{sim}(\mathbf{h}_{w,i}^{(r)},\mathbf{h}_{w,j}^{(r)})" display="block"><semantics id="S3.E6.m1.11a"><mrow id="S3.E6.m1.11.11" xref="S3.E6.m1.11.11.cmml"><msub id="S3.E6.m1.11.11.4" xref="S3.E6.m1.11.11.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.11.11.4.2" xref="S3.E6.m1.11.11.4.2.cmml">ℒ</mi><mtext id="S3.E6.m1.11.11.4.3" xref="S3.E6.m1.11.11.4.3a.cmml">diverse</mtext></msub><mo rspace="0.111em" id="S3.E6.m1.11.11.3" xref="S3.E6.m1.11.11.3.cmml">=</mo><mrow id="S3.E6.m1.11.11.2" xref="S3.E6.m1.11.11.2.cmml"><munderover id="S3.E6.m1.11.11.2.3" xref="S3.E6.m1.11.11.2.3.cmml"><mo movablelimits="false" rspace="0em" id="S3.E6.m1.11.11.2.3.2.2" xref="S3.E6.m1.11.11.2.3.2.2.cmml">∑</mo><mrow id="S3.E6.m1.11.11.2.3.2.3" xref="S3.E6.m1.11.11.2.3.2.3.cmml"><mi id="S3.E6.m1.11.11.2.3.2.3.2" xref="S3.E6.m1.11.11.2.3.2.3.2.cmml">r</mi><mo id="S3.E6.m1.11.11.2.3.2.3.1" xref="S3.E6.m1.11.11.2.3.2.3.1.cmml">=</mo><mn id="S3.E6.m1.11.11.2.3.2.3.3" xref="S3.E6.m1.11.11.2.3.2.3.3.cmml">2</mn></mrow><mi id="S3.E6.m1.11.11.2.3.3" xref="S3.E6.m1.11.11.2.3.3.cmml">R</mi></munderover><mrow id="S3.E6.m1.11.11.2.2" xref="S3.E6.m1.11.11.2.2.cmml"><munderover id="S3.E6.m1.11.11.2.2.3" xref="S3.E6.m1.11.11.2.2.3.cmml"><mo movablelimits="false" rspace="0em" id="S3.E6.m1.11.11.2.2.3.2.2" xref="S3.E6.m1.11.11.2.2.3.2.2.cmml">∑</mo><mrow id="S3.E6.m1.11.11.2.2.3.2.3" xref="S3.E6.m1.11.11.2.2.3.2.3.cmml"><mi id="S3.E6.m1.11.11.2.2.3.2.3.2" xref="S3.E6.m1.11.11.2.2.3.2.3.2.cmml">w</mi><mo id="S3.E6.m1.11.11.2.2.3.2.3.1" xref="S3.E6.m1.11.11.2.2.3.2.3.1.cmml">=</mo><mn id="S3.E6.m1.11.11.2.2.3.2.3.3" xref="S3.E6.m1.11.11.2.2.3.2.3.3.cmml">1</mn></mrow><mrow id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.cmml"><mi id="S3.E6.m1.1.1.1.3" xref="S3.E6.m1.1.1.1.3.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.2" xref="S3.E6.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E6.m1.1.1.1.4.2" xref="S3.E6.m1.1.1.1.cmml"><mo stretchy="false" id="S3.E6.m1.1.1.1.4.2.1" xref="S3.E6.m1.1.1.1.cmml">(</mo><mi id="S3.E6.m1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml">r</mi><mo stretchy="false" id="S3.E6.m1.1.1.1.4.2.2" xref="S3.E6.m1.1.1.1.cmml">)</mo></mrow></mrow></munderover><mrow id="S3.E6.m1.11.11.2.2.2" xref="S3.E6.m1.11.11.2.2.2.cmml"><munderover id="S3.E6.m1.11.11.2.2.2.3" xref="S3.E6.m1.11.11.2.2.2.3.cmml"><mo movablelimits="false" rspace="0em" id="S3.E6.m1.11.11.2.2.2.3.2.2" xref="S3.E6.m1.11.11.2.2.2.3.2.2.cmml">∑</mo><mrow id="S3.E6.m1.11.11.2.2.2.3.2.3" xref="S3.E6.m1.11.11.2.2.2.3.2.3.cmml"><mi id="S3.E6.m1.11.11.2.2.2.3.2.3.2" xref="S3.E6.m1.11.11.2.2.2.3.2.3.2.cmml">i</mi><mo id="S3.E6.m1.11.11.2.2.2.3.2.3.1" xref="S3.E6.m1.11.11.2.2.2.3.2.3.1.cmml">=</mo><mn id="S3.E6.m1.11.11.2.2.2.3.2.3.3" xref="S3.E6.m1.11.11.2.2.2.3.2.3.3.cmml">1</mn></mrow><mi id="S3.E6.m1.11.11.2.2.2.3.3" xref="S3.E6.m1.11.11.2.2.2.3.3.cmml">N</mi></munderover><mrow id="S3.E6.m1.11.11.2.2.2.2" xref="S3.E6.m1.11.11.2.2.2.2.cmml"><munderover id="S3.E6.m1.11.11.2.2.2.2.3" xref="S3.E6.m1.11.11.2.2.2.2.3.cmml"><mo movablelimits="false" id="S3.E6.m1.11.11.2.2.2.2.3.2.2" xref="S3.E6.m1.11.11.2.2.2.2.3.2.2.cmml">∑</mo><mrow id="S3.E6.m1.3.3.2.2" xref="S3.E6.m1.3.3.2.3.cmml"><mrow id="S3.E6.m1.2.2.1.1.1" xref="S3.E6.m1.2.2.1.1.1.cmml"><mi id="S3.E6.m1.2.2.1.1.1.2" xref="S3.E6.m1.2.2.1.1.1.2.cmml">j</mi><mo id="S3.E6.m1.2.2.1.1.1.1" xref="S3.E6.m1.2.2.1.1.1.1.cmml">=</mo><mn id="S3.E6.m1.2.2.1.1.1.3" xref="S3.E6.m1.2.2.1.1.1.3.cmml">1</mn></mrow><mo id="S3.E6.m1.3.3.2.2.3" xref="S3.E6.m1.3.3.2.3a.cmml">,</mo><mrow id="S3.E6.m1.3.3.2.2.2" xref="S3.E6.m1.3.3.2.2.2.cmml"><mi id="S3.E6.m1.3.3.2.2.2.2" xref="S3.E6.m1.3.3.2.2.2.2.cmml">j</mi><mo id="S3.E6.m1.3.3.2.2.2.1" xref="S3.E6.m1.3.3.2.2.2.1.cmml">≠</mo><mi id="S3.E6.m1.3.3.2.2.2.3" xref="S3.E6.m1.3.3.2.2.2.3.cmml">i</mi></mrow></mrow><mi id="S3.E6.m1.11.11.2.2.2.2.3.3" xref="S3.E6.m1.11.11.2.2.2.2.3.3.cmml">N</mi></munderover><mrow id="S3.E6.m1.11.11.2.2.2.2.2" xref="S3.E6.m1.11.11.2.2.2.2.2.cmml"><mtext id="S3.E6.m1.11.11.2.2.2.2.2.4" xref="S3.E6.m1.11.11.2.2.2.2.2.4a.cmml">sim</mtext><mo lspace="0em" rspace="0em" id="S3.E6.m1.11.11.2.2.2.2.2.3" xref="S3.E6.m1.11.11.2.2.2.2.2.3.cmml">​</mo><mrow id="S3.E6.m1.11.11.2.2.2.2.2.2.2" xref="S3.E6.m1.11.11.2.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E6.m1.11.11.2.2.2.2.2.2.2.3" xref="S3.E6.m1.11.11.2.2.2.2.2.2.3.cmml">(</mo><msubsup id="S3.E6.m1.10.10.1.1.1.1.1.1.1.1" xref="S3.E6.m1.10.10.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E6.m1.10.10.1.1.1.1.1.1.1.1.2.2" xref="S3.E6.m1.10.10.1.1.1.1.1.1.1.1.2.2.cmml">𝐡</mi><mrow id="S3.E6.m1.5.5.2.4" xref="S3.E6.m1.5.5.2.3.cmml"><mi id="S3.E6.m1.4.4.1.1" xref="S3.E6.m1.4.4.1.1.cmml">w</mi><mo id="S3.E6.m1.5.5.2.4.1" xref="S3.E6.m1.5.5.2.3.cmml">,</mo><mi id="S3.E6.m1.5.5.2.2" xref="S3.E6.m1.5.5.2.2.cmml">i</mi></mrow><mrow id="S3.E6.m1.6.6.1.3" xref="S3.E6.m1.10.10.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E6.m1.6.6.1.3.1" xref="S3.E6.m1.10.10.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E6.m1.6.6.1.1" xref="S3.E6.m1.6.6.1.1.cmml">r</mi><mo stretchy="false" id="S3.E6.m1.6.6.1.3.2" xref="S3.E6.m1.10.10.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.E6.m1.11.11.2.2.2.2.2.2.2.4" xref="S3.E6.m1.11.11.2.2.2.2.2.2.3.cmml">,</mo><msubsup id="S3.E6.m1.11.11.2.2.2.2.2.2.2.2" xref="S3.E6.m1.11.11.2.2.2.2.2.2.2.2.cmml"><mi id="S3.E6.m1.11.11.2.2.2.2.2.2.2.2.2.2" xref="S3.E6.m1.11.11.2.2.2.2.2.2.2.2.2.2.cmml">𝐡</mi><mrow id="S3.E6.m1.8.8.2.4" xref="S3.E6.m1.8.8.2.3.cmml"><mi id="S3.E6.m1.7.7.1.1" xref="S3.E6.m1.7.7.1.1.cmml">w</mi><mo id="S3.E6.m1.8.8.2.4.1" xref="S3.E6.m1.8.8.2.3.cmml">,</mo><mi id="S3.E6.m1.8.8.2.2" xref="S3.E6.m1.8.8.2.2.cmml">j</mi></mrow><mrow id="S3.E6.m1.9.9.1.3" xref="S3.E6.m1.11.11.2.2.2.2.2.2.2.2.cmml"><mo stretchy="false" id="S3.E6.m1.9.9.1.3.1" xref="S3.E6.m1.11.11.2.2.2.2.2.2.2.2.cmml">(</mo><mi id="S3.E6.m1.9.9.1.1" xref="S3.E6.m1.9.9.1.1.cmml">r</mi><mo stretchy="false" id="S3.E6.m1.9.9.1.3.2" xref="S3.E6.m1.11.11.2.2.2.2.2.2.2.2.cmml">)</mo></mrow></msubsup><mo stretchy="false" id="S3.E6.m1.11.11.2.2.2.2.2.2.2.5" xref="S3.E6.m1.11.11.2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.11b"><apply id="S3.E6.m1.11.11.cmml" xref="S3.E6.m1.11.11"><eq id="S3.E6.m1.11.11.3.cmml" xref="S3.E6.m1.11.11.3"></eq><apply id="S3.E6.m1.11.11.4.cmml" xref="S3.E6.m1.11.11.4"><csymbol cd="ambiguous" id="S3.E6.m1.11.11.4.1.cmml" xref="S3.E6.m1.11.11.4">subscript</csymbol><ci id="S3.E6.m1.11.11.4.2.cmml" xref="S3.E6.m1.11.11.4.2">ℒ</ci><ci id="S3.E6.m1.11.11.4.3a.cmml" xref="S3.E6.m1.11.11.4.3"><mtext mathsize="70%" id="S3.E6.m1.11.11.4.3.cmml" xref="S3.E6.m1.11.11.4.3">diverse</mtext></ci></apply><apply id="S3.E6.m1.11.11.2.cmml" xref="S3.E6.m1.11.11.2"><apply id="S3.E6.m1.11.11.2.3.cmml" xref="S3.E6.m1.11.11.2.3"><csymbol cd="ambiguous" id="S3.E6.m1.11.11.2.3.1.cmml" xref="S3.E6.m1.11.11.2.3">superscript</csymbol><apply id="S3.E6.m1.11.11.2.3.2.cmml" xref="S3.E6.m1.11.11.2.3"><csymbol cd="ambiguous" id="S3.E6.m1.11.11.2.3.2.1.cmml" xref="S3.E6.m1.11.11.2.3">subscript</csymbol><sum id="S3.E6.m1.11.11.2.3.2.2.cmml" xref="S3.E6.m1.11.11.2.3.2.2"></sum><apply id="S3.E6.m1.11.11.2.3.2.3.cmml" xref="S3.E6.m1.11.11.2.3.2.3"><eq id="S3.E6.m1.11.11.2.3.2.3.1.cmml" xref="S3.E6.m1.11.11.2.3.2.3.1"></eq><ci id="S3.E6.m1.11.11.2.3.2.3.2.cmml" xref="S3.E6.m1.11.11.2.3.2.3.2">𝑟</ci><cn type="integer" id="S3.E6.m1.11.11.2.3.2.3.3.cmml" xref="S3.E6.m1.11.11.2.3.2.3.3">2</cn></apply></apply><ci id="S3.E6.m1.11.11.2.3.3.cmml" xref="S3.E6.m1.11.11.2.3.3">𝑅</ci></apply><apply id="S3.E6.m1.11.11.2.2.cmml" xref="S3.E6.m1.11.11.2.2"><apply id="S3.E6.m1.11.11.2.2.3.cmml" xref="S3.E6.m1.11.11.2.2.3"><csymbol cd="ambiguous" id="S3.E6.m1.11.11.2.2.3.1.cmml" xref="S3.E6.m1.11.11.2.2.3">superscript</csymbol><apply id="S3.E6.m1.11.11.2.2.3.2.cmml" xref="S3.E6.m1.11.11.2.2.3"><csymbol cd="ambiguous" id="S3.E6.m1.11.11.2.2.3.2.1.cmml" xref="S3.E6.m1.11.11.2.2.3">subscript</csymbol><sum id="S3.E6.m1.11.11.2.2.3.2.2.cmml" xref="S3.E6.m1.11.11.2.2.3.2.2"></sum><apply id="S3.E6.m1.11.11.2.2.3.2.3.cmml" xref="S3.E6.m1.11.11.2.2.3.2.3"><eq id="S3.E6.m1.11.11.2.2.3.2.3.1.cmml" xref="S3.E6.m1.11.11.2.2.3.2.3.1"></eq><ci id="S3.E6.m1.11.11.2.2.3.2.3.2.cmml" xref="S3.E6.m1.11.11.2.2.3.2.3.2">𝑤</ci><cn type="integer" id="S3.E6.m1.11.11.2.2.3.2.3.3.cmml" xref="S3.E6.m1.11.11.2.2.3.2.3.3">1</cn></apply></apply><apply id="S3.E6.m1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"><times id="S3.E6.m1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.2"></times><ci id="S3.E6.m1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.3">𝑊</ci><ci id="S3.E6.m1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1">𝑟</ci></apply></apply><apply id="S3.E6.m1.11.11.2.2.2.cmml" xref="S3.E6.m1.11.11.2.2.2"><apply id="S3.E6.m1.11.11.2.2.2.3.cmml" xref="S3.E6.m1.11.11.2.2.2.3"><csymbol cd="ambiguous" id="S3.E6.m1.11.11.2.2.2.3.1.cmml" xref="S3.E6.m1.11.11.2.2.2.3">superscript</csymbol><apply id="S3.E6.m1.11.11.2.2.2.3.2.cmml" xref="S3.E6.m1.11.11.2.2.2.3"><csymbol cd="ambiguous" id="S3.E6.m1.11.11.2.2.2.3.2.1.cmml" xref="S3.E6.m1.11.11.2.2.2.3">subscript</csymbol><sum id="S3.E6.m1.11.11.2.2.2.3.2.2.cmml" xref="S3.E6.m1.11.11.2.2.2.3.2.2"></sum><apply id="S3.E6.m1.11.11.2.2.2.3.2.3.cmml" xref="S3.E6.m1.11.11.2.2.2.3.2.3"><eq id="S3.E6.m1.11.11.2.2.2.3.2.3.1.cmml" xref="S3.E6.m1.11.11.2.2.2.3.2.3.1"></eq><ci id="S3.E6.m1.11.11.2.2.2.3.2.3.2.cmml" xref="S3.E6.m1.11.11.2.2.2.3.2.3.2">𝑖</ci><cn type="integer" id="S3.E6.m1.11.11.2.2.2.3.2.3.3.cmml" xref="S3.E6.m1.11.11.2.2.2.3.2.3.3">1</cn></apply></apply><ci id="S3.E6.m1.11.11.2.2.2.3.3.cmml" xref="S3.E6.m1.11.11.2.2.2.3.3">𝑁</ci></apply><apply id="S3.E6.m1.11.11.2.2.2.2.cmml" xref="S3.E6.m1.11.11.2.2.2.2"><apply id="S3.E6.m1.11.11.2.2.2.2.3.cmml" xref="S3.E6.m1.11.11.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E6.m1.11.11.2.2.2.2.3.1.cmml" xref="S3.E6.m1.11.11.2.2.2.2.3">superscript</csymbol><apply id="S3.E6.m1.11.11.2.2.2.2.3.2.cmml" xref="S3.E6.m1.11.11.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E6.m1.11.11.2.2.2.2.3.2.1.cmml" xref="S3.E6.m1.11.11.2.2.2.2.3">subscript</csymbol><sum id="S3.E6.m1.11.11.2.2.2.2.3.2.2.cmml" xref="S3.E6.m1.11.11.2.2.2.2.3.2.2"></sum><apply id="S3.E6.m1.3.3.2.3.cmml" xref="S3.E6.m1.3.3.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.3.3.2.3a.cmml" xref="S3.E6.m1.3.3.2.2.3">formulae-sequence</csymbol><apply id="S3.E6.m1.2.2.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1"><eq id="S3.E6.m1.2.2.1.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1"></eq><ci id="S3.E6.m1.2.2.1.1.1.2.cmml" xref="S3.E6.m1.2.2.1.1.1.2">𝑗</ci><cn type="integer" id="S3.E6.m1.2.2.1.1.1.3.cmml" xref="S3.E6.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.E6.m1.3.3.2.2.2.cmml" xref="S3.E6.m1.3.3.2.2.2"><neq id="S3.E6.m1.3.3.2.2.2.1.cmml" xref="S3.E6.m1.3.3.2.2.2.1"></neq><ci id="S3.E6.m1.3.3.2.2.2.2.cmml" xref="S3.E6.m1.3.3.2.2.2.2">𝑗</ci><ci id="S3.E6.m1.3.3.2.2.2.3.cmml" xref="S3.E6.m1.3.3.2.2.2.3">𝑖</ci></apply></apply></apply><ci id="S3.E6.m1.11.11.2.2.2.2.3.3.cmml" xref="S3.E6.m1.11.11.2.2.2.2.3.3">𝑁</ci></apply><apply id="S3.E6.m1.11.11.2.2.2.2.2.cmml" xref="S3.E6.m1.11.11.2.2.2.2.2"><times id="S3.E6.m1.11.11.2.2.2.2.2.3.cmml" xref="S3.E6.m1.11.11.2.2.2.2.2.3"></times><ci id="S3.E6.m1.11.11.2.2.2.2.2.4a.cmml" xref="S3.E6.m1.11.11.2.2.2.2.2.4"><mtext id="S3.E6.m1.11.11.2.2.2.2.2.4.cmml" xref="S3.E6.m1.11.11.2.2.2.2.2.4">sim</mtext></ci><interval closure="open" id="S3.E6.m1.11.11.2.2.2.2.2.2.3.cmml" xref="S3.E6.m1.11.11.2.2.2.2.2.2.2"><apply id="S3.E6.m1.10.10.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.10.10.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.10.10.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.10.10.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E6.m1.10.10.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.10.10.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.10.10.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.10.10.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E6.m1.10.10.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.10.10.1.1.1.1.1.1.1.1.2.2">𝐡</ci><list id="S3.E6.m1.5.5.2.3.cmml" xref="S3.E6.m1.5.5.2.4"><ci id="S3.E6.m1.4.4.1.1.cmml" xref="S3.E6.m1.4.4.1.1">𝑤</ci><ci id="S3.E6.m1.5.5.2.2.cmml" xref="S3.E6.m1.5.5.2.2">𝑖</ci></list></apply><ci id="S3.E6.m1.6.6.1.1.cmml" xref="S3.E6.m1.6.6.1.1">𝑟</ci></apply><apply id="S3.E6.m1.11.11.2.2.2.2.2.2.2.2.cmml" xref="S3.E6.m1.11.11.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.11.11.2.2.2.2.2.2.2.2.1.cmml" xref="S3.E6.m1.11.11.2.2.2.2.2.2.2.2">superscript</csymbol><apply id="S3.E6.m1.11.11.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E6.m1.11.11.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.11.11.2.2.2.2.2.2.2.2.2.1.cmml" xref="S3.E6.m1.11.11.2.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E6.m1.11.11.2.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E6.m1.11.11.2.2.2.2.2.2.2.2.2.2">𝐡</ci><list id="S3.E6.m1.8.8.2.3.cmml" xref="S3.E6.m1.8.8.2.4"><ci id="S3.E6.m1.7.7.1.1.cmml" xref="S3.E6.m1.7.7.1.1">𝑤</ci><ci id="S3.E6.m1.8.8.2.2.cmml" xref="S3.E6.m1.8.8.2.2">𝑗</ci></list></apply><ci id="S3.E6.m1.9.9.1.1.cmml" xref="S3.E6.m1.9.9.1.1">𝑟</ci></apply></interval></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.11c">\mathcal{L}_{\text{diverse}}=\sum_{r=2}^{R}\sum_{w=1}^{W(r)}\sum_{i=1}^{N}\sum_{j=1{,j\neq i}}^{N}\text{sim}(\mathbf{h}_{w,i}^{(r)},\mathbf{h}_{w,j}^{(r)})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p1.4" class="ltx_p">where <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="W(r)" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.2" xref="S3.SS3.p1.1.m1.1.2.cmml"><mi id="S3.SS3.p1.1.m1.1.2.2" xref="S3.SS3.p1.1.m1.1.2.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.2.1" xref="S3.SS3.p1.1.m1.1.2.1.cmml">​</mo><mrow id="S3.SS3.p1.1.m1.1.2.3.2" xref="S3.SS3.p1.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p1.1.m1.1.2.3.2.1" xref="S3.SS3.p1.1.m1.1.2.cmml">(</mo><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">r</mi><mo stretchy="false" id="S3.SS3.p1.1.m1.1.2.3.2.2" xref="S3.SS3.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.2"><times id="S3.SS3.p1.1.m1.1.2.1.cmml" xref="S3.SS3.p1.1.m1.1.2.1"></times><ci id="S3.SS3.p1.1.m1.1.2.2.cmml" xref="S3.SS3.p1.1.m1.1.2.2">𝑊</ci><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">W(r)</annotation></semantics></math> and <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="N(r)" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.2" xref="S3.SS3.p1.2.m2.1.2.cmml"><mi id="S3.SS3.p1.2.m2.1.2.2" xref="S3.SS3.p1.2.m2.1.2.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.2.m2.1.2.1" xref="S3.SS3.p1.2.m2.1.2.1.cmml">​</mo><mrow id="S3.SS3.p1.2.m2.1.2.3.2" xref="S3.SS3.p1.2.m2.1.2.cmml"><mo stretchy="false" id="S3.SS3.p1.2.m2.1.2.3.2.1" xref="S3.SS3.p1.2.m2.1.2.cmml">(</mo><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">r</mi><mo stretchy="false" id="S3.SS3.p1.2.m2.1.2.3.2.2" xref="S3.SS3.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.2.cmml" xref="S3.SS3.p1.2.m2.1.2"><times id="S3.SS3.p1.2.m2.1.2.1.cmml" xref="S3.SS3.p1.2.m2.1.2.1"></times><ci id="S3.SS3.p1.2.m2.1.2.2.cmml" xref="S3.SS3.p1.2.m2.1.2.2">𝑁</ci><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">N(r)</annotation></semantics></math> are the total number of windows and the number of output queries of each window at resolution level <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">r</annotation></semantics></math> respectively, and sim<math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="(\cdot)" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mrow id="S3.SS3.p1.4.m4.1.2.2"><mo stretchy="false" id="S3.SS3.p1.4.m4.1.2.2.1">(</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">⋅</mo><mo stretchy="false" id="S3.SS3.p1.4.m4.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><ci id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">(\cdot)</annotation></semantics></math> is the cosine similarity between two vectors.
Cosine similarity is adopted since it is widely used for semantic similarity measurements, and in video-SALMONN, the output queries are aligned with a semantic space of the LLM input token representations.
This choice is also supported by the fact that the modulus of the output query tokens is very similar due to the layer normalisation operation of the MRC Q-Former. Note that the diversity loss is only needed at the low-resolution levels where there are enough frames in a window to extract diverse information.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Overall, video-SALMONN is trained end-to-end using the cross-entropy (CE) loss and the diversity loss as shown below, where <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\lambda</annotation></semantics></math> controls the importance of the diversity loss.</p>
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1.1" class="ltx_Math" alttext="\mathcal{L}=\mathcal{L}_{\text{CE}}+\lambda\mathcal{L}_{\text{diverse}}," display="block"><semantics id="S3.E7.m1.1a"><mrow id="S3.E7.m1.1.1.1" xref="S3.E7.m1.1.1.1.1.cmml"><mrow id="S3.E7.m1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E7.m1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.2.cmml">ℒ</mi><mo id="S3.E7.m1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E7.m1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.3.cmml"><msub id="S3.E7.m1.1.1.1.1.3.2" xref="S3.E7.m1.1.1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E7.m1.1.1.1.1.3.2.2" xref="S3.E7.m1.1.1.1.1.3.2.2.cmml">ℒ</mi><mtext id="S3.E7.m1.1.1.1.1.3.2.3" xref="S3.E7.m1.1.1.1.1.3.2.3a.cmml">CE</mtext></msub><mo id="S3.E7.m1.1.1.1.1.3.1" xref="S3.E7.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S3.E7.m1.1.1.1.1.3.3" xref="S3.E7.m1.1.1.1.1.3.3.cmml"><mi id="S3.E7.m1.1.1.1.1.3.3.2" xref="S3.E7.m1.1.1.1.1.3.3.2.cmml">λ</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.1.1.3.3.1" xref="S3.E7.m1.1.1.1.1.3.3.1.cmml">​</mo><msub id="S3.E7.m1.1.1.1.1.3.3.3" xref="S3.E7.m1.1.1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E7.m1.1.1.1.1.3.3.3.2" xref="S3.E7.m1.1.1.1.1.3.3.3.2.cmml">ℒ</mi><mtext id="S3.E7.m1.1.1.1.1.3.3.3.3" xref="S3.E7.m1.1.1.1.1.3.3.3.3a.cmml">diverse</mtext></msub></mrow></mrow></mrow><mo id="S3.E7.m1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><apply id="S3.E7.m1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1"><eq id="S3.E7.m1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1"></eq><ci id="S3.E7.m1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.2">ℒ</ci><apply id="S3.E7.m1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.3"><plus id="S3.E7.m1.1.1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.1.1.3.1"></plus><apply id="S3.E7.m1.1.1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.3.2.1.cmml" xref="S3.E7.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E7.m1.1.1.1.1.3.2.2.cmml" xref="S3.E7.m1.1.1.1.1.3.2.2">ℒ</ci><ci id="S3.E7.m1.1.1.1.1.3.2.3a.cmml" xref="S3.E7.m1.1.1.1.1.3.2.3"><mtext mathsize="70%" id="S3.E7.m1.1.1.1.1.3.2.3.cmml" xref="S3.E7.m1.1.1.1.1.3.2.3">CE</mtext></ci></apply><apply id="S3.E7.m1.1.1.1.1.3.3.cmml" xref="S3.E7.m1.1.1.1.1.3.3"><times id="S3.E7.m1.1.1.1.1.3.3.1.cmml" xref="S3.E7.m1.1.1.1.1.3.3.1"></times><ci id="S3.E7.m1.1.1.1.1.3.3.2.cmml" xref="S3.E7.m1.1.1.1.1.3.3.2">𝜆</ci><apply id="S3.E7.m1.1.1.1.1.3.3.3.cmml" xref="S3.E7.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E7.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E7.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E7.m1.1.1.1.1.3.3.3.2">ℒ</ci><ci id="S3.E7.m1.1.1.1.1.3.3.3.3a.cmml" xref="S3.E7.m1.1.1.1.1.3.3.3.3"><mtext mathsize="70%" id="S3.E7.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E7.m1.1.1.1.1.3.3.3.3">diverse</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">\mathcal{L}=\mathcal{L}_{\text{CE}}+\lambda\mathcal{L}_{\text{diverse}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.2" class="ltx_p">Furthermore, to avoid modality dominance in the video, in addition to the small amount of paired audio-visual data, we propose the mixed training scheme where a portion of the training set is augmented with unpaired audio-visual data and the prompt combines the original tasks for audio and video. This way, the model is enforced to extract information from both audio and video inputs without relying on a dominant modality. This strategy improved the balance between different modalities and is a crucial factor leading to audio-visual understanding and co-reasoning abilities.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Speech-Audio-Visual Evaluation Benchmark</h3>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>SAVE benchmark details, including the number of samples used for evaluation and metrics reported. Since TextVQA, GQA, NExT-QA and VGGSS test sets are large, randomly sampled subsets with enough samples for statistical significance were used for efficient evaluation. Zero-shot refers to both instruction and audio-visual inputs that are unseen in the training set. Note that Presentation-QA
is newly proposed AVQA test sets focusing on speech-audio-visual joint information.</figcaption>
<table id="S4.T1.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.3.1.1" class="ltx_tr">
<td id="S4.T1.3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Task</span></td>
<td id="S4.T1.3.1.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Test set</span></td>
<td id="S4.T1.3.1.1.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">#samples</span></td>
<td id="S4.T1.3.1.1.4" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.3.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Metrics</span></td>
<td id="S4.T1.3.1.1.5" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.3.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Zero-shot</span></td>
</tr>
<tr id="S4.T1.3.2.2" class="ltx_tr">
<td id="S4.T1.3.2.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.3.2.2.1.1" class="ltx_text" style="font-size:80%;">ASR</span></td>
<td id="S4.T1.3.2.2.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S4.T1.3.2.2.2.1" class="ltx_text" style="font-size:80%;">LibriSpeech test-clean </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.3.2.2.2.2.1" class="ltx_text" style="font-size:80%;">(</span>Panayotov et al.<span id="S4.T1.3.2.2.2.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib35" title="" class="ltx_ref">2015</a><span id="S4.T1.3.2.2.2.4.3" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S4.T1.3.2.2.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.3.2.2.3.1" class="ltx_text" style="font-size:80%;">2620</span></td>
<td id="S4.T1.3.2.2.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.3.2.2.4.1" class="ltx_text" style="font-size:80%;">WER</span></td>
<td id="S4.T1.3.2.2.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.3.2.2.5.1" class="ltx_text" style="font-size:80%;">No</span></td>
</tr>
<tr id="S4.T1.3.3.3" class="ltx_tr">
<td id="S4.T1.3.3.3.1" class="ltx_td ltx_align_left"><span id="S4.T1.3.3.3.1.1" class="ltx_text" style="font-size:80%;">AAC</span></td>
<td id="S4.T1.3.3.3.2" class="ltx_td ltx_align_left">
<span id="S4.T1.3.3.3.2.1" class="ltx_text" style="font-size:80%;">AudioCaps test </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.3.3.3.2.2.1" class="ltx_text" style="font-size:80%;">(</span>Kim et al.<span id="S4.T1.3.3.3.2.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib23" title="" class="ltx_ref">2019</a><span id="S4.T1.3.3.3.2.4.3" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S4.T1.3.3.3.3" class="ltx_td ltx_align_left"><span id="S4.T1.3.3.3.3.1" class="ltx_text" style="font-size:80%;">938</span></td>
<td id="S4.T1.3.3.3.4" class="ltx_td ltx_align_left"><span id="S4.T1.3.3.3.4.1" class="ltx_text" style="font-size:80%;">SPIDEr</span></td>
<td id="S4.T1.3.3.3.5" class="ltx_td ltx_align_left"><span id="S4.T1.3.3.3.5.1" class="ltx_text" style="font-size:80%;">No</span></td>
</tr>
<tr id="S4.T1.3.4.4" class="ltx_tr">
<td id="S4.T1.3.4.4.1" class="ltx_td ltx_align_left"><span id="S4.T1.3.4.4.1.1" class="ltx_text" style="font-size:80%;">IC</span></td>
<td id="S4.T1.3.4.4.2" class="ltx_td ltx_align_left">
<span id="S4.T1.3.4.4.2.1" class="ltx_text" style="font-size:80%;">Flickr30k test </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.3.4.4.2.2.1" class="ltx_text" style="font-size:80%;">(</span>Young et al.<span id="S4.T1.3.4.4.2.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib51" title="" class="ltx_ref">2014</a><span id="S4.T1.3.4.4.2.4.3" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S4.T1.3.4.4.3" class="ltx_td ltx_align_left"><span id="S4.T1.3.4.4.3.1" class="ltx_text" style="font-size:80%;">1000</span></td>
<td id="S4.T1.3.4.4.4" class="ltx_td ltx_align_left"><span id="S4.T1.3.4.4.4.1" class="ltx_text" style="font-size:80%;">CIDEr</span></td>
<td id="S4.T1.3.4.4.5" class="ltx_td ltx_align_left"><span id="S4.T1.3.4.4.5.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
</tr>
<tr id="S4.T1.3.5.5" class="ltx_tr">
<td id="S4.T1.3.5.5.1" class="ltx_td ltx_align_left"><span id="S4.T1.3.5.5.1.1" class="ltx_text" style="font-size:80%;">OCR</span></td>
<td id="S4.T1.3.5.5.2" class="ltx_td ltx_align_left">
<span id="S4.T1.3.5.5.2.1" class="ltx_text" style="font-size:80%;">TextVQA test </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.3.5.5.2.2.1" class="ltx_text" style="font-size:80%;">(</span>Singh et al.<span id="S4.T1.3.5.5.2.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib44" title="" class="ltx_ref">2019</a><span id="S4.T1.3.5.5.2.4.3" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S4.T1.3.5.5.3" class="ltx_td ltx_align_left"><span id="S4.T1.3.5.5.3.1" class="ltx_text" style="font-size:80%;">1000</span></td>
<td id="S4.T1.3.5.5.4" class="ltx_td ltx_align_left"><span id="S4.T1.3.5.5.4.1" class="ltx_text" style="font-size:80%;">Accuracy</span></td>
<td id="S4.T1.3.5.5.5" class="ltx_td ltx_align_left"><span id="S4.T1.3.5.5.5.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
</tr>
<tr id="S4.T1.3.6.6" class="ltx_tr">
<td id="S4.T1.3.6.6.1" class="ltx_td ltx_align_left"><span id="S4.T1.3.6.6.1.1" class="ltx_text" style="font-size:80%;">VQA</span></td>
<td id="S4.T1.3.6.6.2" class="ltx_td ltx_align_left">
<span id="S4.T1.3.6.6.2.1" class="ltx_text" style="font-size:80%;">GQA test dev balanced </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.3.6.6.2.2.1" class="ltx_text" style="font-size:80%;">(</span>Hudson &amp; Manning<span id="S4.T1.3.6.6.2.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib22" title="" class="ltx_ref">2019</a><span id="S4.T1.3.6.6.2.4.3" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S4.T1.3.6.6.3" class="ltx_td ltx_align_left"><span id="S4.T1.3.6.6.3.1" class="ltx_text" style="font-size:80%;">1000</span></td>
<td id="S4.T1.3.6.6.4" class="ltx_td ltx_align_left"><span id="S4.T1.3.6.6.4.1" class="ltx_text" style="font-size:80%;">Accuracy</span></td>
<td id="S4.T1.3.6.6.5" class="ltx_td ltx_align_left"><span id="S4.T1.3.6.6.5.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
</tr>
<tr id="S4.T1.3.7.7" class="ltx_tr">
<td id="S4.T1.3.7.7.1" class="ltx_td ltx_align_left"><span id="S4.T1.3.7.7.1.1" class="ltx_text" style="font-size:80%;">Video QA</span></td>
<td id="S4.T1.3.7.7.2" class="ltx_td ltx_align_left">
<span id="S4.T1.3.7.7.2.1" class="ltx_text" style="font-size:80%;">NExT-QA test </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.3.7.7.2.2.1" class="ltx_text" style="font-size:80%;">(</span>Xiao et al.<span id="S4.T1.3.7.7.2.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib50" title="" class="ltx_ref">2021</a><span id="S4.T1.3.7.7.2.4.3" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S4.T1.3.7.7.3" class="ltx_td ltx_align_left"><span id="S4.T1.3.7.7.3.1" class="ltx_text" style="font-size:80%;">1000</span></td>
<td id="S4.T1.3.7.7.4" class="ltx_td ltx_align_left"><span id="S4.T1.3.7.7.4.1" class="ltx_text" style="font-size:80%;">Accuracy</span></td>
<td id="S4.T1.3.7.7.5" class="ltx_td ltx_align_left"><span id="S4.T1.3.7.7.5.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
</tr>
<tr id="S4.T1.3.8.8" class="ltx_tr">
<td id="S4.T1.3.8.8.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.3.8.8.1.1" class="ltx_text" style="font-size:80%;">AVSR</span></td>
<td id="S4.T1.3.8.8.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S4.T1.3.8.8.2.1" class="ltx_text" style="font-size:80%;">How2 dev5 </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.3.8.8.2.2.1" class="ltx_text" style="font-size:80%;">(</span>Sanabria et al.<span id="S4.T1.3.8.8.2.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib40" title="" class="ltx_ref">2018</a><span id="S4.T1.3.8.8.2.4.3" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S4.T1.3.8.8.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.3.8.8.3.1" class="ltx_text" style="font-size:80%;">500</span></td>
<td id="S4.T1.3.8.8.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.3.8.8.4.1" class="ltx_text" style="font-size:80%;">WER</span></td>
<td id="S4.T1.3.8.8.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.3.8.8.5.1" class="ltx_text" style="font-size:80%;">No</span></td>
</tr>
<tr id="S4.T1.3.9.9" class="ltx_tr">
<td id="S4.T1.3.9.9.1" class="ltx_td ltx_align_left"><span id="S4.T1.3.9.9.1.1" class="ltx_text" style="font-size:80%;">AVQA</span></td>
<td id="S4.T1.3.9.9.2" class="ltx_td ltx_align_left">
<span id="S4.T1.3.9.9.2.1" class="ltx_text" style="font-size:80%;">Ego4D </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.3.9.9.2.2.1" class="ltx_text" style="font-size:80%;">(</span>Grauman et al.<span id="S4.T1.3.9.9.2.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib19" title="" class="ltx_ref">2022</a><span id="S4.T1.3.9.9.2.4.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S4.T1.3.9.9.2.5" class="ltx_text" style="font-size:80%;"> + Presentation-QA</span>
</td>
<td id="S4.T1.3.9.9.3" class="ltx_td ltx_align_left"><span id="S4.T1.3.9.9.3.1" class="ltx_text" style="font-size:80%;">2000</span></td>
<td id="S4.T1.3.9.9.4" class="ltx_td ltx_align_left"><span id="S4.T1.3.9.9.4.1" class="ltx_text" style="font-size:80%;">Accuracy</span></td>
<td id="S4.T1.3.9.9.5" class="ltx_td ltx_align_left"><span id="S4.T1.3.9.9.5.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
</tr>
<tr id="S4.T1.3.10.10" class="ltx_tr">
<td id="S4.T1.3.10.10.1" class="ltx_td ltx_align_left"><span id="S4.T1.3.10.10.1.1" class="ltx_text" style="font-size:80%;">AVSSD</span></td>
<td id="S4.T1.3.10.10.2" class="ltx_td ltx_align_left">
<span id="S4.T1.3.10.10.2.1" class="ltx_text" style="font-size:80%;">VGGSS </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.3.10.10.2.2.1" class="ltx_text" style="font-size:80%;">(</span>Chen et al.<span id="S4.T1.3.10.10.2.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib6" title="" class="ltx_ref">2020</a>; Zhao et al.<span id="S4.T1.3.10.10.2.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib57" title="" class="ltx_ref">2023</a><span id="S4.T1.3.10.10.2.4.3" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S4.T1.3.10.10.3" class="ltx_td ltx_align_left"><span id="S4.T1.3.10.10.3.1" class="ltx_text" style="font-size:80%;">850</span></td>
<td id="S4.T1.3.10.10.4" class="ltx_td ltx_align_left"><span id="S4.T1.3.10.10.4.1" class="ltx_text" style="font-size:80%;">Accuracy</span></td>
<td id="S4.T1.3.10.10.5" class="ltx_td ltx_align_left"><span id="S4.T1.3.10.10.5.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
</tr>
<tr id="S4.T1.3.11.11" class="ltx_tr">
<td id="S4.T1.3.11.11.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T1.3.11.11.1.1" class="ltx_text" style="font-size:80%;">AVM</span></td>
<td id="S4.T1.3.11.11.2" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S4.T1.3.11.11.2.1" class="ltx_text" style="font-size:80%;">SpokenCOCO </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T1.3.11.11.2.2.1" class="ltx_text" style="font-size:80%;">(</span>Hsu et al.<span id="S4.T1.3.11.11.2.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib20" title="" class="ltx_ref">2020</a><span id="S4.T1.3.11.11.2.4.3" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S4.T1.3.11.11.2.5" class="ltx_text" style="font-size:80%;"> + VGGSS</span>
</td>
<td id="S4.T1.3.11.11.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T1.3.11.11.3.1" class="ltx_text" style="font-size:80%;">1000</span></td>
<td id="S4.T1.3.11.11.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T1.3.11.11.4.1" class="ltx_text" style="font-size:80%;">Accuracy</span></td>
<td id="S4.T1.3.11.11.5" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T1.3.11.11.5.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We introduce the SAVE benchmark to evaluate the performance of video-SALMONN. SAVE benchmark contains selected representative tasks for both single and multi-modal tasks. The six single-modal tasks included are ASR, automatic audio captioning (AAC), image captioning (IC), optical character recognition (OCR), visual question answer (VQA), and video question answer (Video QA), and the four audio-visual tasks spanning 6 datasets are audio-visual speech recognition (AVSR), audio-visual QA (AVQA), audio-visual matching (AVM) and audio-visual sound source detection (AVSSD).</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">In particular, we curate Ego4D-QA and Presentation-QA test sets to evaluate accuracy in audio-visual understanding with speech. The questions for the two sets are generated by prompting GPT-4 with video descriptions and ASR transcriptions for each video clip. Detailed examples for AVQA datasets are in Appendix <a href="#A2" title="Appendix B Examples of the AVQA Dataset ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>. The SAVE benchmark is summarised in Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Speech-Audio-Visual Evaluation Benchmark ‣ 4 Experimental Setup ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, and details about evaluation metrics can be found in Appendix <a href="#A3" title="Appendix C Evaluation Details ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">This paper further proposes the AVM task where audio-visual interaction is necessary. AVM is the task of determining whether the given spoken description in the SpokenCOCO dataset <cite class="ltx_cite ltx_citemacro_citep">(Hsu et al., <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite> matches the image, or whether the given audio clip is compatible with the given video chosen from the VGGSS dataset <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>. AVSSD is another task that requires a strong binding of audio and visual modalities, as a single modality usually only provides partial information about the sound.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Model Configurations</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To validate video-SALMONN on the SAVE benchmark, the Vicuna-v1.5 <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al., <a href="#bib.bib9" title="" class="ltx_ref">2023</a>)</cite> models (including 7B and 13B models, and 13B is the default option if not specified) is used as the LLM, Whisper <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite> large-v2 encoder as the speech encoder, BEATs <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib8" title="" class="ltx_ref">2023d</a>)</cite> encoder as the audio encoder and InstructBLIP <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite> vision Transformer (ViT) plus Q-Former as the visual encoder. The visual encoder outputs 32 feature vectors for each video frame (every 0.5 seconds), and the audio encoder outputs 50 feature vectors per second.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.2" class="ltx_p">The MRC Q-Former has two Transformer blocks with <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">D</annotation></semantics></math>=768-dim hidden states. By default, we adopt two different levels of resolution at 0.5-second and 5-second respectively, with the number of output query vectors being 3 and 30 for each window. The output query vectors of the MRC Q-Former are projected to <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">E</annotation></semantics></math>=5120-dim before being sent to the LLM.
The LLM is adapted using the low-rank adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite> method with rank 32. LoRA parameters of the attention query, key and value projections and feed-forward network weights are updated, which comprises 0.4% of the total number of LLM parameters.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Whisper and InstructBLIP are used as the single-modality baseline systems for comparison. As video-SALMONN uses video data with different styles and focuses, to eliminate the discrepancy in training data and achieve fair comparisons, InstructBLIP is further fine-tuned on the same image and video training data as video-SALMONN. For each video clip, five equally-spaced frames were used resulting in 160 output queries. This is the same as the number of output queries used for 25-second videos in video-SALMONN. Video-LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib55" title="" class="ltx_ref">2023b</a>)</cite> was used as the multimodal baseline where only the Vicuna-7B checkpoint was released for audio-visual input.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>The SAVE benchmark single-modal task results.
If specified, InstructBLIP is fine-tuned on the training data of video-SALMONN (“InstructBLIP fine-tuned”). Evaluation metrics can be found in Appendix <a href="#A3" title="Appendix C Evaluation Details ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>. When using visual-only inputs, the other modality is masked during training and inference. Tasks unable to be performed are marked with “-”.</figcaption>
<table id="S4.T2.6" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.6.6" class="ltx_tr">
<td id="S4.T2.6.6.7" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T2.6.6.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Systems</span></td>
<td id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">ASR</span><span id="S4.T2.1.1.1.2" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T2.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AC</span><span id="S4.T2.2.2.2.2" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.2.2.2.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T2.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T2.3.3.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Video QA</span><span id="S4.T2.3.3.3.2" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T2.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.3.3.3.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T2.3.3.3.m1.1.1" xref="S4.T2.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T2.4.4.4" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T2.4.4.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">IC</span><span id="S4.T2.4.4.4.2" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T2.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.4.4.4.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T2.4.4.4.m1.1.1" xref="S4.T2.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.m1.1b"><ci id="S4.T2.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T2.5.5.5" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T2.5.5.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">OCR</span><span id="S4.T2.5.5.5.2" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T2.5.5.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.5.5.5.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T2.5.5.5.m1.1.1" xref="S4.T2.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.m1.1b"><ci id="S4.T2.5.5.5.m1.1.1.cmml" xref="S4.T2.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T2.6.6.6" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T2.6.6.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">VQA</span><span id="S4.T2.6.6.6.2" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T2.6.6.6.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.6.6.6.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T2.6.6.6.m1.1.1" xref="S4.T2.6.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.m1.1b"><ci id="S4.T2.6.6.6.m1.1.1.cmml" xref="S4.T2.6.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T2.6.7.1" class="ltx_tr">
<td id="S4.T2.6.7.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.6.7.1.1.1" class="ltx_text" style="font-size:80%;">Whisper large-v2</span></td>
<td id="S4.T2.6.7.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.7.1.2.1" class="ltx_text" style="font-size:80%;">2.9%</span></td>
<td id="S4.T2.6.7.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.7.1.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.6.7.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.7.1.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.6.7.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.7.1.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.6.7.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.7.1.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.6.7.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.7.1.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T2.6.8.2" class="ltx_tr">
<td id="S4.T2.6.8.2.1" class="ltx_td ltx_align_left">
<span id="S4.T2.6.8.2.1.1" class="ltx_text" style="font-size:80%;">InstructBLIP 13B </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T2.6.8.2.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Dai et al.<span id="S4.T2.6.8.2.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib11" title="" class="ltx_ref">2023</a><span id="S4.T2.6.8.2.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S4.T2.6.8.2.2" class="ltx_td ltx_align_center"><span id="S4.T2.6.8.2.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.6.8.2.3" class="ltx_td ltx_align_center"><span id="S4.T2.6.8.2.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.6.8.2.4" class="ltx_td ltx_align_center"><span id="S4.T2.6.8.2.4.1" class="ltx_text" style="font-size:80%;">21.0%</span></td>
<td id="S4.T2.6.8.2.5" class="ltx_td ltx_align_center"><span id="S4.T2.6.8.2.5.1" class="ltx_text" style="font-size:80%;">84.5</span></td>
<td id="S4.T2.6.8.2.6" class="ltx_td ltx_align_center"><span id="S4.T2.6.8.2.6.1" class="ltx_text" style="font-size:80%;">36.5%</span></td>
<td id="S4.T2.6.8.2.7" class="ltx_td ltx_align_center">
<span id="S4.T2.6.8.2.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">48.9</span><span id="S4.T2.6.8.2.7.2" class="ltx_text" style="font-size:80%;">%</span>
</td>
</tr>
<tr id="S4.T2.6.9.3" class="ltx_tr">
<td id="S4.T2.6.9.3.1" class="ltx_td ltx_align_left"><span id="S4.T2.6.9.3.1.1" class="ltx_text" style="font-size:80%;">InstructBLIP 13B fine-tuned</span></td>
<td id="S4.T2.6.9.3.2" class="ltx_td ltx_align_center"><span id="S4.T2.6.9.3.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.6.9.3.3" class="ltx_td ltx_align_center"><span id="S4.T2.6.9.3.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.6.9.3.4" class="ltx_td ltx_align_center"><span id="S4.T2.6.9.3.4.1" class="ltx_text" style="font-size:80%;">24.7%</span></td>
<td id="S4.T2.6.9.3.5" class="ltx_td ltx_align_center"><span id="S4.T2.6.9.3.5.1" class="ltx_text" style="font-size:80%;">78.9</span></td>
<td id="S4.T2.6.9.3.6" class="ltx_td ltx_align_center"><span id="S4.T2.6.9.3.6.1" class="ltx_text" style="font-size:80%;">36.7%</span></td>
<td id="S4.T2.6.9.3.7" class="ltx_td ltx_align_center"><span id="S4.T2.6.9.3.7.1" class="ltx_text" style="font-size:80%;">45.6%</span></td>
</tr>
<tr id="S4.T2.6.10.4" class="ltx_tr">
<td id="S4.T2.6.10.4.1" class="ltx_td ltx_align_left">
<span id="S4.T2.6.10.4.1.1" class="ltx_text" style="font-size:80%;">Video-LLaMA 7B </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T2.6.10.4.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Zhang et al.<span id="S4.T2.6.10.4.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib55" title="" class="ltx_ref">2023b</a><span id="S4.T2.6.10.4.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S4.T2.6.10.4.2" class="ltx_td ltx_align_center"><span id="S4.T2.6.10.4.2.1" class="ltx_text" style="font-size:80%;">100%+</span></td>
<td id="S4.T2.6.10.4.3" class="ltx_td ltx_align_center"><span id="S4.T2.6.10.4.3.1" class="ltx_text" style="font-size:80%;">3.5</span></td>
<td id="S4.T2.6.10.4.4" class="ltx_td ltx_align_center"><span id="S4.T2.6.10.4.4.1" class="ltx_text" style="font-size:80%;">22.5%</span></td>
<td id="S4.T2.6.10.4.5" class="ltx_td ltx_align_center"><span id="S4.T2.6.10.4.5.1" class="ltx_text" style="font-size:80%;">22.0</span></td>
<td id="S4.T2.6.10.4.6" class="ltx_td ltx_align_center"><span id="S4.T2.6.10.4.6.1" class="ltx_text" style="font-size:80%;">16.4%</span></td>
<td id="S4.T2.6.10.4.7" class="ltx_td ltx_align_center"><span id="S4.T2.6.10.4.7.1" class="ltx_text" style="font-size:80%;">15.1%</span></td>
</tr>
<tr id="S4.T2.6.11.5" class="ltx_tr">
<td id="S4.T2.6.11.5.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.6.11.5.1.1" class="ltx_text" style="font-size:80%;">video-SALMONN 13B (ours, visual-only)</span></td>
<td id="S4.T2.6.11.5.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.11.5.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.6.11.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.11.5.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.6.11.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.11.5.4.1" class="ltx_text" style="font-size:80%;">44.8%</span></td>
<td id="S4.T2.6.11.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.11.5.5.1" class="ltx_text" style="font-size:80%;">74.0</span></td>
<td id="S4.T2.6.11.5.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.11.5.6.1" class="ltx_text" style="font-size:80%;">34.2%</span></td>
<td id="S4.T2.6.11.5.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.6.11.5.7.1" class="ltx_text" style="font-size:80%;">45.6%</span></td>
</tr>
<tr id="S4.T2.6.12.6" class="ltx_tr">
<td id="S4.T2.6.12.6.1" class="ltx_td ltx_align_left"><span id="S4.T2.6.12.6.1.1" class="ltx_text" style="font-size:80%;">video-SALMONN 7B (ours)</span></td>
<td id="S4.T2.6.12.6.2" class="ltx_td ltx_align_center"><span id="S4.T2.6.12.6.2.1" class="ltx_text" style="font-size:80%;">4.1%</span></td>
<td id="S4.T2.6.12.6.3" class="ltx_td ltx_align_center"><span id="S4.T2.6.12.6.3.1" class="ltx_text" style="font-size:80%;">39.1</span></td>
<td id="S4.T2.6.12.6.4" class="ltx_td ltx_align_center"><span id="S4.T2.6.12.6.4.1" class="ltx_text" style="font-size:80%;">42.5%</span></td>
<td id="S4.T2.6.12.6.5" class="ltx_td ltx_align_center"><span id="S4.T2.6.12.6.5.1" class="ltx_text" style="font-size:80%;">78.1</span></td>
<td id="S4.T2.6.12.6.6" class="ltx_td ltx_align_center"><span id="S4.T2.6.12.6.6.1" class="ltx_text" style="font-size:80%;">34.6%</span></td>
<td id="S4.T2.6.12.6.7" class="ltx_td ltx_align_center"><span id="S4.T2.6.12.6.7.1" class="ltx_text" style="font-size:80%;">45.3%</span></td>
</tr>
<tr id="S4.T2.6.13.7" class="ltx_tr">
<td id="S4.T2.6.13.7.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T2.6.13.7.1.1" class="ltx_text" style="font-size:80%;">video-SALMONN 13B (ours)</span></td>
<td id="S4.T2.6.13.7.2" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T2.6.13.7.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">2.6</span><span id="S4.T2.6.13.7.2.2" class="ltx_text" style="font-size:80%;">%</span>
</td>
<td id="S4.T2.6.13.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.6.13.7.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">49.7</span></td>
<td id="S4.T2.6.13.7.4" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T2.6.13.7.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">49.6</span><span id="S4.T2.6.13.7.4.2" class="ltx_text" style="font-size:80%;">%</span>
</td>
<td id="S4.T2.6.13.7.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.6.13.7.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">89.6</span></td>
<td id="S4.T2.6.13.7.6" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T2.6.13.7.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">37.8</span><span id="S4.T2.6.13.7.6.2" class="ltx_text" style="font-size:80%;">%</span>
</td>
<td id="S4.T2.6.13.7.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.6.13.7.7.1" class="ltx_text" style="font-size:80%;">44.8%</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>The SAVE benchmark audio-visual task results. If specified, InstructBLIP is fine-tuned on the training data of video-SALMONN (“InstructBLIP<math id="S4.T3.2.m1.1" class="ltx_Math" alttext="{\dagger}" display="inline"><semantics id="S4.T3.2.m1.1b"><mo id="S4.T3.2.m1.1.1" xref="S4.T3.2.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.m1.1c"><ci id="S4.T3.2.m1.1.1.cmml" xref="S4.T3.2.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.m1.1d">{\dagger}</annotation></semantics></math>”). The other modality is masked in both training and testing when using visual-only inputs. Tasks unable to be performed are marked with “-”. We split AVQA into Ego4D-QA (E) and Presentation-QA (P).</figcaption>
<table id="S4.T3.9" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.8.6" class="ltx_tr">
<td id="S4.T3.8.6.7" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T3.8.6.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Systems</span></td>
<td id="S4.T3.4.2.2" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T3.4.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AVSR</span><span id="S4.T3.4.2.2.2" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T3.3.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.3.1.1.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T3.3.1.1.m1.1.1" xref="S4.T3.3.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.1.1.m1.1b"><ci id="S4.T3.3.1.1.m1.1.1.cmml" xref="S4.T3.3.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.1.1.m1.1c">\downarrow</annotation></semantics></math><span id="S4.T3.4.2.2.3" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T3.4.2.2.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.4.2.2.m2.1a"><mo mathsize="80%" stretchy="false" id="S4.T3.4.2.2.m2.1.1" xref="S4.T3.4.2.2.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.2.2.m2.1b"><ci id="S4.T3.4.2.2.m2.1.1.cmml" xref="S4.T3.4.2.2.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.2.2.m2.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T3.5.3.3" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T3.5.3.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AVQA (E)</span><span id="S4.T3.5.3.3.2" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T3.5.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.5.3.3.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T3.5.3.3.m1.1.1" xref="S4.T3.5.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.3.3.m1.1b"><ci id="S4.T3.5.3.3.m1.1.1.cmml" xref="S4.T3.5.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.3.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T3.6.4.4" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T3.6.4.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AVQA (P)</span><span id="S4.T3.6.4.4.2" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T3.6.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.6.4.4.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T3.6.4.4.m1.1.1" xref="S4.T3.6.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.4.4.m1.1b"><ci id="S4.T3.6.4.4.m1.1.1.cmml" xref="S4.T3.6.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.4.4.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T3.7.5.5" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T3.7.5.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AVSSD</span><span id="S4.T3.7.5.5.2" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T3.7.5.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.7.5.5.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T3.7.5.5.m1.1.1" xref="S4.T3.7.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.7.5.5.m1.1b"><ci id="S4.T3.7.5.5.m1.1.1.cmml" xref="S4.T3.7.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.5.5.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T3.8.6.6" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T3.8.6.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AVM</span><span id="S4.T3.8.6.6.2" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T3.8.6.6.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.8.6.6.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T3.8.6.6.m1.1.1" xref="S4.T3.8.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.8.6.6.m1.1b"><ci id="S4.T3.8.6.6.m1.1.1.cmml" xref="S4.T3.8.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.6.6.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T3.9.8.1" class="ltx_tr">
<td id="S4.T3.9.8.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T3.9.8.1.1.1" class="ltx_text" style="font-size:80%;">Whisper large-v2</span></td>
<td id="S4.T3.9.8.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.9.8.1.2.1" class="ltx_text" style="font-size:80%;">8.3%</span></td>
<td id="S4.T3.9.8.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.9.8.1.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.9.8.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.9.8.1.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.9.8.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.9.8.1.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.9.8.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.9.8.1.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T3.9.9.2" class="ltx_tr">
<td id="S4.T3.9.9.2.1" class="ltx_td ltx_align_left">
<span id="S4.T3.9.9.2.1.1" class="ltx_text" style="font-size:80%;">InstructBLIP 13B </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T3.9.9.2.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Dai et al.<span id="S4.T3.9.9.2.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib11" title="" class="ltx_ref">2023</a><span id="S4.T3.9.9.2.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S4.T3.9.9.2.2" class="ltx_td ltx_align_center"><span id="S4.T3.9.9.2.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.9.9.2.3" class="ltx_td ltx_align_center"><span id="S4.T3.9.9.2.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.9.9.2.4" class="ltx_td ltx_align_center"><span id="S4.T3.9.9.2.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.9.9.2.5" class="ltx_td ltx_align_center"><span id="S4.T3.9.9.2.5.1" class="ltx_text" style="font-size:80%;">1.1%</span></td>
<td id="S4.T3.9.9.2.6" class="ltx_td ltx_align_center"><span id="S4.T3.9.9.2.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T3.9.7" class="ltx_tr">
<td id="S4.T3.9.7.1" class="ltx_td ltx_align_left">
<span id="S4.T3.9.7.1.1" class="ltx_text" style="font-size:80%;">InstructBLIP</span><math id="S4.T3.9.7.1.m1.1" class="ltx_Math" alttext="{\dagger}" display="inline"><semantics id="S4.T3.9.7.1.m1.1a"><mo mathsize="80%" id="S4.T3.9.7.1.m1.1.1" xref="S4.T3.9.7.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S4.T3.9.7.1.m1.1b"><ci id="S4.T3.9.7.1.m1.1.1.cmml" xref="S4.T3.9.7.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.7.1.m1.1c">{\dagger}</annotation></semantics></math><span id="S4.T3.9.7.1.2" class="ltx_text" style="font-size:80%;"> 13B</span>
</td>
<td id="S4.T3.9.7.2" class="ltx_td ltx_align_center"><span id="S4.T3.9.7.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.9.7.3" class="ltx_td ltx_align_center"><span id="S4.T3.9.7.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.9.7.4" class="ltx_td ltx_align_center"><span id="S4.T3.9.7.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.9.7.5" class="ltx_td ltx_align_center"><span id="S4.T3.9.7.5.1" class="ltx_text" style="font-size:80%;">20.3%</span></td>
<td id="S4.T3.9.7.6" class="ltx_td ltx_align_center"><span id="S4.T3.9.7.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T3.9.10.3" class="ltx_tr">
<td id="S4.T3.9.10.3.1" class="ltx_td ltx_align_left">
<span id="S4.T3.9.10.3.1.1" class="ltx_text" style="font-size:80%;">Video-LLaMA 7B </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S4.T3.9.10.3.1.2.1" class="ltx_text" style="font-size:80%;">(</span>Zhang et al.<span id="S4.T3.9.10.3.1.3.2.1.1" class="ltx_text" style="font-size:80%;">, </span><a href="#bib.bib55" title="" class="ltx_ref">2023b</a><span id="S4.T3.9.10.3.1.4.3" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S4.T3.9.10.3.2" class="ltx_td ltx_align_center"><span id="S4.T3.9.10.3.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.9.10.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.9.10.3.3.1" class="ltx_text" style="font-size:80%;">18.2%</span></td>
<td id="S4.T3.9.10.3.4" class="ltx_td ltx_align_center"><span id="S4.T3.9.10.3.4.1" class="ltx_text" style="font-size:80%;">21.3%</span></td>
<td id="S4.T3.9.10.3.5" class="ltx_td ltx_align_center"><span id="S4.T3.9.10.3.5.1" class="ltx_text" style="font-size:80%;">41.9%</span></td>
<td id="S4.T3.9.10.3.6" class="ltx_td ltx_align_center"><span id="S4.T3.9.10.3.6.1" class="ltx_text" style="font-size:80%;">52.3%</span></td>
</tr>
<tr id="S4.T3.9.11.4" class="ltx_tr">
<td id="S4.T3.9.11.4.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T3.9.11.4.1.1" class="ltx_text" style="font-size:80%;">video-SALMONN 13B (ours, visual-only)</span></td>
<td id="S4.T3.9.11.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.9.11.4.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.9.11.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.9.11.4.3.1" class="ltx_text" style="font-size:80%;">35.0%</span></td>
<td id="S4.T3.9.11.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.9.11.4.4.1" class="ltx_text" style="font-size:80%;">46.5%</span></td>
<td id="S4.T3.9.11.4.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.9.11.4.5.1" class="ltx_text" style="font-size:80%;">23.5%</span></td>
<td id="S4.T3.9.11.4.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.9.11.4.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T3.9.12.5" class="ltx_tr">
<td id="S4.T3.9.12.5.1" class="ltx_td ltx_align_left"><span id="S4.T3.9.12.5.1.1" class="ltx_text" style="font-size:80%;">video-SALMONN 7B (ours)</span></td>
<td id="S4.T3.9.12.5.2" class="ltx_td ltx_align_center"><span id="S4.T3.9.12.5.2.1" class="ltx_text" style="font-size:80%;">8.7%</span></td>
<td id="S4.T3.9.12.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.9.12.5.3.1" class="ltx_text" style="font-size:80%;">36.2%</span></td>
<td id="S4.T3.9.12.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.9.12.5.4.1" class="ltx_text" style="font-size:80%;">41.3%</span></td>
<td id="S4.T3.9.12.5.5" class="ltx_td ltx_align_center">
<span id="S4.T3.9.12.5.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">50.5</span><span id="S4.T3.9.12.5.5.2" class="ltx_text" style="font-size:80%;">%</span>
</td>
<td id="S4.T3.9.12.5.6" class="ltx_td ltx_align_center"><span id="S4.T3.9.12.5.6.1" class="ltx_text" style="font-size:80%;">74.3%</span></td>
</tr>
<tr id="S4.T3.9.13.6" class="ltx_tr">
<td id="S4.T3.9.13.6.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T3.9.13.6.1.1" class="ltx_text" style="font-size:80%;">video-SALMONN 13B (ours)</span></td>
<td id="S4.T3.9.13.6.2" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T3.9.13.6.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">7.7</span><span id="S4.T3.9.13.6.2.2" class="ltx_text" style="font-size:80%;">%</span>
</td>
<td id="S4.T3.9.13.6.3" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T3.9.13.6.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">49.8</span><span id="S4.T3.9.13.6.3.2" class="ltx_text" style="font-size:80%;">%</span>
</td>
<td id="S4.T3.9.13.6.4" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T3.9.13.6.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">70.5</span><span id="S4.T3.9.13.6.4.2" class="ltx_text" style="font-size:80%;">%</span>
</td>
<td id="S4.T3.9.13.6.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.9.13.6.5.1" class="ltx_text" style="font-size:80%;">47.6%</span></td>
<td id="S4.T3.9.13.6.6" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T3.9.13.6.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">79.7</span><span id="S4.T3.9.13.6.6.2" class="ltx_text" style="font-size:80%;">%</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Training Data and Specifications</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Multi-task instruction fine-tuning is used to train model parameters of MRC Q-Former and LoRA in video-SALMONN. Training data contains both single-modal and audio-visual paired data. For audio-only tasks, LibriSpeech train-clean-100 and train-clean-360 sets are used for ASR, and AudioCaps are used for AAC. For visual-only tasks. A mixture of LLAVA-150k <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite> image QA data, OCRVQA OCR data <cite class="ltx_cite ltx_citemacro_citep">(Mishra et al., <a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite>, TextCaps <cite class="ltx_cite ltx_citemacro_citep">(Sidorov et al., <a href="#bib.bib43" title="" class="ltx_ref">2020</a>)</cite> image caption data, NExT-QA<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The instruction format (<span id="footnote1.1" class="ltx_text ltx_font_italic">i.e.</span> multiple choice questions) and videos for testing are all unseen for NExT-QA, hence zero-shot.</span></span></span> video QA training data <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al., <a href="#bib.bib50" title="" class="ltx_ref">2021</a>)</cite>, 5000 samples from COCO train2014 data with spoken captions <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib26" title="" class="ltx_ref">2014</a>)</cite> as well as 11k samples from VideoChat <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib25" title="" class="ltx_ref">2023b</a>)</cite> are used. For audio-visual tasks, randomly selected 600-hour Ego4D video captioning data <cite class="ltx_cite ltx_citemacro_citep">(Grauman et al., <a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite>, How2 300-hour training set AVSR data and audio-visual scene-aware dialogue (AVSD) training set are used. The entire training data only contains 1M samples with fewer than 300k video samples, with only publicly available datasets. Details about the training data can be found in Appendix <a href="#A1" title="Appendix A Training Set and Benchmark Details ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Ablation studies on the core components of video-SALMONN based on single modal and audio-visual tasks. Each row represents removing one or more components with other parts remaining the same. Note the last row is equivalent to Video-LLaMA with the same training data, high frame rate video, speech encoder and LoRA, and the comparison to complete video-SALMONN directly reflected the benefit of the proposed structural and training design. AVQA takes the average among the two datasets.</figcaption>
<table id="S4.T4.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.6.6" class="ltx_tr">
<th id="S4.T4.6.6.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.6.6.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Systems</span></th>
<th id="S4.T4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T4.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">ASR</span><span id="S4.T4.1.1.1.2" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T4.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T4.1.1.1.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T4.1.1.1.m1.1.1" xref="S4.T4.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T4.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">OCR</span><span id="S4.T4.2.2.2.2" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T4.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.2.2.2.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T4.2.2.2.m1.1.1" xref="S4.T4.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.m1.1b"><ci id="S4.T4.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T4.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T4.3.3.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Video QA</span><span id="S4.T4.3.3.3.2" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T4.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.3.3.3.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T4.3.3.3.m1.1.1" xref="S4.T4.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.m1.1b"><ci id="S4.T4.3.3.3.m1.1.1.cmml" xref="S4.T4.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T4.4.4.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AVSR</span><span id="S4.T4.4.4.4.2" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T4.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T4.4.4.4.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T4.4.4.4.m1.1.1" xref="S4.T4.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.m1.1b"><ci id="S4.T4.4.4.4.m1.1.1.cmml" xref="S4.T4.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T4.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T4.5.5.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AVQA</span><span id="S4.T4.5.5.5.2" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T4.5.5.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.5.5.5.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T4.5.5.5.m1.1.1" xref="S4.T4.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.m1.1b"><ci id="S4.T4.5.5.5.m1.1.1.cmml" xref="S4.T4.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T4.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T4.6.6.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AVM</span><span id="S4.T4.6.6.6.2" class="ltx_text" style="font-size:80%;"> </span><math id="S4.T4.6.6.6.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.6.6.6.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T4.6.6.6.m1.1.1" xref="S4.T4.6.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.m1.1b"><ci id="S4.T4.6.6.6.m1.1.1.cmml" xref="S4.T4.6.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.6.7.1" class="ltx_tr">
<td id="S4.T4.6.7.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T4.6.7.1.1.1" class="ltx_text" style="font-size:80%;">video-SALMONN</span></td>
<td id="S4.T4.6.7.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.6.7.1.2.1" class="ltx_text" style="font-size:80%;">2.6%</span></td>
<td id="S4.T4.6.7.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.6.7.1.3.1" class="ltx_text" style="font-size:80%;">37.8%</span></td>
<td id="S4.T4.6.7.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.6.7.1.4.1" class="ltx_text" style="font-size:80%;">49.6%</span></td>
<td id="S4.T4.6.7.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.6.7.1.5.1" class="ltx_text" style="font-size:80%;">7.7%</span></td>
<td id="S4.T4.6.7.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.6.7.1.6.1" class="ltx_text" style="font-size:80%;">60.2%</span></td>
<td id="S4.T4.6.7.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.6.7.1.7.1" class="ltx_text" style="font-size:80%;">79.7%</span></td>
</tr>
<tr id="S4.T4.6.8.2" class="ltx_tr">
<td id="S4.T4.6.8.2.1" class="ltx_td ltx_align_left"><span id="S4.T4.6.8.2.1.1" class="ltx_text" style="font-size:80%;">video-SALMONN without 5s-resolution</span></td>
<td id="S4.T4.6.8.2.2" class="ltx_td ltx_align_center"><span id="S4.T4.6.8.2.2.1" class="ltx_text" style="font-size:80%;">2.5%</span></td>
<td id="S4.T4.6.8.2.3" class="ltx_td ltx_align_center"><span id="S4.T4.6.8.2.3.1" class="ltx_text" style="font-size:80%;">35.4%</span></td>
<td id="S4.T4.6.8.2.4" class="ltx_td ltx_align_center"><span id="S4.T4.6.8.2.4.1" class="ltx_text" style="font-size:80%;">47.2%</span></td>
<td id="S4.T4.6.8.2.5" class="ltx_td ltx_align_center"><span id="S4.T4.6.8.2.5.1" class="ltx_text" style="font-size:80%;">7.7%</span></td>
<td id="S4.T4.6.8.2.6" class="ltx_td ltx_align_center"><span id="S4.T4.6.8.2.6.1" class="ltx_text" style="font-size:80%;">57.2%</span></td>
<td id="S4.T4.6.8.2.7" class="ltx_td ltx_align_center"><span id="S4.T4.6.8.2.7.1" class="ltx_text" style="font-size:80%;">77.5%</span></td>
</tr>
<tr id="S4.T4.6.9.3" class="ltx_tr">
<td id="S4.T4.6.9.3.1" class="ltx_td ltx_align_left"><span id="S4.T4.6.9.3.1.1" class="ltx_text" style="font-size:80%;">video-SALMONN without 0.5s-resolution</span></td>
<td id="S4.T4.6.9.3.2" class="ltx_td ltx_align_center"><span id="S4.T4.6.9.3.2.1" class="ltx_text" style="font-size:80%;">2.9%</span></td>
<td id="S4.T4.6.9.3.3" class="ltx_td ltx_align_center"><span id="S4.T4.6.9.3.3.1" class="ltx_text" style="font-size:80%;">37.1%</span></td>
<td id="S4.T4.6.9.3.4" class="ltx_td ltx_align_center"><span id="S4.T4.6.9.3.4.1" class="ltx_text" style="font-size:80%;">49.9%</span></td>
<td id="S4.T4.6.9.3.5" class="ltx_td ltx_align_center"><span id="S4.T4.6.9.3.5.1" class="ltx_text" style="font-size:80%;">8.3%</span></td>
<td id="S4.T4.6.9.3.6" class="ltx_td ltx_align_center"><span id="S4.T4.6.9.3.6.1" class="ltx_text" style="font-size:80%;">58.9%</span></td>
<td id="S4.T4.6.9.3.7" class="ltx_td ltx_align_center"><span id="S4.T4.6.9.3.7.1" class="ltx_text" style="font-size:80%;">80.6%</span></td>
</tr>
<tr id="S4.T4.6.10.4" class="ltx_tr">
<td id="S4.T4.6.10.4.1" class="ltx_td ltx_align_left"><span id="S4.T4.6.10.4.1.1" class="ltx_text" style="font-size:80%;">video-SALMONN without mixed training scheme</span></td>
<td id="S4.T4.6.10.4.2" class="ltx_td ltx_align_center"><span id="S4.T4.6.10.4.2.1" class="ltx_text" style="font-size:80%;">2.6%</span></td>
<td id="S4.T4.6.10.4.3" class="ltx_td ltx_align_center"><span id="S4.T4.6.10.4.3.1" class="ltx_text" style="font-size:80%;">34.0%</span></td>
<td id="S4.T4.6.10.4.4" class="ltx_td ltx_align_center"><span id="S4.T4.6.10.4.4.1" class="ltx_text" style="font-size:80%;">46.9%</span></td>
<td id="S4.T4.6.10.4.5" class="ltx_td ltx_align_center"><span id="S4.T4.6.10.4.5.1" class="ltx_text" style="font-size:80%;">8.3%</span></td>
<td id="S4.T4.6.10.4.6" class="ltx_td ltx_align_center"><span id="S4.T4.6.10.4.6.1" class="ltx_text" style="font-size:80%;">54.0%</span></td>
<td id="S4.T4.6.10.4.7" class="ltx_td ltx_align_center"><span id="S4.T4.6.10.4.7.1" class="ltx_text" style="font-size:80%;">75.3%</span></td>
</tr>
<tr id="S4.T4.6.11.5" class="ltx_tr">
<td id="S4.T4.6.11.5.1" class="ltx_td ltx_align_left"><span id="S4.T4.6.11.5.1.1" class="ltx_text" style="font-size:80%;">video-SALMONN without diversity loss</span></td>
<td id="S4.T4.6.11.5.2" class="ltx_td ltx_align_center"><span id="S4.T4.6.11.5.2.1" class="ltx_text" style="font-size:80%;">2.5%</span></td>
<td id="S4.T4.6.11.5.3" class="ltx_td ltx_align_center"><span id="S4.T4.6.11.5.3.1" class="ltx_text" style="font-size:80%;">36.8%</span></td>
<td id="S4.T4.6.11.5.4" class="ltx_td ltx_align_center"><span id="S4.T4.6.11.5.4.1" class="ltx_text" style="font-size:80%;">49.3%</span></td>
<td id="S4.T4.6.11.5.5" class="ltx_td ltx_align_center"><span id="S4.T4.6.11.5.5.1" class="ltx_text" style="font-size:80%;">7.7%</span></td>
<td id="S4.T4.6.11.5.6" class="ltx_td ltx_align_center"><span id="S4.T4.6.11.5.6.1" class="ltx_text" style="font-size:80%;">53.5%</span></td>
<td id="S4.T4.6.11.5.7" class="ltx_td ltx_align_center"><span id="S4.T4.6.11.5.7.1" class="ltx_text" style="font-size:80%;">78.6%</span></td>
</tr>
<tr id="S4.T4.6.12.6" class="ltx_tr">
<td id="S4.T4.6.12.6.1" class="ltx_td ltx_align_left"><span id="S4.T4.6.12.6.1.1" class="ltx_text" style="font-size:80%;">video-SALMONN without MRC Q-Former</span></td>
<td id="S4.T4.6.12.6.2" class="ltx_td ltx_align_center"><span id="S4.T4.6.12.6.2.1" class="ltx_text" style="font-size:80%;">3.3%</span></td>
<td id="S4.T4.6.12.6.3" class="ltx_td ltx_align_center"><span id="S4.T4.6.12.6.3.1" class="ltx_text" style="font-size:80%;">34.6%</span></td>
<td id="S4.T4.6.12.6.4" class="ltx_td ltx_align_center"><span id="S4.T4.6.12.6.4.1" class="ltx_text" style="font-size:80%;">42.7%</span></td>
<td id="S4.T4.6.12.6.5" class="ltx_td ltx_align_center"><span id="S4.T4.6.12.6.5.1" class="ltx_text" style="font-size:80%;">8.5%</span></td>
<td id="S4.T4.6.12.6.6" class="ltx_td ltx_align_center"><span id="S4.T4.6.12.6.6.1" class="ltx_text" style="font-size:80%;">45.3%</span></td>
<td id="S4.T4.6.12.6.7" class="ltx_td ltx_align_center"><span id="S4.T4.6.12.6.7.1" class="ltx_text" style="font-size:80%;">74.5%</span></td>
</tr>
<tr id="S4.T4.6.13.7" class="ltx_tr">
<td id="S4.T4.6.13.7.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T4.6.13.7.1.1" class="ltx_text" style="font-size:80%;">video-SALMONN without MRC Q-Former, sync. and div.</span></td>
<td id="S4.T4.6.13.7.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.6.13.7.2.1" class="ltx_text" style="font-size:80%;">3.1%</span></td>
<td id="S4.T4.6.13.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.6.13.7.3.1" class="ltx_text" style="font-size:80%;">34.7%</span></td>
<td id="S4.T4.6.13.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.6.13.7.4.1" class="ltx_text" style="font-size:80%;">36.0%</span></td>
<td id="S4.T4.6.13.7.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.6.13.7.5.1" class="ltx_text" style="font-size:80%;">8.9%</span></td>
<td id="S4.T4.6.13.7.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.6.13.7.6.1" class="ltx_text" style="font-size:80%;">44.6%</span></td>
<td id="S4.T4.6.13.7.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.6.13.7.7.1" class="ltx_text" style="font-size:80%;">72.0%</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussions</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Main Results</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The results of video-SALMONN on the SAVE benchmark tasks are summarised in Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Model Configurations ‣ 4 Experimental Setup ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and Table <a href="#S4.T3" title="Table 3 ‣ 4.2 Model Configurations ‣ 4 Experimental Setup ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for single-modal and audio-visual tasks respectively. While other models can only perform a subset of SAVE tasks, video-SALMONN is the first single model that achieves competitive performance on all tasks with remarkably better performance on audio-visual tasks. In particular, video-SALMONN effectively achieves zero-shot audio-visual co-reasoning as an emergent ability, which is reflected by the performance on the two AVQA datasets, the AVSSD and AVM tasks.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">On audio-based tasks in Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Model Configurations ‣ 4 Experimental Setup ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, video-SALMONN obtains both the lowest WER and the highest SPIDEr scores compared to Whisper large-v2 and Video-LLaMA respectively. We do not report WER for Video-LLaMA as that is over 100% due to a very high insertion rate.
On visual tasks, video-SALMONN demonstrates the best results on IC, OCR and Video QA, and on-par results on VQA with InstructBLIP fine-tuned on the same training set. In particular, the multi-resolution causal modelling in video-SALMONN yields over 25% improvements compared to InstructBLIP even though the latter is fine-tuned on the same set of video data. This directly reflects the benefit of the MRC Q-Former.
</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">On audio-visual tasks in Table <a href="#S4.T3" title="Table 3 ‣ 4.2 Model Configurations ‣ 4 Experimental Setup ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, video-SALMONN achieved 7.2% relative WER reduction on the AVSR task compared to Whisper-large-v2. On the AVQA tasks, video-SALMONN achieved over 30% accuracy improvements compared to the Video-LLaMA baseline which does not understand human speech, showcasing its comprehensive understanding ability for speech-audio-visual inputs.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">More importantly, video-SALMONN demonstrated a strong zero-shot audio-visual co-reasoning ability based on the AVM and AVSSD results compared to Video-LLaMA.
Audio-visual co-reasoning (including speech-image co-reasoning) is an important yet challenging ability which requires the model to pay balanced attention to both audio and visual inputs as well as comprehending the intricate instruction beyond simply describing the inputs. This ability is especially enhanced in video-SALMONN by the unpaired audio-visual mixing strategy.
Such tasks were almost infeasible for any other audio-visual models so far, since they were unable to understand both speech and non-speech sounds, or were merely able to verbatim describe the input. Further discussion and qualitative analysis on audio-visual emergent abilities in addition to the audio-visual co-reasoning can be found in Section <a href="#S5.SS5" title="5.5 Emergent Speech-Audio-Visual Co-reasoning ‣ 5 Results and Discussions ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.5</span></a>.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Ablation Studies</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">This section particularly focuses on the key structural novelty, including MRC Q-Former, the fine-grained synchronisation, as well as training techniques in video-SALMONN on selected SAVE benchmark tasks, as summarised in Table <a href="#S4.T4" title="Table 4 ‣ 4.3 Training Data and Specifications ‣ 4 Experimental Setup ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">First, we examine the effect of different resolution levels by training systems with either higher or lower resolutions. Modelling at different resolution levels results in a complementary outcome, where high resolution is better at ASR and AVSR and low resolution is better at OCR and Video-QA. The joint effect of the two resolutions gives the most balanced overall performance on all tasks.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">Next, the effect of the mixed training scheme and diversity loss can be seen by comparing row 4 and row 5 to row 1 in Table <a href="#S4.T4" title="Table 4 ‣ 4.3 Training Data and Specifications ‣ 4 Experimental Setup ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Both techniques provide improvements, particularly to audio-visual understanding tasks including AVQA and AVM, as the model pays balanced attention to both audio and visual streams as well as to different input frames.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">Finally, we provide a comparison of the system without MRC Q-Former, and the system by further removing the temporal synchronisation, as shown in the last two rows of Table <a href="#S5.SS2" title="5.2 Ablation Studies ‣ 5 Results and Discussions ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>. This is a fair comparison to highlight our novel model structure compared to Video-LLaMA under the same training dataset and the same frame rate. Without MRC Q-Former, while experiencing degradation across all tasks, the degradation in ASR, AVSR and Video-QA is the most obvious, as those tasks benefit the most from the multi-resolution design. By further removing the synchronisation, performances on AVSR and AVSSD degrade further due to the lack of cross-modal interactions at the feature level.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2406.15704/assets/x4.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="166" height="74" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Influence of the window sizes <math id="S5.F4.2.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.F4.2.m1.1b"><mi id="S5.F4.2.m1.1.1" xref="S5.F4.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.F4.2.m1.1c"><ci id="S5.F4.2.m1.1.1.cmml" xref="S5.F4.2.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.m1.1d">k</annotation></semantics></math> to the model performance on video QA and AVSR. Results are from systems trained on 10% randomly sampled data for efficient experiments.</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Analysis on Multi-resolution</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.3" class="ltx_p">The MRC Q-Former extracts semantic information from the multimodal inputs at different time scales, which is necessary due to the nature of speech and visual inputs. This can be illustrated by plotting the influence on the performance of video-SALMONN on ASR and Video QA tasks against the number of frames <math id="S5.SS3.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS3.p1.1.m1.1a"><mi id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><ci id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">k</annotation></semantics></math> in a window, as shown in Fig. <a href="#S5.F4" title="Figure 4 ‣ 5.2 Ablation Studies ‣ 5 Results and Discussions ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. For simplicity, only a single resolution level is used for these experiments. The ratio <math id="S5.SS3.p1.2.m2.1" class="ltx_Math" alttext="N/k" display="inline"><semantics id="S5.SS3.p1.2.m2.1a"><mrow id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml"><mi id="S5.SS3.p1.2.m2.1.1.2" xref="S5.SS3.p1.2.m2.1.1.2.cmml">N</mi><mo id="S5.SS3.p1.2.m2.1.1.1" xref="S5.SS3.p1.2.m2.1.1.1.cmml">/</mo><mi id="S5.SS3.p1.2.m2.1.1.3" xref="S5.SS3.p1.2.m2.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><apply id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"><divide id="S5.SS3.p1.2.m2.1.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1.1"></divide><ci id="S5.SS3.p1.2.m2.1.1.2.cmml" xref="S5.SS3.p1.2.m2.1.1.2">𝑁</ci><ci id="S5.SS3.p1.2.m2.1.1.3.cmml" xref="S5.SS3.p1.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">N/k</annotation></semantics></math> is kept constant which keeps the total number of output queries <math id="S5.SS3.p1.3.m3.1" class="ltx_Math" alttext="C=W\times N" display="inline"><semantics id="S5.SS3.p1.3.m3.1a"><mrow id="S5.SS3.p1.3.m3.1.1" xref="S5.SS3.p1.3.m3.1.1.cmml"><mi id="S5.SS3.p1.3.m3.1.1.2" xref="S5.SS3.p1.3.m3.1.1.2.cmml">C</mi><mo id="S5.SS3.p1.3.m3.1.1.1" xref="S5.SS3.p1.3.m3.1.1.1.cmml">=</mo><mrow id="S5.SS3.p1.3.m3.1.1.3" xref="S5.SS3.p1.3.m3.1.1.3.cmml"><mi id="S5.SS3.p1.3.m3.1.1.3.2" xref="S5.SS3.p1.3.m3.1.1.3.2.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S5.SS3.p1.3.m3.1.1.3.1" xref="S5.SS3.p1.3.m3.1.1.3.1.cmml">×</mo><mi id="S5.SS3.p1.3.m3.1.1.3.3" xref="S5.SS3.p1.3.m3.1.1.3.3.cmml">N</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.3.m3.1b"><apply id="S5.SS3.p1.3.m3.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1"><eq id="S5.SS3.p1.3.m3.1.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1.1"></eq><ci id="S5.SS3.p1.3.m3.1.1.2.cmml" xref="S5.SS3.p1.3.m3.1.1.2">𝐶</ci><apply id="S5.SS3.p1.3.m3.1.1.3.cmml" xref="S5.SS3.p1.3.m3.1.1.3"><times id="S5.SS3.p1.3.m3.1.1.3.1.cmml" xref="S5.SS3.p1.3.m3.1.1.3.1"></times><ci id="S5.SS3.p1.3.m3.1.1.3.2.cmml" xref="S5.SS3.p1.3.m3.1.1.3.2">𝑊</ci><ci id="S5.SS3.p1.3.m3.1.1.3.3.cmml" xref="S5.SS3.p1.3.m3.1.1.3.3">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.3.m3.1c">C=W\times N</annotation></semantics></math> unchanged for varying window sizes.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Speech contains temporally fine-grained information which requires high-resolution modelling to achieve better performance. Hence the WER decreases when the window size becomes smaller (<span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_italic">i.e.</span> higher resolution). On the other hand, when the window size becomes smaller, fewer output tokens are used to encapsulate all the visual information within that window, causing performance degradation on video QA. Therefore, it presents a trade-off between speech and visual inputs about the granularity of the sliding windows, validating our motivation for the multi-resolution design.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">To illustrate the functionality of each resolution level, we apply zero masks to the output query of one resolution level and observe the performance of another, as shown in Table <a href="#S5.T5" title="Table 5 ‣ 5.3 Analysis on Multi-resolution ‣ 5 Results and Discussions ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. The system learns to split the functionality into two resolutions: the high resolution takes care of speech content-related information and the low resolution takes care of high-level information such as Video QA. This agrees with our findings from the ablation studies. Moreover, the complementarity of the two resolution levels is further processed by the LLM to achieve the best outcome.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>Analysis of the effect of each resolution level reflected by ASR, IC and Video-QA tasks, with average cosine similarity between query vectors and word embeddings shown in brackets.</figcaption>
<table id="S5.T5.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.3.3" class="ltx_tr">
<th id="S5.T5.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S5.T5.3.3.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Resolution level</span></th>
<th id="S5.T5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T5.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">ASR</span><span id="S5.T5.1.1.1.2" class="ltx_text" style="font-size:80%;"> </span><math id="S5.T5.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T5.1.1.1.m1.1a"><mo mathsize="80%" stretchy="false" id="S5.T5.1.1.1.m1.1.1" xref="S5.T5.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.m1.1b"><ci id="S5.T5.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S5.T5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T5.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">IC</span><span id="S5.T5.2.2.2.2" class="ltx_text" style="font-size:80%;"> </span><math id="S5.T5.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T5.2.2.2.m1.1a"><mo mathsize="80%" stretchy="false" id="S5.T5.2.2.2.m1.1.1" xref="S5.T5.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.2.m1.1b"><ci id="S5.T5.2.2.2.m1.1.1.cmml" xref="S5.T5.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S5.T5.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S5.T5.3.3.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Video QA</span><span id="S5.T5.3.3.3.2" class="ltx_text" style="font-size:80%;"> </span><math id="S5.T5.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T5.3.3.3.m1.1a"><mo mathsize="80%" stretchy="false" id="S5.T5.3.3.3.m1.1.1" xref="S5.T5.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T5.3.3.3.m1.1b"><ci id="S5.T5.3.3.3.m1.1.1.cmml" xref="S5.T5.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.3.4.1" class="ltx_tr">
<th id="S5.T5.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S5.T5.3.4.1.1.1" class="ltx_text" style="font-size:80%;">Both</span></th>
<td id="S5.T5.3.4.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.3.4.1.2.1" class="ltx_text" style="font-size:80%;">2.6%</span></td>
<td id="S5.T5.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.3.4.1.3.1" class="ltx_text" style="font-size:80%;">89.6</span></td>
<td id="S5.T5.3.4.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.3.4.1.4.1" class="ltx_text" style="font-size:80%;">49.6%</span></td>
</tr>
<tr id="S5.T5.3.5.2" class="ltx_tr">
<th id="S5.T5.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T5.3.5.2.1.1" class="ltx_text" style="font-size:80%;">0.5s</span></th>
<td id="S5.T5.3.5.2.2" class="ltx_td ltx_align_center"><span id="S5.T5.3.5.2.2.1" class="ltx_text" style="font-size:80%;">2.6%</span></td>
<td id="S5.T5.3.5.2.3" class="ltx_td ltx_align_center"><span id="S5.T5.3.5.2.3.1" class="ltx_text" style="font-size:80%;">35.8</span></td>
<td id="S5.T5.3.5.2.4" class="ltx_td ltx_align_center"><span id="S5.T5.3.5.2.4.1" class="ltx_text" style="font-size:80%;">14.4%</span></td>
</tr>
<tr id="S5.T5.3.6.3" class="ltx_tr">
<th id="S5.T5.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S5.T5.3.6.3.1.1" class="ltx_text" style="font-size:80%;">5.0s</span></th>
<td id="S5.T5.3.6.3.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T5.3.6.3.2.1" class="ltx_text" style="font-size:80%;">100+%</span></td>
<td id="S5.T5.3.6.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T5.3.6.3.3.1" class="ltx_text" style="font-size:80%;">23.0</span></td>
<td id="S5.T5.3.6.3.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T5.3.6.3.4.1" class="ltx_text" style="font-size:80%;">41.9%</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Analysis of the Diversity Loss</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Analysis of the effect of diversity loss is also performed using 10% of the training data as shown in Figure <a href="#S5.F5" title="Figure 5 ‣ 5.4 Analysis of the Diversity Loss ‣ 5 Results and Discussions ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, and examples of cosine similarity matrices among output queries are shown in Appendix <a href="#A5" title="Appendix E Visualisation of Diversity Loss Effect ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>. For ASR, the model is trained to include all the speech information in the audio sequence and the cosine similarity varies according to the length of the speech. For videos, the cosine similarity does not vary a lot for different video lengths, and hence diversity loss effectively acts as a way to encourage more diversified information to be captured. However, when a high <math id="S5.SS4.p1.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S5.SS4.p1.1.m1.1a"><mi id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><ci id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">\lambda</annotation></semantics></math> is employed, diverse output queries confuse the LLM and hence cause severe hallucination problems (<span id="S5.SS4.p1.1.1" class="ltx_text ltx_font_italic">e.g.</span> high insertion rate in WER) that degrades performance.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2406.15704/assets/x5.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="175" height="68" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Variations of model performance by varying diversity loss factor, <span id="S5.F5.6.1" class="ltx_text ltx_font_italic">i.e.</span> <math id="S5.F5.3.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S5.F5.3.m1.1b"><mi id="S5.F5.3.m1.1.1" xref="S5.F5.3.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S5.F5.3.m1.1c"><ci id="S5.F5.3.m1.1.1.cmml" xref="S5.F5.3.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.3.m1.1d">\lambda</annotation></semantics></math> in Eqn. (<a href="#S3.E6" title="Equation 6 ‣ 3.3 System Training ‣ 3 video-SALMONN ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>), on (a) AVSR (%WER), and (b) Video QA (%Accuracy). Variations of average cosine similarities among output query vectors are also shown under different <math id="S5.F5.4.m2.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S5.F5.4.m2.1b"><mi id="S5.F5.4.m2.1.1" xref="S5.F5.4.m2.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S5.F5.4.m2.1c"><ci id="S5.F5.4.m2.1.1.cmml" xref="S5.F5.4.m2.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.4.m2.1d">\lambda</annotation></semantics></math>’s.</figcaption>
</figure>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Emergent Speech-Audio-Visual Co-reasoning</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">In addition to objective measurements, we illustrate the unprecedented emergent speech-audio-visual co-reasoning abilities of video-SALMONN via examples in Appendix <a href="#A10" title="Appendix J Case Studies ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">J</span></a>. For instance,
video-SALMONN can answer questions in the speech about the image or video (see Fig. <a href="#A10.F9" title="Figure 9 ‣ Appendix J Case Studies ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>). Benefiting from the mixed training scheme, video-SALMONN can write a coherent story based on unpaired audio and video (see Fig. <a href="#A10.F11" title="Figure 11 ‣ Appendix J Case Studies ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>). More importantly,
in response to questions about why a movie clip is funny or romantic, video-SALMONN combines the video, dialogue between characters and background audio or music to generate a more encompassing and convincing answer (see Fig. <a href="#A10.F12" title="Figure 12 ‣ Appendix J Case Studies ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> and <a href="#A10.F15" title="Figure 15 ‣ Appendix J Case Studies ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>). Besides, video-SALMONN can understand the scene better by using knowledge from the speech, such as the species of a particular fish introduced in a documentary (see Fig. <a href="#A10.F13" title="Figure 13 ‣ Appendix J Case Studies ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>). Moreover, the co-occurrence of speech and video events, such as attributing an utterance to a specific character (see Fig. <a href="#A10.F14" title="Figure 14 ‣ Appendix J Case Studies ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> and Fig. <a href="#A10.F16" title="Figure 16 ‣ Appendix J Case Studies ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>), can only be achieved by the dedicated structural design of video-SALMONN.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper proposes video-SALMONN, the first single end-to-end av-LLMs that can understand all elements in video data, including visual frame sequence, speech, audio events, and music. To enhance the model’s speech and comprehensive video understanding abilities, structural designs including MRC Q-Former, fine-grained synchronisation and a mixed training scheme are proposed. Evaluated on the introduced SAVE benchmark, video-SALMONN demonstrates superior performance compared to single-modal baselines, while achieving 25% accuracy improvements on Video QA and over 30% accuracy improvements on audio-visual QA compared to a strong baseline of Video-LLaMA. Moreover, video-SALMONN showcases unprecedented audio-visual, and particularly strong speech-visual co-reasoning abilities, with remarkable emergent abilities illustrated via examples.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Impact Statement</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Enabling speech understanding in av-LLMs marks an advancement towards achieving artificial general intelligence (AGI). By integrating speech input on top of existing non-speech audio and visual inputs, such a model would gain a holistic understanding of human interaction and the environment and is enabled to a broader range of applications. The potential positive impacts include:</p>
<ul id="Sx1.I1" class="ltx_itemize">
<li id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i1.p1.1" class="ltx_p">video-SALMONN enables more natural and intuitive interactions with technology, reducing the learning curve for users and making LLM-based technologies more approachable <span id="Sx1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">e.g.</span> for children and the elderly.</p>
</div>
</li>
<li id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i2.p1.1" class="ltx_p">video-SALMONN can potentially enhance the accessibility of LLM-based technologies, including those with motor impairments that make typing difficult.</p>
</div>
</li>
<li id="Sx1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i3.p1.1" class="ltx_p">The video-QA demonstrates the potential of using video-SALMONN in academic presentations and educational applications to facilitate learning.</p>
</div>
</li>
</ul>
<p id="Sx1.p1.2" class="ltx_p">The approaches in this paper do not give rise to any additional potential biases beyond the ones directly inherited from the pre-trained model checkpoints used. The audio encoder and visual encoder might work worse for people from particular demographics. The framework also inherits biases from all the LLMs used in this paper. To mitigate potential biases, we clearly describe the nature of each dataset and provide clear and adequate references to all the resources we used for video-SALMONN.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">The ability of video-SALMONN to understand speech in videos could lead to potential technology abuses like surveillance and eavesdropping. To counter this, we’ve consulted with legal experts to establish clear usage guidelines, reducing risks and addressing concerns, highlighting our dedication to responsible research sharing.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al. (2022)</span>
<span class="ltx_bibblock">
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., et al.

</span>
<span class="ltx_bibblock">Flamingo: A visual language model for few-shot learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proc. NeurIPS</em>, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anil et al. (2023)</span>
<span class="ltx_bibblock">
Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., et al.

</span>
<span class="ltx_bibblock">PaLM 2 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv:2305.10403</em>, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proc. NeurIPS</em>, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023a)</span>
<span class="ltx_bibblock">
Chen, F., Han, M., Zhao, H., Zhang, Q., Shi, J., Xu, S., and Xu, B.

</span>
<span class="ltx_bibblock">X-LLM: Bootstrapping advanced large language models by treating multi-modalities as foreign languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv:2305.04160</em>, 2023a.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023b)</span>
<span class="ltx_bibblock">
Chen, G., Zheng, Y.-D., Wang, J., Xu, J., Huang, Y., Pan, J., Wang, Y., Wang, Y., Qiao, Y., Lu, T., and Wang, L.

</span>
<span class="ltx_bibblock">VideoLLM: Modeling video sequence with large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv:2305.13292</em>, 2023b.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Chen, H., Xie, W., Vedaldi, A., and Zisserman, A.

</span>
<span class="ltx_bibblock">VGGSound: A large-scale audio-visual dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023c)</span>
<span class="ltx_bibblock">
Chen, S., Li, H., Wang, Q., Zhao, Z., Sun, M., Zhu, X., and Liu, J.

</span>
<span class="ltx_bibblock">Vast: A vision-audio-subtitle-text omni-modality foundation model and dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proc. NeurIPS</em>, 2023c.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023d)</span>
<span class="ltx_bibblock">
Chen, S., Wu, Y., Wang, C., Liu, S., Tompkins, D., Chen, Z., Che, W., Yu, X., and Wei, F.

</span>
<span class="ltx_bibblock">BEATs: Audio pre-training with acoustic tokenizers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proc. ICML</em>, 2023d.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al. (2023)</span>
<span class="ltx_bibblock">
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P.

</span>
<span class="ltx_bibblock">Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality, March 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://lmsys.org/blog/2023-03-30-vicuna/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://lmsys.org/blog/2023-03-30-vicuna/</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. (2022)</span>
<span class="ltx_bibblock">
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., et al.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv:2210.11416</em>, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2023)</span>
<span class="ltx_bibblock">
Dai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S.

</span>
<span class="ltx_bibblock">InstructBLIP: Towards general-purpose vision-language models with instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv:2305.06500</em>, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. (2022)</span>
<span class="ltx_bibblock">
Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., and Tang, J.

</span>
<span class="ltx_bibblock">GLM: General language model pretraining with autoregressive blank infilling.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proc. ACL</em>, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fiscus et al. (2006a)</span>
<span class="ltx_bibblock">
Fiscus, J. G., Ajot, J., Michel, M., and Garofolo, J. S.

</span>
<span class="ltx_bibblock">The rich transcription 2006 spring meeting recognition evaluation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Machine Learning for Multimodal Interaction: Third International Workshop, MLMI 2006, Bethesda, MD, USA, May 1-4, 2006, Revised Selected Papers 3</em>, pp.  309–322. Springer, 2006a.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fiscus et al. (2006b)</span>
<span class="ltx_bibblock">
Fiscus, J. G., Radde, N., Garofolo, J. S., Le, A., Ajot, J., and Laprun, C.

</span>
<span class="ltx_bibblock">The rich transcription 2005 spring meeting recognition evaluation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Machine Learning for Multimodal Interaction: Second International Workshop, MLMI 2005, Edinburgh, UK, July 11-13, 2005, Revised Selected Papers 2</em>, pp.  369–389. Springer, 2006b.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fiscus et al. (2007)</span>
<span class="ltx_bibblock">
Fiscus, J. G., Ajot, J., and Garofolo, J. S.

</span>
<span class="ltx_bibblock">The rich transcription 2007 meeting recognition evaluation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">CLEaR</em>, 2007.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:15113788" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:15113788</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garofolo et al. (2004)</span>
<span class="ltx_bibblock">
Garofolo, J. S., Fiscus, J. G., and Laprun, C. D.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">The rich transcription 2004 spring meeting recognition evaluation</em>.

</span>
<span class="ltx_bibblock">US Department of Commerce, National Institute of Standards and Technology, 2004.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Girdhar et al. (2023)</span>
<span class="ltx_bibblock">
Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K. V., Joulin, A., and Misra, I.

</span>
<span class="ltx_bibblock">ImageBind: One embedding space to bind them all.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv:2305.05665</em>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. (2023)</span>
<span class="ltx_bibblock">
Gong, Y., Luo, H., Liu, A. H., Karlinsky, L., and Glass, J.

</span>
<span class="ltx_bibblock">Listen, think, and understand.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv:2305.10790</em>, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grauman et al. (2022)</span>
<span class="ltx_bibblock">
Grauman, K., Westbury, A., Byrne, E., et al.

</span>
<span class="ltx_bibblock">Ego4D: Around the world in 3,000 hours of egocentric video.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proc. CVPR</em>, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2020)</span>
<span class="ltx_bibblock">
Hsu, W.-N., Harwath, D., Song, C., and Glass, J.

</span>
<span class="ltx_bibblock">Text-free image-to-speech synthesis using learned segmental units.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proc. NeurIPS Workshop on Self-Supervised Learning for Speech and Audio Processing</em>, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2022)</span>
<span class="ltx_bibblock">
Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.

</span>
<span class="ltx_bibblock">LoRA: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proc. ICLR</em>, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson &amp; Manning (2019)</span>
<span class="ltx_bibblock">
Hudson, D. A. and Manning, C. D.

</span>
<span class="ltx_bibblock">GQA: A new dataset for real-world visual reasoning and compositional question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proc. CVPR</em>, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2019)</span>
<span class="ltx_bibblock">
Kim, C. D., Kim, B., Lee, H., and Kim, G.

</span>
<span class="ltx_bibblock">AudioCaps: Generating captions for audios in the wild.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proc. NAACL-HLT</em>, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023a)</span>
<span class="ltx_bibblock">
Li, J., Li, D., Savarese, S., and Hoi, S.

</span>
<span class="ltx_bibblock">BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proc. ICML</em>, 2023a.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023b)</span>
<span class="ltx_bibblock">
Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L., and Qiao, Y.

</span>
<span class="ltx_bibblock">VideoChat: Chat-centric video understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv:2305.06355</em>, 2023b.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Lin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L., and Dollár, P.

</span>
<span class="ltx_bibblock">Microsoft COCO: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proc. ECCV</em>, 2014.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Liu, H., Li, C., Wu, Q., and Lee, Y. J.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv:2304.08485</em>, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2017)</span>
<span class="ltx_bibblock">
Liu, S., Zhu, Z., Ye, N., Guadarrama, S., and Murphy, K.

</span>
<span class="ltx_bibblock">Improved image captioning via policy gradient optimization of spider.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proc. ICCV</em>, Venice, 2017.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2023)</span>
<span class="ltx_bibblock">
Luo, R., Zhao, Z., Yang, M., Dong, J., Qiu, M., Lu, P., Wang, T., and Wei, Z.

</span>
<span class="ltx_bibblock">Valley: Video assistant with large language model enhanced ability.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv: 2306.07207</em>, 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. (2023)</span>
<span class="ltx_bibblock">
Lyu, C., Wu, M., Wang, L., Huang, X., Liu, B., Du, Z., Shi, S., and Tu, Z.

</span>
<span class="ltx_bibblock">Macaw-LLM: Multi-modal language modeling with image, audio, video, and text integration.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv:2306.09093</em>, 2023.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maaz et al. (2023)</span>
<span class="ltx_bibblock">
Maaz, M., Rasheed, H., Khan, S., and Khan, F. S.

</span>
<span class="ltx_bibblock">Video-ChatGPT: Towards detailed video understanding via large vision and language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv:2306.05424</em>, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et al. (2019)</span>
<span class="ltx_bibblock">
Mishra, A., Shekhar, S., Singh, A. K., and Chakraborty, A.

</span>
<span class="ltx_bibblock">OCR-VQA: Visual question answering by reading text in images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proc. ICDAR</em>, 2019.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-4 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv:2303.08774</em>, 2023.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2022)</span>
<span class="ltx_bibblock">
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., et al.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proc. NeurIPS</em>, 2022.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panayotov et al. (2015)</span>
<span class="ltx_bibblock">
Panayotov, V., Chen, G., Povey, D., and Khudanpur, S.

</span>
<span class="ltx_bibblock">Librispeech: An ASR corpus based on public domain audio books.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2015.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2023)</span>
<span class="ltx_bibblock">
Peng, B., Li, C., He, P., Galley, M., and Gao, J.

</span>
<span class="ltx_bibblock">Instruction tuning with GPT-4.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv:2304.03277</em>, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Piergiovanni et al. (2023)</span>
<span class="ltx_bibblock">
Piergiovanni, A., Noble, I., Kim, D., Ryoo, M. S., Gomes, V., and Angelova, A.

</span>
<span class="ltx_bibblock">Mirasol3b: A multimodal autoregressive model for time-aligned and contextual modalities.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.05698</em>, 2023.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2023)</span>
<span class="ltx_bibblock">
Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I.

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proc. ICML</em>, 2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rubenstein et al. (2023)</span>
<span class="ltx_bibblock">
Rubenstein, P. K., Asawaroengchai, C., Nguyen, D. D., et al.

</span>
<span class="ltx_bibblock">AudioPaLM: A large language model that can speak and listen.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv:2306.12925</em>, 2023.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanabria et al. (2018)</span>
<span class="ltx_bibblock">
Sanabria, R., Caglayan, O., Palaskar, S., Elliott, D., Barrault, L., Specia, L., and Metze, F.

</span>
<span class="ltx_bibblock">How2: A large-scale dataset for multimodal language understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proc. ViGIL</em>, 2018.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shu et al. (2023a)</span>
<span class="ltx_bibblock">
Shu, F., Zhang, L., Jiang, H., and Xie, C.

</span>
<span class="ltx_bibblock">Audio-visual llm for video understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv: 2312.06720</em>, 2023a.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shu et al. (2023b)</span>
<span class="ltx_bibblock">
Shu, F., Zhang, L., Jiang, H., and Xie, C.

</span>
<span class="ltx_bibblock">Audio-visual llm for video understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.06720</em>, 2023b.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sidorov et al. (2020)</span>
<span class="ltx_bibblock">
Sidorov, O., Hu, R., Rohrbach, M., and Singh, A.

</span>
<span class="ltx_bibblock">Textcaps: a dataset for image captioningwith reading comprehension.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proc. European Conference on Computer Vision</em>, 2020.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2019)</span>
<span class="ltx_bibblock">
Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M.

</span>
<span class="ltx_bibblock">Towards vqa models that can read.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proc. CVPR</em>, 2019.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2023)</span>
<span class="ltx_bibblock">
Su, Y., Lan, T., Li, H., Xu, J., Wang, Y., and Cai, D.

</span>
<span class="ltx_bibblock">PandaGPT: One model to instruction-follow them all.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv:2305.16355</em>, 2023.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2023)</span>
<span class="ltx_bibblock">
Sun, G., Yu, W., Tang, C., Chen, X., Tan, T., Li, W., Lu, L., Ma, Z., and Zhang, C.

</span>
<span class="ltx_bibblock">Fine-grained audio-visual joint representations for multimodal large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv:2310.05863</em>, 2023.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2023)</span>
<span class="ltx_bibblock">
Tang, C., Yu, W., Sun, G., Chen, X., Tan, T., Li, W., Lu, L., Ma, Z., and Zhang, C.

</span>
<span class="ltx_bibblock">SALMONN: Towards generic hearing abilities for large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">arXiv:2310.13289</em>, 2023.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2024)</span>
<span class="ltx_bibblock">
Tang, C., Yu, W., Sun, G., Chen, X., Tan, T., Li, W., Lu, L., Ma, Z., and Zhang, C.

</span>
<span class="ltx_bibblock">Extending large language models for speech and audio captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">To appear in Proc. ICASSP</em>, 2024.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G.

</span>
<span class="ltx_bibblock">LLaMA: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv:2302.13971</em>, 2023.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. (2021)</span>
<span class="ltx_bibblock">
Xiao, J., Shang, X., Yao, A., and Chua, T.-S.

</span>
<span class="ltx_bibblock">NExT-QA: Next phase of question-answering to explaining temporal actions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proc. CVPR</em>, 2021.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Young et al. (2014)</span>
<span class="ltx_bibblock">
Young, P., Lai, A., Hodosh, M., and Hockenmaier, J.

</span>
<span class="ltx_bibblock">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 2:67–78, 2014.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2024)</span>
<span class="ltx_bibblock">
Yu, W., Tang, C., Sun, G., Chen, X., Tan, T., Li, W., Lu, L., Ma, Z., and Zhang, C.

</span>
<span class="ltx_bibblock">Connecting speech encoder and large language model for ASR.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">To appear in Proc. ICASSP</em>, 2024.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. (2023)</span>
<span class="ltx_bibblock">
Zeng, Y., Zhang, H., Zheng, J., Xia, J., Wei, G., Wei, Y., Zhang, Y., and Kong, T.

</span>
<span class="ltx_bibblock">What matters in training a gpt4-style language model with multimodal inputs?

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">arXiv:2307.02469</em>, 2023.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023a)</span>
<span class="ltx_bibblock">
Zhang, D., Li, S., Zhang, X., Zhan, J., Wang, P., Zhou, Y., and Qiu, X.

</span>
<span class="ltx_bibblock">SpeechGPT: Empowering large language models with intrinsic cross-modal conversational abilities.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">arXiv:2305.11000</em>, 2023a.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023b)</span>
<span class="ltx_bibblock">
Zhang, H., Li, X., and Bing, L.

</span>
<span class="ltx_bibblock">Video-LLaMA: An instruction-tuned audio-visual language model for video understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">arXiv:2306.02858</em>, 2023b.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2022)</span>
<span class="ltx_bibblock">
Zhao, Y., Misra, I., Krähenbühl, P., and Girdhar, R.

</span>
<span class="ltx_bibblock">Learning video representations from large language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proc. CVPR</em>, 2022.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023)</span>
<span class="ltx_bibblock">
Zhao, Y., Lin, Z., Zhou, D., Huang, Z., Feng, J., and Kang, B.

</span>
<span class="ltx_bibblock">BuboGPT: Enabling visual grounding in multi-modal LLMs.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">arXiv:2307.08581</em>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Training Set and Benchmark Details</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">A range of datasets spanning audio and visual tasks are used in our experiments. Table <a href="#A1.T6" title="Table 6 ‣ Appendix A Training Set and Benchmark Details ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and <a href="#A1.T7" title="Table 7 ‣ Appendix A Training Set and Benchmark Details ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> summarise these datasets in detail, with individual descriptions and relevant prompt designs.</p>
</div>
<figure id="A1.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 6: </span>Dataset and benchmark details part 1</figcaption>
<table id="A1.T6.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T6.3.1.1" class="ltx_tr">
<td id="A1.T6.3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="A1.T6.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Data</span></td>
<td id="A1.T6.3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T6.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">In Train</span></td>
<td id="A1.T6.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A1.T6.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">In SAVE</span></td>
<td id="A1.T6.3.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T6.3.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.3.1.1.4.1.1" class="ltx_p" style="width:274.6pt;"><span id="A1.T6.3.1.1.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Description</span></span>
</span>
</td>
</tr>
<tr id="A1.T6.3.2.2" class="ltx_tr">
<td id="A1.T6.3.2.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A1.T6.3.2.2.1.1" class="ltx_text" style="font-size:80%;">LibriSpeech</span></td>
<td id="A1.T6.3.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.2.2.2.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.2.2.3.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.3.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.3.2.2.4.1.1" class="ltx_p" style="width:274.6pt;"><span id="A1.T6.3.2.2.4.1.1.1" class="ltx_text" style="font-size:80%;">LibriSpeech is an English audiobook data. The train-clean-100 and train-clean-360 splits were used for training, and test-clean was used in SAVE. Prompt example: “Transcribe the speech into text.”</span></span>
</span>
</td>
</tr>
<tr id="A1.T6.3.3.3" class="ltx_tr">
<td id="A1.T6.3.3.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A1.T6.3.3.3.1.1" class="ltx_text" style="font-size:80%;">AudioCaps</span></td>
<td id="A1.T6.3.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.3.3.2.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.3.3.3.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.3.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.3.3.3.4.1.1" class="ltx_p" style="width:274.6pt;"><span id="A1.T6.3.3.3.4.1.1.1" class="ltx_text" style="font-size:80%;">AudioCaps is a widely used audio caption dataset containing 46k 10-second audio samples with manually annotated captions. Example prompt: “Please describe the audio.”</span></span>
</span>
</td>
</tr>
<tr id="A1.T6.3.4.4" class="ltx_tr">
<td id="A1.T6.3.4.4.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A1.T6.3.4.4.1.1" class="ltx_text" style="font-size:80%;">LLAVA-150k</span></td>
<td id="A1.T6.3.4.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.4.4.2.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.4.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.4.4.3.1" class="ltx_text" style="font-size:80%;">No</span></td>
<td id="A1.T6.3.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.3.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.3.4.4.4.1.1" class="ltx_p" style="width:274.6pt;"><span id="A1.T6.3.4.4.4.1.1.1" class="ltx_text" style="font-size:80%;">LLAVA-150k contain QA pairs generated using ChatGPT. Example prompt: “What does the man hold in the image?”</span></span>
</span>
</td>
</tr>
<tr id="A1.T6.3.5.5" class="ltx_tr">
<td id="A1.T6.3.5.5.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A1.T6.3.5.5.1.1" class="ltx_text" style="font-size:80%;">OCRVQA</span></td>
<td id="A1.T6.3.5.5.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.5.5.2.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.5.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.5.5.3.1" class="ltx_text" style="font-size:80%;">No</span></td>
<td id="A1.T6.3.5.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.3.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.3.5.5.4.1.1" class="ltx_p" style="width:274.6pt;"><span id="A1.T6.3.5.5.4.1.1.1" class="ltx_text" style="font-size:80%;">OCRVQA is an OCR-based QA dataset containing questions mostly about printed words in an image. Example prompt: “Who wrote this book?”</span></span>
</span>
</td>
</tr>
<tr id="A1.T6.3.6.6" class="ltx_tr">
<td id="A1.T6.3.6.6.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A1.T6.3.6.6.1.1" class="ltx_text" style="font-size:80%;">TextVQA</span></td>
<td id="A1.T6.3.6.6.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.6.6.2.1" class="ltx_text" style="font-size:80%;">No</span></td>
<td id="A1.T6.3.6.6.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.6.6.3.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.6.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.3.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.3.6.6.4.1.1" class="ltx_p" style="width:274.6pt;"><span id="A1.T6.3.6.6.4.1.1.1" class="ltx_text" style="font-size:80%;">OCR-based QA dataset containing questions about various words in realistic scenes (</span><span id="A1.T6.3.6.6.4.1.1.2" class="ltx_text ltx_font_italic" style="font-size:80%;">c.f.</span><span id="A1.T6.3.6.6.4.1.1.3" class="ltx_text" style="font-size:80%;"> printed words). Example prompt: “What is the brand of this camera?”</span></span>
</span>
</td>
</tr>
<tr id="A1.T6.3.7.7" class="ltx_tr">
<td id="A1.T6.3.7.7.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A1.T6.3.7.7.1.1" class="ltx_text" style="font-size:80%;">Flickr30k</span></td>
<td id="A1.T6.3.7.7.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.7.7.2.1" class="ltx_text" style="font-size:80%;">No</span></td>
<td id="A1.T6.3.7.7.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.7.7.3.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.7.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.3.7.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.3.7.7.4.1.1" class="ltx_p" style="width:274.6pt;"><span id="A1.T6.3.7.7.4.1.1.1" class="ltx_text" style="font-size:80%;">Image caption dataset where each image is annotated with manual single-sentence descriptions. Example prompt: “Describe this image in one short sentence.”</span></span>
</span>
</td>
</tr>
<tr id="A1.T6.3.8.8" class="ltx_tr">
<td id="A1.T6.3.8.8.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A1.T6.3.8.8.1.1" class="ltx_text" style="font-size:80%;">GQA</span></td>
<td id="A1.T6.3.8.8.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.8.8.2.1" class="ltx_text" style="font-size:80%;">No</span></td>
<td id="A1.T6.3.8.8.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.8.8.3.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.8.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.3.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.3.8.8.4.1.1" class="ltx_p" style="width:274.6pt;"><span id="A1.T6.3.8.8.4.1.1.1" class="ltx_text" style="font-size:80%;">GQA consists of questions about various day-to-day real-world images. This involves reasoning skills about the objects in the image. Example prompt: “What kind of device is on top of the desk?”</span></span>
</span>
</td>
</tr>
<tr id="A1.T6.3.9.9" class="ltx_tr">
<td id="A1.T6.3.9.9.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A1.T6.3.9.9.1.1" class="ltx_text" style="font-size:80%;">TextCaps</span></td>
<td id="A1.T6.3.9.9.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.9.9.2.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.9.9.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.9.9.3.1" class="ltx_text" style="font-size:80%;">No</span></td>
<td id="A1.T6.3.9.9.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.3.9.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.3.9.9.4.1.1" class="ltx_p" style="width:274.6pt;"><span id="A1.T6.3.9.9.4.1.1.1" class="ltx_text" style="font-size:80%;">Image caption data particularly focusing on capturing text in the image. Only 80k samples were randomly selected for training. Example prompt: “Describe the image.”</span></span>
</span>
</td>
</tr>
<tr id="A1.T6.3.10.10" class="ltx_tr">
<td id="A1.T6.3.10.10.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A1.T6.3.10.10.1.1" class="ltx_text" style="font-size:80%;">MSVD-QA</span></td>
<td id="A1.T6.3.10.10.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.10.10.2.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.10.10.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.10.10.3.1" class="ltx_text" style="font-size:80%;">No</span></td>
<td id="A1.T6.3.10.10.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.3.10.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.3.10.10.4.1.1" class="ltx_p" style="width:274.6pt;"><span id="A1.T6.3.10.10.4.1.1.1" class="ltx_text" style="font-size:80%;">MSVD-QA is a dataset with questions about real-world video clips. Example prompt: “In the video, what is the man with long hair playing?”</span></span>
</span>
</td>
</tr>
<tr id="A1.T6.3.11.11" class="ltx_tr">
<td id="A1.T6.3.11.11.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A1.T6.3.11.11.1.1" class="ltx_text" style="font-size:80%;">NExT-QA</span></td>
<td id="A1.T6.3.11.11.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.11.11.2.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.11.11.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.11.11.3.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.11.11.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.3.11.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.3.11.11.4.1.1" class="ltx_p" style="width:274.6pt;"><span id="A1.T6.3.11.11.4.1.1.1" class="ltx_text" style="font-size:80%;">NExT-QA is a video QA dataset, particularly focusing on causal and temporal correlations. Example prompt: “What does the girl in white do after bending down in the middle? Options/Choose one from: (Add choices here during inference)”.</span></span>
</span>
</td>
</tr>
<tr id="A1.T6.3.12.12" class="ltx_tr">
<td id="A1.T6.3.12.12.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A1.T6.3.12.12.1.1" class="ltx_text" style="font-size:80%;">VideoChat</span></td>
<td id="A1.T6.3.12.12.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.12.12.2.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.12.12.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.12.12.3.1" class="ltx_text" style="font-size:80%;">No</span></td>
<td id="A1.T6.3.12.12.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.3.12.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.3.12.12.4.1.1" class="ltx_p" style="width:274.6pt;"><span id="A1.T6.3.12.12.4.1.1.1" class="ltx_text" style="font-size:80%;">A GPT4-generated video QA dataset where the question mainly asks for detailed descriptions of the video. Example prompt: “Provide a detailed description of the given video.”</span></span>
</span>
</td>
</tr>
<tr id="A1.T6.3.13.13" class="ltx_tr">
<td id="A1.T6.3.13.13.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A1.T6.3.13.13.1.1" class="ltx_text" style="font-size:80%;">AVSD</span></td>
<td id="A1.T6.3.13.13.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.13.13.2.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.13.13.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.13.13.3.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.13.13.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.3.13.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.3.13.13.4.1.1" class="ltx_p" style="width:274.6pt;"><span id="A1.T6.3.13.13.4.1.1.1" class="ltx_text" style="font-size:80%;">Audio-visual scene-aware dialogue data where questions are raised in turns about the video and the audio in the video. Example prompt: “And then what happened?” and “Is the man saying anything?”</span></span>
</span>
</td>
</tr>
<tr id="A1.T6.3.14.14" class="ltx_tr">
<td id="A1.T6.3.14.14.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A1.T6.3.14.14.1.1" class="ltx_text" style="font-size:80%;">Ego4D</span></td>
<td id="A1.T6.3.14.14.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.14.14.2.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.14.14.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.3.14.14.3.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.14.14.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T6.3.14.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.3.14.14.4.1.1" class="ltx_p" style="width:274.6pt;"><span id="A1.T6.3.14.14.4.1.1.1" class="ltx_text" style="font-size:80%;">An audio-visual dataset containing egocentric videos. Video descriptions were used as supervision signals which came from single-sentence short clip descriptions that were concatenated and refined using ChatGPT. Example prompt: “Describe the video in detail.”</span></span>
</span>
</td>
</tr>
<tr id="A1.T6.3.15.15" class="ltx_tr">
<td id="A1.T6.3.15.15.1" class="ltx_td"></td>
<td id="A1.T6.3.15.15.2" class="ltx_td"></td>
<td id="A1.T6.3.15.15.3" class="ltx_td"></td>
<td id="A1.T6.3.15.15.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T6.3.15.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.3.15.15.4.1.1" class="ltx_p" style="width:274.6pt;"><span id="A1.T6.3.15.15.4.1.1.1" class="ltx_text" style="font-size:80%;">1000 video clips from the test set were used to make multiple choice questions by prompting ChatGPT with audio-visual caption and ASR transcription.</span></span>
</span>
</td>
</tr>
<tr id="A1.T6.3.16.16" class="ltx_tr">
<td id="A1.T6.3.16.16.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="A1.T6.3.16.16.1.1" class="ltx_text" style="font-size:80%;">How2</span></td>
<td id="A1.T6.3.16.16.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A1.T6.3.16.16.2.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.16.16.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A1.T6.3.16.16.3.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T6.3.16.16.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A1.T6.3.16.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T6.3.16.16.4.1.1" class="ltx_p" style="width:274.6pt;"><span id="A1.T6.3.16.16.4.1.1.1" class="ltx_text" style="font-size:80%;">An audio-visual speech recognition dataset containing videos explaining how to perform various tasks. Example prompt: “Transcribe the speech into text, paying attention to both audio and video.”</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="A1.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 7: </span>Dataset and benchmark details part 2</figcaption>
<table id="A1.T7.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T7.3.1.1" class="ltx_tr">
<th id="A1.T7.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="A1.T7.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Data</span></th>
<th id="A1.T7.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T7.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">In Train</span></th>
<th id="A1.T7.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T7.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">In SAVE</span></th>
<th id="A1.T7.3.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A1.T7.3.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.3.1.1.4.1.1" class="ltx_p" style="width:274.6pt;"><span id="A1.T7.3.1.1.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Description</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T7.3.2.1" class="ltx_tr">
<td id="A1.T7.3.2.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A1.T7.3.2.1.1.1" class="ltx_text" style="font-size:80%;">VGGSS</span></td>
<td id="A1.T7.3.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T7.3.2.1.2.1" class="ltx_text" style="font-size:80%;">No</span></td>
<td id="A1.T7.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T7.3.2.1.3.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T7.3.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T7.3.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.3.2.1.4.1.1" class="ltx_p" style="width:274.6pt;"><span id="A1.T7.3.2.1.4.1.1.1" class="ltx_text" style="font-size:80%;">Sound source localisation data containing questions about the sound source in a 5-to-10-second video clip. Example prompt: “What is the source of the sound?”</span></span>
</span>
</td>
</tr>
<tr id="A1.T7.3.3.2" class="ltx_tr">
<td id="A1.T7.3.3.2.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="A1.T7.3.3.2.1.1" class="ltx_text" style="font-size:80%;">Presentation-QA</span></td>
<td id="A1.T7.3.3.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A1.T7.3.3.2.2.1" class="ltx_text" style="font-size:80%;">No</span></td>
<td id="A1.T7.3.3.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A1.T7.3.3.2.3.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="A1.T7.3.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A1.T7.3.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T7.3.3.2.4.1.1" class="ltx_p" style="width:274.6pt;"><span id="A1.T7.3.3.2.4.1.1.1" class="ltx_text" style="font-size:80%;">A presentation video dataset labelled with slides text and speech transcriptions. 1000 video clips from the test set were used to make multiple-choice questions by prompting ChatGPT with slide content and ASR transcription.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Examples of the AVQA Dataset</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">The English AVQA datasets include Ego4D-QA and Presentation-QA with two examples shown in Fig. <a href="#A2.F6" title="Figure 6 ‣ Appendix B Examples of the AVQA Dataset ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and <a href="#A2.F7" title="Figure 7 ‣ Appendix B Examples of the AVQA Dataset ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure id="A2.F6" class="ltx_figure"><img src="/html/2406.15704/assets/x6.png" id="A2.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Example of Presentation-QA dataset.</figcaption>
</figure>
<figure id="A2.F7" class="ltx_figure"><img src="/html/2406.15704/assets/x7.png" id="A2.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="345" height="157" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Example of Ego4D-QA dataset.</figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Evaluation Details</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">ASR and AAC are evaluated using word error rate (WER) and SPIDEr <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib28" title="" class="ltx_ref">2017</a>)</cite>, a combination of SPICE and CIDEr respectively. The evaluation of IC uses CIDEr following <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>. OCR, VQA, and Video QA are measured using top-1 accuracy. For OCR, the scoring follows <cite class="ltx_cite ltx_citemacro_citep">(Singh et al., <a href="#bib.bib44" title="" class="ltx_ref">2019</a>)</cite> where each hit in the reference answer contributes 1/3 to the total hit. For VQA and Video QA, it is counted as correct if the reference answer exactly exists in the generated answer using word-by-word matching. It is needed to check the opposite answer doesn’t exist for yes-or-no questions. In particular, during inference only, Video QA is formulated as an in-context multiple-choice task where the choices are given in the prompt, and one hit is counted only when the generated answer exactly matches the reference. The same measurement is taken for AVM. Furthermore, for AVSSD, as the reference answer is a full sentence, ChatGPT-assisted scoring is used to determine whether the generated answer is equivalent to the reference answer (see the prompt design in <a href="#A4" title="Appendix D GPT Scoring Prompt Design ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>).</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>GPT Scoring Prompt Design</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">As open-ended questions in VGGSS dataset contain full-sentence answers rather than one or two words, it is difficult to evaluate via string matching. Therefore, ChatGPT (GPT-3.5-turbo) was used to assist with the evaluation. Prompt designs for each task are described in Table <a href="#A4.T8" title="Table 8 ‣ Appendix D GPT Scoring Prompt Design ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure id="A4.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Prompt designs for ChatGPT-based evaluation. Note that <span id="A4.T8.4.1" class="ltx_text ltx_font_typewriter">QUESTION</span> refers to the question, <span id="A4.T8.5.2" class="ltx_text ltx_font_typewriter">HYPOTHESIS</span> is the model-generated answer and <span id="A4.T8.6.3" class="ltx_text ltx_font_typewriter">REFERENCE</span> is the reference answer.</figcaption>
<table id="A4.T8.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A4.T8.7.1.1" class="ltx_tr">
<th id="A4.T8.7.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Task</th>
<td id="A4.T8.7.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A4.T8.7.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.7.1.1.2.1.1" class="ltx_p" style="width:339.7pt;">Description</span>
</span>
</td>
</tr>
<tr id="A4.T8.7.2.2" class="ltx_tr">
<th id="A4.T8.7.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">VGGSS</th>
<td id="A4.T8.7.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.T8.7.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.7.2.2.2.1.1" class="ltx_p" style="width:339.7pt;">Is the sound source mentioned in answer “<span id="A4.T8.7.2.2.2.1.1.1" class="ltx_text ltx_font_typewriter">REFERENCE</span>” the same as the sound source mentioned in answer “<span id="A4.T8.7.2.2.2.1.1.2" class="ltx_text ltx_font_typewriter">HYPOTHESIS</span>”? Answer “Yes” if they are the same, and ”No” if they are different or one does not mention the sound source.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Visualisation of Diversity Loss Effect</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">The cosine similarities among output query representations of the causal Q-Former under different diversity loss factors are shown in Fig. <a href="#A5.F8" title="Figure 8 ‣ Appendix E Visualisation of Diversity Loss Effect ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure id="A5.F8" class="ltx_figure"><img src="/html/2406.15704/assets/x8.png" id="A5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="323" height="94" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Visualisation of cosine similarity matrix with different diversity loss factors.</figcaption>
</figure>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Additional Results on Lip Reading</h2>

<div id="A6.p1" class="ltx_para">
<p id="A6.p1.1" class="ltx_p">We further include the performance of video-SALMONN on the Oxford-BBC lip reading sentences 2 (LRS2) dataset. Results are shown in Table <a href="#A6.T9" title="Table 9 ‣ Appendix F Additional Results on Lip Reading ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. video-SALMONN achieved better results than the Whisper baseline by a relative 7.5% WER reduction.</p>
</div>
<figure id="A6.T9" class="ltx_table">
<table id="A6.T9.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A6.T9.1.1.1" class="ltx_tr">
<th id="A6.T9.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">System</th>
<th id="A6.T9.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">LRS2 %WER</th>
<th id="A6.T9.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">LSR2 + 0dB Gaussian noise %WER</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A6.T9.1.2.1" class="ltx_tr">
<th id="A6.T9.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Whisper large-v2</th>
<td id="A6.T9.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">5.3</td>
<td id="A6.T9.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">22.4</td>
</tr>
<tr id="A6.T9.1.3.2" class="ltx_tr">
<th id="A6.T9.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">video-SALMONN audio alone</th>
<td id="A6.T9.1.3.2.2" class="ltx_td ltx_align_center">5.1</td>
<td id="A6.T9.1.3.2.3" class="ltx_td ltx_align_center">22.4</td>
</tr>
<tr id="A6.T9.1.4.3" class="ltx_tr">
<th id="A6.T9.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">video-SALMONN audio + video</th>
<td id="A6.T9.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="A6.T9.1.4.3.2.1" class="ltx_text ltx_font_bold">4.9</span></td>
<td id="A6.T9.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="A6.T9.1.4.3.3.1" class="ltx_text ltx_font_bold">21.6</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>%WER on LRS2 lip-reading test set with clean speech, or with speech corrupted by 0dB Gaussian noise.</figcaption>
</figure>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Additional Results on MUSIC-AVQA</h2>

<div id="A7.p1" class="ltx_para">
<p id="A7.p1.1" class="ltx_p">We report our zero-shot MUSIC-AVQA results (without training on the MUSIC-AVQA dataset) in the following table, with a comparison to the AV-LLM <cite class="ltx_cite ltx_citemacro_citep">(Shu et al., <a href="#bib.bib41" title="" class="ltx_ref">2023a</a>)</cite> and Video-LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib55" title="" class="ltx_ref">2023b</a>)</cite>.</p>
</div>
<figure id="A7.T10" class="ltx_table">
<table id="A7.T10.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A7.T10.1.1.1" class="ltx_tr">
<th id="A7.T10.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">System</th>
<td id="A7.T10.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">MUSIC-AVQA Acc (%)</td>
</tr>
<tr id="A7.T10.1.2.2" class="ltx_tr">
<th id="A7.T10.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Video-LLaMA</th>
<td id="A7.T10.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">36.6%</td>
</tr>
<tr id="A7.T10.1.3.3" class="ltx_tr">
<th id="A7.T10.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">AV-LLM</th>
<td id="A7.T10.1.3.3.2" class="ltx_td ltx_align_center">45.2%</td>
</tr>
<tr id="A7.T10.1.4.4" class="ltx_tr">
<th id="A7.T10.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">video-SALMONN</th>
<td id="A7.T10.1.4.4.2" class="ltx_td ltx_align_center ltx_border_bb">
<span id="A7.T10.1.4.4.2.1" class="ltx_text ltx_font_bold">52.6</span>%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>%Acc on MUSIC-AVQA using Video-LLaMA, AV-LLM and video-SALMONN.</figcaption>
</figure>
</section>
<section id="A8" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Comparison between Vicuna and Llama-2 as Backbone LLMs</h2>

<div id="A8.p1" class="ltx_para">
<p id="A8.p1.1" class="ltx_p">We provide the additional results using Llama-2 in contrast to Vicuna-v1.5 on SAVE in Table <a href="#A8.T11" title="Table 11 ‣ Appendix H Comparison between Vicuna and Llama-2 as Backbone LLMs ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> and <a href="#A8.T12" title="Table 12 ‣ Appendix H Comparison between Vicuna and Llama-2 as Backbone LLMs ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>.</p>
</div>
<figure id="A8.T11" class="ltx_table">
<table id="A8.T11.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A8.T11.1.1.1" class="ltx_tr">
<th id="A8.T11.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">System</th>
<th id="A8.T11.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ASR</th>
<th id="A8.T11.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AC</th>
<th id="A8.T11.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Video QA</th>
<th id="A8.T11.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">IC</th>
<th id="A8.T11.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">OCR</th>
<th id="A8.T11.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">VQA</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A8.T11.1.2.1" class="ltx_tr">
<th id="A8.T11.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">video-SALMONN Vicuna-v1.5</th>
<td id="A8.T11.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">2.6%</td>
<td id="A8.T11.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">49.7%</td>
<td id="A8.T11.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">49.6%</td>
<td id="A8.T11.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">89.6%</td>
<td id="A8.T11.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">37.8%</td>
<td id="A8.T11.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">44.8%</td>
</tr>
<tr id="A8.T11.1.3.2" class="ltx_tr">
<th id="A8.T11.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">video-SALMONN Llama-2</th>
<td id="A8.T11.1.3.2.2" class="ltx_td ltx_align_center ltx_border_bb">2.6%</td>
<td id="A8.T11.1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb">50.6%</td>
<td id="A8.T11.1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb">36.7%</td>
<td id="A8.T11.1.3.2.5" class="ltx_td ltx_align_center ltx_border_bb">91.6%</td>
<td id="A8.T11.1.3.2.6" class="ltx_td ltx_align_center ltx_border_bb">33.8%</td>
<td id="A8.T11.1.3.2.7" class="ltx_td ltx_align_center ltx_border_bb">45.4%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>Audio or visual-only tasks in SAVE for comparison between Llama-2 and Vicuna-v1.5 backbone LLM.</figcaption>
</figure>
<figure id="A8.T12" class="ltx_table">
<table id="A8.T12.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A8.T12.1.1.1" class="ltx_tr">
<th id="A8.T12.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">System</th>
<th id="A8.T12.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AVSR</th>
<th id="A8.T12.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AVQA (E)</th>
<th id="A8.T12.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AVQA (P)</th>
<th id="A8.T12.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AVSSD</th>
<th id="A8.T12.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AVM</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A8.T12.1.2.1" class="ltx_tr">
<th id="A8.T12.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">video-SALMONN Vicuna-v1.5</th>
<td id="A8.T12.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">7.7%</td>
<td id="A8.T12.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">49.8%</td>
<td id="A8.T12.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">70.5%</td>
<td id="A8.T12.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">47.6%</td>
<td id="A8.T12.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">79.7%</td>
</tr>
<tr id="A8.T12.1.3.2" class="ltx_tr">
<th id="A8.T12.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">video-SALMONN Llama-2</th>
<td id="A8.T12.1.3.2.2" class="ltx_td ltx_align_center ltx_border_bb">7.8%</td>
<td id="A8.T12.1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb">40.6%</td>
<td id="A8.T12.1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb">53.5%</td>
<td id="A8.T12.1.3.2.5" class="ltx_td ltx_align_center ltx_border_bb">48.6%</td>
<td id="A8.T12.1.3.2.6" class="ltx_td ltx_align_center ltx_border_bb">79.6%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span>Audio-visual tasks in SAVE for comparison between Llama-2 and Vicuna-v1.5 backbone LLM.</figcaption>
</figure>
</section>
<section id="A9" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix I </span>Spotlight for Static Image</h2>

<div id="A9.p1" class="ltx_para">
<p id="A9.p1.1" class="ltx_p">We noticed that the performance of video-SALMONN on image tasks (e.g. VQA and OCR) may be limited by the lack of spatial resolution such that it is insufficient to extract details. To capture the finer details of an image, we make an extension to the MRC Q-Former by applying an image spotlight approach. We split the original image into a sequence of sub-images, and send the encodings of these sub-images to the MRC Q-Former in sequence. This is analogous to a video clip that scans the image patch by patch using a spotlight from the top left to the bottom right. The results of using the spotlight method (applied from the beginning of the instruction tuning) yielded better performance on OCR as shown in Table <a href="#A9.T13" title="Table 13 ‣ Appendix I Spotlight for Static Image ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
<figure id="A9.T13" class="ltx_table">
<table id="A9.T13.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A9.T13.1.1.1" class="ltx_tr">
<th id="A9.T13.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">System</th>
<th id="A9.T13.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ASR</th>
<th id="A9.T13.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AC</th>
<th id="A9.T13.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Video QA</th>
<th id="A9.T13.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">IC</th>
<th id="A9.T13.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">OCR</th>
<th id="A9.T13.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">VQA</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A9.T13.1.2.1" class="ltx_tr">
<td id="A9.T13.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">InstructBLIP</td>
<td id="A9.T13.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="A9.T13.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="A9.T13.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">21.0%</td>
<td id="A9.T13.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">84.5</td>
<td id="A9.T13.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">36.5%</td>
<td id="A9.T13.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">48.9%</td>
</tr>
<tr id="A9.T13.1.3.2" class="ltx_tr">
<td id="A9.T13.1.3.2.1" class="ltx_td ltx_align_left">video-SALMONN</td>
<td id="A9.T13.1.3.2.2" class="ltx_td ltx_align_center">2.6%</td>
<td id="A9.T13.1.3.2.3" class="ltx_td ltx_align_center">49.7%</td>
<td id="A9.T13.1.3.2.4" class="ltx_td ltx_align_center">
<span id="A9.T13.1.3.2.4.1" class="ltx_text ltx_font_bold">49.6</span>%</td>
<td id="A9.T13.1.3.2.5" class="ltx_td ltx_align_center"><span id="A9.T13.1.3.2.5.1" class="ltx_text ltx_font_bold">89.6</span></td>
<td id="A9.T13.1.3.2.6" class="ltx_td ltx_align_center">37.8%</td>
<td id="A9.T13.1.3.2.7" class="ltx_td ltx_align_center">44.8%</td>
</tr>
<tr id="A9.T13.1.4.3" class="ltx_tr">
<td id="A9.T13.1.4.3.1" class="ltx_td ltx_align_left ltx_border_bb">video-SALMONN + image spotlight</td>
<td id="A9.T13.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">
<span id="A9.T13.1.4.3.2.1" class="ltx_text ltx_font_bold">2.6</span>%</td>
<td id="A9.T13.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">
<span id="A9.T13.1.4.3.3.1" class="ltx_text ltx_font_bold">50.6</span>%</td>
<td id="A9.T13.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">49.1%</td>
<td id="A9.T13.1.4.3.5" class="ltx_td ltx_align_center ltx_border_bb">87.3</td>
<td id="A9.T13.1.4.3.6" class="ltx_td ltx_align_center ltx_border_bb">56.1%</td>
<td id="A9.T13.1.4.3.7" class="ltx_td ltx_align_center ltx_border_bb">46.2%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 13: </span>The SAVE benchmark single-modal task results using the spotlight of the static image.</figcaption>
</figure>
<div id="A9.p2" class="ltx_para">
<p id="A9.p2.1" class="ltx_p">Spotlight of the static image helped video-SALMONN to achieve much better results on OCR tasks, indicating that its performance on OCR is highly dependent on the image resolution. However, this slightly degrades the performance of video tasks. This can be due to the fact that the spotlight method has a different style of exploiting the input sequence from video frames, which slightly confuses the model.</p>
</div>
</section>
<section id="A10" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix J </span>Case Studies</h2>

<div id="A10.p1" class="ltx_para">
<p id="A10.p1.1" class="ltx_p">Six cases are illustrated in Fig. <a href="#A10.F9" title="Figure 9 ‣ Appendix J Case Studies ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> to Fig. <a href="#A10.F14" title="Figure 14 ‣ Appendix J Case Studies ‣ video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>.</p>
</div>
<figure id="A10.F9" class="ltx_figure"><img src="/html/2406.15704/assets/x9.png" id="A10.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="340" height="133" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Case study 1 – an example of the visual-spoken QA emergent ability.</figcaption>
</figure>
<figure id="A10.F10" class="ltx_figure"><img src="/html/2406.15704/assets/x10.png" id="A10.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="347" height="136" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Case study 2 – Audio-visual matching task with the request for explanation. During the benchmark test, the explanation was removed. The answer shows the understanding of both the speech and the image as well as the ability to perform reasoning based on them.</figcaption>
</figure>
<figure id="A10.F11" class="ltx_figure"><img src="/html/2406.15704/assets/x11.png" id="A10.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="320" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Case study 3 – Storytelling task with a video clip and the audio came from a different source. The answer combines the audio event, such as cheering and clapping, coherently with the video content, such as the seal.</figcaption>
</figure>
<figure id="A10.F12" class="ltx_figure"><img src="/html/2406.15704/assets/x12.png" id="A10.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="333" height="225" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Case study 4 – The famous scene in the movie <span id="A10.F12.3.1" class="ltx_text ltx_font_italic">Titanic</span> could be understood by video-SALMONN. The understanding combines the visual scene, the dialogue between characters, <span id="A10.F12.4.2" class="ltx_text ltx_font_italic">e.g.</span> “I’m flying, Jack”, as well as the background music to make the response comprehensive. It also reflects that the system knows the speaker by quoting the heroine’s speech.</figcaption>
</figure>
<figure id="A10.F13" class="ltx_figure"><img src="/html/2406.15704/assets/x13.png" id="A10.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="333" height="177" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Case study 5 – Demonstration of how speech content could provide knowledge for visual understanding. The system was clearly unable to identify the species of the shark without the help of the audio, and just made the most likely guess.</figcaption>
</figure>
<figure id="A10.F14" class="ltx_figure"><img src="/html/2406.15704/assets/x14.png" id="A10.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="320" height="217" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Case study 6 – Demonstration of understanding cartoon clips about the amusing sloth character named “Flash” in <span id="A10.F14.2.1" class="ltx_text ltx_font_italic">Zootopia</span>. video-SALMONN explained using both audio and video and accurately attributed the word “Nick” to the sloth.</figcaption>
</figure>
<figure id="A10.F15" class="ltx_figure"><img src="/html/2406.15704/assets/x15.png" id="A10.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="320" height="195" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Case study 7 – Demonstration of video-SALMONN using audio, speech and video to explain why a specific meme is interesting. The explanation includes the funny sound, the word being said with the facial expression. </figcaption>
</figure>
<figure id="A10.F16" class="ltx_figure"><img src="/html/2406.15704/assets/x16.png" id="A10.F16.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="320" height="161" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Case study 8 – Demonstration of video-SALMONN using audio, speech and video to understand the speech content about who flew to Florence. Without the video content, it is difficult to infer who we are referring to.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.15703" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.15704" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.15704">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.15704" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.15705" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 21:39:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
