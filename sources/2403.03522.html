<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.03522] Non-verbal information in spontaneous speech – towards a new framework of analysis</title><meta property="og:description" content="NNon-verbal signals in speech are encoded by prosody and carry information that ranges from conversation action to attitude and emotion. Despite its importance, the principles that govern prosodic structure are not yet…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Non-verbal information in spontaneous speech – towards a new framework of analysis">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Non-verbal information in spontaneous speech – towards a new framework of analysis">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.03522">

<!--Generated on Fri Apr  5 14:42:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\externaldocument</span>
<p id="p1.2" class="ltx_p">[supp-]supplementary</p>
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p">[1]<span id="p2.1.1" class="ltx_ERROR undefined">\fnm</span>Tirza <span id="p2.1.2" class="ltx_ERROR undefined">\sur</span>Biron</p>
</div>
<div id="p3" class="ltx_para">
<p id="p3.1" class="ltx_p">[1]<span id="p3.1.1" class="ltx_ERROR undefined">\orgdiv</span>Faculty of Mathematics
and Computer Science, <span id="p3.1.2" class="ltx_ERROR undefined">\orgname</span>Weizmann Institute of Science</p>
</div>
<h1 class="ltx_title ltx_title_document">Non-verbal information in spontaneous speech – towards a new framework of analysis</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:tirza.biron@weizmann.ac.il">tirza.biron@weizmann.ac.il</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id1.1.id1" class="ltx_ERROR undefined">\fnm</span>Moshe <span id="id2.2.id2" class="ltx_ERROR undefined">\sur</span>Barboy
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id3.1.id1" class="ltx_ERROR undefined">\fnm</span>Eran <span id="id4.2.id2" class="ltx_ERROR undefined">\sur</span>Ben Artzy
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id5.1.id1" class="ltx_ERROR undefined">\fnm</span>Alona <span id="id6.2.id2" class="ltx_ERROR undefined">\sur</span>Golubchik
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id7.1.id1" class="ltx_ERROR undefined">\fnm</span>Yanir <span id="id8.2.id2" class="ltx_ERROR undefined">\sur</span>Marmor
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id9.1.id1" class="ltx_ERROR undefined">\fnm</span>Smadar <span id="id10.2.id2" class="ltx_ERROR undefined">\sur</span>Szekely
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id11.1.id1" class="ltx_ERROR undefined">\fnm</span>Yaron <span id="id12.2.id2" class="ltx_ERROR undefined">\sur</span>Winter
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id13.1.id1" class="ltx_ERROR undefined">\fnm</span>David <span id="id14.2.id2" class="ltx_ERROR undefined">\sur</span>Harel
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">*
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id15.id1" class="ltx_p">NNon-verbal signals in speech are encoded by prosody and carry information that ranges from conversation action to attitude and emotion. Despite its importance, the principles that govern prosodic structure are not yet adequately understood.</p>
<p id="id16.id2" class="ltx_p">This paper offers an analytical schema and a technological proof-of-concept for the categorization of prosodic signals and their association with meaning. The schema interprets surface-representations of multi-layered prosodic events.</p>
<p id="id17.id3" class="ltx_p">As a first step towards implementation, we present a classification process that disentangles prosodic phenomena of three orders. It relies on fine-tuning a pre-trained speech recognition model, enabling the simultaneous multi-class/multi-label detection. It generalizes over a large variety of spontaneous data, performing on a par with, or superior to, human annotation.</p>
<p id="id18.id4" class="ltx_p">In addition to a standardized formalization of prosody, disentangling prosodic patterns can direct a theory of communication and speech organization. A welcome by-product is an interpretation of prosody that will enhance speech- and language-related technologies.</p>
</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Keywords</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Context formalization,
prosody,
multi-layered information,
computational linguistics,
NLP</p>
</div>
</section>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>A New Schema for Prosody Analysis</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Non-verbal linguistic signals that are encoded by prosody and carry crucial information in speech. Prosodic messages range from conversation action (e.g., request, command) and discourse function (e.g., narration, parentheticals), to saliency of information (de/emphasis), attitude (e.g., sarcasm), and uninhibited emotion.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">Written language registers some of the prosody’s many functions: punctuation denotes segmentation, certain speech-act types, and a few discourse functions. One also encounters the occasional orthographic <span id="S1.SS1.p2.1.1" class="ltx_text ltx_font_italic">emphasis</span> or ’misgivings’.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">Despite its importance, the principles that govern prosodic structuring remain, by and large, unformulated; prosodic variability is a persistent source of debate (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>). This appears to be due to a basic characteristic of prosodic signals – their simultaneity: speakers combine several messages at a time. Consider a potential breakdown for a surprised question <a target="_blank" href="https://a7ce520753ab849816faaa3f4fc591b1.cdn.bubble.io/f1703510258291x582006937189132500/really_surprise.wav" title="" class="ltx_ref ltx_href">“Really?”</a> vs. its sarcastic counterpart <a target="_blank" href="https://a7ce520753ab849816faaa3f4fc591b1.cdn.bubble.io/f1703510282575x126057188639876260/really_sarcasm.wav" title="" class="ltx_ref ltx_href">“Really?…”</a>. The latter exhibits at least two orders of non-verbal information: a rhetorical question and a mocking attitude. An analysis of prosodic structure must therefore account for its multidimensional nature. Recent developments in pattern recognition present a unique opportunity for use in such a context.</p>
</div>
<div id="S1.SS1.p4" class="ltx_para">
<p id="S1.SS1.p4.1" class="ltx_p">This article offers an analytical framework and a technological proof-of-concept for the categorization of prosodic signals and their association with meaning. At the core of our proposal is a schema that interprets the surface-representation of multi-layered prosodic events (cf. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>). As a first step toward implementation, we present a prediction/classification process for the disentanglement of prosodic patterns that relies on a transformer-based architecture.</p>
</div>
<div id="S1.SS1.p5" class="ltx_para">
<p id="S1.SS1.p5.1" class="ltx_p">The primary objective of our experiment is to assess if and to what extent a model may simultaneously learn several prosodic messages of different non-verbal orders. The proposed method, then, enables the simultaneous training, followed by a one-pass multi-labeling. It generalizes well over a large variety of speakers, for several types of data, tagged by different annotators, and performs at 0.91/0.97 (Cohen’s Kappa/accuracy) for intonation unit (IU) detection, 0.55/0.81 for emphasis detection, and 0.45/0.70 for prosodic prototype detection (see section <a href="#S3" title="3 The schema ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> below).</p>
</div>
<div id="S1.SS1.p6" class="ltx_para">
<p id="S1.SS1.p6.1" class="ltx_p">In addition to a standardized, careful explication of prosody, disentangling prosodic patterns can shed light on the organization of speech and expand theories of communication. It can enhance the pairing of prosodic form and function, help articulate the constraints that affect prosodic patterning (cf. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>), and minimize the disparities in their acoustic description.</p>
</div>
<div id="S1.SS1.p7" class="ltx_para">
<p id="S1.SS1.p7.1" class="ltx_p">Furthermore, since prosody reflects much of the communicational context, a reliable analysis would be a gateway to an improved formalization of context. As a welcome by-product, speech technologies will be able to output exhaustive meaning, adding non-verbal conditioning to the recognised words. Speech analytics, natural language understanding, and speech synthesis are all expected to benefit from an accessible deciphering of prosody.</p>
</div>
<div id="S1.SS1.p8" class="ltx_para">
<p id="S1.SS1.p8.1" class="ltx_p">An additional contribution of our work involves simple means for adding prosodic labels to an aligned transcription. The transfer learning process presented here alters the model’s output labels to include new ones in the original, decoded series of tokens. The method may be applied to a variety of different domains. Lastly, we demonstrate the ability of re-training the STT WHISPER model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> for prosodic disentanglement.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Linguistic Framework</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">Aiming at a broad approach to the analysis of prosody, two hypotheses underlie our proposal:</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">The germane unit and arena of prosodic events is the intonation unit (abbreviated IU; <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>), also termed “Tone Group” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, “intermediate intonational phrase” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, “prosodic intermediate phrases” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, “turn construction unit” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, or “minimal discourse unit” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>; and cf. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">IUs exhibit semantically meaningful, sometimes grammaticalized, prosodic patterns <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
</li>
</ol>
</div>
<div id="S1.SS2.p3" class="ltx_para">
<p id="S1.SS2.p3.1" class="ltx_p">Our schema maintains that all prosodic phenomena may be analyzed as variations, either hierarchical or orthogonal, of a very small number of IU prototypes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. The variations include four basic layers: information structure, attitude, emotion, and 3-5 sub-categories of conversation action and/or discourse functions (see Figure <a href="#S1.F1" title="Figure 1 ‣ 1.2 Linguistic Framework ‣ 1 Introduction ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2403.03522/assets/figures/1_new.jpeg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="432" height="438" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">An illustration of the analytical hierarchy for IUs<span id="S1.F1.4.2.1" class="ltx_text ltx_font_upright">. Note that emotion, emphasis and attitude are orthogonal to the prosodic prototype and discourse function hierarchy.</span></span></figcaption>
</figure>
<div id="S1.SS2.p4" class="ltx_para">
<p id="S1.SS2.p4.1" class="ltx_p">The common, unmarked prototypes (see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> for markedness) are analyzed as modulated into stacked variants (cf. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>). The resulting signal is realized as an integration of the above layers with additional constraints, such as syllable structure and unit length (cf. e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>).</p>
</div>
<div id="S1.SS2.p5" class="ltx_para">
<p id="S1.SS2.p5.1" class="ltx_p">To illustrate the principle of multi-layering, consider Figure <a href="#S1.F2.sf3" title="Figure 2(c) ‣ Figure 2 ‣ 1.2 Linguistic Framework ‣ 1 Introduction ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(c)</span></a>, which shows emphasis production for the prosodic prototype “comma”/“continuation”. Note the difference in pitch maxima at the beginning vs. the ending of the IU, echoing its latent pitch template (cf. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>).</p>
</div>
<figure id="S1.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.03522/assets/figures/2a.png" id="S1.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="414" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.03522/assets/figures/2b.png" id="S1.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="414" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.03522/assets/figures/2c.png" id="S1.F2.sf3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="414" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.03522/assets/figures/2d.png" id="S1.F2.sf4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="414" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F2.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.03522/assets/figures/2e.png" id="S1.F2.sf5.g1" class="ltx_graphics ltx_img_landscape" width="598" height="414" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.sf5.2.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S1.F2.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.03522/assets/figures/2f.png" id="S1.F2.sf6.g1" class="ltx_graphics ltx_img_landscape" width="598" height="414" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.sf6.2.1.1" class="ltx_text" style="font-size:90%;">(f)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.10.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.11.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Log pitch course, median-normalized and time-normalized, of manually (a.,c.,e.) and automatically (b., d., f.) annotated IUs, “This American Life” corpus<span id="S1.F2.11.2.1" class="ltx_text ltx_font_upright"> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. <span id="S1.F2.11.2.1.1" class="ltx_text ltx_font_bold">2(a-b)</span> Log pitch course of the prototypes “continuation” (“comma”; <span id="S1.F2.11.2.1.2" class="ltx_text ltx_font_bold">2a</span> n=3,184; <span id="S1.F2.11.2.1.3" class="ltx_text ltx_font_bold">2b</span> n=34,455) and “conclusion” (“period”; <span id="S1.F2.11.2.1.4" class="ltx_text ltx_font_bold">2a</span> n=2,323; <span id="S1.F2.11.2.1.5" class="ltx_text ltx_font_bold">2b</span> n=27,415) for manual and automatic annotation, respectively. <span id="S1.F2.11.2.1.6" class="ltx_text ltx_font_bold">2(c-d)</span> Log pitch course of “continuation” IUs that bear emphasis in their first half (blue), second half (orange), and all “continuation” IUs (green), for manual and automatic annotation, respectively. <span id="S1.F2.11.2.1.7" class="ltx_text ltx_font_bold">2(e-f)</span> Log pitch course of “continuation” for IUs that bear emphasis in their first half (blue) or their second half (orange), for manual and automatic annotation, respectively. Note the influence of the underlying “comma” pitch pattern on the production of emphasis, and the resemblance between manually annotated and automatically obtained IUs.
</span></span></figcaption>
</figure>
<div id="S1.SS2.p6" class="ltx_para">
<p id="S1.SS2.p6.1" class="ltx_p">This view of prosodic template-variation is inspired by Semitic word-formation (see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and the <span id="S1.SS2.p6.1.1" class="ltx_text ltx_font_italic">supplementary material</span>). Non-concatenative morphology - that is, composites of morphemes of different orders - makes a useful metaphor for a layered, integrated patterning. When applied to prosody, this organizing principle enables a substantial reduction in complexity: from seemingly infinite variation to a hierarchical system. It thus facilitates the distinction between different non-verbal messages, readily accounting for the simultaneity of prosodic events.</p>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Related Work</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.1" class="ltx_p">In the interest of smooth reading, and since the article touches upon a number of fields, a more detailed description of related work has been relegated to section <span id="S1.SS3.p1.1.1" class="ltx_text ltx_font_italic">2</span> in the <span id="S1.SS3.p1.1.2" class="ltx_text ltx_font_italic">supplementary material</span>. Here we provide a broad description only.</p>
</div>
<div id="S1.SS3.p2" class="ltx_para">
<p id="S1.SS3.p2.1" class="ltx_p">For overviews of the prevalent linguistic approaches to the study of prosody, see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. As pointed out by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, a predictive, general framework for associating prosodic form and function is yet to be put forward.</p>
</div>
<div id="S1.SS3.p3" class="ltx_para">
<p id="S1.SS3.p3.1" class="ltx_p">In the domain of computational prosody, improving speech synthesis has been a subject of significant research (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>). As for automated analyses of prosody, most have been aimed at detecting single phenomena, such as unit boundaries (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>), prominence/saliency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, or specific dialogue-acts (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>). Crucially, none of the above tackle the multi-layered nature of many prosodic events.</p>
</div>
<div id="S1.SS3.p4" class="ltx_para">
<p id="S1.SS3.p4.1" class="ltx_p">A method for fine-tuning WHISPER to predict IU boundaries is described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. Our proposal is similar, in that it enriches a transcription with prosodic tags. However, to the best of our knowledge, the multi-class/multi-label transfer learning that we employ has not been used for prosody analysis.</p>
</div>
<div id="S1.SS3.p5" class="ltx_para">
<p id="S1.SS3.p5.1" class="ltx_p">Machine learning methods have been used for semantic disentanglement in a large variety of domains, mainly in image processing (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>). As far as we are aware, the disentanglement of non-verbal prosodic layers has not been the focus of such efforts.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Motivation: The Challenge of Context Formalization</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Semiotic studies define context as that which accords meaning to a sign (cf. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>). Verbal contextuality is traditionally viewed as the relationship – and indeed the contrast – between a sign and its fellow signs, with which it can be either joined or replaced <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. Yet, despite its obvious contribution to contextual meaning, non-verbal information is rarely considered in descriptions of phonetics, phonology, and morpho-syntax. In response, Austin <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> stresses that “what we have to study is not the sentence but the issuing of an utterance in a speech situation” (p. 138). Consider example no. <a href="#S2.E1" title="Equation 1 ‣ 2 Motivation: The Challenge of Context Formalization ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>:</p>
</div>
<div id="S2.p2" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><span id="S2.E1.1" class="ltx_text ltx_markedasmath">“There is a bull in the field.” (ibid., p.32)</span></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="0" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">This statement is ordinarily either a description or a warning, the distinction relying on the speaker’s identity and motivation. To the discerning ear, prosody reflects, remarkably and accurately, such speech situations and their contextual meaning: the performance of a speech act, or imparting feelings, conveying epistemological information and other speaker intentions.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Language technologies have been wrestling with contextualization for several decades. Early mathematical representations of linguistic entities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> instituted syntactic analysis as the base for natural language processing (NLP) (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>), treating words as discrete, atomic units. With the introduction of robust word conversions, words and phrases were represented as continuous vectors (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>), relying on an element’s immediate environment (often referred to as “context” in related domains as well (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>). Continuous vectors that represent less immediate neighbors (n-grams) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> were later fed into convolutional neural networks (CNNs) and long-short term memory networks (LSTMs). LSTMs have been using the reciprocal “attention” of words in a text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>; that is, an output that is affected by each element/word in the input series, by considering both their relative and absolute positions. LSTMs eventually culminated in the large, flexible models that stem from transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Those excel at modeling contextual information through statistical learning, and have recently been augmented with visual and audio data, embedded in their input <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">However, unformalised contextual information results in obvious weaknesses of linguistic accounts, whether heuristic or statistical. A more formal solution would require a systematic inclusion of non-verbal conditioning to verbal output. Apparently, a simple rule (paraphrasing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>) would suffice: “A feature F is contextual for an action (or meaning) A if F constrains A, and may affect the outcome of A, but is not a constituent of A”. The prosodic output for example no. <a href="#S2.E1" title="Equation 1 ‣ 2 Motivation: The Challenge of Context Formalization ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> would therefore be either “There is a bull in the field (warning, urgent)” or “There is a bull in the field (description, narrative, neutral)”, or, for that matter, any combination of speech act, intention and attitude/emotion with which the text was produced.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Contextuality and Scope</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Contextuality typically relies on scope. Relationships between linguistic signs, either when joined to- or when replaced with one another, vary according to the size of the unit at hand. These range from a short retort (“Yes!”) to entire genres.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The effect of the wider context on meaning includes multi-unit prosodic patterns. Simple examples are appositions, list patterns <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, and prosodic bi-partites (such as if-then constructions; see samples <a target="_blank" href="https://paper-10.bubbleapps.io/version-test/paper_1_1_audio" title="" class="ltx_ref ltx_href">here</a>). Paragraphs, narratives, and formal addresses encompass a larger scale, which often presents nested structures (e.g., a list of events within a narrative). Those form the syntax of prosodic patterns (cf. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>).</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Analyses of larger-than-sentence entities are central to the domains of text linguistics (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>), discourse analysis (DA), and conversation analysis (CA) (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>), all of which provide valuable tools for our work.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The schema</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The schema we propose here posits that communicative intentions, encoded by prosody, may be ordered and tracked hierarchically. Surface-representations of patterns within IUs may be interpreted through a layered classification procedure. Thus, the overwhelming diversity of prosody, often referred to as its ‘elusive’ nature (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>), may be broken down beneficially.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Similarly to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, our proposal concurs with the idea of stacking, and follows the functional-contrastive approach, whereby a sign-function draws its systemic value from the contrast with other sign-functions. Conversely, our schema stipulates a different discrete unit and hence a different scope of patterning, as well as a different view on stacking (cf. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>).</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">We offer a framework that will eventually become predictive, in that it will describe the underlying structures and constraints that form a consequent pattern. We propose between 3 and 8 (but no more) classes of variation on 3 to 5 basic labels (see Figure <a href="#S3.F3" title="Figure 3 ‣ 3 The schema ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2403.03522/assets/figures/3.jpg" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="268" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">An example of the categories and labels that constitute the prosodic messages in <a target="_blank" href="https://a7ce520753ab849816faaa3f4fc591b1.cdn.bubble.io/f1701163514358x930943014691363400/01You_have_no_guide_when_you're_a_young_person_(rethorical_questions_list).wav" title="" class="ltx_ref ltx_href">audio</a>.<span id="S3.F3.4.2.1" class="ltx_text ltx_font_upright"> Excerpt drawn from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>.</span></span></figcaption>
</figure>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Once identified, IUs are classified into the following categories:</p>
</div>
<div id="S3.p5" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Para-syntactic modality, termed here prosodic prototype</span> –
The prototypes that we identify are: (,) “continuation”; (.) “conclusion”; and (?) “request for response” (cf. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>).</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Discourse function and/or conversation action</span> –
These patterns signal the organization of discourse. It is a category that covers a wide scope, from syntax to rhetoric, and its labels include, for example, “circumstantial unit”, “title of discourse”, “background of narrative”, “narrative event” and so on (see Table <span id="S3.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">SM3</span> in the <span id="S3.I1.i2.p1.1.3" class="ltx_text ltx_font_italic">supplementary material</span>).
The sub-category of conversation action refers to speech acts that are designed to affect the interlocutor’s behavior; for example, questions that serve as requests, warnings, commands, etc.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Information structure</span> –
The prosodic signaling of saliency of information (de/emphasis).</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">Express sentiment/attitude</span> – Irony, feigned anger and calculated indifference are examples of overt attitudes.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p"><span id="S3.I1.i5.p1.1.1" class="ltx_text ltx_font_italic">Unintentional/unplanned emotion</span> – This category includes, for example, delight, disgust, reserve, fear, pain and other emotions and feelings that can change one’s prosody (see overview in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>).</p>
</div>
</li>
</ol>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">Our theory of patterning posits a prosodic prototype (category (1)) that is interpreted within a set of pre-established alterations (categories (2)-(5)). In other words, the global signal can lead the listener to infer the speaker’s intentions based on their prior knowledge of the prototypical template and its available variations. The text in example no. <a href="#S3.E2" title="Equation 2 ‣ 3 The schema ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, below, should be read as a disapproving rhetorical question with an emphasis on the last word:</p>
</div>
<div id="S3.p7" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><span id="S3.E2.1" class="ltx_text ltx_markedasmath">“You want to go <span id="S3.E2.1.1" class="ltx_text ltx_font_italic">home</span>?!”</span></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="0" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p8" class="ltx_para">
<p id="S3.p8.1" class="ltx_p">When the underlying patterns of an IU are identified, other prosodic messages may be disentangled. Thus, the question pattern in example no. <a href="#S3.E2" title="Equation 2 ‣ 3 The schema ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, can be extricated for differential flagging and distinguished from the pattern of disapproval and the emphasis on “home”.</p>
</div>
<div id="S3.p9" class="ltx_para">
<p id="S3.p9.1" class="ltx_p">The resulting classification outlines an inventory of variations that are projected, or ’grafted’, onto a prototype-pattern (see Tables <span id="S3.p9.1.1" class="ltx_text ltx_font_italic">SM1</span>, <span id="S3.p9.1.2" class="ltx_text ltx_font_italic">SM2</span> and <span id="S3.p9.1.3" class="ltx_text ltx_font_italic">SM3</span> in the <span id="S3.p9.1.4" class="ltx_text ltx_font_italic">supplementary material</span>). As stated above, in a technological context, the disentanglement enables an enhanced detection of prosodic semantics.
For a detailed presentation of the schema, see the <span id="S3.p9.1.5" class="ltx_text ltx_font_italic">supplementary material</span>.</p>
</div>
<div id="S3.p10" class="ltx_para">
<p id="S3.p10.1" class="ltx_p">Some prosodic layers are more subtly marked, while others are more clear cut (e.g., discourse function vs. information structure). Still, when analyzing speech, our description strives to be as detailed as possible, in as much as the details may be perceived. An advanced prototype-classification tree would define what constitutes a distinctive feature for prosodic patterning on the scale of IUs.</p>
</div>
<div id="S3.p11" class="ltx_para">
<p id="S3.p11.1" class="ltx_p">In the following sections, we report upon the methods for, and results of, a successful disentanglement procedure of three prosodic categories, as detected simultaneously through fine-tuning the WHISPER speech language model.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methods</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets and data preparation</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The problem of multi-layered prosodic classification has received little attention in the ML community. Moreover, existing datasets and benchmarks do not match our analytical framework. Therefore, a substantial part of our work is dedicated to creating designated datasets.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Our principal set is drawn from the “This American Life” podcast (abbreviated TAL, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>). As an auxiliary set, we compiled a collection of 24 interviews, each recording less than 30 seconds long, totalling 7 minutes of tagged speech. Among the speakers are Oprah, Will Smith, Frances Arnold and Connan O’Brian (<a target="_blank" href="https://paper-10.bubbleapps.io/version-test/paper_1_1_interviews" title="" class="ltx_ref ltx_href">interviews dataset</a>). This set differs from TAL, in that it contains spontaneous speech only, with no narrated parts. Created for validation purposes, it was annotated by a different expert and was not represented in the training set. Both sets have partially timestamped transcriptions.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Manual annotation</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.2" class="ltx_p">A primary automatic segmentation was carried out using the TAL transcript: word sequences between punctuation marks were regarded as IU-proxies, and a preliminary classification into prosodic prototypes was done using that same punctuation: (,) (“continuation”), (.) (“conclusion”), or (?) (“request for response”). Of those, 80<math id="S4.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS1.SSS1.p1.1.m1.1a"><mo id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.1c">\%</annotation></semantics></math> of <math id="S4.SS1.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="S4.SS1.SSS1.p1.2.m2.1a"><mo id="S4.SS1.SSS1.p1.2.m2.1.1" xref="S4.SS1.SSS1.p1.2.m2.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.2.m2.1b"><leq id="S4.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p1.2.m2.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.2.m2.1c">\leq</annotation></semantics></math> 7-word units were found to correspond to IUs, and were therefore included as a suggestion for manual tagging – their labels to be confirmed or corrected.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.1" class="ltx_p">The annotation was added manually, per word, using INCEpTION <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>. For the experiments presented here, word labels included IU boundary information, IU prototype, and a class of saliency (primary or secondary emphasis, and de-emphasis). The result of the annotation process is a table of time-aligned, tagged words. See Table <a href="#S4.T1" title="Table 1 ‣ 4.1.1 Manual annotation ‣ 4.1 Datasets and data preparation ‣ 4 Methods ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for the statistics of the annotation.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_align_left ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.2.1.1" class="ltx_tr">
<th id="S4.T1.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.2.1.1.1.1" class="ltx_text ltx_font_bold">(a) Main speaker vs. interviewees</span></th>
<th id="S4.T1.2.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t"></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.2.1" class="ltx_tr">
<th id="S4.T1.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Speaker</th>
<td id="S4.T1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Number (Fraction)</td>
</tr>
<tr id="S4.T1.2.3.2" class="ltx_tr">
<th id="S4.T1.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Narrator</th>
<td id="S4.T1.2.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1,385 (23.33%)</td>
</tr>
<tr id="S4.T1.2.4.3" class="ltx_tr">
<th id="S4.T1.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Interviewee</th>
<td id="S4.T1.2.4.3.2" class="ltx_td ltx_align_center ltx_border_r">4,551 (76.67%)</td>
</tr>
<tr id="S4.T1.2.5.4" class="ltx_tr">
<th id="S4.T1.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Total</th>
<td id="S4.T1.2.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5,936</td>
</tr>
<tr id="S4.T1.2.6.5" class="ltx_tr">
<th id="S4.T1.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt"><span id="S4.T1.2.6.5.1.1" class="ltx_text ltx_font_bold">(b) Prosodic prototypes</span></th>
<td id="S4.T1.2.6.5.2" class="ltx_td ltx_border_r ltx_border_tt"></td>
</tr>
<tr id="S4.T1.2.7.6" class="ltx_tr">
<th id="S4.T1.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Prototype</th>
<td id="S4.T1.2.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Number (Fraction)</td>
</tr>
<tr id="S4.T1.2.8.7" class="ltx_tr">
<th id="S4.T1.2.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Continuation (comma)</th>
<td id="S4.T1.2.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3,246 (54.99%)</td>
</tr>
<tr id="S4.T1.2.9.8" class="ltx_tr">
<th id="S4.T1.2.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Conclusion (period)</th>
<td id="S4.T1.2.9.8.2" class="ltx_td ltx_align_center ltx_border_r">2,362 (39.79%)</td>
</tr>
<tr id="S4.T1.2.10.9" class="ltx_tr">
<th id="S4.T1.2.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Request for response (question mark)</th>
<td id="S4.T1.2.10.9.2" class="ltx_td ltx_align_center ltx_border_r">310 (5.22%)</td>
</tr>
<tr id="S4.T1.2.11.10" class="ltx_tr">
<th id="S4.T1.2.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Total</th>
<td id="S4.T1.2.11.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5,936</td>
</tr>
<tr id="S4.T1.2.12.11" class="ltx_tr">
<th id="S4.T1.2.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt"><span id="S4.T1.2.12.11.1.1" class="ltx_text ltx_font_bold">(c) Emphasis tags</span></th>
<td id="S4.T1.2.12.11.2" class="ltx_td ltx_border_r ltx_border_tt"></td>
</tr>
<tr id="S4.T1.2.13.12" class="ltx_tr">
<th id="S4.T1.2.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Emphasis</th>
<td id="S4.T1.2.13.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Number (Fraction)</td>
</tr>
<tr id="S4.T1.2.14.13" class="ltx_tr">
<th id="S4.T1.2.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Primary</th>
<td id="S4.T1.2.14.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5,320 (26.34%)</td>
</tr>
<tr id="S4.T1.2.15.14" class="ltx_tr">
<th id="S4.T1.2.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Secondary</th>
<td id="S4.T1.2.15.14.2" class="ltx_td ltx_align_center ltx_border_r">2,726 (12.99%)</td>
</tr>
<tr id="S4.T1.2.16.15" class="ltx_tr">
<th id="S4.T1.2.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Non-emphasized words</th>
<td id="S4.T1.2.16.15.2" class="ltx_td ltx_align_center ltx_border_r">12,946 (61.67%)</td>
</tr>
<tr id="S4.T1.2.17.16" class="ltx_tr">
<th id="S4.T1.2.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Total</th>
<td id="S4.T1.2.17.16.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">20,992</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_align_left"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.4.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.5.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The annotated data<span id="S4.T1.5.2.1" class="ltx_text ltx_font_upright">. 1a. Number and fraction of main speaker data vs. interviewees (n=82); 1b. Number and fraction of prosodic prototypes; 1c. Number and fraction of emphasis tags (= the number of words annotated).</span></span></figcaption>
</figure>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Preprocessing for labeling and training</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">TAL transcripts were normalized as follows:</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p">The text was converted into lower case; abbreviations (e.g., Dr., Ms.) and transcribed digits were replaced by their long forms using <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>; for the purposes of our analysis, transcribed (–) was replaced by (,), and (!) by (.).</p>
</div>
<div id="S4.SS1.SSS2.p3" class="ltx_para">
<p id="S4.SS1.SSS2.p3.1" class="ltx_p">To remove background music, the audio was processed using SPLEETER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>. The transcription of TAL and Interviews were force-aligned using the Montreal Forced Aligner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, in order to produce timestamps for each word and phone.</p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Turn compilation</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">In conversation analysis (CA), continued speech by a single speaker is termed a “turn” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>. Turns are typically constructed of at least one IU, and may extend to entire communications. In our experiments, however, “turn” is the audio unit that is input to the model for analysis.</p>
</div>
<div id="S4.SS1.SSS3.p2" class="ltx_para">
<p id="S4.SS1.SSS3.p2.1" class="ltx_p">Our considerations for obtaining optimal turns in this context included:</p>
</div>
<div id="S4.SS1.SSS3.p3" class="ltx_para">
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Turns should contain at least two IUs by the same speaker, so that the model may learn IU switches;</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Turns should not contain long pauses, both for efficiency of computation and in order to avoid IU switches that are too obvious;</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Multiple speakers in a turn may be beneficial, as they better reflect real-life speech situations;</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Turns should not exceed 30 seconds or 448 tokens, as per the WHISPER constraints.</p>
</div>
</li>
</ol>
</div>
<div id="S4.SS1.SSS3.p4" class="ltx_para">
<p id="S4.SS1.SSS3.p4.3" class="ltx_p">Preliminary tests for optimizing turn generation considered three parameters: avoid/use multiple speakers; determine maximal speech pause; and determine the minimal number of IUs. These considerations led to a dataset in which most “natural” turns measure less than 10 sec.; 88<math id="S4.SS1.SSS3.p4.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS1.SSS3.p4.1.m1.1a"><mo id="S4.SS1.SSS3.p4.1.m1.1.1" xref="S4.SS1.SSS3.p4.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p4.1.m1.1b"><csymbol cd="latexml" id="S4.SS1.SSS3.p4.1.m1.1.1.cmml" xref="S4.SS1.SSS3.p4.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p4.1.m1.1c">\%</annotation></semantics></math> feature one speaker, 11.5<math id="S4.SS1.SSS3.p4.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS1.SSS3.p4.2.m2.1a"><mo id="S4.SS1.SSS3.p4.2.m2.1.1" xref="S4.SS1.SSS3.p4.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p4.2.m2.1b"><csymbol cd="latexml" id="S4.SS1.SSS3.p4.2.m2.1.1.cmml" xref="S4.SS1.SSS3.p4.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p4.2.m2.1c">\%</annotation></semantics></math> feature two, and 0.5<math id="S4.SS1.SSS3.p4.3.m3.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS1.SSS3.p4.3.m3.1a"><mo id="S4.SS1.SSS3.p4.3.m3.1.1" xref="S4.SS1.SSS3.p4.3.m3.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p4.3.m3.1b"><csymbol cd="latexml" id="S4.SS1.SSS3.p4.3.m3.1.1.cmml" xref="S4.SS1.SSS3.p4.3.m3.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p4.3.m3.1c">\%</annotation></semantics></math> three speakers.</p>
</div>
<div id="S4.SS1.SSS3.p5" class="ltx_para">
<p id="S4.SS1.SSS3.p5.1" class="ltx_p">In order to determine the best turn-compilation strategy, we chose the WHISPER-Small model, one of six sub-models published along with the WHISPER paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. This choice stemmed from its performance, which is close to the best obtained result (see section <span id="S4.SS1.SSS3.p5.1.1" class="ltx_text ltx_font_italic">1</span> in the <span id="S4.SS1.SSS3.p5.1.2" class="ltx_text ltx_font_italic">supplementary material</span>; For further details, see Figure <a href="#S5.F5" title="Figure 5 ‣ 5 Results ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and Table <a href="#S5.T2" title="Table 2 ‣ 5 Results ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experiment objectives and setup</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">As mentioned above, the primary objective of our experiment was to assess if and to what extent a model may simultaneously learn several prosodic messages of different non-verbal orders. Another objective was to predict these labels simultaneously. To this end, we applied transfer learning and fine-tuning to the WHISPER model, the backbone of our experiments (Figure <a href="#S4.F4" title="Figure 4 ‣ 4.2 Experiment objectives and setup ‣ 4 Methods ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2403.03522/assets/figures/5.jpg" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="393" height="471" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Training scheme<span id="S4.F4.4.2.1" class="ltx_text ltx_font_upright">. The backbone of our method is the fine-tuned WHISPER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Its input includes speech audio, its corresponding text and prosodic labels; output predicts label combinations for each word of the input text.</span></span></figcaption>
</figure>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Training</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.2" class="ltx_p">We used the HuggingFace WHISPER implementation to fine-tune the various models. The default optimization procedure is described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, and the learning rate was fixed at <math id="S4.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="10^{-5}" display="inline"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><msup id="S4.SS2.SSS1.p1.1.m1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.cmml"><mn id="S4.SS2.SSS1.p1.1.m1.1.1.2" xref="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml">10</mn><mrow id="S4.SS2.SSS1.p1.1.m1.1.1.3" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.cmml"><mo id="S4.SS2.SSS1.p1.1.m1.1.1.3a" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.cmml">−</mo><mn id="S4.SS2.SSS1.p1.1.m1.1.1.3.2" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.1b"><apply id="S4.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.2">10</cn><apply id="S4.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.3"><minus id="S4.SS2.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.3"></minus><cn type="integer" id="S4.SS2.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.1c">10^{-5}</annotation></semantics></math>. We applied an early stop mechanism, using 5<math id="S4.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS2.SSS1.p1.2.m2.1a"><mo id="S4.SS2.SSS1.p1.2.m2.1.1" xref="S4.SS2.SSS1.p1.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.2.m2.1b"><csymbol cd="latexml" id="S4.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.2.m2.1c">\%</annotation></semantics></math> of the training set for evaluation, which induced 5-15 epochs of training. For efficiency, turns were sorted by length (i.e., the number of words), and the generated batches included 256 tokens, inducing mini-batch sizes of 1-20.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">Each turn of speech was treated as a single instance, the training input consisting of its audio and transcription. The transcription was enriched with prosodic labels per word. Those were inserted alternately, as single strings, with text-words preceded by their prosodic label-combination (see Figure <a href="#S4.F4" title="Figure 4 ‣ 4.2 Experiment objectives and setup ‣ 4 Methods ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). As far as we are aware, this method of multi-class/multi-label transfer learning has not yet been used for prosody analysis (cf. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>).</p>
</div>
<div id="S4.SS2.SSS1.p3" class="ltx_para">
<p id="S4.SS2.SSS1.p3.1" class="ltx_p">In addition to training for the triple, simultaneous recognition of prosodic events, we trained for three distinct recognition tasks of the same events. This required replacing the complex labels (that represent a combination of phenomena) by simplex labels (that denote just one).</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Prediction</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">The WHISPER base building block is a transformer, whose input is a spectrogram and a sequence of tokens that represent the audio and the text, respectively. The transformer then generates predictions for the next token to be concatenated to the sequence. Our challenge was to predict correct prosodic labels only, excluding textual ones. The prediction proceeded as described in Algorithm <a href="#alg1" title="Algorithm 1 ‣ 4.2.2 Prediction ‣ 4.2 Experiment objectives and setup ‣ 4 Methods ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.6.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> <span id="alg1.7.2" class="ltx_text ltx_font_italic">Pseudo code of the inference procedure</span>. This method enables prosodic labeling only. Note that <span id="alg1.8.3" class="ltx_text ltx_markedasmath ltx_font_italic">next_label</span> holds the predicted label of a multiclass-multilabel combination</figcaption>
<div id="alg1.9" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l1.1.1.1" class="ltx_text" style="font-size:80%;">1:</span></span>Model: the re-trained model (based on WHISPER), which consists of an audio encoder and a text decoder.

</div>
<div id="alg1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l2.1.1.1" class="ltx_text" style="font-size:80%;">2:</span></span>Tokenizer: converts text into the model’s known tokens.

</div>
<div id="alg1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l3.1.1.1" class="ltx_text" style="font-size:80%;">3:</span></span>Audio_spectrogram: audio in the format suited for the model’s input (of the currently handled turn).

</div>
<div id="alg1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l4.1.1.1" class="ltx_text" style="font-size:80%;">4:</span></span>Word_list: the words in the transcription, sorted by order of utterance.

</div>
<div id="alg1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l5.1.1.1" class="ltx_text" style="font-size:80%;">5:</span></span>Label_list: the tags corresponding to the word list and aligned with them.

</div>
<div id="alg1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l6.1.1.1" class="ltx_text" style="font-size:80%;">6:</span></span><math id="alg1.l6.m1.1" class="ltx_Math" alttext="label\_list\leftarrow\text{empty list}" display="inline"><semantics id="alg1.l6.m1.1a"><mrow id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml"><mrow id="alg1.l6.m1.1.1.2" xref="alg1.l6.m1.1.1.2.cmml"><mi id="alg1.l6.m1.1.1.2.2" xref="alg1.l6.m1.1.1.2.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.2.1" xref="alg1.l6.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l6.m1.1.1.2.3" xref="alg1.l6.m1.1.1.2.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.2.1a" xref="alg1.l6.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l6.m1.1.1.2.4" xref="alg1.l6.m1.1.1.2.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.2.1b" xref="alg1.l6.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l6.m1.1.1.2.5" xref="alg1.l6.m1.1.1.2.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.2.1c" xref="alg1.l6.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l6.m1.1.1.2.6" xref="alg1.l6.m1.1.1.2.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.2.1d" xref="alg1.l6.m1.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="alg1.l6.m1.1.1.2.7" xref="alg1.l6.m1.1.1.2.7.cmml">_</mi><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.2.1e" xref="alg1.l6.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l6.m1.1.1.2.8" xref="alg1.l6.m1.1.1.2.8.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.2.1f" xref="alg1.l6.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l6.m1.1.1.2.9" xref="alg1.l6.m1.1.1.2.9.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.2.1g" xref="alg1.l6.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l6.m1.1.1.2.10" xref="alg1.l6.m1.1.1.2.10.cmml">s</mi><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.2.1h" xref="alg1.l6.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l6.m1.1.1.2.11" xref="alg1.l6.m1.1.1.2.11.cmml">t</mi></mrow><mo stretchy="false" id="alg1.l6.m1.1.1.1" xref="alg1.l6.m1.1.1.1.cmml">←</mo><mtext id="alg1.l6.m1.1.1.3" xref="alg1.l6.m1.1.1.3a.cmml">empty list</mtext></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><apply id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1"><ci id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1">←</ci><apply id="alg1.l6.m1.1.1.2.cmml" xref="alg1.l6.m1.1.1.2"><times id="alg1.l6.m1.1.1.2.1.cmml" xref="alg1.l6.m1.1.1.2.1"></times><ci id="alg1.l6.m1.1.1.2.2.cmml" xref="alg1.l6.m1.1.1.2.2">𝑙</ci><ci id="alg1.l6.m1.1.1.2.3.cmml" xref="alg1.l6.m1.1.1.2.3">𝑎</ci><ci id="alg1.l6.m1.1.1.2.4.cmml" xref="alg1.l6.m1.1.1.2.4">𝑏</ci><ci id="alg1.l6.m1.1.1.2.5.cmml" xref="alg1.l6.m1.1.1.2.5">𝑒</ci><ci id="alg1.l6.m1.1.1.2.6.cmml" xref="alg1.l6.m1.1.1.2.6">𝑙</ci><ci id="alg1.l6.m1.1.1.2.7.cmml" xref="alg1.l6.m1.1.1.2.7">_</ci><ci id="alg1.l6.m1.1.1.2.8.cmml" xref="alg1.l6.m1.1.1.2.8">𝑙</ci><ci id="alg1.l6.m1.1.1.2.9.cmml" xref="alg1.l6.m1.1.1.2.9">𝑖</ci><ci id="alg1.l6.m1.1.1.2.10.cmml" xref="alg1.l6.m1.1.1.2.10">𝑠</ci><ci id="alg1.l6.m1.1.1.2.11.cmml" xref="alg1.l6.m1.1.1.2.11">𝑡</ci></apply><ci id="alg1.l6.m1.1.1.3a.cmml" xref="alg1.l6.m1.1.1.3"><mtext id="alg1.l6.m1.1.1.3.cmml" xref="alg1.l6.m1.1.1.3">empty list</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">label\_list\leftarrow\text{empty list}</annotation></semantics></math>

</div>
<div id="alg1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l7.1.1.1" class="ltx_text" style="font-size:80%;">7:</span></span><math id="alg1.l7.m1.1" class="ltx_Math" alttext="token\_list\leftarrow\text{model's starting tokens}" display="inline"><semantics id="alg1.l7.m1.1a"><mrow id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml"><mrow id="alg1.l7.m1.1.1.2" xref="alg1.l7.m1.1.1.2.cmml"><mi id="alg1.l7.m1.1.1.2.2" xref="alg1.l7.m1.1.1.2.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l7.m1.1.1.2.1" xref="alg1.l7.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l7.m1.1.1.2.3" xref="alg1.l7.m1.1.1.2.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="alg1.l7.m1.1.1.2.1a" xref="alg1.l7.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l7.m1.1.1.2.4" xref="alg1.l7.m1.1.1.2.4.cmml">k</mi><mo lspace="0em" rspace="0em" id="alg1.l7.m1.1.1.2.1b" xref="alg1.l7.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l7.m1.1.1.2.5" xref="alg1.l7.m1.1.1.2.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l7.m1.1.1.2.1c" xref="alg1.l7.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l7.m1.1.1.2.6" xref="alg1.l7.m1.1.1.2.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="alg1.l7.m1.1.1.2.1d" xref="alg1.l7.m1.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="alg1.l7.m1.1.1.2.7" xref="alg1.l7.m1.1.1.2.7.cmml">_</mi><mo lspace="0em" rspace="0em" id="alg1.l7.m1.1.1.2.1e" xref="alg1.l7.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l7.m1.1.1.2.8" xref="alg1.l7.m1.1.1.2.8.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l7.m1.1.1.2.1f" xref="alg1.l7.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l7.m1.1.1.2.9" xref="alg1.l7.m1.1.1.2.9.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l7.m1.1.1.2.1g" xref="alg1.l7.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l7.m1.1.1.2.10" xref="alg1.l7.m1.1.1.2.10.cmml">s</mi><mo lspace="0em" rspace="0em" id="alg1.l7.m1.1.1.2.1h" xref="alg1.l7.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l7.m1.1.1.2.11" xref="alg1.l7.m1.1.1.2.11.cmml">t</mi></mrow><mo stretchy="false" id="alg1.l7.m1.1.1.1" xref="alg1.l7.m1.1.1.1.cmml">←</mo><mtext id="alg1.l7.m1.1.1.3" xref="alg1.l7.m1.1.1.3a.cmml">model’s starting tokens</mtext></mrow><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><apply id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1"><ci id="alg1.l7.m1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1">←</ci><apply id="alg1.l7.m1.1.1.2.cmml" xref="alg1.l7.m1.1.1.2"><times id="alg1.l7.m1.1.1.2.1.cmml" xref="alg1.l7.m1.1.1.2.1"></times><ci id="alg1.l7.m1.1.1.2.2.cmml" xref="alg1.l7.m1.1.1.2.2">𝑡</ci><ci id="alg1.l7.m1.1.1.2.3.cmml" xref="alg1.l7.m1.1.1.2.3">𝑜</ci><ci id="alg1.l7.m1.1.1.2.4.cmml" xref="alg1.l7.m1.1.1.2.4">𝑘</ci><ci id="alg1.l7.m1.1.1.2.5.cmml" xref="alg1.l7.m1.1.1.2.5">𝑒</ci><ci id="alg1.l7.m1.1.1.2.6.cmml" xref="alg1.l7.m1.1.1.2.6">𝑛</ci><ci id="alg1.l7.m1.1.1.2.7.cmml" xref="alg1.l7.m1.1.1.2.7">_</ci><ci id="alg1.l7.m1.1.1.2.8.cmml" xref="alg1.l7.m1.1.1.2.8">𝑙</ci><ci id="alg1.l7.m1.1.1.2.9.cmml" xref="alg1.l7.m1.1.1.2.9">𝑖</ci><ci id="alg1.l7.m1.1.1.2.10.cmml" xref="alg1.l7.m1.1.1.2.10">𝑠</ci><ci id="alg1.l7.m1.1.1.2.11.cmml" xref="alg1.l7.m1.1.1.2.11">𝑡</ci></apply><ci id="alg1.l7.m1.1.1.3a.cmml" xref="alg1.l7.m1.1.1.3"><mtext id="alg1.l7.m1.1.1.3.cmml" xref="alg1.l7.m1.1.1.3">model’s starting tokens</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">token\_list\leftarrow\text{model's starting tokens}</annotation></semantics></math>

</div>
<div id="alg1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l8.1.1.1" class="ltx_text" style="font-size:80%;">8:</span></span><math id="alg1.l8.m1.1" class="ltx_Math" alttext="audio\_features\leftarrow\text{model.audio\_encoder(audio\_spectrogram)}" display="inline"><semantics id="alg1.l8.m1.1a"><mrow id="alg1.l8.m1.1.1" xref="alg1.l8.m1.1.1.cmml"><mrow id="alg1.l8.m1.1.1.2" xref="alg1.l8.m1.1.1.2.cmml"><mi id="alg1.l8.m1.1.1.2.2" xref="alg1.l8.m1.1.1.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="alg1.l8.m1.1.1.2.1" xref="alg1.l8.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l8.m1.1.1.2.3" xref="alg1.l8.m1.1.1.2.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="alg1.l8.m1.1.1.2.1a" xref="alg1.l8.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l8.m1.1.1.2.4" xref="alg1.l8.m1.1.1.2.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="alg1.l8.m1.1.1.2.1b" xref="alg1.l8.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l8.m1.1.1.2.5" xref="alg1.l8.m1.1.1.2.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l8.m1.1.1.2.1c" xref="alg1.l8.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l8.m1.1.1.2.6" xref="alg1.l8.m1.1.1.2.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="alg1.l8.m1.1.1.2.1d" xref="alg1.l8.m1.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="alg1.l8.m1.1.1.2.7" xref="alg1.l8.m1.1.1.2.7.cmml">_</mi><mo lspace="0em" rspace="0em" id="alg1.l8.m1.1.1.2.1e" xref="alg1.l8.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l8.m1.1.1.2.8" xref="alg1.l8.m1.1.1.2.8.cmml">f</mi><mo lspace="0em" rspace="0em" id="alg1.l8.m1.1.1.2.1f" xref="alg1.l8.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l8.m1.1.1.2.9" xref="alg1.l8.m1.1.1.2.9.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l8.m1.1.1.2.1g" xref="alg1.l8.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l8.m1.1.1.2.10" xref="alg1.l8.m1.1.1.2.10.cmml">a</mi><mo lspace="0em" rspace="0em" id="alg1.l8.m1.1.1.2.1h" xref="alg1.l8.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l8.m1.1.1.2.11" xref="alg1.l8.m1.1.1.2.11.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l8.m1.1.1.2.1i" xref="alg1.l8.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l8.m1.1.1.2.12" xref="alg1.l8.m1.1.1.2.12.cmml">u</mi><mo lspace="0em" rspace="0em" id="alg1.l8.m1.1.1.2.1j" xref="alg1.l8.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l8.m1.1.1.2.13" xref="alg1.l8.m1.1.1.2.13.cmml">r</mi><mo lspace="0em" rspace="0em" id="alg1.l8.m1.1.1.2.1k" xref="alg1.l8.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l8.m1.1.1.2.14" xref="alg1.l8.m1.1.1.2.14.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l8.m1.1.1.2.1l" xref="alg1.l8.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l8.m1.1.1.2.15" xref="alg1.l8.m1.1.1.2.15.cmml">s</mi></mrow><mo stretchy="false" id="alg1.l8.m1.1.1.1" xref="alg1.l8.m1.1.1.1.cmml">←</mo><mtext id="alg1.l8.m1.1.1.3" xref="alg1.l8.m1.1.1.3a.cmml">model.audio_encoder(audio_spectrogram)</mtext></mrow><annotation-xml encoding="MathML-Content" id="alg1.l8.m1.1b"><apply id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1"><ci id="alg1.l8.m1.1.1.1.cmml" xref="alg1.l8.m1.1.1.1">←</ci><apply id="alg1.l8.m1.1.1.2.cmml" xref="alg1.l8.m1.1.1.2"><times id="alg1.l8.m1.1.1.2.1.cmml" xref="alg1.l8.m1.1.1.2.1"></times><ci id="alg1.l8.m1.1.1.2.2.cmml" xref="alg1.l8.m1.1.1.2.2">𝑎</ci><ci id="alg1.l8.m1.1.1.2.3.cmml" xref="alg1.l8.m1.1.1.2.3">𝑢</ci><ci id="alg1.l8.m1.1.1.2.4.cmml" xref="alg1.l8.m1.1.1.2.4">𝑑</ci><ci id="alg1.l8.m1.1.1.2.5.cmml" xref="alg1.l8.m1.1.1.2.5">𝑖</ci><ci id="alg1.l8.m1.1.1.2.6.cmml" xref="alg1.l8.m1.1.1.2.6">𝑜</ci><ci id="alg1.l8.m1.1.1.2.7.cmml" xref="alg1.l8.m1.1.1.2.7">_</ci><ci id="alg1.l8.m1.1.1.2.8.cmml" xref="alg1.l8.m1.1.1.2.8">𝑓</ci><ci id="alg1.l8.m1.1.1.2.9.cmml" xref="alg1.l8.m1.1.1.2.9">𝑒</ci><ci id="alg1.l8.m1.1.1.2.10.cmml" xref="alg1.l8.m1.1.1.2.10">𝑎</ci><ci id="alg1.l8.m1.1.1.2.11.cmml" xref="alg1.l8.m1.1.1.2.11">𝑡</ci><ci id="alg1.l8.m1.1.1.2.12.cmml" xref="alg1.l8.m1.1.1.2.12">𝑢</ci><ci id="alg1.l8.m1.1.1.2.13.cmml" xref="alg1.l8.m1.1.1.2.13">𝑟</ci><ci id="alg1.l8.m1.1.1.2.14.cmml" xref="alg1.l8.m1.1.1.2.14">𝑒</ci><ci id="alg1.l8.m1.1.1.2.15.cmml" xref="alg1.l8.m1.1.1.2.15">𝑠</ci></apply><ci id="alg1.l8.m1.1.1.3a.cmml" xref="alg1.l8.m1.1.1.3"><mtext id="alg1.l8.m1.1.1.3.cmml" xref="alg1.l8.m1.1.1.3">model.audio_encoder(audio_spectrogram)</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m1.1c">audio\_features\leftarrow\text{model.audio\_encoder(audio\_spectrogram)}</annotation></semantics></math>

</div>
<div id="alg1.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l9.1.1.1" class="ltx_text" style="font-size:80%;">9:</span></span><span id="alg1.l9.2" class="ltx_text ltx_font_bold">for</span> <math id="alg1.l9.m1.1" class="ltx_Math" alttext="word" display="inline"><semantics id="alg1.l9.m1.1a"><mrow id="alg1.l9.m1.1.1" xref="alg1.l9.m1.1.1.cmml"><mi id="alg1.l9.m1.1.1.2" xref="alg1.l9.m1.1.1.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="alg1.l9.m1.1.1.1" xref="alg1.l9.m1.1.1.1.cmml">​</mo><mi id="alg1.l9.m1.1.1.3" xref="alg1.l9.m1.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="alg1.l9.m1.1.1.1a" xref="alg1.l9.m1.1.1.1.cmml">​</mo><mi id="alg1.l9.m1.1.1.4" xref="alg1.l9.m1.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="alg1.l9.m1.1.1.1b" xref="alg1.l9.m1.1.1.1.cmml">​</mo><mi id="alg1.l9.m1.1.1.5" xref="alg1.l9.m1.1.1.5.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.1b"><apply id="alg1.l9.m1.1.1.cmml" xref="alg1.l9.m1.1.1"><times id="alg1.l9.m1.1.1.1.cmml" xref="alg1.l9.m1.1.1.1"></times><ci id="alg1.l9.m1.1.1.2.cmml" xref="alg1.l9.m1.1.1.2">𝑤</ci><ci id="alg1.l9.m1.1.1.3.cmml" xref="alg1.l9.m1.1.1.3">𝑜</ci><ci id="alg1.l9.m1.1.1.4.cmml" xref="alg1.l9.m1.1.1.4">𝑟</ci><ci id="alg1.l9.m1.1.1.5.cmml" xref="alg1.l9.m1.1.1.5">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.1c">word</annotation></semantics></math> in <math id="alg1.l9.m2.1" class="ltx_Math" alttext="word\_list" display="inline"><semantics id="alg1.l9.m2.1a"><mrow id="alg1.l9.m2.1.1" xref="alg1.l9.m2.1.1.cmml"><mi id="alg1.l9.m2.1.1.2" xref="alg1.l9.m2.1.1.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="alg1.l9.m2.1.1.1" xref="alg1.l9.m2.1.1.1.cmml">​</mo><mi id="alg1.l9.m2.1.1.3" xref="alg1.l9.m2.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="alg1.l9.m2.1.1.1a" xref="alg1.l9.m2.1.1.1.cmml">​</mo><mi id="alg1.l9.m2.1.1.4" xref="alg1.l9.m2.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="alg1.l9.m2.1.1.1b" xref="alg1.l9.m2.1.1.1.cmml">​</mo><mi id="alg1.l9.m2.1.1.5" xref="alg1.l9.m2.1.1.5.cmml">d</mi><mo lspace="0em" rspace="0em" id="alg1.l9.m2.1.1.1c" xref="alg1.l9.m2.1.1.1.cmml">​</mo><mi mathvariant="normal" id="alg1.l9.m2.1.1.6" xref="alg1.l9.m2.1.1.6.cmml">_</mi><mo lspace="0em" rspace="0em" id="alg1.l9.m2.1.1.1d" xref="alg1.l9.m2.1.1.1.cmml">​</mo><mi id="alg1.l9.m2.1.1.7" xref="alg1.l9.m2.1.1.7.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l9.m2.1.1.1e" xref="alg1.l9.m2.1.1.1.cmml">​</mo><mi id="alg1.l9.m2.1.1.8" xref="alg1.l9.m2.1.1.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l9.m2.1.1.1f" xref="alg1.l9.m2.1.1.1.cmml">​</mo><mi id="alg1.l9.m2.1.1.9" xref="alg1.l9.m2.1.1.9.cmml">s</mi><mo lspace="0em" rspace="0em" id="alg1.l9.m2.1.1.1g" xref="alg1.l9.m2.1.1.1.cmml">​</mo><mi id="alg1.l9.m2.1.1.10" xref="alg1.l9.m2.1.1.10.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l9.m2.1b"><apply id="alg1.l9.m2.1.1.cmml" xref="alg1.l9.m2.1.1"><times id="alg1.l9.m2.1.1.1.cmml" xref="alg1.l9.m2.1.1.1"></times><ci id="alg1.l9.m2.1.1.2.cmml" xref="alg1.l9.m2.1.1.2">𝑤</ci><ci id="alg1.l9.m2.1.1.3.cmml" xref="alg1.l9.m2.1.1.3">𝑜</ci><ci id="alg1.l9.m2.1.1.4.cmml" xref="alg1.l9.m2.1.1.4">𝑟</ci><ci id="alg1.l9.m2.1.1.5.cmml" xref="alg1.l9.m2.1.1.5">𝑑</ci><ci id="alg1.l9.m2.1.1.6.cmml" xref="alg1.l9.m2.1.1.6">_</ci><ci id="alg1.l9.m2.1.1.7.cmml" xref="alg1.l9.m2.1.1.7">𝑙</ci><ci id="alg1.l9.m2.1.1.8.cmml" xref="alg1.l9.m2.1.1.8">𝑖</ci><ci id="alg1.l9.m2.1.1.9.cmml" xref="alg1.l9.m2.1.1.9">𝑠</ci><ci id="alg1.l9.m2.1.1.10.cmml" xref="alg1.l9.m2.1.1.10">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m2.1c">word\_list</annotation></semantics></math> <span id="alg1.l9.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l10.1.1.1" class="ltx_text" style="font-size:80%;">10:</span></span>     <math id="alg1.l10.m1.2" class="ltx_Math" alttext="label\_logits\leftarrow\text{model.text\_decoder}(token\_list,audio\_features)" display="inline"><semantics id="alg1.l10.m1.2a"><mrow id="alg1.l10.m1.2.2" xref="alg1.l10.m1.2.2.cmml"><mrow id="alg1.l10.m1.2.2.4" xref="alg1.l10.m1.2.2.4.cmml"><mi id="alg1.l10.m1.2.2.4.2" xref="alg1.l10.m1.2.2.4.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.4.1" xref="alg1.l10.m1.2.2.4.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.4.3" xref="alg1.l10.m1.2.2.4.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.4.1a" xref="alg1.l10.m1.2.2.4.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.4.4" xref="alg1.l10.m1.2.2.4.4.cmml">b</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.4.1b" xref="alg1.l10.m1.2.2.4.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.4.5" xref="alg1.l10.m1.2.2.4.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.4.1c" xref="alg1.l10.m1.2.2.4.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.4.6" xref="alg1.l10.m1.2.2.4.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.4.1d" xref="alg1.l10.m1.2.2.4.1.cmml">​</mo><mi mathvariant="normal" id="alg1.l10.m1.2.2.4.7" xref="alg1.l10.m1.2.2.4.7.cmml">_</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.4.1e" xref="alg1.l10.m1.2.2.4.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.4.8" xref="alg1.l10.m1.2.2.4.8.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.4.1f" xref="alg1.l10.m1.2.2.4.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.4.9" xref="alg1.l10.m1.2.2.4.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.4.1g" xref="alg1.l10.m1.2.2.4.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.4.10" xref="alg1.l10.m1.2.2.4.10.cmml">g</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.4.1h" xref="alg1.l10.m1.2.2.4.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.4.11" xref="alg1.l10.m1.2.2.4.11.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.4.1i" xref="alg1.l10.m1.2.2.4.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.4.12" xref="alg1.l10.m1.2.2.4.12.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.4.1j" xref="alg1.l10.m1.2.2.4.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.4.13" xref="alg1.l10.m1.2.2.4.13.cmml">s</mi></mrow><mo stretchy="false" id="alg1.l10.m1.2.2.3" xref="alg1.l10.m1.2.2.3.cmml">←</mo><mrow id="alg1.l10.m1.2.2.2" xref="alg1.l10.m1.2.2.2.cmml"><mtext id="alg1.l10.m1.2.2.2.4" xref="alg1.l10.m1.2.2.2.4a.cmml">model.text_decoder</mtext><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.2.3" xref="alg1.l10.m1.2.2.2.3.cmml">​</mo><mrow id="alg1.l10.m1.2.2.2.2.2" xref="alg1.l10.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="alg1.l10.m1.2.2.2.2.2.3" xref="alg1.l10.m1.2.2.2.2.3.cmml">(</mo><mrow id="alg1.l10.m1.1.1.1.1.1.1" xref="alg1.l10.m1.1.1.1.1.1.1.cmml"><mi id="alg1.l10.m1.1.1.1.1.1.1.2" xref="alg1.l10.m1.1.1.1.1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.1.1.1.1.1.1.1" xref="alg1.l10.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="alg1.l10.m1.1.1.1.1.1.1.3" xref="alg1.l10.m1.1.1.1.1.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.1.1.1.1.1.1.1a" xref="alg1.l10.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="alg1.l10.m1.1.1.1.1.1.1.4" xref="alg1.l10.m1.1.1.1.1.1.1.4.cmml">k</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.1.1.1.1.1.1.1b" xref="alg1.l10.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="alg1.l10.m1.1.1.1.1.1.1.5" xref="alg1.l10.m1.1.1.1.1.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.1.1.1.1.1.1.1c" xref="alg1.l10.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="alg1.l10.m1.1.1.1.1.1.1.6" xref="alg1.l10.m1.1.1.1.1.1.1.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.1.1.1.1.1.1.1d" xref="alg1.l10.m1.1.1.1.1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="alg1.l10.m1.1.1.1.1.1.1.7" xref="alg1.l10.m1.1.1.1.1.1.1.7.cmml">_</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.1.1.1.1.1.1.1e" xref="alg1.l10.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="alg1.l10.m1.1.1.1.1.1.1.8" xref="alg1.l10.m1.1.1.1.1.1.1.8.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.1.1.1.1.1.1.1f" xref="alg1.l10.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="alg1.l10.m1.1.1.1.1.1.1.9" xref="alg1.l10.m1.1.1.1.1.1.1.9.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.1.1.1.1.1.1.1g" xref="alg1.l10.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="alg1.l10.m1.1.1.1.1.1.1.10" xref="alg1.l10.m1.1.1.1.1.1.1.10.cmml">s</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.1.1.1.1.1.1.1h" xref="alg1.l10.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="alg1.l10.m1.1.1.1.1.1.1.11" xref="alg1.l10.m1.1.1.1.1.1.1.11.cmml">t</mi></mrow><mo id="alg1.l10.m1.2.2.2.2.2.4" xref="alg1.l10.m1.2.2.2.2.3.cmml">,</mo><mrow id="alg1.l10.m1.2.2.2.2.2.2" xref="alg1.l10.m1.2.2.2.2.2.2.cmml"><mi id="alg1.l10.m1.2.2.2.2.2.2.2" xref="alg1.l10.m1.2.2.2.2.2.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.2.2.2.2.1" xref="alg1.l10.m1.2.2.2.2.2.2.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.2.2.2.2.3" xref="alg1.l10.m1.2.2.2.2.2.2.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.2.2.2.2.1a" xref="alg1.l10.m1.2.2.2.2.2.2.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.2.2.2.2.4" xref="alg1.l10.m1.2.2.2.2.2.2.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.2.2.2.2.1b" xref="alg1.l10.m1.2.2.2.2.2.2.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.2.2.2.2.5" xref="alg1.l10.m1.2.2.2.2.2.2.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.2.2.2.2.1c" xref="alg1.l10.m1.2.2.2.2.2.2.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.2.2.2.2.6" xref="alg1.l10.m1.2.2.2.2.2.2.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.2.2.2.2.1d" xref="alg1.l10.m1.2.2.2.2.2.2.1.cmml">​</mo><mi mathvariant="normal" id="alg1.l10.m1.2.2.2.2.2.2.7" xref="alg1.l10.m1.2.2.2.2.2.2.7.cmml">_</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.2.2.2.2.1e" xref="alg1.l10.m1.2.2.2.2.2.2.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.2.2.2.2.8" xref="alg1.l10.m1.2.2.2.2.2.2.8.cmml">f</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.2.2.2.2.1f" xref="alg1.l10.m1.2.2.2.2.2.2.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.2.2.2.2.9" xref="alg1.l10.m1.2.2.2.2.2.2.9.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.2.2.2.2.1g" xref="alg1.l10.m1.2.2.2.2.2.2.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.2.2.2.2.10" xref="alg1.l10.m1.2.2.2.2.2.2.10.cmml">a</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.2.2.2.2.1h" xref="alg1.l10.m1.2.2.2.2.2.2.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.2.2.2.2.11" xref="alg1.l10.m1.2.2.2.2.2.2.11.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.2.2.2.2.1i" xref="alg1.l10.m1.2.2.2.2.2.2.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.2.2.2.2.12" xref="alg1.l10.m1.2.2.2.2.2.2.12.cmml">u</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.2.2.2.2.1j" xref="alg1.l10.m1.2.2.2.2.2.2.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.2.2.2.2.13" xref="alg1.l10.m1.2.2.2.2.2.2.13.cmml">r</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.2.2.2.2.1k" xref="alg1.l10.m1.2.2.2.2.2.2.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.2.2.2.2.14" xref="alg1.l10.m1.2.2.2.2.2.2.14.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.2.2.2.2.2.1l" xref="alg1.l10.m1.2.2.2.2.2.2.1.cmml">​</mo><mi id="alg1.l10.m1.2.2.2.2.2.2.15" xref="alg1.l10.m1.2.2.2.2.2.2.15.cmml">s</mi></mrow><mo stretchy="false" id="alg1.l10.m1.2.2.2.2.2.5" xref="alg1.l10.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l10.m1.2b"><apply id="alg1.l10.m1.2.2.cmml" xref="alg1.l10.m1.2.2"><ci id="alg1.l10.m1.2.2.3.cmml" xref="alg1.l10.m1.2.2.3">←</ci><apply id="alg1.l10.m1.2.2.4.cmml" xref="alg1.l10.m1.2.2.4"><times id="alg1.l10.m1.2.2.4.1.cmml" xref="alg1.l10.m1.2.2.4.1"></times><ci id="alg1.l10.m1.2.2.4.2.cmml" xref="alg1.l10.m1.2.2.4.2">𝑙</ci><ci id="alg1.l10.m1.2.2.4.3.cmml" xref="alg1.l10.m1.2.2.4.3">𝑎</ci><ci id="alg1.l10.m1.2.2.4.4.cmml" xref="alg1.l10.m1.2.2.4.4">𝑏</ci><ci id="alg1.l10.m1.2.2.4.5.cmml" xref="alg1.l10.m1.2.2.4.5">𝑒</ci><ci id="alg1.l10.m1.2.2.4.6.cmml" xref="alg1.l10.m1.2.2.4.6">𝑙</ci><ci id="alg1.l10.m1.2.2.4.7.cmml" xref="alg1.l10.m1.2.2.4.7">_</ci><ci id="alg1.l10.m1.2.2.4.8.cmml" xref="alg1.l10.m1.2.2.4.8">𝑙</ci><ci id="alg1.l10.m1.2.2.4.9.cmml" xref="alg1.l10.m1.2.2.4.9">𝑜</ci><ci id="alg1.l10.m1.2.2.4.10.cmml" xref="alg1.l10.m1.2.2.4.10">𝑔</ci><ci id="alg1.l10.m1.2.2.4.11.cmml" xref="alg1.l10.m1.2.2.4.11">𝑖</ci><ci id="alg1.l10.m1.2.2.4.12.cmml" xref="alg1.l10.m1.2.2.4.12">𝑡</ci><ci id="alg1.l10.m1.2.2.4.13.cmml" xref="alg1.l10.m1.2.2.4.13">𝑠</ci></apply><apply id="alg1.l10.m1.2.2.2.cmml" xref="alg1.l10.m1.2.2.2"><times id="alg1.l10.m1.2.2.2.3.cmml" xref="alg1.l10.m1.2.2.2.3"></times><ci id="alg1.l10.m1.2.2.2.4a.cmml" xref="alg1.l10.m1.2.2.2.4"><mtext id="alg1.l10.m1.2.2.2.4.cmml" xref="alg1.l10.m1.2.2.2.4">model.text_decoder</mtext></ci><interval closure="open" id="alg1.l10.m1.2.2.2.2.3.cmml" xref="alg1.l10.m1.2.2.2.2.2"><apply id="alg1.l10.m1.1.1.1.1.1.1.cmml" xref="alg1.l10.m1.1.1.1.1.1.1"><times id="alg1.l10.m1.1.1.1.1.1.1.1.cmml" xref="alg1.l10.m1.1.1.1.1.1.1.1"></times><ci id="alg1.l10.m1.1.1.1.1.1.1.2.cmml" xref="alg1.l10.m1.1.1.1.1.1.1.2">𝑡</ci><ci id="alg1.l10.m1.1.1.1.1.1.1.3.cmml" xref="alg1.l10.m1.1.1.1.1.1.1.3">𝑜</ci><ci id="alg1.l10.m1.1.1.1.1.1.1.4.cmml" xref="alg1.l10.m1.1.1.1.1.1.1.4">𝑘</ci><ci id="alg1.l10.m1.1.1.1.1.1.1.5.cmml" xref="alg1.l10.m1.1.1.1.1.1.1.5">𝑒</ci><ci id="alg1.l10.m1.1.1.1.1.1.1.6.cmml" xref="alg1.l10.m1.1.1.1.1.1.1.6">𝑛</ci><ci id="alg1.l10.m1.1.1.1.1.1.1.7.cmml" xref="alg1.l10.m1.1.1.1.1.1.1.7">_</ci><ci id="alg1.l10.m1.1.1.1.1.1.1.8.cmml" xref="alg1.l10.m1.1.1.1.1.1.1.8">𝑙</ci><ci id="alg1.l10.m1.1.1.1.1.1.1.9.cmml" xref="alg1.l10.m1.1.1.1.1.1.1.9">𝑖</ci><ci id="alg1.l10.m1.1.1.1.1.1.1.10.cmml" xref="alg1.l10.m1.1.1.1.1.1.1.10">𝑠</ci><ci id="alg1.l10.m1.1.1.1.1.1.1.11.cmml" xref="alg1.l10.m1.1.1.1.1.1.1.11">𝑡</ci></apply><apply id="alg1.l10.m1.2.2.2.2.2.2.cmml" xref="alg1.l10.m1.2.2.2.2.2.2"><times id="alg1.l10.m1.2.2.2.2.2.2.1.cmml" xref="alg1.l10.m1.2.2.2.2.2.2.1"></times><ci id="alg1.l10.m1.2.2.2.2.2.2.2.cmml" xref="alg1.l10.m1.2.2.2.2.2.2.2">𝑎</ci><ci id="alg1.l10.m1.2.2.2.2.2.2.3.cmml" xref="alg1.l10.m1.2.2.2.2.2.2.3">𝑢</ci><ci id="alg1.l10.m1.2.2.2.2.2.2.4.cmml" xref="alg1.l10.m1.2.2.2.2.2.2.4">𝑑</ci><ci id="alg1.l10.m1.2.2.2.2.2.2.5.cmml" xref="alg1.l10.m1.2.2.2.2.2.2.5">𝑖</ci><ci id="alg1.l10.m1.2.2.2.2.2.2.6.cmml" xref="alg1.l10.m1.2.2.2.2.2.2.6">𝑜</ci><ci id="alg1.l10.m1.2.2.2.2.2.2.7.cmml" xref="alg1.l10.m1.2.2.2.2.2.2.7">_</ci><ci id="alg1.l10.m1.2.2.2.2.2.2.8.cmml" xref="alg1.l10.m1.2.2.2.2.2.2.8">𝑓</ci><ci id="alg1.l10.m1.2.2.2.2.2.2.9.cmml" xref="alg1.l10.m1.2.2.2.2.2.2.9">𝑒</ci><ci id="alg1.l10.m1.2.2.2.2.2.2.10.cmml" xref="alg1.l10.m1.2.2.2.2.2.2.10">𝑎</ci><ci id="alg1.l10.m1.2.2.2.2.2.2.11.cmml" xref="alg1.l10.m1.2.2.2.2.2.2.11">𝑡</ci><ci id="alg1.l10.m1.2.2.2.2.2.2.12.cmml" xref="alg1.l10.m1.2.2.2.2.2.2.12">𝑢</ci><ci id="alg1.l10.m1.2.2.2.2.2.2.13.cmml" xref="alg1.l10.m1.2.2.2.2.2.2.13">𝑟</ci><ci id="alg1.l10.m1.2.2.2.2.2.2.14.cmml" xref="alg1.l10.m1.2.2.2.2.2.2.14">𝑒</ci><ci id="alg1.l10.m1.2.2.2.2.2.2.15.cmml" xref="alg1.l10.m1.2.2.2.2.2.2.15">𝑠</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m1.2c">label\_logits\leftarrow\text{model.text\_decoder}(token\_list,audio\_features)</annotation></semantics></math>

</div>
<div id="alg1.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l11.1.1.1" class="ltx_text" style="font-size:80%;">11:</span></span>     <math id="alg1.l11.m1.1" class="ltx_Math" alttext="next\_label\leftarrow\text{label with highest probability in }label\_logits" display="inline"><semantics id="alg1.l11.m1.1a"><mrow id="alg1.l11.m1.1.1" xref="alg1.l11.m1.1.1.cmml"><mrow id="alg1.l11.m1.1.1.2" xref="alg1.l11.m1.1.1.2.cmml"><mi id="alg1.l11.m1.1.1.2.2" xref="alg1.l11.m1.1.1.2.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.2.1" xref="alg1.l11.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l11.m1.1.1.2.3" xref="alg1.l11.m1.1.1.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.2.1a" xref="alg1.l11.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l11.m1.1.1.2.4" xref="alg1.l11.m1.1.1.2.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.2.1b" xref="alg1.l11.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l11.m1.1.1.2.5" xref="alg1.l11.m1.1.1.2.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.2.1c" xref="alg1.l11.m1.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="alg1.l11.m1.1.1.2.6" xref="alg1.l11.m1.1.1.2.6.cmml">_</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.2.1d" xref="alg1.l11.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l11.m1.1.1.2.7" xref="alg1.l11.m1.1.1.2.7.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.2.1e" xref="alg1.l11.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l11.m1.1.1.2.8" xref="alg1.l11.m1.1.1.2.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.2.1f" xref="alg1.l11.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l11.m1.1.1.2.9" xref="alg1.l11.m1.1.1.2.9.cmml">b</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.2.1g" xref="alg1.l11.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l11.m1.1.1.2.10" xref="alg1.l11.m1.1.1.2.10.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.2.1h" xref="alg1.l11.m1.1.1.2.1.cmml">​</mo><mi id="alg1.l11.m1.1.1.2.11" xref="alg1.l11.m1.1.1.2.11.cmml">l</mi></mrow><mo stretchy="false" id="alg1.l11.m1.1.1.1" xref="alg1.l11.m1.1.1.1.cmml">←</mo><mrow id="alg1.l11.m1.1.1.3" xref="alg1.l11.m1.1.1.3.cmml"><mtext id="alg1.l11.m1.1.1.3.2" xref="alg1.l11.m1.1.1.3.2a.cmml">label with highest probability in </mtext><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.3.1" xref="alg1.l11.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l11.m1.1.1.3.3" xref="alg1.l11.m1.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.3.1a" xref="alg1.l11.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l11.m1.1.1.3.4" xref="alg1.l11.m1.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.3.1b" xref="alg1.l11.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l11.m1.1.1.3.5" xref="alg1.l11.m1.1.1.3.5.cmml">b</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.3.1c" xref="alg1.l11.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l11.m1.1.1.3.6" xref="alg1.l11.m1.1.1.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.3.1d" xref="alg1.l11.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l11.m1.1.1.3.7" xref="alg1.l11.m1.1.1.3.7.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.3.1e" xref="alg1.l11.m1.1.1.3.1.cmml">​</mo><mi mathvariant="normal" id="alg1.l11.m1.1.1.3.8" xref="alg1.l11.m1.1.1.3.8.cmml">_</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.3.1f" xref="alg1.l11.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l11.m1.1.1.3.9" xref="alg1.l11.m1.1.1.3.9.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.3.1g" xref="alg1.l11.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l11.m1.1.1.3.10" xref="alg1.l11.m1.1.1.3.10.cmml">o</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.3.1h" xref="alg1.l11.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l11.m1.1.1.3.11" xref="alg1.l11.m1.1.1.3.11.cmml">g</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.3.1i" xref="alg1.l11.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l11.m1.1.1.3.12" xref="alg1.l11.m1.1.1.3.12.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.3.1j" xref="alg1.l11.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l11.m1.1.1.3.13" xref="alg1.l11.m1.1.1.3.13.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l11.m1.1.1.3.1k" xref="alg1.l11.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l11.m1.1.1.3.14" xref="alg1.l11.m1.1.1.3.14.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.1b"><apply id="alg1.l11.m1.1.1.cmml" xref="alg1.l11.m1.1.1"><ci id="alg1.l11.m1.1.1.1.cmml" xref="alg1.l11.m1.1.1.1">←</ci><apply id="alg1.l11.m1.1.1.2.cmml" xref="alg1.l11.m1.1.1.2"><times id="alg1.l11.m1.1.1.2.1.cmml" xref="alg1.l11.m1.1.1.2.1"></times><ci id="alg1.l11.m1.1.1.2.2.cmml" xref="alg1.l11.m1.1.1.2.2">𝑛</ci><ci id="alg1.l11.m1.1.1.2.3.cmml" xref="alg1.l11.m1.1.1.2.3">𝑒</ci><ci id="alg1.l11.m1.1.1.2.4.cmml" xref="alg1.l11.m1.1.1.2.4">𝑥</ci><ci id="alg1.l11.m1.1.1.2.5.cmml" xref="alg1.l11.m1.1.1.2.5">𝑡</ci><ci id="alg1.l11.m1.1.1.2.6.cmml" xref="alg1.l11.m1.1.1.2.6">_</ci><ci id="alg1.l11.m1.1.1.2.7.cmml" xref="alg1.l11.m1.1.1.2.7">𝑙</ci><ci id="alg1.l11.m1.1.1.2.8.cmml" xref="alg1.l11.m1.1.1.2.8">𝑎</ci><ci id="alg1.l11.m1.1.1.2.9.cmml" xref="alg1.l11.m1.1.1.2.9">𝑏</ci><ci id="alg1.l11.m1.1.1.2.10.cmml" xref="alg1.l11.m1.1.1.2.10">𝑒</ci><ci id="alg1.l11.m1.1.1.2.11.cmml" xref="alg1.l11.m1.1.1.2.11">𝑙</ci></apply><apply id="alg1.l11.m1.1.1.3.cmml" xref="alg1.l11.m1.1.1.3"><times id="alg1.l11.m1.1.1.3.1.cmml" xref="alg1.l11.m1.1.1.3.1"></times><ci id="alg1.l11.m1.1.1.3.2a.cmml" xref="alg1.l11.m1.1.1.3.2"><mtext id="alg1.l11.m1.1.1.3.2.cmml" xref="alg1.l11.m1.1.1.3.2">label with highest probability in </mtext></ci><ci id="alg1.l11.m1.1.1.3.3.cmml" xref="alg1.l11.m1.1.1.3.3">𝑙</ci><ci id="alg1.l11.m1.1.1.3.4.cmml" xref="alg1.l11.m1.1.1.3.4">𝑎</ci><ci id="alg1.l11.m1.1.1.3.5.cmml" xref="alg1.l11.m1.1.1.3.5">𝑏</ci><ci id="alg1.l11.m1.1.1.3.6.cmml" xref="alg1.l11.m1.1.1.3.6">𝑒</ci><ci id="alg1.l11.m1.1.1.3.7.cmml" xref="alg1.l11.m1.1.1.3.7">𝑙</ci><ci id="alg1.l11.m1.1.1.3.8.cmml" xref="alg1.l11.m1.1.1.3.8">_</ci><ci id="alg1.l11.m1.1.1.3.9.cmml" xref="alg1.l11.m1.1.1.3.9">𝑙</ci><ci id="alg1.l11.m1.1.1.3.10.cmml" xref="alg1.l11.m1.1.1.3.10">𝑜</ci><ci id="alg1.l11.m1.1.1.3.11.cmml" xref="alg1.l11.m1.1.1.3.11">𝑔</ci><ci id="alg1.l11.m1.1.1.3.12.cmml" xref="alg1.l11.m1.1.1.3.12">𝑖</ci><ci id="alg1.l11.m1.1.1.3.13.cmml" xref="alg1.l11.m1.1.1.3.13">𝑡</ci><ci id="alg1.l11.m1.1.1.3.14.cmml" xref="alg1.l11.m1.1.1.3.14">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.1c">next\_label\leftarrow\text{label with highest probability in }label\_logits</annotation></semantics></math>

</div>
<div id="alg1.l12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l12.1.1.1" class="ltx_text" style="font-size:80%;">12:</span></span>     append <span id="alg1.l12.2" class="ltx_text ltx_markedasmath ltx_font_italic">next_label</span> to <span id="alg1.l12.3" class="ltx_text ltx_markedasmath ltx_font_italic">label_list</span>

</div>
<div id="alg1.l13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l13.1.1.1" class="ltx_text" style="font-size:80%;">13:</span></span>     append <span id="alg1.l13.2" class="ltx_text ltx_markedasmath ltx_font_italic">next_label</span> to <span id="alg1.l13.3" class="ltx_text ltx_markedasmath ltx_font_italic">token_list</span>

</div>
<div id="alg1.l14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l14.1.1.1" class="ltx_text" style="font-size:80%;">14:</span></span>     append <math id="alg1.l14.m1.1" class="ltx_Math" alttext="\text{tokenizer.encode}(word)" display="inline"><semantics id="alg1.l14.m1.1a"><mrow id="alg1.l14.m1.1.1" xref="alg1.l14.m1.1.1.cmml"><mtext id="alg1.l14.m1.1.1.3" xref="alg1.l14.m1.1.1.3a.cmml">tokenizer.encode</mtext><mo lspace="0em" rspace="0em" id="alg1.l14.m1.1.1.2" xref="alg1.l14.m1.1.1.2.cmml">​</mo><mrow id="alg1.l14.m1.1.1.1.1" xref="alg1.l14.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="alg1.l14.m1.1.1.1.1.2" xref="alg1.l14.m1.1.1.1.1.1.cmml">(</mo><mrow id="alg1.l14.m1.1.1.1.1.1" xref="alg1.l14.m1.1.1.1.1.1.cmml"><mi id="alg1.l14.m1.1.1.1.1.1.2" xref="alg1.l14.m1.1.1.1.1.1.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="alg1.l14.m1.1.1.1.1.1.1" xref="alg1.l14.m1.1.1.1.1.1.1.cmml">​</mo><mi id="alg1.l14.m1.1.1.1.1.1.3" xref="alg1.l14.m1.1.1.1.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="alg1.l14.m1.1.1.1.1.1.1a" xref="alg1.l14.m1.1.1.1.1.1.1.cmml">​</mo><mi id="alg1.l14.m1.1.1.1.1.1.4" xref="alg1.l14.m1.1.1.1.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="alg1.l14.m1.1.1.1.1.1.1b" xref="alg1.l14.m1.1.1.1.1.1.1.cmml">​</mo><mi id="alg1.l14.m1.1.1.1.1.1.5" xref="alg1.l14.m1.1.1.1.1.1.5.cmml">d</mi></mrow><mo stretchy="false" id="alg1.l14.m1.1.1.1.1.3" xref="alg1.l14.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l14.m1.1b"><apply id="alg1.l14.m1.1.1.cmml" xref="alg1.l14.m1.1.1"><times id="alg1.l14.m1.1.1.2.cmml" xref="alg1.l14.m1.1.1.2"></times><ci id="alg1.l14.m1.1.1.3a.cmml" xref="alg1.l14.m1.1.1.3"><mtext id="alg1.l14.m1.1.1.3.cmml" xref="alg1.l14.m1.1.1.3">tokenizer.encode</mtext></ci><apply id="alg1.l14.m1.1.1.1.1.1.cmml" xref="alg1.l14.m1.1.1.1.1"><times id="alg1.l14.m1.1.1.1.1.1.1.cmml" xref="alg1.l14.m1.1.1.1.1.1.1"></times><ci id="alg1.l14.m1.1.1.1.1.1.2.cmml" xref="alg1.l14.m1.1.1.1.1.1.2">𝑤</ci><ci id="alg1.l14.m1.1.1.1.1.1.3.cmml" xref="alg1.l14.m1.1.1.1.1.1.3">𝑜</ci><ci id="alg1.l14.m1.1.1.1.1.1.4.cmml" xref="alg1.l14.m1.1.1.1.1.1.4">𝑟</ci><ci id="alg1.l14.m1.1.1.1.1.1.5.cmml" xref="alg1.l14.m1.1.1.1.1.1.5">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l14.m1.1c">\text{tokenizer.encode}(word)</annotation></semantics></math> to <span id="alg1.l14.2" class="ltx_text ltx_markedasmath ltx_font_italic">token_list</span>

</div>
<div id="alg1.l15" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l15.1.1.1" class="ltx_text" style="font-size:80%;">15:</span></span><span id="alg1.l15.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l15.3" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l16.1.1.1" class="ltx_text" style="font-size:80%;">16:</span></span><span id="alg1.l16.2" class="ltx_text ltx_font_bold">return</span> <span id="alg1.l16.3" class="ltx_text ltx_markedasmath ltx_font_italic">label_list</span>

</div>
</div>
</figure>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">On each iteration, word tokens are concatenated together with the prosodic token predictions that have accumulated so far. The prosodic token with the highest probability is picked, then inserted between the accumulated word-token predictions. Since prosodic label-combinations are defined per word, the output string alternates the generated prosodic labels and words in the odd and even positions.</p>
</div>
<div id="S4.SS2.SSS2.p3" class="ltx_para">
<p id="S4.SS2.SSS2.p3.1" class="ltx_p">Note the differences vis-a-vis the regular WHISPER prediction scheme: when trained on a language task, the WHISPER inference is not required to distinguish transcription-related tokens from non-transcription ones. Conversely, our method requires that only prosodic labels be drawn at the inference stage (for a manually tagged text vs. the output of the trained model see Figure <span id="S4.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_italic">SM2</span> in the <span id="S4.SS2.SSS2.p3.1.2" class="ltx_text ltx_font_italic">supplementary material</span>).</p>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Validation/Evaluation</h4>

<section id="S4.SS2.SSS3.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Metrics</h5>

<div id="S4.SS2.SSS3.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS3.Px1.p1.1" class="ltx_p">To evaluate the capabilities of the model, we used Cohen’s Kappa (CK) metric of inter-annotator agreement (see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> for IU boundaries and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> citing scores for two of our three labels).</p>
</div>
<div id="S4.SS2.SSS3.Px1.p2" class="ltx_para">
<p id="S4.SS2.SSS3.Px1.p2.1" class="ltx_p">Two CK metrics were used for IU boundary recognition/segmentation: the first considered the prediction for the first uttered word in a turn, and the second did not. Since the beginning of a turn is a predetermined IU boundary, the classification for the first word carries no predictive power.</p>
</div>
<div id="S4.SS2.SSS3.Px1.p3" class="ltx_para">
<p id="S4.SS2.SSS3.Px1.p3.1" class="ltx_p">Prototype performance was calculated per IU, and only for the well-identified IUs (<math id="S4.SS2.SSS3.Px1.p3.1.m1.1" class="ltx_Math" alttext="\sim 94\%" display="inline"><semantics id="S4.SS2.SSS3.Px1.p3.1.m1.1a"><mrow id="S4.SS2.SSS3.Px1.p3.1.m1.1.1" xref="S4.SS2.SSS3.Px1.p3.1.m1.1.1.cmml"><mi id="S4.SS2.SSS3.Px1.p3.1.m1.1.1.2" xref="S4.SS2.SSS3.Px1.p3.1.m1.1.1.2.cmml"></mi><mo id="S4.SS2.SSS3.Px1.p3.1.m1.1.1.1" xref="S4.SS2.SSS3.Px1.p3.1.m1.1.1.1.cmml">∼</mo><mrow id="S4.SS2.SSS3.Px1.p3.1.m1.1.1.3" xref="S4.SS2.SSS3.Px1.p3.1.m1.1.1.3.cmml"><mn id="S4.SS2.SSS3.Px1.p3.1.m1.1.1.3.2" xref="S4.SS2.SSS3.Px1.p3.1.m1.1.1.3.2.cmml">94</mn><mo id="S4.SS2.SSS3.Px1.p3.1.m1.1.1.3.1" xref="S4.SS2.SSS3.Px1.p3.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.Px1.p3.1.m1.1b"><apply id="S4.SS2.SSS3.Px1.p3.1.m1.1.1.cmml" xref="S4.SS2.SSS3.Px1.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.SSS3.Px1.p3.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.Px1.p3.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.SS2.SSS3.Px1.p3.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.Px1.p3.1.m1.1.1.2">absent</csymbol><apply id="S4.SS2.SSS3.Px1.p3.1.m1.1.1.3.cmml" xref="S4.SS2.SSS3.Px1.p3.1.m1.1.1.3"><csymbol cd="latexml" id="S4.SS2.SSS3.Px1.p3.1.m1.1.1.3.1.cmml" xref="S4.SS2.SSS3.Px1.p3.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S4.SS2.SSS3.Px1.p3.1.m1.1.1.3.2.cmml" xref="S4.SS2.SSS3.Px1.p3.1.m1.1.1.3.2">94</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.Px1.p3.1.m1.1c">\sim 94\%</annotation></semantics></math> of the units). The evaluation was based on the predicted prototype label for the first and last words of an IU, assuming that this is a match which best represents the prosodic information required for the task.</p>
</div>
</section>
<section id="S4.SS2.SSS3.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Experiment Setup</h5>

<div id="S4.SS2.SSS3.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS3.Px2.p1.1" class="ltx_p">First, we explored the effect of several pre-trained WHISPER architectures. Whereas fine-tuning the largest model yielded the best results, it required roughly three hours of training on a single GPU. To balance training speed and performance, we tested smaller models, including the “Tiny”, “Small”, “Base” and “Medium” variants. By eliminating gradient accumulation and using a larger batch size, training time for the smaller model was reduced to a half an hour on a smaller GPU.</p>
</div>
<div id="S4.SS2.SSS3.Px2.p2" class="ltx_para">
<p id="S4.SS2.SSS3.Px2.p2.1" class="ltx_p">As mentioned above (section <a href="#S4.SS2.SSS1" title="4.2.1 Training ‣ 4.2 Experiment objectives and setup ‣ 4 Methods ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>), we trained for single recognition tasks in order to compare the performance on a single task vs. the triple one.
In addition, we tested three different representation methods of prosodic labels: (1) ‘raw’, which refers to special ‘words’ that were generated for this process; (2) ‘compact’, which refers to twelve labels that stand for the twelve combinations of prosodic tags; and (3) ‘bits’, which is similar to ‘raw’, and represents each prosodic feature by a single token (see <span id="S4.SS2.SSS3.Px2.p2.1.1" class="ltx_text ltx_font_italic">supplementary material</span>).</p>
</div>
</section>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Fine-tuning the WHISPER models for simultaneous detection of prosodic phenomena proved very successful. This is especially true for predicting IU boundaries, whereas simultaneous detection of prosodic prototypes and emphases were more demanding tasks. Notably, the outcome indicates that the fine-tuned model is on par with human annotators (when they tag individual tasks).
In the task of prototype recognition, the rare prototype “?” (“request for response”) was best recognised when employing the WHISPER-Large V2 (see Table <a href="#S5.T2" title="Table 2 ‣ 5 Results ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.2.1.1" class="ltx_tr">
<th id="S5.T2.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="6"><span id="S5.T2.2.1.1.1.1" class="ltx_text ltx_font_bold">(a)</span></th>
</tr>
<tr id="S5.T2.2.2.2" class="ltx_tr">
<th id="S5.T2.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T2.2.2.2.1.1" class="ltx_text ltx_font_bold">Metric</span></th>
<th id="S5.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.2.2.2.2.1" class="ltx_text ltx_font_bold">Segmentation</span></th>
<th id="S5.T2.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.2.2.2.3.1" class="ltx_text ltx_font_bold">Emphasis</span></th>
<th id="S5.T2.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.2.2.2.4.1" class="ltx_text ltx_font_bold">Question</span></th>
<th id="S5.T2.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.2.2.2.5.1" class="ltx_text ltx_font_bold">Period</span></th>
<th id="S5.T2.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.2.2.2.6.1" class="ltx_text ltx_font_bold">Comma</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.2.3.1" class="ltx_tr">
<th id="S5.T2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Cohen’s Kappa</th>
<td id="S5.T2.2.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.932</td>
<td id="S5.T2.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.588</td>
<td id="S5.T2.2.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.664</td>
<td id="S5.T2.2.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.453</td>
<td id="S5.T2.2.3.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.442</td>
</tr>
<tr id="S5.T2.2.4.2" class="ltx_tr">
<th id="S5.T2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Recall</th>
<td id="S5.T2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_r">0.958</td>
<td id="S5.T2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r">0.713</td>
<td id="S5.T2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_r">0.594</td>
<td id="S5.T2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_r">0.644</td>
<td id="S5.T2.2.4.2.6" class="ltx_td ltx_align_center ltx_border_r">0.789</td>
</tr>
<tr id="S5.T2.2.5.3" class="ltx_tr">
<th id="S5.T2.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Precision</th>
<td id="S5.T2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_r">0.941</td>
<td id="S5.T2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r">0.7</td>
<td id="S5.T2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_r">0.784</td>
<td id="S5.T2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_r">0.724</td>
<td id="S5.T2.2.5.3.6" class="ltx_td ltx_align_center ltx_border_r">0.708</td>
</tr>
<tr id="S5.T2.2.6.4" class="ltx_tr">
<th id="S5.T2.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">F1-score</th>
<td id="S5.T2.2.6.4.2" class="ltx_td ltx_align_center ltx_border_r">0.949</td>
<td id="S5.T2.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r">0.7</td>
<td id="S5.T2.2.6.4.4" class="ltx_td ltx_align_center ltx_border_r">0.676</td>
<td id="S5.T2.2.6.4.5" class="ltx_td ltx_align_center ltx_border_r">0.682</td>
<td id="S5.T2.2.6.4.6" class="ltx_td ltx_align_center ltx_border_r">0.746</td>
</tr>
<tr id="S5.T2.2.7.5" class="ltx_tr">
<th id="S5.T2.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Accuracy</th>
<td id="S5.T2.2.7.5.2" class="ltx_td ltx_align_center ltx_border_r">0.974</td>
<td id="S5.T2.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r">0.831</td>
<td id="S5.T2.2.7.5.4" class="ltx_td ltx_align_center ltx_border_r">0.978</td>
<td id="S5.T2.2.7.5.5" class="ltx_td ltx_align_center ltx_border_r">0.733</td>
<td id="S5.T2.2.7.5.6" class="ltx_td ltx_align_center ltx_border_r">0.722</td>
</tr>
<tr id="S5.T2.2.8.6" class="ltx_tr">
<th id="S5.T2.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" colspan="6"><span id="S5.T2.2.8.6.1.1" class="ltx_text ltx_font_bold">(b)</span></th>
</tr>
<tr id="S5.T2.2.9.7" class="ltx_tr">
<th id="S5.T2.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T2.2.9.7.1.1" class="ltx_text ltx_font_bold">Metric</span></th>
<th id="S5.T2.2.9.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.2.9.7.2.1" class="ltx_text ltx_font_bold">Segmentation</span></th>
<th id="S5.T2.2.9.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.2.9.7.3.1" class="ltx_text ltx_font_bold">Emphasis</span></th>
<th id="S5.T2.2.9.7.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.2.9.7.4.1" class="ltx_text ltx_font_bold">Question</span></th>
<th id="S5.T2.2.9.7.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.2.9.7.5.1" class="ltx_text ltx_font_bold">Period</span></th>
<th id="S5.T2.2.9.7.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T2.2.9.7.6.1" class="ltx_text ltx_font_bold">Comma</span></th>
</tr>
<tr id="S5.T2.2.10.8" class="ltx_tr">
<th id="S5.T2.2.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Cohen’s Kappa</th>
<td id="S5.T2.2.10.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.914</td>
<td id="S5.T2.2.10.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.552</td>
<td id="S5.T2.2.10.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.497</td>
<td id="S5.T2.2.10.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.443</td>
<td id="S5.T2.2.10.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.419</td>
</tr>
<tr id="S5.T2.2.11.9" class="ltx_tr">
<th id="S5.T2.2.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Recall</th>
<td id="S5.T2.2.11.9.2" class="ltx_td ltx_align_center ltx_border_r">0.938</td>
<td id="S5.T2.2.11.9.3" class="ltx_td ltx_align_center ltx_border_r">0.738</td>
<td id="S5.T2.2.11.9.4" class="ltx_td ltx_align_center ltx_border_r">0.391</td>
<td id="S5.T2.2.11.9.5" class="ltx_td ltx_align_center ltx_border_r">0.626</td>
<td id="S5.T2.2.11.9.6" class="ltx_td ltx_align_center ltx_border_r">0.797</td>
</tr>
<tr id="S5.T2.2.12.10" class="ltx_tr">
<th id="S5.T2.2.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Precision</th>
<td id="S5.T2.2.12.10.2" class="ltx_td ltx_align_center ltx_border_r">0.936</td>
<td id="S5.T2.2.12.10.3" class="ltx_td ltx_align_center ltx_border_r">0.639</td>
<td id="S5.T2.2.12.10.4" class="ltx_td ltx_align_center ltx_border_r">0.735</td>
<td id="S5.T2.2.12.10.5" class="ltx_td ltx_align_center ltx_border_r">0.726</td>
<td id="S5.T2.2.12.10.6" class="ltx_td ltx_align_center ltx_border_r">0.672</td>
</tr>
<tr id="S5.T2.2.13.11" class="ltx_tr">
<th id="S5.T2.2.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">F1-score</th>
<td id="S5.T2.2.13.11.2" class="ltx_td ltx_align_center ltx_border_r">0.937</td>
<td id="S5.T2.2.13.11.3" class="ltx_td ltx_align_center ltx_border_r">0.685</td>
<td id="S5.T2.2.13.11.4" class="ltx_td ltx_align_center ltx_border_r">0.510</td>
<td id="S5.T2.2.13.11.5" class="ltx_td ltx_align_center ltx_border_r">0.672</td>
<td id="S5.T2.2.13.11.6" class="ltx_td ltx_align_center ltx_border_r">0.741</td>
</tr>
<tr id="S5.T2.2.14.12" class="ltx_tr">
<th id="S5.T2.2.14.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">Accuracy</th>
<td id="S5.T2.2.14.12.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.968</td>
<td id="S5.T2.2.14.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.808</td>
<td id="S5.T2.2.14.12.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.971</td>
<td id="S5.T2.2.14.12.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.729</td>
<td id="S5.T2.2.14.12.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.711</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.4.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S5.T2.5.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Comparison of various metrics on TAL dataset<span id="S5.T2.5.2.1" class="ltx_text ltx_font_upright">. Results for main split, re-trained WHISPER-Large V2 (2a) and WHISPER-Small (2b), using the “Compact” labels.
</span></span></figcaption>
</figure>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Table <a href="#S5.T3" title="Table 3 ‣ 5 Results ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that the model generalizes well across datasets and genres. It was more successful when employed on the TAL data than on the Interviews data (which were excluded from the training material), and specifically so in regard to IU boundary recognition.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.2.1.1" class="ltx_tr">
<th id="S5.T3.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T3.2.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S5.T3.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T3.2.1.1.2.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S5.T3.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T3.2.1.1.3.1" class="ltx_text ltx_font_bold">Segmentation</span></th>
<th id="S5.T3.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T3.2.1.1.4.1" class="ltx_text ltx_font_bold">Segmentation (wos)</span></th>
<th id="S5.T3.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T3.2.1.1.5.1" class="ltx_text ltx_font_bold">Emphasis</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.2.2.1" class="ltx_tr">
<th id="S5.T3.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">TAL</th>
<th id="S5.T3.2.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Small</th>
<td id="S5.T3.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.914</td>
<td id="S5.T3.2.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.895</td>
<td id="S5.T3.2.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.552</td>
</tr>
<tr id="S5.T3.2.3.2" class="ltx_tr">
<th id="S5.T3.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Interviews</th>
<th id="S5.T3.2.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Small</th>
<td id="S5.T3.2.3.2.3" class="ltx_td ltx_align_center ltx_border_r">0.680</td>
<td id="S5.T3.2.3.2.4" class="ltx_td ltx_align_center ltx_border_r">0.593</td>
<td id="S5.T3.2.3.2.5" class="ltx_td ltx_align_center ltx_border_r">0.456</td>
</tr>
<tr id="S5.T3.2.4.3" class="ltx_tr">
<th id="S5.T3.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">Interviews</th>
<th id="S5.T3.2.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">Large-V2</th>
<td id="S5.T3.2.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.711</td>
<td id="S5.T3.2.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.629</td>
<td id="S5.T3.2.4.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.519</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.4.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.5.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Cohen’s Kappa scores on TAL and Interviews datasets<span id="S5.T3.5.2.1" class="ltx_text ltx_font_upright">. These tests employed the Large version of the model on the main split of TAL dataset, using the “Compact” labels.
</span></span></figcaption>
</figure>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Table <a href="#S5.T4" title="Table 4 ‣ 5 Results ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> reports slight differences in performance for TAL interviewer (Ira Glass) vs. his interviewees (n=49 in the test set; n=81 in the train set). The difference may be attributed to genre: the interviewer’s speech may be scripted/ narrated, whereas the interviewees are spontaneous speakers.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.2.1.1" class="ltx_tr">
<th id="S5.T4.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T4.2.1.1.1.1" class="ltx_text ltx_font_bold">Test Set</span></th>
<th id="S5.T4.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T4.2.1.1.2.1" class="ltx_text ltx_font_bold">#Turns</span></th>
<th id="S5.T4.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T4.2.1.1.3.1" class="ltx_text ltx_font_bold">#Speakers</span></th>
<th id="S5.T4.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T4.2.1.1.4.1" class="ltx_text ltx_font_bold">Segmentation</span></th>
<th id="S5.T4.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T4.2.1.1.5.1" class="ltx_text ltx_font_bold">Segmentation (wos)</span></th>
<th id="S5.T4.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T4.2.1.1.6.1" class="ltx_text ltx_font_bold">Emphasis</span></th>
<th id="S5.T4.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T4.2.1.1.7.1" class="ltx_text ltx_font_bold">Prototype</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.2.2.1" class="ltx_tr">
<td id="S5.T4.2.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">All</td>
<td id="S5.T4.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">192</td>
<td id="S5.T4.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50</td>
<td id="S5.T4.2.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.914</td>
<td id="S5.T4.2.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.895</td>
<td id="S5.T4.2.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.552</td>
<td id="S5.T4.2.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.447</td>
</tr>
<tr id="S5.T4.2.3.2" class="ltx_tr">
<td id="S5.T4.2.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Ira Glass</td>
<td id="S5.T4.2.3.2.2" class="ltx_td ltx_align_center ltx_border_r">47</td>
<td id="S5.T4.2.3.2.3" class="ltx_td ltx_align_center ltx_border_r">1</td>
<td id="S5.T4.2.3.2.4" class="ltx_td ltx_align_center ltx_border_r">0.915</td>
<td id="S5.T4.2.3.2.5" class="ltx_td ltx_align_center ltx_border_r">0.895</td>
<td id="S5.T4.2.3.2.6" class="ltx_td ltx_align_center ltx_border_r">0.574</td>
<td id="S5.T4.2.3.2.7" class="ltx_td ltx_align_center ltx_border_r">0.419</td>
</tr>
<tr id="S5.T4.2.4.3" class="ltx_tr">
<td id="S5.T4.2.4.3.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r">Others</td>
<td id="S5.T4.2.4.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">145</td>
<td id="S5.T4.2.4.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">49</td>
<td id="S5.T4.2.4.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.914</td>
<td id="S5.T4.2.4.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.895</td>
<td id="S5.T4.2.4.3.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.547</td>
<td id="S5.T4.2.4.3.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.451</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.4.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S5.T4.5.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Ira Glass - the show host - vs. other speakers<span id="S5.T4.5.2.1" class="ltx_text ltx_font_upright"> (n=49 in test set), results for WHISPER-Small.</span></span></figcaption>
</figure>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">As for model size, unsurprisingly and generally speaking, the larger the model, the better the performance (Figure <a href="#S5.F5" title="Figure 5 ‣ 5 Results ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). The Large V2 model performed significantly better on Prototype detection. However, over the majority of tasks, the improvement was not dramatic.</p>
</div>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.03522/assets/figures/7.png" id="S5.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="370" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.T5.st1" class="ltx_table ltx_figure_panel ltx_align_center">
<table id="S5.T5.st1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.st1.2.1.1" class="ltx_tr">
<th id="S5.T5.st1.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T5.st1.2.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S5.T5.st1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T5.st1.2.1.1.2.1" class="ltx_text ltx_font_bold">Segmentation</span></th>
<th id="S5.T5.st1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T5.st1.2.1.1.3.1" class="ltx_text ltx_font_bold">Segmentation (wos)</span></th>
<th id="S5.T5.st1.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T5.st1.2.1.1.4.1" class="ltx_text ltx_font_bold">Emphasis</span></th>
<th id="S5.T5.st1.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T5.st1.2.1.1.5.1" class="ltx_text ltx_font_bold">Prototype</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.st1.2.2.1" class="ltx_tr">
<th id="S5.T5.st1.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Tiny</th>
<td id="S5.T5.st1.2.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.776</td>
<td id="S5.T5.st1.2.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.718</td>
<td id="S5.T5.st1.2.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.469</td>
<td id="S5.T5.st1.2.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.205</td>
</tr>
<tr id="S5.T5.st1.2.3.2" class="ltx_tr">
<th id="S5.T5.st1.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Base</th>
<td id="S5.T5.st1.2.3.2.2" class="ltx_td ltx_align_center ltx_border_r">0.815</td>
<td id="S5.T5.st1.2.3.2.3" class="ltx_td ltx_align_center ltx_border_r">0.771</td>
<td id="S5.T5.st1.2.3.2.4" class="ltx_td ltx_align_center ltx_border_r">0.524</td>
<td id="S5.T5.st1.2.3.2.5" class="ltx_td ltx_align_center ltx_border_r">0.228</td>
</tr>
<tr id="S5.T5.st1.2.4.3" class="ltx_tr">
<th id="S5.T5.st1.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Small</th>
<td id="S5.T5.st1.2.4.3.2" class="ltx_td ltx_align_center ltx_border_r">0.914</td>
<td id="S5.T5.st1.2.4.3.3" class="ltx_td ltx_align_center ltx_border_r">0.895</td>
<td id="S5.T5.st1.2.4.3.4" class="ltx_td ltx_align_center ltx_border_r">0.552</td>
<td id="S5.T5.st1.2.4.3.5" class="ltx_td ltx_align_center ltx_border_r">0.447</td>
</tr>
<tr id="S5.T5.st1.2.5.4" class="ltx_tr">
<th id="S5.T5.st1.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Medium</th>
<td id="S5.T5.st1.2.5.4.2" class="ltx_td ltx_align_center ltx_border_r">0.929</td>
<td id="S5.T5.st1.2.5.4.3" class="ltx_td ltx_align_center ltx_border_r">0.914</td>
<td id="S5.T5.st1.2.5.4.4" class="ltx_td ltx_align_center ltx_border_r">0.551</td>
<td id="S5.T5.st1.2.5.4.5" class="ltx_td ltx_align_center ltx_border_r">0.462</td>
</tr>
<tr id="S5.T5.st1.2.6.5" class="ltx_tr">
<th id="S5.T5.st1.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">Large-V2</th>
<td id="S5.T5.st1.2.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T5.st1.2.6.5.2.1" class="ltx_text ltx_font_bold">0.933</span></td>
<td id="S5.T5.st1.2.6.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T5.st1.2.6.5.3.1" class="ltx_text ltx_font_bold">0.918</span></td>
<td id="S5.T5.st1.2.6.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T5.st1.2.6.5.4.1" class="ltx_text ltx_font_bold">0.588</span></td>
<td id="S5.T5.st1.2.6.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T5.st1.2.6.5.5.1" class="ltx_text ltx_font_bold">0.484</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T5.st1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Impact of model size on performance of re-trained WHISPER for three simultaneous tasks<span id="S5.F5.4.2.1" class="ltx_text ltx_font_upright">. (5b) Tests on TAL dataset, with/out considering the first word of a turn.</span></span></figcaption>
</figure>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">The triple detection task begs the question of how well the fine-tuned model would fare when trained to detect a single prosodic phenomenon. Table <a href="#S5.T5" title="Table 5 ‣ 5 Results ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows that the results are not all that different: the performance is stable and somewhat weaker for single tasks.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<table id="S5.T5.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.4.1.1" class="ltx_tr">
<th id="S5.T5.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5"><span id="S5.T5.4.1.1.1.1" class="ltx_text ltx_font_bold">(a)</span></th>
</tr>
<tr id="S5.T5.4.2.2" class="ltx_tr">
<th id="S5.T5.4.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5"><span id="S5.T5.4.2.2.1.1" class="ltx_text ltx_font_bold">Full Train Set</span></th>
</tr>
<tr id="S5.T5.4.3.3" class="ltx_tr">
<th id="S5.T5.4.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T5.4.3.3.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S5.T5.4.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T5.4.3.3.2.1" class="ltx_text ltx_font_bold">IU Detect</span></th>
<th id="S5.T5.4.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T5.4.3.3.3.1" class="ltx_text ltx_font_bold">IU (wos)</span></th>
<th id="S5.T5.4.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T5.4.3.3.4.1" class="ltx_text ltx_font_bold">Emphasis</span></th>
<th id="S5.T5.4.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T5.4.3.3.5.1" class="ltx_text ltx_font_bold">Prototype</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.4.4.1" class="ltx_tr">
<th id="S5.T5.4.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Small</th>
<td id="S5.T5.4.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.941</td>
<td id="S5.T5.4.4.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.928</td>
<td id="S5.T5.4.4.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.561</td>
<td id="S5.T5.4.4.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.471</td>
</tr>
<tr id="S5.T5.4.5.2" class="ltx_tr">
<th id="S5.T5.4.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Large-V2</th>
<td id="S5.T5.4.5.2.2" class="ltx_td ltx_align_center ltx_border_r">0.931</td>
<td id="S5.T5.4.5.2.3" class="ltx_td ltx_align_center ltx_border_r">0.916</td>
<td id="S5.T5.4.5.2.4" class="ltx_td ltx_align_center ltx_border_r">0.563</td>
<td id="S5.T5.4.5.2.5" class="ltx_td ltx_align_center ltx_border_r">0.506</td>
</tr>
<tr id="S5.T5.4.6.3" class="ltx_tr">
<th id="S5.T5.4.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S5.T5.4.6.3.1.1" class="ltx_text ltx_font_italic">Best Multi-Label</span></th>
<td id="S5.T5.4.6.3.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T5.4.6.3.2.1" class="ltx_text ltx_font_italic">0.946</span></td>
<td id="S5.T5.4.6.3.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T5.4.6.3.3.1" class="ltx_text ltx_font_italic">0.934</span></td>
<td id="S5.T5.4.6.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T5.4.6.3.4.1" class="ltx_text ltx_font_italic">0.588</span></td>
<td id="S5.T5.4.6.3.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T5.4.6.3.5.1" class="ltx_text ltx_font_italic">0.503</span></td>
</tr>
<tr id="S5.T5.4.7.4" class="ltx_tr">
<th id="S5.T5.4.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" colspan="5"><span id="S5.T5.4.7.4.1.1" class="ltx_text ltx_font_bold">(b)</span></th>
</tr>
<tr id="S5.T5.4.8.5" class="ltx_tr">
<th id="S5.T5.4.8.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" colspan="5"><span id="S5.T5.4.8.5.1.1" class="ltx_text ltx_font_bold">8% Train Set</span></th>
</tr>
<tr id="S5.T5.4.9.6" class="ltx_tr">
<th id="S5.T5.4.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T5.4.9.6.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S5.T5.4.9.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T5.4.9.6.2.1" class="ltx_text ltx_font_bold">IU Detect</span></th>
<th id="S5.T5.4.9.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T5.4.9.6.3.1" class="ltx_text ltx_font_bold">IU (wos)</span></th>
<th id="S5.T5.4.9.6.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T5.4.9.6.4.1" class="ltx_text ltx_font_bold">Emphasis</span></th>
<th id="S5.T5.4.9.6.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T5.4.9.6.5.1" class="ltx_text ltx_font_bold">Prototype</span></th>
</tr>
<tr id="S5.T5.4.10.7" class="ltx_tr">
<th id="S5.T5.4.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Small</th>
<td id="S5.T5.4.10.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.850</td>
<td id="S5.T5.4.10.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.817</td>
<td id="S5.T5.4.10.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.428</td>
<td id="S5.T5.4.10.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.183</td>
</tr>
<tr id="S5.T5.4.11.8" class="ltx_tr">
<th id="S5.T5.4.11.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Large-V2</th>
<td id="S5.T5.4.11.8.2" class="ltx_td ltx_align_center ltx_border_r">0.887</td>
<td id="S5.T5.4.11.8.3" class="ltx_td ltx_align_center ltx_border_r">0.864</td>
<td id="S5.T5.4.11.8.4" class="ltx_td ltx_align_center ltx_border_r">0.489</td>
<td id="S5.T5.4.11.8.5" class="ltx_td ltx_align_center ltx_border_r">0.325</td>
</tr>
<tr id="S5.T5.4.12.9" class="ltx_tr">
<th id="S5.T5.4.12.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"><span id="S5.T5.4.12.9.1.1" class="ltx_text ltx_font_italic">Best Multi-Label</span></th>
<td id="S5.T5.4.12.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T5.4.12.9.2.1" class="ltx_text ltx_font_italic">0.915</span></td>
<td id="S5.T5.4.12.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T5.4.12.9.3.1" class="ltx_text ltx_font_italic">0.896</span></td>
<td id="S5.T5.4.12.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T5.4.12.9.4.1" class="ltx_text ltx_font_italic">0.504</span></td>
<td id="S5.T5.4.12.9.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T5.4.12.9.5.1" class="ltx_text ltx_font_italic">0.274</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T5.6.2.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S5.T5.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Performance of re-trained WHISPER models for three single tasks vs. the triple task (in italics)<span id="S5.T5.2.1.1" class="ltx_text ltx_font_upright">. Tests on TAL dataset, with/out considering the first word of a turn. Models were trained either on the entire set (5a) or on <math id="S5.T5.2.1.1.m1.1" class="ltx_Math" alttext="8\%" display="inline"><semantics id="S5.T5.2.1.1.m1.1b"><mrow id="S5.T5.2.1.1.m1.1.1" xref="S5.T5.2.1.1.m1.1.1.cmml"><mn id="S5.T5.2.1.1.m1.1.1.2" xref="S5.T5.2.1.1.m1.1.1.2.cmml">8</mn><mo id="S5.T5.2.1.1.m1.1.1.1" xref="S5.T5.2.1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.2.1.1.m1.1c"><apply id="S5.T5.2.1.1.m1.1.1.cmml" xref="S5.T5.2.1.1.m1.1.1"><csymbol cd="latexml" id="S5.T5.2.1.1.m1.1.1.1.cmml" xref="S5.T5.2.1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S5.T5.2.1.1.m1.1.1.2.cmml" xref="S5.T5.2.1.1.m1.1.1.2">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.1.1.m1.1d">8\%</annotation></semantics></math> (5b) of it, to rule out data loss due to label encoding.</span></span></figcaption>
</figure>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">Another finding is the robustness of the models, regardless of the differences in turn generation method (Table <a href="#S5.T3" title="Table 3 ‣ 5 Results ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, section <a href="#S4" title="4 Methods ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). Note the difference in number of turns vis-a-vis the stability of performance.</p>
</div>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.1" class="ltx_p">The results in <a href="#S5.T2" title="In 5 Results ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tables</span> <span class="ltx_text ltx_ref_tag">2</span></a>, <a href="#S5.T3" title="Table 3 ‣ 5 Results ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S5.T4" title="Table 4 ‣ 5 Results ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and Figure <a href="#S5.T5.st1" title="Table 5(a) ‣ Figure 5 ‣ 5 Results ‣ Non-verbal information in spontaneous speech – towards a new framework of analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(a)</span></a> indicate that the re-trained WHISPER models separate well three different prosodic simultaneous messages. They generalize over a large variety of speakers, for several types of data, spontaneous and scripted, and for different expert annotators.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Summary</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">We have shown that simultaneous prosodic messages of different non-verbal orders may be disentangled and detected, independently and simultaneously. This is an encouraging validation of the layered approach to prosodic patterning that this article proposes. The fact that the triple detection task outperforms the single detection ones further corroborates our decision to use the IU as the central arena of prosodic events, and are key to their successful recognition.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">In addition, we presented a new method for multi-label, multi-class transfer learning, which enriches the sequence of ASR training with prosodic labels. This ‘dynamic tokenizer’ – that is, a fine-tuning that uses existing WHISPER tokens for a new task – seems to draw out information that already exists within the weights of the original model.
The performance of this method is just as encouraging. Despite the difficulty in training for various detection tasks at a time, on diverse data, labeled by different annotators, it is either on par with, or superior to, that of average human annotation. As discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, the agreement for annotating prosodic boundary and emphasis (separately) is estimated at 0.52-0.78 Cohen’s Kappa. Therefore, our model can be considered an expert annotator for the prosodic phenomena learned.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Future work</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">Future challenges abound, and encompass many of the domains that this multidisciplinary work touches upon. They may be divided into four principal veins:</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<ol id="S6.I1" class="ltx_enumerate">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p">Extending the repertoire of reliably recognised prosodic patterns of all non-verbal orders, including emotions and speaker attitudes. This includes exploring prosodic universals vs. language- or community-specific phenomena, as well as other socio-linguistic factors and fine-grained analyses.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p">Applying our transfer learning method to additional fields: computer vision, NLP, etc. The method of intertwining new labels with known tokens enables the labeling of “extra” information, which exists in the model’s weights side-by-side with already-formalized data. It can also enhance the measuring of the “new token” recognition.</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p">Exploring the model and its internal representations, in order to determine, and better make use of, the distinctive features of its prosodic classification (cf. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>).</p>
</div>
</li>
<li id="S6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S6.I1.i4.p1" class="ltx_para">
<p id="S6.I1.i4.p1.1" class="ltx_p">Studying the relationships of prosodic patterns with other linguistic components, and developing a new tool for context formalization.</p>
</div>
</li>
</ol>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p">This article offers a first attempt at the disentanglement of prosodic messages, based on IU analysis. Through the systematic recognition of non-verbal messages, it can expand the horizons of speech and language descriptions, and support the long-standing effort on context elucidation. As our framework differentiates between non-verbal signals, it can also set apart emotional from non-emotional patterns. Thus, it might just produce the holy grail of speech analytics – reliable emotion and sentiment recognition for spontaneous speech.</p>
</div>
</section>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">We wish to thank Amir Becker, David Biron, Sharon Fireman and Assaf Marron for valuable suggestions throughout our research. Parts of the work were funded by a research grant to DH from the Israel Science Foundation, and by BINA – the Translational Research and Innovation unit of the Weizmann Institute of Science. Some of the methods and techniques presented in the article have been submitted for US Provisional patent protection (Application No. 63/614,588, filed on 12.24.23).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.1.1" class="ltx_bibitem">
<span class="ltx_bibblock"><span id="bib.1.1.1.1" class="ltx_ERROR undefined">\bibcommenthead</span>
</span>
</li>
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu [2011]</span>
<span class="ltx_bibblock">
Xu, Y.:
Speech prosody: A methodological review.
Journal of Speech Sciences
<span id="bib.bib1.1.1" class="ltx_text ltx_font_bold">1</span>(1),
85–115
(2011)


</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thein [2017]</span>
<span class="ltx_bibblock">
Thein, M.L.:
Die Informationelle Struktur Im Englischen: Syntax und Information Als Mittel der Hervorhebung
vol. 323.
Walter de Gruyter GmbH &amp; Co KG, ???
(2017)


</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2015]</span>
<span class="ltx_bibblock">
Xu, Y.,
Lee, A.,
Prom-On, S.,
Liu, F.:
Explaining the penta model: a reply to arvaniti and ladd.
Phonology
<span id="bib.bib3.1.1" class="ltx_text ltx_font_bold">32</span>(3),
505–535
(2015)


</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cole [2015]</span>
<span class="ltx_bibblock">
Cole, J.:
Prosody in context: A review.
Language, Cognition and Neuroscience
<span id="bib.bib4.1.1" class="ltx_text ltx_font_bold">30</span>(1-2),
1–31
(2015)


</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ladd [2014]</span>
<span class="ltx_bibblock">
Ladd, D.R.:
Simultaneous Structure in Phonology
vol. 28.
OUP Oxford, ???
(2014)


</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2023]</span>
<span class="ltx_bibblock">
Radford, A.,
Kim, J.W.,
Xu, T.,
Brockman, G.,
McLeavey, C.,
Sutskever, I.:
Robust speech recognition via large-scale weak supervision.
In: International Conference on Machine Learning,
pp. 28492–28518
(2023).
PMLR


</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du Bois et al. [2014]</span>
<span class="ltx_bibblock">
Du Bois, J.W.,
Schuetze-Coburn, S.,
Cumming, S.,
Paolino, D.:
Outline of discourse transcription.
In: Talking Data,
pp. 45–89.
Psychology Press, ???
(2014)


</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Himmelmann et al. [2018]</span>
<span class="ltx_bibblock">
Himmelmann, N.P.,
Sandler, M.,
Strunk, J.,
Unterladstetter, V.:
On the universality of intonational phrases: A cross-linguistic interrater study.
Phonology
<span id="bib.bib8.1.1" class="ltx_text ltx_font_bold">35</span>(2),
207–245
(2018)


</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Halliday [2015]</span>
<span class="ltx_bibblock">
Halliday, M.A.K.:
Intonation and Grammar in British English
vol. 48.
Walter de Gruyter GmbH &amp; Co KG, ???
(2015)


</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beckman and Pierrehumbert [1986]</span>
<span class="ltx_bibblock">
Beckman, M.E.,
Pierrehumbert, J.B.:
Intonational structure in japanese and english.
Phonology
<span id="bib.bib10.1.1" class="ltx_text ltx_font_bold">3</span>,
255–309
(1986)


</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silverman et al. [1992]</span>
<span class="ltx_bibblock">
Silverman, K.E.,
Beckman, M.E.,
Pitrelli, J.F.,
Ostendorf, M.,
Wightman, C.W.,
Price, P.,
Pierrehumbert, J.B.,
Hirschberg, J.:
Tobi: A standard for labeling english prosody.
In: ICSLP,
vol. 2,
pp. 867–870
(1992)


</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reed [2009]</span>
<span class="ltx_bibblock">
Reed, B.S.:
Units of interaction:“intonation phrases” or “turn constructional phrases”.
Actes/Proceedings from IDP (Interface Discours &amp; Prosodie),
351–363
(2009)


</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Degand and Simon [2005]</span>
<span class="ltx_bibblock">
Degand, L.,
Simon, A.C.:
Minimal discourse units: Can we define them, and why should we.
Proceedings of SEM-05. Connectors, discourse framing and discourse structure: from corpus-based and experimental analyses to discourse theories, Biarritz,
14–15
(2005)


</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su and Tseng [2018]</span>
<span class="ltx_bibblock">
Su, C.-y.,
Tseng, C.-y.:
Perceivable information structure in discourse prosody-detecting prominent prosodic words in spoken discourse using f0 contour.
In: 2018 11th International Symposium on Chinese Spoken Language Processing (ISCSLP),
pp. 424–428
(2018).
IEEE


</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hannay and Kroon [2005]</span>
<span class="ltx_bibblock">
Hannay, M.,
Kroon, C.:
Acts and the relationship between discourse and grammar.
Functions of language
<span id="bib.bib15.1.1" class="ltx_text ltx_font_bold">12</span>(1),
87–124
(2005)


</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Couper-Kuhlen and Selting [2017]</span>
<span class="ltx_bibblock">
Couper-Kuhlen, E.,
Selting, M.:
Interactional Linguistics: Studying Language in Social Interaction.
Cambridge University Press, ???
(2017)


</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jakobson [1984]</span>
<span class="ltx_bibblock">
Jakobson, R.:
Russian and Slavic Grammar: Studies, 1931-1981
vol. 106.
Walter de Gruyter, ???
(1984)


</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hockett [1960]</span>
<span class="ltx_bibblock">
Hockett, C.F.:
The origin of speech.
Scientific American
<span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">203</span>(3),
88–97
(1960)


</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jacobs et al. [2015]</span>
<span class="ltx_bibblock">
Jacobs, C.L.,
Yiu, L.K.,
Watson, D.G.,
Dell, G.S.:
Why are repeated words produced with reduced durations? evidence from inner speech and homophone production.
Journal of Memory and Language
<span id="bib.bib19.1.1" class="ltx_text ltx_font_bold">84</span>,
37–48
(2015)


</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hirose et al. [1984]</span>
<span class="ltx_bibblock">
Hirose, K.,
Fujisaki, H.,
Yamaguchi, M.:
Synthesis by rule of voice fundamental frequency contours of spoken japanese from linguistic information.
In: ICASSP’84. IEEE International Conference on Acoustics, Speech, and Signal Processing,
vol. 9,
pp. 597–600
(1984).
IEEE


</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glass [1995-present]</span>
<span class="ltx_bibblock">
Glass, I.:
This American Life.
Chicago Public Media.
[Online]. Available: <a target="_blank" href="https://www.thisamericanlife.org/archive" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.thisamericanlife.org/archive</a>
(1995-present)


</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Owens [2013]</span>
<span class="ltx_bibblock">
Owens, J.:
The arabic grammatical tradition.
The Semitic Languages
<span id="bib.bib22.1.1" class="ltx_text ltx_font_bold">46</span>
(2013)


</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schippers [1997]</span>
<span class="ltx_bibblock">
Schippers, A.:
The hebrew grammatical tradition.
The Semitic Languages,
59–65
(1997)


</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wagner and Watson [2010]</span>
<span class="ltx_bibblock">
Wagner, M.,
Watson, D.G.:
Experimental and theoretical advances in prosody: A review.
Language and cognitive processes
<span id="bib.bib24.1.1" class="ltx_text ltx_font_bold">25</span>(7-9),
905–945
(2010)


</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wennerstrom [2001]</span>
<span class="ltx_bibblock">
Wennerstrom, A.:
The Music of Everyday Speech: Prosody and Discourse Analysis.
Oxford University Press, ???
(2001)


</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Triantafyllopoulos et al. [2023]</span>
<span class="ltx_bibblock">
Triantafyllopoulos, A.,
Schuller, B.W.,
İymen, G.,
Sezgin, M.,
He, X.,
Yang, Z.,
Tzirakis, P.,
Liu, S.,
Mertes, S.,
André, E., et al.:
An overview of affective speech synthesis and conversion in the deep learning era.
Proceedings of the IEEE
(2023)


</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biron et al. [2021]</span>
<span class="ltx_bibblock">
Biron, T.,
Baum, D.,
Freche, D.,
Matalon, N.,
Ehrmann, N.,
Weinreb, E.,
Biron, D.,
Moses, E.:
Automatic detection of prosodic boundaries in spontaneous speech.
PloS one
<span id="bib.bib27.1.1" class="ltx_text ltx_font_bold">16</span>(5),
0250969
(2021)


</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rosenberg [2010]</span>
<span class="ltx_bibblock">
Rosenberg, A.:
Classification of prosodic events using quantized contour modeling.
In: Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,
pp. 721–724
(2010)


</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barbosa [2008]</span>
<span class="ltx_bibblock">
Barbosa, P.A.:
Prominence-and boundary-related acoustic correlations in brazilian portuguese read and spontaneous speech.
In: Proceedings of the Speech Prosody 2008 Conference,
pp. 257–260
(2008).
Citeseer


</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Calhoun et al. [2023]</span>
<span class="ltx_bibblock">
Calhoun, S.,
Yan, M.,
Salanoa, H.,
Taupi, F.,
Kruse Va’ai, E.:
Focus effects on immediate and delayed recognition of referents in samoan.
Language and Speech
<span id="bib.bib30.1.1" class="ltx_text ltx_font_bold">66</span>(1),
175–201
(2023)


</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sridhar et al. [2008]</span>
<span class="ltx_bibblock">
Sridhar, V.K.R.,
Bangalore, S.,
Narayanan, S.S.:
Exploiting acoustic and syntactic features for automatic prosody labeling in a maximum entropy framework.
IEEE transactions on audio, speech, and language processing
<span id="bib.bib31.1.1" class="ltx_text ltx_font_bold">16</span>(4),
797–811
(2008)


</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roll et al. [2023]</span>
<span class="ltx_bibblock">
Roll, N.,
Graham, C.,
Todd, S.:
Psst! prosodic speech segmentation with transformers.
arXiv preprint arXiv:2302.01984
(2023)


</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2023]</span>
<span class="ltx_bibblock">
Wu, Q.,
Liu, Y.,
Zhao, H.,
Kale, A.,
Bui, T.,
Yu, T.,
Lin, Z.,
Zhang, Y.,
Chang, S.:
Uncovering the disentanglement capability in text-to-image diffusion models.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 1900–1910
(2023)


</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De Saussure [1989]</span>
<span class="ltx_bibblock">
De Saussure, F.:
Cours de Linguistique Générale
vol. 1.
Otto Harrassowitz Verlag, ???
(1989)


</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hjelmslev and Whitfield [1953]</span>
<span class="ltx_bibblock">
Hjelmslev, L.,
Whitfield, F.J.:
Prolegomena to a theory of language
(1953)


</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Austin [1975]</span>
<span class="ltx_bibblock">
Austin, J.L.:
How to do Things with Words
vol. 88.
Oxford university press, ???
(1975)


</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chomsky [2002]</span>
<span class="ltx_bibblock">
Chomsky, N.:
Syntactic Structures.
Mouton de Gruyter, ???
(2002)


</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caroll et al. [1998]</span>
<span class="ltx_bibblock">
Caroll, J.,
Briscoe, T.,
Sanfilippo, A.:
Parser evaluation: a survey and a new proposal.
In: LREC,
vol. 998,
pp. 447–454
(1998)


</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington et al. [2014]</span>
<span class="ltx_bibblock">
Pennington, J.,
Socher, R.,
Manning, C.D.:
Glove: Global vectors for word representation.
In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),
pp. 1532–1543
(2014)


</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Behre et al. [2023]</span>
<span class="ltx_bibblock">
Behre, P.,
Tan, S.,
Varadharajan, P.,
Chang, S.:
Streaming punctuation: A novel punctuation technique leveraging bidirectional context for continuous speech recognition.
arXiv preprint arXiv:2301.03819
(2023)


</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim [2014]</span>
<span class="ltx_bibblock">
Kim, Y.:
Convolutional neural networks for sentence classification.
In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)
(2014).
Association for Computational Linguistics


</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. [2017]</span>
<span class="ltx_bibblock">
Yin, W.,
Kann, K.,
Yu, M.,
Schütze, H.:
Comparative study of cnn and rnn for natural language processing.
arXiv preprint arXiv:1702.01923
(2017)


</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau et al. [2014]</span>
<span class="ltx_bibblock">
Bahdanau, D.,
Cho, K.,
Bengio, Y.:
Neural machine translation by jointly learning to align and translate.
arXiv preprint arXiv:1409.0473
(2014)


</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Driess et al. [2023]</span>
<span class="ltx_bibblock">
Driess, D.,
Xia, F.,
Sajjadi, M.S.,
Lynch, C.,
Chowdhery, A.,
Ichter, B.,
Wahid, A.,
Tompson, J.,
Vuong, Q.,
Yu, T., et al.:
Palm-e: An embodied multimodal language model.
arXiv preprint arXiv:2303.03378
(2023)


</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. [2023]</span>
<span class="ltx_bibblock">
Peng, C.,
Chen, K.,
Shou, L.,
Chen, G.:
Carat: Contrastive feature reconstruction and aggregation for multi-modal multi-label emotion recognition.
arXiv preprint arXiv:2312.10201
(2023)


</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023]</span>
<span class="ltx_bibblock">
Li, Y.,
Du, H.,
Ni, Y.,
Zhao, P.,
Guo, Q.,
Yuan, F.,
Zhou, X.:
Multi-modality is all you need for transferable recommender systems.
arXiv preprint arXiv:2312.09602
(2023)


</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin [2005]</span>
<span class="ltx_bibblock">
Devlin, K.:
Confronting context effects in intelligence analysis: How can mathematics help.
Center for the Study of Language and Information, Stanford University
(2005)


</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Matalon [2021]</span>
<span class="ltx_bibblock">
Matalon, N.:
The camel humps prosodic pattern.
Building categories in interaction: Linguistic resources at work
<span id="bib.bib48.1.1" class="ltx_text ltx_font_bold">220</span>,
155
(2021)


</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Matalon et al. [Under revision]</span>
<span class="ltx_bibblock">
Matalon, N.,
Weinreb, E.,
Freche, D.,
Volk, E.,
Biron, T.,
Moses, E.,
Biron, D.:
Structure in Conversational Prosody: Evidence for Vocabulary, Semantics and Syntax of Intonation Units.
Under revision
(Under revision)


</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weinrich [2024]</span>
<span class="ltx_bibblock">
Weinrich, H.:
Tempus: The World of Discussion and the World of Narration.
Fordham Univ Press, ???
(2024)


</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shisha-Halevy [2005]</span>
<span class="ltx_bibblock">
Shisha-Halevy, A.:
Epistolary grammar: Syntactical highlights in kate roberts’s correspondence with saunders lewis.
Journal of Celtic Linguistics
<span id="bib.bib51.1.1" class="ltx_text ltx_font_bold">9</span>(1),
83–103
(2005)


</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shisha-Halevy [2007]</span>
<span class="ltx_bibblock">
Shisha-Halevy, A.:
Converbs in welsh and irish.
In: 13th International Congress of Celtic Studies,
Bonn
(2007).
Conference Paper


</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Couper-Kuhlen [2015]</span>
<span class="ltx_bibblock">
Couper-Kuhlen, E.:
Intonation and discourse.
The handbook of discourse analysis,
82–104
(2015)


</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Couper-Kuhlen [1986]</span>
<span class="ltx_bibblock">
Couper-Kuhlen, E.:
An Introduction to English Prosody.
TUEBINGEN, ???
(1986)


</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Selting et al. [2010]</span>
<span class="ltx_bibblock">
Selting, M.,
Barth-Weingarten, D.,
Reber, E.,
Selting, M.:
Prosody in interaction.
Prosody in Interaction, Amsterdam/Philadelphia, John Benjamins,
3–40
(2010)


</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dogil [2003]</span>
<span class="ltx_bibblock">
Dogil, G.:
Understanding prosody.
In: Rickheit, G.,
Herrmann, T.,
Deutsch, W. (eds.)
Psycholinguistics: Ein Internationales Handbuch,
pp. 544–565.
De Gruyter Mouton,
Berlin • New York
(2003).
<a target="_blank" href="https://doi.org/10.1515/9783110114249.4.544" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1515/9783110114249.4.544</a>


</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cornille et al. [2022]</span>
<span class="ltx_bibblock">
Cornille, T.,
Wang, F.,
Bekker, J.:
Interactive multi-level prosody control for expressive speech synthesis.
In: ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pp. 8312–8316
(2022).
IEEE


</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cenceschi et al. [2021]</span>
<span class="ltx_bibblock">
Cenceschi, S.,
Sbattella, L.,
Tedesco, R.:
Calliope: A multi-dimensional model for the prosodic characterization of information units.
Estudios de fonética experimental,
227–245
(2021)


</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du Bois et al. [2000]</span>
<span class="ltx_bibblock">
Du Bois, J.W.,
Chafe, W.L.,
Meyer, C.,
Thompson, S.A.,
Martey, N.:
Santa barbara corpus of spoken american english.
CD-ROM. Philadelphia: Linguistic Data Consortium
(2000)


</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hashem et al. [2023]</span>
<span class="ltx_bibblock">
Hashem, A.,
Arif, M.,
Alghamdi, M.:
Speech emotion recognition approaches: A systematic review.
Speech Communication,
102974
(2023)


</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Klie et al. [2018]</span>
<span class="ltx_bibblock">
Klie, J.-C.,
Bugert, M.,
Boullosa, B.,
Castilho, R.E.,
Gurevych, I.:
The inception platform: Machine-assisted and knowledge-oriented interactive annotation.
In: Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations,
pp. 5–9
(2018)


</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Honnibal et al. [2020]</span>
<span class="ltx_bibblock">
Honnibal, M.,
Montani, I.,
Landeghem, S.V.,
Boyd, A.:
spaCy: Industrial-strength Natural Language Processing in Python.
Available online.
Accessed: 2023-01-14
(2020)


</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hennequin et al. [2020]</span>
<span class="ltx_bibblock">
Hennequin, R.,
Khlif, A.,
Voituret, F.,
Moussallam, M.:
Spleeter: a fast and efficient music source separation tool with pre-trained models.
Journal of Open Source Software
<span id="bib.bib63.1.1" class="ltx_text ltx_font_bold">5</span>(50),
2154
(2020)


</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McAuliffe et al. [2017]</span>
<span class="ltx_bibblock">
McAuliffe, M.,
Socolof, M.,
Mihuc, S.,
Wagner, M.,
Sonderegger, M.:
Montreal forced aligner: Trainable text-speech alignment using kaldi.
In: Interspeech,
vol. 2017,
pp. 498–502
(2017)


</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodwin and Heritage [1990]</span>
<span class="ltx_bibblock">
Goodwin, C.,
Heritage, J.:
Conversation analysis.
Annual review of anthropology
<span id="bib.bib65.1.1" class="ltx_text ltx_font_bold">19</span>(1),
283–307
(1990)


</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. [2019]</span>
<span class="ltx_bibblock">
Wolf, T.,
Debut, L.,
Sanh, V.,
Chaumond, J.,
Delangue, C.,
Moi, A.,
Cistac, P.,
Rault, T.,
Louf, R.,
Funtowicz, M., et al.:
Huggingface’s transformers: State-of-the-art natural language processing.
arXiv preprint arXiv:1910.03771
(2019)


</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Breen et al. [2012]</span>
<span class="ltx_bibblock">
Breen, M.,
Dilley, L.C.,
Kraemer, J.,
Gibson, E.:
Inter-transcriber reliability for two systems of prosodic annotation: Tobi (tones and break indices) and rap (rhythm and pitch).
Corpus linguistics and linguistic theory
<span id="bib.bib67.1.1" class="ltx_text ltx_font_bold">8</span>(2),
277–312
(2012)


</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belinkov and Glass [2017]</span>
<span class="ltx_bibblock">
Belinkov, Y.,
Glass, J.:
Analyzing hidden representations in end-to-end automatic speech recognition systems.
Advances in Neural Information Processing Systems
<span id="bib.bib68.1.1" class="ltx_text ltx_font_bold">30</span>
(2017)


</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.03521" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.03522" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.03522">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.03522" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.03523" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 14:42:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
