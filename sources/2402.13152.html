<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.13152] AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies</title><meta property="og:description" content="More than 7,000 known languages are spoken around the world. However, due to the lack of annotated resources, only a small fraction of them are currently covered by speech technologies. Albeit self-supervised speech re…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.13152">

<!--Generated on Tue Mar  5 19:01:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">AnnoTheia: A Semi-Automatic Annotation Toolkit
<br class="ltx_break">for Audio-Visual Speech Technologies</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">More than 7,000 known languages are spoken around the world. However, due to the lack of annotated resources, only a small fraction of them are currently covered by speech technologies. Albeit self-supervised speech representations, recent massive speech corpora collections, as well as the organization of challenges, have alleviated this inequality, most studies are mainly benchmarked on English. This situation is aggravated when tasks involving both acoustic and visual speech modalities are addressed. In order to promote research on low-resource languages for audio-visual speech technologies, we present AnnoTheia, a semi-automatic annotation toolkit that detects when a person speaks on the scene and the corresponding transcription. In addition, to show the complete process of preparing AnnoTheia for a language of interest, we also describe the adaptation of a pre-trained model for active speaker detection to Spanish, using a database not initially conceived for this type of task. The AnnoTheia toolkit, tutorials, and pre-trained models are available at <a target="_blank" href="https://github.com/joactr/AnnoTheia/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/joactr/AnnoTheia/</a>.

<br class="ltx_break">
<br class="ltx_break">
<span id="id2.id1.1" class="ltx_text ltx_font_bold">Keywords: </span>data annotation, speech technologies, audio-visual databases, active speaker detection</p>
</div>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\newcites</span>
<p id="p1.2" class="ltx_p">languageresourceLanguage Resources








</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text"></span></p>
</div>
<div id="id1" class="ltx_logical-block">
<div id="id1.p1" class="ltx_para">
<p id="id1.p1.1" class="ltx_p ltx_align_center"><span id="id1.p1.1.1" class="ltx_text ltx_font_bold" style="font-size:144%;">AnnoTheia: A Semi-Automatic Annotation Toolkit
<br class="ltx_break">for Audio-Visual Speech Technologies</span></p>
<br class="ltx_break ltx_centering">
<table id="id1.p1.2" class="ltx_tabular ltx_centering ltx_align_top">
<tbody class="ltx_tbody">
<tr id="id1.p1.2.1.1" class="ltx_tr">
<td id="id1.p1.2.1.1.1" class="ltx_td ltx_align_center"><span id="id1.p1.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:120%;">José-M. Acosta-Triana<sup id="id1.p1.2.1.1.1.1.1" class="ltx_sup">1</sup>, David Gimeno-Gómez<sup id="id1.p1.2.1.1.1.1.2" class="ltx_sup">2</sup>, Carlos-D. Martínez-Hinarejos<sup id="id1.p1.2.1.1.1.1.3" class="ltx_sup">2</sup></span></td>
</tr>
<tr id="id1.p1.2.2.2" class="ltx_tr">
<td id="id1.p1.2.2.2.1" class="ltx_td ltx_align_center">
<sup id="id1.p1.2.2.2.1.1" class="ltx_sup">1</sup> ValgrAI - Valencian Graduate School and Research Network of Artificial Intelligence</td>
</tr>
<tr id="id1.p1.2.3.3" class="ltx_tr">
<td id="id1.p1.2.3.3.1" class="ltx_td ltx_align_center">Camino de Vera, s/n, 3Q Building, 46022, València, Spain</td>
</tr>
<tr id="id1.p1.2.4.4" class="ltx_tr">
<td id="id1.p1.2.4.4.1" class="ltx_td ltx_align_center">
<sup id="id1.p1.2.4.4.1.1" class="ltx_sup">2</sup> Pattern Recognition and Human Language Technologies Research Center</td>
</tr>
<tr id="id1.p1.2.5.5" class="ltx_tr">
<td id="id1.p1.2.5.5.1" class="ltx_td ltx_align_center">Universitat Politècnica de València, Camino de Vera, s/n, 46022, València, Spain</td>
</tr>
<tr id="id1.p1.2.6.6" class="ltx_tr">
<td id="id1.p1.2.6.6.1" class="ltx_td ltx_align_center"><span id="id1.p1.2.6.6.1.1" class="ltx_text ltx_font_typewriter">joactr@inf.upv.es, dagigo1@dsic.upv.es, cmartine@dsic.upv.es</span></td>
</tr>
</tbody>
</table>
<p id="id1.p1.3" class="ltx_p ltx_align_center"><span id="id1.p1.3.1" class="ltx_text ltx_font_italic">Abstract content</span></p>
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.   Introduction</h2>

<figure id="S1.T1" class="ltx_table">
<div id="S1.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:455.2pt;height:152.3pt;vertical-align:-27.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-43.5pt,11.9pt) scale(0.8397,0.8397) ;">
<p id="S1.T1.1.1" class="ltx_p"><span id="S1.T1.1.1.1" class="ltx_text" style="font-size:90%;">
<span id="S1.T1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:542.2pt;height:181.4pt;vertical-align:-32.9pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S1.T1.1.1.1.1.1" class="ltx_p"><span id="S1.T1.1.1.1.1.1.1" class="ltx_text">

<span id="S1.T1.1.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S1.T1.1.1.1.1.1.1.1.2.1" class="ltx_tr">
<span id="S1.T1.1.1.1.1.1.1.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"></span>
<span id="S1.T1.1.1.1.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S1.T1.1.1.1.1.1.1.1.2.1.2.1" class="ltx_text ltx_font_bold">English</span></span>
<span id="S1.T1.1.1.1.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S1.T1.1.1.1.1.1.1.1.2.1.3.1" class="ltx_text ltx_font_bold">Persian</span></span>
<span id="S1.T1.1.1.1.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S1.T1.1.1.1.1.1.1.1.2.1.4.1" class="ltx_text ltx_font_bold">Spanish</span></span>
<span id="S1.T1.1.1.1.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S1.T1.1.1.1.1.1.1.1.2.1.5.1" class="ltx_text ltx_font_bold">French</span></span>
<span id="S1.T1.1.1.1.1.1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S1.T1.1.1.1.1.1.1.1.2.1.6.1" class="ltx_text ltx_font_bold">Portuguese</span></span>
<span id="S1.T1.1.1.1.1.1.1.1.2.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S1.T1.1.1.1.1.1.1.1.2.1.7.1" class="ltx_text ltx_font_bold">Italian</span></span>
<span id="S1.T1.1.1.1.1.1.1.1.2.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S1.T1.1.1.1.1.1.1.1.2.1.8.1" class="ltx_text ltx_font_bold">Chinese</span></span>
<span id="S1.T1.1.1.1.1.1.1.1.2.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S1.T1.1.1.1.1.1.1.1.2.1.9.1" class="ltx_text ltx_font_bold">Russian</span></span>
<span id="S1.T1.1.1.1.1.1.1.1.2.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S1.T1.1.1.1.1.1.1.1.2.1.10.1" class="ltx_text ltx_font_bold">German</span></span>
<span id="S1.T1.1.1.1.1.1.1.1.2.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S1.T1.1.1.1.1.1.1.1.2.1.11.1" class="ltx_text ltx_font_bold">Greek</span></span>
<span id="S1.T1.1.1.1.1.1.1.1.2.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S1.T1.1.1.1.1.1.1.1.2.1.12.1" class="ltx_text ltx_font_bold">Arabic</span></span></span>
</span>
<span class="ltx_tbody">
<span id="S1.T1.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S1.T1.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S1.T1.1.1.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">MuAViC<sup id="S1.T1.1.1.1.1.1.1.1.1.2.1.1" class="ltx_sup"><span id="S1.T1.1.1.1.1.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_medium">1</span></sup></span></span>
<span id="S1.T1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">438.9<sup id="S1.T1.1.1.1.1.1.1.1.1.1.1" class="ltx_sup"><sup id="S1.T1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_sup">†</sup></sup></span>
<span id="S1.T1.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">181.0</span>
<span id="S1.T1.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">179.0</span>
<span id="S1.T1.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">156.0</span>
<span id="S1.T1.1.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">104.0</span>
<span id="S1.T1.1.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">52.0</span>
<span id="S1.T1.1.1.1.1.1.1.1.1.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">13.0</span>
<span id="S1.T1.1.1.1.1.1.1.1.1.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">29.0</span>
<span id="S1.T1.1.1.1.1.1.1.1.1.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">19.0</span></span>
<span id="S1.T1.1.1.1.1.1.1.1.3.1" class="ltx_tr">
<span id="S1.T1.1.1.1.1.1.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S1.T1.1.1.1.1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">CMU-MOSEAS<sup id="S1.T1.1.1.1.1.1.1.1.3.1.1.1.1" class="ltx_sup"><span id="S1.T1.1.1.1.1.1.1.1.3.1.1.1.1.1" class="ltx_text ltx_font_medium">2</span></sup></span></span>
<span id="S1.T1.1.1.1.1.1.1.1.3.1.2" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.3.1.3" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.3.1.4" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">16.3</span>
<span id="S1.T1.1.1.1.1.1.1.1.3.1.5" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">15.6</span>
<span id="S1.T1.1.1.1.1.1.1.1.3.1.6" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">16.0</span>
<span id="S1.T1.1.1.1.1.1.1.1.3.1.7" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.3.1.8" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.3.1.9" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.3.1.10" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">18.6</span>
<span id="S1.T1.1.1.1.1.1.1.1.3.1.11" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.3.1.12" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></span></span>
<span id="S1.T1.1.1.1.1.1.1.1.4.2" class="ltx_tr">
<span id="S1.T1.1.1.1.1.1.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S1.T1.1.1.1.1.1.1.1.4.2.1.1" class="ltx_text ltx_font_bold">LRS2-BBC<sup id="S1.T1.1.1.1.1.1.1.1.4.2.1.1.1" class="ltx_sup"><span id="S1.T1.1.1.1.1.1.1.1.4.2.1.1.1.1" class="ltx_text ltx_font_medium">3</span></sup></span></span>
<span id="S1.T1.1.1.1.1.1.1.1.4.2.2" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">224.5</span>
<span id="S1.T1.1.1.1.1.1.1.1.4.2.3" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.4.2.4" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.4.2.5" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.4.2.6" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.4.2.7" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.4.2.8" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.4.2.9" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.4.2.10" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.4.2.11" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.4.2.12" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span></span>
<span id="S1.T1.1.1.1.1.1.1.1.5.3" class="ltx_tr">
<span id="S1.T1.1.1.1.1.1.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S1.T1.1.1.1.1.1.1.1.5.3.1.1" class="ltx_text ltx_font_bold">LRS3-TED<sup id="S1.T1.1.1.1.1.1.1.1.5.3.1.1.1" class="ltx_sup"><span id="S1.T1.1.1.1.1.1.1.1.5.3.1.1.1.1" class="ltx_text ltx_font_medium">4</span></sup></span></span>
<span id="S1.T1.1.1.1.1.1.1.1.5.3.2" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">438.9</span>
<span id="S1.T1.1.1.1.1.1.1.1.5.3.3" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.5.3.4" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.5.3.5" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.5.3.6" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.5.3.7" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.5.3.8" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.5.3.9" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.5.3.10" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.5.3.11" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.5.3.12" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span></span>
<span id="S1.T1.1.1.1.1.1.1.1.6.4" class="ltx_tr">
<span id="S1.T1.1.1.1.1.1.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S1.T1.1.1.1.1.1.1.1.6.4.1.1" class="ltx_text ltx_font_bold">CMLR<sup id="S1.T1.1.1.1.1.1.1.1.6.4.1.1.1" class="ltx_sup"><span id="S1.T1.1.1.1.1.1.1.1.6.4.1.1.1.1" class="ltx_text ltx_font_medium">5</span></sup></span></span>
<span id="S1.T1.1.1.1.1.1.1.1.6.4.2" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.6.4.3" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.6.4.4" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.6.4.5" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.6.4.6" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.6.4.7" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.6.4.8" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">86.5</span>
<span id="S1.T1.1.1.1.1.1.1.1.6.4.9" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.6.4.10" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.6.4.11" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.6.4.12" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span></span>
<span id="S1.T1.1.1.1.1.1.1.1.7.5" class="ltx_tr">
<span id="S1.T1.1.1.1.1.1.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S1.T1.1.1.1.1.1.1.1.7.5.1.1" class="ltx_text ltx_font_bold">Arman-AV<sup id="S1.T1.1.1.1.1.1.1.1.7.5.1.1.1" class="ltx_sup"><span id="S1.T1.1.1.1.1.1.1.1.7.5.1.1.1.1" class="ltx_text ltx_font_medium">6</span></sup></span></span>
<span id="S1.T1.1.1.1.1.1.1.1.7.5.2" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.7.5.3" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">220.0</span>
<span id="S1.T1.1.1.1.1.1.1.1.7.5.4" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.7.5.5" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.7.5.6" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.7.5.7" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.7.5.8" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.7.5.9" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.7.5.10" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.7.5.11" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.7.5.12" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span></span>
<span id="S1.T1.1.1.1.1.1.1.1.8.6" class="ltx_tr">
<span id="S1.T1.1.1.1.1.1.1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S1.T1.1.1.1.1.1.1.1.8.6.1.1" class="ltx_text ltx_font_bold">LIP-RTVE<sup id="S1.T1.1.1.1.1.1.1.1.8.6.1.1.1" class="ltx_sup"><span id="S1.T1.1.1.1.1.1.1.1.8.6.1.1.1.1" class="ltx_text ltx_font_medium">7</span></sup></span></span>
<span id="S1.T1.1.1.1.1.1.1.1.8.6.2" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.8.6.3" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.8.6.4" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">13.0</span>
<span id="S1.T1.1.1.1.1.1.1.1.8.6.5" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.8.6.6" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.8.6.7" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.8.6.8" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.8.6.9" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.8.6.10" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.8.6.11" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span>
<span id="S1.T1.1.1.1.1.1.1.1.8.6.12" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;">-</span></span>
<span id="S1.T1.1.1.1.1.1.1.1.9.7" class="ltx_tr">
<span id="S1.T1.1.1.1.1.1.1.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S1.T1.1.1.1.1.1.1.1.9.7.1.1" class="ltx_text ltx_font_bold">Total of Hours</span></span>
<span id="S1.T1.1.1.1.1.1.1.1.9.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">663.4</span>
<span id="S1.T1.1.1.1.1.1.1.1.9.7.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">220.0</span>
<span id="S1.T1.1.1.1.1.1.1.1.9.7.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">210.3</span>
<span id="S1.T1.1.1.1.1.1.1.1.9.7.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">194.6</span>
<span id="S1.T1.1.1.1.1.1.1.1.9.7.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">172.0</span>
<span id="S1.T1.1.1.1.1.1.1.1.9.7.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">104.0</span>
<span id="S1.T1.1.1.1.1.1.1.1.9.7.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">86.5</span>
<span id="S1.T1.1.1.1.1.1.1.1.9.7.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">52.0</span>
<span id="S1.T1.1.1.1.1.1.1.1.9.7.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">31.6</span>
<span id="S1.T1.1.1.1.1.1.1.1.9.7.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">29.0</span>
<span id="S1.T1.1.1.1.1.1.1.1.9.7.12" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">19.0</span></span>
</span>
</span>

<span id="S1.I1" class="ltx_itemize">
<span id="S1.I1.ix1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item"><span id="S1.I1.ix1.1.1.1" class="ltx_text" style="font-size:111%;">1</span></span> 
<span id="S1.I1.ix1.p1" class="ltx_para">
<span id="S1.I1.ix1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Anwar et al., 2023</a>]</cite></span>
</span></span>
<span id="S1.I1.ix2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item"><span id="S1.I1.ix2.1.1.1" class="ltx_text" style="font-size:111%;">2</span></span> 
<span id="S1.I1.ix2.p1" class="ltx_para">
<span id="S1.I1.ix2.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx64" title="" class="ltx_ref">Zadeh et al., 2020</a>]</cite></span>
</span></span>
<span id="S1.I1.ix3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item"><span id="S1.I1.ix3.1.1.1" class="ltx_text" style="font-size:111%;">3</span></span> 
<span id="S1.I1.ix3.p1" class="ltx_para">
<span id="S1.I1.ix3.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Afouras et al., 2018b</a>]</cite></span>
</span></span>
<span id="S1.I1.ix4" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item"><span id="S1.I1.ix4.1.1.1" class="ltx_text" style="font-size:111%;">4</span></span> 
<span id="S1.I1.ix4.p1" class="ltx_para">
<span id="S1.I1.ix4.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Afouras et al., 2018a</a>]</cite></span>
</span></span>
<span id="S1.I1.ix5" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item"><span id="S1.I1.ix5.1.1.1" class="ltx_text" style="font-size:111%;">5</span></span> 
<span id="S1.I1.ix5.p1" class="ltx_para">
<span id="S1.I1.ix5.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx66" title="" class="ltx_ref">Zhao et al., 2020</a>]</cite></span>
</span></span>
<span id="S1.I1.ix6" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item"><span id="S1.I1.ix6.1.1.1" class="ltx_text" style="font-size:111%;">6</span></span> 
<span id="S1.I1.ix6.p1" class="ltx_para">
<span id="S1.I1.ix6.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx50" title="" class="ltx_ref">Peymanfard et al., 2023</a>]</cite></span>
</span></span>
<span id="S1.I1.ix7" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item"><span id="S1.I1.ix7.1.1.1" class="ltx_text" style="font-size:111%;">7</span></span> 
<span id="S1.I1.ix7.p1" class="ltx_para">
<span id="S1.I1.ix7.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">Gimeno-Gómez and Martínez-Hinarejos, 2022</a>]</cite></span>
</span></span>
<span id="S1.I1.ix8" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item"><math id="S1.I1.ix8.1.1.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S1.I1.ix8.1.1.m1.1b"><mo mathsize="111%" id="S1.I1.ix8.1.1.m1.1.1" xref="S1.I1.ix8.1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S1.I1.ix8.1.1.m1.1c"><ci id="S1.I1.ix8.1.1.m1.1.1.cmml" xref="S1.I1.ix8.1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.ix8.1.1.m1.1d">\dagger</annotation></semantics></math></span> 
<span id="S1.I1.ix8.p1" class="ltx_para ltx_noindent">
<span id="S1.I1.ix8.p1.1" class="ltx_p">The MuAViC database was sourced from TED and TEDx talks: thus, it includes directly the LRS3-TED corpus.</span>
</span></span>
</span></span></span>
</span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Language coverage in terms of hours by databases focused on unconstrained continuous audio-visual speech recognition. Corpora oriented to word-level classification, collected in controlled recording studios, or not providing transcriptions were not considered.</figcaption>
</figure>
<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Speech technologies aim to develop tools capable of enabling human-machine interaction via different types of communication. Multiple research areas are included in this vast field, e.g., speech recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Baevski et al., 2020</a>, <a href="#bib.bibx53" title="" class="ltx_ref">Radford et al., 2023</a>, <a href="#bib.bibx58" title="" class="ltx_ref">Shi et al., 2022</a>, <a href="#bib.bibx44" title="" class="ltx_ref">Ma et al., 2021</a>]</cite>; speech synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx57" title="" class="ltx_ref">Shen et al., 2018</a>, <a href="#bib.bibx41" title="" class="ltx_ref">Li et al., 2019b</a>, <a href="#bib.bibx43" title="" class="ltx_ref">Liu et al., 2023</a>]</cite>; speech translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx35" title="" class="ltx_ref">Jia et al., 2019</a>, <a href="#bib.bibx39" title="" class="ltx_ref">Lee et al., 2022</a>, <a href="#bib.bibx34" title="" class="ltx_ref">Huang et al., 2023</a>]</cite>; sign language recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">Koller et al., 2015</a>, <a href="#bib.bibx16" title="" class="ltx_ref">Cihan Camgöz et al., 2020</a>, <a href="#bib.bibx4" title="" class="ltx_ref">Albanie et al., 2021</a>]</cite>; silent speech interfaces <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Denby et al., 2010</a>, <a href="#bib.bibx28" title="" class="ltx_ref">González-López et al., 2020</a>]</cite>; speech-based disease detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Ablimit et al., 2022</a>, <a href="#bib.bibx46" title="" class="ltx_ref">Milling et al., 2022</a>]</cite>; speech enhancement and separation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx49" title="" class="ltx_ref">Pascual et al., 2017</a>, <a href="#bib.bibx63" title="" class="ltx_ref">Yen et al., 2023</a>, <a href="#bib.bibx33" title="" class="ltx_ref">Huang et al., 2022</a>]</cite>; speaker diarization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Fujita et al., 2019</a>, <a href="#bib.bibx9" title="" class="ltx_ref">Bredin and Laurent, 2021</a>]</cite>; spoken dialogue systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx36" title="" class="ltx_ref">Jokinen and McTear, 2022</a>]</cite>. All these studies reveal the importance that speech technologies and their applications can represent in our daily lives.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">One of the most widely studied tasks in the field is automatic speech recognition. However, although more than 7,000 known languages are spoken around the world <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Eberhard et al., 2023</a>]</cite>, only a small fraction of them are currently covered by these speech technologies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx52" title="" class="ltx_ref">Pratap et al., 2023</a>]</cite>. Taking into account that many of these languages are considered endangered <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">Bromham et al., 2022</a>]</cite>, the lack of support from current technologies may contribute negatively to this situation. For this reason, to avoid any type of discrimination or exclusion, different works have promoted scaling speech technologies to cover the greatest possible number of languages. Massive multi-lingual speech corpora collections <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">Ardila et al., 2020</a>, <a href="#bib.bibx56" title="" class="ltx_ref">Salesky et al., 2021</a>, <a href="#bib.bibx5" title="" class="ltx_ref">Anwar et al., 2023</a>, <a href="#bib.bibx52" title="" class="ltx_ref">Pratap et al., 2023</a>]</cite>, large multi-lingual speech recognition models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx53" title="" class="ltx_ref">Radford et al., 2023</a>, <a href="#bib.bibx52" title="" class="ltx_ref">Pratap et al., 2023</a>, <a href="#bib.bibx65" title="" class="ltx_ref">Zhang et al., 2023</a>, <a href="#bib.bibx14" title="" class="ltx_ref">Chen et al., 2023</a>]</cite>, self-supervised speech representations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Baevski et al., 2020</a>, <a href="#bib.bibx54" title="" class="ltx_ref">Ravanelli et al., 2020</a>, <a href="#bib.bibx7" title="" class="ltx_ref">Babu et al., 2022</a>, <a href="#bib.bibx58" title="" class="ltx_ref">Shi et al., 2022</a>]</cite>, as well as the organization of challenges, such as the ML-SUPERB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx59" title="" class="ltx_ref">Shi et al., 2023</a>]</cite>, have been some of the most common approaches to alleviate this inequality. Nevertheless, despite all these efforts, most studies are still benchmarked on English <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx52" title="" class="ltx_ref">Pratap et al., 2023</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">In addition, the described approaches mainly focused on auditory-based speech recognition, contrasting with the recent trend towards developing audio-visual speech technologies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx44" title="" class="ltx_ref">Ma et al., 2021</a>, <a href="#bib.bibx58" title="" class="ltx_ref">Shi et al., 2022</a>, <a href="#bib.bibx43" title="" class="ltx_ref">Liu et al., 2023</a>]</cite>. Tasks such as automatic lipreading <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Afouras et al., 2018b</a>, <a href="#bib.bibx45" title="" class="ltx_ref">Ma et al., 2022</a>, <a href="#bib.bibx51" title="" class="ltx_ref">Prajwal et al., 2022</a>]</cite>, audio-visual speech recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx44" title="" class="ltx_ref">Ma et al., 2021</a>, <a href="#bib.bibx58" title="" class="ltx_ref">Shi et al., 2022</a>, <a href="#bib.bibx13" title="" class="ltx_ref">Burchi and Timofte, 2023</a>]</cite>, or audio-visual speech synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx43" title="" class="ltx_ref">Liu et al., 2023</a>]</cite> have received increasing interest in the last decades. However, the scarcity of data for low-resource languages is exacerbated when this type of tasks is addressed due to the difficulty of collecting data where audio and video are aligned. Table <a href="#S1.T1" title="Table 1 ‣ 1. Introduction ‣ AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> reflects the limited number of languages covered by the audio-visual corpora that have been publicly released for unconstrained audio-visual speech recognition. Corpora oriented to word-level classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">Chung and Zisserman, 2017</a>, <a href="#bib.bibx62" title="" class="ltx_ref">Yang et al., 2019</a>, <a href="#bib.bibx21" title="" class="ltx_ref">Egorov et al., 2021</a>]</cite> or collected in controlled recording studios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">Harte and Gillen, 2015</a>, <a href="#bib.bibx24" title="" class="ltx_ref">Fernandez-Lopez et al., 2017</a>]</cite> were not considered, since they do not represent the actual nature of speech<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>A more comprehensive review of existing audio-visual databases can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">Fernandez-Lopez and Sukno, 2018</a>]</cite>.</span></span></span>.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">A special case is the AV-Speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Ephrat et al., 2018</a>]</cite>, a large-scale database offering over 4k hours of data for multiple languages, which was also not considered because it does not provide transcriptions. It should be noted that this corpus could be used for pre-training multilingual encoders in a self-supervised manner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx58" title="" class="ltx_ref">Shi et al., 2022</a>]</cite>. However, the current lack of audio-visual language resources would impede a more effective use of this technology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Anwar et al., 2023</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">Despite all these efforts in the audio-visual domain, there is no comparison to large-scale audio-based speech databases that cover more than a thousand languages with thousands of hours of data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx52" title="" class="ltx_ref">Pratap et al., 2023</a>, <a href="#bib.bibx65" title="" class="ltx_ref">Zhang et al., 2023</a>]</cite>. Besides, the inequality in terms of the number of hours among the different languages reflected in Table <a href="#S1.T1" title="Table 1 ‣ 1. Introduction ‣ AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and the fact that most studies based on audio-visual technologies are mainly benchmarked on English, highlights the need to promote and encourage the collection and annotation of new audio-visual databases to cover the largest possible number of languages.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">For this reason, numerous works have described multi-step pipelines to collect and annotate their own audio-visual databases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Afouras et al., 2018b</a>, <a href="#bib.bibx2" title="" class="ltx_ref">Afouras et al., 2018a</a>, <a href="#bib.bibx50" title="" class="ltx_ref">Peymanfard et al., 2023</a>, <a href="#bib.bibx22" title="" class="ltx_ref">Ephrat et al., 2018</a>]</cite>. However, while building these pipelines is not trivial, to the best of our knowledge, no toolkit has been publicly released for the research community. In addition, one of the most important components of these pipelines is the Active Speaker Detection (ASD) module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx60" title="" class="ltx_ref">Tao et al., 2021</a>, <a href="#bib.bibx42" title="" class="ltx_ref">Liao et al., 2023</a>, <a href="#bib.bibx47" title="" class="ltx_ref">Min et al., 2022</a>]</cite>, which allows us to identify the person on the scene who is talking. Nevertheless, these ASD models are also heavily benchmarked on English <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx60" title="" class="ltx_ref">Tao et al., 2021</a>, <a href="#bib.bibx42" title="" class="ltx_ref">Liao et al., 2023</a>, <a href="#bib.bibx47" title="" class="ltx_ref">Min et al., 2022</a>]</cite>.</p>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.1" class="ltx_p">Motivated by this fact, we present AnnoTheia, a semi-automatic toolkit that detects when a person speaks on the scene and the corresponding transcription. One of the most notable aspects of the proposed toolkit is the flexibility to replace a module with another of our preference or, where appropriate, adapted to our language of interest. Therefore, to show the complete process of preparing AnnoTheia for a language of interest, we also describe in this paper the adaptation of a pre-trained ASD model to Spanish, using a database not initially conceived for this type of task.</p>
</div>
<div id="S1.p8" class="ltx_para ltx_noindent">
<p id="S1.p8.1" class="ltx_p">These were the main reasons that motivated our work, whose key contributions are:</p>
</div>
<div id="S1.p9" class="ltx_para ltx_noindent">
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i1.p1" class="ltx_para ltx_noindent">
<p id="S1.I2.i1.p1.1" class="ltx_p">We present AnnoTheia, a semi-automatic annotation toolkit to encourage the largest possible language coverage for audio-visual speech technologies, highlighting its intuitive user interface and its flexibility to replace the different modules that composed it.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i2.p1" class="ltx_para ltx_noindent">
<p id="S1.I2.i2.p1.1" class="ltx_p">We described how to adapt a pre-trained ASD model to a language of interest, even in those occasions where there are no databases originally conceived for this type of task.</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i3.p1" class="ltx_para ltx_noindent">
<p id="S1.I2.i3.p1.1" class="ltx_p">We consequently present RTVE-ASD, a new corpus for ASD in Spanish collected from TV newscast programs, where multiple challenging in-the-wild situations can be found.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2402.13152/assets/annotheia_user_interface.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="628" height="305" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>AnnoTheia toolkit user interface. <span id="S1.F1.6.1" class="ltx_text" style="color:#0000FF;"> <span id="S1.F1.6.1.1" class="ltx_text ltx_font_bold">A</span></span>: Video display of the scene candidate to be a new sample of the future database. An overlying green bounding box highlights the active speaker detected by the toolkit. <span id="S1.F1.7.2" class="ltx_text" style="color:#0000FF;"> <span id="S1.F1.7.2.1" class="ltx_text ltx_font_bold">B</span></span>: Keyword legend to control the video display. <span id="S1.F1.8.3" class="ltx_text" style="color:#0000FF;"> <span id="S1.F1.8.3.1" class="ltx_text ltx_font_bold">C</span></span>: Transcription automatically generated by the toolkit. It can be edited by the annotator. <span id="S1.F1.9.4" class="ltx_text" style="color:#0000FF;"> <span id="S1.F1.9.4.1" class="ltx_text ltx_font_bold">D</span></span>: Buttons to allow the annotator to accept or discard the candidate scene sample. <span id="S1.F1.10.5" class="ltx_text" style="color:#0000FF;"> <span id="S1.F1.10.5.1" class="ltx_text ltx_font_bold">E</span></span>: Navigation buttons through candidate scenes. It can be useful to correct possible annotation mistakes.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.   The AnnoTheia Toolkit</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">We present AnnoTheia, the first open-source toolkit for the semi-automatic annotation of audio-visual speech resources, promoting the further development of speech technologies for low-resource languages. In this section, we describe the multiple modules that compose the toolkit, as well as the design of the user interface. However, we highlight the flexibility with which the toolkit was provided when replacing one of these modules with another of our preference or, as described in Section <a href="#S3" title="3. Fine-Tuning an ASD Model ‣ AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, adapted to our language of interest.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Scene Bounding Detection.</span> PySceneDetection<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/Breakthrough/PySceneDetect" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Breakthrough/PySceneDetect</a></span></span></span> is a toolkit designed to detect bounding between scenes in videos. Specifically, it computes a score representing the difference in content between the current and previous frame in the HSV color space for each video frame. In this way, we were able to accelerate the processing of large video clips by checking if at least one person was present on the scene simply by checking the first frame that composed each detected scene. Hence, using the face detector described below, we discarded those scenes where no people were present.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Face Detection.</span> Using Dual Shot Face Detector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx40" title="" class="ltx_ref">Li et al., 2019a</a>]</cite>, we were able to accurately extract faces from the scenes previously selected. However, more than one person could appear on the scene at the same time, hindering the subsequent Active Speaker Detection. Therefore, we implemented a frame-by-frame face-distance comparison algorithm, where each face was matched to a specific individual based on proximity, allowing for the precise assignment of faces to their respective individuals throughout the video sequence. The detected bounding boxes were saved not only because we needed the cropped faces for the Active Speaker Detection module described below, but also because this information might be useful once the future database is compiled. An alternative would be using the state-of-the-art RetinaFace face detector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">Deng et al., 2020</a>]</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Face Alignment.</span> On multiple occasions, it can also be useful to consider a specific region of the face, as it is the case with the visual speech recognition task, also known as automatic lipreading <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">Afouras et al., 2018b</a>, <a href="#bib.bibx45" title="" class="ltx_ref">Ma et al., 2022</a>, <a href="#bib.bibx58" title="" class="ltx_ref">Shi et al., 2022</a>]</cite>. For this reason, we also introduced a face alignment module based on the Face Alignment Network architecture proposed by <span id="S2.p4.1.2" class="ltx_text ltx_font_bold">?</span>). Hence, our toolkit will also provide 68 2-dimensional facial landmarks for each frame of the detected scene. An alternative would be using the face landmark detector provided by the Google’s MediaPipe framework<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://developers.google.com/mediapipe/solutions/vision/face_landmarker" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://developers.google.com/mediapipe/solutions/vision/face_landmarker</a></span></span></span>.</p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Active Speaker Detection.</span> Once the different people appearing on the scene were detected, we used the TalkNet-ASD model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx60" title="" class="ltx_ref">Tao et al., 2021</a>]</cite> to identify who of them was really speaking, since, on certain occasions, we may encounter dubbing or voiceovers. Besides, we studied different strategies when processing large video clips, concluding that a non-overlapping window-slicing method offered the best trade-off between quality and speed. A more detailed description of the TalkNet-ASD model architecture can be found in Subsection <a href="#S3.SS1" title="3.1. The TalkNet-ASD Model ‣ 3. Fine-Tuning an ASD Model ‣ AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. An alternative would be using the state-of-the-art Light-ASD model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx42" title="" class="ltx_ref">Liao et al., 2023</a>]</cite>.</p>
</div>
<div id="S2.p6" class="ltx_para ltx_noindent">
<p id="S2.p6.1" class="ltx_p"><span id="S2.p6.1.1" class="ltx_text ltx_font_bold">Post-Process.</span> The ASD module previously described is not exempt from errors. Therefore, we applied a smoothing average strategy for stabilizing the complete scene detection process. This method entailed calculating the mean score provided by the ASD model for a defined sliding window of frames, assigning this averaged score to the central frame within the window. The window then shifts by one frame at a time, repeating the process across the entire sequence. This approach effectively mitigates potential fluctuations in classification scores by incorporating information from neighboring frames, leading to a more consistent and stable classification.</p>
</div>
<div id="S2.p7" class="ltx_para ltx_noindent">
<p id="S2.p7.1" class="ltx_p"><span id="S2.p7.1.1" class="ltx_text ltx_font_bold">Scene Trimming.</span> We only considered as candidate scene samples those that encompassed frames in which the ASD module detected that a person was actively talking, trimming the scenes with a margin of a small number of frames both at the beginning and at the end of the scene.</p>
</div>
<div id="S2.p8" class="ltx_para ltx_noindent">
<p id="S2.p8.1" class="ltx_p"><span id="S2.p8.1.1" class="ltx_text ltx_font_bold">Automatic Transcription.</span> Whisper is a multi-lingual speech recognition system covering around 100 different languages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx53" title="" class="ltx_ref">Radford et al., 2023</a>]</cite>. By applying this technology to the raw waveform of our trimmed scenes, we were able to obtain word-level aligned transcriptions, thus facilitating the annotator’s task. Although the model automatically detects the spoken language, it can also be manually declared by the user. An alternative would be using the state-of-the-art MMS models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx52" title="" class="ltx_ref">Pratap et al., 2023</a>]</cite><span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/facebookresearch/fairseq/tree/main/examples/mms" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/fairseq/tree/main/examples/mms</a></span></span></span>.</p>
</div>
<div id="S2.p9" class="ltx_para ltx_noindent">
<p id="S2.p9.1" class="ltx_p"><span id="S2.p9.1.1" class="ltx_text ltx_font_bold">User Interface.</span> In order to facilitate the annotator’s task, an intuitive user interface has been designed to accelerate the data collection process. Details of the design can be found in Figure <a href="#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.   Fine-Tuning an ASD Model</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">This section describes the entire process we followed to prepare AnnoTheia for a language of interest by fine-tuning a pre-trained ASD model even in those situations where there are no databases originally conceived for this type of task. In our case, we adapted the TalkNet-ASD model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx60" title="" class="ltx_ref">Tao et al., 2021</a>]</cite> pre-trained for the AVA-ActiveSpeaker benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx55" title="" class="ltx_ref">Roth et al., 2020</a>]</cite> to the Spanish language using the LIP-RTVE database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">Gimeno-Gómez and Martínez-Hinarejos, 2022</a>]</cite>. Hence, in this section, we described the model architecture, how to construct a dataset to estimate an ASD model, and the training setup, as well as a discussion of the findings we observed in our results.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">3.1.   The TalkNet-ASD Model</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">The TalkNet-ASD model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx60" title="" class="ltx_ref">Tao et al., 2021</a>]</cite> was explicitly designed for the ASD task, where the model aims to identify who of the people appearing in a video clip is actually speaking and at what time. It required two simultaneous inputs: raw grayscale frames of the cropped person’s face, providing the visual cues, and pre-processed audio features in the form of 13 Mel Frequency Cepstral Coefficients (MFCC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">Gales and Young, 2008</a>]</cite>, representing the audio content. It was able to achieve state-of-the-art results on the AVA-ActiveSpeaker benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx55" title="" class="ltx_ref">Roth et al., 2020</a>]</cite>. As reflected in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1. The TalkNet-ASD Model ‣ 3. Fine-Tuning an ASD Model ‣ AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the TalkNet-ASD architecture is composed of three main modules.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2402.13152/assets/talknet-ASD.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="565" height="140" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The overall architecture of the TalkNet-ASD model.</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Window Sampling.</span> Due to the unfeasibility, in computational terms, of processing large videos in their entirety, TalkNet-ASD worked at the window sample level. In the original work, the authors studied multiple window sizes, covering contexts of different numbers of seconds.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Feature Representation Frontend.</span> Once the context windows are sampled, the visual and acoustic cues described above are each processed by an independent encoder.</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Visual Temporal Encoder.</span> This encoder was aimed to capture long-term facial expression dynamics. It comprises a visual frontend for spatial information and a visual temporal block for temporal patterns. The frontend consists of a 3D convolutional layer followed by a 2D ResNet-18 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx31" title="" class="ltx_ref">He et al., 2016</a>]</cite>, providing a sequence of frame-based embeddings. Subsequently, a block mainly based on depth-wise separable convolutional layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx29" title="" class="ltx_ref">Guo et al., 2018</a>]</cite> with residual connections is used to model temporal relationships of the previous visual representation.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Audio Temporal Encoder.</span> By using a 2D ResNet-34 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx31" title="" class="ltx_ref">He et al., 2016</a>]</cite> with a squeeze-and-excitation module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">Hu et al., 2018</a>]</cite>, this encoder was able to learn audio representations from temporal dynamics. Given the MFCCs described above, the encoder produces a sequence of audio embeddings that, thanks to the use of dilated convolutions when defining the ResNet-34, match the length of the visual embedding sequence.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">Speaker Detection Backend.</span> This module is composed of two network blocks mainly based on attention mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx61" title="" class="ltx_ref">Vaswani et al., 2017</a>]</cite>. First, a cross-attention network is employed to dynamically capture audio-visual interactions by aligning the audio and visual embeddings previously obtained by the feature frontend. Once a joint audio-visual representation is computed, a self-attention network focuses on temporal relationships, distinguishing whether the person is speaking or not across the video frames.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS1.p5.1" class="ltx_p"><span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_bold">Post-Process</span> Another important aspect to highlight is the use of an optimum classification threshold. Because the main purpose of the proposed toolkit is to provide the annotator with candidate scene samples to create a new database, we were interested in losing, within reason, as few candidate scenes as possible due to false negatives that the model could predict, delegating the final decision to the annotator’s criteria. Therefore, we computed the optimum threshold<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/</a></span></span></span> to find the best trade-off between the true-positive and false-positive rates on a validation set.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">3.2.   The ASD-RTVE corpus</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">This section describes how, from a database that was not originally conceived to address this type of tasks, we were able to construct a dataset to estimate and evaluate an ASD model.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p">We present ASD-RTVE, a new corpus for ASD in Spanish. It was constructed using the LIP-RTVE database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">Gimeno-Gómez and Martínez-Hinarejos, 2022</a>]</cite>, where multiple challenging in-the-wild situations can be found, as it was collected from TV newscast programs. Due to the fact that LIP-RTVE was conceived to address audio-visual speech recognition, all their samples include a person talking during the entire video clip. Therefore, in order to estimate a model focused on the ASD task, we defined different types of samples, intending to cover all the possible situations we might find when applying our model in a realistic application scenario.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Positive Samples.</span> We consider a positive sample when audio and video correspond to each other, i.e., the voice is aligned with the talking face.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Negative Samples.</span> With the intention of providing the model with a certain robustness against all types of undesired situations that might occur in a realistic application scenario, we defined three different types of negative samples:</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para ltx_noindent">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Temporal Mismatch.</span> The audio and video cues correspond to each other. However, the audio was randomly shifted to simulate a temporal mismatch, as long as the overlap between both modalities did not exceed 50%. This type of sample is aimed to make the model discard those scenes where audio and video cues are not aligned.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para ltx_noindent">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Partial Speaker Mismatch.</span> In this case, although the voice and the face belong to the same person, the audio and video cues do not correspond to each other. This type of sample is aimed to ensure that the model does not learn to simply associate a voice with its corresponding face.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para ltx_noindent">
<p id="S3.I2.i3.p1.1" class="ltx_p"><span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Complete Speaker Mismatch.</span> The voice and the face belong to different people. This type of sample was aimed to provide the model examples of situations we can find in a real scenario, such as dubbing or voiceovers.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.p6.1" class="ltx_p">Specifically, we split the corpus into training, validation, and test sets, ensuring a balance between all types of samples previously described. More details about the data sets defined for the ASD-RTVE database can be found in Table <a href="#S3.T2" title="Table 2 ‣ 3.2. The ASD-RTVE corpus ‣ 3. Fine-Tuning an ASD Model ‣ AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:236.5pt;height:99pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S3.T2.1.1" class="ltx_p"><span id="S3.T2.1.1.1" class="ltx_text">
<span id="S3.T2.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S3.T2.1.1.1.1.1.1" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt ltx_rowspan ltx_rowspan_2"><span id="S3.T2.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></span>
<span id="S3.T2.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3"><span id="S3.T2.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">No. of Speakers</span></span>
<span id="S3.T2.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_rowspan ltx_rowspan_2"><span id="S3.T2.1.1.1.1.1.1.3.1" class="ltx_text">
<span id="S3.T2.1.1.1.1.1.1.3.1.1" class="ltx_tabular ltx_align_middle">
<span id="S3.T2.1.1.1.1.1.1.3.1.1.1" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.1.1.1.1.1.3.1.1.1.1.1" class="ltx_text ltx_font_bold">No. of</span></span></span>
<span id="S3.T2.1.1.1.1.1.1.3.1.1.2" class="ltx_tr">
<span id="S3.T2.1.1.1.1.1.1.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.1.1.1.1.1.1.3.1.1.2.1.1" class="ltx_text ltx_font_bold">Utterances</span></span></span>
</span></span></span></span>
<span id="S3.T2.1.1.1.1.2.2" class="ltx_tr">
<span id="S3.T2.1.1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.1.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Female</span></span>
<span id="S3.T2.1.1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.1.1.1.2.2.2.1" class="ltx_text ltx_font_bold">Male</span></span>
<span id="S3.T2.1.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.1.1.1.2.2.3.1" class="ltx_text ltx_font_bold">Total</span></span></span>
</span>
<span class="ltx_tbody">
<span id="S3.T2.1.1.1.1.3.1" class="ltx_tr">
<span id="S3.T2.1.1.1.1.3.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Traning</span></span>
<span id="S3.T2.1.1.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">19</span>
<span id="S3.T2.1.1.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">10</span>
<span id="S3.T2.1.1.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">29</span>
<span id="S3.T2.1.1.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">100k</span></span>
<span id="S3.T2.1.1.1.1.4.2" class="ltx_tr">
<span id="S3.T2.1.1.1.1.4.2.1" class="ltx_td ltx_align_left"><span id="S3.T2.1.1.1.1.4.2.1.1" class="ltx_text ltx_font_bold">Validation</span></span>
<span id="S3.T2.1.1.1.1.4.2.2" class="ltx_td ltx_align_center">65</span>
<span id="S3.T2.1.1.1.1.4.2.3" class="ltx_td ltx_align_center">86</span>
<span id="S3.T2.1.1.1.1.4.2.4" class="ltx_td ltx_align_center">151</span>
<span id="S3.T2.1.1.1.1.4.2.5" class="ltx_td ltx_align_center">30k</span></span>
<span id="S3.T2.1.1.1.1.5.3" class="ltx_tr">
<span id="S3.T2.1.1.1.1.5.3.1" class="ltx_td ltx_align_left"><span id="S3.T2.1.1.1.1.5.3.1.1" class="ltx_text ltx_font_bold">Test</span></span>
<span id="S3.T2.1.1.1.1.5.3.2" class="ltx_td ltx_align_center">76</span>
<span id="S3.T2.1.1.1.1.5.3.3" class="ltx_td ltx_align_center">67</span>
<span id="S3.T2.1.1.1.1.5.3.4" class="ltx_td ltx_align_center">143</span>
<span id="S3.T2.1.1.1.1.5.3.5" class="ltx_td ltx_align_center">30k</span></span>
<span id="S3.T2.1.1.1.1.6.4" class="ltx_tr">
<span id="S3.T2.1.1.1.1.6.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="S3.T2.1.1.1.1.6.4.1.1" class="ltx_text ltx_font_bold">Total</span></span>
<span id="S3.T2.1.1.1.1.6.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">160</span>
<span id="S3.T2.1.1.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">163</span>
<span id="S3.T2.1.1.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">323</span>
<span id="S3.T2.1.1.1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">160k</span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Details of the ASD-RTVE corpus.</figcaption>
</figure>
<div id="S3.SS2.p7" class="ltx_para ltx_noindent">
<p id="S3.SS2.p7.1" class="ltx_p">It should be noted that, if we do not have a database from which to generate data to train an ASD model for our language of interest, we can always use public sources to compile it, e.g., a set of vlogs where a single person is usually talking in front of the camera during the entire video clip can be used to obtain raw audio-visual data, and then, by following, the procedure described above, the ASD-oriented dataset could be created. In the worst-case scenario, an alternative approach would be to use an ASD module trained for English, for instance, and then set its decision threshold to 0.0. We are aware that this may entail more effort from the annotator’s point of view, but we still believe the toolkit can speed up the process.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">3.3.   Implementation Details</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">Experiments were conducted on a GeForce RTX 4090 GPU with 24GB memory. The design of our ASD model was based on the official implementation<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://github.com/TaoRuijie/TalkNet-ASD" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/TaoRuijie/TalkNet-ASD</a></span></span></span> of the TalkNet-ASD model.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Training.</span> In all our experiments, we used the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx37" title="" class="ltx_ref">Kingma and Ba, 2014</a>]</cite> with a learning rate of 0.0001 decaying through a linear scheduler across 9 epochs, with a batch size of 32 samples. Because we fine-tuned the TalkNet-ASD model pre-trained for the AVA-ActiveSpeaker database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx55" title="" class="ltx_ref">Roth et al., 2020</a>]</cite>, its original architecture was not modified, referring readers to <span id="S3.SS3.p2.1.2" class="ltx_text ltx_font_bold">?</span>) to a more detailed description. Nonetheless, for the rest of the hyper-parameters, multiple settings were explored until we found the optimum we described above.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Evaluation.</span> Similar to other works in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx60" title="" class="ltx_ref">Tao et al., 2021</a>, <a href="#bib.bibx42" title="" class="ltx_ref">Liao et al., 2023</a>, <a href="#bib.bibx47" title="" class="ltx_ref">Min et al., 2022</a>]</cite>, we reported our results in terms of different evaluation metrics, namely the accuracy, the mean Average Precision (mAP), and the Area Under the Curve (AUC).</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection">3.4.   Results &amp; Discussion</h3>

<div id="S3.SS4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.p1.1" class="ltx_p">Similar to <span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_bold">?</span>), we studied how the size of the context window was affecting our system performance. Table <a href="#S3.T3" title="Table 3 ‣ 3.4. Results &amp; Discussion ‣ 3. Fine-Tuning an ASD Model ‣ AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates a clear positive correlation on the validations set. As the number of context seconds increased, the ASD model showed a consistent upward trend, suggesting that incorporating more contextual information leads to more accurate and reliable predictions regarding speech activity detection.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<div id="S3.T3.9" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:293.3pt;height:181.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S3.T3.9.9" class="ltx_p"><span id="S3.T3.9.9.9" class="ltx_text">
<span id="S3.T3.9.9.9.9" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S3.T3.9.9.9.9.10.1" class="ltx_tr">
<span id="S3.T3.9.9.9.9.10.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt ltx_colspan ltx_colspan_2"><span id="S3.T3.9.9.9.9.10.1.1.1" class="ltx_text ltx_font_bold">Context Window</span></span>
<span id="S3.T3.9.9.9.9.10.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_rowspan ltx_rowspan_2"><span id="S3.T3.9.9.9.9.10.1.2.1" class="ltx_text ltx_font_bold">Accuracy</span></span>
<span id="S3.T3.9.9.9.9.10.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_rowspan ltx_rowspan_2"><span id="S3.T3.9.9.9.9.10.1.3.1" class="ltx_text ltx_font_bold">mAP (%)</span></span>
<span id="S3.T3.9.9.9.9.10.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_rowspan ltx_rowspan_2"><span id="S3.T3.9.9.9.9.10.1.4.1" class="ltx_text ltx_font_bold">AUC (%)</span></span></span>
<span id="S3.T3.9.9.9.9.11.2" class="ltx_tr">
<span id="S3.T3.9.9.9.9.11.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S3.T3.9.9.9.9.11.2.1.1" class="ltx_text" style="font-size:90%;">no. of frames</span></span>
<span id="S3.T3.9.9.9.9.11.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t"><span id="S3.T3.9.9.9.9.11.2.2.1" class="ltx_text" style="font-size:90%;">no. of seconds</span></span></span>
</span>
<span class="ltx_tbody">
<span id="S3.T3.1.1.1.1.1" class="ltx_tr">
<span id="S3.T3.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S3.T3.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">5</span></span>
<span id="S3.T3.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S3.T3.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">0.20</span></span>
<span id="S3.T3.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">70.2<math id="S3.T3.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.T3.1.1.1.1.1.1.m1.1a"><mo mathsize="70%" id="S3.T3.1.1.1.1.1.1.m1.1.1" xref="S3.T3.1.1.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S3.T3.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.1.1.m1.1c">\pm</annotation></semantics></math><span id="S3.T3.1.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">0.5</span></span>
<span id="S3.T3.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">60.0</span>
<span id="S3.T3.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">75.3</span></span>
<span id="S3.T3.2.2.2.2.2" class="ltx_tr">
<span id="S3.T3.2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T3.2.2.2.2.2.2.1" class="ltx_text ltx_font_bold">9</span></span>
<span id="S3.T3.2.2.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T3.2.2.2.2.2.3.1" class="ltx_text ltx_font_bold">0.36</span></span>
<span id="S3.T3.2.2.2.2.2.1" class="ltx_td ltx_align_center">79.2<math id="S3.T3.2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.T3.2.2.2.2.2.1.m1.1a"><mo mathsize="70%" id="S3.T3.2.2.2.2.2.1.m1.1.1" xref="S3.T3.2.2.2.2.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.2.2.1.m1.1b"><csymbol cd="latexml" id="S3.T3.2.2.2.2.2.1.m1.1.1.cmml" xref="S3.T3.2.2.2.2.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.2.2.1.m1.1c">\pm</annotation></semantics></math><span id="S3.T3.2.2.2.2.2.1.1" class="ltx_text" style="font-size:70%;">0.5</span></span>
<span id="S3.T3.2.2.2.2.2.4" class="ltx_td ltx_align_center">72.0</span>
<span id="S3.T3.2.2.2.2.2.5" class="ltx_td ltx_align_center">84.2</span></span>
<span id="S3.T3.3.3.3.3.3" class="ltx_tr">
<span id="S3.T3.3.3.3.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T3.3.3.3.3.3.2.1" class="ltx_text ltx_font_bold">13</span></span>
<span id="S3.T3.3.3.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T3.3.3.3.3.3.3.1" class="ltx_text ltx_font_bold">0.52</span></span>
<span id="S3.T3.3.3.3.3.3.1" class="ltx_td ltx_align_center">83.5<math id="S3.T3.3.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.T3.3.3.3.3.3.1.m1.1a"><mo mathsize="70%" id="S3.T3.3.3.3.3.3.1.m1.1.1" xref="S3.T3.3.3.3.3.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.3.3.3.1.m1.1b"><csymbol cd="latexml" id="S3.T3.3.3.3.3.3.1.m1.1.1.cmml" xref="S3.T3.3.3.3.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.3.3.3.1.m1.1c">\pm</annotation></semantics></math><span id="S3.T3.3.3.3.3.3.1.1" class="ltx_text" style="font-size:70%;">0.4</span></span>
<span id="S3.T3.3.3.3.3.3.4" class="ltx_td ltx_align_center">77.7</span>
<span id="S3.T3.3.3.3.3.3.5" class="ltx_td ltx_align_center">88.0</span></span>
<span id="S3.T3.4.4.4.4.4" class="ltx_tr">
<span id="S3.T3.4.4.4.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T3.4.4.4.4.4.2.1" class="ltx_text ltx_font_bold">17</span></span>
<span id="S3.T3.4.4.4.4.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T3.4.4.4.4.4.3.1" class="ltx_text ltx_font_bold">0.68</span></span>
<span id="S3.T3.4.4.4.4.4.1" class="ltx_td ltx_align_center">85.7<math id="S3.T3.4.4.4.4.4.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.T3.4.4.4.4.4.1.m1.1a"><mo mathsize="70%" id="S3.T3.4.4.4.4.4.1.m1.1.1" xref="S3.T3.4.4.4.4.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.4.4.4.1.m1.1b"><csymbol cd="latexml" id="S3.T3.4.4.4.4.4.1.m1.1.1.cmml" xref="S3.T3.4.4.4.4.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.4.4.4.1.m1.1c">\pm</annotation></semantics></math><span id="S3.T3.4.4.4.4.4.1.1" class="ltx_text" style="font-size:70%;">0.4</span></span>
<span id="S3.T3.4.4.4.4.4.4" class="ltx_td ltx_align_center">78.4</span>
<span id="S3.T3.4.4.4.4.4.5" class="ltx_td ltx_align_center">91.2</span></span>
<span id="S3.T3.5.5.5.5.5" class="ltx_tr">
<span id="S3.T3.5.5.5.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T3.5.5.5.5.5.2.1" class="ltx_text ltx_font_bold">21</span></span>
<span id="S3.T3.5.5.5.5.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T3.5.5.5.5.5.3.1" class="ltx_text ltx_font_bold">0.84</span></span>
<span id="S3.T3.5.5.5.5.5.1" class="ltx_td ltx_align_center">89.9<math id="S3.T3.5.5.5.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.T3.5.5.5.5.5.1.m1.1a"><mo mathsize="70%" id="S3.T3.5.5.5.5.5.1.m1.1.1" xref="S3.T3.5.5.5.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.5.5.5.5.5.1.m1.1b"><csymbol cd="latexml" id="S3.T3.5.5.5.5.5.1.m1.1.1.cmml" xref="S3.T3.5.5.5.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.5.5.5.5.1.m1.1c">\pm</annotation></semantics></math><span id="S3.T3.5.5.5.5.5.1.1" class="ltx_text" style="font-size:70%;">0.3</span></span>
<span id="S3.T3.5.5.5.5.5.4" class="ltx_td ltx_align_center">85.2</span>
<span id="S3.T3.5.5.5.5.5.5" class="ltx_td ltx_align_center">94.8</span></span>
<span id="S3.T3.6.6.6.6.6" class="ltx_tr">
<span id="S3.T3.6.6.6.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T3.6.6.6.6.6.2.1" class="ltx_text ltx_font_bold">25</span></span>
<span id="S3.T3.6.6.6.6.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T3.6.6.6.6.6.3.1" class="ltx_text ltx_font_bold">1.00</span></span>
<span id="S3.T3.6.6.6.6.6.1" class="ltx_td ltx_align_center">91.8<math id="S3.T3.6.6.6.6.6.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.T3.6.6.6.6.6.1.m1.1a"><mo mathsize="70%" id="S3.T3.6.6.6.6.6.1.m1.1.1" xref="S3.T3.6.6.6.6.6.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.6.6.6.6.6.1.m1.1b"><csymbol cd="latexml" id="S3.T3.6.6.6.6.6.1.m1.1.1.cmml" xref="S3.T3.6.6.6.6.6.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.6.6.6.6.1.m1.1c">\pm</annotation></semantics></math><span id="S3.T3.6.6.6.6.6.1.1" class="ltx_text" style="font-size:70%;">0.3</span></span>
<span id="S3.T3.6.6.6.6.6.4" class="ltx_td ltx_align_center">87.5</span>
<span id="S3.T3.6.6.6.6.6.5" class="ltx_td ltx_align_center">95.8</span></span>
<span id="S3.T3.7.7.7.7.7" class="ltx_tr">
<span id="S3.T3.7.7.7.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T3.7.7.7.7.7.2.1" class="ltx_text ltx_font_bold">35</span></span>
<span id="S3.T3.7.7.7.7.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T3.7.7.7.7.7.3.1" class="ltx_text ltx_font_bold">1.40</span></span>
<span id="S3.T3.7.7.7.7.7.1" class="ltx_td ltx_align_center">93.1<math id="S3.T3.7.7.7.7.7.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.T3.7.7.7.7.7.1.m1.1a"><mo mathsize="70%" id="S3.T3.7.7.7.7.7.1.m1.1.1" xref="S3.T3.7.7.7.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.7.7.7.7.7.1.m1.1b"><csymbol cd="latexml" id="S3.T3.7.7.7.7.7.1.m1.1.1.cmml" xref="S3.T3.7.7.7.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.7.7.7.7.7.1.m1.1c">\pm</annotation></semantics></math><span id="S3.T3.7.7.7.7.7.1.1" class="ltx_text" style="font-size:70%;">0.3</span></span>
<span id="S3.T3.7.7.7.7.7.4" class="ltx_td ltx_align_center">90.7</span>
<span id="S3.T3.7.7.7.7.7.5" class="ltx_td ltx_align_center">98.5</span></span>
<span id="S3.T3.8.8.8.8.8" class="ltx_tr">
<span id="S3.T3.8.8.8.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T3.8.8.8.8.8.2.1" class="ltx_text ltx_font_bold">43</span></span>
<span id="S3.T3.8.8.8.8.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T3.8.8.8.8.8.3.1" class="ltx_text ltx_font_bold">1.72</span></span>
<span id="S3.T3.8.8.8.8.8.1" class="ltx_td ltx_align_center">97.0<math id="S3.T3.8.8.8.8.8.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.T3.8.8.8.8.8.1.m1.1a"><mo mathsize="70%" id="S3.T3.8.8.8.8.8.1.m1.1.1" xref="S3.T3.8.8.8.8.8.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.8.8.8.8.8.1.m1.1b"><csymbol cd="latexml" id="S3.T3.8.8.8.8.8.1.m1.1.1.cmml" xref="S3.T3.8.8.8.8.8.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.8.8.8.8.8.1.m1.1c">\pm</annotation></semantics></math><span id="S3.T3.8.8.8.8.8.1.1" class="ltx_text" style="font-size:70%;">0.2</span></span>
<span id="S3.T3.8.8.8.8.8.4" class="ltx_td ltx_align_center">94.8</span>
<span id="S3.T3.8.8.8.8.8.5" class="ltx_td ltx_align_center">99.3</span></span>
<span id="S3.T3.9.9.9.9.9" class="ltx_tr">
<span id="S3.T3.9.9.9.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S3.T3.9.9.9.9.9.2.1" class="ltx_text ltx_font_bold">51</span></span>
<span id="S3.T3.9.9.9.9.9.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S3.T3.9.9.9.9.9.3.1" class="ltx_text ltx_font_bold">2.04</span></span>
<span id="S3.T3.9.9.9.9.9.1" class="ltx_td ltx_align_center ltx_border_bb">97.3<math id="S3.T3.9.9.9.9.9.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.T3.9.9.9.9.9.1.m1.1a"><mo mathsize="70%" id="S3.T3.9.9.9.9.9.1.m1.1.1" xref="S3.T3.9.9.9.9.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.9.9.9.9.9.1.m1.1b"><csymbol cd="latexml" id="S3.T3.9.9.9.9.9.1.m1.1.1.cmml" xref="S3.T3.9.9.9.9.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.9.9.9.9.9.1.m1.1c">\pm</annotation></semantics></math><span id="S3.T3.9.9.9.9.9.1.1" class="ltx_text" style="font-size:70%;">0.2</span></span>
<span id="S3.T3.9.9.9.9.9.4" class="ltx_td ltx_align_center ltx_border_bb">95.8</span>
<span id="S3.T3.9.9.9.9.9.5" class="ltx_td ltx_align_center ltx_border_bb">99.5</span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results for the ASD-RTVE validation set depending on the context window size.</figcaption>
</figure>
<div id="S3.SS4.p2" class="ltx_para ltx_noindent">
<p id="S3.SS4.p2.1" class="ltx_p">Furthermore, we evaluated the ASD models estimated for each window size in terms of inference time. However, no significant differences were found between the studied context windows. Therefore, for AnnoTheia, we used the ASD model trained with 51-frame windows, which was able to achieve, on the ASD-RTVE test set, results around 95.4<math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mo mathsize="90%" id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><csymbol cd="latexml" id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">\pm</annotation></semantics></math><span id="S3.SS4.p2.1.1" class="ltx_text" style="font-size:90%;">0.2</span> accuracy, 91.6 % mAP, and 99.3 % AUC.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.   Conclusions &amp; Future Work</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">In this work, we presented the AnnoTheia toolkit to facilitate the task of annotating audio-visual resources for speech technologies, thus promoting further research on low-resource languages in the field. In addition, because some of its modules are language-dependent, we showed the complete process of preparing AnnoTheia for a language of interest, even in those occasions where there are no databases originally conceived for that purpose. Regarding our future work, we are considering introducing further refinements to improve the toolkit both from a performance and usability perspective. One important contribution would be integrating a module capable of ensuring speaker-independent partitions, e.g., via face, body, and voice clustering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">Brown et al., 2021</a>, <a href="#bib.bibx18" title="" class="ltx_ref">Deng et al., 2019</a>]</cite>. Furthermore, we plan to conduct a comprehensive experiment to properly assess the effectiveness and advantages of using the AnnoTheia toolkit versus a completely manual procedure.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.   Limitations</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">Our toolkit’s main limitation is the need to adapt the ASD module to the language of interest, which could hinder the experience for users unfamiliar with training machine learning-based systems. Another aspect to consider is the possibility that the pre-trained models composing the toolkit can be biased. Due to the lack of demographically diverse data, certain social groups could be underrepresented because of their age, gender, and cultural background, among other factors.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.   Ethical Considerations</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">The toolkit we present can be used to collect and annotate audio-visual databases from social media where a person’s face and voice are recorded. Both information cues could be considered as biometrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx48" title="" class="ltx_ref">Nagrani et al., 2017</a>]</cite>, raising privacy concerns regarding a person’s identity. Therefore, any data collection must protect privacy rights, taking necessary measures and always asking all the people involved for permission beforehand.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">7.   Acknowledgements</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">The work of José-M. Acosta-Triana was in the framework of the Valencian Graduate School and Research Network for Artificial Intelligence (ValgrAI) funded by Generalitat Valenciana. The work of David Gimeno-Gómez and Carlos-D. Martínez-Hinarejos was partially supported by Grant CIACIF/2021/295 funded by Generalitat Valenciana and by Grant PID2021-124719OB-I00 under project LLEER (PID2021-124719OB-100) funded by MCIN/AEI/10.13039/501100011033/ and by ERDF, EU A way of making Europe.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">8.   Bibliographical References</h2>

</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ablimit et al., 2022</span>
<span class="ltx_bibblock">
Ablimit, A., Botelho, C., Abad, A., Schultz, T., and Trancoso, I.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">Exploring dementia detection from speech: Cross corpus analysis.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx1.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing</span>, pages 6472–6476.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Afouras et al., 2018a</span>
<span class="ltx_bibblock">
Afouras, T., Chung, J., and Zisserman, A.

</span>
<span class="ltx_bibblock">(2018a).

</span>
<span class="ltx_bibblock">Lrs3-ted: a large-scale dataset for visual speech recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bibx2.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1809.00496</span>.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Afouras et al., 2018b</span>
<span class="ltx_bibblock">
Afouras, T., Chung, J. S., Senior, A., Vinyals, O., and Zisserman, A.

</span>
<span class="ltx_bibblock">(2018b).

</span>
<span class="ltx_bibblock">Deep audio-visual speech recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bibx3.1.1" class="ltx_text ltx_font_italic">Transactions on PAMI</span>, 44(12):8717–8727.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Albanie et al., 2021</span>
<span class="ltx_bibblock">
Albanie, S., Varol, G., Momeni, L., Bull, H., Afouras, T., Chowdhury, H., Fox, N., Woll, B., Cooper, R., McParland, A., and Zisserman, A.

</span>
<span class="ltx_bibblock">(2021).

</span>
<span class="ltx_bibblock">BOBSL: BBC-Oxford British Sign Language Dataset.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anwar et al., 2023</span>
<span class="ltx_bibblock">
Anwar, M., Shi, B., Goswami, V., Hsu, W.-N., Pino, J., and Wang, C.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx5.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH</span>, pages 4064–4068.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ardila et al., 2020</span>
<span class="ltx_bibblock">
Ardila, R., Branson, M., Davis, K., Kohler, M., Meyer, J., Henretty, M., Morais, R., Saunders, L., Tyers, F., and Weber, G.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">Common voice: A massively-multilingual speech corpus.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx6.1.1" class="ltx_text ltx_font_italic">Proceedings of the Twelfth Language Resources and Evaluation Conference</span>, pages 4218–4222.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Babu et al., 2022</span>
<span class="ltx_bibblock">
Babu, A., Wang, C., Tjandra, A., Lakhotia, K., Xu, Q., Goyal, N., Singh, K., von Platen, P., Saraf, Y., Pino, J., Baevski, A., Conneau, A., and Auli, M.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx7.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, pages 2278–2282.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et al., 2020</span>
<span class="ltx_bibblock">
Baevski, A., Zhou, Y., Mohamed, A., and Auli, M.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">wav2vec 2.0: A framework for self-supervised learning of speech representations.

</span>
<span class="ltx_bibblock"><span id="bib.bibx8.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 33:12449–12460.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bredin and Laurent, 2021</span>
<span class="ltx_bibblock">
Bredin, H. and Laurent, A.

</span>
<span class="ltx_bibblock">(2021).

</span>
<span class="ltx_bibblock">End-To-End Speaker Segmentation for Overlap-Aware Resegmentation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx9.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, pages 3111–3115.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bromham et al., 2022</span>
<span class="ltx_bibblock">
Bromham, L., Dinnage, R., Skirgård, H., Ritchie, A., Cardillo, M., Meakins, F., Greenhill, S., and Hua, X.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">Global predictors of language endangerment and the future of linguistic diversity.

</span>
<span class="ltx_bibblock"><span id="bib.bibx10.1.1" class="ltx_text ltx_font_italic">Nature ecology &amp; evolution</span>, 6(2):163–173.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al., 2021</span>
<span class="ltx_bibblock">
Brown, A., Kalogeiton, V., and Zisserman, A.

</span>
<span class="ltx_bibblock">(2021).

</span>
<span class="ltx_bibblock">Face, body, voice: Video person-clustering with multiple modalities.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx11.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</span>, pages 3184–3194, October.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bulat and Tzimiropoulos, 2017</span>
<span class="ltx_bibblock">
Bulat, A. and Tzimiropoulos, G.

</span>
<span class="ltx_bibblock">(2017).

</span>
<span class="ltx_bibblock">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks).

</span>
<span class="ltx_bibblock">In <span id="bib.bibx12.1.1" class="ltx_text ltx_font_italic">ICCV</span>, pages 1021–1030.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burchi and Timofte, 2023</span>
<span class="ltx_bibblock">
Burchi, M. and Timofte, R.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">Audio-visual efficient conformer for robust speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx13.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</span>, pages 2258–2267.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al., 2023</span>
<span class="ltx_bibblock">
Chen, W., Yan, B., Shi, J., Peng, Y., Maiti, S., and Watanabe, S.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">Improving massively multilingual asr with auxiliary ctc objectives.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx14.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing</span>, pages 1–5.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung and Zisserman, 2017</span>
<span class="ltx_bibblock">
Chung, J. and Zisserman, A.

</span>
<span class="ltx_bibblock">(2017).

</span>
<span class="ltx_bibblock">Lip reading in the wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx15.1.1" class="ltx_text ltx_font_italic">13th Asian Conference on Computer Vision</span>, pages 87–103. Springer.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cihan Camgöz et al., 2020</span>
<span class="ltx_bibblock">
Cihan Camgöz, N., Koller, O., Hadfield, S., and Bowden, R.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">Sign language transformers: Joint end-to-end sign language recognition and translation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx16.1.1" class="ltx_text ltx_font_italic">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 10020–10030.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Denby et al., 2010</span>
<span class="ltx_bibblock">
Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J. M., and Brumberg, J. S.

</span>
<span class="ltx_bibblock">(2010).

</span>
<span class="ltx_bibblock">Silent speech interfaces.

</span>
<span class="ltx_bibblock"><span id="bib.bibx17.1.1" class="ltx_text ltx_font_italic">Speech Communication</span>, 52(4):270–287.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al., 2019</span>
<span class="ltx_bibblock">
Deng, J., Guo, J., Xue, N., and Zafeiriou, S.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">Arcface: Additive angular margin loss for deep face recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx18.1.1" class="ltx_text ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 4685–4694.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al., 2020</span>
<span class="ltx_bibblock">
Deng, J., Guo, J., Ververas, E., Kotsia, I., and Zafeiriou, S.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">Retinaface: Single-shot multi-level face localisation in the wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx19.1.1" class="ltx_text ltx_font_italic">CVPR</span>, pages 5202–5211.

</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eberhard et al., 2023</span>
<span class="ltx_bibblock">
Eberhard, D. M., Simons, G. F., and Fennig, C. D.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">Ethnologue: languages of the world.

</span>
<span class="ltx_bibblock"><span id="bib.bibx20.1.1" class="ltx_text ltx_font_italic">Online version: http://www.ethnologue.com</span>.

</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Egorov et al., 2021</span>
<span class="ltx_bibblock">
Egorov, E., Kostyumov, V., Konyk, M., and Kolesnikov, S.

</span>
<span class="ltx_bibblock">(2021).

</span>
<span class="ltx_bibblock">LRWR: large-scale benchmark for lip reading in Russian language.

</span>
<span class="ltx_bibblock"><span id="bib.bibx21.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2109.06692</span>.

</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ephrat et al., 2018</span>
<span class="ltx_bibblock">
Ephrat, A., Mosseri, I., Lang, O., Dekel, T., Wilson, K., Hassidim, A., Freeman, W., and Rubinstein, M.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation.

</span>
<span class="ltx_bibblock"><span id="bib.bibx22.1.1" class="ltx_text ltx_font_italic">ACM Trans. Graph.</span>, 37(4):1–11.

</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fernandez-Lopez and Sukno, 2018</span>
<span class="ltx_bibblock">
Fernandez-Lopez, A. and Sukno, F.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">Survey on automatic lip-reading in the era of deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bibx23.1.1" class="ltx_text ltx_font_italic">Image and Vision Computing</span>, 78:53–72.

</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fernandez-Lopez et al., 2017</span>
<span class="ltx_bibblock">
Fernandez-Lopez, A., Martinez, O., and Sukno, F. M.

</span>
<span class="ltx_bibblock">(2017).

</span>
<span class="ltx_bibblock">Towards estimating the upper bound of visual-speech recognition: The visual lip-reading feasibility database.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx24.1.1" class="ltx_text ltx_font_italic">12th FG)</span>, pages 208–215. IEEE.

</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fujita et al., 2019</span>
<span class="ltx_bibblock">
Fujita, Y., Kanda, N., Horiguchi, S., Nagamatsu, K., and Watanabe, S.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">End-to-End Neural Speaker Diarization with Permutation-Free Objectives.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx25.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, pages 4300–4304.

</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gales and Young, 2008</span>
<span class="ltx_bibblock">
Gales, M. and Young, S.

</span>
<span class="ltx_bibblock">(2008).

</span>
<span class="ltx_bibblock"><span id="bib.bibx26.1.1" class="ltx_text ltx_font_italic">The application of hidden Markov models in speech recognition</span>.

</span>
<span class="ltx_bibblock">Now Publishers Inc.

</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gimeno-Gómez and Martínez-Hinarejos, 2022</span>
<span class="ltx_bibblock">
Gimeno-Gómez, D. and Martínez-Hinarejos, C.-D.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">LIP-RTVE: An Audiovisual Database for Continuous Spanish in the Wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx27.1.1" class="ltx_text ltx_font_italic">LREC</span>, pages 2750–2758. ELRA, June.

</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">González-López et al., 2020</span>
<span class="ltx_bibblock">
González-López, J. A., Gómez-Alanís, A., Martín Doñas, J. M., Pérez-Córdoba, J. L., and Gómez, A. M.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">Silent speech interfaces for speech restoration: A review.

</span>
<span class="ltx_bibblock"><span id="bib.bibx28.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 8:177995–178021.

</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al., 2018</span>
<span class="ltx_bibblock">
Guo, J., Li, Y., Lin, W., Chen, Y., and Li, J.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">Network decoupling: From regular to depthwise separable convolutions.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx29.1.1" class="ltx_text ltx_font_italic">British Machine Vision Conference</span>.

</span>
</li>
<li id="bib.bibx30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harte and Gillen, 2015</span>
<span class="ltx_bibblock">
Harte, N. and Gillen, E.

</span>
<span class="ltx_bibblock">(2015).

</span>
<span class="ltx_bibblock">TCD-TIMIT: An audio-visual corpus of continuous speech.

</span>
<span class="ltx_bibblock"><span id="bib.bibx30.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Multimedia</span>, 17(5):603–615.

</span>
</li>
<li id="bib.bibx31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al., 2016</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., and Sun, J.

</span>
<span class="ltx_bibblock">(2016).

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx31.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 770–778.

</span>
</li>
<li id="bib.bibx32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al., 2018</span>
<span class="ltx_bibblock">
Hu, J., Shen, L., and Sun, G.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">Squeeze-and-excitation networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx32.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 7132–7141.

</span>
</li>
<li id="bib.bibx33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al., 2022</span>
<span class="ltx_bibblock">
Huang, Z., Watanabe, S., Yang, S.-w., García, P., and Khudanpur, S.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">Investigating self-supervised learning for speech enhancement and separation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx33.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 6837–6841.

</span>
</li>
<li id="bib.bibx34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al., 2023</span>
<span class="ltx_bibblock">
Huang, R., Liu, H., Cheng, X., Ren, Y., Li, L., Ye, Z., He, J., Zhang, L., Liu, J., Yin, X., and Zhao, Z.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">AV-TranSpeech: Audio-visual robust speech-to-speech translation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx34.1.1" class="ltx_text ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 8590–8604.

</span>
</li>
<li id="bib.bibx35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al., 2019</span>
<span class="ltx_bibblock">
Jia, Y., Weiss, R., Biadsy, F., Macherey, W., Johnson, M., Chen, Z., and Wu, Y.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">Direct Speech-to-Speech Translation with a Sequence-to-Sequence Model.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx35.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, pages 1123–1127.

</span>
</li>
<li id="bib.bibx36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jokinen and McTear, 2022</span>
<span class="ltx_bibblock">
Jokinen, K. and McTear, M.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock"><span id="bib.bibx36.1.1" class="ltx_text ltx_font_italic">Spoken dialogue systems</span>.

</span>
<span class="ltx_bibblock">Springer Nature.

</span>
</li>
<li id="bib.bibx37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba, 2014</span>
<span class="ltx_bibblock">
Kingma, D. P. and Ba, J.

</span>
<span class="ltx_bibblock">(2014).

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bibx37.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1412.6980</span>.

</span>
</li>
<li id="bib.bibx38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koller et al., 2015</span>
<span class="ltx_bibblock">
Koller, O., Forster, J., and Ney, H.

</span>
<span class="ltx_bibblock">(2015).

</span>
<span class="ltx_bibblock">Continuous sign language recognition: Towards large vocabulary statistical recognition systems handling multiple signers.

</span>
<span class="ltx_bibblock"><span id="bib.bibx38.1.1" class="ltx_text ltx_font_italic">Computer Vision and Image Understanding</span>, 141:108–125.

</span>
</li>
<li id="bib.bibx39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al., 2022</span>
<span class="ltx_bibblock">
Lee, A., Chen, P., Wang, C., Gu, J., Popuri, S., Ma, X., Polyak, A., Adi, Y., He, Q., Tang, Y., Pino, J., and Hsu, W.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">Direct speech-to-speech translation with discrete units.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx39.1.1" class="ltx_text ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</span>, pages 3327–3339.

</span>
</li>
<li id="bib.bibx40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al., 2019a</span>
<span class="ltx_bibblock">
Li, J., Wang, Y., Wang, C., Tai, Y., Qian, J., Yang, J., Wang, C., Li, J., and Huang, F.

</span>
<span class="ltx_bibblock">(2019a).

</span>
<span class="ltx_bibblock">Dsfd: Dual shot face detector.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx40.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 5055–5064.

</span>
</li>
<li id="bib.bibx41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al., 2019b</span>
<span class="ltx_bibblock">
Li, N., Liu, S., Liu, Y., Zhao, S., and Liu, M.

</span>
<span class="ltx_bibblock">(2019b).

</span>
<span class="ltx_bibblock">Neural speech synthesis with transformer network.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx41.1.1" class="ltx_text ltx_font_italic">Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</span>, page 6706–6713.

</span>
</li>
<li id="bib.bibx42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al., 2023</span>
<span class="ltx_bibblock">
Liao, J., Duan, H., Feng, K., Zhao, W., Yang, Y., and Chen, L.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">A light weight model for active speaker detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx42.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, pages 22932–22941.

</span>
</li>
<li id="bib.bibx43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al., 2023</span>
<span class="ltx_bibblock">
Liu, X., Lakomkin, E., Vougioukas, K., Ma, P., Chen, H., Xie, R., Doulaty, M., Moritz, N., Kolar, J., Petridis, S., Pantic, M., and Fuegen, C.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">Synthvsr: Scaling up visual speech recognition with synthetic supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx43.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 18806–18815.

</span>
</li>
<li id="bib.bibx44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al., 2021</span>
<span class="ltx_bibblock">
Ma, P., Petridis, S., and Pantic, M.

</span>
<span class="ltx_bibblock">(2021).

</span>
<span class="ltx_bibblock">End-to-end audio-visual speech recognition with conformers.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx44.1.1" class="ltx_text ltx_font_italic">ICASSP</span>, pages 7613–7617.

</span>
</li>
<li id="bib.bibx45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al., 2022</span>
<span class="ltx_bibblock">
Ma, P., Petridis, S., and Pantic, M.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">Visual speech recognition for multiple languages in the wild.

</span>
<span class="ltx_bibblock"><span id="bib.bibx45.1.1" class="ltx_text ltx_font_italic">Nature Machine Intelligence</span>, 4(11):930–939.

</span>
</li>
<li id="bib.bibx46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Milling et al., 2022</span>
<span class="ltx_bibblock">
Milling, M., Pokorny, F. B., Bartl-Pokorny, K. D., and Schuller, B. W.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">Is speech the new blood? recent progress in ai-based disease detection from audio in a nutshell.

</span>
<span class="ltx_bibblock"><span id="bib.bibx46.1.1" class="ltx_text ltx_font_italic">Frontiers in Digital Health</span>, 4.

</span>
</li>
<li id="bib.bibx47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min et al., 2022</span>
<span class="ltx_bibblock">
Min, K., Roy, S., Tripathi, S., Guha, T., and Majumdar, S.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">Learning long-term spatial-temporal graphs for active speaker detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx47.1.1" class="ltx_text ltx_font_italic">ECCV</span>, pages 371–387, Cham. Springer Nature Switzerland.

</span>
</li>
<li id="bib.bibx48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nagrani et al., 2017</span>
<span class="ltx_bibblock">
Nagrani, A., Chung, J. S., and Zisserman, A.

</span>
<span class="ltx_bibblock">(2017).

</span>
<span class="ltx_bibblock">VoxCeleb: A Large-Scale Speaker Identification Dataset.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx48.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, pages 2616–2620.

</span>
</li>
<li id="bib.bibx49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pascual et al., 2017</span>
<span class="ltx_bibblock">
Pascual, S., Bonafonte, A., and Serrà, J.

</span>
<span class="ltx_bibblock">(2017).

</span>
<span class="ltx_bibblock">SEGAN: Speech Enhancement Generative Adversarial Network.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx49.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, pages 3642–3646.

</span>
</li>
<li id="bib.bibx50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peymanfard et al., 2023</span>
<span class="ltx_bibblock">
Peymanfard, J., Heydarian, S., Lashini, A., Zeinali, H., Mohammadi, M., and Mozayani, N.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">A multi-purpose audio-visual corpus for multi-modal persian speech recognition: the arman-av dataset.

</span>
<span class="ltx_bibblock"><span id="bib.bibx50.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2301.10180</span>.

</span>
</li>
<li id="bib.bibx51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prajwal et al., 2022</span>
<span class="ltx_bibblock">
Prajwal, K., Afouras, T., and Zisserman, A.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">Sub-word level lip reading with visual attention.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx51.1.1" class="ltx_text ltx_font_italic">CVPR</span>, pages 5162–5172.

</span>
</li>
<li id="bib.bibx52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pratap et al., 2023</span>
<span class="ltx_bibblock">
Pratap, V., Tjandra, A., Shi, B., Tomasello, P., Babu, A., Kundu, S., Elkahky, A., Ni, Z., Vyas, A., Fazel-Zarandi, M., Baevski, A., Adi, Y., Z., X., Hsu, W., Conneau, A., and M., A.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">Scaling speech technology to 1,000+ languages.

</span>
<span class="ltx_bibblock"><span id="bib.bibx52.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.13516</span>.

</span>
</li>
<li id="bib.bibx53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al., 2023</span>
<span class="ltx_bibblock">
Radford, A., Kim, J., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx53.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages 28492–28518. PMLR.

</span>
</li>
<li id="bib.bibx54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ravanelli et al., 2020</span>
<span class="ltx_bibblock">
Ravanelli, M., Zhong, J., Pascual, S., Swietojanski, P., Monteiro, J., Trmal, J., and Bengio, Y.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">Multi-task self-supervised learning for robust speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx54.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing</span>, pages 6989–6993.

</span>
</li>
<li id="bib.bibx55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roth et al., 2020</span>
<span class="ltx_bibblock">
Roth, J., Chaudhuri, S., Klejch, O., Marvin, R., Gallagher, A., Kaver, L., Ramaswamy, S., Stopczynski, A., Schmid, C., Xi, Z., and Pantofaru, C.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">AVA Active Speaker: An Audio-Visual Dataset for Active Speaker Detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx55.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 4492–4496. IEEE.

</span>
</li>
<li id="bib.bibx56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salesky et al., 2021</span>
<span class="ltx_bibblock">
Salesky, E., Wiesner, M., Bremerman, J., Cattoni, R., Negri, M., Turchi, M., Oard, D., and Post, M.

</span>
<span class="ltx_bibblock">(2021).

</span>
<span class="ltx_bibblock">The Multilingual TEDx Corpus for Speech Recognition and Translation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx56.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, pages 3655–3659.

</span>
</li>
<li id="bib.bibx57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al., 2018</span>
<span class="ltx_bibblock">
Shen, J., Pang, R., Weiss, R., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerrv-Ryan, R., Saurous, R., Agiomvrgiannakis, Y., and Wu, Y.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx57.1.1" class="ltx_text ltx_font_italic">2018 IEEE international conference on acoustics, speech and signal processing</span>, pages 4779–4783. IEEE.

</span>
</li>
<li id="bib.bibx58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al., 2022</span>
<span class="ltx_bibblock">
Shi, B., Hsu, W.-N., Lakhotia, K., and Mohamed, A.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">Learning audio-visual speech representation by masked multimodal cluster prediction.

</span>
<span class="ltx_bibblock"><span id="bib.bibx58.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2201.02184</span>.

</span>
</li>
<li id="bib.bibx59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al., 2023</span>
<span class="ltx_bibblock">
Shi, J., Berrebbi, D., Chen, W., Chung, H., Hu, E., Huang, W., Chang, X., Li, S., Mohamed, A., Lee, H., and Watanabe, S.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">Ml-superb: Multilingual speech universal performance benchmark.

</span>
<span class="ltx_bibblock"><span id="bib.bibx59.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.10615</span>.

</span>
</li>
<li id="bib.bibx60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tao et al., 2021</span>
<span class="ltx_bibblock">
Tao, R., Pan, Z., Das, R., Qian, X., Shou, M., and Li, h.

</span>
<span class="ltx_bibblock">(2021).

</span>
<span class="ltx_bibblock">Is someone speaking? exploring long-term temporal features for audio-visual active speaker detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx60.1.1" class="ltx_text ltx_font_italic">Proceedings of the 29th ACM International Conference on Multimedia</span>, MM ’21, page 3927–3935. Association for Computing Machinery.

</span>
</li>
<li id="bib.bibx61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al., 2017</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I.

</span>
<span class="ltx_bibblock">(2017).

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span id="bib.bibx61.1.1" class="ltx_text ltx_font_italic">NeurIPS</span>, 30:6000–6010.

</span>
</li>
<li id="bib.bibx62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al., 2019</span>
<span class="ltx_bibblock">
Yang, S., Zhang, Y., Feng, D., Yang, M., Wang, C., Xiao, J., Long, K., Shan, S., and Chen, X.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx62.1.1" class="ltx_text ltx_font_italic">14th IEEE International Conference on Automatic Face &amp; Gesture Recognition</span>, pages 1–8.

</span>
</li>
<li id="bib.bibx63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yen et al., 2023</span>
<span class="ltx_bibblock">
Yen, H., Germain, F., Wichern, G., and Roux, J.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">Cold diffusion for speech enhancement.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx63.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing</span>, pages 1–5.

</span>
</li>
<li id="bib.bibx64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zadeh et al., 2020</span>
<span class="ltx_bibblock">
Zadeh, A. B., Cao, Y., Hessner, S., Liang, P. P., Poria, S., and Morency, L.-P.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">CMU-MOSEAS: A multimodal language dataset for spanish, portuguese, german and french.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx64.1.1" class="ltx_text ltx_font_italic">EMNLP</span>, pages 1801–1812.

</span>
</li>
<li id="bib.bibx65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al., 2023</span>
<span class="ltx_bibblock">
Zhang, Y., Han, W., Qin, J., Wang, Y., Bapna, A., Chen, Z., Chen, N., Li, B., Axelrod, V., Wang, G., Mengh, Z., Hu, K., Rosenberg, A., Prabhavalkar, R., Park, D., Haghani, P., Riesa, J., Perng, G., Soltau, H., Strohman, T., Ramabhadran, B., Sainath, T., Moreno, P., Chlu, C., Schalkwyk, J., Beaufays, F., and Wu, Y.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">Google usm: Scaling automatic speech recognition beyond 100 languages.

</span>
<span class="ltx_bibblock"><span id="bib.bibx65.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.01037</span>.

</span>
</li>
<li id="bib.bibx66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al., 2020</span>
<span class="ltx_bibblock">
Zhao, Y., Xu, R., and Song, M.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">A cascade sequence-to-sequence model for chinese mandarin lip reading.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx66.1.1" class="ltx_text ltx_font_italic">Proceedings of the ACM Multimedia Asia</span>, pages 1–6. Association for Computing Machinery.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2402.13151" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2402.13152" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2402.13152">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.13152" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2402.13153" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 19:01:56 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
