<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.08711] Text-To-Speech Synthesis In The Wild</title><meta property="og:description" content="Text-to-speech (TTS) systems are traditionally trained using modest databases of studio-quality, prompted or read speech collected in benign acoustic environments such as anechoic rooms. The recent literature nonethele…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Text-To-Speech Synthesis In The Wild">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Text-To-Speech Synthesis In The Wild">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.08711">

<!--Generated on Sat Oct  5 20:15:54 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Text-to-speech synthesis,  in the wild
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Text-To-Speech Synthesis In The Wild</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id24.24.24" class="ltx_tabular ltx_align_middle">
<span id="id6.6.6.6" class="ltx_tr">
<span id="id6.6.6.6.6" class="ltx_td ltx_align_center">Jee-weon Jung<sup id="id6.6.6.6.6.1" class="ltx_sup">1</sup>, Wangyou Zhang<sup id="id6.6.6.6.6.2" class="ltx_sup">2</sup>, Soumi Maiti<sup id="id6.6.6.6.6.3" class="ltx_sup">3</sup>, Yihan Wu<sup id="id6.6.6.6.6.4" class="ltx_sup">4</sup>, Xin Wang<sup id="id6.6.6.6.6.5" class="ltx_sup">5</sup>, Ji-Hoon Kim<sup id="id6.6.6.6.6.6" class="ltx_sup">6</sup>,</span></span>
<span id="id11.11.11.11" class="ltx_tr">
<span id="id11.11.11.11.5" class="ltx_td ltx_align_center">Yuta Matsunaga<sup id="id11.11.11.11.5.1" class="ltx_sup">7</sup>, Seyun Um<sup id="id11.11.11.11.5.2" class="ltx_sup">8</sup>, Jinchuan Tian<sup id="id11.11.11.11.5.3" class="ltx_sup">1</sup>, Hye-jin Shim<sup id="id11.11.11.11.5.4" class="ltx_sup">1</sup>, Nicholas Evans<sup id="id11.11.11.11.5.5" class="ltx_sup">9</sup>,</span></span>
<span id="id14.14.14.14" class="ltx_tr">
<span id="id14.14.14.14.3" class="ltx_td ltx_align_center">Joon Son Chung<sup id="id14.14.14.14.3.1" class="ltx_sup">8</sup>, Shinnosuke Takamichi<sup id="id14.14.14.14.3.2" class="ltx_sup"><span id="id14.14.14.14.3.2.1" class="ltx_text ltx_font_italic">10</span></sup>, Shinji Watanabe<sup id="id14.14.14.14.3.3" class="ltx_sup">1</sup></span></span>
<span id="id17.17.17.17" class="ltx_tr">
<span id="id17.17.17.17.3" class="ltx_td ltx_align_center"><sup id="id17.17.17.17.3.1" class="ltx_sup">1</sup>Carnegie Mellon University, USA,
<sup id="id17.17.17.17.3.2" class="ltx_sup">2</sup>Shanghai Jiao Tong University, China,
<sup id="id17.17.17.17.3.3" class="ltx_sup">3</sup>Meta, USA</span></span>
<span id="id19.19.19.19" class="ltx_tr">
<span id="id19.19.19.19.2" class="ltx_td ltx_align_center"><sup id="id19.19.19.19.2.1" class="ltx_sup">4</sup>Renmin University of China, China,
<sup id="id19.19.19.19.2.2" class="ltx_sup">5</sup>National Institute of Informatics, Japan</span></span>
<span id="id21.21.21.21" class="ltx_tr">
<span id="id21.21.21.21.2" class="ltx_td ltx_align_center"><sup id="id21.21.21.21.2.1" class="ltx_sup">6</sup>Korea Advanced Institute of Science and Technology, South Korea,
<sup id="id21.21.21.21.2.2" class="ltx_sup">7</sup>University of Tokyo, Japan</span></span>
<span id="id24.24.24.24" class="ltx_tr">
<span id="id24.24.24.24.3" class="ltx_td ltx_align_center"><sup id="id24.24.24.24.3.1" class="ltx_sup">8</sup>Yonsei University, South Korea,
<sup id="id24.24.24.24.3.2" class="ltx_sup">9</sup>EURECOM, France,
<sup id="id24.24.24.24.3.3" class="ltx_sup"><span id="id24.24.24.24.3.3.1" class="ltx_text ltx_font_italic">10</span></sup>Keio University, Japan</span></span>
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id25.id1" class="ltx_p">Text-to-speech (TTS) systems are traditionally trained using modest databases of studio-quality, prompted or read speech collected in benign acoustic environments such as anechoic rooms. The recent literature nonetheless shows efforts to train TTS systems using data collected in the wild. While this approach allows for the use of massive quantities of natural speech, until now, there are no common datasets. We introduce the TTS In the Wild (TITW) dataset, the result of a fully automated pipeline, in this case, applied to the VoxCeleb1 dataset commonly used for speaker recognition. We further propose two training sets. TITW-Hard is derived from the transcription, segmentation, and selection of VoxCeleb1 source data. TITW-Easy is derived from the additional application of enhancement and additional data selection based on DNSMOS. We show that a number of recent TTS models can be trained successfully using TITW-Easy, but that it remains extremely challenging to produce similar results using TITW-Hard. Both the dataset and protocols are publicly available and support the benchmarking of TTS systems trained using TITW data.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Text-to-speech synthesis, in the wild

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Generative speech technology is evolving rapidly, driven in part by advances in diffusion models, speech codecs, and speech-language modeling methodologies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Numerous ever-more advanced Text-To-Speech (TTS) algorithms continue to emerge. In contrast to state-of-the-art systems from only a few years ago, today’s TTS systems can generate speech samples that are largely indistinguishable from real human speech in terms of both intelligibility and naturalness.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While minutes of target speaker data, usually studio-quality collected in controlled conditions (e.g. anechoic rooms), was once necessary, today’s systems often need only a few seconds of target speaker data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Some systems even perform well when using clean, though not necessarily studio-quality speech. Nonetheless, TTS models themselves are almost exclusively trained using relatively clean speech data. The need for high-quality training data can hinder the training of TTS systems using more plentiful speech data collected in the wild, implying that synthetic speech might lack the characteristics of natural, spontaneous speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
The lack of high-quality data in sufficient quantities is also a barrier to the development of TTS systems for some under-resourced languages.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Low-quality data was found to be challenging for training the legacy hidden Markov model-based TTS systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
The recent literature shows a more thrust to train TTS models using lower-quality data, even data collected in the wild (called noisy TTS in some cases) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Results from the fifth edition of the ASVspoof challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> confirm the potential to generate high-quality synthetic speech when TTS systems are trained using non-studio-quality data sourced from a database of audiobooks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Even if there is scarce evidence that TTS models can be trained using even lower quality training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, most attempts have been made using proprietary rather than publicly available data resources or data where artificial noise was added.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we introduce a new public data resource to support research in this direction – the Text-To-Speech Synthesis In The Wild (TITW) database. Our goal is to overcome data constraints and to promote the research and development of TTS systems which can be trained with data collected in the wild, which can synthesize speech with a more natural and spontaneous style. The TITW database is constructed using the VoxCeleb1 source database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, a large collection of speech data collected from celebrity speakers and extracted from videos posted to social media.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We report the use of a fully automated pipeline involving automatic transcription, segmentation, and speech enhancement, which largely mitigates the need for any manual processing. We describe its use to generate two TTS training sets, each involving different levels of automatic processing and speech quality. TITW-Hard is the result of automatic transcription, segmentation, and data selection based on heuristically-defined rules. TITW-Easy is a subset derived from TITW-Hard using additional speech enhancement and data selection. We also propose common protocols for the evaluation and benchmarking of TTS solutions built using the TITW database. Last, we report the successful training of contemporary TTS models using the TITW dataset.
The full data and protocols are available at <a target="_blank" href="https://jungjee.github.io/titw" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://jungjee.github.io/titw</a> under a Creative Commons Attribution 4.0 International License.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.08711/assets/figs/pipeline_v1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="150" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">
Fully automated Text-To-Speech Synthesis In The Wild (TITW) pre-processing pipeline.
The pipeline incorporates transcription, segmentation, data selection, enhancement, and filtering based on DNSMOS.
The TITW dataset comprises two editions.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related works</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Numerous databases have been used for the training of TTS models.
Legacy databases such as CMU ARCTIC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and VCTK <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> were very carefully designed and curated. They contain phonetically-balanced utterances, all recorded in highly controlled benign acoustic environments. Because of the high cost of recording, these and similar databases usually contain data collected from only a single speaker or a modest number of speakers. Typically, the speech data they contain are neutral to emotions and expressiveness. Such data resources were widely used, mostly for the training of speaker-dependent and multiple-speaker legacy TTS systems (e.g., unit-selection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and HMM-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>).</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The revolution in deep-learning-based TTS systems called for larger-scale resources.
LJSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> contains twenty-four hours of audiobook recordings but is collected from a single speaker. The Multi-lingual Librispeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and LibriTTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> databases are also sourced from audiobook recordings but from a greater number of speakers. Both databases have been used widely for the training of deep-learning-based TTS systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.
The larger-scale corpora are neither collected entirely in high-quality recording environments nor using carefully designed text prompts. Nonetheless, their adoption marks a shift towards the use of training data collected in less controlled conditions.
Even so, the data is far from being representative of the variation in speaker style and acoustic/recording conditions which typify so-called in-the-wild data; the signal-to-noise ratio remains high and utterances are generally clearly enunciated.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>See Librivox documentation and guidelines for recording an audiobook <a target="_blank" href="https://librivox.org/pages/about-recording" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://librivox.org/pages/about-recording</a>.</span></span></span></p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The Text-to-speech synthesis In The Wild (TITW) database which we introduce in this paper is intended to support research aimed at overcoming such data constraints. We envisage TTS systems which can be trained successfully using speech data collected in uncontrolled conditions. We see two avenues for such research.
The first, most challenging direction entails the use of training data collected in the wild with minimal manual intervention, mostly only that to produce automatic transcriptions, segmentation, and data selection. The second involves the use of a subset of data stemming from additional speech enhancement and data selection.
The TITW database contains in-the-wild recordings of interviews, podcasts, and more, all posted to social media, and is, to the best of our knowledge, one of the first of its kind.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We recognize EMILIA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> as the most similar, parallel work. However, the goals of the two works differ. EMEILIA focuses on developing a data processing pipeline that yields high-quality data from in-the-wild data. Therefore, it strives to provide the highest achievable quality. TITW aims to foster research in the training of TTS systems using more noisy and real-world data and hence we provide TITW-Easy which can be used for the training of contemporary TTS systems, and TITW-Hard which targets future TTS systems to be developed.</span></span></span></p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2409.08711/assets/x1.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="531" height="440" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">
An example of the transcription and segmentation in TITW automatic pipeline.
The top spectrogram shows a randomly selected utterance from VoxCeleb1.
Through our transcription and segmentation pipeline, the middle and the bottom spectrograms are derived.
A segment in the middle is deleted because it is a non-speech segment over 500ms.
Numbers in the parentheses are word-segment scores from WhisperX.
</span></figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">TITW</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">For multiple reasons, we selected VoxCeleb1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, speech data from entirely <math id="S3.p1.1.m1.2" class="ltx_Math" alttext="1,251" display="inline"><semantics id="S3.p1.1.m1.2a"><mrow id="S3.p1.1.m1.2.3.2" xref="S3.p1.1.m1.2.3.1.cmml"><mn id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">1</mn><mo id="S3.p1.1.m1.2.3.2.1" xref="S3.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.p1.1.m1.2.2" xref="S3.p1.1.m1.2.2.cmml">251</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.2b"><list id="S3.p1.1.m1.2.3.1.cmml" xref="S3.p1.1.m1.2.3.2"><cn type="integer" id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">1</cn><cn type="integer" id="S3.p1.1.m1.2.2.cmml" xref="S3.p1.1.m1.2.2">251</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.2c">1,251</annotation></semantics></math> speakers, as source data for the TITW database. First, VoxCeleb1 is itself sourced from the wild, and it spans diverse acoustic environments.
Second, as a dataset for automatic speaker recognition, each utterance contains data collected from a single speaker.
Last, its use will help facilitate future research in speech deepfake detection and spoofing-robust automatic speaker verification, especially important to safeguard the speech generation technology from malicious usage.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Transcription and segmentation</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">Transcription.</span> Since TTS training typically requires paired speech and text data, we generated transcriptions for the entire VoxCeleb1 corpus.
Given the large number of utterances and both the time and cost involved in generating manual transcriptions, we generated transcription automatically using pretrained automatic speech recognition (ASR) models.
We used the WhisperX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> toolkit to generate transcriptions with word-level timestamps. WhisperX incorporates the OpenAI Whisper Large v2 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> for transcription and a phoneme-based another ASR model for word-level alignment.
We additionally employ the OSWMv3 speech foundation model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and transcribe the data in parallel.
Transcriptions driven with the OWSMv3 model are used for the manual inspection of transcription accuracy.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Segmentation.</span> We adopt the default WhisperX pipeline to divide each sample into isolated segments via voice activity detection (VAD), and transcribe each segment separately.
Practically, whenever a non-speech persists longer than <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="500" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mn id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><cn type="integer" id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">500</annotation></semantics></math>ms, we trimmed the silence and split it into two segments.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The applied segmentation rule is based on empirical iterative investigations.
When we initially tried to train TTS models without segmentation, too long silences within a training sample were one of the sources of TTS training failure.</span></span></span>
The above procedure results in approximately 280k transcribed speech segments.
Figure <a href="#S2.F2" title="Figure 2 ‣ II Related works ‣ Text-To-Speech Synthesis In The Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> displays an example of a VoxCeleb1 utterance (top spectrogram) which is segmented and then transcribed at the word-level.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Data selection</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">An initial round of data selection is then applied using four heuristically defined rules, the result of months of iterative efforts to train TTS models with selected data.
If any of the following conditions are met, the data is removed and discarded from further consideration.
</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">The language is not English. To simplify TTS training, we use Whisper’s language recognition capability to detect and remove utterances in languages other than English.
Multilingual extensions are left for future work.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">The segment duration is shorter than 1 s or longer than 8 s. Like many others, we found empirically that the use of utterances of such a semi-consistent duration to be beneficial to training stability.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">The per-word duration is longer than 500 ms.
The typical speaking rate is in the order of 2 words per second.
Outliers often correspond to emotional or pathological speech, or long intervals of non-speech, all of which can destabilise TTS training and are hence removed.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">The automatic transcription is empty. Such cases indicate a non-speech segment or ASR failure. In either case, they cannot be used for TTS training and are removed.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The application of transcription, segmentation and data selection results in the TITW-Hard database. Since the raw data is collected from videos posted to social media, utterances in the TITW-Hard database still contain background noise or low-quality speech.
Preliminary experiments revealed that the training of TTS models using TITW-Hard data is extremely challenging; most attempts failed to converge.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2409.08711/assets/figs/data_quality.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="298" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">
Histograms of samples in the TITW-Easy and -Hard sets. “bak”: background noise (shows denoising effect), “ovrl”: DNSMOS (shows overall quality).
</span></figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.12.2.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S3.T1.2.1" class="ltx_text" style="font-size:90%;">
Text-To-Speech Synthesis In The Wild (TITW) statistics. The dataset involves <math id="S3.T1.2.1.m1.2" class="ltx_Math" alttext="1,251" display="inline"><semantics id="S3.T1.2.1.m1.2b"><mrow id="S3.T1.2.1.m1.2.3.2" xref="S3.T1.2.1.m1.2.3.1.cmml"><mn id="S3.T1.2.1.m1.1.1" xref="S3.T1.2.1.m1.1.1.cmml">1</mn><mo id="S3.T1.2.1.m1.2.3.2.1" xref="S3.T1.2.1.m1.2.3.1.cmml">,</mo><mn id="S3.T1.2.1.m1.2.2" xref="S3.T1.2.1.m1.2.2.cmml">251</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.2.1.m1.2c"><list id="S3.T1.2.1.m1.2.3.1.cmml" xref="S3.T1.2.1.m1.2.3.2"><cn type="integer" id="S3.T1.2.1.m1.1.1.cmml" xref="S3.T1.2.1.m1.1.1">1</cn><cn type="integer" id="S3.T1.2.1.m1.2.2.cmml" xref="S3.T1.2.1.m1.2.2">251</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.1.m1.2d">1,251</annotation></semantics></math> speakers.
</span></figcaption>
<table id="S3.T1.10" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T1.10.9" class="ltx_tr">
<td id="S3.T1.10.9.1" class="ltx_td ltx_border_tt"></td>
<td id="S3.T1.10.9.2" class="ltx_td ltx_align_center ltx_border_tt"># samples</td>
<td id="S3.T1.10.9.3" class="ltx_td ltx_align_center ltx_border_tt">Avg dur (s)</td>
<td id="S3.T1.10.9.4" class="ltx_td ltx_align_center ltx_border_tt">Tot dur (h)</td>
<td id="S3.T1.10.9.5" class="ltx_td ltx_align_center ltx_border_tt">Avg # words</td>
</tr>
<tr id="S3.T1.6.4" class="ltx_tr">
<td id="S3.T1.6.4.5" class="ltx_td ltx_align_left ltx_border_tt">TITW-Easy</td>
<td id="S3.T1.3.1.1" class="ltx_td ltx_align_center ltx_border_tt"><math id="S3.T1.3.1.1.m1.2" class="ltx_Math" alttext="282,606" display="inline"><semantics id="S3.T1.3.1.1.m1.2a"><mrow id="S3.T1.3.1.1.m1.2.3.2" xref="S3.T1.3.1.1.m1.2.3.1.cmml"><mn id="S3.T1.3.1.1.m1.1.1" xref="S3.T1.3.1.1.m1.1.1.cmml">282</mn><mo id="S3.T1.3.1.1.m1.2.3.2.1" xref="S3.T1.3.1.1.m1.2.3.1.cmml">,</mo><mn id="S3.T1.3.1.1.m1.2.2" xref="S3.T1.3.1.1.m1.2.2.cmml">606</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.3.1.1.m1.2b"><list id="S3.T1.3.1.1.m1.2.3.1.cmml" xref="S3.T1.3.1.1.m1.2.3.2"><cn type="integer" id="S3.T1.3.1.1.m1.1.1.cmml" xref="S3.T1.3.1.1.m1.1.1">282</cn><cn type="integer" id="S3.T1.3.1.1.m1.2.2.cmml" xref="S3.T1.3.1.1.m1.2.2">606</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.1.1.m1.2c">282,606</annotation></semantics></math></td>
<td id="S3.T1.4.2.2" class="ltx_td ltx_align_center ltx_border_tt"><math id="S3.T1.4.2.2.m1.1" class="ltx_Math" alttext="2.42" display="inline"><semantics id="S3.T1.4.2.2.m1.1a"><mn id="S3.T1.4.2.2.m1.1.1" xref="S3.T1.4.2.2.m1.1.1.cmml">2.42</mn><annotation-xml encoding="MathML-Content" id="S3.T1.4.2.2.m1.1b"><cn type="float" id="S3.T1.4.2.2.m1.1.1.cmml" xref="S3.T1.4.2.2.m1.1.1">2.42</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.2.2.m1.1c">2.42</annotation></semantics></math></td>
<td id="S3.T1.5.3.3" class="ltx_td ltx_align_center ltx_border_tt"><math id="S3.T1.5.3.3.m1.1" class="ltx_Math" alttext="189" display="inline"><semantics id="S3.T1.5.3.3.m1.1a"><mn id="S3.T1.5.3.3.m1.1.1" xref="S3.T1.5.3.3.m1.1.1.cmml">189</mn><annotation-xml encoding="MathML-Content" id="S3.T1.5.3.3.m1.1b"><cn type="integer" id="S3.T1.5.3.3.m1.1.1.cmml" xref="S3.T1.5.3.3.m1.1.1">189</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.3.3.m1.1c">189</annotation></semantics></math></td>
<td id="S3.T1.6.4.4" class="ltx_td ltx_align_center ltx_border_tt"><math id="S3.T1.6.4.4.m1.1" class="ltx_Math" alttext="10.84" display="inline"><semantics id="S3.T1.6.4.4.m1.1a"><mn id="S3.T1.6.4.4.m1.1.1" xref="S3.T1.6.4.4.m1.1.1.cmml">10.84</mn><annotation-xml encoding="MathML-Content" id="S3.T1.6.4.4.m1.1b"><cn type="float" id="S3.T1.6.4.4.m1.1.1.cmml" xref="S3.T1.6.4.4.m1.1.1">10.84</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.4.4.m1.1c">10.84</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.10.8" class="ltx_tr">
<td id="S3.T1.10.8.5" class="ltx_td ltx_align_left ltx_border_bb">TITW-Hard</td>
<td id="S3.T1.7.5.1" class="ltx_td ltx_align_center ltx_border_bb"><math id="S3.T1.7.5.1.m1.2" class="ltx_Math" alttext="248,024" display="inline"><semantics id="S3.T1.7.5.1.m1.2a"><mrow id="S3.T1.7.5.1.m1.2.3.2" xref="S3.T1.7.5.1.m1.2.3.1.cmml"><mn id="S3.T1.7.5.1.m1.1.1" xref="S3.T1.7.5.1.m1.1.1.cmml">248</mn><mo id="S3.T1.7.5.1.m1.2.3.2.1" xref="S3.T1.7.5.1.m1.2.3.1.cmml">,</mo><mn id="S3.T1.7.5.1.m1.2.2" xref="S3.T1.7.5.1.m1.2.2.cmml">024</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.7.5.1.m1.2b"><list id="S3.T1.7.5.1.m1.2.3.1.cmml" xref="S3.T1.7.5.1.m1.2.3.2"><cn type="integer" id="S3.T1.7.5.1.m1.1.1.cmml" xref="S3.T1.7.5.1.m1.1.1">248</cn><cn type="integer" id="S3.T1.7.5.1.m1.2.2.cmml" xref="S3.T1.7.5.1.m1.2.2">024</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.5.1.m1.2c">248,024</annotation></semantics></math></td>
<td id="S3.T1.8.6.2" class="ltx_td ltx_align_center ltx_border_bb"><math id="S3.T1.8.6.2.m1.1" class="ltx_Math" alttext="2.51" display="inline"><semantics id="S3.T1.8.6.2.m1.1a"><mn id="S3.T1.8.6.2.m1.1.1" xref="S3.T1.8.6.2.m1.1.1.cmml">2.51</mn><annotation-xml encoding="MathML-Content" id="S3.T1.8.6.2.m1.1b"><cn type="float" id="S3.T1.8.6.2.m1.1.1.cmml" xref="S3.T1.8.6.2.m1.1.1">2.51</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.6.2.m1.1c">2.51</annotation></semantics></math></td>
<td id="S3.T1.9.7.3" class="ltx_td ltx_align_center ltx_border_bb"><math id="S3.T1.9.7.3.m1.1" class="ltx_Math" alttext="173" display="inline"><semantics id="S3.T1.9.7.3.m1.1a"><mn id="S3.T1.9.7.3.m1.1.1" xref="S3.T1.9.7.3.m1.1.1.cmml">173</mn><annotation-xml encoding="MathML-Content" id="S3.T1.9.7.3.m1.1b"><cn type="integer" id="S3.T1.9.7.3.m1.1.1.cmml" xref="S3.T1.9.7.3.m1.1.1">173</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.7.3.m1.1c">173</annotation></semantics></math></td>
<td id="S3.T1.10.8.4" class="ltx_td ltx_align_center ltx_border_bb"><math id="S3.T1.10.8.4.m1.1" class="ltx_Math" alttext="10.55" display="inline"><semantics id="S3.T1.10.8.4.m1.1a"><mn id="S3.T1.10.8.4.m1.1.1" xref="S3.T1.10.8.4.m1.1.1.cmml">10.55</mn><annotation-xml encoding="MathML-Content" id="S3.T1.10.8.4.m1.1b"><cn type="float" id="S3.T1.10.8.4.m1.1.1.cmml" xref="S3.T1.10.8.4.m1.1.1">10.55</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.8.4.m1.1c">10.55</annotation></semantics></math></td>
</tr>
</table>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Enhancement and DNSMOS-based further data selection</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Given the difficulty of training TTS models using the TITW-Hard database, we produced a second, relatively less challenging database named TITW-Easy.
First, we apply a pre-trained speech enhancement model, DEMUCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite><span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/facebookresearch/denoiser" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/denoiser</a></span></span></span> to attenuate additive, background noise.We then apply a second round of data selection, this time to the enhanced data. This is done by estimating DNSMOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> scores for each utterance and then by removing all utterances for which the DNSMOS ‘BAK’ score is below a threshold of 3.0 except when the segment belongs to speakers whose data is included in the generation protocol (Section <a href="#S4" title="IV Evaluation and benchmarking ‣ Text-To-Speech Synthesis In The Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>).</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Figure <a href="#S3.F3" title="Figure 3 ‣ III-B Data selection ‣ III TITW ‣ Text-To-Speech Synthesis In The Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows histograms of DNSMOS scores for both the TITW-Hard and TITW-Easy databases.
Table <a href="#S3.T2" title="Table II ‣ III-C Enhancement and DNSMOS-based further data selection ‣ III TITW ‣ Text-To-Speech Synthesis In The Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> further details the UTMOS, DNSMOS overall, and Word Error Rate (WER) of TITW-Easy and -Hard to gauge the overall quality and intelligibility.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.2.1.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S3.T2.3.2" class="ltx_text" style="font-size:90%;">
Speech quality of the TITW dataset’s Easy and Hard sets. TITW remains significantly more challenging that VCTK or even EMILIA with a DNSMOS of 3.20 and 3.26.
</span></figcaption>
<table id="S3.T2.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T2.4.1" class="ltx_tr">
<td id="S3.T2.4.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S3.T2.4.1.2" class="ltx_td ltx_align_center ltx_border_tt">UTMOS</td>
<td id="S3.T2.4.1.3" class="ltx_td ltx_align_center ltx_border_tt">DNSMOS</td>
<td id="S3.T2.4.1.4" class="ltx_td ltx_align_center ltx_border_tt">WER (%)</td>
</tr>
<tr id="S3.T2.4.2" class="ltx_tr">
<td id="S3.T2.4.2.1" class="ltx_td ltx_align_left ltx_border_tt">TITW-Hard</td>
<td id="S3.T2.4.2.2" class="ltx_td ltx_align_center ltx_border_tt">3.00</td>
<td id="S3.T2.4.2.3" class="ltx_td ltx_align_center ltx_border_tt">2.38</td>
<td id="S3.T2.4.2.4" class="ltx_td ltx_align_center ltx_border_tt">9.30</td>
</tr>
<tr id="S3.T2.4.3" class="ltx_tr">
<td id="S3.T2.4.3.1" class="ltx_td ltx_align_left ltx_border_bb">TITW-Easy</td>
<td id="S3.T2.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">3.32</td>
<td id="S3.T2.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">2.78</td>
<td id="S3.T2.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">9.10</td>
</tr>
</table>
</figure>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.10.1.1" class="ltx_text" style="font-size:90%;">TABLE III</span>: </span><span id="S3.T3.11.2" class="ltx_text" style="font-size:90%;">
Speech quality of the segments generated from the baselines on the TITW-KSKT and -KSUT protocols.
All models were trained using either the TITW-Easy or the TITW-Hard data.
</span></figcaption>
<table id="S3.T3.8" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T3.8.9" class="ltx_tr">
<td id="S3.T3.8.9.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T3.8.9.1.1" class="ltx_text">System</span></td>
<td id="S3.T3.8.9.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">TITW-KSKT</td>
<td id="S3.T3.8.9.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">TITW-KSUT</td>
</tr>
<tr id="S3.T3.8.8" class="ltx_tr">
<td id="S3.T3.1.1.1" class="ltx_td ltx_align_center">MCD<math id="S3.T3.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T3.1.1.1.m1.1a"><mo stretchy="false" id="S3.T3.1.1.1.m1.1.1" xref="S3.T3.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S3.T3.2.2.2" class="ltx_td ltx_align_center">UTMOS<math id="S3.T3.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T3.2.2.2.m1.1a"><mo stretchy="false" id="S3.T3.2.2.2.m1.1.1" xref="S3.T3.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.m1.1b"><ci id="S3.T3.2.2.2.m1.1.1.cmml" xref="S3.T3.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S3.T3.3.3.3" class="ltx_td ltx_align_center">DNSMOS<math id="S3.T3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T3.3.3.3.m1.1a"><mo stretchy="false" id="S3.T3.3.3.3.m1.1.1" xref="S3.T3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.3.m1.1b"><ci id="S3.T3.3.3.3.m1.1.1.cmml" xref="S3.T3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S3.T3.4.4.4" class="ltx_td ltx_align_center ltx_border_r">WER (%) <math id="S3.T3.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T3.4.4.4.m1.1a"><mo stretchy="false" id="S3.T3.4.4.4.m1.1.1" xref="S3.T3.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.4.m1.1b"><ci id="S3.T3.4.4.4.m1.1.1.cmml" xref="S3.T3.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S3.T3.5.5.5" class="ltx_td ltx_align_center">MCD<math id="S3.T3.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T3.5.5.5.m1.1a"><mo stretchy="false" id="S3.T3.5.5.5.m1.1.1" xref="S3.T3.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T3.5.5.5.m1.1b"><ci id="S3.T3.5.5.5.m1.1.1.cmml" xref="S3.T3.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S3.T3.6.6.6" class="ltx_td ltx_align_center">UTMOS<math id="S3.T3.6.6.6.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T3.6.6.6.m1.1a"><mo stretchy="false" id="S3.T3.6.6.6.m1.1.1" xref="S3.T3.6.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T3.6.6.6.m1.1b"><ci id="S3.T3.6.6.6.m1.1.1.cmml" xref="S3.T3.6.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.6.6.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S3.T3.7.7.7" class="ltx_td ltx_align_center">DNSMOS<math id="S3.T3.7.7.7.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T3.7.7.7.m1.1a"><mo stretchy="false" id="S3.T3.7.7.7.m1.1.1" xref="S3.T3.7.7.7.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T3.7.7.7.m1.1b"><ci id="S3.T3.7.7.7.m1.1.1.cmml" xref="S3.T3.7.7.7.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.7.7.7.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S3.T3.8.8.8" class="ltx_td ltx_align_center">WER (%) <math id="S3.T3.8.8.8.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T3.8.8.8.m1.1a"><mo stretchy="false" id="S3.T3.8.8.8.m1.1.1" xref="S3.T3.8.8.8.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T3.8.8.8.m1.1b"><ci id="S3.T3.8.8.8.m1.1.1.cmml" xref="S3.T3.8.8.8.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.8.8.8.m1.1c">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr id="S3.T3.8.10" class="ltx_tr">
<td id="S3.T3.8.10.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt">TransformerTTS-ParallelWaveGAN</td>
<td id="S3.T3.8.10.2" class="ltx_td ltx_align_center ltx_border_tt">11.68</td>
<td id="S3.T3.8.10.3" class="ltx_td ltx_align_center ltx_border_tt">2.06</td>
<td id="S3.T3.8.10.4" class="ltx_td ltx_align_center ltx_border_tt">2.50</td>
<td id="S3.T3.8.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">24.90</td>
<td id="S3.T3.8.10.6" class="ltx_td ltx_align_center ltx_border_tt">N/A</td>
<td id="S3.T3.8.10.7" class="ltx_td ltx_align_center ltx_border_tt">1.79</td>
<td id="S3.T3.8.10.8" class="ltx_td ltx_align_center ltx_border_tt">2.54</td>
<td id="S3.T3.8.10.9" class="ltx_td ltx_align_center ltx_border_tt">107.90</td>
</tr>
<tr id="S3.T3.8.11" class="ltx_tr">
<td id="S3.T3.8.11.1" class="ltx_td ltx_align_left ltx_border_r">GradTTS-DiffWave</td>
<td id="S3.T3.8.11.2" class="ltx_td ltx_align_center">6.76</td>
<td id="S3.T3.8.11.3" class="ltx_td ltx_align_center">2.18</td>
<td id="S3.T3.8.11.4" class="ltx_td ltx_align_center">2.39</td>
<td id="S3.T3.8.11.5" class="ltx_td ltx_align_center ltx_border_r">11.90</td>
<td id="S3.T3.8.11.6" class="ltx_td ltx_align_center">N/A</td>
<td id="S3.T3.8.11.7" class="ltx_td ltx_align_center">2.30</td>
<td id="S3.T3.8.11.8" class="ltx_td ltx_align_center">2.54</td>
<td id="S3.T3.8.11.9" class="ltx_td ltx_align_center">54.00</td>
</tr>
<tr id="S3.T3.8.12" class="ltx_tr">
<td id="S3.T3.8.12.1" class="ltx_td ltx_align_left ltx_border_r">VITS</td>
<td id="S3.T3.8.12.2" class="ltx_td ltx_align_center">8.61</td>
<td id="S3.T3.8.12.3" class="ltx_td ltx_align_center">2.77</td>
<td id="S3.T3.8.12.4" class="ltx_td ltx_align_center">2.74</td>
<td id="S3.T3.8.12.5" class="ltx_td ltx_align_center ltx_border_r">53.00</td>
<td id="S3.T3.8.12.6" class="ltx_td ltx_align_center">N/A</td>
<td id="S3.T3.8.12.7" class="ltx_td ltx_align_center">2.78</td>
<td id="S3.T3.8.12.8" class="ltx_td ltx_align_center">2.81</td>
<td id="S3.T3.8.12.9" class="ltx_td ltx_align_center">120.50</td>
</tr>
<tr id="S3.T3.8.13" class="ltx_tr">
<td id="S3.T3.8.13.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">MQTTS</td>
<td id="S3.T3.8.13.2" class="ltx_td ltx_align_center ltx_border_bb">6.99</td>
<td id="S3.T3.8.13.3" class="ltx_td ltx_align_center ltx_border_bb">3.08</td>
<td id="S3.T3.8.13.4" class="ltx_td ltx_align_center ltx_border_bb">2.83</td>
<td id="S3.T3.8.13.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">23.30</td>
<td id="S3.T3.8.13.6" class="ltx_td ltx_align_center ltx_border_bb">N/A</td>
<td id="S3.T3.8.13.7" class="ltx_td ltx_align_center ltx_border_bb">3.20</td>
<td id="S3.T3.8.13.8" class="ltx_td ltx_align_center ltx_border_bb">2.94</td>
<td id="S3.T3.8.13.9" class="ltx_td ltx_align_center ltx_border_bb">67.10</td>
</tr>
</table>
</figure>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T4.2.1.1" class="ltx_text" style="font-size:90%;">TABLE IV</span>: </span><span id="S3.T4.3.2" class="ltx_text" style="font-size:90%;">
Comparative results of the identical TTS system being trained with TITW-Easy and TITW-Hard data.
Metrics reported on the TITW-KSKT protocol.
</span></figcaption>
<table id="S3.T4.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T4.4.1" class="ltx_tr">
<td id="S3.T4.4.1.1" class="ltx_td ltx_align_left ltx_border_tt">System</td>
<td id="S3.T4.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Train</td>
<td id="S3.T4.4.1.3" class="ltx_td ltx_align_center ltx_border_tt">MCD</td>
<td id="S3.T4.4.1.4" class="ltx_td ltx_align_center ltx_border_tt">UTMOS</td>
<td id="S3.T4.4.1.5" class="ltx_td ltx_align_center ltx_border_tt">DNSMOS</td>
<td id="S3.T4.4.1.6" class="ltx_td ltx_align_center ltx_border_tt">WER</td>
</tr>
<tr id="S3.T4.4.2" class="ltx_tr">
<td id="S3.T4.4.2.1" class="ltx_td ltx_align_left ltx_border_tt">GTmel-DiffWave</td>
<td id="S3.T4.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Easy</td>
<td id="S3.T4.4.2.3" class="ltx_td ltx_align_center ltx_border_tt">5.05</td>
<td id="S3.T4.4.2.4" class="ltx_td ltx_align_center ltx_border_tt">2.63</td>
<td id="S3.T4.4.2.5" class="ltx_td ltx_align_center ltx_border_tt">2.68</td>
<td id="S3.T4.4.2.6" class="ltx_td ltx_align_center ltx_border_tt">11.90</td>
</tr>
<tr id="S3.T4.4.3" class="ltx_tr">
<td id="S3.T4.4.3.1" class="ltx_td ltx_align_left">GTmel-DiffWave</td>
<td id="S3.T4.4.3.2" class="ltx_td ltx_align_center ltx_border_r">Hard</td>
<td id="S3.T4.4.3.3" class="ltx_td ltx_align_center">4.97</td>
<td id="S3.T4.4.3.4" class="ltx_td ltx_align_center">2.24</td>
<td id="S3.T4.4.3.5" class="ltx_td ltx_align_center">2.30</td>
<td id="S3.T4.4.3.6" class="ltx_td ltx_align_center">12.20</td>
</tr>
<tr id="S3.T4.4.4" class="ltx_tr">
<td id="S3.T4.4.4.1" class="ltx_td ltx_align_left ltx_border_t">GradTTS-DiffWave</td>
<td id="S3.T4.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Easy</td>
<td id="S3.T4.4.4.3" class="ltx_td ltx_align_center ltx_border_t">6.76</td>
<td id="S3.T4.4.4.4" class="ltx_td ltx_align_center ltx_border_t">2.18</td>
<td id="S3.T4.4.4.5" class="ltx_td ltx_align_center ltx_border_t">2.39</td>
<td id="S3.T4.4.4.6" class="ltx_td ltx_align_center ltx_border_t">11.90</td>
</tr>
<tr id="S3.T4.4.5" class="ltx_tr">
<td id="S3.T4.4.5.1" class="ltx_td ltx_align_left">GradTTS-DiffWave</td>
<td id="S3.T4.4.5.2" class="ltx_td ltx_align_center ltx_border_r">Hard</td>
<td id="S3.T4.4.5.3" class="ltx_td ltx_align_center">8.23</td>
<td id="S3.T4.4.5.4" class="ltx_td ltx_align_center">1.29</td>
<td id="S3.T4.4.5.5" class="ltx_td ltx_align_center">1.47</td>
<td id="S3.T4.4.5.6" class="ltx_td ltx_align_center">26.20</td>
</tr>
<tr id="S3.T4.4.6" class="ltx_tr">
<td id="S3.T4.4.6.1" class="ltx_td ltx_align_left ltx_border_tt">VITS</td>
<td id="S3.T4.4.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Easy</td>
<td id="S3.T4.4.6.3" class="ltx_td ltx_align_center ltx_border_tt">8.61</td>
<td id="S3.T4.4.6.4" class="ltx_td ltx_align_center ltx_border_tt">2.77</td>
<td id="S3.T4.4.6.5" class="ltx_td ltx_align_center ltx_border_tt">2.74</td>
<td id="S3.T4.4.6.6" class="ltx_td ltx_align_center ltx_border_tt">53.00</td>
</tr>
<tr id="S3.T4.4.7" class="ltx_tr">
<td id="S3.T4.4.7.1" class="ltx_td ltx_align_left ltx_border_bb">VITS</td>
<td id="S3.T4.4.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Hard</td>
<td id="S3.T4.4.7.3" class="ltx_td ltx_align_center ltx_border_bb">9.06</td>
<td id="S3.T4.4.7.4" class="ltx_td ltx_align_center ltx_border_bb">2.48</td>
<td id="S3.T4.4.7.5" class="ltx_td ltx_align_center ltx_border_bb">2.69</td>
<td id="S3.T4.4.7.6" class="ltx_td ltx_align_center ltx_border_bb">59.50</td>
</tr>
</table>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Evaluation and benchmarking</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Once a TTS model is trained using the TITW database, it can be used with one of two protocols to generate new, synthetic speech.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.2" class="ltx_p"><span id="S4.p2.2.1" class="ltx_text ltx_font_bold">TITW-KSKT (Known Speaker, Known Text)</span> can be used to generate synthetic speech
for speakers and text which are <em id="S4.p2.2.2" class="ltx_emph ltx_font_italic">both</em> seen in TITW-Hard <em id="S4.p2.2.3" class="ltx_emph ltx_font_italic">and</em> TITW-Easy datasets.
The sets of speakers and text are both extracted arbitrarily from those contained in the VoxCeleb1-O automatic speaker verification evaluation protocol.
The number of speakers is hence the same, 40, as that for the VoxCeleb1-O evaluation protocol. However, due to the data preparation processes described in Section <a href="#S3" title="III TITW ‣ Text-To-Speech Synthesis In The Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, the number of segments is dissimilar (<math id="S4.p2.1.m1.2" class="ltx_Math" alttext="9,113" display="inline"><semantics id="S4.p2.1.m1.2a"><mrow id="S4.p2.1.m1.2.3.2" xref="S4.p2.1.m1.2.3.1.cmml"><mn id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">9</mn><mo id="S4.p2.1.m1.2.3.2.1" xref="S4.p2.1.m1.2.3.1.cmml">,</mo><mn id="S4.p2.1.m1.2.2" xref="S4.p2.1.m1.2.2.cmml">113</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.2b"><list id="S4.p2.1.m1.2.3.1.cmml" xref="S4.p2.1.m1.2.3.2"><cn type="integer" id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">9</cn><cn type="integer" id="S4.p2.1.m1.2.2.cmml" xref="S4.p2.1.m1.2.2">113</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.2c">9,113</annotation></semantics></math>, not <math id="S4.p2.2.m2.2" class="ltx_Math" alttext="4,708" display="inline"><semantics id="S4.p2.2.m2.2a"><mrow id="S4.p2.2.m2.2.3.2" xref="S4.p2.2.m2.2.3.1.cmml"><mn id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">4</mn><mo id="S4.p2.2.m2.2.3.2.1" xref="S4.p2.2.m2.2.3.1.cmml">,</mo><mn id="S4.p2.2.m2.2.2" xref="S4.p2.2.m2.2.2.cmml">708</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.2b"><list id="S4.p2.2.m2.2.3.1.cmml" xref="S4.p2.2.m2.2.3.2"><cn type="integer" id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">4</cn><cn type="integer" id="S4.p2.2.m2.2.2.cmml" xref="S4.p2.2.m2.2.2">708</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.2c">4,708</annotation></semantics></math>).</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.5" class="ltx_p"><span id="S4.p3.5.1" class="ltx_text ltx_font_bold">TITW-KSUT (Known Speaker, Unknown Text)</span> can be used to generate synthetic speech using text which is unseen in either the TITW-Hard or TITW-Easy datasets.
We use two sources of text.
The first is the Rainbow Passage <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> which covers many English sounds and their combinations and which has been used widely in other data collection efforts, for example the VCTK corpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
The second is a set of Semantically Unpredictable Sentences (SUS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> selected from past Blizzard challenges <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.
There are totally <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="200" display="inline"><semantics id="S4.p3.1.m1.1a"><mn id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><cn type="integer" id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">200</annotation></semantics></math> different text samples (<math id="S4.p3.2.m2.1" class="ltx_Math" alttext="31" display="inline"><semantics id="S4.p3.2.m2.1a"><mn id="S4.p3.2.m2.1.1" xref="S4.p3.2.m2.1.1.cmml">31</mn><annotation-xml encoding="MathML-Content" id="S4.p3.2.m2.1b"><cn type="integer" id="S4.p3.2.m2.1.1.cmml" xref="S4.p3.2.m2.1.1">31</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.2.m2.1c">31</annotation></semantics></math> from The Rainbow Passage and <math id="S4.p3.3.m3.1" class="ltx_Math" alttext="169" display="inline"><semantics id="S4.p3.3.m3.1a"><mn id="S4.p3.3.m3.1.1" xref="S4.p3.3.m3.1.1.cmml">169</mn><annotation-xml encoding="MathML-Content" id="S4.p3.3.m3.1b"><cn type="integer" id="S4.p3.3.m3.1.1.cmml" xref="S4.p3.3.m3.1.1">169</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.3.m3.1c">169</annotation></semantics></math> from the set of SUS).
With the same set of 40 speakers, the protocol stipulates the generation of <math id="S4.p3.4.m4.2" class="ltx_Math" alttext="8,000" display="inline"><semantics id="S4.p3.4.m4.2a"><mrow id="S4.p3.4.m4.2.3.2" xref="S4.p3.4.m4.2.3.1.cmml"><mn id="S4.p3.4.m4.1.1" xref="S4.p3.4.m4.1.1.cmml">8</mn><mo id="S4.p3.4.m4.2.3.2.1" xref="S4.p3.4.m4.2.3.1.cmml">,</mo><mn id="S4.p3.4.m4.2.2" xref="S4.p3.4.m4.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.4.m4.2b"><list id="S4.p3.4.m4.2.3.1.cmml" xref="S4.p3.4.m4.2.3.2"><cn type="integer" id="S4.p3.4.m4.1.1.cmml" xref="S4.p3.4.m4.1.1">8</cn><cn type="integer" id="S4.p3.4.m4.2.2.cmml" xref="S4.p3.4.m4.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.4.m4.2c">8,000</annotation></semantics></math> (<math id="S4.p3.5.m5.1" class="ltx_Math" alttext="=40\times 200" display="inline"><semantics id="S4.p3.5.m5.1a"><mrow id="S4.p3.5.m5.1.1" xref="S4.p3.5.m5.1.1.cmml"><mi id="S4.p3.5.m5.1.1.2" xref="S4.p3.5.m5.1.1.2.cmml"></mi><mo id="S4.p3.5.m5.1.1.1" xref="S4.p3.5.m5.1.1.1.cmml">=</mo><mrow id="S4.p3.5.m5.1.1.3" xref="S4.p3.5.m5.1.1.3.cmml"><mn id="S4.p3.5.m5.1.1.3.2" xref="S4.p3.5.m5.1.1.3.2.cmml">40</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p3.5.m5.1.1.3.1" xref="S4.p3.5.m5.1.1.3.1.cmml">×</mo><mn id="S4.p3.5.m5.1.1.3.3" xref="S4.p3.5.m5.1.1.3.3.cmml">200</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.5.m5.1b"><apply id="S4.p3.5.m5.1.1.cmml" xref="S4.p3.5.m5.1.1"><eq id="S4.p3.5.m5.1.1.1.cmml" xref="S4.p3.5.m5.1.1.1"></eq><csymbol cd="latexml" id="S4.p3.5.m5.1.1.2.cmml" xref="S4.p3.5.m5.1.1.2">absent</csymbol><apply id="S4.p3.5.m5.1.1.3.cmml" xref="S4.p3.5.m5.1.1.3"><times id="S4.p3.5.m5.1.1.3.1.cmml" xref="S4.p3.5.m5.1.1.3.1"></times><cn type="integer" id="S4.p3.5.m5.1.1.3.2.cmml" xref="S4.p3.5.m5.1.1.3.2">40</cn><cn type="integer" id="S4.p3.5.m5.1.1.3.3.cmml" xref="S4.p3.5.m5.1.1.3.3">200</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.5.m5.1c">=40\times 200</annotation></semantics></math>) synthetic utterances.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We now describe the automated evaluation pipeline which can be used to benchmark competing TTS models.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Metrics</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We adopt four metrics to assess the quality of generated synthetic speech: Mel Cepstral Distortion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> (MCD); UTMOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>; DNSMOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>; the ASR WER.
We use all four metrics as different proxies for speech quality.
The WER is evaluated using the OpenAI Whisper-Large model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.
In practice, we use the VERSA toolkit to compute all four metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">TTS training data</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">As a reference, we first assessed the two TITW datasets among others commonly used for TTS training.
Results shown in Table <a href="#S3.T2" title="Table II ‣ III-C Enhancement and DNSMOS-based further data selection ‣ III TITW ‣ Text-To-Speech Synthesis In The Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> show that the TITW-Easy dataset is of greater quality than the TITW-Hard dataset.
As expected, the quality of data in both TITW datasets is less than that used typically for TTS training. By way of example,
the DNOSMOS for the VCTK database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> is 3.20. That for the MLS database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> is 3.33 while that for EMILIA database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> is 3.22.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.5.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.6.2" class="ltx_text ltx_font_italic">Baseline TTS systems</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We also show results of four different TTS systems all trained using TITW datasets: (i) TransformerTTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> with ParallelWaveGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>; (ii) GradTTS-DiffWave <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>; (iii) VITS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>; (iv) MQTTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
These four systems provide a reasonably diverse mix of contemporary TTS systems.
Table <a href="#S3.T3" title="Table III ‣ III-C Enhancement and DNSMOS-based further data selection ‣ III TITW ‣ Text-To-Speech Synthesis In The Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> shows results for the four TTS models trained using the TITW-Easy dataset and reports the results on the two protocols.
Results show that all four systems are successfully trained, yet suffer from the challenging nature of the TITW data. UTMOS and DNSMOS of the synthesized speech remain comparable, however, WER increases significantly in most cases.
The limitations of the baselines are shown further on TITW-KSUT results.
All performances degrade compared to that on the TITW-KSKT protocol.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Table <a href="#S3.T4" title="Table IV ‣ III-C Enhancement and DNSMOS-based further data selection ‣ III TITW ‣ Text-To-Speech Synthesis In The Wild" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> describes the comparison of models trained on the TITW-Easy and -Hard data.
We employ two systems, GradTTS-DiffWave and VITS systems for this exploration; the other two baseline systems were not successful when trained with TITW-Hard.
We also present the result replacing GradTTS with a mel-spectrogram extracted from the original speech file (i.e., copy synthesis), which serves as the upper bound for the waveform model, DiffWave.
Results consistently confirm that in all cases models trained on the TITW-Hard data generate speech with worse quality, demonstrating the challenging nature of TITW-Hard.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion and remarks</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We introduce TITW, a new dataset for the training, evaluation and benchmarking of TTS systems using data collected in the wild.
The dataset was designed in response to current trends in TTS research which show a drive to develop resources and techniques to train TTS systems without studio-quality data.
We devised a fully automated processing pipeline and, while it is readily extendible to a larger scale, we describe its application to the VoxCeleb1 database.
We show that four contemporary TTS systems trained using the TITW-Easy dataset produce synthetic speech of quality close to that of the training data itself and not far from the quality of some studio-quality training databases.
Nonetheless, this is achieved only with the application of speech enhancement and data selection.
Use of the original data sourced from videos posted to social media remains challenging, even if results show potential.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Results from two years of work (not reported in this paper) show that only recent deep-learning-based TTS systems can be trained using the TITW database; our attempts to train legacy TTS systems were largely unsuccessful.
Training is also sensitive to data preparation which might explain why attempts to train TTS systems with data collected in the wild remain sparse in the literature.
We hope that the release of the TITW database into the public domain will fuel greater interest and research effort in the future, that it might help others to develop TTS systems for under-resourced languages for which studio-quality training data is lacking, and that it might also contribute to the development of systems with the capacity to generate more natural and expressive synthetic speech more typical of that encountered in the wild.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
M. Le, A. Vyas, B. Shi, B. Karrer <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Voicebox: Text-guided multilingual universal speech generation at scale,” in <em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic">Proc. NeurIPS</em>, 2024.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S. Kim, K. Shih, J. F. Santos, E. Bakhturina <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “P-flow: a fast and data-efficient zero-shot TTS through speech prompting,” in <em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">Proc. NeurIPS</em>, 2024.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
T. D. Nguyen, J.-H. Kim, Y. Jang, J. Kim, and J. S. Chung, “Fregrad: Lightweight and fast frequency-aware diffusion vocoder,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE ICASSP</em>, 2024.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
X. Zhang, D. Zhang, S. Li, Y. Zhou, and X. Qiu, “Speechtokenizer: Unified speech tokenizer for speech large language models,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proc. ICLR</em>, 2024.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
D. Yang, J. Tian, X. Tan, R. Huang, S. Liu, X. Chang, J. Shi, S. Zhao, J. Bian, X. Wu <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “UniAudio: An audio foundation model toward universal audio generation,” in <em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic">Proc. ICML</em>, 2024.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J.-H. Lee, S.-H. Lee, J.-H. Kim, and S.-W. Lee, “PVAE-TTS: Adaptive text-to-speech via progressive style adaptation,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE ICASSP</em>, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
E. Kharitonov, D. Vincent, Z. Borsos, R. Marinier, S. Girgin, O. Pietquin, M. Sharifi, M. Tagliasacchi, and N. Zeghidour, “Speak, read and prompt: High-fidelity text-to-speech with minimal supervision,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. Kim, K. Lee, S. Chung, and J. Cho, “Clam-TTS: Improving neural codec language model for zero-shot text-to-speech,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proc. ICLR</em>, 2024.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
E. Song, R. Yamamoto, O. Kwon, C.-H. Song, M.-J. Hwang, S. Oh, H.-W. Yoon, J.-S. Kim, and J.-M. Kim, “TTS-by-TTS 2: Data-selective augmentation for neural speech synthesis using ranking support vector machine with variational autoencoder,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. Yamagishi, B. Usabaev, S. King, O. Watts, J. Dines, J. Tian, Y. Guan, R. Hu, K. Oura, Y.-J. Wu <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Thousands of voices for HMM-based speech synthesis–Analysis and application of TTS systems built on various ASR corpora,” <em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic">IEEE Transactions on Audio, Speech, and Language Processing</em>, vol. 18, no. 5, pp. 984–1004, 2010.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
R. Karhila, U. Remes, and M. Kurimo, “Noise in HMM-Based Speech Synthesis Adaptation: Analysis, Evaluation Methods and Experiments,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, vol. 8, no. 2, pp. 285–295, Apr. 2014.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
C. Zhang, Y. Ren, X. Tan, J. Liu, K. Zhang, T. Qin, S. Zhao, and T.-Y. Liu, “DenoiSpeech: Denoising text to speech with frame-level noise modeling,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE ICASSP</em>, 2021, pp. 7063–7067.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
K. Fujita, H. Sato, T. Ashihara, H. Kanagawa, M. Delcroix, T. Moriya, and Y. Ijima, “Noise-robust zero-shot text-to-speech synthesis conditioned on self-supervised speech-representation model with adapters,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE ICASSP</em>, 2024.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
K. Fujita, T. Ashihara, H. Kanagawa, T. Moriya, and Y. Ijima, “Zero-shot text-to-speech synthesis conditioned using self-supervised speech representation model,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE ICASSP</em>, 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
T. Saeki, K. Tachibana, and R. Yamamoto, “DRSpeech: Degradation-robust text-to-speech synthesis with frame-level and utterance-level acoustic representation learning,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S. Ogun, V. Colotte, and E. Vincent, “Can we use Common Voice to train a multi-speaker TTS system?” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proc. SLT</em>, 2023, pp. 900–905.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
X. Wang, H. Delgado, H. Tak, J.-w. Jung <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “ASVspoof 5: Crowdsourced speech data, deepfakes, and adversarial attacks at scale,” in <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2024.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
V. Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert, “MLS: A large-scale multilingual dataset for speech research,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
L.-W. Chen, S. Watanabe, and A. Rudnicky, “A vector quantized approach for text to speech synthesis on real-world spontaneous speech,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proc. AAAI</em>, 2023, pp. 12 644–12 652.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A. Nagrani, J. S. Chung, and A. Zisserman, “VoxCeleb: A large-scale speaker identification dataset,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2017.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J. Kominek and A. W. Black, “The CMU arctic speech databases,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2004.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J. Yamagishi, C. Veaux, and K. MacDonald, “CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92),” 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A. J. Hunt and A. W. Black, “Unit selection in a concatenative speech synthesis system using a large speech database,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE ICASSP</em>, 1996.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
K. Tokuda, Y. Nankaku, T. Toda, H. Zen, J. Yamagishi, and K. Oura, “Speech synthesis based on hidden Markov models,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE</em>, vol. 101, no. 5, pp. 1234–1252, 2013.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
K. Ito and L. Johnson, “The LJ speech dataset,” <a target="_blank" href="https://keithito.com/LJ-Speech-Dataset/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://keithito.com/LJ-Speech-Dataset/</a>, 2017.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and Y. Wu, “LibriTTS: A corpus derived from librispeech for text-to-speech,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2019.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Neural codec language models are zero-shot text to speech synthesizers,” <em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.02111</em>, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
K. Shen, Z. Ju, X. Tan, E. Liu, Y. Leng, L. He, T. Qin, J. Bian <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “NaturalSpeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers,” in <em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic">Proc. ICLR</em>, 2024.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
H. He, Z. Shang, C. Wang, X. Li, Y. Gu, H. Hua, L. Liu, C. Yang, J. Li, P. Shi <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Emilia: An extensive, multilingual, and diverse speech dataset for large-scale speech generation,” <em id="bib.bib29.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2407.05361</em>, 2024.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
M. Bain, J. Huh, T. Han, and A. Zisserman, “WhisperX: Time-accurate speech transcription of long-form audio,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2023, pp. 4489–4493.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, “Robust speech recognition via large-scale weak supervision,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proc. ICML</em>, 2023, pp. 28 492–28 518.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Y. Peng, J. Tian, W. Chen, S. Arora, B. Yan, Y. Sudo, M. Shakeel, K. Choi, J. Shi, X. Chang, J.-w. Jung, and S. Watanabe, “OWSM v3.1: Better and faster open Whisper-style speech models based on e-branchformer,” in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2024, pp. 352–356.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
A. Défossez, G. Synnaeve, and Y. Adi, “Real time speech enhancement in the waveform domain,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2020, pp. 3291–3295.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
C. K. Reddy, V. Gopal, and R. Cutler, “DNSMOS: A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE ICASSP</em>, 2021, pp. 6493–6497.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
——, “DNSMOS P.835: A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE ICASSP</em>, 2022, pp. 886–890.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
G. Fairbanks, “Voice and articulation drillbook,” 1960.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
C. Benoît, M. Grice, and V. Hazan, “The SUS test: A method for the assessment of text-to-speech synthesis intelligibility using semantically unpredictable sentences,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Speech Communication</em>, vol. 18, no. 4, pp. 381–392, 1996.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
S. King, “Measuring a decade of progress in text-to-speech,” <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Loquens</em>, vol. 1, no. 1, pp. e006–e006, 2014.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
R. Kubichek, “Mel-cepstral distance measure for objective speech quality assessment,” in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE pacific rim conference on communications computers and signal processing</em>, 1993.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
T. Saeki, D. Xin, W. Nakata, T. Koriyama, S. Takamichi, and H. Saruwatari, “UTMOS: UTokyo-SaruLab system for VoiceMOS challenge 2022,” in <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2022.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, “Robust speech recognition via large-scale weak supervision,” in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proc. ICML</em>.   PMLR, 2023.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
J. Shi, J. Tian, Y. Wu, J.-w. Jung <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “ESPnet-Codec: Comprehensive training and evaluation of neural codecs for audio, music, and speech,” in <em id="bib.bib42.2.2" class="ltx_emph ltx_font_italic">Proc. SLT</em>, 2024.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
C. Veaux, J. Yamagishi, and K. MacDonald, “CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit,” The Centre for Speech Technology Research (CSTR), University of Edinburgh, 2019.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
N. Li, S. Liu, Y. Liu, S. Zhao, and M. Liu, “Neural speech synthesis with transformer network,” in <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proc. AAAI</em>, 2019, pp. 6706–6713.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
R. Yamamoto, E. Song, and J.-M. Kim, “Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram,” in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE ICASSP</em>, 2020, pp. 6199–6203.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
V. Popov, I. Vovk, V. Gogoryan, T. Sadekova, and M. Kudinov, “Grad-TTS: A diffusion probabilistic model for text-to-speech,” in <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proc. ICML</em>, 2021, pp. 8599–8608.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, “DiffWave: A versatile diffusion model for audio synthesis,” in <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proc. ICLR</em>, 2021.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
J. Kim, J. Kong, and J. Son, “Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech,” in <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proc. ICML</em>, 2021, pp. 5530–5540.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.08710" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.08711" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.08711">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.08711" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.08712" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 20:15:54 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
