<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.15216] Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition</title><meta property="og:description" content="Language models (LMs) have long been used to improve results of automatic speech recognition (ASR) systems, but they are unaware of the errors that ASR systems make.
Error correction models are designed to fix ASR erroâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.15216">

<!--Generated on Wed Jun  5 18:58:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zijin Gu, Tatiana Likhomanenko, He Bai, Erik McDermott, Ronan Collobert, Navdeep Jaitly 
<br class="ltx_break">Apple 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">{zgu26,antares,hbai22,erik_mcdermott,collobert,njaitly}@apple.com</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Language models (LMs) have long been used to improve results of automatic speech recognition (ASR) systems, but they are unaware of the errors that ASR systems make.
Error correction models are designed to fix ASR errors, however, they showed little improvement over traditional LMs mainly due to the lack of supervised training data.
In this paper, we present Denoising LM (DLM), which is a <span id="id2.id1.1" class="ltx_text ltx_font_italic">scaled</span> error correction model trained with vast amounts of synthetic data, significantly exceeding prior attempts meanwhile achieving new state-of-the-art ASR performance.
We use text-to-speech (TTS) systems to synthesize audio, which is fed into an ASR system to produce noisy hypotheses, which are then paired with the original texts to train the DLM.
DLM has several <span id="id2.id1.2" class="ltx_text ltx_font_italic">key ingredients</span>: (i) up-scaled model and data; (ii) usage of multi-speaker TTS systems; (iii) combination of multiple noise augmentation strategies; and (iv) new decoding techniques.
With a Transformer-CTC ASR, DLM achieves 1.5% word error rate (WER) on <span id="id2.id1.3" class="ltx_text ltx_font_italic">test-clean</span> and 3.3% WER on <span id="id2.id1.4" class="ltx_text ltx_font_italic">test-other</span> on Librispeech, which to our knowledge are the best reported numbers in the setting where no external audio data are used and even match self-supervised methods which use external audio data. Furthermore, a single DLM is applicable to different ASRs, and greatly surpassing the performance of conventional LM based beam-search rescoring.
These results indicate that properly investigated error correction models have the potential to replace conventional LMs, holding the key to a new level of accuracy in ASR systems.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Language models (LMs) have long been used to rerank hypotheses produced by automatic speech recognition (ASR) systems. In order to do so, the acoustic score of each hypothesis from the ASR is blended with the LM score for the hypothesis.
However, LMs are trained on text corpora and are agnostic to the mistakes made by ASR. Thus significant work has gone into developing techniques to train ASR systems that incorporate an existing LM, so that the combined system works well.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Error correction models, on the other hand, are trained specifically to correct errors from ASR models, mapping hypotheses (â€œnoisyâ€ text from the ASR system) to the clean text, thus it sidesteps the problem of learning the correct weighting between the LM and the ASR. This can be an especially useful strategy when the ASR model has limited capacity and cannot produce structured output â€“ for example with non-autoregressive ASR models. In those cases, a sequence-to-sequence error correction model can turn approximately correct outputs into structured outputs. Further, it can also take the text normalization role, simplifying the overall pipeline for ASR system.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">One hurdle that arises is that error correction models typically require a large number of supervised training examples, but the number of utterances in a typical ASR dataset is comparatively small.
In this paper, we propose <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">Denoising LM (DLM)</span>, an ASR error correction model that uses text-to-speech (TTS) systems to avoid this data scarcity issue. Text from a language model corpus can be fed into a TTS system to produce audio, which is in turn fed into an ASR system to generate hypotheses. These hypotheses are then paired with the original text, to form the training dataset. The amount of training data can thus be scaled up arbitrarily by using a larger language corpus. Our key contributions are outlined below:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">To our knowledge, we are the first to make ASR error correction model state-of-the-art (SOTA) and show it can be significantly better than traditional neural LM. The proposed DLM achieves word error rate (WER) on LibriSpeechÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> â€“ 1.4% (3.1%) on <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">dev-clean (other)</span> and 1.5% (3.3%) on <span id="S1.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">test-clean (other)</span>, without using any external audio data.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We present DLM with its working recipe â€“ there are several key ingredients that distinguish DLM from previous attempts and make error correction models work in practice:</p>
<ul id="S1.I1.i2.I1" class="ltx_itemize">
<li id="S1.I1.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S1.I1.i2.I1.i1.1.1.1" class="ltx_text ltx_font_bold">â€“</span></span> 
<div id="S1.I1.i2.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i2.I1.i1.p1.1" class="ltx_p">We use multiple zero-shot, multi-speaker TTS systems to generate audio in different styles.</p>
</div>
</li>
<li id="S1.I1.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S1.I1.i2.I1.i2.1.1.1" class="ltx_text ltx_font_bold">â€“</span></span> 
<div id="S1.I1.i2.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.I1.i2.p1.1" class="ltx_p">We mix real and synthetic data during training to keep the DLM grounded.</p>
</div>
</li>
<li id="S1.I1.i2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S1.I1.i2.I1.i3.1.1.1" class="ltx_text ltx_font_bold">â€“</span></span> 
<div id="S1.I1.i2.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i2.I1.i3.p1.1" class="ltx_p">We combine multiple noise augmentation strategies such as frequency masking of spectral features and random substitution of characters of ASR hypotheses.</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">The proposed DLM demonstrates good LM characteristics:</p>
<ul id="S1.I1.i3.I1" class="ltx_itemize">
<li id="S1.I1.i3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S1.I1.i3.I1.i1.1.1.1" class="ltx_text ltx_font_bold">â€“</span></span> 
<div id="S1.I1.i3.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i3.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i3.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Universality</span>: the same DLM is applicable to ASR systems of different architectures and denoises different datasets.</p>
</div>
</li>
<li id="S1.I1.i3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S1.I1.i3.I1.i2.1.1.1" class="ltx_text ltx_font_bold">â€“</span></span> 
<div id="S1.I1.i3.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i3.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i3.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Scalability</span>: the performance of DLMs improves with the increasing number of speakers, the size of the model, and the size of training text corpus.</p>
</div>
</li>
<li id="S1.I1.i3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S1.I1.i3.I1.i3.1.1.1" class="ltx_text ltx_font_bold">â€“</span></span> 
<div id="S1.I1.i3.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Efficiency</span>: DLMs can match traditional neural LM results by simply performing greedy decoding, with no need of heavy beam search decoding and rescoring.</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The success of DLMs suggests that instead of complicated LM integration techniques, error correction models show more promising possibilities to bring the next level ASR accuracy.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Finding better techniques to leverage LMs most effectively with neural acoustic models has been a long standing research problemÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>.
Earlier techniques back-propagated errors from LMs into neural ASRs using sequence discriminative criterion (e.g. Maximum Mutual Information)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>.
Later approaches attempted to merge features of text-only LMs with features of ASRs in either the shallower or deeper layers of the modelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.2" class="ltx_p">Recently, error correction models, which post-process outputs from an ASR system to fix errors, have also been proposedÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.
These approaches typically convert first-pass ASR hypotheses into cleaned up text.
Even with just one best hypothesis from the ASR model, Transformer-based error correction models have shown improvements over the baseline acoustic models and acoustic models with <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.p2.1.m1.1a"><mi id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">n</annotation></semantics></math>-gram language modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. Further improvements can be made by using <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.p2.2.m2.1a"><mi id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">n</annotation></semantics></math>-best ASR hypotheses as inputs to the clean up model as this provides more informationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>; by using more advanced WER based metrics, instead of cross-entropy for model optimizationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>; or by using different variants that incorporate more compact inputs, such as phonemesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and word latticesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
More recently, large language models (LLMs) such as ChatGPT and LLaMA, were also shown to improve the transcription accuracy with their powerful linguistic representationsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">However, the efficacy of these error correction approaches suffers due to the scarcity of the paired ASR output and ground truth transcriptions and <span id="S2.p3.1.1" class="ltx_text ltx_font_italic">thus were not able to outperform conventional neural LMs integration yet.</span>
As a result most clean up approaches start with a pretrained language model, e.g., BARTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, that is finetuned with the limited noisy ASR dataÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
Others incorporate different data augmentation strategies, such as SpecAugmentÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and dropoutÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.
While these are helpful, the improvement may be limited by the data size.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> have attempted to resolve this problem by using synthetic data for training error correction models on top of listen, attend and spell (LAS)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> models. They show that error correction can improve results when combined with neural LMs, but error correction by itself is inferior to neural LM integration.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Another line of work is followed byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.
These papers attempt to subtract the language model learned in the end-to-end ASR models, while integrating external LMs.
Nevertheless, we believe that any combination of independently designed ASR and LM models is inherently limited by the the ability to optimize the model subtraction, and the inherent lower expressive power of the combination method.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Denoising Speech Recognition (DSR)</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.7" class="ltx_p">To motivate error correction approach and our variant of it, we consider a probabilistic model <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="p_{\tt{DSR}}(y|x)" display="inline"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><msub id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml"><mi id="S3.p1.1.m1.1.1.3.2" xref="S3.p1.1.m1.1.1.3.2.cmml">p</mi><mi id="S3.p1.1.m1.1.1.3.3" xref="S3.p1.1.m1.1.1.3.3.cmml">ğ™³ğš‚ğš</mi></msub><mo lspace="0em" rspace="0em" id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">â€‹</mo><mrow id="S3.p1.1.m1.1.1.1.1" xref="S3.p1.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.p1.1.m1.1.1.1.1.2" xref="S3.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.p1.1.m1.1.1.1.1.1" xref="S3.p1.1.m1.1.1.1.1.1.cmml"><mi id="S3.p1.1.m1.1.1.1.1.1.2" xref="S3.p1.1.m1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S3.p1.1.m1.1.1.1.1.1.1" xref="S3.p1.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.p1.1.m1.1.1.1.1.1.3" xref="S3.p1.1.m1.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.p1.1.m1.1.1.1.1.3" xref="S3.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><times id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2"></times><apply id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.3.1.cmml" xref="S3.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.p1.1.m1.1.1.3.2.cmml" xref="S3.p1.1.m1.1.1.3.2">ğ‘</ci><ci id="S3.p1.1.m1.1.1.3.3.cmml" xref="S3.p1.1.m1.1.1.3.3">ğ™³ğš‚ğš</ci></apply><apply id="S3.p1.1.m1.1.1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1.1"><csymbol cd="latexml" id="S3.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.1.1.1.2">ğ‘¦</ci><ci id="S3.p1.1.m1.1.1.1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.1.1.1.3">ğ‘¥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">p_{\tt{DSR}}(y|x)</annotation></semantics></math>, which we call a error correcting speech recognition (DSR) model. DSR is a cascade of two stochastic, discrete models â€“ an ASR model which produces sequencesÂ <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="\tilde{y}" display="inline"><semantics id="S3.p1.2.m2.1a"><mover accent="true" id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">y</mi><mo id="S3.p1.2.m2.1.1.1" xref="S3.p1.2.m2.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><ci id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1.1">~</ci><ci id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">\tilde{y}</annotation></semantics></math> of discrete tokens from audio input <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">x</annotation></semantics></math> with probability <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="p_{\tt{ASR}}(\tilde{y}|x)" display="inline"><semantics id="S3.p1.4.m4.1a"><mrow id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml"><msub id="S3.p1.4.m4.1.1.3" xref="S3.p1.4.m4.1.1.3.cmml"><mi id="S3.p1.4.m4.1.1.3.2" xref="S3.p1.4.m4.1.1.3.2.cmml">p</mi><mi id="S3.p1.4.m4.1.1.3.3" xref="S3.p1.4.m4.1.1.3.3.cmml">ğ™°ğš‚ğš</mi></msub><mo lspace="0em" rspace="0em" id="S3.p1.4.m4.1.1.2" xref="S3.p1.4.m4.1.1.2.cmml">â€‹</mo><mrow id="S3.p1.4.m4.1.1.1.1" xref="S3.p1.4.m4.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.p1.4.m4.1.1.1.1.2" xref="S3.p1.4.m4.1.1.1.1.1.cmml">(</mo><mrow id="S3.p1.4.m4.1.1.1.1.1" xref="S3.p1.4.m4.1.1.1.1.1.cmml"><mover accent="true" id="S3.p1.4.m4.1.1.1.1.1.2" xref="S3.p1.4.m4.1.1.1.1.1.2.cmml"><mi id="S3.p1.4.m4.1.1.1.1.1.2.2" xref="S3.p1.4.m4.1.1.1.1.1.2.2.cmml">y</mi><mo id="S3.p1.4.m4.1.1.1.1.1.2.1" xref="S3.p1.4.m4.1.1.1.1.1.2.1.cmml">~</mo></mover><mo fence="false" id="S3.p1.4.m4.1.1.1.1.1.1" xref="S3.p1.4.m4.1.1.1.1.1.1.cmml">|</mo><mi id="S3.p1.4.m4.1.1.1.1.1.3" xref="S3.p1.4.m4.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.p1.4.m4.1.1.1.1.3" xref="S3.p1.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1"><times id="S3.p1.4.m4.1.1.2.cmml" xref="S3.p1.4.m4.1.1.2"></times><apply id="S3.p1.4.m4.1.1.3.cmml" xref="S3.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.3.1.cmml" xref="S3.p1.4.m4.1.1.3">subscript</csymbol><ci id="S3.p1.4.m4.1.1.3.2.cmml" xref="S3.p1.4.m4.1.1.3.2">ğ‘</ci><ci id="S3.p1.4.m4.1.1.3.3.cmml" xref="S3.p1.4.m4.1.1.3.3">ğ™°ğš‚ğš</ci></apply><apply id="S3.p1.4.m4.1.1.1.1.1.cmml" xref="S3.p1.4.m4.1.1.1.1"><csymbol cd="latexml" id="S3.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.p1.4.m4.1.1.1.1.1.1">conditional</csymbol><apply id="S3.p1.4.m4.1.1.1.1.1.2.cmml" xref="S3.p1.4.m4.1.1.1.1.1.2"><ci id="S3.p1.4.m4.1.1.1.1.1.2.1.cmml" xref="S3.p1.4.m4.1.1.1.1.1.2.1">~</ci><ci id="S3.p1.4.m4.1.1.1.1.1.2.2.cmml" xref="S3.p1.4.m4.1.1.1.1.1.2.2">ğ‘¦</ci></apply><ci id="S3.p1.4.m4.1.1.1.1.1.3.cmml" xref="S3.p1.4.m4.1.1.1.1.1.3">ğ‘¥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">p_{\tt{ASR}}(\tilde{y}|x)</annotation></semantics></math>, and a DLM which transforms an input sequence, <math id="S3.p1.5.m5.1" class="ltx_Math" alttext="\tilde{y}" display="inline"><semantics id="S3.p1.5.m5.1a"><mover accent="true" id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml"><mi id="S3.p1.5.m5.1.1.2" xref="S3.p1.5.m5.1.1.2.cmml">y</mi><mo id="S3.p1.5.m5.1.1.1" xref="S3.p1.5.m5.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><apply id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1"><ci id="S3.p1.5.m5.1.1.1.cmml" xref="S3.p1.5.m5.1.1.1">~</ci><ci id="S3.p1.5.m5.1.1.2.cmml" xref="S3.p1.5.m5.1.1.2">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">\tilde{y}</annotation></semantics></math>, into an output sequence <math id="S3.p1.6.m6.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.p1.6.m6.1a"><mi id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.1b"><ci id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.1c">y</annotation></semantics></math> with probability <math id="S3.p1.7.m7.1" class="ltx_Math" alttext="p_{\tt{DLM}}(y|\tilde{y})" display="inline"><semantics id="S3.p1.7.m7.1a"><mrow id="S3.p1.7.m7.1.1" xref="S3.p1.7.m7.1.1.cmml"><msub id="S3.p1.7.m7.1.1.3" xref="S3.p1.7.m7.1.1.3.cmml"><mi id="S3.p1.7.m7.1.1.3.2" xref="S3.p1.7.m7.1.1.3.2.cmml">p</mi><mi id="S3.p1.7.m7.1.1.3.3" xref="S3.p1.7.m7.1.1.3.3.cmml">ğ™³ğ™»ğ™¼</mi></msub><mo lspace="0em" rspace="0em" id="S3.p1.7.m7.1.1.2" xref="S3.p1.7.m7.1.1.2.cmml">â€‹</mo><mrow id="S3.p1.7.m7.1.1.1.1" xref="S3.p1.7.m7.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.p1.7.m7.1.1.1.1.2" xref="S3.p1.7.m7.1.1.1.1.1.cmml">(</mo><mrow id="S3.p1.7.m7.1.1.1.1.1" xref="S3.p1.7.m7.1.1.1.1.1.cmml"><mi id="S3.p1.7.m7.1.1.1.1.1.2" xref="S3.p1.7.m7.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S3.p1.7.m7.1.1.1.1.1.1" xref="S3.p1.7.m7.1.1.1.1.1.1.cmml">|</mo><mover accent="true" id="S3.p1.7.m7.1.1.1.1.1.3" xref="S3.p1.7.m7.1.1.1.1.1.3.cmml"><mi id="S3.p1.7.m7.1.1.1.1.1.3.2" xref="S3.p1.7.m7.1.1.1.1.1.3.2.cmml">y</mi><mo id="S3.p1.7.m7.1.1.1.1.1.3.1" xref="S3.p1.7.m7.1.1.1.1.1.3.1.cmml">~</mo></mover></mrow><mo stretchy="false" id="S3.p1.7.m7.1.1.1.1.3" xref="S3.p1.7.m7.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.7.m7.1b"><apply id="S3.p1.7.m7.1.1.cmml" xref="S3.p1.7.m7.1.1"><times id="S3.p1.7.m7.1.1.2.cmml" xref="S3.p1.7.m7.1.1.2"></times><apply id="S3.p1.7.m7.1.1.3.cmml" xref="S3.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.p1.7.m7.1.1.3.1.cmml" xref="S3.p1.7.m7.1.1.3">subscript</csymbol><ci id="S3.p1.7.m7.1.1.3.2.cmml" xref="S3.p1.7.m7.1.1.3.2">ğ‘</ci><ci id="S3.p1.7.m7.1.1.3.3.cmml" xref="S3.p1.7.m7.1.1.3.3">ğ™³ğ™»ğ™¼</ci></apply><apply id="S3.p1.7.m7.1.1.1.1.1.cmml" xref="S3.p1.7.m7.1.1.1.1"><csymbol cd="latexml" id="S3.p1.7.m7.1.1.1.1.1.1.cmml" xref="S3.p1.7.m7.1.1.1.1.1.1">conditional</csymbol><ci id="S3.p1.7.m7.1.1.1.1.1.2.cmml" xref="S3.p1.7.m7.1.1.1.1.1.2">ğ‘¦</ci><apply id="S3.p1.7.m7.1.1.1.1.1.3.cmml" xref="S3.p1.7.m7.1.1.1.1.1.3"><ci id="S3.p1.7.m7.1.1.1.1.1.3.1.cmml" xref="S3.p1.7.m7.1.1.1.1.1.3.1">~</ci><ci id="S3.p1.7.m7.1.1.1.1.1.3.2.cmml" xref="S3.p1.7.m7.1.1.1.1.1.3.2">ğ‘¦</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m7.1c">p_{\tt{DLM}}(y|\tilde{y})</annotation></semantics></math>. Under this model:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\log p_{\tt{DSR}}(y|x)=\log\left(\sum_{\tilde{y}}p_{\tt{DLM}}(y|\tilde{y})p_{\tt{ASR}}(\tilde{y}|x)\right)." display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.1.cmml"><mrow id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.1.3.1" xref="S3.E1.m1.2.2.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E1.m1.2.2.1.1.1.3a" xref="S3.E1.m1.2.2.1.1.1.3.cmml">â¡</mo><msub id="S3.E1.m1.2.2.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.1.3.2.cmml"><mi id="S3.E1.m1.2.2.1.1.1.3.2.2" xref="S3.E1.m1.2.2.1.1.1.3.2.2.cmml">p</mi><mi id="S3.E1.m1.2.2.1.1.1.3.2.3" xref="S3.E1.m1.2.2.1.1.1.3.2.3.cmml">ğ™³ğš‚ğš</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.2.2.1.1.2.1" xref="S3.E1.m1.2.2.1.1.2.2.cmml"><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">log</mi><mo id="S3.E1.m1.2.2.1.1.2.1a" xref="S3.E1.m1.2.2.1.1.2.2.cmml">â¡</mo><mrow id="S3.E1.m1.2.2.1.1.2.1.1" xref="S3.E1.m1.2.2.1.1.2.2.cmml"><mo id="S3.E1.m1.2.2.1.1.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.2.cmml">(</mo><mrow id="S3.E1.m1.2.2.1.1.2.1.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.cmml"><munder id="S3.E1.m1.2.2.1.1.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.3.cmml"><mo lspace="0em" movablelimits="false" id="S3.E1.m1.2.2.1.1.2.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.3.2.cmml">âˆ‘</mo><mover accent="true" id="S3.E1.m1.2.2.1.1.2.1.1.1.3.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.3.3.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.1.1.3.3.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.3.3.2.cmml">y</mi><mo id="S3.E1.m1.2.2.1.1.2.1.1.1.3.3.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.3.3.1.cmml">~</mo></mover></munder><mrow id="S3.E1.m1.2.2.1.1.2.1.1.1.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.cmml"><msub id="S3.E1.m1.2.2.1.1.2.1.1.1.2.4" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.4.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.1.1.2.4.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.4.2.cmml">p</mi><mi id="S3.E1.m1.2.2.1.1.2.1.1.1.2.4.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.4.3.cmml">ğ™³ğ™»ğ™¼</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.2.1.1.1.2.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.3.cmml">â€‹</mo><mrow id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.cmml">|</mo><mover accent="true" id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.2.cmml">y</mi><mo id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.1.cmml">~</mo></mover></mrow><mo stretchy="false" id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.2.1.1.1.2.3a" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.3.cmml">â€‹</mo><msub id="S3.E1.m1.2.2.1.1.2.1.1.1.2.5" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.5.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.1.1.2.5.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.5.2.cmml">p</mi><mi id="S3.E1.m1.2.2.1.1.2.1.1.1.2.5.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.5.3.cmml">ğ™°ğš‚ğš</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.2.1.1.1.2.3b" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.3.cmml">â€‹</mo><mrow id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.cmml"><mover accent="true" id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.2.cmml"><mi id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.2.2" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.2.2.cmml">y</mi><mo id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.2.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.2.1.cmml">~</mo></mover><mo fence="false" id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.1.cmml">|</mo><mi id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.3" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.2.2.1.1.2.1.1.3" xref="S3.E1.m1.2.2.1.1.2.2.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1"><eq id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3"></eq><apply id="S3.E1.m1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.2"></times><apply id="S3.E1.m1.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3"><log id="S3.E1.m1.2.2.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.1.3.1"></log><apply id="S3.E1.m1.2.2.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.3.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.3.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.3.2.2">ğ‘</ci><ci id="S3.E1.m1.2.2.1.1.1.3.2.3.cmml" xref="S3.E1.m1.2.2.1.1.1.3.2.3">ğ™³ğš‚ğš</ci></apply></apply><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2">ğ‘¦</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3">ğ‘¥</ci></apply></apply><apply id="S3.E1.m1.2.2.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1"><log id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"></log><apply id="S3.E1.m1.2.2.1.1.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1"><apply id="S3.E1.m1.2.2.1.1.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.3">subscript</csymbol><sum id="S3.E1.m1.2.2.1.1.2.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.3.2"></sum><apply id="S3.E1.m1.2.2.1.1.2.1.1.1.3.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.3.3"><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.3.3.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.3.3.1">~</ci><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.3.3.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.3.3.2">ğ‘¦</ci></apply></apply><apply id="S3.E1.m1.2.2.1.1.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2"><times id="S3.E1.m1.2.2.1.1.2.1.1.1.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.3"></times><apply id="S3.E1.m1.2.2.1.1.2.1.1.1.2.4.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.1.1.2.4.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.4">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.2.4.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.4.2">ğ‘</ci><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.2.4.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.4.3">ğ™³ğ™»ğ™¼</ci></apply><apply id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.2">ğ‘¦</ci><apply id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3"><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.1">~</ci><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.1.1.1.1.3.2">ğ‘¦</ci></apply></apply><apply id="S3.E1.m1.2.2.1.1.2.1.1.1.2.5.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.5"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.1.1.1.2.5.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.5">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.2.5.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.5.2">ğ‘</ci><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.2.5.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.5.3">ğ™°ğš‚ğš</ci></apply><apply id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1"><csymbol cd="latexml" id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.1">conditional</csymbol><apply id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.2"><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.2.1">~</ci><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.2.2">ğ‘¦</ci></apply><ci id="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.2.1.1.1.2.2.1.1.3">ğ‘¥</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\log p_{\tt{DSR}}(y|x)=\log\left(\sum_{\tilde{y}}p_{\tt{DLM}}(y|\tilde{y})p_{\tt{ASR}}(\tilde{y}|x)\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.p1.9" class="ltx_p">Most error correction approaches optimize</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\log p_{\tt{DLM}}(y|\hat{y}),\,\,\,\text{where }\hat{y}=\operatorname*{arg\,max}_{\tilde{y}}p_{\tt{ASR}}(\tilde{y}|x)," display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.2.3.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E2.m1.1.1.1.1.1.1.1.3a" xref="S3.E2.m1.1.1.1.1.1.1.1.3.cmml">â¡</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.3.2.2.cmml">p</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.3.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.3.2.3.cmml">ğ™³ğ™»ğ™¼</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><mover accent="true" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">y</mi><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.667em" id="S3.E2.m1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.2.3.cmml">,</mo><mrow id="S3.E2.m1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.cmml"><mtext id="S3.E2.m1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.1.1.1.1.2.2.2.2a.cmml">whereÂ </mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.2.2.2.1" xref="S3.E2.m1.1.1.1.1.2.2.2.1.cmml">â€‹</mo><mover accent="true" id="S3.E2.m1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.1.1.1.1.2.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.2.2.2.3.2" xref="S3.E2.m1.1.1.1.1.2.2.2.3.2.cmml">y</mi><mo id="S3.E2.m1.1.1.1.1.2.2.2.3.1" xref="S3.E2.m1.1.1.1.1.2.2.2.3.1.cmml">^</mo></mover></mrow></mrow><mo id="S3.E2.m1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.4.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mrow id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml"><munder id="S3.E2.m1.1.1.1.1.3.3.1" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.3.3.1.2" xref="S3.E2.m1.1.1.1.1.3.3.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.3.3.1.2.2" xref="S3.E2.m1.1.1.1.1.3.3.1.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.3.1.2.1" xref="S3.E2.m1.1.1.1.1.3.3.1.2.1.cmml">â€‹</mo><mi id="S3.E2.m1.1.1.1.1.3.3.1.2.3" xref="S3.E2.m1.1.1.1.1.3.3.1.2.3.cmml">max</mi></mrow><mover accent="true" id="S3.E2.m1.1.1.1.1.3.3.1.3" xref="S3.E2.m1.1.1.1.1.3.3.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.3.1.3.2" xref="S3.E2.m1.1.1.1.1.3.3.1.3.2.cmml">y</mi><mo id="S3.E2.m1.1.1.1.1.3.3.1.3.1" xref="S3.E2.m1.1.1.1.1.3.3.1.3.1.cmml">~</mo></mover></munder><mo id="S3.E2.m1.1.1.1.1.3.3a" xref="S3.E2.m1.1.1.1.1.3.3.cmml">â¡</mo><msub id="S3.E2.m1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.3.3.2.cmml"><mi id="S3.E2.m1.1.1.1.1.3.3.2.2" xref="S3.E2.m1.1.1.1.1.3.3.2.2.cmml">p</mi><mi id="S3.E2.m1.1.1.1.1.3.3.2.3" xref="S3.E2.m1.1.1.1.1.3.3.2.3.cmml">ğ™°ğš‚ğš</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml">â€‹</mo><mrow id="S3.E2.m1.1.1.1.1.3.1.1" xref="S3.E2.m1.1.1.1.1.3.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.3.1.1.2" xref="S3.E2.m1.1.1.1.1.3.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.3.1.1.1" xref="S3.E2.m1.1.1.1.1.3.1.1.1.cmml"><mover accent="true" id="S3.E2.m1.1.1.1.1.3.1.1.1.2" xref="S3.E2.m1.1.1.1.1.3.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.3.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.3.1.1.1.2.2.cmml">y</mi><mo id="S3.E2.m1.1.1.1.1.3.1.1.1.2.1" xref="S3.E2.m1.1.1.1.1.3.1.1.1.2.1.cmml">~</mo></mover><mo fence="false" id="S3.E2.m1.1.1.1.1.3.1.1.1.1" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1.cmml">|</mo><mi id="S3.E2.m1.1.1.1.1.3.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.3.1.1.3" xref="S3.E2.m1.1.1.1.1.3.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.4"></eq><list id="S3.E2.m1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2"><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3"><log id="S3.E2.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.1"></log><apply id="S3.E2.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.2.2">ğ‘</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.3.2.3">ğ™³ğ™»ğ™¼</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.2">ğ‘¦</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3"><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.1">^</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.3.2">ğ‘¦</ci></apply></apply></apply><apply id="S3.E2.m1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2"><times id="S3.E2.m1.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.1"></times><ci id="S3.E2.m1.1.1.1.1.2.2.2.2a.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2"><mtext id="S3.E2.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.2">whereÂ </mtext></ci><apply id="S3.E2.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.3"><ci id="S3.E2.m1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.3.1">^</ci><ci id="S3.E2.m1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.2.2.2.3.2">ğ‘¦</ci></apply></apply></list><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><times id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2"></times><apply id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3"><apply id="S3.E2.m1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.3.1.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.3.3.1.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1.2"><times id="S3.E2.m1.1.1.1.1.3.3.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1.2.1"></times><ci id="S3.E2.m1.1.1.1.1.3.3.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1.2.2">arg</ci><ci id="S3.E2.m1.1.1.1.1.3.3.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1.2.3">max</ci></apply><apply id="S3.E2.m1.1.1.1.1.3.3.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1.3"><ci id="S3.E2.m1.1.1.1.1.3.3.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1.3.1">~</ci><ci id="S3.E2.m1.1.1.1.1.3.3.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1.3.2">ğ‘¦</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2.2">ğ‘</ci><ci id="S3.E2.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2.3">ğ™°ğš‚ğš</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.3.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1"><csymbol cd="latexml" id="S3.E2.m1.1.1.1.1.3.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1.1.1">conditional</csymbol><apply id="S3.E2.m1.1.1.1.1.3.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1.1.2"><ci id="S3.E2.m1.1.1.1.1.3.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1.1.2.1">~</ci><ci id="S3.E2.m1.1.1.1.1.3.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1.1.2.2">ğ‘¦</ci></apply><ci id="S3.E2.m1.1.1.1.1.3.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3.1.1.1.3">ğ‘¥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\log p_{\tt{DLM}}(y|\hat{y}),\,\,\,\text{where }\hat{y}=\operatorname*{arg\,max}_{\tilde{y}}p_{\tt{ASR}}(\tilde{y}|x),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.p1.10" class="ltx_p">which can be viewed as using an approximation to the lower bound of EquationÂ (<a href="#S3.E1" title="Equation 1 â€£ 3 Denoising Speech Recognition (DSR) â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>)</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.3" class="ltx_Math" alttext="\log p_{\tt{DSR}}(y|x)\geq\sum_{\tilde{y}}p_{\tt{ASR}}(\tilde{y}|x)\log p_{\tt{DLM}}(y|\tilde{y})" display="block"><semantics id="S3.E3.m1.3a"><mrow id="S3.E3.m1.3.3" xref="S3.E3.m1.3.3.cmml"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E3.m1.1.1.1.3a" xref="S3.E3.m1.1.1.1.3.cmml">â¡</mo><msub id="S3.E3.m1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.3.2.cmml"><mi id="S3.E3.m1.1.1.1.3.2.2" xref="S3.E3.m1.1.1.1.3.2.2.cmml">p</mi><mi id="S3.E3.m1.1.1.1.3.2.3" xref="S3.E3.m1.1.1.1.3.2.3.cmml">ğ™³ğš‚ğš</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S3.E3.m1.3.3.4" xref="S3.E3.m1.3.3.4.cmml">â‰¥</mo><mrow id="S3.E3.m1.3.3.3" xref="S3.E3.m1.3.3.3.cmml"><munder id="S3.E3.m1.3.3.3.3" xref="S3.E3.m1.3.3.3.3.cmml"><mo movablelimits="false" id="S3.E3.m1.3.3.3.3.2" xref="S3.E3.m1.3.3.3.3.2.cmml">âˆ‘</mo><mover accent="true" id="S3.E3.m1.3.3.3.3.3" xref="S3.E3.m1.3.3.3.3.3.cmml"><mi id="S3.E3.m1.3.3.3.3.3.2" xref="S3.E3.m1.3.3.3.3.3.2.cmml">y</mi><mo id="S3.E3.m1.3.3.3.3.3.1" xref="S3.E3.m1.3.3.3.3.3.1.cmml">~</mo></mover></munder><mrow id="S3.E3.m1.3.3.3.2" xref="S3.E3.m1.3.3.3.2.cmml"><msub id="S3.E3.m1.3.3.3.2.4" xref="S3.E3.m1.3.3.3.2.4.cmml"><mi id="S3.E3.m1.3.3.3.2.4.2" xref="S3.E3.m1.3.3.3.2.4.2.cmml">p</mi><mi id="S3.E3.m1.3.3.3.2.4.3" xref="S3.E3.m1.3.3.3.2.4.3.cmml">ğ™°ğš‚ğš</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.3.2.3" xref="S3.E3.m1.3.3.3.2.3.cmml">â€‹</mo><mrow id="S3.E3.m1.2.2.2.1.1.1" xref="S3.E3.m1.2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.2.2.2.1.1.1.2" xref="S3.E3.m1.2.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.2.2.2.1.1.1.1" xref="S3.E3.m1.2.2.2.1.1.1.1.cmml"><mover accent="true" id="S3.E3.m1.2.2.2.1.1.1.1.2" xref="S3.E3.m1.2.2.2.1.1.1.1.2.cmml"><mi id="S3.E3.m1.2.2.2.1.1.1.1.2.2" xref="S3.E3.m1.2.2.2.1.1.1.1.2.2.cmml">y</mi><mo id="S3.E3.m1.2.2.2.1.1.1.1.2.1" xref="S3.E3.m1.2.2.2.1.1.1.1.2.1.cmml">~</mo></mover><mo fence="false" id="S3.E3.m1.2.2.2.1.1.1.1.1" xref="S3.E3.m1.2.2.2.1.1.1.1.1.cmml">|</mo><mi id="S3.E3.m1.2.2.2.1.1.1.1.3" xref="S3.E3.m1.2.2.2.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.E3.m1.2.2.2.1.1.1.3" xref="S3.E3.m1.2.2.2.1.1.1.1.cmml">)</mo></mrow><mo lspace="0.167em" rspace="0em" id="S3.E3.m1.3.3.3.2.3a" xref="S3.E3.m1.3.3.3.2.3.cmml">â€‹</mo><mrow id="S3.E3.m1.3.3.3.2.5" xref="S3.E3.m1.3.3.3.2.5.cmml"><mi id="S3.E3.m1.3.3.3.2.5.1" xref="S3.E3.m1.3.3.3.2.5.1.cmml">log</mi><mo lspace="0.167em" id="S3.E3.m1.3.3.3.2.5a" xref="S3.E3.m1.3.3.3.2.5.cmml">â¡</mo><msub id="S3.E3.m1.3.3.3.2.5.2" xref="S3.E3.m1.3.3.3.2.5.2.cmml"><mi id="S3.E3.m1.3.3.3.2.5.2.2" xref="S3.E3.m1.3.3.3.2.5.2.2.cmml">p</mi><mi id="S3.E3.m1.3.3.3.2.5.2.3" xref="S3.E3.m1.3.3.3.2.5.2.3.cmml">ğ™³ğ™»ğ™¼</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.3.2.3b" xref="S3.E3.m1.3.3.3.2.3.cmml">â€‹</mo><mrow id="S3.E3.m1.3.3.3.2.2.1" xref="S3.E3.m1.3.3.3.2.2.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.3.3.3.2.2.1.2" xref="S3.E3.m1.3.3.3.2.2.1.1.cmml">(</mo><mrow id="S3.E3.m1.3.3.3.2.2.1.1" xref="S3.E3.m1.3.3.3.2.2.1.1.cmml"><mi id="S3.E3.m1.3.3.3.2.2.1.1.2" xref="S3.E3.m1.3.3.3.2.2.1.1.2.cmml">y</mi><mo fence="false" id="S3.E3.m1.3.3.3.2.2.1.1.1" xref="S3.E3.m1.3.3.3.2.2.1.1.1.cmml">|</mo><mover accent="true" id="S3.E3.m1.3.3.3.2.2.1.1.3" xref="S3.E3.m1.3.3.3.2.2.1.1.3.cmml"><mi id="S3.E3.m1.3.3.3.2.2.1.1.3.2" xref="S3.E3.m1.3.3.3.2.2.1.1.3.2.cmml">y</mi><mo id="S3.E3.m1.3.3.3.2.2.1.1.3.1" xref="S3.E3.m1.3.3.3.2.2.1.1.3.1.cmml">~</mo></mover></mrow><mo stretchy="false" id="S3.E3.m1.3.3.3.2.2.1.3" xref="S3.E3.m1.3.3.3.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.3b"><apply id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3"><geq id="S3.E3.m1.3.3.4.cmml" xref="S3.E3.m1.3.3.4"></geq><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><times id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></times><apply id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3"><log id="S3.E3.m1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.3.1"></log><apply id="S3.E3.m1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.1.3.2.2">ğ‘</ci><ci id="S3.E3.m1.1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.1.3.2.3">ğ™³ğš‚ğš</ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2">ğ‘¦</ci><ci id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3">ğ‘¥</ci></apply></apply><apply id="S3.E3.m1.3.3.3.cmml" xref="S3.E3.m1.3.3.3"><apply id="S3.E3.m1.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.3.3.1.cmml" xref="S3.E3.m1.3.3.3.3">subscript</csymbol><sum id="S3.E3.m1.3.3.3.3.2.cmml" xref="S3.E3.m1.3.3.3.3.2"></sum><apply id="S3.E3.m1.3.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3.3"><ci id="S3.E3.m1.3.3.3.3.3.1.cmml" xref="S3.E3.m1.3.3.3.3.3.1">~</ci><ci id="S3.E3.m1.3.3.3.3.3.2.cmml" xref="S3.E3.m1.3.3.3.3.3.2">ğ‘¦</ci></apply></apply><apply id="S3.E3.m1.3.3.3.2.cmml" xref="S3.E3.m1.3.3.3.2"><times id="S3.E3.m1.3.3.3.2.3.cmml" xref="S3.E3.m1.3.3.3.2.3"></times><apply id="S3.E3.m1.3.3.3.2.4.cmml" xref="S3.E3.m1.3.3.3.2.4"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.3.2.4.1.cmml" xref="S3.E3.m1.3.3.3.2.4">subscript</csymbol><ci id="S3.E3.m1.3.3.3.2.4.2.cmml" xref="S3.E3.m1.3.3.3.2.4.2">ğ‘</ci><ci id="S3.E3.m1.3.3.3.2.4.3.cmml" xref="S3.E3.m1.3.3.3.2.4.3">ğ™°ğš‚ğš</ci></apply><apply id="S3.E3.m1.2.2.2.1.1.1.1.cmml" xref="S3.E3.m1.2.2.2.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.2.2.2.1.1.1.1.1.cmml" xref="S3.E3.m1.2.2.2.1.1.1.1.1">conditional</csymbol><apply id="S3.E3.m1.2.2.2.1.1.1.1.2.cmml" xref="S3.E3.m1.2.2.2.1.1.1.1.2"><ci id="S3.E3.m1.2.2.2.1.1.1.1.2.1.cmml" xref="S3.E3.m1.2.2.2.1.1.1.1.2.1">~</ci><ci id="S3.E3.m1.2.2.2.1.1.1.1.2.2.cmml" xref="S3.E3.m1.2.2.2.1.1.1.1.2.2">ğ‘¦</ci></apply><ci id="S3.E3.m1.2.2.2.1.1.1.1.3.cmml" xref="S3.E3.m1.2.2.2.1.1.1.1.3">ğ‘¥</ci></apply><apply id="S3.E3.m1.3.3.3.2.5.cmml" xref="S3.E3.m1.3.3.3.2.5"><log id="S3.E3.m1.3.3.3.2.5.1.cmml" xref="S3.E3.m1.3.3.3.2.5.1"></log><apply id="S3.E3.m1.3.3.3.2.5.2.cmml" xref="S3.E3.m1.3.3.3.2.5.2"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.3.2.5.2.1.cmml" xref="S3.E3.m1.3.3.3.2.5.2">subscript</csymbol><ci id="S3.E3.m1.3.3.3.2.5.2.2.cmml" xref="S3.E3.m1.3.3.3.2.5.2.2">ğ‘</ci><ci id="S3.E3.m1.3.3.3.2.5.2.3.cmml" xref="S3.E3.m1.3.3.3.2.5.2.3">ğ™³ğ™»ğ™¼</ci></apply></apply><apply id="S3.E3.m1.3.3.3.2.2.1.1.cmml" xref="S3.E3.m1.3.3.3.2.2.1"><csymbol cd="latexml" id="S3.E3.m1.3.3.3.2.2.1.1.1.cmml" xref="S3.E3.m1.3.3.3.2.2.1.1.1">conditional</csymbol><ci id="S3.E3.m1.3.3.3.2.2.1.1.2.cmml" xref="S3.E3.m1.3.3.3.2.2.1.1.2">ğ‘¦</ci><apply id="S3.E3.m1.3.3.3.2.2.1.1.3.cmml" xref="S3.E3.m1.3.3.3.2.2.1.1.3"><ci id="S3.E3.m1.3.3.3.2.2.1.1.3.1.cmml" xref="S3.E3.m1.3.3.3.2.2.1.1.3.1">~</ci><ci id="S3.E3.m1.3.3.3.2.2.1.1.3.2.cmml" xref="S3.E3.m1.3.3.3.2.2.1.1.3.2">ğ‘¦</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.3c">\log p_{\tt{DSR}}(y|x)\geq\sum_{\tilde{y}}p_{\tt{ASR}}(\tilde{y}|x)\log p_{\tt{DLM}}(y|\tilde{y})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.p1.8" class="ltx_p">under the assumption that the ASR model is really peaked on a single transcript (the inequality comes from applying Jensenâ€™s inequality). Note, however, that using samples from the posterior of the model, <math id="S3.p1.8.m1.3" class="ltx_Math" alttext="p_{\tt{DSR}}(\tilde{y}|x,y)" display="inline"><semantics id="S3.p1.8.m1.3a"><mrow id="S3.p1.8.m1.3.3" xref="S3.p1.8.m1.3.3.cmml"><msub id="S3.p1.8.m1.3.3.3" xref="S3.p1.8.m1.3.3.3.cmml"><mi id="S3.p1.8.m1.3.3.3.2" xref="S3.p1.8.m1.3.3.3.2.cmml">p</mi><mi id="S3.p1.8.m1.3.3.3.3" xref="S3.p1.8.m1.3.3.3.3.cmml">ğ™³ğš‚ğš</mi></msub><mo lspace="0em" rspace="0em" id="S3.p1.8.m1.3.3.2" xref="S3.p1.8.m1.3.3.2.cmml">â€‹</mo><mrow id="S3.p1.8.m1.3.3.1.1" xref="S3.p1.8.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.p1.8.m1.3.3.1.1.2" xref="S3.p1.8.m1.3.3.1.1.1.cmml">(</mo><mrow id="S3.p1.8.m1.3.3.1.1.1" xref="S3.p1.8.m1.3.3.1.1.1.cmml"><mover accent="true" id="S3.p1.8.m1.3.3.1.1.1.2" xref="S3.p1.8.m1.3.3.1.1.1.2.cmml"><mi id="S3.p1.8.m1.3.3.1.1.1.2.2" xref="S3.p1.8.m1.3.3.1.1.1.2.2.cmml">y</mi><mo id="S3.p1.8.m1.3.3.1.1.1.2.1" xref="S3.p1.8.m1.3.3.1.1.1.2.1.cmml">~</mo></mover><mo fence="false" id="S3.p1.8.m1.3.3.1.1.1.1" xref="S3.p1.8.m1.3.3.1.1.1.1.cmml">|</mo><mrow id="S3.p1.8.m1.3.3.1.1.1.3.2" xref="S3.p1.8.m1.3.3.1.1.1.3.1.cmml"><mi id="S3.p1.8.m1.1.1" xref="S3.p1.8.m1.1.1.cmml">x</mi><mo id="S3.p1.8.m1.3.3.1.1.1.3.2.1" xref="S3.p1.8.m1.3.3.1.1.1.3.1.cmml">,</mo><mi id="S3.p1.8.m1.2.2" xref="S3.p1.8.m1.2.2.cmml">y</mi></mrow></mrow><mo stretchy="false" id="S3.p1.8.m1.3.3.1.1.3" xref="S3.p1.8.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.8.m1.3b"><apply id="S3.p1.8.m1.3.3.cmml" xref="S3.p1.8.m1.3.3"><times id="S3.p1.8.m1.3.3.2.cmml" xref="S3.p1.8.m1.3.3.2"></times><apply id="S3.p1.8.m1.3.3.3.cmml" xref="S3.p1.8.m1.3.3.3"><csymbol cd="ambiguous" id="S3.p1.8.m1.3.3.3.1.cmml" xref="S3.p1.8.m1.3.3.3">subscript</csymbol><ci id="S3.p1.8.m1.3.3.3.2.cmml" xref="S3.p1.8.m1.3.3.3.2">ğ‘</ci><ci id="S3.p1.8.m1.3.3.3.3.cmml" xref="S3.p1.8.m1.3.3.3.3">ğ™³ğš‚ğš</ci></apply><apply id="S3.p1.8.m1.3.3.1.1.1.cmml" xref="S3.p1.8.m1.3.3.1.1"><csymbol cd="latexml" id="S3.p1.8.m1.3.3.1.1.1.1.cmml" xref="S3.p1.8.m1.3.3.1.1.1.1">conditional</csymbol><apply id="S3.p1.8.m1.3.3.1.1.1.2.cmml" xref="S3.p1.8.m1.3.3.1.1.1.2"><ci id="S3.p1.8.m1.3.3.1.1.1.2.1.cmml" xref="S3.p1.8.m1.3.3.1.1.1.2.1">~</ci><ci id="S3.p1.8.m1.3.3.1.1.1.2.2.cmml" xref="S3.p1.8.m1.3.3.1.1.1.2.2">ğ‘¦</ci></apply><list id="S3.p1.8.m1.3.3.1.1.1.3.1.cmml" xref="S3.p1.8.m1.3.3.1.1.1.3.2"><ci id="S3.p1.8.m1.1.1.cmml" xref="S3.p1.8.m1.1.1">ğ‘¥</ci><ci id="S3.p1.8.m1.2.2.cmml" xref="S3.p1.8.m1.2.2">ğ‘¦</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.8.m1.3c">p_{\tt{DSR}}(\tilde{y}|x,y)</annotation></semantics></math>, would be the correct distribution to optimize the above model on, but we (and prior works on error correction) do not follow that approach here.
Variational approaches are alternative directions to consider under this framework which we leave as a future work.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<p id="S3.F1.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S3.F1.1.1.1" class="ltx_text"><img src="/html/2405.15216/assets/figs/decoding.png" id="S3.F1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="322" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Conventional LM decoding and the proposed DSR-decoding. Left panel: given an input audio, an <math id="S3.F1.5.5.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.F1.5.5.m1.1b"><mi id="S3.F1.5.5.m1.1.1" xref="S3.F1.5.5.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.F1.5.5.m1.1c"><ci id="S3.F1.5.5.m1.1.1.cmml" xref="S3.F1.5.5.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.5.5.m1.1d">n</annotation></semantics></math>-best list of the hypotheses from the beam-search decoding of the ASR model is generated with a <math id="S3.F1.6.6.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.F1.6.6.m2.1b"><mi id="S3.F1.6.6.m2.1.1" xref="S3.F1.6.6.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.F1.6.6.m2.1c"><ci id="S3.F1.6.6.m2.1.1.cmml" xref="S3.F1.6.6.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.6.6.m2.1d">N</annotation></semantics></math>-gram word-level LM, which is then rescored using the neural LM scores and ASR scores. Right panel: given an input audio, ASR generates its greedy hypothesis, which is fed into DLM and it generates an <math id="S3.F1.7.7.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.F1.7.7.m3.1b"><mi id="S3.F1.7.7.m3.1.1" xref="S3.F1.7.7.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.F1.7.7.m3.1c"><ci id="S3.F1.7.7.m3.1.1.cmml" xref="S3.F1.7.7.m3.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.7.7.m3.1d">n</annotation></semantics></math>-best list of the corrected hypotheses, which are then rescored using both DLM scores and ASR scores.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data generation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.10" class="ltx_p">We would like to optimize EquationÂ (<a href="#S3.E2" title="Equation 2 â€£ 3 Denoising Speech Recognition (DSR) â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) in order to improve <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\log p_{\tt{DSR}}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">log</mi><mo lspace="0.167em" id="S3.SS1.p1.1.m1.1.1a" xref="S3.SS1.p1.1.m1.1.1.cmml">â¡</mo><msub id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2.2" xref="S3.SS1.p1.1.m1.1.1.2.2.cmml">p</mi><mi id="S3.SS1.p1.1.m1.1.1.2.3" xref="S3.SS1.p1.1.m1.1.1.2.3.cmml">ğ™³ğš‚ğš</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><log id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></log><apply id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2.2">ğ‘</ci><ci id="S3.SS1.p1.1.m1.1.1.2.3.cmml" xref="S3.SS1.p1.1.m1.1.1.2.3">ğ™³ğš‚ğš</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\log p_{\tt{DSR}}</annotation></semantics></math>.
Given the unavailability of large amount of supervised data and the availability of a large corpus of text with distribution <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="p(y)" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.2" xref="S3.SS1.p1.2.m2.1.2.cmml"><mi id="S3.SS1.p1.2.m2.1.2.2" xref="S3.SS1.p1.2.m2.1.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.2.m2.1.2.1" xref="S3.SS1.p1.2.m2.1.2.1.cmml">â€‹</mo><mrow id="S3.SS1.p1.2.m2.1.2.3.2" xref="S3.SS1.p1.2.m2.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.2.m2.1.2.3.2.1" xref="S3.SS1.p1.2.m2.1.2.cmml">(</mo><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">y</mi><mo stretchy="false" id="S3.SS1.p1.2.m2.1.2.3.2.2" xref="S3.SS1.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.2.cmml" xref="S3.SS1.p1.2.m2.1.2"><times id="S3.SS1.p1.2.m2.1.2.1.cmml" xref="S3.SS1.p1.2.m2.1.2.1"></times><ci id="S3.SS1.p1.2.m2.1.2.2.cmml" xref="S3.SS1.p1.2.m2.1.2.2">ğ‘</ci><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">p(y)</annotation></semantics></math>, we take the approach that we can learn a denoising distribution using a cascade of a TTS system and an ASR system to generate a reasonable distribution <math id="S3.SS1.p1.3.m3.3" class="ltx_Math" alttext="\{(\hat{y},y)\}" display="inline"><semantics id="S3.SS1.p1.3.m3.3a"><mrow id="S3.SS1.p1.3.m3.3.3.1" xref="S3.SS1.p1.3.m3.3.3.2.cmml"><mo stretchy="false" id="S3.SS1.p1.3.m3.3.3.1.2" xref="S3.SS1.p1.3.m3.3.3.2.cmml">{</mo><mrow id="S3.SS1.p1.3.m3.3.3.1.1.2" xref="S3.SS1.p1.3.m3.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p1.3.m3.3.3.1.1.2.1" xref="S3.SS1.p1.3.m3.3.3.1.1.1.cmml">(</mo><mover accent="true" id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">y</mi><mo id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">^</mo></mover><mo id="S3.SS1.p1.3.m3.3.3.1.1.2.2" xref="S3.SS1.p1.3.m3.3.3.1.1.1.cmml">,</mo><mi id="S3.SS1.p1.3.m3.2.2" xref="S3.SS1.p1.3.m3.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS1.p1.3.m3.3.3.1.1.2.3" xref="S3.SS1.p1.3.m3.3.3.1.1.1.cmml">)</mo></mrow><mo stretchy="false" id="S3.SS1.p1.3.m3.3.3.1.3" xref="S3.SS1.p1.3.m3.3.3.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.3b"><set id="S3.SS1.p1.3.m3.3.3.2.cmml" xref="S3.SS1.p1.3.m3.3.3.1"><interval closure="open" id="S3.SS1.p1.3.m3.3.3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.3.3.1.1.2"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><ci id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1">^</ci><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">ğ‘¦</ci></apply><ci id="S3.SS1.p1.3.m3.2.2.cmml" xref="S3.SS1.p1.3.m3.2.2">ğ‘¦</ci></interval></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.3c">\{(\hat{y},y)\}</annotation></semantics></math> to learn the error correcting distribution. The TTS system is used to create the distribution <math id="S3.SS1.p1.4.m4.3" class="ltx_Math" alttext="\mathcal{D_{\tt{DSR}}}=\{(\tilde{x},y)\}" display="inline"><semantics id="S3.SS1.p1.4.m4.3a"><mrow id="S3.SS1.p1.4.m4.3.3" xref="S3.SS1.p1.4.m4.3.3.cmml"><msub id="S3.SS1.p1.4.m4.3.3.3" xref="S3.SS1.p1.4.m4.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.4.m4.3.3.3.2" xref="S3.SS1.p1.4.m4.3.3.3.2.cmml">ğ’Ÿ</mi><mi id="S3.SS1.p1.4.m4.3.3.3.3" xref="S3.SS1.p1.4.m4.3.3.3.3.cmml">ğ™³ğš‚ğš</mi></msub><mo id="S3.SS1.p1.4.m4.3.3.2" xref="S3.SS1.p1.4.m4.3.3.2.cmml">=</mo><mrow id="S3.SS1.p1.4.m4.3.3.1.1" xref="S3.SS1.p1.4.m4.3.3.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.4.m4.3.3.1.1.2" xref="S3.SS1.p1.4.m4.3.3.1.2.cmml">{</mo><mrow id="S3.SS1.p1.4.m4.3.3.1.1.1.2" xref="S3.SS1.p1.4.m4.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p1.4.m4.3.3.1.1.1.2.1" xref="S3.SS1.p1.4.m4.3.3.1.1.1.1.cmml">(</mo><mover accent="true" id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">x</mi><mo id="S3.SS1.p1.4.m4.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.cmml">~</mo></mover><mo id="S3.SS1.p1.4.m4.3.3.1.1.1.2.2" xref="S3.SS1.p1.4.m4.3.3.1.1.1.1.cmml">,</mo><mi id="S3.SS1.p1.4.m4.2.2" xref="S3.SS1.p1.4.m4.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS1.p1.4.m4.3.3.1.1.1.2.3" xref="S3.SS1.p1.4.m4.3.3.1.1.1.1.cmml">)</mo></mrow><mo stretchy="false" id="S3.SS1.p1.4.m4.3.3.1.1.3" xref="S3.SS1.p1.4.m4.3.3.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.3b"><apply id="S3.SS1.p1.4.m4.3.3.cmml" xref="S3.SS1.p1.4.m4.3.3"><eq id="S3.SS1.p1.4.m4.3.3.2.cmml" xref="S3.SS1.p1.4.m4.3.3.2"></eq><apply id="S3.SS1.p1.4.m4.3.3.3.cmml" xref="S3.SS1.p1.4.m4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.3.3.3.1.cmml" xref="S3.SS1.p1.4.m4.3.3.3">subscript</csymbol><ci id="S3.SS1.p1.4.m4.3.3.3.2.cmml" xref="S3.SS1.p1.4.m4.3.3.3.2">ğ’Ÿ</ci><ci id="S3.SS1.p1.4.m4.3.3.3.3.cmml" xref="S3.SS1.p1.4.m4.3.3.3.3">ğ™³ğš‚ğš</ci></apply><set id="S3.SS1.p1.4.m4.3.3.1.2.cmml" xref="S3.SS1.p1.4.m4.3.3.1.1"><interval closure="open" id="S3.SS1.p1.4.m4.3.3.1.1.1.1.cmml" xref="S3.SS1.p1.4.m4.3.3.1.1.1.2"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><ci id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1">~</ci><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">ğ‘¥</ci></apply><ci id="S3.SS1.p1.4.m4.2.2.cmml" xref="S3.SS1.p1.4.m4.2.2">ğ‘¦</ci></interval></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.3c">\mathcal{D_{\tt{DSR}}}=\{(\tilde{x},y)\}</annotation></semantics></math>, where the audio <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="\tilde{x}" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mover accent="true" id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">x</mi><mo id="S3.SS1.p1.5.m5.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><ci id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1">~</ci><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\tilde{x}</annotation></semantics></math> is generated from the sentence <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">y</annotation></semantics></math> using the TTS system, and the ASR system is used to generate <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><mover accent="true" id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><mi id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml">y</mi><mo id="S3.SS1.p1.7.m7.1.1.1" xref="S3.SS1.p1.7.m7.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><ci id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1.1">^</ci><ci id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">\hat{y}</annotation></semantics></math> from <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="\tilde{x}" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><mover accent="true" id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml">x</mi><mo id="S3.SS1.p1.8.m8.1.1.1" xref="S3.SS1.p1.8.m8.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><ci id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1.1">~</ci><ci id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">\tilde{x}</annotation></semantics></math>.
The dataset <math id="S3.SS1.p1.9.m9.3" class="ltx_Math" alttext="\{(\hat{y},y)\}" display="inline"><semantics id="S3.SS1.p1.9.m9.3a"><mrow id="S3.SS1.p1.9.m9.3.3.1" xref="S3.SS1.p1.9.m9.3.3.2.cmml"><mo stretchy="false" id="S3.SS1.p1.9.m9.3.3.1.2" xref="S3.SS1.p1.9.m9.3.3.2.cmml">{</mo><mrow id="S3.SS1.p1.9.m9.3.3.1.1.2" xref="S3.SS1.p1.9.m9.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p1.9.m9.3.3.1.1.2.1" xref="S3.SS1.p1.9.m9.3.3.1.1.1.cmml">(</mo><mover accent="true" id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml"><mi id="S3.SS1.p1.9.m9.1.1.2" xref="S3.SS1.p1.9.m9.1.1.2.cmml">y</mi><mo id="S3.SS1.p1.9.m9.1.1.1" xref="S3.SS1.p1.9.m9.1.1.1.cmml">^</mo></mover><mo id="S3.SS1.p1.9.m9.3.3.1.1.2.2" xref="S3.SS1.p1.9.m9.3.3.1.1.1.cmml">,</mo><mi id="S3.SS1.p1.9.m9.2.2" xref="S3.SS1.p1.9.m9.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS1.p1.9.m9.3.3.1.1.2.3" xref="S3.SS1.p1.9.m9.3.3.1.1.1.cmml">)</mo></mrow><mo stretchy="false" id="S3.SS1.p1.9.m9.3.3.1.3" xref="S3.SS1.p1.9.m9.3.3.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.3b"><set id="S3.SS1.p1.9.m9.3.3.2.cmml" xref="S3.SS1.p1.9.m9.3.3.1"><interval closure="open" id="S3.SS1.p1.9.m9.3.3.1.1.1.cmml" xref="S3.SS1.p1.9.m9.3.3.1.1.2"><apply id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1"><ci id="S3.SS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1.1">^</ci><ci id="S3.SS1.p1.9.m9.1.1.2.cmml" xref="S3.SS1.p1.9.m9.1.1.2">ğ‘¦</ci></apply><ci id="S3.SS1.p1.9.m9.2.2.cmml" xref="S3.SS1.p1.9.m9.2.2">ğ‘¦</ci></interval></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.3c">\{(\hat{y},y)\}</annotation></semantics></math> is used to train a sequence-to-sequence model <math id="S3.SS1.p1.10.m10.1" class="ltx_Math" alttext="p_{\tt{DLM}}(y|\hat{y})" display="inline"><semantics id="S3.SS1.p1.10.m10.1a"><mrow id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml"><msub id="S3.SS1.p1.10.m10.1.1.3" xref="S3.SS1.p1.10.m10.1.1.3.cmml"><mi id="S3.SS1.p1.10.m10.1.1.3.2" xref="S3.SS1.p1.10.m10.1.1.3.2.cmml">p</mi><mi id="S3.SS1.p1.10.m10.1.1.3.3" xref="S3.SS1.p1.10.m10.1.1.3.3.cmml">ğ™³ğ™»ğ™¼</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p1.10.m10.1.1.2" xref="S3.SS1.p1.10.m10.1.1.2.cmml">â€‹</mo><mrow id="S3.SS1.p1.10.m10.1.1.1.1" xref="S3.SS1.p1.10.m10.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p1.10.m10.1.1.1.1.2" xref="S3.SS1.p1.10.m10.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.10.m10.1.1.1.1.1" xref="S3.SS1.p1.10.m10.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.10.m10.1.1.1.1.1.2" xref="S3.SS1.p1.10.m10.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S3.SS1.p1.10.m10.1.1.1.1.1.1" xref="S3.SS1.p1.10.m10.1.1.1.1.1.1.cmml">|</mo><mover accent="true" id="S3.SS1.p1.10.m10.1.1.1.1.1.3" xref="S3.SS1.p1.10.m10.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p1.10.m10.1.1.1.1.1.3.2" xref="S3.SS1.p1.10.m10.1.1.1.1.1.3.2.cmml">y</mi><mo id="S3.SS1.p1.10.m10.1.1.1.1.1.3.1" xref="S3.SS1.p1.10.m10.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo stretchy="false" id="S3.SS1.p1.10.m10.1.1.1.1.3" xref="S3.SS1.p1.10.m10.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.1b"><apply id="S3.SS1.p1.10.m10.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1"><times id="S3.SS1.p1.10.m10.1.1.2.cmml" xref="S3.SS1.p1.10.m10.1.1.2"></times><apply id="S3.SS1.p1.10.m10.1.1.3.cmml" xref="S3.SS1.p1.10.m10.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.10.m10.1.1.3.1.cmml" xref="S3.SS1.p1.10.m10.1.1.3">subscript</csymbol><ci id="S3.SS1.p1.10.m10.1.1.3.2.cmml" xref="S3.SS1.p1.10.m10.1.1.3.2">ğ‘</ci><ci id="S3.SS1.p1.10.m10.1.1.3.3.cmml" xref="S3.SS1.p1.10.m10.1.1.3.3">ğ™³ğ™»ğ™¼</ci></apply><apply id="S3.SS1.p1.10.m10.1.1.1.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.10.m10.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.p1.10.m10.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.10.m10.1.1.1.1.1.2">ğ‘¦</ci><apply id="S3.SS1.p1.10.m10.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.10.m10.1.1.1.1.1.3"><ci id="S3.SS1.p1.10.m10.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.10.m10.1.1.1.1.1.3.1">^</ci><ci id="S3.SS1.p1.10.m10.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.10.m10.1.1.1.1.1.3.2">ğ‘¦</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.1c">p_{\tt{DLM}}(y|\hat{y})</annotation></semantics></math>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Unlike conventional LMs which place their probability mass on text sequences, these DLMs are conditional models whose input distribution reflects a corrupted version of the text distribution. Thus, it is critical to make sure that the corrupted distribution learned by the DLM is close to what is needed at test time. In this sense, TTS corrupted distribution is better than just randomly mutated text by itselfÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>,<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>As initial experiments we also tried carefully designed by hand corruptions, e.g. <math id="footnote1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="footnote1.m1.1b"><mi id="footnote1.m1.1.1" xref="footnote1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="footnote1.m1.1c"><ci id="footnote1.m1.1.1.cmml" xref="footnote1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m1.1d">n</annotation></semantics></math>-gram modifications of the text which did not work in practice.</span></span></span> because it captures the biases of the ASR system.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Different TTS systems give different corrupted text distributions. We found that good TTS systems need to be able to generate stylistic variations in audio (see SectionÂ <a href="#S5.SS3.SSS0.Px3" title="Scaling numbers of speakers â€£ 5.3 DLMs are scalable â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>). Besides TTS system, we show different useful strategies for introducing variant noise types to the text in SectionÂ <a href="#S5.SS4" title="5.4 Ablation: design proper error correction data distribution â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.4</span></a>.
Finally, scaling the training text corpus size together with the number of parameters in the DLM is also beneficial (SectionÂ <a href="#S5.SS3" title="5.3 DLMs are scalable â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>).</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Decoding techniques</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.2" class="ltx_p">After both ASR and DLM models are trained, in order to decode an audio signal <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">x</annotation></semantics></math> to the DSR model, we seek the transcript <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="y^{*}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><msup id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">y</mi><mo id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">ğ‘¦</ci><times id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">y^{*}</annotation></semantics></math>, which satisfies the following maximization:</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="y^{*}=\operatorname*{arg\,max}_{y}\sum_{\tilde{y}}p_{\tt{DLM}}(y|\tilde{y})p_{\tt{ASR}}(\tilde{y}|x)." display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><msup id="S3.E4.m1.1.1.1.1.4" xref="S3.E4.m1.1.1.1.1.4.cmml"><mi id="S3.E4.m1.1.1.1.1.4.2" xref="S3.E4.m1.1.1.1.1.4.2.cmml">y</mi><mo id="S3.E4.m1.1.1.1.1.4.3" xref="S3.E4.m1.1.1.1.1.4.3.cmml">âˆ—</mo></msup><mo id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E4.m1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.cmml"><munder id="S3.E4.m1.1.1.1.1.2.4" xref="S3.E4.m1.1.1.1.1.2.4.cmml"><mrow id="S3.E4.m1.1.1.1.1.2.4.2" xref="S3.E4.m1.1.1.1.1.2.4.2.cmml"><mi id="S3.E4.m1.1.1.1.1.2.4.2.2" xref="S3.E4.m1.1.1.1.1.2.4.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em" id="S3.E4.m1.1.1.1.1.2.4.2.1" xref="S3.E4.m1.1.1.1.1.2.4.2.1.cmml">â€‹</mo><mi id="S3.E4.m1.1.1.1.1.2.4.2.3" xref="S3.E4.m1.1.1.1.1.2.4.2.3.cmml">max</mi></mrow><mi id="S3.E4.m1.1.1.1.1.2.4.3" xref="S3.E4.m1.1.1.1.1.2.4.3.cmml">y</mi></munder><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.2.3.cmml">â€‹</mo><mrow id="S3.E4.m1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.2.2.cmml"><munder id="S3.E4.m1.1.1.1.1.2.2.3" xref="S3.E4.m1.1.1.1.1.2.2.3.cmml"><mo movablelimits="false" id="S3.E4.m1.1.1.1.1.2.2.3.2" xref="S3.E4.m1.1.1.1.1.2.2.3.2.cmml">âˆ‘</mo><mover accent="true" id="S3.E4.m1.1.1.1.1.2.2.3.3" xref="S3.E4.m1.1.1.1.1.2.2.3.3.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2.3.3.2" xref="S3.E4.m1.1.1.1.1.2.2.3.3.2.cmml">y</mi><mo id="S3.E4.m1.1.1.1.1.2.2.3.3.1" xref="S3.E4.m1.1.1.1.1.2.2.3.3.1.cmml">~</mo></mover></munder><mrow id="S3.E4.m1.1.1.1.1.2.2.2" xref="S3.E4.m1.1.1.1.1.2.2.2.cmml"><msub id="S3.E4.m1.1.1.1.1.2.2.2.4" xref="S3.E4.m1.1.1.1.1.2.2.2.4.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2.2.4.2" xref="S3.E4.m1.1.1.1.1.2.2.2.4.2.cmml">p</mi><mi id="S3.E4.m1.1.1.1.1.2.2.2.4.3" xref="S3.E4.m1.1.1.1.1.2.2.2.4.3.cmml">ğ™³ğ™»ğ™¼</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.2.2.2.3" xref="S3.E4.m1.1.1.1.1.2.2.2.3.cmml">â€‹</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><mover accent="true" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">y</mi><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">~</mo></mover></mrow><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.2.2.2.3a" xref="S3.E4.m1.1.1.1.1.2.2.2.3.cmml">â€‹</mo><msub id="S3.E4.m1.1.1.1.1.2.2.2.5" xref="S3.E4.m1.1.1.1.1.2.2.2.5.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2.2.5.2" xref="S3.E4.m1.1.1.1.1.2.2.2.5.2.cmml">p</mi><mi id="S3.E4.m1.1.1.1.1.2.2.2.5.3" xref="S3.E4.m1.1.1.1.1.2.2.2.5.3.cmml">ğ™°ğš‚ğš</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.2.2.2.3b" xref="S3.E4.m1.1.1.1.1.2.2.2.3.cmml">â€‹</mo><mrow id="S3.E4.m1.1.1.1.1.2.2.2.2.1" xref="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.1.2.2.2.2.1.2" xref="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.2.2.2.2.1.1" xref="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.cmml"><mover accent="true" id="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.2" xref="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.2.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.2.2" xref="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.2.2.cmml">y</mi><mo id="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.2.1" xref="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.2.1.cmml">~</mo></mover><mo fence="false" id="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.1" xref="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.1.cmml">|</mo><mi id="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.3" xref="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.E4.m1.1.1.1.1.2.2.2.2.1.3" xref="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><eq id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3"></eq><apply id="S3.E4.m1.1.1.1.1.4.cmml" xref="S3.E4.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.4.1.cmml" xref="S3.E4.m1.1.1.1.1.4">superscript</csymbol><ci id="S3.E4.m1.1.1.1.1.4.2.cmml" xref="S3.E4.m1.1.1.1.1.4.2">ğ‘¦</ci><times id="S3.E4.m1.1.1.1.1.4.3.cmml" xref="S3.E4.m1.1.1.1.1.4.3"></times></apply><apply id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2"><times id="S3.E4.m1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.2.3"></times><apply id="S3.E4.m1.1.1.1.1.2.4.cmml" xref="S3.E4.m1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.2.4.1.cmml" xref="S3.E4.m1.1.1.1.1.2.4">subscript</csymbol><apply id="S3.E4.m1.1.1.1.1.2.4.2.cmml" xref="S3.E4.m1.1.1.1.1.2.4.2"><times id="S3.E4.m1.1.1.1.1.2.4.2.1.cmml" xref="S3.E4.m1.1.1.1.1.2.4.2.1"></times><ci id="S3.E4.m1.1.1.1.1.2.4.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.4.2.2">arg</ci><ci id="S3.E4.m1.1.1.1.1.2.4.2.3.cmml" xref="S3.E4.m1.1.1.1.1.2.4.2.3">max</ci></apply><ci id="S3.E4.m1.1.1.1.1.2.4.3.cmml" xref="S3.E4.m1.1.1.1.1.2.4.3">ğ‘¦</ci></apply><apply id="S3.E4.m1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2"><apply id="S3.E4.m1.1.1.1.1.2.2.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.3">subscript</csymbol><sum id="S3.E4.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.3.2"></sum><apply id="S3.E4.m1.1.1.1.1.2.2.3.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.3.3"><ci id="S3.E4.m1.1.1.1.1.2.2.3.3.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.3.3.1">~</ci><ci id="S3.E4.m1.1.1.1.1.2.2.3.3.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.3.3.2">ğ‘¦</ci></apply></apply><apply id="S3.E4.m1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2"><times id="S3.E4.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.3"></times><apply id="S3.E4.m1.1.1.1.1.2.2.2.4.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.4"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.2.2.2.4.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.4">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.2.2.2.4.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.4.2">ğ‘</ci><ci id="S3.E4.m1.1.1.1.1.2.2.2.4.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.4.3">ğ™³ğ™»ğ™¼</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.2">ğ‘¦</ci><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3"><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.1">~</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.3.2">ğ‘¦</ci></apply></apply><apply id="S3.E4.m1.1.1.1.1.2.2.2.5.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.5"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.2.2.2.5.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.5">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.2.2.2.5.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.5.2">ğ‘</ci><ci id="S3.E4.m1.1.1.1.1.2.2.2.5.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.5.3">ğ™°ğš‚ğš</ci></apply><apply id="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.2.1"><csymbol cd="latexml" id="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.1">conditional</csymbol><apply id="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.2"><ci id="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.2.1">~</ci><ci id="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.2.2">ğ‘¦</ci></apply><ci id="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2.2.1.1.3">ğ‘¥</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">y^{*}=\operatorname*{arg\,max}_{y}\sum_{\tilde{y}}p_{\tt{DLM}}(y|\tilde{y})p_{\tt{ASR}}(\tilde{y}|x).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.3" class="ltx_p">However, the exact optimization problem is computationally intractable. We consider one simplest greedy decoding, and also propose our novel decoding approach, which we call DSR-decoding.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Greedy-decoding</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.4" class="ltx_p">In the simplest case, greedy-decoding follows two steps: 1)Â ASR takes in the audio <math id="S3.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">x</annotation></semantics></math> and outputs the greedy hypothesis <math id="S3.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a"><mover accent="true" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml">y</mi><mo id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1"><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1">^</ci><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">\hat{y}</annotation></semantics></math>; and 2)Â DLM takes in the noisy hypothesis <math id="S3.SS2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.3.m3.1a"><mover accent="true" id="S3.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml">y</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1"><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1">^</ci><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.1c">\hat{y}</annotation></semantics></math> and do greedy decoding of it to <math id="S3.SS2.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="y^{*}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.4.m4.1a"><msup id="S3.SS2.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml">y</mi><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.4.m4.1b"><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2">ğ‘¦</ci><times id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.4.m4.1c">y^{*}</annotation></semantics></math>, agnostic to acoustic model probabilities:<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We experimented on CTC ASR models, so technically we do not use an exact maximization for <math id="footnote2.m1.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="footnote2.m1.1b"><mover accent="true" id="footnote2.m1.1.1" xref="footnote2.m1.1.1.cmml"><mi id="footnote2.m1.1.1.2" xref="footnote2.m1.1.1.2.cmml">y</mi><mo id="footnote2.m1.1.1.1" xref="footnote2.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="footnote2.m1.1c"><apply id="footnote2.m1.1.1.cmml" xref="footnote2.m1.1.1"><ci id="footnote2.m1.1.1.1.cmml" xref="footnote2.m1.1.1.1">^</ci><ci id="footnote2.m1.1.1.2.cmml" xref="footnote2.m1.1.1.2">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m1.1d">\hat{y}</annotation></semantics></math>. In practice we take an argmax of labels over label probabilities for each frame, deduplicate the inputs, and remove blank tokens. And we find that with this approximation DLM is able to generate good corrections.</span></span></span></p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.1" class="ltx_Math" alttext="y^{*}=\operatorname*{arg\,max}_{y}p_{\tt{DLM}}(y|\hat{y});\,\,\hat{y}=\operatorname*{arg\,max}_{\tilde{y}}p_{\tt{ASR}}(\tilde{y}|x)," display="block"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1.1"><mrow id="S3.E5.m1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.3.cmml"><mrow id="S3.E5.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml"><msup id="S3.E5.m1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.1.1.3.2.cmml">y</mi><mo id="S3.E5.m1.1.1.1.1.1.1.3.3" xref="S3.E5.m1.1.1.1.1.1.1.3.3.cmml">âˆ—</mo></msup><mo id="S3.E5.m1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E5.m1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.1.3.cmml"><munder id="S3.E5.m1.1.1.1.1.1.1.1.3.1" xref="S3.E5.m1.1.1.1.1.1.1.1.3.1.cmml"><mrow id="S3.E5.m1.1.1.1.1.1.1.1.3.1.2" xref="S3.E5.m1.1.1.1.1.1.1.1.3.1.2.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.1.3.1.2.2" xref="S3.E5.m1.1.1.1.1.1.1.1.3.1.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em" id="S3.E5.m1.1.1.1.1.1.1.1.3.1.2.1" xref="S3.E5.m1.1.1.1.1.1.1.1.3.1.2.1.cmml">â€‹</mo><mi id="S3.E5.m1.1.1.1.1.1.1.1.3.1.2.3" xref="S3.E5.m1.1.1.1.1.1.1.1.3.1.2.3.cmml">max</mi></mrow><mi id="S3.E5.m1.1.1.1.1.1.1.1.3.1.3" xref="S3.E5.m1.1.1.1.1.1.1.1.3.1.3.cmml">y</mi></munder><mo id="S3.E5.m1.1.1.1.1.1.1.1.3a" xref="S3.E5.m1.1.1.1.1.1.1.1.3.cmml">â¡</mo><msub id="S3.E5.m1.1.1.1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.1.3.2.2" xref="S3.E5.m1.1.1.1.1.1.1.1.3.2.2.cmml">p</mi><mi id="S3.E5.m1.1.1.1.1.1.1.1.3.2.3" xref="S3.E5.m1.1.1.1.1.1.1.1.3.2.3.cmml">ğ™³ğ™»ğ™¼</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E5.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><mover accent="true" id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">y</mi><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo stretchy="false" id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo rspace="0.497em" id="S3.E5.m1.1.1.1.1.2.3" xref="S3.E5.m1.1.1.1.1.3a.cmml">;</mo><mrow id="S3.E5.m1.1.1.1.1.2.2" xref="S3.E5.m1.1.1.1.1.2.2.cmml"><mover accent="true" id="S3.E5.m1.1.1.1.1.2.2.3" xref="S3.E5.m1.1.1.1.1.2.2.3.cmml"><mi id="S3.E5.m1.1.1.1.1.2.2.3.2" xref="S3.E5.m1.1.1.1.1.2.2.3.2.cmml">y</mi><mo id="S3.E5.m1.1.1.1.1.2.2.3.1" xref="S3.E5.m1.1.1.1.1.2.2.3.1.cmml">^</mo></mover><mo id="S3.E5.m1.1.1.1.1.2.2.2" xref="S3.E5.m1.1.1.1.1.2.2.2.cmml">=</mo><mrow id="S3.E5.m1.1.1.1.1.2.2.1" xref="S3.E5.m1.1.1.1.1.2.2.1.cmml"><mrow id="S3.E5.m1.1.1.1.1.2.2.1.3" xref="S3.E5.m1.1.1.1.1.2.2.1.3.cmml"><munder id="S3.E5.m1.1.1.1.1.2.2.1.3.1" xref="S3.E5.m1.1.1.1.1.2.2.1.3.1.cmml"><mrow id="S3.E5.m1.1.1.1.1.2.2.1.3.1.2" xref="S3.E5.m1.1.1.1.1.2.2.1.3.1.2.cmml"><mi id="S3.E5.m1.1.1.1.1.2.2.1.3.1.2.2" xref="S3.E5.m1.1.1.1.1.2.2.1.3.1.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em" id="S3.E5.m1.1.1.1.1.2.2.1.3.1.2.1" xref="S3.E5.m1.1.1.1.1.2.2.1.3.1.2.1.cmml">â€‹</mo><mi id="S3.E5.m1.1.1.1.1.2.2.1.3.1.2.3" xref="S3.E5.m1.1.1.1.1.2.2.1.3.1.2.3.cmml">max</mi></mrow><mover accent="true" id="S3.E5.m1.1.1.1.1.2.2.1.3.1.3" xref="S3.E5.m1.1.1.1.1.2.2.1.3.1.3.cmml"><mi id="S3.E5.m1.1.1.1.1.2.2.1.3.1.3.2" xref="S3.E5.m1.1.1.1.1.2.2.1.3.1.3.2.cmml">y</mi><mo id="S3.E5.m1.1.1.1.1.2.2.1.3.1.3.1" xref="S3.E5.m1.1.1.1.1.2.2.1.3.1.3.1.cmml">~</mo></mover></munder><mo id="S3.E5.m1.1.1.1.1.2.2.1.3a" xref="S3.E5.m1.1.1.1.1.2.2.1.3.cmml">â¡</mo><msub id="S3.E5.m1.1.1.1.1.2.2.1.3.2" xref="S3.E5.m1.1.1.1.1.2.2.1.3.2.cmml"><mi id="S3.E5.m1.1.1.1.1.2.2.1.3.2.2" xref="S3.E5.m1.1.1.1.1.2.2.1.3.2.2.cmml">p</mi><mi id="S3.E5.m1.1.1.1.1.2.2.1.3.2.3" xref="S3.E5.m1.1.1.1.1.2.2.1.3.2.3.cmml">ğ™°ğš‚ğš</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.1.2.2.1.2" xref="S3.E5.m1.1.1.1.1.2.2.1.2.cmml">â€‹</mo><mrow id="S3.E5.m1.1.1.1.1.2.2.1.1.1" xref="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E5.m1.1.1.1.1.2.2.1.1.1.2" xref="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.1.1.1.1.2.2.1.1.1.1" xref="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.cmml"><mover accent="true" id="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.2.cmml"><mi id="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.2.2" xref="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.2.2.cmml">y</mi><mo id="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.2.1" xref="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.2.1.cmml">~</mo></mover><mo fence="false" id="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.1.cmml">|</mo><mi id="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.E5.m1.1.1.1.1.2.2.1.1.1.3" xref="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E5.m1.1.1.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.3a.cmml" xref="S3.E5.m1.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S3.E5.m1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1"><eq id="S3.E5.m1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2"></eq><apply id="S3.E5.m1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3.2">ğ‘¦</ci><times id="S3.E5.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3.3"></times></apply><apply id="S3.E5.m1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1"><times id="S3.E5.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.2"></times><apply id="S3.E5.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.3"><apply id="S3.E5.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.1.3.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.3.1">subscript</csymbol><apply id="S3.E5.m1.1.1.1.1.1.1.1.3.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.3.1.2"><times id="S3.E5.m1.1.1.1.1.1.1.1.3.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.3.1.2.1"></times><ci id="S3.E5.m1.1.1.1.1.1.1.1.3.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.3.1.2.2">arg</ci><ci id="S3.E5.m1.1.1.1.1.1.1.1.3.1.2.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.3.1.2.3">max</ci></apply><ci id="S3.E5.m1.1.1.1.1.1.1.1.3.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.3.1.3">ğ‘¦</ci></apply><apply id="S3.E5.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.3.2.2">ğ‘</ci><ci id="S3.E5.m1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.3.2.3">ğ™³ğ™»ğ™¼</ci></apply></apply><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.2">ğ‘¦</ci><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3"><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.1">^</ci><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.3.2">ğ‘¦</ci></apply></apply></apply></apply><apply id="S3.E5.m1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.2.2"><eq id="S3.E5.m1.1.1.1.1.2.2.2.cmml" xref="S3.E5.m1.1.1.1.1.2.2.2"></eq><apply id="S3.E5.m1.1.1.1.1.2.2.3.cmml" xref="S3.E5.m1.1.1.1.1.2.2.3"><ci id="S3.E5.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.E5.m1.1.1.1.1.2.2.3.1">^</ci><ci id="S3.E5.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.E5.m1.1.1.1.1.2.2.3.2">ğ‘¦</ci></apply><apply id="S3.E5.m1.1.1.1.1.2.2.1.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1"><times id="S3.E5.m1.1.1.1.1.2.2.1.2.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.2"></times><apply id="S3.E5.m1.1.1.1.1.2.2.1.3.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.3"><apply id="S3.E5.m1.1.1.1.1.2.2.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.3.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.2.2.1.3.1.1.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.3.1">subscript</csymbol><apply id="S3.E5.m1.1.1.1.1.2.2.1.3.1.2.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.3.1.2"><times id="S3.E5.m1.1.1.1.1.2.2.1.3.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.3.1.2.1"></times><ci id="S3.E5.m1.1.1.1.1.2.2.1.3.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.3.1.2.2">arg</ci><ci id="S3.E5.m1.1.1.1.1.2.2.1.3.1.2.3.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.3.1.2.3">max</ci></apply><apply id="S3.E5.m1.1.1.1.1.2.2.1.3.1.3.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.3.1.3"><ci id="S3.E5.m1.1.1.1.1.2.2.1.3.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.3.1.3.1">~</ci><ci id="S3.E5.m1.1.1.1.1.2.2.1.3.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.3.1.3.2">ğ‘¦</ci></apply></apply><apply id="S3.E5.m1.1.1.1.1.2.2.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.2.2.1.3.2.1.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.3.2">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.2.2.1.3.2.2.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.3.2.2">ğ‘</ci><ci id="S3.E5.m1.1.1.1.1.2.2.1.3.2.3.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.3.2.3">ğ™°ğš‚ğš</ci></apply></apply><apply id="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.1.1"><csymbol cd="latexml" id="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.1">conditional</csymbol><apply id="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.2"><ci id="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.2.1">~</ci><ci id="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.2.2">ğ‘¦</ci></apply><ci id="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.2.2.1.1.1.1.3">ğ‘¥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">y^{*}=\operatorname*{arg\,max}_{y}p_{\tt{DLM}}(y|\hat{y});\,\,\hat{y}=\operatorname*{arg\,max}_{\tilde{y}}p_{\tt{ASR}}(\tilde{y}|x),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS0.Px1.p1.5" class="ltx_p">where both maximizations are approximated by the greedy algorithm.
Note that with this simple method, error correction could be performed without access to the audio or the ASR prediction scores, which is not achievable when using conventional LMs.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">DSR-decoding</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.6" class="ltx_p">To further enhance inference by incorporating acoustic model outputs alongside DLM, we propose a new variant of the decoding algorithm, dubbed <span id="S3.SS2.SSS0.Px2.p1.6.1" class="ltx_text ltx_font_italic">DSR-decoding</span>. It follows four steps: 1)Â ASR takes in the audio <math id="S3.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.1a"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.1c">x</annotation></semantics></math> and outputs the greedy hypothesis <math id="S3.SS2.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.2.m2.1a"><mover accent="true" id="S3.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml">y</mi><mo id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1"><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.1">^</ci><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.2">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.2.m2.1c">\hat{y}</annotation></semantics></math>, same as in greedy-decoding; 2)Â DLM takes in the greedy hypothesis <math id="S3.SS2.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.3.m3.1a"><mover accent="true" id="S3.SS2.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.cmml">y</mi><mo id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1"><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.1">^</ci><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.2">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.3.m3.1c">\hat{y}</annotation></semantics></math> and generates an <math id="S3.SS2.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.4.m4.1a"><mi id="S3.SS2.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.4.m4.1b"><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.4.m4.1c">n</annotation></semantics></math>-best list of corrected candidates using a beam search;
3)Â For each candidate generated by DLM, we compute the acoustic score with ASR; 4)Â Each candidate is rescored using a weighted sum of the ASR score and the DLM score, using a single hyperparameter <math id="S3.SS2.SSS0.Px2.p1.5.m5.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.5.m5.1a"><mi id="S3.SS2.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.5.m5.1b"><ci id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.5.m5.1c">\lambda</annotation></semantics></math> that is tuned on the entire validation set.
In <math id="S3.SS2.SSS0.Px2.p1.6.m6.1" class="ltx_Math" alttext="\log" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.6.m6.1a"><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml">log</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.6.m6.1b"><log id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1"></log></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.6.m6.1c">\log</annotation></semantics></math> space, the above can be writtenÂ as:</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.39" class="ltx_math_unparsed" alttext="\begin{split}y^{*}=\operatorname*{arg\,max}\limits_{y\in n\text{-best}[p_{\tt{DLM}}(.|\hat{y})]}\lambda\log{p_{\tt{DLM}}(y|\hat{y})}+\log{p_{\tt{ASR}}(y|x)}\qquad\text{with}\qquad\hat{y}=\mathrm{arg}\max_{\tilde{y}}p_{\tt{ASR}}(\tilde{y}|x),\end{split}" display="block"><semantics id="S3.E6.m1.39a"><mtable displaystyle="true" id="S3.E6.m1.39.39.2"><mtr id="S3.E6.m1.39.39.2a"><mtd class="ltx_align_right" columnalign="right" id="S3.E6.m1.39.39.2b"><mrow id="S3.E6.m1.39.39.2.38.38.38.38"><mrow id="S3.E6.m1.39.39.2.38.38.38.38.1"><mrow id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1"><msup id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.2"><mi id="S3.E6.m1.1.1.1.1.1.1">y</mi><mo id="S3.E6.m1.2.2.2.2.2.2.1">âˆ—</mo></msup><mo id="S3.E6.m1.3.3.3.3.3.3">=</mo><mrow id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1"><mrow id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1"><mrow id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1.1"><mrow id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1.1.3"><munder id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1.1.3.1"><mrow id="S3.E6.m1.4.4.4.4.4.4"><mi id="S3.E6.m1.4.4.4.4.4.4.2">arg</mi><mo lspace="0.170em" rspace="0em" id="S3.E6.m1.4.4.4.4.4.4.1">â€‹</mo><mi id="S3.E6.m1.4.4.4.4.4.4.3">max</mi></mrow><mrow id="S3.E6.m1.5.5.5.5.5.5.1"><mi id="S3.E6.m1.5.5.5.5.5.5.1.1">y</mi><mo id="S3.E6.m1.5.5.5.5.5.5.1.2">âˆˆ</mo><mi id="S3.E6.m1.5.5.5.5.5.5.1.3">n</mi><mtext id="S3.E6.m1.5.5.5.5.5.5.1.4">-best</mtext><mrow id="S3.E6.m1.5.5.5.5.5.5.1.5"><mo stretchy="false" id="S3.E6.m1.5.5.5.5.5.5.1.5.1">[</mo><msub id="S3.E6.m1.5.5.5.5.5.5.1.5.2"><mi id="S3.E6.m1.5.5.5.5.5.5.1.5.2.2">p</mi><mi id="S3.E6.m1.5.5.5.5.5.5.1.5.2.3">ğ™³ğ™»ğ™¼</mi></msub><mrow id="S3.E6.m1.5.5.5.5.5.5.1.5.3"><mo stretchy="false" id="S3.E6.m1.5.5.5.5.5.5.1.5.3.1">(</mo><mo lspace="0em" rspace="0.167em" id="S3.E6.m1.5.5.5.5.5.5.1.5.3.2">.</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E6.m1.5.5.5.5.5.5.1.5.3.3">|</mo><mover accent="true" id="S3.E6.m1.5.5.5.5.5.5.1.5.3.4"><mi id="S3.E6.m1.5.5.5.5.5.5.1.5.3.4.2">y</mi><mo id="S3.E6.m1.5.5.5.5.5.5.1.5.3.4.1">^</mo></mover><mo stretchy="false" id="S3.E6.m1.5.5.5.5.5.5.1.5.3.5">)</mo></mrow><mo stretchy="false" id="S3.E6.m1.5.5.5.5.5.5.1.5.4">]</mo></mrow></mrow></munder><mo id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1.1.3a">â¡</mo><mrow id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1.1.3.2"><mi id="S3.E6.m1.6.6.6.6.6.6">Î»</mi><mo lspace="0.167em" rspace="0em" id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1.1.3.2.1">â€‹</mo><mrow id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1.1.3.2.2"><mi id="S3.E6.m1.7.7.7.7.7.7">log</mi><mo lspace="0.167em" id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1.1.3.2.2a">â¡</mo><msub id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1.1.3.2.2.1"><mi id="S3.E6.m1.8.8.8.8.8.8">p</mi><mi id="S3.E6.m1.9.9.9.9.9.9.1">ğ™³ğ™»ğ™¼</mi></msub></mrow></mrow></mrow><mo lspace="0em" rspace="0em" id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1.1.2">â€‹</mo><mrow id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1.1.1.1"><mo stretchy="false" id="S3.E6.m1.10.10.10.10.10.10">(</mo><mrow id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1.1.1.1.1"><mi id="S3.E6.m1.11.11.11.11.11.11">y</mi><mo fence="false" id="S3.E6.m1.12.12.12.12.12.12a">|</mo><mover accent="true" id="S3.E6.m1.13.13.13.13.13.13"><mi id="S3.E6.m1.13.13.13.13.13.13.2">y</mi><mo id="S3.E6.m1.13.13.13.13.13.13.1">^</mo></mover></mrow><mo stretchy="false" id="S3.E6.m1.14.14.14.14.14.14">)</mo></mrow></mrow><mo id="S3.E6.m1.15.15.15.15.15.15">+</mo><mrow id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1.2"><mrow id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1.2.3"><mi id="S3.E6.m1.16.16.16.16.16.16">log</mi><mo lspace="0.167em" id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1.2.3a">â¡</mo><msub id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1.2.3.1"><mi id="S3.E6.m1.17.17.17.17.17.17">p</mi><mi id="S3.E6.m1.18.18.18.18.18.18.1">ğ™°ğš‚ğš</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1.2.2">â€‹</mo><mrow id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1.2.1.1"><mo stretchy="false" id="S3.E6.m1.19.19.19.19.19.19">(</mo><mrow id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.1.2.1.1.1"><mi id="S3.E6.m1.20.20.20.20.20.20">y</mi><mo fence="false" id="S3.E6.m1.21.21.21.21.21.21a">|</mo><mi id="S3.E6.m1.22.22.22.22.22.22">x</mi></mrow><mo stretchy="false" id="S3.E6.m1.23.23.23.23.23.23">)</mo></mrow></mrow></mrow><mspace width="2em" id="S3.E6.m1.39.39.2.38.38.38.38.1.1.1.1.1.2"></mspace><mtext id="S3.E6.m1.24.24.24.24.24.24">with</mtext></mrow></mrow><mspace width="2em" id="S3.E6.m1.39.39.2.38.38.38.38.1.3"></mspace><mrow id="S3.E6.m1.39.39.2.38.38.38.38.1.2.2"><mover accent="true" id="S3.E6.m1.25.25.25.25.25.25"><mi id="S3.E6.m1.25.25.25.25.25.25.2">y</mi><mo id="S3.E6.m1.25.25.25.25.25.25.1">^</mo></mover><mo id="S3.E6.m1.26.26.26.26.26.26">=</mo><mrow id="S3.E6.m1.39.39.2.38.38.38.38.1.2.2.1"><mi id="S3.E6.m1.27.27.27.27.27.27">arg</mi><mo lspace="0.167em" rspace="0em" id="S3.E6.m1.39.39.2.38.38.38.38.1.2.2.1.2">â€‹</mo><mrow id="S3.E6.m1.39.39.2.38.38.38.38.1.2.2.1.3"><munder id="S3.E6.m1.39.39.2.38.38.38.38.1.2.2.1.3.1"><mi id="S3.E6.m1.28.28.28.28.28.28">max</mi><mover accent="true" id="S3.E6.m1.29.29.29.29.29.29.1"><mi id="S3.E6.m1.29.29.29.29.29.29.1.2">y</mi><mo id="S3.E6.m1.29.29.29.29.29.29.1.1">~</mo></mover></munder><mo lspace="0.167em" id="S3.E6.m1.39.39.2.38.38.38.38.1.2.2.1.3a">â¡</mo><msub id="S3.E6.m1.39.39.2.38.38.38.38.1.2.2.1.3.2"><mi id="S3.E6.m1.30.30.30.30.30.30">p</mi><mi id="S3.E6.m1.31.31.31.31.31.31.1">ğ™°ğš‚ğš</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E6.m1.39.39.2.38.38.38.38.1.2.2.1.2a">â€‹</mo><mrow id="S3.E6.m1.39.39.2.38.38.38.38.1.2.2.1.1.1"><mo stretchy="false" id="S3.E6.m1.32.32.32.32.32.32">(</mo><mrow id="S3.E6.m1.39.39.2.38.38.38.38.1.2.2.1.1.1.1"><mover accent="true" id="S3.E6.m1.33.33.33.33.33.33"><mi id="S3.E6.m1.33.33.33.33.33.33.2">y</mi><mo id="S3.E6.m1.33.33.33.33.33.33.1">~</mo></mover><mo fence="false" id="S3.E6.m1.34.34.34.34.34.34a">|</mo><mi id="S3.E6.m1.35.35.35.35.35.35">x</mi></mrow><mo stretchy="false" id="S3.E6.m1.36.36.36.36.36.36">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E6.m1.37.37.37.37.37.37">,</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex" id="S3.E6.m1.39b">\begin{split}y^{*}=\operatorname*{arg\,max}\limits_{y\in n\text{-best}[p_{\tt{DLM}}(.|\hat{y})]}\lambda\log{p_{\tt{DLM}}(y|\hat{y})}+\log{p_{\tt{ASR}}(y|x)}\qquad\text{with}\qquad\hat{y}=\mathrm{arg}\max_{\tilde{y}}p_{\tt{ASR}}(\tilde{y}|x),\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS0.Px2.p1.8" class="ltx_p">where <math id="S3.SS2.SSS0.Px2.p1.7.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.7.m1.1a"><mi id="S3.SS2.SSS0.Px2.p1.7.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.7.m1.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.7.m1.1b"><ci id="S3.SS2.SSS0.Px2.p1.7.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m1.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.7.m1.1c">\lambda</annotation></semantics></math> is a scalar that can be used to adjust the relative strengths of the ASR and the DLM and <math id="S3.SS2.SSS0.Px2.p1.8.m2.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.8.m2.1a"><mover accent="true" id="S3.SS2.SSS0.Px2.p1.8.m2.1.1" xref="S3.SS2.SSS0.Px2.p1.8.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.8.m2.1.1.2" xref="S3.SS2.SSS0.Px2.p1.8.m2.1.1.2.cmml">y</mi><mo id="S3.SS2.SSS0.Px2.p1.8.m2.1.1.1" xref="S3.SS2.SSS0.Px2.p1.8.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.8.m2.1b"><apply id="S3.SS2.SSS0.Px2.p1.8.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m2.1.1"><ci id="S3.SS2.SSS0.Px2.p1.8.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m2.1.1.1">^</ci><ci id="S3.SS2.SSS0.Px2.p1.8.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m2.1.1.2">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.8.m2.1c">\hat{y}</annotation></semantics></math> is obtained by the greedy algorithm.
Note that DSR-decoding is quite different from rescoring that is performed with neural LMs, and we show a comparison of DSR-decoding with conventional decoding approach in Figure <a href="#S3.F1" title="Figure 1 â€£ 3 Denoising Speech Recognition (DSR) â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. See Appendix <a href="#A4.SS1" title="D.1 LM-rescoring v.s. DSR-decoding â€£ Appendix D Training Details â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D.1</span></a> for more details.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Details</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>TTS systems and DLM training set</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.7" class="ltx_p">All our DLMs are trained on data derived from the LibriSpeechÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> LM corpus (40M sentences with a total of 800M words) if not stated otherwise.
We use three different TTS systems to synthesize audio from this corpus in our experiments:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Tacotron-2</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. A single speaker high quality TTS system with a model fromÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">YourTTS</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. A zero-shot, multi-speaker TTS system with model fromÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">RichTTS</span>. An in-house autoregressive, zero-shot, multi-speaker TTS, similar in large part to Tacotron-2, but with additional inputs for speaker embeddings, using D-vectorsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. This TTS is trained on the LibriSpeech ASR corpus.</p>
</div>
</li>
</ul>
<p id="S4.SS1.p1.6" class="ltx_p">For each sentence <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">y</annotation></semantics></math> in the text corpus we generate a corresponding audio waveform <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\tilde{x}" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mover accent="true" id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mi id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">x</mi><mo id="S4.SS1.p1.2.m2.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><ci id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1">~</ci><ci id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\tilde{x}</annotation></semantics></math> from the TTS system. For multi-speaker TTS systems, we randomly select a speaker from speakers in the LibriSpeech ASR training corpus. For YourTTS, this involves providing an audio of a randomly selected speaker as the conditioning example.
For our internal RichTTS system, this is done by computing a D-vector from a random audio from a random speaker.
ASR system is trained with Connectionist Temporal ClassificationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> (CTC) loss.
Then <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="\tilde{x}" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mover accent="true" id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml"><mi id="S4.SS1.p1.3.m3.1.1.2" xref="S4.SS1.p1.3.m3.1.1.2.cmml">x</mi><mo id="S4.SS1.p1.3.m3.1.1.1" xref="S4.SS1.p1.3.m3.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><apply id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"><ci id="S4.SS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1.1">~</ci><ci id="S4.SS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">\tilde{x}</annotation></semantics></math> is processed with this ASR system to generate a hypothesis <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mover accent="true" id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><mi id="S4.SS1.p1.4.m4.1.1.2" xref="S4.SS1.p1.4.m4.1.1.2.cmml">y</mi><mo id="S4.SS1.p1.4.m4.1.1.1" xref="S4.SS1.p1.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><ci id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1.1">^</ci><ci id="S4.SS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">\hat{y}</annotation></semantics></math> that is paired with the sentence <math id="S4.SS1.p1.5.m5.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS1.p1.5.m5.1a"><mi id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><ci id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">y</annotation></semantics></math>.
The entire dataset of <math id="S4.SS1.p1.6.m6.3" class="ltx_Math" alttext="\{(\hat{y},y)\}" display="inline"><semantics id="S4.SS1.p1.6.m6.3a"><mrow id="S4.SS1.p1.6.m6.3.3.1" xref="S4.SS1.p1.6.m6.3.3.2.cmml"><mo stretchy="false" id="S4.SS1.p1.6.m6.3.3.1.2" xref="S4.SS1.p1.6.m6.3.3.2.cmml">{</mo><mrow id="S4.SS1.p1.6.m6.3.3.1.1.2" xref="S4.SS1.p1.6.m6.3.3.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p1.6.m6.3.3.1.1.2.1" xref="S4.SS1.p1.6.m6.3.3.1.1.1.cmml">(</mo><mover accent="true" id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml"><mi id="S4.SS1.p1.6.m6.1.1.2" xref="S4.SS1.p1.6.m6.1.1.2.cmml">y</mi><mo id="S4.SS1.p1.6.m6.1.1.1" xref="S4.SS1.p1.6.m6.1.1.1.cmml">^</mo></mover><mo id="S4.SS1.p1.6.m6.3.3.1.1.2.2" xref="S4.SS1.p1.6.m6.3.3.1.1.1.cmml">,</mo><mi id="S4.SS1.p1.6.m6.2.2" xref="S4.SS1.p1.6.m6.2.2.cmml">y</mi><mo stretchy="false" id="S4.SS1.p1.6.m6.3.3.1.1.2.3" xref="S4.SS1.p1.6.m6.3.3.1.1.1.cmml">)</mo></mrow><mo stretchy="false" id="S4.SS1.p1.6.m6.3.3.1.3" xref="S4.SS1.p1.6.m6.3.3.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.3b"><set id="S4.SS1.p1.6.m6.3.3.2.cmml" xref="S4.SS1.p1.6.m6.3.3.1"><interval closure="open" id="S4.SS1.p1.6.m6.3.3.1.1.1.cmml" xref="S4.SS1.p1.6.m6.3.3.1.1.2"><apply id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1"><ci id="S4.SS1.p1.6.m6.1.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1.1">^</ci><ci id="S4.SS1.p1.6.m6.1.1.2.cmml" xref="S4.SS1.p1.6.m6.1.1.2">ğ‘¦</ci></apply><ci id="S4.SS1.p1.6.m6.2.2.cmml" xref="S4.SS1.p1.6.m6.2.2">ğ‘¦</ci></interval></set></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.3c">\{(\hat{y},y)\}</annotation></semantics></math> pairs is used to train the DLMs.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>ASR models</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In most experiments, ASR is a Transformer-based encoder model with 255M parameters. This ASR model is used throughout the paper to provide baseline WERs and we refer to it in the text as a <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">baseline ASR</span>.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We used an earlier version of the <span id="footnote3.1" class="ltx_text ltx_font_italic">baseline ASR</span> with 2.6 (6.4)% WER on <span id="footnote3.2" class="ltx_text ltx_font_italic">test-clean (other)</span> to generate data.</span></span></span>
We also experiment with a number of different ASR models to assess whether DLMs trained on noise generated by the <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">baseline ASR</span> would transfer to other ASR architectures.
This includes QuartznetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, ConformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and WhisperÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.
We implemented our own Quartznet (7M params) and Conformer (102M params).
Whisper models (Whisper-base, Whisper-small) are pretrained modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> with a much larger proprietary audio corpus.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">We train all ASR models (Transformer, Quartznet, and Conformer) on the LibriSpeech 960h train set with CTC loss and a character vocabulary, including apostrophe (â€˜).
As is shown in prior work that TTS data can improve ASR modelsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, we train several Transformer-based ASR models with different mixture proportions of LibriSpeech and TTS audio in a minibatch, and other settings remain the same as the <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_italic">baseline ASR</span>. We denote these ASRs as <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_italic">baseline ASR (LS+TTS)</span>.
All training details are in AppendixÂ <a href="#A4.SS2" title="D.2 ASR models â€£ Appendix D Training Details â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D.2</span></a>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Denoising LMs and neural LMs</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Denoising LM</span>Â Â Â DLM is a Transformer-based model with a sequence-to-sequence encoder-decoder architecture trained with a cross entropy loss and a character vocabulary. There are 16 encoder layers and 4 decoder layers.
Dropout and layer drop are set to 0.1.
We vary the embedding dimension and the MLP hidden dimension for DLMs of different sizes: the embedding dimensions are 512, 768 and 1280, and MLP hidden dimensions are 2048, 3072 and 6144 for the 69M, 155M and 484M size DLMs respectively.
The 484M architecture design was chosen based on reported results fromÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. Sinpos position embedding is used.
The DLM training set is generated from LibriSpeech LM text corpus by different TTS systems but from a single early version <span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_italic">baseline ASR</span> model, which is trained with a smaller total batch size.
Other details are in AppendixÂ <a href="#A4.SS3" title="D.3 DLMs â€£ Appendix D Training Details â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D.3</span></a>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Neural LM</span>Â Â Â Neural LM is a Transformer-based model with a decoder-only architecture trained with a cross entropy loss and 10K word-piece vocabulary. It has 20 layers in the decoder, and shares the same embedding dimension and the MLP hidden dimension with DLM. Sinpos position embedding is used. Neural LMs are trained on LibriSpeech LM text corpus (the same text data are used for synthetic audio generation by TTS for DLM training).
To performing neural LM rescoring (Figure <a href="#S3.F1" title="Figure 1 â€£ 3 Denoising Speech Recognition (DSR) â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> right panel), we generate an <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mi id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><ci id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">n</annotation></semantics></math>-best list from the beam-search decoding of the CTC ASR model with a 4-gram word-level LM trained on the same LibriSpeech LM text corpus with top-200k words, and rescore using the neural LM scores and CTC ASR scores, similar toÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. Perplexity of trained models and other training details can be found in AppendixÂ <a href="#A4.SS4" title="D.4 Neural LMs â€£ Appendix D Training Details â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D.4</span></a>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Pushing the limits: DLM achieves new SOTA WER on LibriSpeech</h3>

<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>LibriSpeech validation and test sets word error rate (WER, %) for: i) Transformer ASR model (256M, <span id="S5.T1.3.1" class="ltx_text ltx_font_italic">baseline</span>) with greedy decoding trained on LibriSpeech (960h), denoted as â€œbaseline ASRâ€, or on LibriSpeech (960h) and extra TTS data generated from LibriSpeech LM corpus by RichTTS, denoted as â€œbaseline ASR (LS+TTS)â€ (mixing proportion is 1:1); ii) DLM (484M) with DSR-greedy/DSR-decoding and a neural LM (neLM, 500M) rescoring applied on top of ASR models.
DLM is trained on data generated by the <span id="S5.T1.4.2" class="ltx_text ltx_font_italic">baseline ASR</span> applied to LibriSpeech (960h) and YourTTS and RichTTS synthetic data (LibriSpeech LM corpus), used in proportion real:synthetic of 1:9 in the minibatch, with frequency masking and 10% random substitutions of characters.</figcaption>
<div id="S5.T1.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:156.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.0pt,2.7pt) scale(0.966323869484448,0.966323869484448) ;">
<table id="S5.T1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T1.5.1.1" class="ltx_tr">
<td id="S5.T1.5.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Model</td>
<td id="S5.T1.5.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">dev-clean</td>
<td id="S5.T1.5.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">dev-other</td>
<td id="S5.T1.5.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">test-clean</td>
<td id="S5.T1.5.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">test-other</td>
</tr>
<tr id="S5.T1.5.1.2" class="ltx_tr">
<td id="S5.T1.5.1.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T1.5.1.2.1.1" class="ltx_text ltx_font_italic">baseline ASR</span></td>
<td id="S5.T1.5.1.2.2" class="ltx_td ltx_align_center ltx_border_t">2.1</td>
<td id="S5.T1.5.1.2.3" class="ltx_td ltx_align_center ltx_border_t">5.5</td>
<td id="S5.T1.5.1.2.4" class="ltx_td ltx_align_center ltx_border_t">2.2</td>
<td id="S5.T1.5.1.2.5" class="ltx_td ltx_align_center ltx_border_t">5.3</td>
</tr>
<tr id="S5.T1.5.1.3" class="ltx_tr">
<td id="S5.T1.5.1.3.1" class="ltx_td ltx_align_left">+ neLM (LM-rescoring)</td>
<td id="S5.T1.5.1.3.2" class="ltx_td ltx_align_center">1.5</td>
<td id="S5.T1.5.1.3.3" class="ltx_td ltx_align_center">4.0</td>
<td id="S5.T1.5.1.3.4" class="ltx_td ltx_align_center">2.0</td>
<td id="S5.T1.5.1.3.5" class="ltx_td ltx_align_center">4.1</td>
</tr>
<tr id="S5.T1.5.1.4" class="ltx_tr">
<td id="S5.T1.5.1.4.1" class="ltx_td ltx_align_left">+ DLM (Greedy-decoding)</td>
<td id="S5.T1.5.1.4.2" class="ltx_td ltx_align_center">1.9</td>
<td id="S5.T1.5.1.4.3" class="ltx_td ltx_align_center">3.9</td>
<td id="S5.T1.5.1.4.4" class="ltx_td ltx_align_center">2.0</td>
<td id="S5.T1.5.1.4.5" class="ltx_td ltx_align_center">4.1</td>
</tr>
<tr id="S5.T1.5.1.5" class="ltx_tr">
<td id="S5.T1.5.1.5.1" class="ltx_td ltx_align_left">+ DLM (DSR-decoding)</td>
<td id="S5.T1.5.1.5.2" class="ltx_td ltx_align_center">1.5</td>
<td id="S5.T1.5.1.5.3" class="ltx_td ltx_align_center">3.4</td>
<td id="S5.T1.5.1.5.4" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.5.4.1" class="ltx_text ltx_font_bold">1.6</span></td>
<td id="S5.T1.5.1.5.5" class="ltx_td ltx_align_center"><span id="S5.T1.5.1.5.5.1" class="ltx_text ltx_font_bold">3.6</span></td>
</tr>
<tr id="S5.T1.5.1.6" class="ltx_tr">
<td id="S5.T1.5.1.6.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T1.5.1.6.1.1" class="ltx_text ltx_font_italic">baseline ASR (LS+TTS)</span></td>
<td id="S5.T1.5.1.6.2" class="ltx_td ltx_align_center ltx_border_t">1.9</td>
<td id="S5.T1.5.1.6.3" class="ltx_td ltx_align_center ltx_border_t">4.7</td>
<td id="S5.T1.5.1.6.4" class="ltx_td ltx_align_center ltx_border_t">2.0</td>
<td id="S5.T1.5.1.6.5" class="ltx_td ltx_align_center ltx_border_t">4.7</td>
</tr>
<tr id="S5.T1.5.1.7" class="ltx_tr">
<td id="S5.T1.5.1.7.1" class="ltx_td ltx_align_left">+ neLM (LM-rescoring)</td>
<td id="S5.T1.5.1.7.2" class="ltx_td ltx_align_center">1.3</td>
<td id="S5.T1.5.1.7.3" class="ltx_td ltx_align_center">3.4</td>
<td id="S5.T1.5.1.7.4" class="ltx_td ltx_align_center">1.8</td>
<td id="S5.T1.5.1.7.5" class="ltx_td ltx_align_center">3.7</td>
</tr>
<tr id="S5.T1.5.1.8" class="ltx_tr">
<td id="S5.T1.5.1.8.1" class="ltx_td ltx_align_left">+ DLM (Greedy-decoding)</td>
<td id="S5.T1.5.1.8.2" class="ltx_td ltx_align_center">1.8</td>
<td id="S5.T1.5.1.8.3" class="ltx_td ltx_align_center">3.6</td>
<td id="S5.T1.5.1.8.4" class="ltx_td ltx_align_center">1.9</td>
<td id="S5.T1.5.1.8.5" class="ltx_td ltx_align_center">3.9</td>
</tr>
<tr id="S5.T1.5.1.9" class="ltx_tr">
<td id="S5.T1.5.1.9.1" class="ltx_td ltx_align_left ltx_border_bb">+ DLM (DSR-decoding)</td>
<td id="S5.T1.5.1.9.2" class="ltx_td ltx_align_center ltx_border_bb">1.4</td>
<td id="S5.T1.5.1.9.3" class="ltx_td ltx_align_center ltx_border_bb">3.1</td>
<td id="S5.T1.5.1.9.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.5.1.9.4.1" class="ltx_text ltx_font_bold">1.5</span></td>
<td id="S5.T1.5.1.9.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.5.1.9.5.1" class="ltx_text ltx_font_bold">3.3</span></td>
</tr>
</table>
</span></div>
</figure>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Examples of ASR hypotheses, neural LM hypotheses and DLM corrections.</figcaption>
<div id="S5.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:56pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-131.7pt,17.0pt) scale(0.6220727726804,0.6220727726804) ;">
<table id="S5.T2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<td id="S5.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">ASR</td>
<td id="S5.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_border_tt">Neural LM</td>
<td id="S5.T2.1.1.1.3" class="ltx_td ltx_align_left ltx_border_tt">DLM</td>
<td id="S5.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Error Type</td>
</tr>
<tr id="S5.T2.1.1.2" class="ltx_tr">
<td id="S5.T2.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T2.1.1.2.1.1" class="ltx_text ltx_font_sansserif">but heâ€™d <span id="S5.T2.1.1.2.1.1.1" class="ltx_text" style="color:#FF0000;">biâ€™n</span> too sick and too long <span id="S5.T2.1.1.2.1.1.2" class="ltx_text" style="color:#FF0000;">a bed</span></span></td>
<td id="S5.T2.1.1.2.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T2.1.1.2.2.1" class="ltx_text ltx_font_sansserif">but heâ€™d <span id="S5.T2.1.1.2.2.1.1" class="ltx_text" style="color:#0000FF;"> been</span> too sick and too long <span id="S5.T2.1.1.2.2.1.2" class="ltx_text" style="color:#FF0000;">a bed</span></span></td>
<td id="S5.T2.1.1.2.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T2.1.1.2.3.1" class="ltx_text ltx_font_sansserif">but heâ€™d <span id="S5.T2.1.1.2.3.1.1" class="ltx_text" style="color:#0000FF;"> been</span> too sick and too long <span id="S5.T2.1.1.2.3.1.2" class="ltx_text" style="color:#0000FF;"> abed</span></span></td>
<td id="S5.T2.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">word boundary error</td>
</tr>
<tr id="S5.T2.1.1.3" class="ltx_tr">
<td id="S5.T2.1.1.3.1" class="ltx_td ltx_align_left"><span id="S5.T2.1.1.3.1.1" class="ltx_text ltx_font_sansserif">humanity was <span id="S5.T2.1.1.3.1.1.1" class="ltx_text" style="color:#FF0000;">torred</span> in her country</span></td>
<td id="S5.T2.1.1.3.2" class="ltx_td ltx_align_left"><span id="S5.T2.1.1.3.2.1" class="ltx_text ltx_font_sansserif">humanity was <span id="S5.T2.1.1.3.2.1.1" class="ltx_text" style="color:#FF0000;">stored</span> in our country</span></td>
<td id="S5.T2.1.1.3.3" class="ltx_td ltx_align_left"><span id="S5.T2.1.1.3.3.1" class="ltx_text ltx_font_sansserif">humanity was <span id="S5.T2.1.1.3.3.1.1" class="ltx_text" style="color:#0000FF;">stirred</span> in our country</span></td>
<td id="S5.T2.1.1.3.4" class="ltx_td ltx_align_center">spelling mistake</td>
</tr>
<tr id="S5.T2.1.1.4" class="ltx_tr">
<td id="S5.T2.1.1.4.1" class="ltx_td ltx_align_left"><span id="S5.T2.1.1.4.1.1" class="ltx_text ltx_font_sansserif" style="color:#FF0000;">media<span id="S5.T2.1.1.4.1.1.1" class="ltx_text" style="color:#000000;"> too</span></span></td>
<td id="S5.T2.1.1.4.2" class="ltx_td ltx_align_left"><span id="S5.T2.1.1.4.2.1" class="ltx_text ltx_font_sansserif" style="color:#FF0000;">media<span id="S5.T2.1.1.4.2.1.1" class="ltx_text" style="color:#000000;"> too</span></span></td>
<td id="S5.T2.1.1.4.3" class="ltx_td ltx_align_left"><span id="S5.T2.1.1.4.3.1" class="ltx_text ltx_font_sansserif" style="color:#0000FF;">medea<span id="S5.T2.1.1.4.3.1.1" class="ltx_text" style="color:#000000;"> too</span></span></td>
<td id="S5.T2.1.1.4.4" class="ltx_td ltx_align_center">wrong word</td>
</tr>
<tr id="S5.T2.1.1.5" class="ltx_tr">
<td id="S5.T2.1.1.5.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T2.1.1.5.1.1" class="ltx_text ltx_font_sansserif">the fact is the castle <span id="S5.T2.1.1.5.1.1.1" class="ltx_text" style="color:#FF0000;"> was</span> much later than</span></td>
<td id="S5.T2.1.1.5.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T2.1.1.5.2.1" class="ltx_text ltx_font_sansserif">the fact is the castle <span id="S5.T2.1.1.5.2.1.1" class="ltx_text" style="color:#FF0000;"> was</span> much later than</span></td>
<td id="S5.T2.1.1.5.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T2.1.1.5.3.1" class="ltx_text ltx_font_sansserif">the fact is the castle <span id="S5.T2.1.1.5.3.1.1" class="ltx_text" style="color:#0000FF;"> is</span> much later than</span></td>
<td id="S5.T2.1.1.5.4" class="ltx_td ltx_align_center ltx_border_bb">grammatical error</td>
</tr>
</table>
</span></div>
</figure>
<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Our main results are in Table <a href="#S5.T1" title="Table 1 â€£ 5.1 Pushing the limits: DLM achieves new SOTA WER on LibriSpeech â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
We consider two ASR systems: one is the <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_italic">baseline ASR</span> (Transformer CTC-based model with 255M parameters) trained on LibriSpeech; the other is the <span id="S5.SS1.p1.1.2" class="ltx_text ltx_font_italic">baseline ASR (LS+TTS)</span> which is the same as <span id="S5.SS1.p1.1.3" class="ltx_text ltx_font_italic">baseline ASR</span> but trained on LibriSpeech together with TTS audio data generated by RichTTS from the LibriSpeech LM corpus. The mixing ratio of LibriSpeech audio and TTS audio in a minibatch is 1:1, which leads to improved performance compared to <span id="S5.SS1.p1.1.4" class="ltx_text ltx_font_italic">baseline ASR</span>. We show the other mixtures of LibriSpeech audio and TTS audio in a minibatch for <span id="S5.SS1.p1.1.5" class="ltx_text ltx_font_italic">baseline ASR (LS+TTS)</span> inÂ AppendixÂ <a href="#A4.SS2" title="D.2 ASR models â€£ Appendix D Training Details â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D.2</span></a> Table <a href="#A4.T8" title="Table 8 â€£ D.2 ASR models â€£ Appendix D Training Details â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">In Table <a href="#S5.T1" title="Table 1 â€£ 5.1 Pushing the limits: DLM achieves new SOTA WER on LibriSpeech â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, DLMs are trained on the mix of LibriSpeech audio and audio generated by YourTTS and RichTTS (total corpus is 2x) from LibriSpeech LM corpus. In both cases, DLM with greedy-decoding is comparable to neural LM with more expensive beam search and rescoring.
DLM with DSR-decoding significantly outperforms a similar size neural LM with rescoring. Note that DSR-decoding does not necessarily increase the computational cost comparing with standard rescoring.
In Table <a href="#S5.T2" title="Table 2 â€£ 5.1 Pushing the limits: DLM achieves new SOTA WER on LibriSpeech â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we show error analyses of hypotheses obtained from ASR, neural LM and DLM. In these cases, neural LM fails to choose the correct answer from the beam of ASR but DLM is able to correctly identify the error and generate the correct transcription.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Our best combination comes from applying DLM to <span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_italic">baseline ASR (LS+TTS)</span>, which results in new SOTA WERs on LibriSpeech: 1.5% on <span id="S5.SS1.p3.1.2" class="ltx_text ltx_font_italic">test-clean</span> and 3.3% on <span id="S5.SS1.p3.1.3" class="ltx_text ltx_font_italic">test-other</span>.
As shown in Table <a href="#S5.T3" title="Table 3 â€£ 5.1 Pushing the limits: DLM achieves new SOTA WER on LibriSpeech â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, our vanilla Transformer CTC-based ASR with DLM outperforms prior works with more sophisticated models and training criteria that do not use external audio data, and it even matches self-supervised models of the same size trained with additional 60k hours of unlabeled audio data.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>LibriSpeech test sets WER (%) for DLM and prior works that either do not use external audio data, or use self-supervised pretraining on Libri-LightÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> audio (LL-60k).</figcaption>
<div id="S5.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:259.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-17.6pt,11.7pt) scale(0.917347106000043,0.917347106000043) ;">
<table id="S5.T3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<td id="S5.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Prior work</td>
<td id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S5.T3.1.1.1.2.1" class="ltx_text"></span> <span id="S5.T3.1.1.1.2.2" class="ltx_text">
<span id="S5.T3.1.1.1.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.1.2.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.1.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">External Data</span></span>
</span></span><span id="S5.T3.1.1.1.2.3" class="ltx_text"></span></td>
<td id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">test-clean</td>
<td id="S5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">test-other</td>
</tr>
<tr id="S5.T3.1.1.2" class="ltx_tr">
<td id="S5.T3.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S5.T3.1.1.2.1.1" class="ltx_text"></span><span id="S5.T3.1.1.2.1.2" class="ltx_text">
<span id="S5.T3.1.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.2.1.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite></span></span>
</span></span><span id="S5.T3.1.1.2.1.3" class="ltx_text"></span></td>
<td id="S5.T3.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">2.3</td>
<td id="S5.T3.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">5.2</td>
</tr>
<tr id="S5.T3.1.1.3" class="ltx_tr">
<td id="S5.T3.1.1.3.1" class="ltx_td ltx_align_left">
<span id="S5.T3.1.1.3.1.1" class="ltx_text"></span><span id="S5.T3.1.1.3.1.2" class="ltx_text">
<span id="S5.T3.1.1.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.3.1.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.3.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Context-Net(L) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite></span></span>
</span></span><span id="S5.T3.1.1.3.1.3" class="ltx_text"></span></td>
<td id="S5.T3.1.1.3.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.3.3" class="ltx_td ltx_align_center">1.9</td>
<td id="S5.T3.1.1.3.4" class="ltx_td ltx_align_center">4.1</td>
</tr>
<tr id="S5.T3.1.1.4" class="ltx_tr">
<td id="S5.T3.1.1.4.1" class="ltx_td ltx_align_left">
<span id="S5.T3.1.1.4.1.1" class="ltx_text"></span><span id="S5.T3.1.1.4.1.2" class="ltx_text">
<span id="S5.T3.1.1.4.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.4.1.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.4.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Conformer (Transducer) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite></span></span>
</span></span><span id="S5.T3.1.1.4.1.3" class="ltx_text"></span></td>
<td id="S5.T3.1.1.4.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.4.3" class="ltx_td ltx_align_center">1.9</td>
<td id="S5.T3.1.1.4.4" class="ltx_td ltx_align_center">3.9</td>
</tr>
<tr id="S5.T3.1.1.5" class="ltx_tr">
<td id="S5.T3.1.1.5.1" class="ltx_td ltx_align_left">
<span id="S5.T3.1.1.5.1.1" class="ltx_text"></span><span id="S5.T3.1.1.5.1.2" class="ltx_text">
<span id="S5.T3.1.1.5.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.5.1.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.5.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">ASAPP-ASR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite></span></span>
</span></span><span id="S5.T3.1.1.5.1.3" class="ltx_text"></span></td>
<td id="S5.T3.1.1.5.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.5.3" class="ltx_td ltx_align_center">1.8</td>
<td id="S5.T3.1.1.5.4" class="ltx_td ltx_align_center">4.5</td>
</tr>
<tr id="S5.T3.1.1.6" class="ltx_tr">
<td id="S5.T3.1.1.6.1" class="ltx_td ltx_align_left">
<span id="S5.T3.1.1.6.1.1" class="ltx_text"></span><span id="S5.T3.1.1.6.1.2" class="ltx_text">
<span id="S5.T3.1.1.6.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.6.1.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.6.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">E-branchformer + ILME <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite></span></span>
</span></span><span id="S5.T3.1.1.6.1.3" class="ltx_text"></span></td>
<td id="S5.T3.1.1.6.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.6.3" class="ltx_td ltx_align_center">1.8</td>
<td id="S5.T3.1.1.6.4" class="ltx_td ltx_align_center">3.7</td>
</tr>
<tr id="S5.T3.1.1.7" class="ltx_tr">
<td id="S5.T3.1.1.7.1" class="ltx_td ltx_align_left">SYNT++Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</td>
<td id="S5.T3.1.1.7.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.7.3" class="ltx_td ltx_align_center">2.4</td>
<td id="S5.T3.1.1.7.4" class="ltx_td ltx_align_center">6.3</td>
</tr>
<tr id="S5.T3.1.1.8" class="ltx_tr">
<td id="S5.T3.1.1.8.1" class="ltx_td ltx_align_left">LAS + SC + LM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S5.T3.1.1.8.2" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.1.8.3" class="ltx_td ltx_align_center">4.3</td>
<td id="S5.T3.1.1.8.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T3.1.1.9" class="ltx_tr">
<td id="S5.T3.1.1.9.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S5.T3.1.1.9.1.1" class="ltx_text"></span><span id="S5.T3.1.1.9.1.2" class="ltx_text">
<span id="S5.T3.1.1.9.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.9.1.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.9.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">wav2vec 2.0-Large <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite></span></span>
</span></span><span id="S5.T3.1.1.9.1.3" class="ltx_text"></span></td>
<td id="S5.T3.1.1.9.2" class="ltx_td ltx_align_center ltx_border_t">LL-60k</td>
<td id="S5.T3.1.1.9.3" class="ltx_td ltx_align_center ltx_border_t">1.8</td>
<td id="S5.T3.1.1.9.4" class="ltx_td ltx_align_center ltx_border_t">3.3</td>
</tr>
<tr id="S5.T3.1.1.10" class="ltx_tr">
<td id="S5.T3.1.1.10.1" class="ltx_td ltx_align_left">
<span id="S5.T3.1.1.10.1.1" class="ltx_text"></span><span id="S5.T3.1.1.10.1.2" class="ltx_text">
<span id="S5.T3.1.1.10.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.10.1.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.10.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">HUBERT-Large <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite></span></span>
</span></span><span id="S5.T3.1.1.10.1.3" class="ltx_text"></span></td>
<td id="S5.T3.1.1.10.2" class="ltx_td ltx_align_center">LL-60k</td>
<td id="S5.T3.1.1.10.3" class="ltx_td ltx_align_center">1.9</td>
<td id="S5.T3.1.1.10.4" class="ltx_td ltx_align_center">3.3</td>
</tr>
<tr id="S5.T3.1.1.11" class="ltx_tr">
<td id="S5.T3.1.1.11.1" class="ltx_td ltx_align_left">HUBERT-XL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S5.T3.1.1.11.2" class="ltx_td ltx_align_center">LL-60k</td>
<td id="S5.T3.1.1.11.3" class="ltx_td ltx_align_center">1.8</td>
<td id="S5.T3.1.1.11.4" class="ltx_td ltx_align_center">2.9</td>
</tr>
<tr id="S5.T3.1.1.12" class="ltx_tr">
<td id="S5.T3.1.1.12.1" class="ltx_td ltx_align_left">
<span id="S5.T3.1.1.12.1.1" class="ltx_text"></span><span id="S5.T3.1.1.12.1.2" class="ltx_text">
<span id="S5.T3.1.1.12.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.12.1.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.12.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Conformer XXL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite></span></span>
</span></span><span id="S5.T3.1.1.12.1.3" class="ltx_text"></span></td>
<td id="S5.T3.1.1.12.2" class="ltx_td ltx_align_center">LL-60k</td>
<td id="S5.T3.1.1.12.3" class="ltx_td ltx_align_center">1.5</td>
<td id="S5.T3.1.1.12.4" class="ltx_td ltx_align_center">3.1</td>
</tr>
<tr id="S5.T3.1.1.13" class="ltx_tr">
<td id="S5.T3.1.1.13.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="S5.T3.1.1.13.1.1" class="ltx_text ltx_font_italic">baseline ASR (LS+TTS)</span> + DSR-decoding (Ours)</td>
<td id="S5.T3.1.1.13.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">-</td>
<td id="S5.T3.1.1.13.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T3.1.1.13.3.1" class="ltx_text ltx_font_bold">1.5</span></td>
<td id="S5.T3.1.1.13.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S5.T3.1.1.13.4.1" class="ltx_text ltx_font_bold">3.3</span></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>DLMs are universal</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">A single DLM has capability of correcting errors for different ASRs and different datasets while outperforming the neural LMs.
In TableÂ <a href="#S5.T4" title="Table 4 â€£ Scaling the DLM size â€£ 5.3 DLMs are scalable â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we show results of DLM trained to correct hypotheses from the <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">baseline ASR</span> on out-of-domain ASRs of different sizes and architectures: DLM not only improves over the ASR models but also consistently outperforms the same size neural LM.
Also, the same DLM can be applied to Whisper models to further correct hypothesis generated by their default beam-search decoding.
Note that Whisper models were trained on an entirely different and much larger proprietary data with word-piece tokens instead of characters; further Whisper is an encoder-decoder Transformer model with an architecture that is quite different from our CTC based <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_italic">baseline ASR</span>.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Furthermore, we show that the DLM trained on the LibriSpeech LM corpus also improves results of the <span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_italic">baseline ASR</span> system when evaluating on different data. We use the TED-LIUM datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, which is based on TeD talks, conversational in nature, and covers more contemporary topics. We observe around 16% WER improvement on dev and test set of TED-LIUM after apply DLM with DSR-decoding, which still outperforms neural LM (see TableÂ <a href="#A5.T9" title="Table 9 â€£ Appendix E Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> in AppendixÂ <a href="#A5" title="Appendix E Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a> for details). Note that TED-LIUM is different from the read speech of the LibriSpeech, which corresponds to Gutenberg books, and hence the gains here are expected to be small.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>DLMs are scalable</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We train different DLMs and apply them to the <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_italic">baseline ASR</span> to test the scalability of DLMs in three aspects: 1) scaling the number of speakers, 2) scaling the model size and 3) scaling the training dataset size.</p>
</div>
<section id="S5.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Scaling the DLM size</h4>

<div id="S5.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS3.SSS0.Px1.p1.1" class="ltx_p">TableÂ <a href="#S5.T5" title="Table 5 â€£ Scaling the DLM size â€£ 5.3 DLMs are scalable â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (left) compares DLMs and neural LMs of different size: i) while model size increases the WER decreases; ii) DLMs outperforms neural LMs of the same size.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>LibriSpeech dev and test sets WER (%) of different ASR models trained on LibriSpeech (960h) with greedy decoding, with neural LM (neLM, 500M) rescoring or with DLM (484M) DSR-greedy/DSR-decoding. Both neural LM and DLM are the same as reported in TableÂ <a href="#S5.T1" title="Table 1 â€£ 5.1 Pushing the limits: DLM achieves new SOTA WER on LibriSpeech â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</figcaption>
<div id="S5.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:260.2pt;height:215.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-22.2pt,18.4pt) scale(0.853963679358939,0.853963679358939) ;">
<table id="S5.T4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<td id="S5.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="S5.T4.1.1.1.1.1" class="ltx_text">Model</span></td>
<td id="S5.T4.1.1.1.2" class="ltx_td ltx_align_right ltx_border_tt" rowspan="2"><span id="S5.T4.1.1.1.2.1" class="ltx_text">Size</span></td>
<td id="S5.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">dev</td>
<td id="S5.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">test</td>
</tr>
<tr id="S5.T4.1.1.2" class="ltx_tr">
<td id="S5.T4.1.1.2.1" class="ltx_td ltx_align_right">clean</td>
<td id="S5.T4.1.1.2.2" class="ltx_td ltx_align_right">other</td>
<td id="S5.T4.1.1.2.3" class="ltx_td ltx_align_right">clean</td>
<td id="S5.T4.1.1.2.4" class="ltx_td ltx_align_right">other</td>
</tr>
<tr id="S5.T4.1.1.3" class="ltx_tr">
<td id="S5.T4.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">QuartzNet-CTC (Ours)</td>
<td id="S5.T4.1.1.3.2" class="ltx_td ltx_align_right ltx_border_t" rowspan="4"><span id="S5.T4.1.1.3.2.1" class="ltx_text">7M</span></td>
<td id="S5.T4.1.1.3.3" class="ltx_td ltx_align_right ltx_border_t">6.4</td>
<td id="S5.T4.1.1.3.4" class="ltx_td ltx_align_right ltx_border_t">17.0</td>
<td id="S5.T4.1.1.3.5" class="ltx_td ltx_align_right ltx_border_t">6.5</td>
<td id="S5.T4.1.1.3.6" class="ltx_td ltx_align_right ltx_border_t">16.9</td>
</tr>
<tr id="S5.T4.1.1.4" class="ltx_tr">
<td id="S5.T4.1.1.4.1" class="ltx_td ltx_align_left">+ neLM (LM-rescoring)</td>
<td id="S5.T4.1.1.4.2" class="ltx_td ltx_align_right">2.3</td>
<td id="S5.T4.1.1.4.3" class="ltx_td ltx_align_right">8.1</td>
<td id="S5.T4.1.1.4.4" class="ltx_td ltx_align_right">2.9</td>
<td id="S5.T4.1.1.4.5" class="ltx_td ltx_align_right">8.6</td>
</tr>
<tr id="S5.T4.1.1.5" class="ltx_tr">
<td id="S5.T4.1.1.5.1" class="ltx_td ltx_align_left">+ DLM (DSR-greedy)</td>
<td id="S5.T4.1.1.5.2" class="ltx_td ltx_align_right">2.7</td>
<td id="S5.T4.1.1.5.3" class="ltx_td ltx_align_right">8.1</td>
<td id="S5.T4.1.1.5.4" class="ltx_td ltx_align_right">2.8</td>
<td id="S5.T4.1.1.5.5" class="ltx_td ltx_align_right">8.3</td>
</tr>
<tr id="S5.T4.1.1.6" class="ltx_tr">
<td id="S5.T4.1.1.6.1" class="ltx_td ltx_align_left">+ DLM (DSR-decoding)</td>
<td id="S5.T4.1.1.6.2" class="ltx_td ltx_align_right"><span id="S5.T4.1.1.6.2.1" class="ltx_text ltx_font_bold">2.1</span></td>
<td id="S5.T4.1.1.6.3" class="ltx_td ltx_align_right"><span id="S5.T4.1.1.6.3.1" class="ltx_text ltx_font_bold">6.9</span></td>
<td id="S5.T4.1.1.6.4" class="ltx_td ltx_align_right"><span id="S5.T4.1.1.6.4.1" class="ltx_text ltx_font_bold">2.3</span></td>
<td id="S5.T4.1.1.6.5" class="ltx_td ltx_align_right"><span id="S5.T4.1.1.6.5.1" class="ltx_text ltx_font_bold">7.1</span></td>
</tr>
<tr id="S5.T4.1.1.7" class="ltx_tr">
<td id="S5.T4.1.1.7.1" class="ltx_td ltx_align_left ltx_border_t">Conformer-CTC (Ours)</td>
<td id="S5.T4.1.1.7.2" class="ltx_td ltx_align_right ltx_border_t" rowspan="4"><span id="S5.T4.1.1.7.2.1" class="ltx_text">102M</span></td>
<td id="S5.T4.1.1.7.3" class="ltx_td ltx_align_right ltx_border_t">2.4</td>
<td id="S5.T4.1.1.7.4" class="ltx_td ltx_align_right ltx_border_t">5.7</td>
<td id="S5.T4.1.1.7.5" class="ltx_td ltx_align_right ltx_border_t">2.6</td>
<td id="S5.T4.1.1.7.6" class="ltx_td ltx_align_right ltx_border_t">5.6</td>
</tr>
<tr id="S5.T4.1.1.8" class="ltx_tr">
<td id="S5.T4.1.1.8.1" class="ltx_td ltx_align_left">+ neLM (LM-rescoring)</td>
<td id="S5.T4.1.1.8.2" class="ltx_td ltx_align_right">1.6</td>
<td id="S5.T4.1.1.8.3" class="ltx_td ltx_align_right">4.0</td>
<td id="S5.T4.1.1.8.4" class="ltx_td ltx_align_right">2.2</td>
<td id="S5.T4.1.1.8.5" class="ltx_td ltx_align_right">4.2</td>
</tr>
<tr id="S5.T4.1.1.9" class="ltx_tr">
<td id="S5.T4.1.1.9.1" class="ltx_td ltx_align_left">+ DLM (DSR-greedy)</td>
<td id="S5.T4.1.1.9.2" class="ltx_td ltx_align_right">1.9</td>
<td id="S5.T4.1.1.9.3" class="ltx_td ltx_align_right">4.0</td>
<td id="S5.T4.1.1.9.4" class="ltx_td ltx_align_right">2.2</td>
<td id="S5.T4.1.1.9.5" class="ltx_td ltx_align_right">4.1</td>
</tr>
<tr id="S5.T4.1.1.10" class="ltx_tr">
<td id="S5.T4.1.1.10.1" class="ltx_td ltx_align_left">+ DLM (DSR-decoding)</td>
<td id="S5.T4.1.1.10.2" class="ltx_td ltx_align_right"><span id="S5.T4.1.1.10.2.1" class="ltx_text ltx_font_bold">1.5</span></td>
<td id="S5.T4.1.1.10.3" class="ltx_td ltx_align_right"><span id="S5.T4.1.1.10.3.1" class="ltx_text ltx_font_bold">3.4</span></td>
<td id="S5.T4.1.1.10.4" class="ltx_td ltx_align_right"><span id="S5.T4.1.1.10.4.1" class="ltx_text ltx_font_bold">1.7</span></td>
<td id="S5.T4.1.1.10.5" class="ltx_td ltx_align_right"><span id="S5.T4.1.1.10.5.1" class="ltx_text ltx_font_bold">3.6</span></td>
</tr>
<tr id="S5.T4.1.1.11" class="ltx_tr">
<td id="S5.T4.1.1.11.1" class="ltx_td ltx_align_left ltx_border_tt">Whisper-base</td>
<td id="S5.T4.1.1.11.2" class="ltx_td ltx_align_right ltx_border_tt" rowspan="2"><span id="S5.T4.1.1.11.2.1" class="ltx_text">74M</span></td>
<td id="S5.T4.1.1.11.3" class="ltx_td ltx_align_right ltx_border_tt">4.5</td>
<td id="S5.T4.1.1.11.4" class="ltx_td ltx_align_right ltx_border_tt">10.3</td>
<td id="S5.T4.1.1.11.5" class="ltx_td ltx_align_right ltx_border_tt">4.5</td>
<td id="S5.T4.1.1.11.6" class="ltx_td ltx_align_right ltx_border_tt">10.9</td>
</tr>
<tr id="S5.T4.1.1.12" class="ltx_tr">
<td id="S5.T4.1.1.12.1" class="ltx_td ltx_align_left">+ DLM (DSR-greedy)</td>
<td id="S5.T4.1.1.12.2" class="ltx_td ltx_align_right">2.9</td>
<td id="S5.T4.1.1.12.3" class="ltx_td ltx_align_right">7.1</td>
<td id="S5.T4.1.1.12.4" class="ltx_td ltx_align_right">3.1</td>
<td id="S5.T4.1.1.12.5" class="ltx_td ltx_align_right">7.9</td>
</tr>
<tr id="S5.T4.1.1.13" class="ltx_tr">
<td id="S5.T4.1.1.13.1" class="ltx_td ltx_align_left ltx_border_t">Whisper-small</td>
<td id="S5.T4.1.1.13.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" rowspan="2"><span id="S5.T4.1.1.13.2.1" class="ltx_text">244M</span></td>
<td id="S5.T4.1.1.13.3" class="ltx_td ltx_align_right ltx_border_t">3.6</td>
<td id="S5.T4.1.1.13.4" class="ltx_td ltx_align_right ltx_border_t">7.1</td>
<td id="S5.T4.1.1.13.5" class="ltx_td ltx_align_right ltx_border_t">3.3</td>
<td id="S5.T4.1.1.13.6" class="ltx_td ltx_align_right ltx_border_t">7.6</td>
</tr>
<tr id="S5.T4.1.1.14" class="ltx_tr">
<td id="S5.T4.1.1.14.1" class="ltx_td ltx_align_left ltx_border_bb">+ DLM (DSR-greedy)</td>
<td id="S5.T4.1.1.14.2" class="ltx_td ltx_align_right ltx_border_bb">2.8</td>
<td id="S5.T4.1.1.14.3" class="ltx_td ltx_align_right ltx_border_bb">5.4</td>
<td id="S5.T4.1.1.14.4" class="ltx_td ltx_align_right ltx_border_bb">2.7</td>
<td id="S5.T4.1.1.14.5" class="ltx_td ltx_align_right ltx_border_bb">6.1</td>
</tr>
</table>
</span></div>
</figure>
<figure id="S5.T5" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>LibriSpeech WER (%) for <span id="S5.T5.2.1" class="ltx_text ltx_font_italic">baseline ASR</span> with (left) DLMs and neural LMs of different size with 800M words of text; (right) DLMs trained on different data size with 484M params. DSR-greedy (DSR-decoding) is reported for DLMs and rescoring for neural LMs. (left) YourTTS synthetic data, 1:9 real:synthetic data proportion, frequency masking and 10% random substitutions of characters are used; (right) RichTTS with 10% random substitutions of characters are used.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S5.T5.3" class="ltx_inline-block ltx_figure_panel ltx_minipage ltx_align_middle ltx_transformed_outer" style="width:216.8pt;height:174.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(65.9pt,-33.1pt) scale(1.61347321905915,1.61347321905915) ;">
<table id="S5.T5.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T5.3.1.1" class="ltx_tr">
<td id="S5.T5.3.1.1.1" class="ltx_td ltx_align_right ltx_border_tt" rowspan="2"><span id="S5.T5.3.1.1.1.1" class="ltx_text">Size</span></td>
<td id="S5.T5.3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">dev-clean</td>
<td id="S5.T5.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">dev-other</td>
</tr>
<tr id="S5.T5.3.1.2" class="ltx_tr">
<td id="S5.T5.3.1.2.1" class="ltx_td ltx_align_right">DLM</td>
<td id="S5.T5.3.1.2.2" class="ltx_td ltx_align_right">neLM</td>
<td id="S5.T5.3.1.2.3" class="ltx_td ltx_align_right">DLM</td>
<td id="S5.T5.3.1.2.4" class="ltx_td ltx_align_right">neLM</td>
</tr>
<tr id="S5.T5.3.1.3" class="ltx_tr">
<td id="S5.T5.3.1.3.1" class="ltx_td ltx_align_right ltx_border_t">69M</td>
<td id="S5.T5.3.1.3.2" class="ltx_td ltx_align_right ltx_border_t">2.1 (1.7)</td>
<td id="S5.T5.3.1.3.3" class="ltx_td ltx_align_right ltx_border_t">1.8</td>
<td id="S5.T5.3.1.3.4" class="ltx_td ltx_align_right ltx_border_t">4.8 (4.2)</td>
<td id="S5.T5.3.1.3.5" class="ltx_td ltx_align_right ltx_border_t">4.6</td>
</tr>
<tr id="S5.T5.3.1.4" class="ltx_tr">
<td id="S5.T5.3.1.4.1" class="ltx_td ltx_align_right">155M</td>
<td id="S5.T5.3.1.4.2" class="ltx_td ltx_align_right">2.1 (1.6)</td>
<td id="S5.T5.3.1.4.3" class="ltx_td ltx_align_right">1.6</td>
<td id="S5.T5.3.1.4.4" class="ltx_td ltx_align_right">4.5 (4.0)</td>
<td id="S5.T5.3.1.4.5" class="ltx_td ltx_align_right">4.1</td>
</tr>
<tr id="S5.T5.3.1.5" class="ltx_tr">
<td id="S5.T5.3.1.5.1" class="ltx_td ltx_align_right">484M</td>
<td id="S5.T5.3.1.5.2" class="ltx_td ltx_align_right">2.0 (1.5)</td>
<td id="S5.T5.3.1.5.3" class="ltx_td ltx_align_right">1.6</td>
<td id="S5.T5.3.1.5.4" class="ltx_td ltx_align_right">4.3 (3.8)</td>
<td id="S5.T5.3.1.5.5" class="ltx_td ltx_align_right">4.0</td>
</tr>
<tr id="S5.T5.3.1.6" class="ltx_tr">
<td id="S5.T5.3.1.6.1" class="ltx_td ltx_align_right ltx_border_bb">1B</td>
<td id="S5.T5.3.1.6.2" class="ltx_td ltx_align_right ltx_border_bb">2.0 (1.5)</td>
<td id="S5.T5.3.1.6.3" class="ltx_td ltx_align_right ltx_border_bb">-</td>
<td id="S5.T5.3.1.6.4" class="ltx_td ltx_align_right ltx_border_bb">4.3 (3.7)</td>
<td id="S5.T5.3.1.6.5" class="ltx_td ltx_align_right ltx_border_bb">-</td>
</tr>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S5.T5.4" class="ltx_inline-block ltx_figure_panel ltx_minipage ltx_align_middle ltx_transformed_outer" style="width:195.1pt;height:124.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(64.3pt,-26.5pt) scale(1.73529580075925,1.73529580075925) ;">
<table id="S5.T5.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T5.4.1.1" class="ltx_tr">
<td id="S5.T5.4.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"># of words</td>
<td id="S5.T5.4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">dev-clean</td>
<td id="S5.T5.4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">dev-other</td>
</tr>
<tr id="S5.T5.4.1.2" class="ltx_tr">
<td id="S5.T5.4.1.2.1" class="ltx_td ltx_align_left ltx_border_t">0.5x (400M)</td>
<td id="S5.T5.4.1.2.2" class="ltx_td ltx_align_center ltx_border_t">2.6 (1.8)</td>
<td id="S5.T5.4.1.2.3" class="ltx_td ltx_align_center ltx_border_t">4.7 (3.8)</td>
</tr>
<tr id="S5.T5.4.1.3" class="ltx_tr">
<td id="S5.T5.4.1.3.1" class="ltx_td ltx_align_left">1x (800M)</td>
<td id="S5.T5.4.1.3.2" class="ltx_td ltx_align_center">2.2 (1.6)</td>
<td id="S5.T5.4.1.3.3" class="ltx_td ltx_align_center">4.3 (3.6)</td>
</tr>
<tr id="S5.T5.4.1.4" class="ltx_tr">
<td id="S5.T5.4.1.4.1" class="ltx_td ltx_align_left ltx_border_bb">2x (1.6B)</td>
<td id="S5.T5.4.1.4.2" class="ltx_td ltx_align_center ltx_border_bb">2.2 (1.6)</td>
<td id="S5.T5.4.1.4.3" class="ltx_td ltx_align_center ltx_border_bb">4.2 (3.5)</td>
</tr>
</table>
</span></div>
</div>
</div>
</figure>
<figure id="S5.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>LibriSpeech DSR-greedy (DSR-decoding) WER (%) for DLMs (155M) trained on RichTTS synthetic data (400M words) with 10% random substitutions and different # of speakers.</figcaption>
<div id="S5.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:45.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-36.9pt,4.3pt) scale(0.841066023115248,0.841066023115248) ;">
<table id="S5.T6.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.1.1" class="ltx_tr">
<td id="S5.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"># of speakers</td>
<td id="S5.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">1</td>
<td id="S5.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">10</td>
<td id="S5.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">100</td>
<td id="S5.T6.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">1000</td>
<td id="S5.T6.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">2000</td>
<td id="S5.T6.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt">1 (w/ 100 speaker embeddings)</td>
</tr>
<tr id="S5.T6.1.1.2" class="ltx_tr">
<td id="S5.T6.1.1.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">dev-clean</td>
<td id="S5.T6.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">3.2 (2.0)</td>
<td id="S5.T6.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">2.6 (1.8)</td>
<td id="S5.T6.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">2.6 (1.8)</td>
<td id="S5.T6.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">2.5 (1.8)</td>
<td id="S5.T6.1.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2.6 (1.8)</td>
<td id="S5.T6.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t">2.9 (1.9)</td>
</tr>
<tr id="S5.T6.1.1.3" class="ltx_tr">
<td id="S5.T6.1.1.3.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r">dev-other</td>
<td id="S5.T6.1.1.3.2" class="ltx_td ltx_align_center ltx_border_bb">5.3 (4.1)</td>
<td id="S5.T6.1.1.3.3" class="ltx_td ltx_align_center ltx_border_bb">4.9 (4.0)</td>
<td id="S5.T6.1.1.3.4" class="ltx_td ltx_align_center ltx_border_bb">4.7 (4.1)</td>
<td id="S5.T6.1.1.3.5" class="ltx_td ltx_align_center ltx_border_bb">4.7 (4.1)</td>
<td id="S5.T6.1.1.3.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">4.7 (4.0)</td>
<td id="S5.T6.1.1.3.7" class="ltx_td ltx_align_center ltx_border_bb">5.1 (4.1)</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Scaling text corpus size</h4>

<div id="S5.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS3.SSS0.Px2.p1.1" class="ltx_p">To vary the text corpus used to generate the synthetic audio for DLM training, i) we randomly selected 20M sentences from the LibriSpeech LM corpus and formed 0.5x corpus (400M words); ii) we added in 40M sentences randomly selected from the Gutenberg datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> and formed the 2x corpus (1.6B words).
TableÂ <a href="#S5.T5" title="Table 5 â€£ Scaling the DLM size â€£ 5.3 DLMs are scalable â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (right) shows that DLMs benefits from more trainig data.
We expect to see further gains from scaling to larger corpora in future work.</p>
</div>
</section>
<section id="S5.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Scaling numbers of speakers</h4>

<div id="S5.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS3.SSS0.Px3.p1.1" class="ltx_p">To see the impact of the number of speakers used in audio generation, we obtained 20M unique sentences from 1, 10, 100, 1000, and 2000 speakers to form five training sets.
We train five different DLMs (155M) on these data: using more speakers leads to lower WER, probably due to larger variability in types of errors generated, see TableÂ <a href="#S5.T6" title="Table 6 â€£ Scaling the DLM size â€£ 5.3 DLMs are scalable â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
100 speakers seems to be enough for Librispeech dev sets, but we keep using all speakers from LibriSpeech for the other experiments.
Even if we use only one speaker, we can improve results by using a random example of their utterances for style-conditioning (see the last column in TableÂ <a href="#S5.T6" title="Table 6 â€£ Scaling the DLM size â€£ 5.3 DLMs are scalable â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). This indicates that the variance from the same speakerâ€™s different embeddings can also provide a certain amount of noise, which is beneficial for the DLM to learn and correct.</p>
</div>
</section>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Ablation: design proper error correction data distribution</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.2" class="ltx_p">In this section, we show the data construction path to our final results highlighting the importance to generate a variety of relevant error typesÂ (see TableÂ <a href="#S5.T7" title="Table 7 â€£ 5.4 Ablation: design proper error correction data distribution â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, left).
We first generate a dataset of 40M <math id="S5.SS4.p1.1.m1.3" class="ltx_Math" alttext="\{(\hat{y},y)\}" display="inline"><semantics id="S5.SS4.p1.1.m1.3a"><mrow id="S5.SS4.p1.1.m1.3.3.1" xref="S5.SS4.p1.1.m1.3.3.2.cmml"><mo stretchy="false" id="S5.SS4.p1.1.m1.3.3.1.2" xref="S5.SS4.p1.1.m1.3.3.2.cmml">{</mo><mrow id="S5.SS4.p1.1.m1.3.3.1.1.2" xref="S5.SS4.p1.1.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="S5.SS4.p1.1.m1.3.3.1.1.2.1" xref="S5.SS4.p1.1.m1.3.3.1.1.1.cmml">(</mo><mover accent="true" id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml"><mi id="S5.SS4.p1.1.m1.1.1.2" xref="S5.SS4.p1.1.m1.1.1.2.cmml">y</mi><mo id="S5.SS4.p1.1.m1.1.1.1" xref="S5.SS4.p1.1.m1.1.1.1.cmml">^</mo></mover><mo id="S5.SS4.p1.1.m1.3.3.1.1.2.2" xref="S5.SS4.p1.1.m1.3.3.1.1.1.cmml">,</mo><mi id="S5.SS4.p1.1.m1.2.2" xref="S5.SS4.p1.1.m1.2.2.cmml">y</mi><mo stretchy="false" id="S5.SS4.p1.1.m1.3.3.1.1.2.3" xref="S5.SS4.p1.1.m1.3.3.1.1.1.cmml">)</mo></mrow><mo stretchy="false" id="S5.SS4.p1.1.m1.3.3.1.3" xref="S5.SS4.p1.1.m1.3.3.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.3b"><set id="S5.SS4.p1.1.m1.3.3.2.cmml" xref="S5.SS4.p1.1.m1.3.3.1"><interval closure="open" id="S5.SS4.p1.1.m1.3.3.1.1.1.cmml" xref="S5.SS4.p1.1.m1.3.3.1.1.2"><apply id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1"><ci id="S5.SS4.p1.1.m1.1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1.1">^</ci><ci id="S5.SS4.p1.1.m1.1.1.2.cmml" xref="S5.SS4.p1.1.m1.1.1.2">ğ‘¦</ci></apply><ci id="S5.SS4.p1.1.m1.2.2.cmml" xref="S5.SS4.p1.1.m1.2.2">ğ‘¦</ci></interval></set></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.3c">\{(\hat{y},y)\}</annotation></semantics></math> pairs using YourTTS.
Training on these data improves WER of ASR on <span id="S5.SS4.p1.2.1" class="ltx_text ltx_font_italic">dev-other</span>, with slight degradation on <span id="S5.SS4.p1.2.2" class="ltx_text ltx_font_italic">dev-clean</span> (row 2). We hypothesise that training data are still relatively clean and there are not many corrections for the model to learn. We experiment with adding different types of noise carefully such that the noise distribution does not shift the training distribution away from the test time distribution too significantly.
First, we found that randomly substituting some percentage of the characters in the input transcript is beneficial (row 4) as long as it is not too high (<math id="S5.SS4.p1.2.m2.1" class="ltx_Math" alttext="s=10\%" display="inline"><semantics id="S5.SS4.p1.2.m2.1a"><mrow id="S5.SS4.p1.2.m2.1.1" xref="S5.SS4.p1.2.m2.1.1.cmml"><mi id="S5.SS4.p1.2.m2.1.1.2" xref="S5.SS4.p1.2.m2.1.1.2.cmml">s</mi><mo id="S5.SS4.p1.2.m2.1.1.1" xref="S5.SS4.p1.2.m2.1.1.1.cmml">=</mo><mrow id="S5.SS4.p1.2.m2.1.1.3" xref="S5.SS4.p1.2.m2.1.1.3.cmml"><mn id="S5.SS4.p1.2.m2.1.1.3.2" xref="S5.SS4.p1.2.m2.1.1.3.2.cmml">10</mn><mo id="S5.SS4.p1.2.m2.1.1.3.1" xref="S5.SS4.p1.2.m2.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.2.m2.1b"><apply id="S5.SS4.p1.2.m2.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1"><eq id="S5.SS4.p1.2.m2.1.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1.1"></eq><ci id="S5.SS4.p1.2.m2.1.1.2.cmml" xref="S5.SS4.p1.2.m2.1.1.2">ğ‘ </ci><apply id="S5.SS4.p1.2.m2.1.1.3.cmml" xref="S5.SS4.p1.2.m2.1.1.3"><csymbol cd="latexml" id="S5.SS4.p1.2.m2.1.1.3.1.cmml" xref="S5.SS4.p1.2.m2.1.1.3.1">percent</csymbol><cn type="integer" id="S5.SS4.p1.2.m2.1.1.3.2.cmml" xref="S5.SS4.p1.2.m2.1.1.3.2">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.2.m2.1c">s=10\%</annotation></semantics></math> works quite well).
Second, generating hypotheses from noisy audio, where the noise comes from masking frequency bands of the spectrogram input to the ASR model (2 masks with max width 30), in a manner similar to SpecAugmentÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, also helps (row 5).
To ground the data distribution of the DLM more closely to real data, we added a small portion (10%) of noisy hypotheses by running the ASR system on the LibriSpeech 960h data set.
This gave us further gains on both <span id="S5.SS4.p1.2.3" class="ltx_text ltx_font_italic">dev-clean</span> and <span id="S5.SS4.p1.2.4" class="ltx_text ltx_font_italic">dev-other</span> sets (row 7).</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">We then repeated the audio generation process with RichTTS trained with LibriSpeech, and got a new set of data.
We found that combining the data from two TTS systems led to better WER compared to using only one TTS system (row 6).
Lastly, we mixed all the data construction tricks together, which produced the best results (row 8).
Although not reported, we found that generating multiple synthetic audio samples using the same TTS system from the same sentence did not improve results, underscoring the importance of generating stylistic variations of noise for better error correction.</p>
</div>
<figure id="S5.T7" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>LibriSpeech WER (%) for DLMs (484M) trained on different data reported with DSR-greedy (DSR-decoding): <math id="S5.T7.2.m1.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S5.T7.2.m1.1b"><mi id="S5.T7.2.m1.1.1" xref="S5.T7.2.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S5.T7.2.m1.1c"><ci id="S5.T7.2.m1.1.1.cmml" xref="S5.T7.2.m1.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.2.m1.1d">s</annotation></semantics></math> is a random character substitution rate; â€œFMâ€ is a frequency masking; â€œrealâ€ refers to hypotheses from a real audio.</figcaption>
<div id="S5.T7.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:144.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.0pt,18.0pt) scale(0.8003028440235,0.8003028440235) ;">
<table id="S5.T7.5.3" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.5.3.4" class="ltx_tr">
<td id="S5.T7.5.3.4.1" class="ltx_td ltx_align_left ltx_border_tt">Model</td>
<td id="S5.T7.5.3.4.2" class="ltx_td ltx_align_left ltx_border_tt">dev-clean</td>
<td id="S5.T7.5.3.4.3" class="ltx_td ltx_align_left ltx_border_tt">dev-other</td>
</tr>
<tr id="S5.T7.5.3.5" class="ltx_tr">
<td id="S5.T7.5.3.5.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T7.5.3.5.1.1" class="ltx_text ltx_font_italic">baseline ASR</span></td>
<td id="S5.T7.5.3.5.2" class="ltx_td ltx_align_left ltx_border_t">2.1</td>
<td id="S5.T7.5.3.5.3" class="ltx_td ltx_align_left ltx_border_t">5.5</td>
</tr>
<tr id="S5.T7.3.1.1" class="ltx_tr">
<td id="S5.T7.3.1.1.1" class="ltx_td ltx_align_left ltx_border_t">+ DLM, Tacotron + <math id="S5.T7.3.1.1.1.m1.1" class="ltx_Math" alttext="s=10\%" display="inline"><semantics id="S5.T7.3.1.1.1.m1.1a"><mrow id="S5.T7.3.1.1.1.m1.1.1" xref="S5.T7.3.1.1.1.m1.1.1.cmml"><mi id="S5.T7.3.1.1.1.m1.1.1.2" xref="S5.T7.3.1.1.1.m1.1.1.2.cmml">s</mi><mo id="S5.T7.3.1.1.1.m1.1.1.1" xref="S5.T7.3.1.1.1.m1.1.1.1.cmml">=</mo><mrow id="S5.T7.3.1.1.1.m1.1.1.3" xref="S5.T7.3.1.1.1.m1.1.1.3.cmml"><mn id="S5.T7.3.1.1.1.m1.1.1.3.2" xref="S5.T7.3.1.1.1.m1.1.1.3.2.cmml">10</mn><mo id="S5.T7.3.1.1.1.m1.1.1.3.1" xref="S5.T7.3.1.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T7.3.1.1.1.m1.1b"><apply id="S5.T7.3.1.1.1.m1.1.1.cmml" xref="S5.T7.3.1.1.1.m1.1.1"><eq id="S5.T7.3.1.1.1.m1.1.1.1.cmml" xref="S5.T7.3.1.1.1.m1.1.1.1"></eq><ci id="S5.T7.3.1.1.1.m1.1.1.2.cmml" xref="S5.T7.3.1.1.1.m1.1.1.2">ğ‘ </ci><apply id="S5.T7.3.1.1.1.m1.1.1.3.cmml" xref="S5.T7.3.1.1.1.m1.1.1.3"><csymbol cd="latexml" id="S5.T7.3.1.1.1.m1.1.1.3.1.cmml" xref="S5.T7.3.1.1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S5.T7.3.1.1.1.m1.1.1.3.2.cmml" xref="S5.T7.3.1.1.1.m1.1.1.3.2">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.3.1.1.1.m1.1c">s=10\%</annotation></semantics></math>
</td>
<td id="S5.T7.3.1.1.2" class="ltx_td ltx_align_left ltx_border_t">4.1 (2.0)</td>
<td id="S5.T7.3.1.1.3" class="ltx_td ltx_align_left ltx_border_t">6.6 (4.8)</td>
</tr>
<tr id="S5.T7.5.3.6" class="ltx_tr">
<td id="S5.T7.5.3.6.1" class="ltx_td ltx_align_left ltx_border_t">+ DLM (YourTTS)</td>
<td id="S5.T7.5.3.6.2" class="ltx_td ltx_align_left ltx_border_t">2.3 (1.6)</td>
<td id="S5.T7.5.3.6.3" class="ltx_td ltx_align_left ltx_border_t">4.9 (4.0)</td>
</tr>
<tr id="S5.T7.4.2.2" class="ltx_tr">
<td id="S5.T7.4.2.2.1" class="ltx_td ltx_align_left">Â Â Â Â Â  + <math id="S5.T7.4.2.2.1.m1.1" class="ltx_Math" alttext="s=10\%" display="inline"><semantics id="S5.T7.4.2.2.1.m1.1a"><mrow id="S5.T7.4.2.2.1.m1.1.1" xref="S5.T7.4.2.2.1.m1.1.1.cmml"><mi id="S5.T7.4.2.2.1.m1.1.1.2" xref="S5.T7.4.2.2.1.m1.1.1.2.cmml">s</mi><mo id="S5.T7.4.2.2.1.m1.1.1.1" xref="S5.T7.4.2.2.1.m1.1.1.1.cmml">=</mo><mrow id="S5.T7.4.2.2.1.m1.1.1.3" xref="S5.T7.4.2.2.1.m1.1.1.3.cmml"><mn id="S5.T7.4.2.2.1.m1.1.1.3.2" xref="S5.T7.4.2.2.1.m1.1.1.3.2.cmml">10</mn><mo id="S5.T7.4.2.2.1.m1.1.1.3.1" xref="S5.T7.4.2.2.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T7.4.2.2.1.m1.1b"><apply id="S5.T7.4.2.2.1.m1.1.1.cmml" xref="S5.T7.4.2.2.1.m1.1.1"><eq id="S5.T7.4.2.2.1.m1.1.1.1.cmml" xref="S5.T7.4.2.2.1.m1.1.1.1"></eq><ci id="S5.T7.4.2.2.1.m1.1.1.2.cmml" xref="S5.T7.4.2.2.1.m1.1.1.2">ğ‘ </ci><apply id="S5.T7.4.2.2.1.m1.1.1.3.cmml" xref="S5.T7.4.2.2.1.m1.1.1.3"><csymbol cd="latexml" id="S5.T7.4.2.2.1.m1.1.1.3.1.cmml" xref="S5.T7.4.2.2.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S5.T7.4.2.2.1.m1.1.1.3.2.cmml" xref="S5.T7.4.2.2.1.m1.1.1.3.2">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.4.2.2.1.m1.1c">s=10\%</annotation></semantics></math>
</td>
<td id="S5.T7.4.2.2.2" class="ltx_td ltx_align_left">2.2 (1.5)</td>
<td id="S5.T7.4.2.2.3" class="ltx_td ltx_align_left">4.6 (3.8)</td>
</tr>
<tr id="S5.T7.5.3.7" class="ltx_tr">
<td id="S5.T7.5.3.7.1" class="ltx_td ltx_align_left">Â Â Â Â Â Â  + FM</td>
<td id="S5.T7.5.3.7.2" class="ltx_td ltx_align_left">2.3 (1.6)</td>
<td id="S5.T7.5.3.7.3" class="ltx_td ltx_align_left">4.6 (3.7)</td>
</tr>
<tr id="S5.T7.5.3.8" class="ltx_tr">
<td id="S5.T7.5.3.8.1" class="ltx_td ltx_align_left">Â Â Â Â Â Â  + RichTTS</td>
<td id="S5.T7.5.3.8.2" class="ltx_td ltx_align_left">2.1 (1.5)</td>
<td id="S5.T7.5.3.8.3" class="ltx_td ltx_align_left">4.2 (3.7)</td>
</tr>
<tr id="S5.T7.5.3.9" class="ltx_tr">
<td id="S5.T7.5.3.9.1" class="ltx_td ltx_align_left">Â Â Â Â Â Â  + FM + real</td>
<td id="S5.T7.5.3.9.2" class="ltx_td ltx_align_left">2.0 (1.5)</td>
<td id="S5.T7.5.3.9.3" class="ltx_td ltx_align_left">4.3 (3.8)</td>
</tr>
<tr id="S5.T7.5.3.10" class="ltx_tr">
<td id="S5.T7.5.3.10.1" class="ltx_td ltx_align_left">Â Â Â Â Â Â  + RichTTS + FM + real</td>
<td id="S5.T7.5.3.10.2" class="ltx_td ltx_align_left">1.9 (1.5)</td>
<td id="S5.T7.5.3.10.3" class="ltx_td ltx_align_left">3.9 (3.4)</td>
</tr>
<tr id="S5.T7.5.3.3" class="ltx_tr">
<td id="S5.T7.5.3.3.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">+ DLM, RichTTS + <math id="S5.T7.5.3.3.1.m1.1" class="ltx_Math" alttext="s=10\%" display="inline"><semantics id="S5.T7.5.3.3.1.m1.1a"><mrow id="S5.T7.5.3.3.1.m1.1.1" xref="S5.T7.5.3.3.1.m1.1.1.cmml"><mi id="S5.T7.5.3.3.1.m1.1.1.2" xref="S5.T7.5.3.3.1.m1.1.1.2.cmml">s</mi><mo id="S5.T7.5.3.3.1.m1.1.1.1" xref="S5.T7.5.3.3.1.m1.1.1.1.cmml">=</mo><mrow id="S5.T7.5.3.3.1.m1.1.1.3" xref="S5.T7.5.3.3.1.m1.1.1.3.cmml"><mn id="S5.T7.5.3.3.1.m1.1.1.3.2" xref="S5.T7.5.3.3.1.m1.1.1.3.2.cmml">10</mn><mo id="S5.T7.5.3.3.1.m1.1.1.3.1" xref="S5.T7.5.3.3.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T7.5.3.3.1.m1.1b"><apply id="S5.T7.5.3.3.1.m1.1.1.cmml" xref="S5.T7.5.3.3.1.m1.1.1"><eq id="S5.T7.5.3.3.1.m1.1.1.1.cmml" xref="S5.T7.5.3.3.1.m1.1.1.1"></eq><ci id="S5.T7.5.3.3.1.m1.1.1.2.cmml" xref="S5.T7.5.3.3.1.m1.1.1.2">ğ‘ </ci><apply id="S5.T7.5.3.3.1.m1.1.1.3.cmml" xref="S5.T7.5.3.3.1.m1.1.1.3"><csymbol cd="latexml" id="S5.T7.5.3.3.1.m1.1.1.3.1.cmml" xref="S5.T7.5.3.3.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S5.T7.5.3.3.1.m1.1.1.3.2.cmml" xref="S5.T7.5.3.3.1.m1.1.1.3.2">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.5.3.3.1.m1.1c">s=10\%</annotation></semantics></math>
</td>
<td id="S5.T7.5.3.3.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">2.2 (1.6)</td>
<td id="S5.T7.5.3.3.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">4.3 (3.6)</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>TTS audio is as good as real audio for DLMs</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.4" class="ltx_p">We explore whether there is a gap between the performance of DLMs trained on <math id="S5.SS5.p1.1.m1.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="S5.SS5.p1.1.m1.1a"><mover accent="true" id="S5.SS5.p1.1.m1.1.1" xref="S5.SS5.p1.1.m1.1.1.cmml"><mi id="S5.SS5.p1.1.m1.1.1.2" xref="S5.SS5.p1.1.m1.1.1.2.cmml">y</mi><mo id="S5.SS5.p1.1.m1.1.1.1" xref="S5.SS5.p1.1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.1.m1.1b"><apply id="S5.SS5.p1.1.m1.1.1.cmml" xref="S5.SS5.p1.1.m1.1.1"><ci id="S5.SS5.p1.1.m1.1.1.1.cmml" xref="S5.SS5.p1.1.m1.1.1.1">^</ci><ci id="S5.SS5.p1.1.m1.1.1.2.cmml" xref="S5.SS5.p1.1.m1.1.1.2">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.1.m1.1c">\hat{y}</annotation></semantics></math> obtained from synthetic audio vs from real audio.
We use <math id="S5.SS5.p1.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.SS5.p1.2.m2.1a"><mo id="S5.SS5.p1.2.m2.1.1" xref="S5.SS5.p1.2.m2.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.2.m2.1b"><csymbol cd="latexml" id="S5.SS5.p1.2.m2.1.1.cmml" xref="S5.SS5.p1.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.2.m2.1c">\sim</annotation></semantics></math>60k hours of real labeled data from LibriheavyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> which gives 400M transcribed words, and generate <math id="S5.SS5.p1.3.m3.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="S5.SS5.p1.3.m3.1a"><mover accent="true" id="S5.SS5.p1.3.m3.1.1" xref="S5.SS5.p1.3.m3.1.1.cmml"><mi id="S5.SS5.p1.3.m3.1.1.2" xref="S5.SS5.p1.3.m3.1.1.2.cmml">y</mi><mo id="S5.SS5.p1.3.m3.1.1.1" xref="S5.SS5.p1.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.3.m3.1b"><apply id="S5.SS5.p1.3.m3.1.1.cmml" xref="S5.SS5.p1.3.m3.1.1"><ci id="S5.SS5.p1.3.m3.1.1.1.cmml" xref="S5.SS5.p1.3.m3.1.1.1">^</ci><ci id="S5.SS5.p1.3.m3.1.1.2.cmml" xref="S5.SS5.p1.3.m3.1.1.2">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.3.m3.1c">\hat{y}</annotation></semantics></math> from it using the <span id="S5.SS5.p1.4.1" class="ltx_text ltx_font_italic">baseline ASR</span>.
For a fair comparison, we also generate TTS data by RichTTS for the same sentences. For both datasets of <math id="S5.SS5.p1.4.m4.2" class="ltx_Math" alttext="(\hat{y},y)" display="inline"><semantics id="S5.SS5.p1.4.m4.2a"><mrow id="S5.SS5.p1.4.m4.2.3.2" xref="S5.SS5.p1.4.m4.2.3.1.cmml"><mo stretchy="false" id="S5.SS5.p1.4.m4.2.3.2.1" xref="S5.SS5.p1.4.m4.2.3.1.cmml">(</mo><mover accent="true" id="S5.SS5.p1.4.m4.1.1" xref="S5.SS5.p1.4.m4.1.1.cmml"><mi id="S5.SS5.p1.4.m4.1.1.2" xref="S5.SS5.p1.4.m4.1.1.2.cmml">y</mi><mo id="S5.SS5.p1.4.m4.1.1.1" xref="S5.SS5.p1.4.m4.1.1.1.cmml">^</mo></mover><mo id="S5.SS5.p1.4.m4.2.3.2.2" xref="S5.SS5.p1.4.m4.2.3.1.cmml">,</mo><mi id="S5.SS5.p1.4.m4.2.2" xref="S5.SS5.p1.4.m4.2.2.cmml">y</mi><mo stretchy="false" id="S5.SS5.p1.4.m4.2.3.2.3" xref="S5.SS5.p1.4.m4.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.4.m4.2b"><interval closure="open" id="S5.SS5.p1.4.m4.2.3.1.cmml" xref="S5.SS5.p1.4.m4.2.3.2"><apply id="S5.SS5.p1.4.m4.1.1.cmml" xref="S5.SS5.p1.4.m4.1.1"><ci id="S5.SS5.p1.4.m4.1.1.1.cmml" xref="S5.SS5.p1.4.m4.1.1.1">^</ci><ci id="S5.SS5.p1.4.m4.1.1.2.cmml" xref="S5.SS5.p1.4.m4.1.1.2">ğ‘¦</ci></apply><ci id="S5.SS5.p1.4.m4.2.2.cmml" xref="S5.SS5.p1.4.m4.2.2">ğ‘¦</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.4.m4.2c">(\hat{y},y)</annotation></semantics></math> we train a DLM-real and DLM-synthetic.
On the <span id="S5.SS5.p1.4.2" class="ltx_text ltx_font_italic">dev-clean</span> set, the DLM-synthetic (WER 1.7%) is worse than the DLM-real (WER 1.5%).
Surprisingly, on the <span id="S5.SS5.p1.4.3" class="ltx_text ltx_font_italic">dev-other</span> set, DLM-synthetic (WER 3.7%) outperforms the DLM-real (WER 4.1%).
Overall, the gap between DLM-synthetic and DLM-real is small and thus effect of TTS generated data is small for DLMs. See TableÂ <a href="#A5.T10" title="Table 10 â€£ Appendix E Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> in AppendixÂ <a href="#A5" title="Appendix E Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a> for more details.</p>
</div>
</section>
<section id="S5.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>High quality TTS are not what you need</h3>

<div id="S5.SS6.p1" class="ltx_para">
<p id="S5.SS6.p1.1" class="ltx_p">We assess the impact of using different TTS systems for generating DLMs training data.
We compare Tacotron, YourTTS and RichTTS by evaluating ASR systems on the audio generated by these TTS models: synthetic audio from Tacotron (6%) has significantly lower WER than that from YourTTS (10%) and RichTTS (12%) when evaluated with Whisper-small. See TableÂ <a href="#A5.T11" title="Table 11 â€£ Appendix E Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> in AppendixÂ <a href="#A5" title="Appendix E Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a> for more details.
We then trained DLMs using data generated by the three TTS systems separately, and applied them to the <span id="S5.SS6.p1.1.1" class="ltx_text ltx_font_italic">baseline ASR</span> model, see TableÂ <a href="#S5.T7" title="Table 7 â€£ 5.4 Ablation: design proper error correction data distribution â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
Though Tacotron data has higher quality than YourTTS and RichTTS data, it performs poorly in terms of DLM error correction.
We posit that there might not be enough meaningful noise in the Tacotron data for the error correction model to learn.
In this sense, improving ASR systems in the proposed error correction paradigm is easy â€“ it does not require high-quality TTS.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We present DLM, a scaled ASR error correction model that leverages synthetic data and learns the proper error distribution.
For the first time, we show that state-of-the-art WER can be achieved with such error correction models on the well-known LibriSpeech benchmark, and that error correction models are able to outperform conventional neural language models by a large margin.
Meanwhile, we provide a detailed investigations on how to design the right error correction data distribution, including mixing real data, using multiple TTS systems, and employing noise augmentation strategies. With comprehensive experiments, we demonstrate that the proposed DLMs are universal, scalable and efficient, showing great potential to replace neural LM in modern ASR systems. We further discuss limitations of our work in AppendixÂ <a href="#A1" title="Appendix A Limitations â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> and broader impact in AppendixÂ <a href="#A2" title="Appendix B Broader Impact â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>. The success of DLMs leads us to contend that the time is right to reimagine how large text corpora might improve ASR systems.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Tacotron-2 and yourtts pretrained models.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/coqui-ai/TTS" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/coqui-ai/TTS</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Whisper pretrained models.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/openai/whisper" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/openai/whisper</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.

</span>
<span class="ltx_bibblock">wav2vec 2.0: A framework for self-supervised learning of speech representations.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 33:12449â€“12460, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Edresson Casanova, Julian Weber, ChristopherÂ D Shulby, ArnaldoÂ Candido Junior, Eren GÃ¶lge, and MoacirÂ A Ponti.

</span>
<span class="ltx_bibblock">Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages 2709â€“2720. PMLR, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
William Chan, Navdeep Jaitly, QuocÂ V Le, and Oriol Vinyals.

</span>
<span class="ltx_bibblock">Listen, attend and spell.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1508.01211</span>, 2015.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Lingfeng Dai, LuÂ Chen, Zhikai Zhou, and Kai Yu.

</span>
<span class="ltx_bibblock">Latticebart: Lattice-to-lattice pre-training for speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 6112â€“6116. IEEE, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Samrat Dutta, Shreyansh Jain, Ayush Maheshwari, Souvik Pal, Ganesh Ramakrishnan, and Preethi Jyothi.

</span>
<span class="ltx_bibblock">Error correction in asr using sequence-to-sequence models.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2202.01157</span>, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Alex Graves, Santiago FernÃ¡ndez, Faustino Gomez, and JÃ¼rgen Schmidhuber.

</span>
<span class="ltx_bibblock">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proceedings of the 23rd International Conference on Machine Learning</span>, pages 369â€“376, 2006.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, YuÂ Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, etÂ al.

</span>
<span class="ltx_bibblock">Conformer: Convolution-augmented transformer for speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">On using monolingual corpora in neural machine translation.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1503.03535</span>, 2015.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Jinxi Guo, TaraÂ N Sainath, and RonÂ J Weiss.

</span>
<span class="ltx_bibblock">A spelling correction model for end-to-end speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 5651â€“5655. IEEE, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Wei Han, Zhengdong Zhang, YuÂ Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming Pang, and Yonghui Wu.

</span>
<span class="ltx_bibblock">Contextnet: Improving convolutional neural networks for automatic speech recognition with global context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
FranÃ§ois Hernandez, Vincent Nguyen, Sahar Ghannay, Natalia Tomashenko, and Yannick Esteve.

</span>
<span class="ltx_bibblock">Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker adaptation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Speech and Computer: 20th International Conference, SPECOM 2018, Leipzig, Germany, September 18â€“22, 2018, Proceedings 20</span>, pages 198â€“208. Springer, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Takaaki Hori, Chiori Hori, Shinji Watanabe, and JohnÂ R. Hershey.

</span>
<span class="ltx_bibblock">Minimum word error training of long short-term memory recurrent neural network language models for speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 5990â€“5994, 2016.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Oleksii Hrinchuk, Mariya Popova, and Boris Ginsburg.

</span>
<span class="ltx_bibblock">Correction of automatic speech recognition with transformer sequence-to-sequence model.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 7074â€“7078. IEEE, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Wei-Ning Hsu, Benjamin Bolte, Yao-HungÂ Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed.

</span>
<span class="ltx_bibblock">Hubert: Self-supervised speech representation learning by masked prediction of hidden units.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span>, 29:3451â€“3460, 2021.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Ting-Yao Hu, Mohammadreza Armandpour, Ashish Shrivastava, Jen-HaoÂ Rick Chang, Hema Koppula, and Oncel Tuzel.

</span>
<span class="ltx_bibblock">Synt++: Utilizing imperfect synthetic data to improve speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 7682â€“7686. IEEE, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Navdeep Jaitly, Patrick Nguyen, Andrew Senior, and Vincent Vanhoucke.

</span>
<span class="ltx_bibblock">Application of pretrained deep neural networks to large vocabulary speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, pages 2578â€“2581, 2012.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Jacob Kahn, Morgane Riviere, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel MazarÃ©, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, etÂ al.

</span>
<span class="ltx_bibblock">Libri-light: A benchmark for asr with limited or no supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 7669â€“7673. IEEE, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Wei Kang, Xiaoyu Yang, Zengwei Yao, Fangjun Kuang, Yifan Yang, Liyong Guo, Long Lin, and Daniel Povey.

</span>
<span class="ltx_bibblock">Libriheavy: a 50,000 hours asr corpus with punctuation casing and context.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2309.08105</span>, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Anjuli Kannan, Yonghui Wu, Patrick Nguyen, TaraÂ N Sainath, Zhijeng Chen, and Rohit Prabhavalkar.

</span>
<span class="ltx_bibblock">An analysis of incorporating an external language model into a sequence-to-sequence model.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 1â€“5828. IEEE, 2018.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Sridhar, KyuÂ J Han, and Shinji Watanabe.

</span>
<span class="ltx_bibblock">E-branchformer: Branchformer with enhanced merging for speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">2022 IEEE Spoken Language Technology Workshop (SLT)</span>, pages 84â€“91. IEEE, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Brian Kingsbury.

</span>
<span class="ltx_bibblock">Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">2009 IEEE International Conference on Acoustics, Speech and Signal Processing</span>, pages 3761â€“3764. IEEE, 2009.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Samuel Kriman, Stanislav Beliaev, Boris Ginsburg, Jocelyn Huang, Oleksii Kuchaiev, Vitaly Lavrukhin, Ryan Leary, Jason Li, and Yang Zhang.

</span>
<span class="ltx_bibblock">Quartznet: Deep automatic speech recognition with 1d time-channel separable convolutions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 6124â€“6128. IEEE, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Yichong Leng, XuÂ Tan, Rui Wang, Linchen Zhu, Jin Xu, Wenjie Liu, Linquan Liu, Xiang-Yang Li, Tao Qin, Edward Lin, etÂ al.

</span>
<span class="ltx_bibblock">Fastcorrect 2: Fast error correction on multiple candidates for automatic speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2021</span>, pages 4328â€“4337, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</span>, pages 7871â€“7880, 2020.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Rao Ma, MarkÂ JF Gales, KateÂ M Knill, and Mengjie Qian.

</span>
<span class="ltx_bibblock">N-best t5: Robust asr error correction using multiple input hypotheses and constrained decoding space.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.00456</span>, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Rao Ma, Hao Li, QiÂ Liu, LuÂ Chen, and Kai Yu.

</span>
<span class="ltx_bibblock">Neural lattice search for speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 7794â€“7798. IEEE, 2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Rao Ma, Mengjie Qian, Potsawee Manakul, Mark Gales, and Kate Knill.

</span>
<span class="ltx_bibblock">Can generative large language models perform asr error correction?

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.04172</span>, 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Erik McDermott, Hasim Sak, and Ehsan Variani.

</span>
<span class="ltx_bibblock">A density ratio approach to language model fusion in end-to-end automatic speech recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</span>, pages 434â€“441, 2019.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu Li, and Yifan Gong.

</span>
<span class="ltx_bibblock">Internal language model estimation for domain-adaptive end-to-end speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">2021 IEEE Spoken Language Technology Workshop (SLT)</span>, pages 243â€“250. IEEE, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Mohammad Norouzi, Samy Bengio, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans, etÂ al.

</span>
<span class="ltx_bibblock">Reward augmented maximum likelihood for neural structured prediction.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Advances In Neural Information Processing Systems</span>, 29, 2016.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Jing Pan, Joshua Shapiro, Jeremy Wohlwend, KyuÂ J Han, Tao Lei, and Tao Ma.

</span>
<span class="ltx_bibblock">Asapp-asr: Multistream cnn and self-attentive sru for sota speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, 2020.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur.

</span>
<span class="ltx_bibblock">Librispeech: an asr corpus based on public domain audio books.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)</span>, pages 5206â€“5210. IEEE, 2015.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
DanielÂ S. Park, William Chan, YuÂ Zhang, Chung-Cheng Chiu, Barret Zoph, EkinÂ D. Cubuk, and QuocÂ V. Le.

</span>
<span class="ltx_bibblock">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, pages 2613â€“2617, 2019.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Alec Radford, JongÂ Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages 28492â€“28518. PMLR, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Srijith Radhakrishnan, Chao-HanÂ Huck Yang, SumeerÂ Ahmad Khan, Rohit Kumar, NarsisÂ A. Kiani, David Gomez-Cabrero, and Jesper TegnÃ©r.

</span>
<span class="ltx_bibblock">Whispering LLaMA: A cross-modal generative error correction framework for speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">The 2023 Conference on Empirical Methods in Natural Language Processing</span>, 2023.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Jonathan Shen, Ruoming Pang, RonÂ J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, YuÂ Zhang, Yuxuan Wang, RjÂ Skerrv-Ryan, etÂ al.

</span>
<span class="ltx_bibblock">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)</span>, pages 4779â€“4783. IEEE, 2018.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
PrashanthÂ Gurunath Shivakumar, Haoqi Li, Kevin Knight, and Panayiotis Georgiou.

</span>
<span class="ltx_bibblock">Learning from past mistakes: improving automatic speech recognition output via noisy-clean phrase context modeling.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">APSIPA Transactions on Signal and Information Processing</span>, 8:e8, 2019.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Gan Song, Zelin Wu, Golan Pundak, Angad Chandorkar, Kandarp Joshi, Xavier Velez, Diamantino Caseiro, Ben Haynor, Weiran Wang, Nikhil Siddhartha, etÂ al.

</span>
<span class="ltx_bibblock">Contextual spelling correction with large language models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</span>, pages 1â€“8. IEEE, 2023.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Anuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and Adam Coates.

</span>
<span class="ltx_bibblock">Cold fusion: Training seq2seq models together with language models, 2018.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Gabriel Synnaeve, Qiantong Xu, Jacob Kahn, Tatiana Likhomanenko, Edouard Grave, Vineel Pratap, Anuroop Sriram, Vitaliy Liptchinsky, and Ronan Collobert.

</span>
<span class="ltx_bibblock">End-to-end ASR: from supervised to semi-supervised learning with modern architectures.

</span>
<span class="ltx_bibblock">In <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">ICML 2020 Workshop on Self-supervision in Audio and Speech</span>, 2020.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Tomohiro Tanaka, Ryo Masumura, Hirokazu Masataki, and Yushi Aono.

</span>
<span class="ltx_bibblock">Neural error corrective language models for automatic speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, pages 401â€“405, 2018.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Shubham Toshniwal, Anjuli Kannan, Chung-Cheng Chiu, Yonghui Wu, TaraÂ N Sainath, and Karen Livescu.

</span>
<span class="ltx_bibblock">A comparison of techniques for language model integration in encoder-decoder speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">2018 IEEE spoken language technology workshop (SLT)</span>, pages 369â€“375. IEEE, 2018.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Ehsan Variani, Xin Lei, Erik McDermott, IgnacioÂ Lopez Moreno, and Javier Gonzalez-Dominguez.

</span>
<span class="ltx_bibblock">Deep neural networks for small footprint text-dependent speaker verification.

</span>
<span class="ltx_bibblock">In <span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">2014 IEEE international conference on acoustics, speech and signal processing (ICASSP)</span>, pages 4052â€“4056. IEEE, 2014.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Ehsan Variani, David Rybach, Cyril Allauzen, and Michael Riley.

</span>
<span class="ltx_bibblock">Hybrid autoregressive transducer (hat).

</span>
<span class="ltx_bibblock">In <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span>, pages 6139â€“6143, 2020.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Karel VeselÃ½, Arnab Ghoshal, LukÃ¡Å¡ Burget, and Daniel Povey.

</span>
<span class="ltx_bibblock">Sequence-discriminative training of deep neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, pages 2345â€“2349, 2013.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Haoyu Wang, Shuyan Dong, Yue Liu, James Logan, Ashish Agrawal, and Yang Liu.

</span>
<span class="ltx_bibblock">Asr error correction with augmented transformer for entity retrieval.

</span>
<span class="ltx_bibblock">In <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, 2020.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Qiantong Xu, Tatiana Likhomanenko, Jacob Kahn, Awni Hannun, Gabriel Synnaeve, and Ronan Collobert.

</span>
<span class="ltx_bibblock">Iterative Pseudo-Labeling for Speech Recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, pages 1006â€“1010, 2020.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Shiliang Zhang, Ming Lei, and Zhijie Yan.

</span>
<span class="ltx_bibblock">Investigation of Transformer Based Spelling Correction Model for CTC-Based End-to-End Mandarin Speech Recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, pages 2180â€“2184, 2019.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
YuÂ Zhang, James Qin, DanielÂ S Park, Wei Han, Chung-Cheng Chiu, Ruoming Pang, QuocÂ V Le, and Yonghui Wu.

</span>
<span class="ltx_bibblock">Pushing the limits of semi-supervised learning for automatic speech recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.10504</span>, 2020.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Yun Zhao, Xuerui Yang, Jinchao Wang, Yongyu Gao, Chao Yan, and Yuanfu Zhou.

</span>
<span class="ltx_bibblock">Bart based semantic correction for mandarin automatic speech recognition system.

</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2104.05507</span>, 2021.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Limitations</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">In our empirical results we reported only CTC-based ASR models. While our preliminary results show similar trends for encoder-decoder models, deeper assessment is further needed.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p">While we evaluated applicability of trained DLMs (for read speech and text from books) to out-of-domain speech data (evaluation on TED-LIUM with conversational speech) and other ASR systems (conv-based and Conformers), broader investigation on applicability to cases with domain shift is needed.</p>
</div>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p">We assessed initial scaling up to 1.6B words in text and 1.3B model size, however further scaling experiments are needed with other datasets as performance on LibriSpeech dataset is already at human level.</p>
</div>
<div id="A1.p4" class="ltx_para">
<p id="A1.p4.1" class="ltx_p">As the main goal of the paper to show that error correction models can outperform neural LMs and potential for future developments of LMs in error correction direction, we did not work on efficiency of the inference as well as on the integration of DLMs with streaming models.</p>
</div>
<div id="A1.p5" class="ltx_para">
<p id="A1.p5.1" class="ltx_p">As all machine learning models, DLMs are trained on data which has bias: in our case the bias comes not only from the text data but also from the text to speech systems and real audio data. As we demonstrated, diversity of the generated data by text to speech systems (e.g. multi-speaker, different utterances for the same speaker, several text to speech systems) helps to reduce the final error of ASR systems, however further research is needed to alleviate other potential biases, discrimination, and privacy risks.</p>
</div>
<div id="A1.p6" class="ltx_para">
<p id="A1.p6.1" class="ltx_p">As we showed, DLMs can outperform neural LMs without the access to the real audio data while using large amount of synthetic data.
On the one hand, this unlocks research for low resource languages where vast amount of audio is not accessible.
On the other hand, it is possible that it reinforces biases of the text to speech systems into the ASR system, and further, it needs additional computational resources to generate synthetic data.
We attempted to assess the bias introduced by the text to speech system by comparing it with the real data usage, where we see that careful design of text to speech system can reduce the introduced bias.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Broader Impact</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">This paper presents a way to improve speech recognition by using text to speech systems. Improved ASR could lead to improved user experiences for people, especially those requiring assistive technologies.
We do use zero-shot text to speech systems, which can be used for mis-information; however, zero-shot text to speech is not the theme of this paper, nor do we aim to release it as a part of this work.
In our work, we use text to speech system is used solely to improve ASR system.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Data, Pretrained Models, Code and Reproducibility</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">Whisperâ€™s code and models weights are released under the MIT licenseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. YourTTS and Tacotron-2 models and code are released under MPL-2.0 licenseÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="A3.p2" class="ltx_para">
<p id="A3.p2.1" class="ltx_p">LibriSpeechÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> dataset is released under CC BY 4.0, LibriheavyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> dataset and its code are released under Apache-2.0 license, TED-LIUM v3Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> is released under CC BY-NC-ND 3.0.</p>
</div>
<div id="A3.p3" class="ltx_para">
<p id="A3.p3.1" class="ltx_p">We do not plan to open source our in-house RichTTS model or its training code. We also do not plan to open source any generated data by any TTS model used in the paper.</p>
</div>
<div id="A3.p4" class="ltx_para">
<p id="A3.p4.1" class="ltx_p">We plan to open source the code of ASR and DLM training and inference as well as instructions how we generated data from YourTTS.</p>
</div>
<div id="A3.p5" class="ltx_para">
<p id="A3.p5.1" class="ltx_p">We tried to include as much information as possible about all models configurations, training recipes and extra experiments we did with their outcomes throughout the paper and in Appendix.
We are working on open sourcing the code for ASR and DLM training and inference to speed up adoption of our research results.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Training Details</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">All models are optimized on the validation data of LibriSpeech by adjusting the learning rate and optimizer. For the final results we evaluated all trained models on the test sets of LibriSpeech.</p>
</div>
<section id="A4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>LM-rescoring v.s. DSR-decoding</h3>

<div id="A4.SS1.p1" class="ltx_para">
<p id="A4.SS1.p1.2" class="ltx_p">In conventional neural LM rescoringÂ (e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>), the <span id="A4.SS1.p1.2.1" class="ltx_text ltx_font_bold ltx_font_italic">ASR generates a beam</span> (possibly with a weak <math id="A4.SS1.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="A4.SS1.p1.1.m1.1a"><mi id="A4.SS1.p1.1.m1.1.1" xref="A4.SS1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.1.m1.1b"><ci id="A4.SS1.p1.1.m1.1.1.cmml" xref="A4.SS1.p1.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.1.m1.1c">n</annotation></semantics></math>-gram LM), which is rescored using the neural LM.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>
Alternatively, joint decoding is also possible, using ASR model and neural LM together in the decoding, but it is more expensive and in practice the oracle WER of the first pass <math id="footnote4.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="footnote4.m1.1b"><mi id="footnote4.m1.1.1" xref="footnote4.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="footnote4.m1.1c"><ci id="footnote4.m1.1.1.cmml" xref="footnote4.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote4.m1.1d">n</annotation></semantics></math>-gram decoding is already very low.</span></span></span>
For the exact details of rescoring with neural LM we refer the reader to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>.
In contrast, here, <span id="A4.SS1.p1.2.2" class="ltx_text ltx_font_bold ltx_font_italic">DLM generates a beam</span>, using only the greedy output of the ASR and the ASR scores are then used to rescore the DLM beam by blending them with the DLM scores. We adopted nucleus sampling with threshold <math id="A4.SS1.p1.2.m2.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="A4.SS1.p1.2.m2.1a"><mn id="A4.SS1.p1.2.m2.1.1" xref="A4.SS1.p1.2.m2.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.2.m2.1b"><cn type="float" id="A4.SS1.p1.2.m2.1.1.cmml" xref="A4.SS1.p1.2.m2.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.2.m2.1c">0.9</annotation></semantics></math> to ensure we get full length beams.
In terms of computational cost (for beam generation, scores evaluation and re-ranking), the rescoring with neural LMs and DSR-decoding approaches are similar, while DLM greedy error correction is cheaper as it does not involve the beam search at all.</p>
</div>
</section>
<section id="A4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>ASR models</h3>

<div id="A4.SS2.p1" class="ltx_para">
<p id="A4.SS2.p1.1" class="ltx_p">The <span id="A4.SS2.p1.1.1" class="ltx_text ltx_font_italic">baseline ASR</span> is a Transformer-based encoder model with 255M parameters: 1D convolution of kernel 7 and stride 3 followed by absolute sinusoidal positional embedding and 36 transformer blocks with pre-LayerNorm, an embedding dimension of 768, 4 heads, 3072 units in the MLP layers, dropout of 0.1 and layer drop of 0.1.
The model is trained with CTC loss and a character vocabulary, including apostrophe (â€˜).
This ASR model is used throughout the paper to provide noisy transcripts for DLMs training.</p>
</div>
<div id="A4.SS2.p2" class="ltx_para">
<p id="A4.SS2.p2.1" class="ltx_p">We implemented our own Quartznet and Conformer, trained also with a CTC loss and a character vocabulary, including apostrophe (â€˜).
Conformer model has 102M parameters and consists of 1D convolution of kernel 7 and stride 3 followed by 16 conformer blocks with an embedding dimension of 512, 4 heads, 2048 units in the MLP layers, and dropout of 0.1.</p>
</div>
<div id="A4.SS2.p3" class="ltx_para">
<p id="A4.SS2.p3.1" class="ltx_p">All acoustic models are fed with 80 channel log-mel filterbanks, with a 25ms sliding window strided by 10ms.
SpecAugmentÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> is used as augmentation with 2 frequency masks with max width 30 and 10 time masks with max width 50 and ratio 0.1.
We pack total 3.56h, 4.44h and 4.44h of audio via dynamic batching for Transformer, Conformer and Quartznet, respectively.
The gradient clipping is set to 1.0 / 0.5 / 1.0, while learning rate warmup is performed for 64k / 10k / 64k steps, with a peak learning rate of 0.001 / 0.002 / 0.0022 for the TransformerÂ / ConformerÂ / Quartznet models.
For Transformer and Conformer, a cosine learning rate decay schedule is applied followed by a step decay of rate 0.5, while for Quartznet we use exponential learning rate decay instead.
All models are trained with the AdamW optimizer with 1e-6 weight decay until the greedy WER stops improving on the validation sets (<span id="A4.SS2.p3.1.1" class="ltx_text ltx_font_italic">dev-clean, dev-other</span>).</p>
</div>
<div id="A4.SS2.p4" class="ltx_para">
<p id="A4.SS2.p4.1" class="ltx_p">In addition, we train <span id="A4.SS2.p4.1.1" class="ltx_text ltx_font_italic">baseline ASR</span> with the mixture of LibriSpeech 960h train data (real) and synthetic audio data generated by RichTTS model from the LibriSpeech LM text corpus. The training setup is the same as for the <span id="A4.SS2.p4.1.2" class="ltx_text ltx_font_italic">baseline ASR</span> trained on real data only, but we vary the mixture proportion of real and synthetic audio in the minibatch. Results are shown in TableÂ <a href="#A4.T8" title="Table 8 â€£ D.2 ASR models â€£ Appendix D Training Details â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure id="A4.T8" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>LibriSpeech dev sets WER (%) for Transformer<span id="A4.T8.2.1" class="ltx_text ltx_font_italic"> baseline ASR</span> models (255M) trained with a mixture of real (LibriSpeech train data) and synthetic (generated from LibriSpeech LM text corpus by RichTTS) audio data. â€œreal:syntheticâ€ denotes the proportion (%) of corresponding data in a minibatch.</figcaption>
<div id="A4.T8.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:173.4pt;height:102.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-4.9pt,2.9pt) scale(0.9460641950237,0.9460641950237) ;">
<table id="A4.T8.3.1" class="ltx_tabular ltx_align_middle">
<tr id="A4.T8.3.1.1" class="ltx_tr">
<td id="A4.T8.3.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">real:synthetic</td>
<td id="A4.T8.3.1.1.2" class="ltx_td ltx_align_right ltx_border_tt">dev-clean</td>
<td id="A4.T8.3.1.1.3" class="ltx_td ltx_align_right ltx_border_tt">dev-other</td>
</tr>
<tr id="A4.T8.3.1.2" class="ltx_tr">
<td id="A4.T8.3.1.2.1" class="ltx_td ltx_align_center ltx_border_t">0:100</td>
<td id="A4.T8.3.1.2.2" class="ltx_td ltx_align_right ltx_border_t">12.7</td>
<td id="A4.T8.3.1.2.3" class="ltx_td ltx_align_right ltx_border_t">33.4</td>
</tr>
<tr id="A4.T8.3.1.3" class="ltx_tr">
<td id="A4.T8.3.1.3.1" class="ltx_td ltx_align_center">30:70</td>
<td id="A4.T8.3.1.3.2" class="ltx_td ltx_align_right">2.1</td>
<td id="A4.T8.3.1.3.3" class="ltx_td ltx_align_right">5.3</td>
</tr>
<tr id="A4.T8.3.1.4" class="ltx_tr">
<td id="A4.T8.3.1.4.1" class="ltx_td ltx_align_center">50:50</td>
<td id="A4.T8.3.1.4.2" class="ltx_td ltx_align_right">1.9</td>
<td id="A4.T8.3.1.4.3" class="ltx_td ltx_align_right">4.7</td>
</tr>
<tr id="A4.T8.3.1.5" class="ltx_tr">
<td id="A4.T8.3.1.5.1" class="ltx_td ltx_align_center">70:30</td>
<td id="A4.T8.3.1.5.2" class="ltx_td ltx_align_right">2.0</td>
<td id="A4.T8.3.1.5.3" class="ltx_td ltx_align_right">4.9</td>
</tr>
<tr id="A4.T8.3.1.6" class="ltx_tr">
<td id="A4.T8.3.1.6.1" class="ltx_td ltx_align_center ltx_border_bb">100:0</td>
<td id="A4.T8.3.1.6.2" class="ltx_td ltx_align_right ltx_border_bb">2.1</td>
<td id="A4.T8.3.1.6.3" class="ltx_td ltx_align_right ltx_border_bb">5.5</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="A4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.3 </span>DLMs</h3>

<div id="A4.SS3.p1" class="ltx_para">
<p id="A4.SS3.p1.1" class="ltx_p">DLMs are trained with a dynamic batch size of total 160,000 tokens and a character vocabulary.
We use AdamW optimizer with weight decay 0.01, gradient clipping of 0.1 and learning rate warmup of 64k steps, followed by a constant learning rate of 0.001 for 300k steps, then a step decay with rate of 0.5 and step size of 200k until WER of DLM-greedy reaches plateau on validation sets (<span id="A4.SS3.p1.1.1" class="ltx_text ltx_font_italic">dev-clean, dev-other</span>).</p>
</div>
<div id="A4.SS3.p2" class="ltx_para">
<p id="A4.SS3.p2.1" class="ltx_p">In all experiments we use exactly the same training hyperparameters changing only model size and / or data on which DLMs are trained.</p>
</div>
</section>
<section id="A4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.4 </span>Neural LMs</h3>

<div id="A4.SS4.p1" class="ltx_para">
<p id="A4.SS4.p1.1" class="ltx_p">For neural LM training, we use a 10K word-piece vocabulary created from the LibriSpeech train data.
The models are trained on LibriSpeech LM text corpus (the same text data are used for synthetic audio generation by TTS for DLM training) where tokenized texts are concatenated together, separated by the end of sentence (EOS) token and batched into total 768 examples with a window of 256 tokens (total 196,608 tokens).
Neural LMs are trained with AdamW optimizer with <math id="A4.SS4.p1.1.m1.1" class="ltx_Math" alttext="1e-8" display="inline"><semantics id="A4.SS4.p1.1.m1.1a"><mrow id="A4.SS4.p1.1.m1.1.1" xref="A4.SS4.p1.1.m1.1.1.cmml"><mrow id="A4.SS4.p1.1.m1.1.1.2" xref="A4.SS4.p1.1.m1.1.1.2.cmml"><mn id="A4.SS4.p1.1.m1.1.1.2.2" xref="A4.SS4.p1.1.m1.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="A4.SS4.p1.1.m1.1.1.2.1" xref="A4.SS4.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="A4.SS4.p1.1.m1.1.1.2.3" xref="A4.SS4.p1.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="A4.SS4.p1.1.m1.1.1.1" xref="A4.SS4.p1.1.m1.1.1.1.cmml">âˆ’</mo><mn id="A4.SS4.p1.1.m1.1.1.3" xref="A4.SS4.p1.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.SS4.p1.1.m1.1b"><apply id="A4.SS4.p1.1.m1.1.1.cmml" xref="A4.SS4.p1.1.m1.1.1"><minus id="A4.SS4.p1.1.m1.1.1.1.cmml" xref="A4.SS4.p1.1.m1.1.1.1"></minus><apply id="A4.SS4.p1.1.m1.1.1.2.cmml" xref="A4.SS4.p1.1.m1.1.1.2"><times id="A4.SS4.p1.1.m1.1.1.2.1.cmml" xref="A4.SS4.p1.1.m1.1.1.2.1"></times><cn type="integer" id="A4.SS4.p1.1.m1.1.1.2.2.cmml" xref="A4.SS4.p1.1.m1.1.1.2.2">1</cn><ci id="A4.SS4.p1.1.m1.1.1.2.3.cmml" xref="A4.SS4.p1.1.m1.1.1.2.3">ğ‘’</ci></apply><cn type="integer" id="A4.SS4.p1.1.m1.1.1.3.cmml" xref="A4.SS4.p1.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS4.p1.1.m1.1c">1e-8</annotation></semantics></math> weight decay and cosine learning rate schedules with a peak learning rate of 0.001 for 500k steps. Warmup is performed 16k steps. Perplexity for 69M, 155M and 484M LMs are 34.45, 31.49 and 30.9 respectively.</p>
</div>
</section>
<section id="A4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.5 </span>Compute</h3>

<div id="A4.SS5.p1" class="ltx_para">
<p id="A4.SS5.p1.1" class="ltx_p">All models are trained on A100 with 80GB memory with 1 node of 8 GPUs for 3-5 days. Some models we train with larger batch size which results in longer training with gradient accumulation.</p>
</div>
<div id="A4.SS5.p2" class="ltx_para">
<p id="A4.SS5.p2.1" class="ltx_p">For data generation by YourTTS model we use CPU inference and it takes 100k CPU hours (as it is not batched version of public code). With our in house RichTTS (116M), we spent 12,800 GPU hours to synthesize 40M audios (roughly 137k hours of data).</p>
</div>
<div id="A4.SS5.p3" class="ltx_para">
<p id="A4.SS5.p3.1" class="ltx_p">DSR-decoding with beam of 64 is sufficient, while the beam of 10 gives similar results within 0.1% variation in WER), which takes about 10Â mins to finish on 8 A100 GPU for dev sets.</p>
</div>
</section>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Results</h2>

<figure id="A5.T9" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span>LibriSpeech dev and test sets WER (%) of <span id="A5.T9.2.1" class="ltx_text ltx_font_italic">baseline ASR</span> model trained on LibriSpeech (960h) with greedy decoding, with neural LM (neLM, 500M) rescoring or with DLM (484M) DSR-greedy/DSR-decoding. Both neural LM and DLM are the same as reported in TableÂ <a href="#S5.T1" title="Table 1 â€£ 5.1 Pushing the limits: DLM achieves new SOTA WER on LibriSpeech â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We also report WER (%) on dev and test sets of out-of-domain TED-LIUM.</figcaption>
<table id="A5.T9.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A5.T9.3.1" class="ltx_tr">
<td id="A5.T9.3.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="A5.T9.3.1.1.1" class="ltx_text">Model</span></td>
<td id="A5.T9.3.1.2" class="ltx_td ltx_align_right ltx_border_tt" rowspan="2"><span id="A5.T9.3.1.2.1" class="ltx_text">Size</span></td>
<td id="A5.T9.3.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">dev</td>
<td id="A5.T9.3.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">test</td>
<td id="A5.T9.3.1.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">TED-LIUM</td>
</tr>
<tr id="A5.T9.3.2" class="ltx_tr">
<td id="A5.T9.3.2.1" class="ltx_td ltx_align_right ltx_border_t">clean</td>
<td id="A5.T9.3.2.2" class="ltx_td ltx_align_right ltx_border_t">other</td>
<td id="A5.T9.3.2.3" class="ltx_td ltx_align_right ltx_border_t">clean</td>
<td id="A5.T9.3.2.4" class="ltx_td ltx_align_right ltx_border_t">other</td>
<td id="A5.T9.3.2.5" class="ltx_td ltx_align_right ltx_border_t">dev</td>
<td id="A5.T9.3.2.6" class="ltx_td ltx_align_right ltx_border_t">test</td>
</tr>
<tr id="A5.T9.3.3" class="ltx_tr">
<td id="A5.T9.3.3.1" class="ltx_td ltx_align_left ltx_border_t">Transformer ASR (LS)</td>
<td id="A5.T9.3.3.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" rowspan="4"><span id="A5.T9.3.3.2.1" class="ltx_text">256M</span></td>
<td id="A5.T9.3.3.3" class="ltx_td ltx_align_right ltx_border_t">2.1</td>
<td id="A5.T9.3.3.4" class="ltx_td ltx_align_right ltx_border_t">5.5</td>
<td id="A5.T9.3.3.5" class="ltx_td ltx_align_right ltx_border_t">2.2</td>
<td id="A5.T9.3.3.6" class="ltx_td ltx_align_right ltx_border_t">5.3</td>
<td id="A5.T9.3.3.7" class="ltx_td ltx_align_right ltx_border_t">11.6</td>
<td id="A5.T9.3.3.8" class="ltx_td ltx_align_right ltx_border_t">10.8</td>
</tr>
<tr id="A5.T9.3.4" class="ltx_tr">
<td id="A5.T9.3.4.1" class="ltx_td ltx_align_left">+ neLM (LM-rescoring)</td>
<td id="A5.T9.3.4.2" class="ltx_td ltx_align_right">1.5</td>
<td id="A5.T9.3.4.3" class="ltx_td ltx_align_right">4.0</td>
<td id="A5.T9.3.4.4" class="ltx_td ltx_align_right">2.0</td>
<td id="A5.T9.3.4.5" class="ltx_td ltx_align_right">4.1</td>
<td id="A5.T9.3.4.6" class="ltx_td ltx_align_right">10.6</td>
<td id="A5.T9.3.4.7" class="ltx_td ltx_align_right">9.7</td>
</tr>
<tr id="A5.T9.3.5" class="ltx_tr">
<td id="A5.T9.3.5.1" class="ltx_td ltx_align_left">+ DLM (DSR-greedy)</td>
<td id="A5.T9.3.5.2" class="ltx_td ltx_align_right">1.8</td>
<td id="A5.T9.3.5.3" class="ltx_td ltx_align_right">3.9</td>
<td id="A5.T9.3.5.4" class="ltx_td ltx_align_right">2.0</td>
<td id="A5.T9.3.5.5" class="ltx_td ltx_align_right">4.1</td>
<td id="A5.T9.3.5.6" class="ltx_td ltx_align_right">11.0</td>
<td id="A5.T9.3.5.7" class="ltx_td ltx_align_right">10.8</td>
</tr>
<tr id="A5.T9.3.6" class="ltx_tr">
<td id="A5.T9.3.6.1" class="ltx_td ltx_align_left ltx_border_bb">+ DLM (DSR-decoding)</td>
<td id="A5.T9.3.6.2" class="ltx_td ltx_align_right ltx_border_bb"><span id="A5.T9.3.6.2.1" class="ltx_text ltx_font_bold">1.5</span></td>
<td id="A5.T9.3.6.3" class="ltx_td ltx_align_right ltx_border_bb"><span id="A5.T9.3.6.3.1" class="ltx_text ltx_font_bold">3.4</span></td>
<td id="A5.T9.3.6.4" class="ltx_td ltx_align_right ltx_border_bb"><span id="A5.T9.3.6.4.1" class="ltx_text ltx_font_bold">1.6</span></td>
<td id="A5.T9.3.6.5" class="ltx_td ltx_align_right ltx_border_bb"><span id="A5.T9.3.6.5.1" class="ltx_text ltx_font_bold">3.6</span></td>
<td id="A5.T9.3.6.6" class="ltx_td ltx_align_right ltx_border_bb"><span id="A5.T9.3.6.6.1" class="ltx_text ltx_font_bold">9.7</span></td>
<td id="A5.T9.3.6.7" class="ltx_td ltx_align_right ltx_border_bb"><span id="A5.T9.3.6.7.1" class="ltx_text ltx_font_bold">9.2</span></td>
</tr>
</table>
</figure>
<figure id="A5.T10" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10: </span>LibriSpeech WER (%) for DLMs (484M) reported with DSR-greedy (DSR-decoding): DLM-synthetic is using audio generated by RichTTS from Libriheavy transcriptions, DLM-real is using real audio of Libriheavy for <math id="A5.T10.2.m1.1" class="ltx_Math" alttext="\hat{y}" display="inline"><semantics id="A5.T10.2.m1.1b"><mover accent="true" id="A5.T10.2.m1.1.1" xref="A5.T10.2.m1.1.1.cmml"><mi id="A5.T10.2.m1.1.1.2" xref="A5.T10.2.m1.1.1.2.cmml">y</mi><mo id="A5.T10.2.m1.1.1.1" xref="A5.T10.2.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="A5.T10.2.m1.1c"><apply id="A5.T10.2.m1.1.1.cmml" xref="A5.T10.2.m1.1.1"><ci id="A5.T10.2.m1.1.1.1.cmml" xref="A5.T10.2.m1.1.1.1">^</ci><ci id="A5.T10.2.m1.1.1.2.cmml" xref="A5.T10.2.m1.1.1.2">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T10.2.m1.1d">\hat{y}</annotation></semantics></math> generation by <span id="A5.T10.4.1" class="ltx_text ltx_font_italic">baseline ASR</span>. Both DLMs are trained with 10% random characters substitutions.</figcaption>
<table id="A5.T10.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A5.T10.5.1" class="ltx_tr">
<td id="A5.T10.5.1.1" class="ltx_td ltx_align_left ltx_border_tt">Model</td>
<td id="A5.T10.5.1.2" class="ltx_td ltx_align_left ltx_border_tt">dev-clean</td>
<td id="A5.T10.5.1.3" class="ltx_td ltx_align_left ltx_border_tt">dev-other</td>
</tr>
<tr id="A5.T10.5.2" class="ltx_tr">
<td id="A5.T10.5.2.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A5.T10.5.2.1.1" class="ltx_text ltx_font_italic">baseline ASR</span></td>
<td id="A5.T10.5.2.2" class="ltx_td ltx_align_left ltx_border_t">2.1</td>
<td id="A5.T10.5.2.3" class="ltx_td ltx_align_left ltx_border_t">5.5</td>
</tr>
<tr id="A5.T10.5.3" class="ltx_tr">
<td id="A5.T10.5.3.1" class="ltx_td ltx_align_left ltx_border_t">+ DLM-synthetic</td>
<td id="A5.T10.5.3.2" class="ltx_td ltx_align_left ltx_border_t">2.4 (1.7)</td>
<td id="A5.T10.5.3.3" class="ltx_td ltx_align_left ltx_border_t">4.5 (3.7)</td>
</tr>
<tr id="A5.T10.5.4" class="ltx_tr">
<td id="A5.T10.5.4.1" class="ltx_td ltx_align_left ltx_border_bb">+ DLM-real</td>
<td id="A5.T10.5.4.2" class="ltx_td ltx_align_left ltx_border_bb">2.0 (1.5)</td>
<td id="A5.T10.5.4.3" class="ltx_td ltx_align_left ltx_border_bb">4.6 (4.1)</td>
</tr>
</table>
</figure>
<figure id="A5.T11" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 11: </span>Whisper and <span id="A5.T11.2.1" class="ltx_text ltx_font_italic">baseline ASR</span> WER (%) on audio generated from LibriSpeech dev sets by different TTS models.</figcaption>
<table id="A5.T11.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A5.T11.3.1" class="ltx_tr">
<td id="A5.T11.3.1.1" class="ltx_td ltx_align_left ltx_border_tt" rowspan="2"><span id="A5.T11.3.1.1.1" class="ltx_text">TTS System</span></td>
<td id="A5.T11.3.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">Whisper-small</td>
<td id="A5.T11.3.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">Baseline ASR</td>
</tr>
<tr id="A5.T11.3.2" class="ltx_tr">
<td id="A5.T11.3.2.1" class="ltx_td ltx_align_right">dev-clean</td>
<td id="A5.T11.3.2.2" class="ltx_td ltx_align_right">dev-other</td>
<td id="A5.T11.3.2.3" class="ltx_td ltx_align_right">dev-clean</td>
<td id="A5.T11.3.2.4" class="ltx_td ltx_align_right">dev-other</td>
</tr>
<tr id="A5.T11.3.3" class="ltx_tr">
<td id="A5.T11.3.3.1" class="ltx_td ltx_align_left ltx_border_t">Tacotron</td>
<td id="A5.T11.3.3.2" class="ltx_td ltx_align_right ltx_border_t">5.9</td>
<td id="A5.T11.3.3.3" class="ltx_td ltx_align_right ltx_border_t">5.2</td>
<td id="A5.T11.3.3.4" class="ltx_td ltx_align_right ltx_border_t">6.8</td>
<td id="A5.T11.3.3.5" class="ltx_td ltx_align_right ltx_border_t">6.2</td>
</tr>
<tr id="A5.T11.3.4" class="ltx_tr">
<td id="A5.T11.3.4.1" class="ltx_td ltx_align_left ltx_border_t">YourTTS</td>
<td id="A5.T11.3.4.2" class="ltx_td ltx_align_right ltx_border_t">8.2</td>
<td id="A5.T11.3.4.3" class="ltx_td ltx_align_right ltx_border_t">9.8</td>
<td id="A5.T11.3.4.4" class="ltx_td ltx_align_right ltx_border_t">8.6</td>
<td id="A5.T11.3.4.5" class="ltx_td ltx_align_right ltx_border_t">10.5</td>
</tr>
<tr id="A5.T11.3.5" class="ltx_tr">
<td id="A5.T11.3.5.1" class="ltx_td ltx_align_left ltx_border_bb">RichTTS</td>
<td id="A5.T11.3.5.2" class="ltx_td ltx_align_right ltx_border_bb">7.7</td>
<td id="A5.T11.3.5.3" class="ltx_td ltx_align_right ltx_border_bb">11.9</td>
<td id="A5.T11.3.5.4" class="ltx_td ltx_align_right ltx_border_bb">9.5</td>
<td id="A5.T11.3.5.5" class="ltx_td ltx_align_right ltx_border_bb">16.7</td>
</tr>
</table>
</figure>
<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">In TableÂ <a href="#A5.T9" title="Table 9 â€£ Appendix E Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> we show results for <span id="A5.p1.1.1" class="ltx_text ltx_font_italic">baseline ASR</span> when neural LM and DLM are applied on top for both LibriSpeech and TED-LIUM data.
Note, that for TED-LIUM evaluation both ASR model and neural LM / DLM are out-of-domain as trained on LibriSpeech audio data and LibriSpeech LM text corpus, emulating both domain shifts: in acoustics and linguistics.</p>
</div>
<div id="A5.p2" class="ltx_para">
<p id="A5.p2.1" class="ltx_p">Comparison between usage real audio and synthetic audio for DLM data generation is given in TableÂ <a href="#A5.T10" title="Table 10 â€£ Appendix E Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> and between TTS systems is given in TableÂ <a href="#A5.T11" title="Table 11 â€£ Appendix E Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.
For the later, we first generate audio data by TTS models from the text data of LibriSpeech <span id="A5.p2.1.1" class="ltx_text ltx_font_italic">dev-clean</span> and <span id="A5.p2.1.2" class="ltx_text ltx_font_italic">dev-other</span> and then evaluate quality of ASR models in WER on these audio data.</p>
</div>
<figure id="A5.T12" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 12: </span>Key differences between DLMs and spelling correction models from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</figcaption>
<table id="A5.T12.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A5.T12.1.1" class="ltx_tr">
<td id="A5.T12.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="A5.T12.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">DLM</td>
<td id="A5.T12.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">SC</td>
<td id="A5.T12.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Reference</td>
</tr>
<tr id="A5.T12.1.2" class="ltx_tr">
<td id="A5.T12.1.2.1" class="ltx_td ltx_align_left ltx_border_t">TTS</td>
<td id="A5.T12.1.2.2" class="ltx_td ltx_align_center ltx_border_t">multi-speaker</td>
<td id="A5.T12.1.2.3" class="ltx_td ltx_align_center ltx_border_t">single-speaker</td>
<td id="A5.T12.1.2.4" class="ltx_td ltx_align_center ltx_border_t">Table <a href="#S5.T7" title="Table 7 â€£ 5.4 Ablation: design proper error correction data distribution â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>
</td>
</tr>
<tr id="A5.T12.1.3" class="ltx_tr">
<td id="A5.T12.1.3.1" class="ltx_td ltx_align_left ltx_border_t">Augmentation</td>
<td id="A5.T12.1.3.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="A5.T12.1.3.2.1" class="ltx_text"></span> <span id="A5.T12.1.3.2.2" class="ltx_text">
<span id="A5.T12.1.3.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="A5.T12.1.3.2.2.1.1" class="ltx_tr">
<span id="A5.T12.1.3.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">text substitution</span></span>
<span id="A5.T12.1.3.2.2.1.2" class="ltx_tr">
<span id="A5.T12.1.3.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">frequency masking</span></span>
<span id="A5.T12.1.3.2.2.1.3" class="ltx_tr">
<span id="A5.T12.1.3.2.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">multiple TTS</span></span>
<span id="A5.T12.1.3.2.2.1.4" class="ltx_tr">
<span id="A5.T12.1.3.2.2.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center">real data</span></span>
</span></span><span id="A5.T12.1.3.2.3" class="ltx_text"></span></td>
<td id="A5.T12.1.3.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="A5.T12.1.3.3.1" class="ltx_text"></span> <span id="A5.T12.1.3.3.2" class="ltx_text">
<span id="A5.T12.1.3.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="A5.T12.1.3.3.2.1.1" class="ltx_tr">
<span id="A5.T12.1.3.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">reverberation</span></span>
<span id="A5.T12.1.3.3.2.1.2" class="ltx_tr">
<span id="A5.T12.1.3.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">background noise</span></span>
<span id="A5.T12.1.3.3.2.1.3" class="ltx_tr">
<span id="A5.T12.1.3.3.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">environmental noise</span></span>
</span></span><span id="A5.T12.1.3.3.3" class="ltx_text"></span></td>
<td id="A5.T12.1.3.4" class="ltx_td ltx_align_center ltx_border_t">Table <a href="#S5.T7" title="Table 7 â€£ 5.4 Ablation: design proper error correction data distribution â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>
</td>
</tr>
<tr id="A5.T12.1.4" class="ltx_tr">
<td id="A5.T12.1.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Decoding</td>
<td id="A5.T12.1.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">DSR-decoding</td>
<td id="A5.T12.1.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">LM-rescoring</td>
<td id="A5.T12.1.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Table <a href="#S5.T1" title="Table 1 â€£ 5.1 Pushing the limits: DLM achieves new SOTA WER on LibriSpeech â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>
</td>
</tr>
</table>
</figure>
<figure id="A5.T13" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 13: </span>Oracle WER (%) in the beam for DLM (484M) and neLM (500M) reported in Table <a href="#S5.T1" title="Table 1 â€£ 5.1 Pushing the limits: DLM achieves new SOTA WER on LibriSpeech â€£ 5 Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a></figcaption>
<table id="A5.T13.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A5.T13.1.1" class="ltx_tr">
<td id="A5.T13.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Model</td>
<td id="A5.T13.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">dev-clean</td>
<td id="A5.T13.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">dev-other</td>
<td id="A5.T13.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">test-clean</td>
<td id="A5.T13.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">test-other</td>
</tr>
<tr id="A5.T13.1.2" class="ltx_tr">
<td id="A5.T13.1.2.1" class="ltx_td ltx_align_center ltx_border_t">DLM</td>
<td id="A5.T13.1.2.2" class="ltx_td ltx_align_center ltx_border_t">0.36</td>
<td id="A5.T13.1.2.3" class="ltx_td ltx_align_center ltx_border_t">1.29</td>
<td id="A5.T13.1.2.4" class="ltx_td ltx_align_center ltx_border_t">0.46</td>
<td id="A5.T13.1.2.5" class="ltx_td ltx_align_center ltx_border_t">1.33</td>
</tr>
<tr id="A5.T13.1.3" class="ltx_tr">
<td id="A5.T13.1.3.1" class="ltx_td ltx_align_center ltx_border_bb">neLM</td>
<td id="A5.T13.1.3.2" class="ltx_td ltx_align_center ltx_border_bb">0.24</td>
<td id="A5.T13.1.3.3" class="ltx_td ltx_align_center ltx_border_bb">1.52</td>
<td id="A5.T13.1.3.4" class="ltx_td ltx_align_center ltx_border_bb">0.69</td>
<td id="A5.T13.1.3.5" class="ltx_td ltx_align_center ltx_border_bb">1.60</td>
</tr>
</table>
</figure>
<section id="A5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Other empirical findings</h3>

<section id="A5.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Encoder and decoder layers</h4>

<div id="A5.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="A5.SS1.SSS0.Px1.p1.1" class="ltx_p">We investigate what part of the model is more important in the error correction model: encoder or decoder. With decreasing the number of encoder layers and increasing the number of decoder layers while the total number of layers is constant the WER degrades, thus showing that DLMs should have deep encoders and shallow decoders.</p>
</div>
</section>
<section id="A5.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Role of tokenization</h4>

<div id="A5.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="A5.SS1.SSS0.Px2.p1.1" class="ltx_p">As preliminary experiments we compared word-piece tokenization vs character one for DLMs training. However, word-piece one underperforms a lot character-based, thus all final results are reported with characters tokenization. This might be due to that our ASR is char-based so its errors are mostly on character level.</p>
</div>
</section>
<section id="A5.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Applicability to ASR trained on 100h</h4>

<div id="A5.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="A5.SS1.SSS0.Px3.p1.1" class="ltx_p">We also conducted similar experiments on ASR trained on LibriSpeech 100h which has 6% (18%) WER on <span id="A5.SS1.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">dev-clean (other)</span>. We observe over 50% WER reduction by simply applying a 180M DLM without the need of any design of the training data. Error correction for weaker ASR is overall much easier to obtain improvements comparing with an already powerful ASR.</p>
</div>
</section>
<section id="A5.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Oracle WER in the beam</h4>

<div id="A5.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="A5.SS1.SSS0.Px4.p1.1" class="ltx_p">Oracle WER in the beam for our best DLM and neLM is reported in Table <a href="#A5.T13" title="Table 13 â€£ Appendix E Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
</section>
</section>
<section id="A5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2 </span>Pushing the limits of DLM: comparison with prior works</h3>

<div id="A5.SS2.p1" class="ltx_para">
<p id="A5.SS2.p1.1" class="ltx_p">Using TTS data for ASR error correction is not new, as prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> has made such attempts to Listen, Attend and Spell (LAS) models.
However, it is worth noting that the improvement from their spelling correction (SC) model is marginal, even when applying to a relatively weak ASR system.
Besides some fundamental differences like the ASR type and tokenization, we outline several key ingredients in TableÂ <a href="#A5.T12" title="Table 12 â€£ Appendix E Results â€£ Denoising LM: Pushing the Limits of Error Correction Models for Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> that distinguish us from this prior work and <span id="A5.SS2.p1.1.1" class="ltx_text ltx_font_italic">are crucial to push the limits of DLMs to outperform neural LMs.</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.15215" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.15216" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.15216">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.15216" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.15217" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 18:58:31 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
