<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.11849] Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation</title><meta property="og:description" content="The rapid advancement of large language models (LLMs) has significantly propelled the development of text-based chatbots, demonstrating their capability to engage in coherent and contextually relevant dialogues. Howeve…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.11849">

<!--Generated on Thu Sep  5 13:31:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yinghao Aaron Li &amp; Xilin Jiang  
<br class="ltx_break">Department of Electrical Engineering
<br class="ltx_break">Columbia University
<br class="ltx_break">New York, NY, USA 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">{yl4579, xj2289}@columbia.edu</span> 
<br class="ltx_break">&amp;Jordan Darefsky &amp; Ge Zhu 
<br class="ltx_break">Department of Electrical and Computer Engineering 
<br class="ltx_break">University of Rochester 
<br class="ltx_break">Rochester, NY, USA 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">{jdarefsk, ge.zhu}@rochester.edu</span> 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_ERROR undefined">\AND</span>Nima Mesgarani 
<br class="ltx_break">Department of Electrical Engineering 
<br class="ltx_break">Columbia University 
<br class="ltx_break">New York, NY, USA 
<br class="ltx_break"><span id="id4.4.id4" class="ltx_text ltx_font_typewriter">nima@ee.columbia.edu </span>
</span><span class="ltx_author_notes">These authors contributed equally.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">The rapid advancement of large language models (LLMs) has significantly propelled the development of text-based chatbots, demonstrating their capability to engage in coherent and contextually relevant dialogues. However, extending these advancements to enable end-to-end speech-to-speech conversation bots remains a formidable challenge, primarily due to the extensive dataset and computational resources required. The conventional approach of cascading automatic speech recognition (ASR), LLM, and text-to-speech (TTS) models in a pipeline, while effective, suffers from unnatural prosody because it lacks direct interactions between the input audio and its transcribed text and the output audio. These systems are also limited by their inherent latency from the ASR process for real-time applications. This paper introduces Style-Talker, an innovative framework that fine-tunes an audio LLM alongside a style-based TTS model for fast spoken dialog generation. Style-Talker takes user input audio and uses transcribed chat history and speech styles to generate both the speaking style and text for the response. Subsequently, the TTS model synthesizes the speech, which is then played back to the user. While the response speech is being played, the input speech undergoes ASR processing to extract the transcription and speaking style, serving as the context for the ensuing dialogue turn. This novel pipeline accelerates the traditional cascade ASR-LLM-TTS systems while integrating rich paralinguistic information from input speech. Our experimental results show that Style-Talker significantly outperforms the conventional cascade and speech-to-speech baselines in terms of both dialogue naturalness and coherence while being more than 50% faster. The demo and code are available at <a target="_blank" href="https://styletalker.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://styletalker.github.io/</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Conversing with a machine as if it were talking to a human has always been a dream for many computer scientists. Traditional spoken dialog systems (SDS) have relied on a multi-component architecture encompassing automatic speech recognition (ASR), language comprehension, dialog managers for turn-taking, response generation, and text-to-speech (TTS) synthesis. This complex pipeline enables interactions between humans and machines by converting input speech to text, understanding and managing the dialog’s context, generating appropriate responses, and ultimately synthesizing these responses back into speech <cite class="ltx_cite ltx_citemacro_citep">(Jokinen &amp; McTear, <a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>. Despite its effectiveness, this traditional setup has been challenged by the recent seismic shifts brought about by deep learning innovations.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Recent advances in deep learning, particularly in the development of large language models (LLMs), have dramatically simplified the SDS architecture dramatically <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>. By integrating language comprehension, turn-taking, and response generation into a single LLM, the dialog system pipeline has been condensed into a more streamlined ASR-LLM-TTS process <cite class="ltx_cite ltx_citemacro_citep">(Mitsui et al., <a href="#bib.bib23" title="" class="ltx_ref">2023</a>; Yi et al., <a href="#bib.bib30" title="" class="ltx_ref">2024</a>)</cite>. Furthermore, cutting-edge approaches now aim to achieve more direct end-to-end (E2E) speech-to-speech generation by treating speech as language tokens and modeling it similarly to LLMs, thereby eliminating the need for a separate ASR and TTS process <cite class="ltx_cite ltx_citemacro_citep">(Lakhotia et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>; Nguyen et al., <a href="#bib.bib25" title="" class="ltx_ref">2023</a>; Nachmani et al., <a href="#bib.bib24" title="" class="ltx_ref">2023</a>; Kim et al., <a href="#bib.bib13" title="" class="ltx_ref">2024</a>)</cite>. This paradigm shift promises to further streamline dialog systems, making them more efficient and versatile.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">However, both the ASR-LLM-TTS pipeline and speech LLM approaches exhibit significant limitations. The former struggles with capturing the emotional nuances of the input audio, failing to integrate this crucial aspect of human communication into the dialog process. While some research has attempted to address this by understanding input emotions <cite class="ltx_cite ltx_citemacro_citep">(Xue et al., <a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite>, the synthesized response still remains emotionally disconnected from the input, as the TTS component operates independently of the ASR and LLM. Another very recent work <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib21" title="" class="ltx_ref">2024</a>)</cite> tries to incorporate speaking style in the LLM output for subsequent TTS synthesis. Still, the styles are derived from pre-defined categories, requiring extensive labeling works from more powerful LLM such as GPT-4, hindering its applications on in-the-wild datasets. Moreover, the autoregressive decoding required for ASR slows the entire system, impeding its applicability in real-time scenarios. On the other hand, the end-to-end speech LLM approach, despite its theoretical advantages, faces practical challenges in data acquisition and processing speed <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al., <a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>, making it less feasible for real-time applications due to the extensive computational resources required to generate speech units.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">In this paper, we introduce Style-Talker, a novel SDS framework designed to overcome these challenges. Style-Talker innovatively integrates input audio directly with transcribed speech context of previous turns from the ASR model and its corresponding speaking style from a style-based TTS model. This approach not only preserves the prosodic (style) and semantic (text) aspects of speech but also significantly enhances the system’s efficiency by eliminating the ASR component prior to the LLM in the response generation process. By training the audio LLM to output both the response text and its associated speech style, Style-Talker enables the synthesis of speech that accurately reflects the intended emotional and stylistic nuances. Concurrently, it processes the current input audio for transcription and style analysis, using this information to inform the next dialog turn. This seamless integration and the elimination of intermediate ASR processing dramatically improve the real-time factor (RTF) of the system, making Style-Talker nearly twice faster than the ASR-LLM-TTS cascade baseline with Whisper <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">large-v2</span> model <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite> and more than four times faster than a recent speech-to-speech baseline, SpeechGPT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib31" title="" class="ltx_ref">2023</a>)</cite>. Moreover, our system outperforms both the cascade and speech-to-speech baselines in generating responses that are not only more natural and coherent but also emotionally congruent with the dialog context. Since our system does not require manual labeling, it can also be applied directly to datasets mined in the wild, greatly diversifying its applicability and efficiency.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2408.11849/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="209" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An overview of SDS with a comparison between the conventional cascaded system and Style-Talker. The cascaded system has three steps: input speech transcription (ASR), response text generation (LLM), and response speech synthesis (TTS). Style-Talker adopts an audio LLM to merge the first two steps and generate a response text and corresponding style directly, preserving the content prosody while achieving generation efficiency.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Recent advancements in spoken dialog generation have primarily followed two distinct approaches: the text-based approach and the end-to-end (E2E) speech-to-speech approach. The text-based approach generates a text response that is subsequently converted into speech via text-to-speech (TTS) synthesis. In contrast, the E2E approach aims for direct speech output, bypassing the intermediate step for text generation altogether. Each of these methods, while effective, presents unique challenges and limitations that have spurred ongoing research efforts to improve the efficiency and naturalness of dialog systems.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Text-Based Approaches and Paralinguistic Decoupling</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">The text-based approach, despite its widespread adoption, is inherently limited by the fundamental decoupling from the paralinguistic features of the input and output speech, such as tones, emotions, and intonations. Recent efforts in text-based dialog systems have sought to bridge the gap between speech content and its underlying paralinguistic information. For example, <cite class="ltx_cite ltx_citemacro_cite">Xue et al. (<a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite> employed a speech encoder to inform LLMs about paralinguistic cues for generating emotionally congruent responses. Further, <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib21" title="" class="ltx_ref">2024</a>)</cite> introduced multimodal approaches that combine text and speech inputs, or text with emotion vectors, to better interpret these cues. Yet, these methods require speech transcription into text, introducing latency that impacts processing speed. Contrary to these, Style-Talker directly processes speech inputs, circumventing transcription delays and achieving significant speed improvements without sacrificing prosodic or conversational coherence.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p">Moreover, efforts have been made to enhance the paralinguistic aspects of generated responses by incorporating emotion labels into text responses <cite class="ltx_cite ltx_citemacro_citep">(Varshney et al., <a href="#bib.bib28" title="" class="ltx_ref">2021</a>; Liu et al., <a href="#bib.bib22" title="" class="ltx_ref">2021</a>; Lin et al., <a href="#bib.bib20" title="" class="ltx_ref">2023</a>; <a href="#bib.bib21" title="" class="ltx_ref">2024</a>)</cite>. These works, however, rely on predefined text labels for each utterance, complicating application to diverse and unstructured in-the-wild datasets. Style-Talker, in contrast, adopts a self-supervised approach with a style-based TTS model, learning speech styles directly optimized for speech synthesis. This method mitigates limitations of predefined emotion categories, offering a more flexible and end-to-end solution.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>End-to-End Speech-to-Speech Generation</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">E2E models represent a paradigm shift in spoken dialog systems by generating speech output directly from speech input, modeling speech in an autoregressive fashion, which has been used both for dialog generation <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a href="#bib.bib25" title="" class="ltx_ref">2023</a>); Kim et al. (<a href="#bib.bib13" title="" class="ltx_ref">2024</a>)</cite> and translation <cite class="ltx_cite ltx_citemacro_cite">Duquenne et al. (<a href="#bib.bib9" title="" class="ltx_ref">2023</a>); Barrault et al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>. The discrete dGSLM framework <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al., <a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite>, for example, treats speech as discrete units akin to text tokens, achieving emotional and prosodic coherence. Spectron <cite class="ltx_cite ltx_citemacro_citep">(Nachmani et al., <a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite> extends this E2E framework by directly generating mel-spectrograms instead of discreet tokens. However, due to data limitations, these E2E dialog systems trained from scratch on speech corpora often lack semantic coherence in dialog responses. SpeechGPT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib31" title="" class="ltx_ref">2023</a>; <a href="#bib.bib32" title="" class="ltx_ref">2024</a>)</cite> and USDM <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a href="#bib.bib13" title="" class="ltx_ref">2024</a>)</cite> attempt to address this by integrating text-based LLMs with speech datasets, though challenges in achieving human-like speech quality remain, alongside the complexities of training models across multiple modalities. Unlike the E2E approach, Style-Talker fine-tunes an audio language model and a style-based TTS model to generate semantically and paralinguistically coherent responses. This approach not only enhances dialog naturalness but also offers substantial improvements in processing speed, marking a new advancement for dialog generation technology.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2408.11849/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="240" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Model components and processing pipelines of Style-Talker for response generation. Audio <math id="S2.F2.7.m1.1" class="ltx_Math" alttext="\bm{x}_{n}" display="inline"><semantics id="S2.F2.7.m1.1b"><msub id="S2.F2.7.m1.1.1" xref="S2.F2.7.m1.1.1.cmml"><mi id="S2.F2.7.m1.1.1.2" xref="S2.F2.7.m1.1.1.2.cmml">𝒙</mi><mi id="S2.F2.7.m1.1.1.3" xref="S2.F2.7.m1.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.7.m1.1c"><apply id="S2.F2.7.m1.1.1.cmml" xref="S2.F2.7.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.7.m1.1.1.1.cmml" xref="S2.F2.7.m1.1.1">subscript</csymbol><ci id="S2.F2.7.m1.1.1.2.cmml" xref="S2.F2.7.m1.1.1.2">𝒙</ci><ci id="S2.F2.7.m1.1.1.3.cmml" xref="S2.F2.7.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.7.m1.1d">\bm{x}_{n}</annotation></semantics></math> from the incoming speaker, a reference and past speaker styles in conversation, and transcriptions from previous rounds (<math id="S2.F2.8.m2.1" class="ltx_Math" alttext="\bm{c}_{n}" display="inline"><semantics id="S2.F2.8.m2.1b"><msub id="S2.F2.8.m2.1.1" xref="S2.F2.8.m2.1.1.cmml"><mi id="S2.F2.8.m2.1.1.2" xref="S2.F2.8.m2.1.1.2.cmml">𝒄</mi><mi id="S2.F2.8.m2.1.1.3" xref="S2.F2.8.m2.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.8.m2.1c"><apply id="S2.F2.8.m2.1.1.cmml" xref="S2.F2.8.m2.1.1"><csymbol cd="ambiguous" id="S2.F2.8.m2.1.1.1.cmml" xref="S2.F2.8.m2.1.1">subscript</csymbol><ci id="S2.F2.8.m2.1.1.2.cmml" xref="S2.F2.8.m2.1.1.2">𝒄</ci><ci id="S2.F2.8.m2.1.1.3.cmml" xref="S2.F2.8.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.8.m2.1d">\bm{c}_{n}</annotation></semantics></math>) are all embedded into the same space by the audio encoder, the style projection <math id="S2.F2.9.m3.1" class="ltx_Math" alttext="\mathcal{P}_{\text{in}}" display="inline"><semantics id="S2.F2.9.m3.1b"><msub id="S2.F2.9.m3.1.1" xref="S2.F2.9.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.F2.9.m3.1.1.2" xref="S2.F2.9.m3.1.1.2.cmml">𝒫</mi><mtext id="S2.F2.9.m3.1.1.3" xref="S2.F2.9.m3.1.1.3a.cmml">in</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.F2.9.m3.1c"><apply id="S2.F2.9.m3.1.1.cmml" xref="S2.F2.9.m3.1.1"><csymbol cd="ambiguous" id="S2.F2.9.m3.1.1.1.cmml" xref="S2.F2.9.m3.1.1">subscript</csymbol><ci id="S2.F2.9.m3.1.1.2.cmml" xref="S2.F2.9.m3.1.1.2">𝒫</ci><ci id="S2.F2.9.m3.1.1.3a.cmml" xref="S2.F2.9.m3.1.1.3"><mtext mathsize="70%" id="S2.F2.9.m3.1.1.3.cmml" xref="S2.F2.9.m3.1.1.3">in</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.9.m3.1d">\mathcal{P}_{\text{in}}</annotation></semantics></math>, and the text tokenizer and embedder (not shown), respectively. They are jointly processed by an LLM QwenLM to generate the response text <math id="S2.F2.10.m4.1" class="ltx_Math" alttext="\bm{t}_{n+1}" display="inline"><semantics id="S2.F2.10.m4.1b"><msub id="S2.F2.10.m4.1.1" xref="S2.F2.10.m4.1.1.cmml"><mi id="S2.F2.10.m4.1.1.2" xref="S2.F2.10.m4.1.1.2.cmml">𝒕</mi><mrow id="S2.F2.10.m4.1.1.3" xref="S2.F2.10.m4.1.1.3.cmml"><mi id="S2.F2.10.m4.1.1.3.2" xref="S2.F2.10.m4.1.1.3.2.cmml">n</mi><mo id="S2.F2.10.m4.1.1.3.1" xref="S2.F2.10.m4.1.1.3.1.cmml">+</mo><mn id="S2.F2.10.m4.1.1.3.3" xref="S2.F2.10.m4.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.F2.10.m4.1c"><apply id="S2.F2.10.m4.1.1.cmml" xref="S2.F2.10.m4.1.1"><csymbol cd="ambiguous" id="S2.F2.10.m4.1.1.1.cmml" xref="S2.F2.10.m4.1.1">subscript</csymbol><ci id="S2.F2.10.m4.1.1.2.cmml" xref="S2.F2.10.m4.1.1.2">𝒕</ci><apply id="S2.F2.10.m4.1.1.3.cmml" xref="S2.F2.10.m4.1.1.3"><plus id="S2.F2.10.m4.1.1.3.1.cmml" xref="S2.F2.10.m4.1.1.3.1"></plus><ci id="S2.F2.10.m4.1.1.3.2.cmml" xref="S2.F2.10.m4.1.1.3.2">𝑛</ci><cn type="integer" id="S2.F2.10.m4.1.1.3.3.cmml" xref="S2.F2.10.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.10.m4.1d">\bm{t}_{n+1}</annotation></semantics></math> and style <math id="S2.F2.11.m5.1" class="ltx_Math" alttext="\bm{s}_{n+1}" display="inline"><semantics id="S2.F2.11.m5.1b"><msub id="S2.F2.11.m5.1.1" xref="S2.F2.11.m5.1.1.cmml"><mi id="S2.F2.11.m5.1.1.2" xref="S2.F2.11.m5.1.1.2.cmml">𝒔</mi><mrow id="S2.F2.11.m5.1.1.3" xref="S2.F2.11.m5.1.1.3.cmml"><mi id="S2.F2.11.m5.1.1.3.2" xref="S2.F2.11.m5.1.1.3.2.cmml">n</mi><mo id="S2.F2.11.m5.1.1.3.1" xref="S2.F2.11.m5.1.1.3.1.cmml">+</mo><mn id="S2.F2.11.m5.1.1.3.3" xref="S2.F2.11.m5.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.F2.11.m5.1c"><apply id="S2.F2.11.m5.1.1.cmml" xref="S2.F2.11.m5.1.1"><csymbol cd="ambiguous" id="S2.F2.11.m5.1.1.1.cmml" xref="S2.F2.11.m5.1.1">subscript</csymbol><ci id="S2.F2.11.m5.1.1.2.cmml" xref="S2.F2.11.m5.1.1.2">𝒔</ci><apply id="S2.F2.11.m5.1.1.3.cmml" xref="S2.F2.11.m5.1.1.3"><plus id="S2.F2.11.m5.1.1.3.1.cmml" xref="S2.F2.11.m5.1.1.3.1"></plus><ci id="S2.F2.11.m5.1.1.3.2.cmml" xref="S2.F2.11.m5.1.1.3.2">𝑛</ci><cn type="integer" id="S2.F2.11.m5.1.1.3.3.cmml" xref="S2.F2.11.m5.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.11.m5.1d">\bm{s}_{n+1}</annotation></semantics></math> for StyleTTS 2 to synthesize a response speech <math id="S2.F2.12.m6.1" class="ltx_Math" alttext="\bm{x}_{n+1}" display="inline"><semantics id="S2.F2.12.m6.1b"><msub id="S2.F2.12.m6.1.1" xref="S2.F2.12.m6.1.1.cmml"><mi id="S2.F2.12.m6.1.1.2" xref="S2.F2.12.m6.1.1.2.cmml">𝒙</mi><mrow id="S2.F2.12.m6.1.1.3" xref="S2.F2.12.m6.1.1.3.cmml"><mi id="S2.F2.12.m6.1.1.3.2" xref="S2.F2.12.m6.1.1.3.2.cmml">n</mi><mo id="S2.F2.12.m6.1.1.3.1" xref="S2.F2.12.m6.1.1.3.1.cmml">+</mo><mn id="S2.F2.12.m6.1.1.3.3" xref="S2.F2.12.m6.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.F2.12.m6.1c"><apply id="S2.F2.12.m6.1.1.cmml" xref="S2.F2.12.m6.1.1"><csymbol cd="ambiguous" id="S2.F2.12.m6.1.1.1.cmml" xref="S2.F2.12.m6.1.1">subscript</csymbol><ci id="S2.F2.12.m6.1.1.2.cmml" xref="S2.F2.12.m6.1.1.2">𝒙</ci><apply id="S2.F2.12.m6.1.1.3.cmml" xref="S2.F2.12.m6.1.1.3"><plus id="S2.F2.12.m6.1.1.3.1.cmml" xref="S2.F2.12.m6.1.1.3.1"></plus><ci id="S2.F2.12.m6.1.1.3.2.cmml" xref="S2.F2.12.m6.1.1.3.2">𝑛</ci><cn type="integer" id="S2.F2.12.m6.1.1.3.3.cmml" xref="S2.F2.12.m6.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.12.m6.1d">\bm{x}_{n+1}</annotation></semantics></math>.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Style-Talker</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">Style-Talker is a spoken dialog system (SDS) designed for seamless, real-time speech-to-speech interaction by integrating two major components: a multi-modal audio large language model (LLM) for spoken language understanding (SLU) and text-style response generation, and a style-based text-to-speech (TTS) model for synthesizing speech that mirrors the specific speaking style indicated by the style vector generated from the language model. This innovative architecture enables Style-Talker to produce responses that are not only contextually relevant but also paralinguistically congruent with the previous speaker’s speaking style, significantly enhancing the naturalness and coherence of dialogues.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">The training of Style-Talker involves a two-stage process. Initially, the StyleTTS 2 model <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib18" title="" class="ltx_ref">2024</a>)</cite>, a state-of-the-art style-based TTS model, is fine-tuned on a dialog dataset wherein speakers’ utterances are diarized or segmented into individual speech sentences. The model learns the nuances of speaking styles in a self-supervised manner, without the need for explicit style annotations, with which the speaking styles are extracted by the style encoder into a fixed-length style vector. Subsequently, an audio LLM, such as Qwen-Audio <cite class="ltx_cite ltx_citemacro_citep">(Chu et al., <a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>, is fine-tuned to generate both the response text and its associated speaking style, utilizing the previously transcribed text and style from each utterance as contextual input. This phase is critical for aligning the generated responses with the conversational context through texts and its underlying paralinguistic features via styles, thus ensuring coherence and style consistency in dialogues.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.6" class="ltx_p">During inference, we provide a reference style vector of the response speaker as a target speaker embedding at the beginning of the input prompt. For simplicity, we do not model turn-taking explicitly. The system instead infers the end of a speaker’s turn based on a predetermined time threshold. If the speaker’s silence exceeds this threshold, the system proceeds to process the input speech.
For each incoming utterance <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="\bm{x}_{n}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">𝒙</mi><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">𝒙</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\bm{x}_{n}</annotation></semantics></math>, the fine-tuned audio LLM takes both the speech input <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="\bm{x}_{n}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><msub id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">𝒙</mi><mi id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">𝒙</ci><ci id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\bm{x}_{n}</annotation></semantics></math> and its preceding context <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="\bm{c}_{n-1}" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><msub id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">𝒄</mi><mrow id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml"><mi id="S3.SS1.p3.3.m3.1.1.3.2" xref="S3.SS1.p3.3.m3.1.1.3.2.cmml">n</mi><mo id="S3.SS1.p3.3.m3.1.1.3.1" xref="S3.SS1.p3.3.m3.1.1.3.1.cmml">−</mo><mn id="S3.SS1.p3.3.m3.1.1.3.3" xref="S3.SS1.p3.3.m3.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">𝒄</ci><apply id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3"><minus id="S3.SS1.p3.3.m3.1.1.3.1.cmml" xref="S3.SS1.p3.3.m3.1.1.3.1"></minus><ci id="S3.SS1.p3.3.m3.1.1.3.2.cmml" xref="S3.SS1.p3.3.m3.1.1.3.2">𝑛</ci><cn type="integer" id="S3.SS1.p3.3.m3.1.1.3.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\bm{c}_{n-1}</annotation></semantics></math> as detailed in Section <a href="#S3.SS1.SSS3" title="3.1.3 Conversation Context ‣ 3.1 Style-Talker ‣ 3 Methods ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.3</span></a>. The model then generates the response text <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="\bm{t}_{n+1}" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><msub id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml"><mi id="S3.SS1.p3.4.m4.1.1.2" xref="S3.SS1.p3.4.m4.1.1.2.cmml">𝒕</mi><mrow id="S3.SS1.p3.4.m4.1.1.3" xref="S3.SS1.p3.4.m4.1.1.3.cmml"><mi id="S3.SS1.p3.4.m4.1.1.3.2" xref="S3.SS1.p3.4.m4.1.1.3.2.cmml">n</mi><mo id="S3.SS1.p3.4.m4.1.1.3.1" xref="S3.SS1.p3.4.m4.1.1.3.1.cmml">+</mo><mn id="S3.SS1.p3.4.m4.1.1.3.3" xref="S3.SS1.p3.4.m4.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2">𝒕</ci><apply id="S3.SS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3"><plus id="S3.SS1.p3.4.m4.1.1.3.1.cmml" xref="S3.SS1.p3.4.m4.1.1.3.1"></plus><ci id="S3.SS1.p3.4.m4.1.1.3.2.cmml" xref="S3.SS1.p3.4.m4.1.1.3.2">𝑛</ci><cn type="integer" id="S3.SS1.p3.4.m4.1.1.3.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">\bm{t}_{n+1}</annotation></semantics></math> and the corresponding speaking style <math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="\bm{s}_{n+1}" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><msub id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml"><mi id="S3.SS1.p3.5.m5.1.1.2" xref="S3.SS1.p3.5.m5.1.1.2.cmml">𝒔</mi><mrow id="S3.SS1.p3.5.m5.1.1.3" xref="S3.SS1.p3.5.m5.1.1.3.cmml"><mi id="S3.SS1.p3.5.m5.1.1.3.2" xref="S3.SS1.p3.5.m5.1.1.3.2.cmml">n</mi><mo id="S3.SS1.p3.5.m5.1.1.3.1" xref="S3.SS1.p3.5.m5.1.1.3.1.cmml">+</mo><mn id="S3.SS1.p3.5.m5.1.1.3.3" xref="S3.SS1.p3.5.m5.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><apply id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.5.m5.1.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p3.5.m5.1.1.2.cmml" xref="S3.SS1.p3.5.m5.1.1.2">𝒔</ci><apply id="S3.SS1.p3.5.m5.1.1.3.cmml" xref="S3.SS1.p3.5.m5.1.1.3"><plus id="S3.SS1.p3.5.m5.1.1.3.1.cmml" xref="S3.SS1.p3.5.m5.1.1.3.1"></plus><ci id="S3.SS1.p3.5.m5.1.1.3.2.cmml" xref="S3.SS1.p3.5.m5.1.1.3.2">𝑛</ci><cn type="integer" id="S3.SS1.p3.5.m5.1.1.3.3.cmml" xref="S3.SS1.p3.5.m5.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">\bm{s}_{n+1}</annotation></semantics></math>. These outputs are fed into the TTS model, which synthesizes the speech <math id="S3.SS1.p3.6.m6.1" class="ltx_Math" alttext="\bm{x}_{n+1}" display="inline"><semantics id="S3.SS1.p3.6.m6.1a"><msub id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml"><mi id="S3.SS1.p3.6.m6.1.1.2" xref="S3.SS1.p3.6.m6.1.1.2.cmml">𝒙</mi><mrow id="S3.SS1.p3.6.m6.1.1.3" xref="S3.SS1.p3.6.m6.1.1.3.cmml"><mi id="S3.SS1.p3.6.m6.1.1.3.2" xref="S3.SS1.p3.6.m6.1.1.3.2.cmml">n</mi><mo id="S3.SS1.p3.6.m6.1.1.3.1" xref="S3.SS1.p3.6.m6.1.1.3.1.cmml">+</mo><mn id="S3.SS1.p3.6.m6.1.1.3.3" xref="S3.SS1.p3.6.m6.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.1b"><apply id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.1.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p3.6.m6.1.1.2.cmml" xref="S3.SS1.p3.6.m6.1.1.2">𝒙</ci><apply id="S3.SS1.p3.6.m6.1.1.3.cmml" xref="S3.SS1.p3.6.m6.1.1.3"><plus id="S3.SS1.p3.6.m6.1.1.3.1.cmml" xref="S3.SS1.p3.6.m6.1.1.3.1"></plus><ci id="S3.SS1.p3.6.m6.1.1.3.2.cmml" xref="S3.SS1.p3.6.m6.1.1.3.2">𝑛</ci><cn type="integer" id="S3.SS1.p3.6.m6.1.1.3.3.cmml" xref="S3.SS1.p3.6.m6.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.1c">\bm{x}_{n+1}</annotation></semantics></math> that reflects both the content and style of the response, playing it back to the user.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.5" class="ltx_p">Simultaneously, as the response speech is being played back, the system employs an ASR model to transcribe the input speech <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="\bm{x}_{n}" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><msub id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">𝒙</mi><mi id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2">𝒙</ci><ci id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">\bm{x}_{n}</annotation></semantics></math> into text <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="\bm{t}_{n}" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><msub id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml"><mi id="S3.SS1.p4.2.m2.1.1.2" xref="S3.SS1.p4.2.m2.1.1.2.cmml">𝒕</mi><mi id="S3.SS1.p4.2.m2.1.1.3" xref="S3.SS1.p4.2.m2.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><apply id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.2.m2.1.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p4.2.m2.1.1.2.cmml" xref="S3.SS1.p4.2.m2.1.1.2">𝒕</ci><ci id="S3.SS1.p4.2.m2.1.1.3.cmml" xref="S3.SS1.p4.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">\bm{t}_{n}</annotation></semantics></math>, and a style encoder from StyleTTS 2 to extract the speaking style into a vector <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="\bm{s}_{n}" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><msub id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml"><mi id="S3.SS1.p4.3.m3.1.1.2" xref="S3.SS1.p4.3.m3.1.1.2.cmml">𝒔</mi><mi id="S3.SS1.p4.3.m3.1.1.3" xref="S3.SS1.p4.3.m3.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><apply id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.3.m3.1.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p4.3.m3.1.1.2.cmml" xref="S3.SS1.p4.3.m3.1.1.2">𝒔</ci><ci id="S3.SS1.p4.3.m3.1.1.3.cmml" xref="S3.SS1.p4.3.m3.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">\bm{s}_{n}</annotation></semantics></math>. These elements are then aggregated into the contextual data <math id="S3.SS1.p4.4.m4.2" class="ltx_Math" alttext="\bm{c}_{n}=\{\bm{t}_{i},\bm{s}_{i}\}_{i=1}^{n}" display="inline"><semantics id="S3.SS1.p4.4.m4.2a"><mrow id="S3.SS1.p4.4.m4.2.2" xref="S3.SS1.p4.4.m4.2.2.cmml"><msub id="S3.SS1.p4.4.m4.2.2.4" xref="S3.SS1.p4.4.m4.2.2.4.cmml"><mi id="S3.SS1.p4.4.m4.2.2.4.2" xref="S3.SS1.p4.4.m4.2.2.4.2.cmml">𝒄</mi><mi id="S3.SS1.p4.4.m4.2.2.4.3" xref="S3.SS1.p4.4.m4.2.2.4.3.cmml">n</mi></msub><mo id="S3.SS1.p4.4.m4.2.2.3" xref="S3.SS1.p4.4.m4.2.2.3.cmml">=</mo><msubsup id="S3.SS1.p4.4.m4.2.2.2" xref="S3.SS1.p4.4.m4.2.2.2.cmml"><mrow id="S3.SS1.p4.4.m4.2.2.2.2.2.2" xref="S3.SS1.p4.4.m4.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS1.p4.4.m4.2.2.2.2.2.2.3" xref="S3.SS1.p4.4.m4.2.2.2.2.2.3.cmml">{</mo><msub id="S3.SS1.p4.4.m4.1.1.1.1.1.1.1" xref="S3.SS1.p4.4.m4.1.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p4.4.m4.1.1.1.1.1.1.1.2" xref="S3.SS1.p4.4.m4.1.1.1.1.1.1.1.2.cmml">𝒕</mi><mi id="S3.SS1.p4.4.m4.1.1.1.1.1.1.1.3" xref="S3.SS1.p4.4.m4.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p4.4.m4.2.2.2.2.2.2.4" xref="S3.SS1.p4.4.m4.2.2.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p4.4.m4.2.2.2.2.2.2.2" xref="S3.SS1.p4.4.m4.2.2.2.2.2.2.2.cmml"><mi id="S3.SS1.p4.4.m4.2.2.2.2.2.2.2.2" xref="S3.SS1.p4.4.m4.2.2.2.2.2.2.2.2.cmml">𝒔</mi><mi id="S3.SS1.p4.4.m4.2.2.2.2.2.2.2.3" xref="S3.SS1.p4.4.m4.2.2.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS1.p4.4.m4.2.2.2.2.2.2.5" xref="S3.SS1.p4.4.m4.2.2.2.2.2.3.cmml">}</mo></mrow><mrow id="S3.SS1.p4.4.m4.2.2.2.2.4" xref="S3.SS1.p4.4.m4.2.2.2.2.4.cmml"><mi id="S3.SS1.p4.4.m4.2.2.2.2.4.2" xref="S3.SS1.p4.4.m4.2.2.2.2.4.2.cmml">i</mi><mo id="S3.SS1.p4.4.m4.2.2.2.2.4.1" xref="S3.SS1.p4.4.m4.2.2.2.2.4.1.cmml">=</mo><mn id="S3.SS1.p4.4.m4.2.2.2.2.4.3" xref="S3.SS1.p4.4.m4.2.2.2.2.4.3.cmml">1</mn></mrow><mi id="S3.SS1.p4.4.m4.2.2.2.4" xref="S3.SS1.p4.4.m4.2.2.2.4.cmml">n</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.2b"><apply id="S3.SS1.p4.4.m4.2.2.cmml" xref="S3.SS1.p4.4.m4.2.2"><eq id="S3.SS1.p4.4.m4.2.2.3.cmml" xref="S3.SS1.p4.4.m4.2.2.3"></eq><apply id="S3.SS1.p4.4.m4.2.2.4.cmml" xref="S3.SS1.p4.4.m4.2.2.4"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.2.2.4.1.cmml" xref="S3.SS1.p4.4.m4.2.2.4">subscript</csymbol><ci id="S3.SS1.p4.4.m4.2.2.4.2.cmml" xref="S3.SS1.p4.4.m4.2.2.4.2">𝒄</ci><ci id="S3.SS1.p4.4.m4.2.2.4.3.cmml" xref="S3.SS1.p4.4.m4.2.2.4.3">𝑛</ci></apply><apply id="S3.SS1.p4.4.m4.2.2.2.cmml" xref="S3.SS1.p4.4.m4.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.2.2.2.3.cmml" xref="S3.SS1.p4.4.m4.2.2.2">superscript</csymbol><apply id="S3.SS1.p4.4.m4.2.2.2.2.cmml" xref="S3.SS1.p4.4.m4.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.2.2.2.2.3.cmml" xref="S3.SS1.p4.4.m4.2.2.2">subscript</csymbol><set id="S3.SS1.p4.4.m4.2.2.2.2.2.3.cmml" xref="S3.SS1.p4.4.m4.2.2.2.2.2.2"><apply id="S3.SS1.p4.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p4.4.m4.1.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p4.4.m4.1.1.1.1.1.1.1.2">𝒕</ci><ci id="S3.SS1.p4.4.m4.1.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p4.4.m4.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS1.p4.4.m4.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p4.4.m4.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.2.2.2.2.2.2.2.1.cmml" xref="S3.SS1.p4.4.m4.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p4.4.m4.2.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p4.4.m4.2.2.2.2.2.2.2.2">𝒔</ci><ci id="S3.SS1.p4.4.m4.2.2.2.2.2.2.2.3.cmml" xref="S3.SS1.p4.4.m4.2.2.2.2.2.2.2.3">𝑖</ci></apply></set><apply id="S3.SS1.p4.4.m4.2.2.2.2.4.cmml" xref="S3.SS1.p4.4.m4.2.2.2.2.4"><eq id="S3.SS1.p4.4.m4.2.2.2.2.4.1.cmml" xref="S3.SS1.p4.4.m4.2.2.2.2.4.1"></eq><ci id="S3.SS1.p4.4.m4.2.2.2.2.4.2.cmml" xref="S3.SS1.p4.4.m4.2.2.2.2.4.2">𝑖</ci><cn type="integer" id="S3.SS1.p4.4.m4.2.2.2.2.4.3.cmml" xref="S3.SS1.p4.4.m4.2.2.2.2.4.3">1</cn></apply></apply><ci id="S3.SS1.p4.4.m4.2.2.2.4.cmml" xref="S3.SS1.p4.4.m4.2.2.2.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.2c">\bm{c}_{n}=\{\bm{t}_{i},\bm{s}_{i}\}_{i=1}^{n}</annotation></semantics></math> for processing the next speech input <math id="S3.SS1.p4.5.m5.1" class="ltx_Math" alttext="\bm{x}_{n+2}" display="inline"><semantics id="S3.SS1.p4.5.m5.1a"><msub id="S3.SS1.p4.5.m5.1.1" xref="S3.SS1.p4.5.m5.1.1.cmml"><mi id="S3.SS1.p4.5.m5.1.1.2" xref="S3.SS1.p4.5.m5.1.1.2.cmml">𝒙</mi><mrow id="S3.SS1.p4.5.m5.1.1.3" xref="S3.SS1.p4.5.m5.1.1.3.cmml"><mi id="S3.SS1.p4.5.m5.1.1.3.2" xref="S3.SS1.p4.5.m5.1.1.3.2.cmml">n</mi><mo id="S3.SS1.p4.5.m5.1.1.3.1" xref="S3.SS1.p4.5.m5.1.1.3.1.cmml">+</mo><mn id="S3.SS1.p4.5.m5.1.1.3.3" xref="S3.SS1.p4.5.m5.1.1.3.3.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m5.1b"><apply id="S3.SS1.p4.5.m5.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.5.m5.1.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p4.5.m5.1.1.2.cmml" xref="S3.SS1.p4.5.m5.1.1.2">𝒙</ci><apply id="S3.SS1.p4.5.m5.1.1.3.cmml" xref="S3.SS1.p4.5.m5.1.1.3"><plus id="S3.SS1.p4.5.m5.1.1.3.1.cmml" xref="S3.SS1.p4.5.m5.1.1.3.1"></plus><ci id="S3.SS1.p4.5.m5.1.1.3.2.cmml" xref="S3.SS1.p4.5.m5.1.1.3.2">𝑛</ci><cn type="integer" id="S3.SS1.p4.5.m5.1.1.3.3.cmml" xref="S3.SS1.p4.5.m5.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m5.1c">\bm{x}_{n+2}</annotation></semantics></math>. This methodology effectively reduces the system’s reliance on ASR during the user’s wait time for a response, thereby improving the real-time applicability of the system. By streamlining the dialog process to involve only a single LLM decoder for response generation, rather than two separate autoregressive decoders for both ASR and response generation, Style-Talker achieves remarkable efficiency and speed, signifying a new step toward real-time spoken dialog systems. In the following section, we detail each component, context aggregation, and prompt settings. For more implementation details, please refer to Appendix <a href="#A1" title="Appendix A Implementation Details ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>StyleTTS 2</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">StyleTTS 2 is a state-of-the-art TTS model with human-level performance and public pre-trained checkpoints. It differs from other TTS models in how it captures and represents the speaking style. At the core of StyleTTS 2’s design is an autoencoder framework that learns a fixed-length vector capturing the speaking style as its latent representation, from which the speech is decoded back to waveform conditioned on the transcribed text. This style representation is comprehensive, containing a broad range of paralinguistic features that extend beyond the mere content of speech. It includes, but is not limited to, the speaker identity, prosody, lexical stress, formant transitions, and speaking rate <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib18" title="" class="ltx_ref">2024</a>)</cite>. In essence, the speaking style vector serves as a paralinguistic summary of the speech, effectively distilling the essence of how something is said, separate from what is being said.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">To leverage the capabilities of StyleTTS 2 for dialog generation, we begin by fine-tuning the model using the LibriTTS checkpoint <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Available at <a target="_blank" href="https://github.com/yl4579/StyleTTS2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/yl4579/StyleTTS2</a></span></span></span>. We prepared our dataset at the utterance level, with each utterance attributing to a single speaker (see Section <a href="#S4.SS1" title="4.1 Datasets ‣ 4 Experiments ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>). This is because the style encoder of StyleTTS 2 is designed to process speech from individual speakers and cannot encode utterances with multiple speakers. We then compute the utterance-level style for each utterance in the dataset for audio LM fine-tuning.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Qwen-Audio</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS2.p1.2" class="ltx_p">Qwen-Audio is an open-source multi-modal language model, combining the text and audio modalities to understand and respond to both text and audio inputs. This model is an adaptation of Qwen-7B <cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>, fine-tuned with an audio encoder from Whisper <span id="S3.SS1.SSS2.p1.2.1" class="ltx_text ltx_font_italic">large-v2</span> <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite> <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Available at <a target="_blank" href="https://github.com/QwenLM/Qwen-Audio" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/QwenLM/Qwen-Audio</a></span></span></span>. This integration enables Qwen-Audio to comprehend audio inputs and generate corresponding text responses with the capacity to understand both the spoken content and its paralinguistic attributes. In Style-Talker, we have further fine-tuned Qwen-Audio to incorporate speaking style both as an input and output modality. We modified the input head to the transformer layers to accept not only text information of previous conversation turns but also the associated speaking style as the context (see Section <a href="#S3.SS1.SSS3" title="3.1.3 Conversation Context ‣ 3.1 Style-Talker ‣ 3 Methods ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.3</span></a>). The details of this modification, including specific prompts used, can be found in Appendix <a href="#A1.SS1" title="A.1 Prompt Settings ‣ Appendix A Implementation Details ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>. In this setup, the speaking style is transformed into a hidden representation that aligns with those of the text tokens in dimensionality through a linear projection layer <math id="S3.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{P}_{\text{in}}" display="inline"><semantics id="S3.SS1.SSS2.p1.1.m1.1a"><msub id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.p1.1.m1.1.1.2" xref="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml">𝒫</mi><mtext id="S3.SS1.SSS2.p1.1.m1.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.1.1.3a.cmml">in</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.1b"><apply id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.2">𝒫</ci><ci id="S3.SS1.SSS2.p1.1.m1.1.1.3a.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.3">in</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.1c">\mathcal{P}_{\text{in}}</annotation></semantics></math>. Similarly, when generating responses, an additional linear projection head <math id="S3.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{P}_{\text{out}}" display="inline"><semantics id="S3.SS1.SSS2.p1.2.m2.1a"><msub id="S3.SS1.SSS2.p1.2.m2.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS2.p1.2.m2.1.1.2" xref="S3.SS1.SSS2.p1.2.m2.1.1.2.cmml">𝒫</mi><mtext id="S3.SS1.SSS2.p1.2.m2.1.1.3" xref="S3.SS1.SSS2.p1.2.m2.1.1.3a.cmml">out</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.1b"><apply id="S3.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.2">𝒫</ci><ci id="S3.SS1.SSS2.p1.2.m2.1.1.3a.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.3"><mtext mathsize="70%" id="S3.SS1.SSS2.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1.3">out</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.1c">\mathcal{P}_{\text{out}}</annotation></semantics></math> is applied to a special <code id="S3.SS1.SSS2.p1.2.2" class="ltx_verbatim ltx_font_typewriter">&lt;STY_OUT&gt;</code> token appended near the end of the model’s input sequence to decode the speaking style corresponding to the response text (see Figure <a href="#S2.F2" title="Figure 2 ‣ 2.2 End-to-End Speech-to-Speech Generation ‣ 2 Related Works ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for detailed illustrations). Since we only care about the paralinguistic information of the response speech rather than the pre-determined response speaker identity, we only predict the prosodic style in StyleTTS 2 and use a pre-computed acoustic style for the timbre of the target speaker’s voice. We found that this approach helps retain the speaker’s identity while maintaining the ability to produce prosodically coherent speech responses.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Conversation Context</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">In multi-turn dialog systems, the context of the conversation plays a pivotal role in generating responses that are both congruent and coherent. Research has consistently shown that incorporating longer contexts leads to more meaningful and contextually appropriate dialogues <cite class="ltx_cite ltx_citemacro_citep">(Agarwal et al., <a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite>. Traditional text-based spoken dialog systems, however, typically limit context to the text content of previous interactions <cite class="ltx_cite ltx_citemacro_citep">(Xue et al., <a href="#bib.bib29" title="" class="ltx_ref">2023</a>; Lin et al., <a href="#bib.bib21" title="" class="ltx_ref">2024</a>)</cite>. This approach falls short in capturing the full spectrum of communicative nuances, particularly in regards to the speaking style, due to the absence of speech information within the dialog context. To address this limitation, Style-Talker enhances context representation by including both the text of each utterance and its corresponding speaking style. This enriched context is structured within the prompt in a novel manner: adjacent to each speaker’s name, and preceding the transcribed speech content, a special style token is introduced. This token directly corresponds to the speaking style vector for the utterance, effectively embedding paralinguistic information within the dialog context (see Appendix <a href="#A1.SS1" title="A.1 Prompt Settings ‣ Appendix A Implementation Details ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a> for more details).</p>
</div>
<div id="S3.SS1.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS3.p2.1" class="ltx_p">The integration of texts and style vectors into the conversation context is achieved through the use of ASR, with the Whisper model and the style encoder from StyleTTS 2. This process occurs while the current response speech is being played back to the user. This design choice minimizes potential delays in response time attributed to input transcription, ensuring the system remains highly responsive. By maintaining a comprehensive context that encompasses both semantic content and paralinguistic attributes, Style-Talker significantly advances the capabilities of spoken dialog systems. Since our model operates on the text domain, we can have a speech context spanning for up to 2 minutes, much longer than speech-to-speech models such as SpeechGPT, which often suffer from limited context windows, as speech is inherently longer than its transcribed texts.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training Objectives</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.4" class="ltx_p">For a more natural style and coherent content in the response speech, we jointly optimize the style error and the next token probability. The style loss <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\text{style}}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3a.cmml">style</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">ℒ</ci><ci id="S3.SS2.p1.1.m1.1.1.3a.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">style</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\mathcal{L}_{\text{style}}</annotation></semantics></math> is defined as the L1 distance between the ground truth prosodic style <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\hat{\bm{s}}_{n+1}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mover accent="true" id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2.2" xref="S3.SS2.p1.2.m2.1.1.2.2.cmml">𝒔</mi><mo id="S3.SS2.p1.2.m2.1.1.2.1" xref="S3.SS2.p1.2.m2.1.1.2.1.cmml">^</mo></mover><mrow id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.p1.2.m2.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.3.2.cmml">n</mi><mo id="S3.SS2.p1.2.m2.1.1.3.1" xref="S3.SS2.p1.2.m2.1.1.3.1.cmml">+</mo><mn id="S3.SS2.p1.2.m2.1.1.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><apply id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2"><ci id="S3.SS2.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.p1.2.m2.1.1.2.1">^</ci><ci id="S3.SS2.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2.2">𝒔</ci></apply><apply id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3"><plus id="S3.SS2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3.1"></plus><ci id="S3.SS2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.2">𝑛</ci><cn type="integer" id="S3.SS2.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\hat{\bm{s}}_{n+1}</annotation></semantics></math> and the style <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="\bm{s}_{n+1}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">𝒔</mi><mrow id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.p1.3.m3.1.1.3.2" xref="S3.SS2.p1.3.m3.1.1.3.2.cmml">n</mi><mo id="S3.SS2.p1.3.m3.1.1.3.1" xref="S3.SS2.p1.3.m3.1.1.3.1.cmml">+</mo><mn id="S3.SS2.p1.3.m3.1.1.3.3" xref="S3.SS2.p1.3.m3.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">𝒔</ci><apply id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3"><plus id="S3.SS2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.p1.3.m3.1.1.3.1"></plus><ci id="S3.SS2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.p1.3.m3.1.1.3.2">𝑛</ci><cn type="integer" id="S3.SS2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\bm{s}_{n+1}</annotation></semantics></math> decoded from the last layer representation <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="h_{\text{&lt;STY\_OUT&gt;}}" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><msub id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">h</mi><mtext id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3a.cmml">&lt;STY_OUT&gt;</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">ℎ</ci><ci id="S3.SS2.p1.4.m4.1.1.3a.cmml" xref="S3.SS2.p1.4.m4.1.1.3"><mtext mathsize="70%" id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">&lt;STY_OUT&gt;</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">h_{\text{&lt;STY\_OUT&gt;}}</annotation></semantics></math> of the special token <code id="S3.SS2.p1.4.1" class="ltx_verbatim ltx_font_typewriter">&lt;STY_OUT&gt;</code>:</p>
<table id="A3.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\text{style}}=|\bm{s}_{n+1}-\hat{\bm{s}}_{n+1}|," display="inline"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml">ℒ</mi><mtext id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3a.cmml">style</mtext></msub><mo id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.2.1.cmml">|</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.cmml">𝒔</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.2.cmml">n</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.2.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.1.cmml">+</mo><mn id="S3.E1.m1.1.1.1.1.1.1.1.2.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S3.E1.m1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.2.cmml">𝒔</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.3.2.1" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mrow id="S3.E1.m1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3.2.cmml">n</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3.1.cmml">+</mo><mn id="S3.E1.m1.1.1.1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></eq><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2">ℒ</ci><ci id="S3.E1.m1.1.1.1.1.3.3a.cmml" xref="S3.E1.m1.1.1.1.1.3.3"><mtext mathsize="70%" id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3">style</mtext></ci></apply><apply id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><abs id="S3.E1.m1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2"></abs><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2">𝒔</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3"><plus id="S3.E1.m1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.1"></plus><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.2">𝑛</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2"><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.1">^</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.2.2">𝒔</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3"><plus id="S3.E1.m1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3.1"></plus><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3.2">𝑛</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3.3.3">1</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\displaystyle\mathcal{L}_{\text{style}}=|\bm{s}_{n+1}-\hat{\bm{s}}_{n+1}|,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table id="A3.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle\hskip 4.0pt\bm{s}_{n+1}=\mathcal{P}_{\text{out}}(h_{\text{&lt;STY\_OUT&gt;}})." display="inline"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml">𝒔</mi><mrow id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.3.2" xref="S3.E2.m1.1.1.1.1.3.3.2.cmml">n</mi><mo id="S3.E2.m1.1.1.1.1.3.3.1" xref="S3.E2.m1.1.1.1.1.3.3.1.cmml">+</mo><mn id="S3.E2.m1.1.1.1.1.3.3.3" xref="S3.E2.m1.1.1.1.1.3.3.3.cmml">1</mn></mrow></msub><mo id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.3.2.cmml">𝒫</mi><mtext id="S3.E2.m1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.3.3a.cmml">out</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml">h</mi><mtext id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3a.cmml">&lt;STY_OUT&gt;</mtext></msub><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"></eq><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2">𝒔</ci><apply id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3"><plus id="S3.E2.m1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.3.1"></plus><ci id="S3.E2.m1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.3.2">𝑛</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3.3">1</cn></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.3.2">𝒫</ci><ci id="S3.E2.m1.1.1.1.1.1.3.3a.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3"><mtext mathsize="70%" id="S3.E2.m1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3">out</mtext></ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2">ℎ</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3a.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3"><mtext mathsize="70%" id="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3">&lt;STY_OUT&gt;</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle\hskip 4.0pt\bm{s}_{n+1}=\mathcal{P}_{\text{out}}(h_{\text{&lt;STY\_OUT&gt;}}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.7" class="ltx_p">The response text generation maximizes the next text token (<math id="S3.SS2.p1.5.m1.1" class="ltx_Math" alttext="\bm{t}_{n+1}" display="inline"><semantics id="S3.SS2.p1.5.m1.1a"><msub id="S3.SS2.p1.5.m1.1.1" xref="S3.SS2.p1.5.m1.1.1.cmml"><mi id="S3.SS2.p1.5.m1.1.1.2" xref="S3.SS2.p1.5.m1.1.1.2.cmml">𝒕</mi><mrow id="S3.SS2.p1.5.m1.1.1.3" xref="S3.SS2.p1.5.m1.1.1.3.cmml"><mi id="S3.SS2.p1.5.m1.1.1.3.2" xref="S3.SS2.p1.5.m1.1.1.3.2.cmml">n</mi><mo id="S3.SS2.p1.5.m1.1.1.3.1" xref="S3.SS2.p1.5.m1.1.1.3.1.cmml">+</mo><mn id="S3.SS2.p1.5.m1.1.1.3.3" xref="S3.SS2.p1.5.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m1.1b"><apply id="S3.SS2.p1.5.m1.1.1.cmml" xref="S3.SS2.p1.5.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m1.1.1.1.cmml" xref="S3.SS2.p1.5.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.5.m1.1.1.2.cmml" xref="S3.SS2.p1.5.m1.1.1.2">𝒕</ci><apply id="S3.SS2.p1.5.m1.1.1.3.cmml" xref="S3.SS2.p1.5.m1.1.1.3"><plus id="S3.SS2.p1.5.m1.1.1.3.1.cmml" xref="S3.SS2.p1.5.m1.1.1.3.1"></plus><ci id="S3.SS2.p1.5.m1.1.1.3.2.cmml" xref="S3.SS2.p1.5.m1.1.1.3.2">𝑛</ci><cn type="integer" id="S3.SS2.p1.5.m1.1.1.3.3.cmml" xref="S3.SS2.p1.5.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m1.1c">\bm{t}_{n+1}</annotation></semantics></math>) probability given the incoming speech <math id="S3.SS2.p1.6.m2.1" class="ltx_Math" alttext="\bm{x}_{n}" display="inline"><semantics id="S3.SS2.p1.6.m2.1a"><msub id="S3.SS2.p1.6.m2.1.1" xref="S3.SS2.p1.6.m2.1.1.cmml"><mi id="S3.SS2.p1.6.m2.1.1.2" xref="S3.SS2.p1.6.m2.1.1.2.cmml">𝒙</mi><mi id="S3.SS2.p1.6.m2.1.1.3" xref="S3.SS2.p1.6.m2.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m2.1b"><apply id="S3.SS2.p1.6.m2.1.1.cmml" xref="S3.SS2.p1.6.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m2.1.1.1.cmml" xref="S3.SS2.p1.6.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.6.m2.1.1.2.cmml" xref="S3.SS2.p1.6.m2.1.1.2">𝒙</ci><ci id="S3.SS2.p1.6.m2.1.1.3.cmml" xref="S3.SS2.p1.6.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m2.1c">\bm{x}_{n}</annotation></semantics></math> and preceding context <math id="S3.SS2.p1.7.m3.1" class="ltx_Math" alttext="\bm{c}_{n-1}" display="inline"><semantics id="S3.SS2.p1.7.m3.1a"><msub id="S3.SS2.p1.7.m3.1.1" xref="S3.SS2.p1.7.m3.1.1.cmml"><mi id="S3.SS2.p1.7.m3.1.1.2" xref="S3.SS2.p1.7.m3.1.1.2.cmml">𝒄</mi><mrow id="S3.SS2.p1.7.m3.1.1.3" xref="S3.SS2.p1.7.m3.1.1.3.cmml"><mi id="S3.SS2.p1.7.m3.1.1.3.2" xref="S3.SS2.p1.7.m3.1.1.3.2.cmml">n</mi><mo id="S3.SS2.p1.7.m3.1.1.3.1" xref="S3.SS2.p1.7.m3.1.1.3.1.cmml">−</mo><mn id="S3.SS2.p1.7.m3.1.1.3.3" xref="S3.SS2.p1.7.m3.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m3.1b"><apply id="S3.SS2.p1.7.m3.1.1.cmml" xref="S3.SS2.p1.7.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.7.m3.1.1.1.cmml" xref="S3.SS2.p1.7.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.7.m3.1.1.2.cmml" xref="S3.SS2.p1.7.m3.1.1.2">𝒄</ci><apply id="S3.SS2.p1.7.m3.1.1.3.cmml" xref="S3.SS2.p1.7.m3.1.1.3"><minus id="S3.SS2.p1.7.m3.1.1.3.1.cmml" xref="S3.SS2.p1.7.m3.1.1.3.1"></minus><ci id="S3.SS2.p1.7.m3.1.1.3.2.cmml" xref="S3.SS2.p1.7.m3.1.1.3.2">𝑛</ci><cn type="integer" id="S3.SS2.p1.7.m3.1.1.3.3.cmml" xref="S3.SS2.p1.7.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m3.1c">\bm{c}_{n-1}</annotation></semantics></math> containing both styles and transcriptions:</p>
<table id="A3.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3.m1.4" class="ltx_Math" alttext="\displaystyle\max\limits_{\theta_{\text{in}},\phi_{\text{AE}},\phi_{\text{LM}}}P(\bm{t}_{n+1}|\bm{x}_{n},\bm{c}_{n-1})," display="inline"><semantics id="S3.E3.m1.4a"><mrow id="S3.E3.m1.4.4.1" xref="S3.E3.m1.4.4.1.1.cmml"><mrow id="S3.E3.m1.4.4.1.1" xref="S3.E3.m1.4.4.1.1.cmml"><mrow id="S3.E3.m1.4.4.1.1.3" xref="S3.E3.m1.4.4.1.1.3.cmml"><munder id="S3.E3.m1.4.4.1.1.3.1" xref="S3.E3.m1.4.4.1.1.3.1.cmml"><mi id="S3.E3.m1.4.4.1.1.3.1.2" xref="S3.E3.m1.4.4.1.1.3.1.2.cmml">max</mi><mrow id="S3.E3.m1.3.3.3.3" xref="S3.E3.m1.3.3.3.4.cmml"><msub id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.2.cmml">θ</mi><mtext id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.3a.cmml">in</mtext></msub><mo id="S3.E3.m1.3.3.3.3.4" xref="S3.E3.m1.3.3.3.4.cmml">,</mo><msub id="S3.E3.m1.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.cmml"><mi id="S3.E3.m1.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.cmml">ϕ</mi><mtext id="S3.E3.m1.2.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.2.3a.cmml">AE</mtext></msub><mo id="S3.E3.m1.3.3.3.3.5" xref="S3.E3.m1.3.3.3.4.cmml">,</mo><msub id="S3.E3.m1.3.3.3.3.3" xref="S3.E3.m1.3.3.3.3.3.cmml"><mi id="S3.E3.m1.3.3.3.3.3.2" xref="S3.E3.m1.3.3.3.3.3.2.cmml">ϕ</mi><mtext id="S3.E3.m1.3.3.3.3.3.3" xref="S3.E3.m1.3.3.3.3.3.3a.cmml">LM</mtext></msub></mrow></munder><mo lspace="0.167em" id="S3.E3.m1.4.4.1.1.3a" xref="S3.E3.m1.4.4.1.1.3.cmml">⁡</mo><mi id="S3.E3.m1.4.4.1.1.3.2" xref="S3.E3.m1.4.4.1.1.3.2.cmml">P</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.4.1.1.2" xref="S3.E3.m1.4.4.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.4.4.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.4.4.1.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.4.4.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.cmml"><msub id="S3.E3.m1.4.4.1.1.1.1.1.4" xref="S3.E3.m1.4.4.1.1.1.1.1.4.cmml"><mi id="S3.E3.m1.4.4.1.1.1.1.1.4.2" xref="S3.E3.m1.4.4.1.1.1.1.1.4.2.cmml">𝒕</mi><mrow id="S3.E3.m1.4.4.1.1.1.1.1.4.3" xref="S3.E3.m1.4.4.1.1.1.1.1.4.3.cmml"><mi id="S3.E3.m1.4.4.1.1.1.1.1.4.3.2" xref="S3.E3.m1.4.4.1.1.1.1.1.4.3.2.cmml">n</mi><mo id="S3.E3.m1.4.4.1.1.1.1.1.4.3.1" xref="S3.E3.m1.4.4.1.1.1.1.1.4.3.1.cmml">+</mo><mn id="S3.E3.m1.4.4.1.1.1.1.1.4.3.3" xref="S3.E3.m1.4.4.1.1.1.1.1.4.3.3.cmml">1</mn></mrow></msub><mo fence="false" id="S3.E3.m1.4.4.1.1.1.1.1.3" xref="S3.E3.m1.4.4.1.1.1.1.1.3.cmml">|</mo><mrow id="S3.E3.m1.4.4.1.1.1.1.1.2.2" xref="S3.E3.m1.4.4.1.1.1.1.1.2.3.cmml"><msub id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.2.cmml">𝒙</mi><mi id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.3.cmml">n</mi></msub><mo id="S3.E3.m1.4.4.1.1.1.1.1.2.2.3" xref="S3.E3.m1.4.4.1.1.1.1.1.2.3.cmml">,</mo><msub id="S3.E3.m1.4.4.1.1.1.1.1.2.2.2" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.2" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.2.cmml">𝒄</mi><mrow id="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.3" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.3.cmml"><mi id="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.3.2" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.3.2.cmml">n</mi><mo id="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.3.1" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.3.1.cmml">−</mo><mn id="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.3.3" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.3.3.cmml">1</mn></mrow></msub></mrow></mrow><mo stretchy="false" id="S3.E3.m1.4.4.1.1.1.1.3" xref="S3.E3.m1.4.4.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.4.4.1.2" xref="S3.E3.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.4b"><apply id="S3.E3.m1.4.4.1.1.cmml" xref="S3.E3.m1.4.4.1"><times id="S3.E3.m1.4.4.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.2"></times><apply id="S3.E3.m1.4.4.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.3"><apply id="S3.E3.m1.4.4.1.1.3.1.cmml" xref="S3.E3.m1.4.4.1.1.3.1"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.3.1.1.cmml" xref="S3.E3.m1.4.4.1.1.3.1">subscript</csymbol><max id="S3.E3.m1.4.4.1.1.3.1.2.cmml" xref="S3.E3.m1.4.4.1.1.3.1.2"></max><list id="S3.E3.m1.3.3.3.4.cmml" xref="S3.E3.m1.3.3.3.3"><apply id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2">𝜃</ci><ci id="S3.E3.m1.1.1.1.1.1.3a.cmml" xref="S3.E3.m1.1.1.1.1.1.3"><mtext mathsize="50%" id="S3.E3.m1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.3">in</mtext></ci></apply><apply id="S3.E3.m1.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.2.1.cmml" xref="S3.E3.m1.2.2.2.2.2">subscript</csymbol><ci id="S3.E3.m1.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2">italic-ϕ</ci><ci id="S3.E3.m1.2.2.2.2.2.3a.cmml" xref="S3.E3.m1.2.2.2.2.2.3"><mtext mathsize="50%" id="S3.E3.m1.2.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.2.3">AE</mtext></ci></apply><apply id="S3.E3.m1.3.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.3.3.3.1.cmml" xref="S3.E3.m1.3.3.3.3.3">subscript</csymbol><ci id="S3.E3.m1.3.3.3.3.3.2.cmml" xref="S3.E3.m1.3.3.3.3.3.2">italic-ϕ</ci><ci id="S3.E3.m1.3.3.3.3.3.3a.cmml" xref="S3.E3.m1.3.3.3.3.3.3"><mtext mathsize="50%" id="S3.E3.m1.3.3.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3.3.3">LM</mtext></ci></apply></list></apply><ci id="S3.E3.m1.4.4.1.1.3.2.cmml" xref="S3.E3.m1.4.4.1.1.3.2">𝑃</ci></apply><apply id="S3.E3.m1.4.4.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.4.4.1.1.1.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.3">conditional</csymbol><apply id="S3.E3.m1.4.4.1.1.1.1.1.4.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.1.1.1.4.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E3.m1.4.4.1.1.1.1.1.4.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.4.2">𝒕</ci><apply id="S3.E3.m1.4.4.1.1.1.1.1.4.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.4.3"><plus id="S3.E3.m1.4.4.1.1.1.1.1.4.3.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.4.3.1"></plus><ci id="S3.E3.m1.4.4.1.1.1.1.1.4.3.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.4.3.2">𝑛</ci><cn type="integer" id="S3.E3.m1.4.4.1.1.1.1.1.4.3.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.4.3.3">1</cn></apply></apply><list id="S3.E3.m1.4.4.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2"><apply id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.2">𝒙</ci><ci id="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.1.1.1.3">𝑛</ci></apply><apply id="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.2">𝒄</ci><apply id="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.3"><minus id="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.3.1"></minus><ci id="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.3.2">𝑛</ci><cn type="integer" id="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.E3.m1.4.4.1.1.1.1.1.2.2.2.3.3">1</cn></apply></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.4c">\displaystyle\max\limits_{\theta_{\text{in}},\phi_{\text{AE}},\phi_{\text{LM}}}P(\bm{t}_{n+1}|\bm{x}_{n},\bm{c}_{n-1}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p1.13" class="ltx_p">where <math id="S3.SS2.p1.8.m1.1" class="ltx_Math" alttext="\theta_{\text{in}}" display="inline"><semantics id="S3.SS2.p1.8.m1.1a"><msub id="S3.SS2.p1.8.m1.1.1" xref="S3.SS2.p1.8.m1.1.1.cmml"><mi id="S3.SS2.p1.8.m1.1.1.2" xref="S3.SS2.p1.8.m1.1.1.2.cmml">θ</mi><mtext id="S3.SS2.p1.8.m1.1.1.3" xref="S3.SS2.p1.8.m1.1.1.3a.cmml">in</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m1.1b"><apply id="S3.SS2.p1.8.m1.1.1.cmml" xref="S3.SS2.p1.8.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.8.m1.1.1.1.cmml" xref="S3.SS2.p1.8.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.8.m1.1.1.2.cmml" xref="S3.SS2.p1.8.m1.1.1.2">𝜃</ci><ci id="S3.SS2.p1.8.m1.1.1.3a.cmml" xref="S3.SS2.p1.8.m1.1.1.3"><mtext mathsize="70%" id="S3.SS2.p1.8.m1.1.1.3.cmml" xref="S3.SS2.p1.8.m1.1.1.3">in</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m1.1c">\theta_{\text{in}}</annotation></semantics></math>, <math id="S3.SS2.p1.9.m2.1" class="ltx_Math" alttext="\phi_{\text{Audio}}" display="inline"><semantics id="S3.SS2.p1.9.m2.1a"><msub id="S3.SS2.p1.9.m2.1.1" xref="S3.SS2.p1.9.m2.1.1.cmml"><mi id="S3.SS2.p1.9.m2.1.1.2" xref="S3.SS2.p1.9.m2.1.1.2.cmml">ϕ</mi><mtext id="S3.SS2.p1.9.m2.1.1.3" xref="S3.SS2.p1.9.m2.1.1.3a.cmml">Audio</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m2.1b"><apply id="S3.SS2.p1.9.m2.1.1.cmml" xref="S3.SS2.p1.9.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.9.m2.1.1.1.cmml" xref="S3.SS2.p1.9.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.9.m2.1.1.2.cmml" xref="S3.SS2.p1.9.m2.1.1.2">italic-ϕ</ci><ci id="S3.SS2.p1.9.m2.1.1.3a.cmml" xref="S3.SS2.p1.9.m2.1.1.3"><mtext mathsize="70%" id="S3.SS2.p1.9.m2.1.1.3.cmml" xref="S3.SS2.p1.9.m2.1.1.3">Audio</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m2.1c">\phi_{\text{Audio}}</annotation></semantics></math>, <math id="S3.SS2.p1.10.m3.1" class="ltx_Math" alttext="\phi_{\text{LM}}" display="inline"><semantics id="S3.SS2.p1.10.m3.1a"><msub id="S3.SS2.p1.10.m3.1.1" xref="S3.SS2.p1.10.m3.1.1.cmml"><mi id="S3.SS2.p1.10.m3.1.1.2" xref="S3.SS2.p1.10.m3.1.1.2.cmml">ϕ</mi><mtext id="S3.SS2.p1.10.m3.1.1.3" xref="S3.SS2.p1.10.m3.1.1.3a.cmml">LM</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m3.1b"><apply id="S3.SS2.p1.10.m3.1.1.cmml" xref="S3.SS2.p1.10.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.10.m3.1.1.1.cmml" xref="S3.SS2.p1.10.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.10.m3.1.1.2.cmml" xref="S3.SS2.p1.10.m3.1.1.2">italic-ϕ</ci><ci id="S3.SS2.p1.10.m3.1.1.3a.cmml" xref="S3.SS2.p1.10.m3.1.1.3"><mtext mathsize="70%" id="S3.SS2.p1.10.m3.1.1.3.cmml" xref="S3.SS2.p1.10.m3.1.1.3">LM</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m3.1c">\phi_{\text{LM}}</annotation></semantics></math> are the trainable parameters in the style input projection, and the audio encoder and the LLM in Qwen-Audio. We optimize the above objective through a cross-entropy loss <math id="S3.SS2.p1.11.m4.1" class="ltx_Math" alttext="\mathcal{L}_{\text{text}}" display="inline"><semantics id="S3.SS2.p1.11.m4.1a"><msub id="S3.SS2.p1.11.m4.1.1" xref="S3.SS2.p1.11.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.11.m4.1.1.2" xref="S3.SS2.p1.11.m4.1.1.2.cmml">ℒ</mi><mtext id="S3.SS2.p1.11.m4.1.1.3" xref="S3.SS2.p1.11.m4.1.1.3a.cmml">text</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.11.m4.1b"><apply id="S3.SS2.p1.11.m4.1.1.cmml" xref="S3.SS2.p1.11.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.11.m4.1.1.1.cmml" xref="S3.SS2.p1.11.m4.1.1">subscript</csymbol><ci id="S3.SS2.p1.11.m4.1.1.2.cmml" xref="S3.SS2.p1.11.m4.1.1.2">ℒ</ci><ci id="S3.SS2.p1.11.m4.1.1.3a.cmml" xref="S3.SS2.p1.11.m4.1.1.3"><mtext mathsize="70%" id="S3.SS2.p1.11.m4.1.1.3.cmml" xref="S3.SS2.p1.11.m4.1.1.3">text</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.11.m4.1c">\mathcal{L}_{\text{text}}</annotation></semantics></math> for <math id="S3.SS2.p1.12.m5.1" class="ltx_Math" alttext="1&lt;t\leq T" display="inline"><semantics id="S3.SS2.p1.12.m5.1a"><mrow id="S3.SS2.p1.12.m5.1.1" xref="S3.SS2.p1.12.m5.1.1.cmml"><mn id="S3.SS2.p1.12.m5.1.1.2" xref="S3.SS2.p1.12.m5.1.1.2.cmml">1</mn><mo id="S3.SS2.p1.12.m5.1.1.3" xref="S3.SS2.p1.12.m5.1.1.3.cmml">&lt;</mo><mi id="S3.SS2.p1.12.m5.1.1.4" xref="S3.SS2.p1.12.m5.1.1.4.cmml">t</mi><mo id="S3.SS2.p1.12.m5.1.1.5" xref="S3.SS2.p1.12.m5.1.1.5.cmml">≤</mo><mi id="S3.SS2.p1.12.m5.1.1.6" xref="S3.SS2.p1.12.m5.1.1.6.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.12.m5.1b"><apply id="S3.SS2.p1.12.m5.1.1.cmml" xref="S3.SS2.p1.12.m5.1.1"><and id="S3.SS2.p1.12.m5.1.1a.cmml" xref="S3.SS2.p1.12.m5.1.1"></and><apply id="S3.SS2.p1.12.m5.1.1b.cmml" xref="S3.SS2.p1.12.m5.1.1"><lt id="S3.SS2.p1.12.m5.1.1.3.cmml" xref="S3.SS2.p1.12.m5.1.1.3"></lt><cn type="integer" id="S3.SS2.p1.12.m5.1.1.2.cmml" xref="S3.SS2.p1.12.m5.1.1.2">1</cn><ci id="S3.SS2.p1.12.m5.1.1.4.cmml" xref="S3.SS2.p1.12.m5.1.1.4">𝑡</ci></apply><apply id="S3.SS2.p1.12.m5.1.1c.cmml" xref="S3.SS2.p1.12.m5.1.1"><leq id="S3.SS2.p1.12.m5.1.1.5.cmml" xref="S3.SS2.p1.12.m5.1.1.5"></leq><share href="#S3.SS2.p1.12.m5.1.1.4.cmml" id="S3.SS2.p1.12.m5.1.1d.cmml" xref="S3.SS2.p1.12.m5.1.1"></share><ci id="S3.SS2.p1.12.m5.1.1.6.cmml" xref="S3.SS2.p1.12.m5.1.1.6">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.12.m5.1c">1&lt;t\leq T</annotation></semantics></math> given a text sequence of length <math id="S3.SS2.p1.13.m6.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.p1.13.m6.1a"><mi id="S3.SS2.p1.13.m6.1.1" xref="S3.SS2.p1.13.m6.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.13.m6.1b"><ci id="S3.SS2.p1.13.m6.1.1.cmml" xref="S3.SS2.p1.13.m6.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.13.m6.1c">T</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.2" class="ltx_p">The final loss is a weighted sum of the style and text loss: <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{L}=\mathcal{L}_{\text{text}}+\lambda\mathcal{L}_{\text{style}}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">ℒ</mi><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml"><msub id="S3.SS2.p2.1.m1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.1.m1.1.1.3.2.2" xref="S3.SS2.p2.1.m1.1.1.3.2.2.cmml">ℒ</mi><mtext id="S3.SS2.p2.1.m1.1.1.3.2.3" xref="S3.SS2.p2.1.m1.1.1.3.2.3a.cmml">text</mtext></msub><mo id="S3.SS2.p2.1.m1.1.1.3.1" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">+</mo><mrow id="S3.SS2.p2.1.m1.1.1.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3.3.2" xref="S3.SS2.p2.1.m1.1.1.3.3.2.cmml">λ</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.3.3.1" xref="S3.SS2.p2.1.m1.1.1.3.3.1.cmml">​</mo><msub id="S3.SS2.p2.1.m1.1.1.3.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.1.m1.1.1.3.3.3.2" xref="S3.SS2.p2.1.m1.1.1.3.3.3.2.cmml">ℒ</mi><mtext id="S3.SS2.p2.1.m1.1.1.3.3.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.3.3a.cmml">style</mtext></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><eq id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></eq><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">ℒ</ci><apply id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><plus id="S3.SS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.1"></plus><apply id="S3.SS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.3.2.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.3.2.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2.2">ℒ</ci><ci id="S3.SS2.p2.1.m1.1.1.3.2.3a.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2.3"><mtext mathsize="70%" id="S3.SS2.p2.1.m1.1.1.3.2.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2.3">text</mtext></ci></apply><apply id="S3.SS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3"><times id="S3.SS2.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.1"></times><ci id="S3.SS2.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.2">𝜆</ci><apply id="S3.SS2.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.3.3.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.3.3.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.3.2">ℒ</ci><ci id="S3.SS2.p2.1.m1.1.1.3.3.3.3a.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.3.3"><mtext mathsize="70%" id="S3.SS2.p2.1.m1.1.1.3.3.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.3.3">style</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\mathcal{L}=\mathcal{L}_{\text{text}}+\lambda\mathcal{L}_{\text{style}}</annotation></semantics></math>, where <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\lambda</annotation></semantics></math> is a hyperparameter for weighing different loss terms.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<figure id="S4.T1" class="ltx_table">
<p id="S4.T1.14" class="ltx_p ltx_align_center"><span id="S4.T1.14.14" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S4.T1.14.14.14.14" class="ltx_inline-block ltx_transformed_outer" style="width:497.3pt;height:145pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S4.T1.14.14.14.14.14" class="ltx_p"><span id="S4.T1.14.14.14.14.14.14" class="ltx_text">
<span id="S4.T1.14.14.14.14.14.14.14" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.14.14.14.14.14.14.14.15" class="ltx_tr">
<span id="S4.T1.14.14.14.14.14.14.14.15.1" class="ltx_td ltx_align_left ltx_border_tt">Dataset</span>
<span id="S4.T1.14.14.14.14.14.14.14.15.2" class="ltx_td ltx_align_left ltx_border_tt">Model</span>
<span id="S4.T1.14.14.14.14.14.14.14.15.3" class="ltx_td ltx_align_center ltx_border_tt">MOS-N (CI)</span>
<span id="S4.T1.14.14.14.14.14.14.14.15.4" class="ltx_td ltx_align_center ltx_border_tt">MOS-C (CI)</span></span>
<span id="S4.T1.2.2.2.2.2.2.2.2" class="ltx_tr">
<span id="S4.T1.2.2.2.2.2.2.2.2.3" class="ltx_td ltx_align_left ltx_border_t ltx_rowspan ltx_rowspan_4"><span id="S4.T1.2.2.2.2.2.2.2.2.3.1" class="ltx_text">DailyTalk</span></span>
<span id="S4.T1.2.2.2.2.2.2.2.2.4" class="ltx_td ltx_align_left ltx_border_t">Ground Truth</span>
<span id="S4.T1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">3.83 (<math id="S4.T1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.1.1.1.1.m1.1c">\pm</annotation></semantics></math> 0.11)</span>
<span id="S4.T1.2.2.2.2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">4.33 (<math id="S4.T1.2.2.2.2.2.2.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.2.2.2.2.2.2.2.2.2.m1.1a"><mo id="S4.T1.2.2.2.2.2.2.2.2.2.m1.1.1" xref="S4.T1.2.2.2.2.2.2.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.2.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S4.T1.2.2.2.2.2.2.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.2.2.2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.2.2.2.2.2.m1.1c">\pm</annotation></semantics></math> 0.07)</span></span>
<span id="S4.T1.4.4.4.4.4.4.4.4" class="ltx_tr">
<span id="S4.T1.4.4.4.4.4.4.4.4.3" class="ltx_td ltx_align_left">SpeechGPT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib31" title="" class="ltx_ref">2023</a>)</cite></span>
<span id="S4.T1.3.3.3.3.3.3.3.3.1" class="ltx_td ltx_align_center">2.87 (<math id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1a"><mo id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1" xref="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1b"><csymbol cd="latexml" id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.3.3.3.3.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.3.3.3.3.1.m1.1c">\pm</annotation></semantics></math> 0.08)</span>
<span id="S4.T1.4.4.4.4.4.4.4.4.2" class="ltx_td ltx_align_center">3.11 (<math id="S4.T1.4.4.4.4.4.4.4.4.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.4.4.4.4.4.4.4.4.2.m1.1a"><mo id="S4.T1.4.4.4.4.4.4.4.4.2.m1.1.1" xref="S4.T1.4.4.4.4.4.4.4.4.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.4.4.4.4.4.2.m1.1b"><csymbol cd="latexml" id="S4.T1.4.4.4.4.4.4.4.4.2.m1.1.1.cmml" xref="S4.T1.4.4.4.4.4.4.4.4.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.4.4.4.4.4.2.m1.1c">\pm</annotation></semantics></math> 0.09)</span></span>
<span id="S4.T1.6.6.6.6.6.6.6.6" class="ltx_tr">
<span id="S4.T1.6.6.6.6.6.6.6.6.3" class="ltx_td ltx_align_left">Cascade (Whisper + Qwen-7B + StyleTTS 2)</span>
<span id="S4.T1.5.5.5.5.5.5.5.5.1" class="ltx_td ltx_align_center">3.24 (<math id="S4.T1.5.5.5.5.5.5.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.5.5.5.5.5.5.5.5.1.m1.1a"><mo id="S4.T1.5.5.5.5.5.5.5.5.1.m1.1.1" xref="S4.T1.5.5.5.5.5.5.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.5.5.5.5.5.1.m1.1b"><csymbol cd="latexml" id="S4.T1.5.5.5.5.5.5.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.5.5.5.5.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.5.5.5.5.5.1.m1.1c">\pm</annotation></semantics></math> 0.10)</span>
<span id="S4.T1.6.6.6.6.6.6.6.6.2" class="ltx_td ltx_align_center">3.63 (<math id="S4.T1.6.6.6.6.6.6.6.6.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.6.6.6.6.6.6.6.6.2.m1.1a"><mo id="S4.T1.6.6.6.6.6.6.6.6.2.m1.1.1" xref="S4.T1.6.6.6.6.6.6.6.6.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.6.6.6.6.6.2.m1.1b"><csymbol cd="latexml" id="S4.T1.6.6.6.6.6.6.6.6.2.m1.1.1.cmml" xref="S4.T1.6.6.6.6.6.6.6.6.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.6.6.6.6.6.2.m1.1c">\pm</annotation></semantics></math> 0.08)</span></span>
<span id="S4.T1.8.8.8.8.8.8.8.8" class="ltx_tr">
<span id="S4.T1.8.8.8.8.8.8.8.8.3" class="ltx_td ltx_align_left">Style-Talker (Proposed)</span>
<span id="S4.T1.7.7.7.7.7.7.7.7.1" class="ltx_td ltx_align_center"><span id="S4.T1.7.7.7.7.7.7.7.7.1.1" class="ltx_text ltx_font_bold">3.55</span> (<math id="S4.T1.7.7.7.7.7.7.7.7.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.7.7.7.7.7.7.7.7.1.m1.1a"><mo id="S4.T1.7.7.7.7.7.7.7.7.1.m1.1.1" xref="S4.T1.7.7.7.7.7.7.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.7.7.7.7.7.7.1.m1.1b"><csymbol cd="latexml" id="S4.T1.7.7.7.7.7.7.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.7.7.7.7.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.7.7.7.7.7.7.1.m1.1c">\pm</annotation></semantics></math> <span id="S4.T1.7.7.7.7.7.7.7.7.1.2" class="ltx_text ltx_font_bold">0.09</span>)</span>
<span id="S4.T1.8.8.8.8.8.8.8.8.2" class="ltx_td ltx_align_center"><span id="S4.T1.8.8.8.8.8.8.8.8.2.1" class="ltx_text ltx_font_bold">3.90</span> (<math id="S4.T1.8.8.8.8.8.8.8.8.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.8.8.8.8.8.8.8.8.2.m1.1a"><mo id="S4.T1.8.8.8.8.8.8.8.8.2.m1.1.1" xref="S4.T1.8.8.8.8.8.8.8.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.8.8.8.8.8.8.2.m1.1b"><csymbol cd="latexml" id="S4.T1.8.8.8.8.8.8.8.8.2.m1.1.1.cmml" xref="S4.T1.8.8.8.8.8.8.8.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.8.8.8.8.8.8.2.m1.1c">\pm</annotation></semantics></math> <span id="S4.T1.8.8.8.8.8.8.8.8.2.2" class="ltx_text ltx_font_bold">0.09</span>)</span></span>
<span id="S4.T1.10.10.10.10.10.10.10.10" class="ltx_tr">
<span id="S4.T1.10.10.10.10.10.10.10.10.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_3"><span id="S4.T1.10.10.10.10.10.10.10.10.3.1" class="ltx_text">PodcastFillers</span></span>
<span id="S4.T1.10.10.10.10.10.10.10.10.4" class="ltx_td ltx_align_left ltx_border_t">Ground Truth</span>
<span id="S4.T1.9.9.9.9.9.9.9.9.1" class="ltx_td ltx_align_center ltx_border_t">4.22 (<math id="S4.T1.9.9.9.9.9.9.9.9.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.9.9.9.9.9.9.9.9.1.m1.1a"><mo id="S4.T1.9.9.9.9.9.9.9.9.1.m1.1.1" xref="S4.T1.9.9.9.9.9.9.9.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.9.9.9.9.9.9.1.m1.1b"><csymbol cd="latexml" id="S4.T1.9.9.9.9.9.9.9.9.1.m1.1.1.cmml" xref="S4.T1.9.9.9.9.9.9.9.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.9.9.9.9.9.9.1.m1.1c">\pm</annotation></semantics></math> 0.08)</span>
<span id="S4.T1.10.10.10.10.10.10.10.10.2" class="ltx_td ltx_align_center ltx_border_t">4.18 (<math id="S4.T1.10.10.10.10.10.10.10.10.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.10.10.10.10.10.10.10.10.2.m1.1a"><mo id="S4.T1.10.10.10.10.10.10.10.10.2.m1.1.1" xref="S4.T1.10.10.10.10.10.10.10.10.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.10.10.10.10.10.10.2.m1.1b"><csymbol cd="latexml" id="S4.T1.10.10.10.10.10.10.10.10.2.m1.1.1.cmml" xref="S4.T1.10.10.10.10.10.10.10.10.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.10.10.10.10.10.10.2.m1.1c">\pm</annotation></semantics></math> 0.09)</span></span>
<span id="S4.T1.12.12.12.12.12.12.12.12" class="ltx_tr">
<span id="S4.T1.12.12.12.12.12.12.12.12.3" class="ltx_td ltx_align_left">Cascade (Whisper + Qwen-7B + StyleTTS 2)</span>
<span id="S4.T1.11.11.11.11.11.11.11.11.1" class="ltx_td ltx_align_center">3.22 (<math id="S4.T1.11.11.11.11.11.11.11.11.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.11.11.11.11.11.11.11.11.1.m1.1a"><mo id="S4.T1.11.11.11.11.11.11.11.11.1.m1.1.1" xref="S4.T1.11.11.11.11.11.11.11.11.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.11.11.11.11.11.11.1.m1.1b"><csymbol cd="latexml" id="S4.T1.11.11.11.11.11.11.11.11.1.m1.1.1.cmml" xref="S4.T1.11.11.11.11.11.11.11.11.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.11.11.11.11.11.11.1.m1.1c">\pm</annotation></semantics></math> 0.09)</span>
<span id="S4.T1.12.12.12.12.12.12.12.12.2" class="ltx_td ltx_align_center">3.53 (<math id="S4.T1.12.12.12.12.12.12.12.12.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.12.12.12.12.12.12.12.12.2.m1.1a"><mo id="S4.T1.12.12.12.12.12.12.12.12.2.m1.1.1" xref="S4.T1.12.12.12.12.12.12.12.12.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.12.12.12.12.12.12.12.12.2.m1.1b"><csymbol cd="latexml" id="S4.T1.12.12.12.12.12.12.12.12.2.m1.1.1.cmml" xref="S4.T1.12.12.12.12.12.12.12.12.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.12.12.12.12.12.12.12.2.m1.1c">\pm</annotation></semantics></math> 0.10)</span></span>
<span id="S4.T1.14.14.14.14.14.14.14.14" class="ltx_tr">
<span id="S4.T1.14.14.14.14.14.14.14.14.3" class="ltx_td ltx_align_left ltx_border_bb">Style-Talker (Proposed)</span>
<span id="S4.T1.13.13.13.13.13.13.13.13.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.13.13.13.13.13.13.13.13.1.1" class="ltx_text ltx_font_bold">3.76</span> (<math id="S4.T1.13.13.13.13.13.13.13.13.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.13.13.13.13.13.13.13.13.1.m1.1a"><mo id="S4.T1.13.13.13.13.13.13.13.13.1.m1.1.1" xref="S4.T1.13.13.13.13.13.13.13.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.13.13.13.13.13.13.13.13.1.m1.1b"><csymbol cd="latexml" id="S4.T1.13.13.13.13.13.13.13.13.1.m1.1.1.cmml" xref="S4.T1.13.13.13.13.13.13.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.13.13.13.13.13.13.13.1.m1.1c">\pm</annotation></semantics></math> <span id="S4.T1.13.13.13.13.13.13.13.13.1.2" class="ltx_text ltx_font_bold">0.09</span>)</span>
<span id="S4.T1.14.14.14.14.14.14.14.14.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.14.14.14.14.14.14.14.14.2.1" class="ltx_text ltx_font_bold">4.02</span> (<math id="S4.T1.14.14.14.14.14.14.14.14.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.14.14.14.14.14.14.14.14.2.m1.1a"><mo id="S4.T1.14.14.14.14.14.14.14.14.2.m1.1.1" xref="S4.T1.14.14.14.14.14.14.14.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.14.14.14.14.14.14.14.14.2.m1.1b"><csymbol cd="latexml" id="S4.T1.14.14.14.14.14.14.14.14.2.m1.1.1.cmml" xref="S4.T1.14.14.14.14.14.14.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.14.14.14.14.14.14.14.2.m1.1c">\pm</annotation></semantics></math> <span id="S4.T1.14.14.14.14.14.14.14.14.2.2" class="ltx_text ltx_font_bold">0.09</span> )</span></span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Mean opinion score of naturalness (MOS-N) and coherence (MOS-C) with 95% confidence intervals (CI) from human subjective evaluations. </figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">We evaluated our system and baseline systems on two benchmark datasets: DailyTalk <cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite> and PodcastFillers <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite>. The former is a two-speaker studio-recorded conversation dataset, and the latter is a spoken dialog dataset mined in the wild.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p">DailyTalk <cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite> is a multi-turn conversation dataset comprising 20 hours of spoken dialog data from two speakers, one male and one female, totaling 2,541 conversations. The dataset was recorded in a studio with contrived scripts by voice actors of non-native English speaker origins. We kept 100 and 200 conversations as validation and test set, respectively, and the remaining 2,241 conversations were used as the training set.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p">Although DailyTalk offers transcribed conversational speech, the audio recordings are contrived rather than captured in natural, real-world settings.
We thus also conducted experiments on the PodcastFillers  <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib36" title="" class="ltx_ref">2022</a>)</cite>, a dataset of conversations from podcast recordings produced in uncontrolled settings.
The PodcastFillers dataset consists of 145 hours of gender-balanced podcast recordings featuring over 350 speakers, sourced from SoundCloud. It contains 199 podcasts in total.
Since transcriptions provided with the PodcastFillers dataset do not contain speaker diarizations and filler words, we diarized and re-transcribed the dataset (see Appendix <a href="#A1.SS4" title="A.4 PodcastFiller Dataset ‣ Appendix A Implementation Details ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.4</span></a> for details). We divided the dataset by podcast, with a 173/6/20 split for training, validation, and test sets.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluations</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">We evaluated our SDS against two other systems: the traditional ASR-LLM-TTS cascade system and the end-to-end (E2E) speech-to-speech approach. Since there is no publicly available E2E speech-to-speech baseline with multi-turn support at the point the paper is written, following <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib13" title="" class="ltx_ref">2024</a>)</cite>, we fine-tuned a single-turn E2E model, SpeechGPT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib31" title="" class="ltx_ref">2023</a>)</cite> <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">7B-cm</span> <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Avaiable at <a target="_blank" href="https://huggingface.co/fnlp/SpeechGPT-7B-cm" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/fnlp/SpeechGPT-7B-cm</a></span></span></span>, as the E2E baseline. Due to the limited context window SpeechGPT presents, we failed to fine-tune it on the more complicated PodcastFillers dataset to generate meaningful responses. Thus, we only evaluated SpeechGPT on the simpler DailyTalk dataset as in <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib13" title="" class="ltx_ref">2024</a>)</cite>. For more details of our baseline systems, including model choices of cascade baseline, please refer to Appendix <a href="#A1.SS3" title="A.3 Baseline Systems ‣ Appendix A Implementation Details ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.3</span></a>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<p id="S4.T2.5" class="ltx_p ltx_align_center"><span id="S4.T2.5.5" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S4.T2.5.5.5.5" class="ltx_inline-block ltx_transformed_outer" style="width:439.3pt;height:109pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S4.T2.5.5.5.5.5" class="ltx_p"><span id="S4.T2.5.5.5.5.5.5" class="ltx_text">
<span id="S4.T2.5.5.5.5.5.5.5" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.5.5.5.5.5.5.5.5" class="ltx_tr">
<span id="S4.T2.5.5.5.5.5.5.5.5.6" class="ltx_td ltx_align_left ltx_border_tt">Dataset</span>
<span id="S4.T2.5.5.5.5.5.5.5.5.7" class="ltx_td ltx_align_left ltx_border_tt">Model</span>
<span id="S4.T2.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">BLEU <math id="S4.T2.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.1.1.1.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span>
<span id="S4.T2.2.2.2.2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt">ROUGE-L <math id="S4.T2.2.2.2.2.2.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.2.2.2.2.2.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T2.2.2.2.2.2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.2.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.2.2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.2.2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.2.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.2.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math></span>
<span id="S4.T2.3.3.3.3.3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">METEOR <math id="S4.T2.3.3.3.3.3.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.3.3.3.3.3.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T2.3.3.3.3.3.3.3.3.3.m1.1.1" xref="S4.T2.3.3.3.3.3.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.3.3.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.3.3.3.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.3.3.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.3.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math></span>
<span id="S4.T2.4.4.4.4.4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_tt">BERT Score <math id="S4.T2.4.4.4.4.4.4.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.4.4.4.4.4.4.4.4.4.m1.1a"><mo stretchy="false" id="S4.T2.4.4.4.4.4.4.4.4.4.m1.1.1" xref="S4.T2.4.4.4.4.4.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.4.4.4.4.4.4.m1.1b"><ci id="S4.T2.4.4.4.4.4.4.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.4.4.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.4.4.4.4.4.4.m1.1c">\uparrow</annotation></semantics></math></span>
<span id="S4.T2.5.5.5.5.5.5.5.5.5" class="ltx_td ltx_align_center ltx_border_tt">WER <math id="S4.T2.5.5.5.5.5.5.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.5.5.5.5.5.5.5.5.5.m1.1a"><mo stretchy="false" id="S4.T2.5.5.5.5.5.5.5.5.5.m1.1.1" xref="S4.T2.5.5.5.5.5.5.5.5.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.5.5.5.5.5.5.m1.1b"><ci id="S4.T2.5.5.5.5.5.5.5.5.5.m1.1.1.cmml" xref="S4.T2.5.5.5.5.5.5.5.5.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.5.5.5.5.5.5.m1.1c">\downarrow</annotation></semantics></math></span></span>
<span id="S4.T2.5.5.5.5.5.5.5.6" class="ltx_tr">
<span id="S4.T2.5.5.5.5.5.5.5.6.1" class="ltx_td ltx_align_left ltx_border_t ltx_rowspan ltx_rowspan_3"><span id="S4.T2.5.5.5.5.5.5.5.6.1.1" class="ltx_text">DailyTalk</span></span>
<span id="S4.T2.5.5.5.5.5.5.5.6.2" class="ltx_td ltx_align_left ltx_border_t">SpeechGPT</span>
<span id="S4.T2.5.5.5.5.5.5.5.6.3" class="ltx_td ltx_align_center ltx_border_t">1.21</span>
<span id="S4.T2.5.5.5.5.5.5.5.6.4" class="ltx_td ltx_align_center ltx_border_t">9.02</span>
<span id="S4.T2.5.5.5.5.5.5.5.6.5" class="ltx_td ltx_align_center ltx_border_t">14.89</span>
<span id="S4.T2.5.5.5.5.5.5.5.6.6" class="ltx_td ltx_align_center ltx_border_t">77.13</span>
<span id="S4.T2.5.5.5.5.5.5.5.6.7" class="ltx_td ltx_align_center ltx_border_t">22.58</span></span>
<span id="S4.T2.5.5.5.5.5.5.5.7" class="ltx_tr">
<span id="S4.T2.5.5.5.5.5.5.5.7.1" class="ltx_td ltx_align_left">Cascade</span>
<span id="S4.T2.5.5.5.5.5.5.5.7.2" class="ltx_td ltx_align_center">3.00</span>
<span id="S4.T2.5.5.5.5.5.5.5.7.3" class="ltx_td ltx_align_center">11.92</span>
<span id="S4.T2.5.5.5.5.5.5.5.7.4" class="ltx_td ltx_align_center">22.48</span>
<span id="S4.T2.5.5.5.5.5.5.5.7.5" class="ltx_td ltx_align_center">88.99</span>
<span id="S4.T2.5.5.5.5.5.5.5.7.6" class="ltx_td ltx_align_center">16.22</span></span>
<span id="S4.T2.5.5.5.5.5.5.5.8" class="ltx_tr">
<span id="S4.T2.5.5.5.5.5.5.5.8.1" class="ltx_td ltx_align_left">Style-Talker</span>
<span id="S4.T2.5.5.5.5.5.5.5.8.2" class="ltx_td ltx_align_center"><span id="S4.T2.5.5.5.5.5.5.5.8.2.1" class="ltx_text ltx_font_bold">4.62</span></span>
<span id="S4.T2.5.5.5.5.5.5.5.8.3" class="ltx_td ltx_align_center"><span id="S4.T2.5.5.5.5.5.5.5.8.3.1" class="ltx_text ltx_font_bold">16.14</span></span>
<span id="S4.T2.5.5.5.5.5.5.5.8.4" class="ltx_td ltx_align_center"><span id="S4.T2.5.5.5.5.5.5.5.8.4.1" class="ltx_text ltx_font_bold">25.72</span></span>
<span id="S4.T2.5.5.5.5.5.5.5.8.5" class="ltx_td ltx_align_center"><span id="S4.T2.5.5.5.5.5.5.5.8.5.1" class="ltx_text ltx_font_bold">90.40</span></span>
<span id="S4.T2.5.5.5.5.5.5.5.8.6" class="ltx_td ltx_align_center"><span id="S4.T2.5.5.5.5.5.5.5.8.6.1" class="ltx_text ltx_font_bold">12.01</span></span></span>
<span id="S4.T2.5.5.5.5.5.5.5.9" class="ltx_tr">
<span id="S4.T2.5.5.5.5.5.5.5.9.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_2"><span id="S4.T2.5.5.5.5.5.5.5.9.1.1" class="ltx_text">PodcastFillers</span></span>
<span id="S4.T2.5.5.5.5.5.5.5.9.2" class="ltx_td ltx_align_left ltx_border_t">Cascade</span>
<span id="S4.T2.5.5.5.5.5.5.5.9.3" class="ltx_td ltx_align_center ltx_border_t">0.43</span>
<span id="S4.T2.5.5.5.5.5.5.5.9.4" class="ltx_td ltx_align_center ltx_border_t">7.08</span>
<span id="S4.T2.5.5.5.5.5.5.5.9.5" class="ltx_td ltx_align_center ltx_border_t">10.44</span>
<span id="S4.T2.5.5.5.5.5.5.5.9.6" class="ltx_td ltx_align_center ltx_border_t">83.98</span>
<span id="S4.T2.5.5.5.5.5.5.5.9.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.5.5.5.5.5.5.5.9.7.1" class="ltx_text ltx_font_bold">10.61</span></span></span>
<span id="S4.T2.5.5.5.5.5.5.5.10" class="ltx_tr">
<span id="S4.T2.5.5.5.5.5.5.5.10.1" class="ltx_td ltx_align_left ltx_border_bb">Style-Talker</span>
<span id="S4.T2.5.5.5.5.5.5.5.10.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.5.5.5.5.5.5.5.10.2.1" class="ltx_text ltx_font_bold">3.31</span></span>
<span id="S4.T2.5.5.5.5.5.5.5.10.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.5.5.5.5.5.5.5.10.3.1" class="ltx_text ltx_font_bold">10.06</span></span>
<span id="S4.T2.5.5.5.5.5.5.5.10.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.5.5.5.5.5.5.5.10.4.1" class="ltx_text ltx_font_bold">17.00</span></span>
<span id="S4.T2.5.5.5.5.5.5.5.10.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.5.5.5.5.5.5.5.10.5.1" class="ltx_text ltx_font_bold">84.26</span></span>
<span id="S4.T2.5.5.5.5.5.5.5.10.6" class="ltx_td ltx_align_center ltx_border_bb">15.59</span></span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Semantic evaluations of F-1 score (%) for BLEU, ROUGE-L, METEOR, BERT Score and word error rate (WER). WER was computed with filler words and punctuation removed. </figcaption>
</figure>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Subjective Evaluations</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">We used two metrics for the mean opinion score (MOS) as subjective evaluations: MOS-N for naturalness and MOS-C for coherence. Unlike <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a href="#bib.bib32" title="" class="ltx_ref">2024</a>)</cite> that only evaluates the speaker similarity, we asked the evaluators to rate the coherence of the generated speech, similar to how written dialogues are evaluated in text <cite class="ltx_cite ltx_citemacro_citep">(Zhong et al., <a href="#bib.bib35" title="" class="ltx_ref">2022</a>)</cite>. Both metrics involve the speech and the content of the responses. For MOS-N, the raters were only instructed to rate the naturalness of the response, that is, how likely the speech response is produced by a human instead of a robot based on both the speech quality and the content spoken. In contrast, MOS-C measures how well the response speech follows the previous conversation context, given the speech’s content and the speaker’s prosody and identity (see Appendix <a href="#A3" title="Appendix C Subjective Evaluation Details ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a> for details). These evaluations were conducted by native English speakers from the U.S. on Amazon MTurk . For each test, we included 10 to 20 conversations, each provided with the previous 3 turns of conversation as the context. Each speech response set was rated by 5 to 10 evaluators on a 1-5 scale, with increments of 1. We followed <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite> that randomized the model order and kept their labels hidden, similar to MUSHRA <cite class="ltx_cite ltx_citemacro_citep">(BS, <a href="#bib.bib6" title="" class="ltx_ref">2003</a>)</cite>. We used the rating of the ground truth response as the attention checker and excluded raters who did not rank the ground truth as the highest for both MOS metric.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Objective Evaluations</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">We followed <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a href="#bib.bib21" title="" class="ltx_ref">2024</a>)</cite> to evaluate the response speech at both acoustic and semantic levels. Since our speaking styles are self-supervised, unlike <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a href="#bib.bib21" title="" class="ltx_ref">2024</a>)</cite> with categorical and pre-defined styles where the generated styles can be compared against the ground truth labels for F-1 score, we conducted acoustic evaluations following <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite> by calculating the correlations of several acoustic metrics for emotion recognition <cite class="ltx_cite ltx_citemacro_citep">(Busso et al., <a href="#bib.bib7" title="" class="ltx_ref">2013</a>)</cite> between generated and ground truth speech. These metrics include pitch mean, pitch standard deviation (STD), energy mean, energy standard deviation (STD), and harmonics-to-noise (HTN) ratio. We also added speech duration as an extra metric to evaluate whether the generated speech follows the same distribution as the ground truth in terms of speech duration. Moreover, we scored the speaker similarity by employing a pre-trained WavLM speaker embedding model <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Available at <a target="_blank" href="https://huggingface.co/microsoft/wavlm-base-plus-sv" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/microsoft/wavlm-base-plus-sv</a></span></span></span> following <cite class="ltx_cite ltx_citemacro_citet">Ju et al. (<a href="#bib.bib12" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">We used the generated text directly for semantic evaluation, as all models evaluated, including SpeechGPT, generate text as responses. We followed <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a href="#bib.bib21" title="" class="ltx_ref">2024</a>); Kim et al. (<a href="#bib.bib13" title="" class="ltx_ref">2024</a>)</cite> and employed commonly used text-generation metrics, including BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a href="#bib.bib26" title="" class="ltx_ref">2002</a>)</cite>, ROUGE <cite class="ltx_cite ltx_citemacro_citep">(Lin, <a href="#bib.bib19" title="" class="ltx_ref">2004</a>)</cite>, METEOR <cite class="ltx_cite ltx_citemacro_citep">(Banerjee &amp; Lavie, <a href="#bib.bib3" title="" class="ltx_ref">2005</a>)</cite> and BERT Score <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib33" title="" class="ltx_ref">2019</a>)</cite> against the ground truth text. Additionally, we calculated the word error rate (WER) with Whisper <span id="S4.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_italic">large-v2</span> as a semantic metric to evaluate the intelligibility of the generated speech. All metrics were computed with the Huggingface Evaluate package <span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://huggingface.co/docs/evaluate/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/docs/evaluate/</a></span></span></span>.</p>
</div>
<div id="S4.SS2.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS2.p3.1" class="ltx_p">Lastly, to test the real-time applicability, we computed the real-time factor (RTF), which is the ratio between the time used to generate a speech utterance and the duration of the generated speech, and the system delay by generating a 10-second response to a 10-second input utterance. The evaluation was conducted on a single Nvidia L40 GPU.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<figure id="S5.T3" class="ltx_table">
<p id="S5.T3.1" class="ltx_p ltx_align_center"><span id="S5.T3.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S5.T3.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:520.2pt;height:132.4pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S5.T3.1.1.1.1" class="ltx_p"><span id="S5.T3.1.1.1.1.1" class="ltx_text">
<span id="S5.T3.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Dataset</span>
<span id="S5.T3.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_tt">Model</span>
<span id="S5.T3.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.1.1.1.1.1.1.1.3.1" class="ltx_text"></span> <span id="S5.T3.1.1.1.1.1.1.1.3.2" class="ltx_text">
<span id="S5.T3.1.1.1.1.1.1.1.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.1.1.1.1.1.3.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.1.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Pitch</span></span>
<span id="S5.T3.1.1.1.1.1.1.1.3.2.1.2" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.1.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">mean</span></span>
</span></span><span id="S5.T3.1.1.1.1.1.1.1.3.3" class="ltx_text"></span></span>
<span id="S5.T3.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.1.1.1.1.1.1.1.4.1" class="ltx_text"></span> <span id="S5.T3.1.1.1.1.1.1.1.4.2" class="ltx_text">
<span id="S5.T3.1.1.1.1.1.1.1.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.1.1.1.1.1.4.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.1.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Pitch</span></span>
<span id="S5.T3.1.1.1.1.1.1.1.4.2.1.2" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.1.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">STD</span></span>
</span></span><span id="S5.T3.1.1.1.1.1.1.1.4.3" class="ltx_text"></span></span>
<span id="S5.T3.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.1.1.1.1.1.1.1.5.1" class="ltx_text"></span> <span id="S5.T3.1.1.1.1.1.1.1.5.2" class="ltx_text">
<span id="S5.T3.1.1.1.1.1.1.1.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.1.1.1.1.1.5.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.1.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Energy</span></span>
<span id="S5.T3.1.1.1.1.1.1.1.5.2.1.2" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.1.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">mean</span></span>
</span></span><span id="S5.T3.1.1.1.1.1.1.1.5.3" class="ltx_text"></span></span>
<span id="S5.T3.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.1.1.1.1.1.1.1.6.1" class="ltx_text"></span> <span id="S5.T3.1.1.1.1.1.1.1.6.2" class="ltx_text">
<span id="S5.T3.1.1.1.1.1.1.1.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.1.1.1.1.1.6.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.1.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Energy</span></span>
<span id="S5.T3.1.1.1.1.1.1.1.6.2.1.2" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.1.6.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">STD</span></span>
</span></span><span id="S5.T3.1.1.1.1.1.1.1.6.3" class="ltx_text"></span></span>
<span id="S5.T3.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.1.1.1.1.1.1.1.7.1" class="ltx_text"></span> <span id="S5.T3.1.1.1.1.1.1.1.7.2" class="ltx_text">
<span id="S5.T3.1.1.1.1.1.1.1.7.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.1.1.1.1.1.7.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.1.7.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">HTN</span></span>
<span id="S5.T3.1.1.1.1.1.1.1.7.2.1.2" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.1.7.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ratio</span></span>
</span></span><span id="S5.T3.1.1.1.1.1.1.1.7.3" class="ltx_text"></span></span>
<span id="S5.T3.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.1.1.1.1.1.1.1.8.1" class="ltx_text"></span> <span id="S5.T3.1.1.1.1.1.1.1.8.2" class="ltx_text">
<span id="S5.T3.1.1.1.1.1.1.1.8.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.1.1.1.1.1.8.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.1.8.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Speech</span></span>
<span id="S5.T3.1.1.1.1.1.1.1.8.2.1.2" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.1.8.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">duration</span></span>
</span></span><span id="S5.T3.1.1.1.1.1.1.1.8.3" class="ltx_text"></span></span>
<span id="S5.T3.1.1.1.1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.1.1.1.1.1.1.1.9.1" class="ltx_text"></span> <span id="S5.T3.1.1.1.1.1.1.1.9.2" class="ltx_text">
<span id="S5.T3.1.1.1.1.1.1.1.9.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T3.1.1.1.1.1.1.1.9.2.1.1" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.1.9.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Speaker</span></span>
<span id="S5.T3.1.1.1.1.1.1.1.9.2.1.2" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.1.9.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">similarity</span></span>
</span></span><span id="S5.T3.1.1.1.1.1.1.1.9.3" class="ltx_text"></span></span></span>
<span id="S5.T3.1.1.1.1.1.1.2" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t ltx_rowspan ltx_rowspan_3"><span id="S5.T3.1.1.1.1.1.1.2.1.1" class="ltx_text">DailyTalk</span></span>
<span id="S5.T3.1.1.1.1.1.1.2.2" class="ltx_td ltx_align_left ltx_border_t">SpeechGPT</span>
<span id="S5.T3.1.1.1.1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">0.62</span>
<span id="S5.T3.1.1.1.1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">0.07</span>
<span id="S5.T3.1.1.1.1.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">0.70</span>
<span id="S5.T3.1.1.1.1.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t">0.00</span>
<span id="S5.T3.1.1.1.1.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t">0.39</span>
<span id="S5.T3.1.1.1.1.1.1.2.8" class="ltx_td ltx_align_center ltx_border_t">0.12</span>
<span id="S5.T3.1.1.1.1.1.1.2.9" class="ltx_td ltx_align_center ltx_border_t">0.75</span></span>
<span id="S5.T3.1.1.1.1.1.1.3" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.3.1" class="ltx_td ltx_align_left">Cascade</span>
<span id="S5.T3.1.1.1.1.1.1.3.2" class="ltx_td ltx_align_center">0.76</span>
<span id="S5.T3.1.1.1.1.1.1.3.3" class="ltx_td ltx_align_center">0.13</span>
<span id="S5.T3.1.1.1.1.1.1.3.4" class="ltx_td ltx_align_center">0.72</span>
<span id="S5.T3.1.1.1.1.1.1.3.5" class="ltx_td ltx_align_center">0.03</span>
<span id="S5.T3.1.1.1.1.1.1.3.6" class="ltx_td ltx_align_center">0.53</span>
<span id="S5.T3.1.1.1.1.1.1.3.7" class="ltx_td ltx_align_center">0.19</span>
<span id="S5.T3.1.1.1.1.1.1.3.8" class="ltx_td ltx_align_center">0.83</span></span>
<span id="S5.T3.1.1.1.1.1.1.4" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.4.1" class="ltx_td ltx_align_left">Style-Talker</span>
<span id="S5.T3.1.1.1.1.1.1.4.2" class="ltx_td ltx_align_center"><span id="S5.T3.1.1.1.1.1.1.4.2.1" class="ltx_text ltx_font_bold">0.81</span></span>
<span id="S5.T3.1.1.1.1.1.1.4.3" class="ltx_td ltx_align_center"><span id="S5.T3.1.1.1.1.1.1.4.3.1" class="ltx_text ltx_font_bold">0.26</span></span>
<span id="S5.T3.1.1.1.1.1.1.4.4" class="ltx_td ltx_align_center"><span id="S5.T3.1.1.1.1.1.1.4.4.1" class="ltx_text ltx_font_bold">0.73</span></span>
<span id="S5.T3.1.1.1.1.1.1.4.5" class="ltx_td ltx_align_center"><span id="S5.T3.1.1.1.1.1.1.4.5.1" class="ltx_text ltx_font_bold">0.07</span></span>
<span id="S5.T3.1.1.1.1.1.1.4.6" class="ltx_td ltx_align_center"><span id="S5.T3.1.1.1.1.1.1.4.6.1" class="ltx_text ltx_font_bold">0.56</span></span>
<span id="S5.T3.1.1.1.1.1.1.4.7" class="ltx_td ltx_align_center"><span id="S5.T3.1.1.1.1.1.1.4.7.1" class="ltx_text ltx_font_bold">0.25</span></span>
<span id="S5.T3.1.1.1.1.1.1.4.8" class="ltx_td ltx_align_center"><span id="S5.T3.1.1.1.1.1.1.4.8.1" class="ltx_text ltx_font_bold">0.86</span></span></span>
<span id="S5.T3.1.1.1.1.1.1.5" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_2"><span id="S5.T3.1.1.1.1.1.1.5.1.1" class="ltx_text">PodcastFillers</span></span>
<span id="S5.T3.1.1.1.1.1.1.5.2" class="ltx_td ltx_align_left ltx_border_t">Cascade</span>
<span id="S5.T3.1.1.1.1.1.1.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.1.1.1.1.5.3.1" class="ltx_text ltx_font_bold">0.72</span></span>
<span id="S5.T3.1.1.1.1.1.1.5.4" class="ltx_td ltx_align_center ltx_border_t">0.24</span>
<span id="S5.T3.1.1.1.1.1.1.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T3.1.1.1.1.1.1.5.5.1" class="ltx_text ltx_font_bold">0.82</span></span>
<span id="S5.T3.1.1.1.1.1.1.5.6" class="ltx_td ltx_align_center ltx_border_t">0.44</span>
<span id="S5.T3.1.1.1.1.1.1.5.7" class="ltx_td ltx_align_center ltx_border_t">0.45</span>
<span id="S5.T3.1.1.1.1.1.1.5.8" class="ltx_td ltx_align_center ltx_border_t">0.17</span>
<span id="S5.T3.1.1.1.1.1.1.5.9" class="ltx_td ltx_align_center ltx_border_t">0.82</span></span>
<span id="S5.T3.1.1.1.1.1.1.6" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.6.1" class="ltx_td ltx_align_left ltx_border_bb">Style-Talker</span>
<span id="S5.T3.1.1.1.1.1.1.6.2" class="ltx_td ltx_align_center ltx_border_bb">0.68</span>
<span id="S5.T3.1.1.1.1.1.1.6.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.1.1.1.1.1.1.6.3.1" class="ltx_text ltx_font_bold">0.28</span></span>
<span id="S5.T3.1.1.1.1.1.1.6.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.1.1.1.1.1.1.6.4.1" class="ltx_text ltx_font_bold">0.82</span></span>
<span id="S5.T3.1.1.1.1.1.1.6.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.1.1.1.1.1.1.6.5.1" class="ltx_text ltx_font_bold">0.58</span></span>
<span id="S5.T3.1.1.1.1.1.1.6.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.1.1.1.1.1.1.6.6.1" class="ltx_text ltx_font_bold">0.51</span></span>
<span id="S5.T3.1.1.1.1.1.1.6.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.1.1.1.1.1.1.6.7.1" class="ltx_text ltx_font_bold">0.20</span></span>
<span id="S5.T3.1.1.1.1.1.1.6.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.1.1.1.1.1.1.6.8.1" class="ltx_text ltx_font_bold">0.84</span></span></span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of Pearson correlation coefficients of acoustic features associated with emotions and speech duration between ground truth and response speech and cosine similarity of speaker embeddings between ground truth and response speech.</figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Model Performance</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">According to the subjective evaluation results presented in Table <a href="#S4.T1" title="Table 1 ‣ 4 Experiments ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, Style-Talker achieves a significant improvement in conversation naturalness and coherence against both the cascade and speech-to-speech baselines on the DailyTalk dataset. Furthermore, when evaluated on the PodcastFillers dataset, which comprises recordings in more realistic and noisy settings, Style-Talker still outperforms the cascade baseline, showcasing its robustness and adaptability in various settings, whereas SpeechGPT fails to produce meaningful speech. Importantly, Table <a href="#S5.T5" title="Table 5 ‣ 5.2 Ablation Study ‣ 5 Results ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows that our system is nearly twice as fast as the cascade system and four times faster than the SpeechGPT baseline. Our system only generates 10-second audio from 10-second input audio in around 1.5 seconds, significantly faster than other baseline systems, making it feasible for real-time applications in human-computer interaction.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p">In terms of semantic similarity to ground truth responses, Style-Talker showed a strong alignment, as evidenced in Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Evaluations ‣ 4 Experiments ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. This is notable even as both the baseline systems and Style-Talker experience reduced semantic metrics on the PodcastFillers dataset due to its complexity from a more natural recording setting. Remarkably, Style-Talker consistently outperforms the cascade baselines in text dialog generation, even though semantic evaluation occurs in the text domain without any involvement of other modalities. This suggests that Style-Talker’s architecture, which integrates audio input and previous speaking styles into the context for generating text and style, contributes positively to the semantic quality of generated text. This aspect is further explored in the ablation study detailed in Section <a href="#S5.SS2" title="5.2 Ablation Study ‣ 5 Results ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>, highlighting the semantic significance of incorporating audio and style context in dialog generation. In Table <a href="#S5.T3" title="Table 3 ‣ 5 Results ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, acoustic features of the speech responses generated by Style-Talker show a higher correlation with those of the ground truth responses when compared to other baseline systems. This evidence strongly supports the efficacy of Style-Talker’s design in capturing and reproducing the nuanced acoustic properties associated with prosody and emotion in human speech, proving its ability to produce emotionally coherent responses.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Ablation Study</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p">To assess the contribution of each component to our system’s performance, we conducted an ablation study by evaluating the system under various configurations, involving the addition or omission of specific features. We focused on three variants of our system: one excluding style vectors from the dialog context (“<span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">– style in context</span>”), another substituting the audio language model (LM) with a conventional LM and instead using the text transcription and style of the input speech (“<span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_italic">– audio input</span>”), and lastly our proposed system provided with the additional text transcription and style vector of input speech (“<span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_italic">+ ASR &amp; style</span>”). We summarized the results in Table <a href="#S5.T5" title="Table 5 ‣ 5.2 Ablation Study ‣ 5 Results ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> by averaging all metrics evaluated and added the averaged metrics on both DailyTalk and PodcastFillers (see Appendix <a href="#A1.SS4" title="A.4 PodcastFiller Dataset ‣ Appendix A Implementation Details ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.4</span></a> for full results).</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p">Table <a href="#S5.T5" title="Table 5 ‣ 5.2 Ablation Study ‣ 5 Results ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows that omitting the speaking styles from previous dialog turns adversely affects the quality of speech responses, impacting both semantic and acoustic aspects. This underscores the importance of incorporating speech styles into the context to produce responses that are semantically rich and prosodically aligned with the previous conversation. Conversely, substituting audio input with its text transcription and style vector has a less detrimental effect. This can be attributed to the style vector serving as a summary of the input’s paralinguistic features. However, this configuration requires the use of an ASR model, markedly slowing down the system’s processing speed. It is notable that including the current input speech’s text and style does not enhance system performance, suggesting that the audio encoder is able to extract necessary information from the input audio that is contained in the text and style vector, making these additional inputs redundant. This finding highlights the efficiency of the audio encoder in extracting relevant text and style cues without the need for explicit provision of such information.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T5.fig1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:151.8pt;">
<table id="S5.T5.fig1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T5.fig1.1.1" class="ltx_tr">
<td id="S5.T5.fig1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Model</td>
<td id="S5.T5.fig1.1.1.2" class="ltx_td ltx_align_left ltx_border_tt">RTF</td>
<td id="S5.T5.fig1.1.1.3" class="ltx_td ltx_align_left ltx_border_tt">Delay</td>
</tr>
<tr id="S5.T5.fig1.1.2" class="ltx_tr">
<td id="S5.T5.fig1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Style-Talker</td>
<td id="S5.T5.fig1.1.2.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S5.T5.fig1.1.2.2.1" class="ltx_text ltx_font_bold">0.3873</span></td>
<td id="S5.T5.fig1.1.2.3" class="ltx_td ltx_align_left ltx_border_t">
<span id="S5.T5.fig1.1.2.3.1" class="ltx_text ltx_font_bold">1.53</span> s</td>
</tr>
<tr id="S5.T5.fig1.1.3" class="ltx_tr">
<td id="S5.T5.fig1.1.3.1" class="ltx_td ltx_align_left">SpeechGPT</td>
<td id="S5.T5.fig1.1.3.2" class="ltx_td ltx_align_left">1.3246</td>
<td id="S5.T5.fig1.1.3.3" class="ltx_td ltx_align_left">13.82 s</td>
</tr>
<tr id="S5.T5.fig1.1.4" class="ltx_tr">
<td id="S5.T5.fig1.1.4.1" class="ltx_td ltx_align_left ltx_border_bb">Cascade</td>
<td id="S5.T5.fig1.1.4.2" class="ltx_td ltx_align_left ltx_border_bb">0.5912</td>
<td id="S5.T5.fig1.1.4.3" class="ltx_td ltx_align_left ltx_border_bb">2.31 s</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 4: </span>The real-time factor (RTF) and system delay when processing a 10s input and producing a 10s output audio between various models.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T5.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:247.2pt;">
<table id="S5.T5.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S5.T5.3.3.3" class="ltx_tr">
<td id="S5.T5.3.3.3.4" class="ltx_td ltx_align_left ltx_border_tt">Model</td>
<td id="S5.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Semantic <math id="S5.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T5.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T5.1.1.1.1.m1.1.1" xref="S5.T5.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.1.m1.1b"><ci id="S5.T5.1.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S5.T5.2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt">Acoustic <math id="S5.T5.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T5.2.2.2.2.m1.1a"><mo stretchy="false" id="S5.T5.2.2.2.2.m1.1.1" xref="S5.T5.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.2.2.m1.1b"><ci id="S5.T5.2.2.2.2.m1.1.1.cmml" xref="S5.T5.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S5.T5.3.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">RTF <math id="S5.T5.3.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T5.3.3.3.3.m1.1a"><mo stretchy="false" id="S5.T5.3.3.3.3.m1.1.1" xref="S5.T5.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T5.3.3.3.3.m1.1b"><ci id="S5.T5.3.3.3.3.m1.1.1.cmml" xref="S5.T5.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T5.3.3.4" class="ltx_tr">
<td id="S5.T5.3.3.4.1" class="ltx_td ltx_align_left ltx_border_t">Proposed</td>
<td id="S5.T5.3.3.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.3.3.4.2.1" class="ltx_text ltx_font_bold">62.88</span></td>
<td id="S5.T5.3.3.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T5.3.3.4.3.1" class="ltx_text ltx_font_bold">1.07</span></td>
<td id="S5.T5.3.3.4.4" class="ltx_td ltx_align_center ltx_border_t">0.3824</td>
</tr>
<tr id="S5.T5.3.3.5" class="ltx_tr">
<td id="S5.T5.3.3.5.1" class="ltx_td ltx_align_left"><span id="S5.T5.3.3.5.1.1" class="ltx_text ltx_font_italic">– style in context</span></td>
<td id="S5.T5.3.3.5.2" class="ltx_td ltx_align_center">61.21</td>
<td id="S5.T5.3.3.5.3" class="ltx_td ltx_align_center">1.01</td>
<td id="S5.T5.3.3.5.4" class="ltx_td ltx_align_center"><span id="S5.T5.3.3.5.4.1" class="ltx_text ltx_font_bold">0.3792</span></td>
</tr>
<tr id="S5.T5.3.3.6" class="ltx_tr">
<td id="S5.T5.3.3.6.1" class="ltx_td ltx_align_left"><span id="S5.T5.3.3.6.1.1" class="ltx_text ltx_font_italic">– audio input</span></td>
<td id="S5.T5.3.3.6.2" class="ltx_td ltx_align_center">62.10</td>
<td id="S5.T5.3.3.6.3" class="ltx_td ltx_align_center">1.04</td>
<td id="S5.T5.3.3.6.4" class="ltx_td ltx_align_center">0.6912</td>
</tr>
<tr id="S5.T5.3.3.7" class="ltx_tr">
<td id="S5.T5.3.3.7.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T5.3.3.7.1.1" class="ltx_text ltx_font_italic">+ ASR &amp; style</span></td>
<td id="S5.T5.3.3.7.2" class="ltx_td ltx_align_center ltx_border_bb">62.76</td>
<td id="S5.T5.3.3.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T5.3.3.7.3.1" class="ltx_text ltx_font_bold">1.07</span></td>
<td id="S5.T5.3.3.7.4" class="ltx_td ltx_align_center ltx_border_bb">0.6857</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 5: </span>Ablation study results with aggregated semantic and acoustic scores (sum of averaged metrics in Table <a href="#A1.T6" title="Table 6 ‣ A.4 PodcastFiller Dataset ‣ Appendix A Implementation Details ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and Table <a href="#A2.T7" title="Table 7 ‣ Appendix B Detailed Ablation Study Results ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> on both DailyTalk and PodcastFillers datasets) and real-time factor. </figcaption>
</figure>
</div>
</div>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions and Limitations</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">In this work, we presented Style-Talker, a novel spoken dialog system (SDS) that integrates audio input with its transcribed speech context and speaking style. Our system demonstrates superior performance in naturalness, coherence, and processing speed on challenging datasets compared to traditional ASR-LLM-TTS cascades and end-to-end baselines. Despite these significant advancements and our careful attention to real-time responsiveness, Style-Talker still faces limitations in response speed and may not always surpass SDS with real-time ASR implemented. Moreover, the quality of the generated speech is dependent on the downstream TTS model’s performance, which may degrade when handling in-the-wild noisy datasets.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p">Future work will focus on enhancing real-time processing by encoding audio input in a causal manner and developing methods to summarize audio efficiently with minimal impact on input context token length. Improving the performance of TTS models for noisy, real-world datasets is another key area for further research. Additionally, our current system does not include explicit turn-taking mechanisms, so future research will explore incorporating efficient turn-taking strategies into the proposed framework.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Acknowledgements</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">We thank Cong Han for helping conducting subjective evaluations during the development of this work. This work is funded by the National Institutes of Health (NIH- NIDCD) and a grant from Marie-Josee and Henry R. Kravis.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agarwal et al. (2018)</span>
<span class="ltx_bibblock">
Shubham Agarwal, Ondrej Dusek, Ioannis Konstas, and Verena Rieser.

</span>
<span class="ltx_bibblock">Improving context modelling in multimodal dialogue generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.11955</em>, 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.16609</em>, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee &amp; Lavie (2005)</span>
<span class="ltx_bibblock">
Satanjeev Banerjee and Alon Lavie.

</span>
<span class="ltx_bibblock">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</em>, pp.  65–72, 2005.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrault et al. (2023)</span>
<span class="ltx_bibblock">
Loïc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, et al.

</span>
<span class="ltx_bibblock">Seamless: Multilingual expressive and streaming speech translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.05187</em>, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bredin et al. (2020)</span>
<span class="ltx_bibblock">
Hervé Bredin, Ruiqing Yin, Juan Manuel Coria, Gregory Gelly, Pavel Korshunov, Marvin Lavechin, Diego Fustes, Hadrien Titeux, Wassim Bouaziz, and Marie-Philippe Gill.

</span>
<span class="ltx_bibblock">Pyannote. audio: neural building blocks for speaker diarization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp.  7124–7128. IEEE, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BS (2003)</span>
<span class="ltx_bibblock">
ITUR BS.

</span>
<span class="ltx_bibblock">1534-1, method for the subjective assessment of intermediate quality level of coding systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">International Telecommunications Union, Geneva, Switzerland</em>, 14, 2003.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Busso et al. (2013)</span>
<span class="ltx_bibblock">
Carlos Busso, Murtaza Bulut, Shrikanth Narayanan, J Gratch, and S Marsella.

</span>
<span class="ltx_bibblock">Toward effective automatic recognition systems of emotion in speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Social emotions in nature and artifact: emotions in human and human-computer interaction</em>, 7(17):110–127, 2013.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al. (2023)</span>
<span class="ltx_bibblock">
Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou.

</span>
<span class="ltx_bibblock">Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.07919</em>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duquenne et al. (2023)</span>
<span class="ltx_bibblock">
Paul-Ambroise Duquenne, Kevin Heffernan, Alexandre Mourachko, Benoît Sagot, and Holger Schwenk.

</span>
<span class="ltx_bibblock">Sonar expressive: Zero-shot expressive speech-to-speech translation.

</span>
<span class="ltx_bibblock">2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2021)</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.09685</em>, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jokinen &amp; McTear (2022)</span>
<span class="ltx_bibblock">
Kristina Jokinen and Michael McTear.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Spoken dialogue systems</em>.

</span>
<span class="ltx_bibblock">Springer Nature, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ju et al. (2024)</span>
<span class="ltx_bibblock">
Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, et al.

</span>
<span class="ltx_bibblock">Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.03100</em>, 2024.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2024)</span>
<span class="ltx_bibblock">
Heeseung Kim, Soonshin Seo, Kyeongseok Jeong, Ohsung Kwon, Jungwhan Kim, Jaehong Lee, Eunwoo Song, Myungwoo Oh, Sungroh Yoon, and Kang Min Yoo.

</span>
<span class="ltx_bibblock">Unified speech-text pretraining for spoken dialog modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.05706</em>, 2024.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lakhotia et al. (2021)</span>
<span class="ltx_bibblock">
Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al.

</span>
<span class="ltx_bibblock">On generative spoken language modeling from raw audio.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 9:1336–1354, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2023)</span>
<span class="ltx_bibblock">
Keon Lee, Kyumin Park, and Daeyoung Kim.

</span>
<span class="ltx_bibblock">Dailytalk: Spoken dialogue dataset for conversational text-to-speech.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp.  1–5. IEEE, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Yinghao Aaron Li, Ali Zare, and Nima Mesgarani.

</span>
<span class="ltx_bibblock">Starganv2-vc: A diverse, unsupervised, non-parallel framework for natural-sounding voice conversion.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.10394</em>, 2021.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022)</span>
<span class="ltx_bibblock">
Yinghao Aaron Li, Cong Han, and Nima Mesgarani.

</span>
<span class="ltx_bibblock">Styletts: A style-based generative model for natural and diverse text-to-speech synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.15439</em>, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
Yinghao Aaron Li, Cong Han, Vinay Raghavan, Gavin Mischler, and Nima Mesgarani.

</span>
<span class="ltx_bibblock">Styletts 2: Towards human-level text-to-speech through style diffusion and adversarial training with large speech language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin.

</span>
<span class="ltx_bibblock">Rouge: A package for automatic evaluation of summaries.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Text summarization branches out</em>, pp.  74–81, 2004.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2023)</span>
<span class="ltx_bibblock">
Guan-Ting Lin, Prashanth Gurunath Shivakumar, Ankur Gandhe, Chao-Han Huck Yang, Yile Gu, Shalini Ghosh, Andreas Stolcke, Hung-yi Lee, and Ivan Bulyko.

</span>
<span class="ltx_bibblock">Paralinguistics-enhanced large language modeling of spoken dialogue.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.15316</em>, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2024)</span>
<span class="ltx_bibblock">
Guan-Ting Lin, Cheng-Han Chiang, and Hung-yi Lee.

</span>
<span class="ltx_bibblock">Advancing large language models to capture varied speaking styles and respond properly in spoken conversations.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.12786</em>, 2024.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
Ruibo Liu, Jason Wei, Chenyan Jia, and Soroush Vosoughi.

</span>
<span class="ltx_bibblock">Modulating language models with emotions.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.07886</em>, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitsui et al. (2023)</span>
<span class="ltx_bibblock">
Kentaro Mitsui, Yukiya Hono, and Kei Sawada.

</span>
<span class="ltx_bibblock">Towards human-like spoken dialogue generation between ai agents from written dialogue.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.01088</em>, 2023.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nachmani et al. (2023)</span>
<span class="ltx_bibblock">
Eliya Nachmani, Alon Levkovitch, Julian Salazar, Chulayutsh Asawaroengchai, Soroosh Mariooryad, RJ Skerry-Ryan, and Michelle Tadmor Ramanovich.

</span>
<span class="ltx_bibblock">Lms with a voice: Spoken language modeling beyond speech tokens.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.15255</em>, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2023)</span>
<span class="ltx_bibblock">
Tu Anh Nguyen, Eugene Kharitonov, Jade Copet, Yossi Adi, Wei-Ning Hsu, Ali Elkahky, Paden Tomasello, Robin Algayres, Benoit Sagot, Abdelrahman Mohamed, et al.

</span>
<span class="ltx_bibblock">Generative spoken dialogue language modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 11:250–266, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</em>, pp.  311–318, 2002.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2023)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Robust speech recognition via large-scale weak supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pp.  28492–28518. PMLR, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Varshney et al. (2021)</span>
<span class="ltx_bibblock">
Deeksha Varshney, Asif Ekbal, and Pushpak Bhattacharyya.

</span>
<span class="ltx_bibblock">Modelling context emotions using multi-task learning for emotion controlled dialog generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</em>, pp.  2919–2931, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al. (2023)</span>
<span class="ltx_bibblock">
Hongfei Xue, Yuhao Liang, Bingshen Mu, Shiliang Zhang, Qian Chen, and Lei Xie.

</span>
<span class="ltx_bibblock">E-chat: Emotion-sensitive spoken dialogue system with large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.00475</em>, 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yi et al. (2024)</span>
<span class="ltx_bibblock">
Zihao Yi, Jiarui Ouyang, Yuwen Liu, Tianhao Liao, Zhe Xu, and Ying Shen.

</span>
<span class="ltx_bibblock">A survey on recent advances in llm-based multi-turn dialogue systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.18013</em>, 2024.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu.

</span>
<span class="ltx_bibblock">Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.11000</em>, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Dong Zhang, Xin Zhang, Jun Zhan, Shimin Li, Yaqian Zhou, and Xipeng Qiu.

</span>
<span class="ltx_bibblock">Speechgpt-gen: Scaling chain-of-information speech generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.13527</em>, 2024.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2019)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi.

</span>
<span class="ltx_bibblock">Bertscore: Evaluating text generation with bert.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.09675</em>, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023)</span>
<span class="ltx_bibblock">
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.

</span>
<span class="ltx_bibblock">A survey of large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.18223</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/2303.18223" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/2303.18223</a>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al. (2022)</span>
<span class="ltx_bibblock">
Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han.

</span>
<span class="ltx_bibblock">Towards a unified multi-dimensional evaluator for text generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.07197</em>, 2022.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2022)</span>
<span class="ltx_bibblock">
Ge Zhu, Juan-Pablo Caceres, and Justin Salamon.

</span>
<span class="ltx_bibblock">Filler Word Detection and Classification: A Dataset and Benchmark.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2022</em>, pp.  3769–3773, 2022.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.21437/Interspeech.2022-10992</span>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Implementation Details</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Prompt Settings</h3>

<div id="A1.SS1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS1.p1.1" class="ltx_p">We adopted the input prompting format from Qwen-Audio and added extra tokens to accommodate speaker styles. The example below is the prompt template for two speakers with an ongoing conversation of three rounds. The last round is the incoming speech (the audio input to Qwen-Audio), in which the transcription and the style have not yet been calculated and appended to the context. Texts in <code id="A1.SS1.p1.1.1" class="ltx_verbatim ltx_font_typewriter">&lt; &gt;</code> are special tokens for Qwen-Audio’s tokenizer, and texts in <code id="A1.SS1.p1.1.2" class="ltx_verbatim ltx_font_typewriter">[ ]</code> vary for each conversation.</p>
</div>
<div id="A1.SS1.p2" class="ltx_para ltx_noindent">
<span id="A1.SS1.p2.1" class="ltx_inline-block ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><pre id="A1.SS1.p2.1.1" class="ltx_verbatim ltx_font_typewriter">

Audio 1:&lt;audio&gt;[audio path]&lt;/audio&gt;

This is the voice of the [spk a] last speaking. There is a conversation
among [spk a]: STYLE: &lt;|extra_123|&gt; [spk b]: STYLE: &lt;|extra_123|&gt;.
Here is some context:

[spk a]: STYLE: &lt;|extra_123|&gt; TEXT: [text]
[spk b]: STYLE: &lt;|extra_123|&gt; TEXT: [text]

Try to recognize what [spk a] just said from the audio,
and generate the style and text of the next speaker [spk b].
Be creative and avoid repeated words and sentences.
STYLE: &lt;|extra_124|&gt; TEXT:
</pre>
</span>
</div>
<div id="A1.SS1.p3" class="ltx_para ltx_noindent">
<p id="A1.SS1.p3.1" class="ltx_p">We chose Qwen-Audio’s pre-allocated special tokens
<span id="A1.SS1.p3.1.1" class="ltx_text ltx_font_typewriter">&lt;|extra_123|&gt;</span> as placeholders for input styles and <span id="A1.SS1.p3.1.2" class="ltx_text ltx_font_typewriter">&lt;|extra_124|&gt;</span> as the location identifier for the output styles. <span id="A1.SS1.p3.1.3" class="ltx_text ltx_font_typewriter">&lt;|extra_123|&gt;</span> tokens are not embedded by the text embedder. Projected input styles replace the embedding at these locations. The response style is predicted from the last layer representation of the <span id="A1.SS1.p3.1.4" class="ltx_text ltx_font_typewriter">&lt;|extra_124|&gt;</span> token.</p>
</div>
<div id="A1.SS1.p4" class="ltx_para ltx_noindent">
<p id="A1.SS1.p4.1" class="ltx_p">We have also crafted similar prompt templates for the baseline and ablation experiments. For example, we removed all styles in the context or removed the audio input for the <span id="A1.SS1.p4.1.1" class="ltx_text ltx_font_italic">– style in context</span> and <span id="A1.SS1.p4.1.2" class="ltx_text ltx_font_italic">– audio input</span> models in Table <a href="#S5.T5" title="Table 5 ‣ 5.2 Ablation Study ‣ 5 Results ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, respectively.</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Training and Inference Details</h3>

<div id="A1.SS2.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.p1.1" class="ltx_p">For both the baselines and our system, we randomly cropped the dialog, used every turn before the cropping point as the context, and trained the model to predict the response of the next turn. We truncated the context to fit the context window for both the baseline and our systems, 512 for SpeechGPT and 1536 for Qwen-7B and Qwen-Audio. 1536 is the maximum context window we can hold in the GPU memory. We used fixed dialog crops that were randomly sampled beforehand for evaluation and randomly sampled new crops for training.</p>
</div>
<div id="A1.SS2.p2" class="ltx_para ltx_noindent">
<p id="A1.SS2.p2.1" class="ltx_p">We fine-tuned Qwen-Audio in Style-Talker and Qwen in the baseline using LoRA <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>. We added LoRA to query, key, and value metrics in self-attention layers and weight metrics in MLPs in both the LLM and the audio encoder. We chose a rank of 16, a dropout ratio of 0.05, and no bias term. For fair comparisons, we trained Style-Talker, baseline, and ablation models with the same training hyperparameters. All models were trained with an AdamW optimizer, a peak learning rate of <math id="A1.SS2.p2.1.m1.1" class="ltx_Math" alttext="1e-4" display="inline"><semantics id="A1.SS2.p2.1.m1.1a"><mrow id="A1.SS2.p2.1.m1.1.1" xref="A1.SS2.p2.1.m1.1.1.cmml"><mrow id="A1.SS2.p2.1.m1.1.1.2" xref="A1.SS2.p2.1.m1.1.1.2.cmml"><mn id="A1.SS2.p2.1.m1.1.1.2.2" xref="A1.SS2.p2.1.m1.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="A1.SS2.p2.1.m1.1.1.2.1" xref="A1.SS2.p2.1.m1.1.1.2.1.cmml">​</mo><mi id="A1.SS2.p2.1.m1.1.1.2.3" xref="A1.SS2.p2.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="A1.SS2.p2.1.m1.1.1.1" xref="A1.SS2.p2.1.m1.1.1.1.cmml">−</mo><mn id="A1.SS2.p2.1.m1.1.1.3" xref="A1.SS2.p2.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p2.1.m1.1b"><apply id="A1.SS2.p2.1.m1.1.1.cmml" xref="A1.SS2.p2.1.m1.1.1"><minus id="A1.SS2.p2.1.m1.1.1.1.cmml" xref="A1.SS2.p2.1.m1.1.1.1"></minus><apply id="A1.SS2.p2.1.m1.1.1.2.cmml" xref="A1.SS2.p2.1.m1.1.1.2"><times id="A1.SS2.p2.1.m1.1.1.2.1.cmml" xref="A1.SS2.p2.1.m1.1.1.2.1"></times><cn type="integer" id="A1.SS2.p2.1.m1.1.1.2.2.cmml" xref="A1.SS2.p2.1.m1.1.1.2.2">1</cn><ci id="A1.SS2.p2.1.m1.1.1.2.3.cmml" xref="A1.SS2.p2.1.m1.1.1.2.3">𝑒</ci></apply><cn type="integer" id="A1.SS2.p2.1.m1.1.1.3.cmml" xref="A1.SS2.p2.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p2.1.m1.1c">1e-4</annotation></semantics></math>, a linear learning rate decay, a maximum gradient norm of 1, a batch size of 1, and a gradient accumulation step of 8 (for an equivalent batch size of 8). We used mixed (bfloat16) precision for training on a Nvidia L40 GPU. We trained all models for 20 epochs on the DailyTalk dataset or 100 epochs on the PodcastFilters dataset.</p>
</div>
<div id="A1.SS2.p3" class="ltx_para ltx_noindent">
<p id="A1.SS2.p3.3" class="ltx_p">In evaluation, we set the style diffusion parameter <math id="A1.SS2.p3.1.m1.2" class="ltx_Math" alttext="\alpha=0.3,\beta=0" display="inline"><semantics id="A1.SS2.p3.1.m1.2a"><mrow id="A1.SS2.p3.1.m1.2.2.2" xref="A1.SS2.p3.1.m1.2.2.3.cmml"><mrow id="A1.SS2.p3.1.m1.1.1.1.1" xref="A1.SS2.p3.1.m1.1.1.1.1.cmml"><mi id="A1.SS2.p3.1.m1.1.1.1.1.2" xref="A1.SS2.p3.1.m1.1.1.1.1.2.cmml">α</mi><mo id="A1.SS2.p3.1.m1.1.1.1.1.1" xref="A1.SS2.p3.1.m1.1.1.1.1.1.cmml">=</mo><mn id="A1.SS2.p3.1.m1.1.1.1.1.3" xref="A1.SS2.p3.1.m1.1.1.1.1.3.cmml">0.3</mn></mrow><mo id="A1.SS2.p3.1.m1.2.2.2.3" xref="A1.SS2.p3.1.m1.2.2.3a.cmml">,</mo><mrow id="A1.SS2.p3.1.m1.2.2.2.2" xref="A1.SS2.p3.1.m1.2.2.2.2.cmml"><mi id="A1.SS2.p3.1.m1.2.2.2.2.2" xref="A1.SS2.p3.1.m1.2.2.2.2.2.cmml">β</mi><mo id="A1.SS2.p3.1.m1.2.2.2.2.1" xref="A1.SS2.p3.1.m1.2.2.2.2.1.cmml">=</mo><mn id="A1.SS2.p3.1.m1.2.2.2.2.3" xref="A1.SS2.p3.1.m1.2.2.2.2.3.cmml">0</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p3.1.m1.2b"><apply id="A1.SS2.p3.1.m1.2.2.3.cmml" xref="A1.SS2.p3.1.m1.2.2.2"><csymbol cd="ambiguous" id="A1.SS2.p3.1.m1.2.2.3a.cmml" xref="A1.SS2.p3.1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="A1.SS2.p3.1.m1.1.1.1.1.cmml" xref="A1.SS2.p3.1.m1.1.1.1.1"><eq id="A1.SS2.p3.1.m1.1.1.1.1.1.cmml" xref="A1.SS2.p3.1.m1.1.1.1.1.1"></eq><ci id="A1.SS2.p3.1.m1.1.1.1.1.2.cmml" xref="A1.SS2.p3.1.m1.1.1.1.1.2">𝛼</ci><cn type="float" id="A1.SS2.p3.1.m1.1.1.1.1.3.cmml" xref="A1.SS2.p3.1.m1.1.1.1.1.3">0.3</cn></apply><apply id="A1.SS2.p3.1.m1.2.2.2.2.cmml" xref="A1.SS2.p3.1.m1.2.2.2.2"><eq id="A1.SS2.p3.1.m1.2.2.2.2.1.cmml" xref="A1.SS2.p3.1.m1.2.2.2.2.1"></eq><ci id="A1.SS2.p3.1.m1.2.2.2.2.2.cmml" xref="A1.SS2.p3.1.m1.2.2.2.2.2">𝛽</ci><cn type="integer" id="A1.SS2.p3.1.m1.2.2.2.2.3.cmml" xref="A1.SS2.p3.1.m1.2.2.2.2.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p3.1.m1.2c">\alpha=0.3,\beta=0</annotation></semantics></math> for the DailyTalk dataset and <math id="A1.SS2.p3.2.m2.1" class="ltx_Math" alttext="\alpha=\beta=0" display="inline"><semantics id="A1.SS2.p3.2.m2.1a"><mrow id="A1.SS2.p3.2.m2.1.1" xref="A1.SS2.p3.2.m2.1.1.cmml"><mi id="A1.SS2.p3.2.m2.1.1.2" xref="A1.SS2.p3.2.m2.1.1.2.cmml">α</mi><mo id="A1.SS2.p3.2.m2.1.1.3" xref="A1.SS2.p3.2.m2.1.1.3.cmml">=</mo><mi id="A1.SS2.p3.2.m2.1.1.4" xref="A1.SS2.p3.2.m2.1.1.4.cmml">β</mi><mo id="A1.SS2.p3.2.m2.1.1.5" xref="A1.SS2.p3.2.m2.1.1.5.cmml">=</mo><mn id="A1.SS2.p3.2.m2.1.1.6" xref="A1.SS2.p3.2.m2.1.1.6.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p3.2.m2.1b"><apply id="A1.SS2.p3.2.m2.1.1.cmml" xref="A1.SS2.p3.2.m2.1.1"><and id="A1.SS2.p3.2.m2.1.1a.cmml" xref="A1.SS2.p3.2.m2.1.1"></and><apply id="A1.SS2.p3.2.m2.1.1b.cmml" xref="A1.SS2.p3.2.m2.1.1"><eq id="A1.SS2.p3.2.m2.1.1.3.cmml" xref="A1.SS2.p3.2.m2.1.1.3"></eq><ci id="A1.SS2.p3.2.m2.1.1.2.cmml" xref="A1.SS2.p3.2.m2.1.1.2">𝛼</ci><ci id="A1.SS2.p3.2.m2.1.1.4.cmml" xref="A1.SS2.p3.2.m2.1.1.4">𝛽</ci></apply><apply id="A1.SS2.p3.2.m2.1.1c.cmml" xref="A1.SS2.p3.2.m2.1.1"><eq id="A1.SS2.p3.2.m2.1.1.5.cmml" xref="A1.SS2.p3.2.m2.1.1.5"></eq><share href="#A1.SS2.p3.2.m2.1.1.4.cmml" id="A1.SS2.p3.2.m2.1.1d.cmml" xref="A1.SS2.p3.2.m2.1.1"></share><cn type="integer" id="A1.SS2.p3.2.m2.1.1.6.cmml" xref="A1.SS2.p3.2.m2.1.1.6">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p3.2.m2.1c">\alpha=\beta=0</annotation></semantics></math> for the PodcastFilters dataset for all models. We fixed the reference style from the context for a consistent evaluation and sampled with a temperature of 1 and <math id="A1.SS2.p3.3.m3.1" class="ltx_Math" alttext="\texttt{top}\_\texttt{k}" display="inline"><semantics id="A1.SS2.p3.3.m3.1a"><mrow id="A1.SS2.p3.3.m3.1.1" xref="A1.SS2.p3.3.m3.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="A1.SS2.p3.3.m3.1.1.2" xref="A1.SS2.p3.3.m3.1.1.2a.cmml">top</mtext><mo lspace="0em" rspace="0em" id="A1.SS2.p3.3.m3.1.1.1" xref="A1.SS2.p3.3.m3.1.1.1.cmml">​</mo><mi mathvariant="normal" id="A1.SS2.p3.3.m3.1.1.3" xref="A1.SS2.p3.3.m3.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="A1.SS2.p3.3.m3.1.1.1a" xref="A1.SS2.p3.3.m3.1.1.1.cmml">​</mo><mtext class="ltx_mathvariant_monospace" id="A1.SS2.p3.3.m3.1.1.4" xref="A1.SS2.p3.3.m3.1.1.4a.cmml">k</mtext></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p3.3.m3.1b"><apply id="A1.SS2.p3.3.m3.1.1.cmml" xref="A1.SS2.p3.3.m3.1.1"><times id="A1.SS2.p3.3.m3.1.1.1.cmml" xref="A1.SS2.p3.3.m3.1.1.1"></times><ci id="A1.SS2.p3.3.m3.1.1.2a.cmml" xref="A1.SS2.p3.3.m3.1.1.2"><mtext class="ltx_mathvariant_monospace" id="A1.SS2.p3.3.m3.1.1.2.cmml" xref="A1.SS2.p3.3.m3.1.1.2">top</mtext></ci><ci id="A1.SS2.p3.3.m3.1.1.3.cmml" xref="A1.SS2.p3.3.m3.1.1.3">_</ci><ci id="A1.SS2.p3.3.m3.1.1.4a.cmml" xref="A1.SS2.p3.3.m3.1.1.4"><mtext class="ltx_mathvariant_monospace" id="A1.SS2.p3.3.m3.1.1.4.cmml" xref="A1.SS2.p3.3.m3.1.1.4">k</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p3.3.m3.1c">\texttt{top}\_\texttt{k}</annotation></semantics></math> of 0.8.</p>
</div>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Baseline Systems</h3>

<div id="A1.SS3.p1" class="ltx_para ltx_noindent">
<p id="A1.SS3.p1.1" class="ltx_p">For the cascade system, we employed Whisper <span id="A1.SS3.p1.1.1" class="ltx_text ltx_font_italic">large-v2</span> as the ASR model since the audio encoder in our system is fine-tuned from the encoder of Whisper <span id="A1.SS3.p1.1.2" class="ltx_text ltx_font_italic">large-v2</span>. We fine-tuned a Qwen-7B model as the LLM for the cascade baseline, as our audio LLM was fine-tuned from Qwen-7B. We kept the same fine-tuned StyleTTS 2 model in both the cascade baseline and our proposed system.</p>
</div>
<div id="A1.SS3.p2" class="ltx_para ltx_noindent">
<p id="A1.SS3.p2.1" class="ltx_p">For both the cascade baselines and our system, we used the previous 3 turns as the context for the input. For SpeechGPT, we only used the previous turn as its context window is limited to only one turn of input audio. Since SpeechGPT generates speech at 16 kHz while StyleTTS 2 produces speech at 24 kHz, we downsampled all audio to 16 kHz for DailyTalk dataset evaluations for fair comparison.</p>
</div>
</section>
<section id="A1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>PodcastFiller Dataset</h3>

<div id="A1.SS4.p1" class="ltx_para ltx_noindent">
<p id="A1.SS4.p1.1" class="ltx_p">Since we need “verbatim” transcriptions, we re-transcribed the podcast audio using Whisper <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite> and use Pyannote <cite class="ltx_cite ltx_citemacro_citep">(Bredin et al., <a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite> to perform speaker diarization.
Specifically, we first apply speaker diarization to approximately parse the long recordings into segments according to different speaker identities.
Following this, we use a Whisper <span id="A1.SS4.p1.1.1" class="ltx_text ltx_font_italic">large-v2</span> model to obtain transcriptions for these segments.
In our experience, the pre-trained Whisper models tend to omit speaker turns, filler words, and other disfluencies; to address this, one can either prompt the model with diarized verbatim text <span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_prompting_guide.ipynb" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/openai/openai-cookbook/blob/main/examples/Whisper_prompting_guide.ipynb</a></span></span></span> or fine-tune the model using verbatim data.
To address this, we use a Whisper <span id="A1.SS4.p1.1.2" class="ltx_text ltx_font_italic">large-v2</span> model that has been fine-tuned on a small dataset comprised of verbatim captions containing speaker turns, e.g., <span id="A1.SS4.p1.1.3" class="ltx_text ltx_font_italic">“…[S1] Um, How are you today? [S2] Oh, I’m fine. Than-Thank you!"</span>.
Lastly, after obtaining transcriptions, we filter out utterances that were not properly diarized by discarding segments whose transcripts contain multiple speaker indicators, e.g. containing both “[S1]” and “[S2]”.</p>
</div>
<figure id="A1.T6" class="ltx_table">
<table id="A1.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A1.T6.1.1" class="ltx_tr">
<td id="A1.T6.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Dataset</td>
<td id="A1.T6.1.1.2" class="ltx_td ltx_align_left ltx_border_tt">Model</td>
<td id="A1.T6.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">BLEU</td>
<td id="A1.T6.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">ROUGE-L</td>
<td id="A1.T6.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">METEOR</td>
<td id="A1.T6.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">BERT Score</td>
</tr>
<tr id="A1.T6.1.2" class="ltx_tr">
<td id="A1.T6.1.2.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="4"><span id="A1.T6.1.2.1.1" class="ltx_text">DailyTalk</span></td>
<td id="A1.T6.1.2.2" class="ltx_td ltx_align_left ltx_border_t">Proposed</td>
<td id="A1.T6.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.1.2.3.1" class="ltx_text ltx_font_bold">4.62</span></td>
<td id="A1.T6.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.1.2.4.1" class="ltx_text ltx_font_bold">16.14</span></td>
<td id="A1.T6.1.2.5" class="ltx_td ltx_align_center ltx_border_t">25.72</td>
<td id="A1.T6.1.2.6" class="ltx_td ltx_align_center ltx_border_t">90.40</td>
</tr>
<tr id="A1.T6.1.3" class="ltx_tr">
<td id="A1.T6.1.3.1" class="ltx_td ltx_align_left"><span id="A1.T6.1.3.1.1" class="ltx_text ltx_font_italic">– style in context</span></td>
<td id="A1.T6.1.3.2" class="ltx_td ltx_align_center">3.79</td>
<td id="A1.T6.1.3.3" class="ltx_td ltx_align_center">15.37</td>
<td id="A1.T6.1.3.4" class="ltx_td ltx_align_center">23.94</td>
<td id="A1.T6.1.3.5" class="ltx_td ltx_align_center">89.57</td>
</tr>
<tr id="A1.T6.1.4" class="ltx_tr">
<td id="A1.T6.1.4.1" class="ltx_td ltx_align_left"><span id="A1.T6.1.4.1.1" class="ltx_text ltx_font_italic">– audio input</span></td>
<td id="A1.T6.1.4.2" class="ltx_td ltx_align_center">4.43</td>
<td id="A1.T6.1.4.3" class="ltx_td ltx_align_center">15.99</td>
<td id="A1.T6.1.4.4" class="ltx_td ltx_align_center"><span id="A1.T6.1.4.4.1" class="ltx_text ltx_font_bold">25.79</span></td>
<td id="A1.T6.1.4.5" class="ltx_td ltx_align_center">89.53</td>
</tr>
<tr id="A1.T6.1.5" class="ltx_tr">
<td id="A1.T6.1.5.1" class="ltx_td ltx_align_left"><span id="A1.T6.1.5.1.1" class="ltx_text ltx_font_italic">+ ASR &amp; style</span></td>
<td id="A1.T6.1.5.2" class="ltx_td ltx_align_center">4.61</td>
<td id="A1.T6.1.5.3" class="ltx_td ltx_align_center">15.29</td>
<td id="A1.T6.1.5.4" class="ltx_td ltx_align_center"><span id="A1.T6.1.5.4.1" class="ltx_text ltx_font_bold">26.13</span></td>
<td id="A1.T6.1.5.5" class="ltx_td ltx_align_center">89.27</td>
</tr>
<tr id="A1.T6.1.6" class="ltx_tr">
<td id="A1.T6.1.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="4"><span id="A1.T6.1.6.1.1" class="ltx_text">PodcastFillers</span></td>
<td id="A1.T6.1.6.2" class="ltx_td ltx_align_left ltx_border_t">Style-Talker</td>
<td id="A1.T6.1.6.3" class="ltx_td ltx_align_center ltx_border_t">3.31</td>
<td id="A1.T6.1.6.4" class="ltx_td ltx_align_center ltx_border_t">10.06</td>
<td id="A1.T6.1.6.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.1.6.5.1" class="ltx_text ltx_font_bold">17.00</span></td>
<td id="A1.T6.1.6.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T6.1.6.6.1" class="ltx_text ltx_font_bold">84.26</span></td>
</tr>
<tr id="A1.T6.1.7" class="ltx_tr">
<td id="A1.T6.1.7.1" class="ltx_td ltx_align_left"><span id="A1.T6.1.7.1.1" class="ltx_text ltx_font_italic">– style in context</span></td>
<td id="A1.T6.1.7.2" class="ltx_td ltx_align_center">3.02</td>
<td id="A1.T6.1.7.3" class="ltx_td ltx_align_center">9.44</td>
<td id="A1.T6.1.7.4" class="ltx_td ltx_align_center">15.49</td>
<td id="A1.T6.1.7.5" class="ltx_td ltx_align_center">84.19</td>
</tr>
<tr id="A1.T6.1.8" class="ltx_tr">
<td id="A1.T6.1.8.1" class="ltx_td ltx_align_left"><span id="A1.T6.1.8.1.1" class="ltx_text ltx_font_italic">– audio input</span></td>
<td id="A1.T6.1.8.2" class="ltx_td ltx_align_center">3.05</td>
<td id="A1.T6.1.8.3" class="ltx_td ltx_align_center">9.55</td>
<td id="A1.T6.1.8.4" class="ltx_td ltx_align_center">15.96</td>
<td id="A1.T6.1.8.5" class="ltx_td ltx_align_center">84.13</td>
</tr>
<tr id="A1.T6.1.9" class="ltx_tr">
<td id="A1.T6.1.9.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="A1.T6.1.9.1.1" class="ltx_text ltx_font_italic">+ ASR &amp; style</span></td>
<td id="A1.T6.1.9.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="A1.T6.1.9.2.1" class="ltx_text ltx_font_bold">3.65</span></td>
<td id="A1.T6.1.9.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="A1.T6.1.9.3.1" class="ltx_text ltx_font_bold">11.08</span></td>
<td id="A1.T6.1.9.4" class="ltx_td ltx_align_center ltx_border_bb">16.91</td>
<td id="A1.T6.1.9.5" class="ltx_td ltx_align_center ltx_border_bb">84.11</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Semantic evaluations of F-1 score (%) for BLEU, ROUGE-L, METEOR, and BERT Score for ablation study. The results for the proposed model are copied from Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Evaluations ‣ 4 Experiments ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</figcaption>
</figure>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Detailed Ablation Study Results</h2>

<div id="A2.p1" class="ltx_para ltx_noindent">
<p id="A2.p1.1" class="ltx_p">In our ablation study, we conducted objective evaluations at both the acoustic and semantic levels to scrutinize the impact of various components within our proposed Style-Talker system. We focused on understanding how the inclusion or omission of specific features influences the system’s overall performance, particularly in terms of acoustic quality and semantic accuracy. Notably, we opted to exclude the word error rate (WER) from our semantic evaluation metrics. This decision was informed by the observation that WER did not exhibit significant variability across the different configurations tested in our ablation study. Furthermore, its inclusion was found to introduce noise into the aggregated scores presented in Table <a href="#S5.T5" title="Table 5 ‣ 5.2 Ablation Study ‣ 5 Results ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, potentially obfuscating the true impact of the system modifications. By focusing on metrics that more directly reflect the semantic integrity and coherence of the generated responses, we aimed to achieve a clearer and more meaningful assessment of each component’s contribution to the system’s efficacy. The full evaluation results for semantic metrics are shown in Table <a href="#A1.T6" title="Table 6 ‣ A.4 PodcastFiller Dataset ‣ Appendix A Implementation Details ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, and those of acoustic metrics are shown in Table <a href="#A2.T7" title="Table 7 ‣ Appendix B Detailed Ablation Study Results ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a></p>
</div>
<figure id="A2.T7" class="ltx_table">
<p id="A2.T7.1" class="ltx_p ltx_align_center"><span id="A2.T7.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="A2.T7.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:544.9pt;height:186.4pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="A2.T7.1.1.1.1" class="ltx_p"><span id="A2.T7.1.1.1.1.1" class="ltx_text">
<span id="A2.T7.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T7.1.1.1.1.1.1.1" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Dataset</span>
<span id="A2.T7.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_tt">Model</span>
<span id="A2.T7.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T7.1.1.1.1.1.1.1.3.1" class="ltx_text"></span> <span id="A2.T7.1.1.1.1.1.1.1.3.2" class="ltx_text">
<span id="A2.T7.1.1.1.1.1.1.1.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T7.1.1.1.1.1.1.1.3.2.1.1" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.1.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Pitch</span></span>
<span id="A2.T7.1.1.1.1.1.1.1.3.2.1.2" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.1.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">mean</span></span>
</span></span><span id="A2.T7.1.1.1.1.1.1.1.3.3" class="ltx_text"></span></span>
<span id="A2.T7.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T7.1.1.1.1.1.1.1.4.1" class="ltx_text"></span> <span id="A2.T7.1.1.1.1.1.1.1.4.2" class="ltx_text">
<span id="A2.T7.1.1.1.1.1.1.1.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T7.1.1.1.1.1.1.1.4.2.1.1" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.1.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Pitch</span></span>
<span id="A2.T7.1.1.1.1.1.1.1.4.2.1.2" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.1.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">STD</span></span>
</span></span><span id="A2.T7.1.1.1.1.1.1.1.4.3" class="ltx_text"></span></span>
<span id="A2.T7.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T7.1.1.1.1.1.1.1.5.1" class="ltx_text"></span> <span id="A2.T7.1.1.1.1.1.1.1.5.2" class="ltx_text">
<span id="A2.T7.1.1.1.1.1.1.1.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T7.1.1.1.1.1.1.1.5.2.1.1" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.1.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Energy</span></span>
<span id="A2.T7.1.1.1.1.1.1.1.5.2.1.2" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.1.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">mean</span></span>
</span></span><span id="A2.T7.1.1.1.1.1.1.1.5.3" class="ltx_text"></span></span>
<span id="A2.T7.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T7.1.1.1.1.1.1.1.6.1" class="ltx_text"></span> <span id="A2.T7.1.1.1.1.1.1.1.6.2" class="ltx_text">
<span id="A2.T7.1.1.1.1.1.1.1.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T7.1.1.1.1.1.1.1.6.2.1.1" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.1.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Energy</span></span>
<span id="A2.T7.1.1.1.1.1.1.1.6.2.1.2" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.1.6.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">STD</span></span>
</span></span><span id="A2.T7.1.1.1.1.1.1.1.6.3" class="ltx_text"></span></span>
<span id="A2.T7.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T7.1.1.1.1.1.1.1.7.1" class="ltx_text"></span> <span id="A2.T7.1.1.1.1.1.1.1.7.2" class="ltx_text">
<span id="A2.T7.1.1.1.1.1.1.1.7.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T7.1.1.1.1.1.1.1.7.2.1.1" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.1.7.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">HTN</span></span>
<span id="A2.T7.1.1.1.1.1.1.1.7.2.1.2" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.1.7.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ratio</span></span>
</span></span><span id="A2.T7.1.1.1.1.1.1.1.7.3" class="ltx_text"></span></span>
<span id="A2.T7.1.1.1.1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T7.1.1.1.1.1.1.1.8.1" class="ltx_text"></span> <span id="A2.T7.1.1.1.1.1.1.1.8.2" class="ltx_text">
<span id="A2.T7.1.1.1.1.1.1.1.8.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T7.1.1.1.1.1.1.1.8.2.1.1" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.1.8.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Speech</span></span>
<span id="A2.T7.1.1.1.1.1.1.1.8.2.1.2" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.1.8.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">duration</span></span>
</span></span><span id="A2.T7.1.1.1.1.1.1.1.8.3" class="ltx_text"></span></span>
<span id="A2.T7.1.1.1.1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T7.1.1.1.1.1.1.1.9.1" class="ltx_text"></span> <span id="A2.T7.1.1.1.1.1.1.1.9.2" class="ltx_text">
<span id="A2.T7.1.1.1.1.1.1.1.9.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T7.1.1.1.1.1.1.1.9.2.1.1" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.1.9.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Speaker</span></span>
<span id="A2.T7.1.1.1.1.1.1.1.9.2.1.2" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.1.9.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">similarity</span></span>
</span></span><span id="A2.T7.1.1.1.1.1.1.1.9.3" class="ltx_text"></span></span></span>
<span id="A2.T7.1.1.1.1.1.1.2" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.2.1" class="ltx_td ltx_align_left ltx_border_t ltx_rowspan ltx_rowspan_4"><span id="A2.T7.1.1.1.1.1.1.2.1.1" class="ltx_text">DailyTalk</span></span>
<span id="A2.T7.1.1.1.1.1.1.2.2" class="ltx_td ltx_align_left ltx_border_t">Proposed</span>
<span id="A2.T7.1.1.1.1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">0.81</span>
<span id="A2.T7.1.1.1.1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">0.26</span>
<span id="A2.T7.1.1.1.1.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T7.1.1.1.1.1.1.2.5.1" class="ltx_text ltx_font_bold">0.73</span></span>
<span id="A2.T7.1.1.1.1.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T7.1.1.1.1.1.1.2.6.1" class="ltx_text ltx_font_bold">0.07</span></span>
<span id="A2.T7.1.1.1.1.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t">0.56</span>
<span id="A2.T7.1.1.1.1.1.1.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T7.1.1.1.1.1.1.2.8.1" class="ltx_text ltx_font_bold">0.25</span></span>
<span id="A2.T7.1.1.1.1.1.1.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T7.1.1.1.1.1.1.2.9.1" class="ltx_text ltx_font_bold">0.86</span></span></span>
<span id="A2.T7.1.1.1.1.1.1.3" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.3.1" class="ltx_td ltx_align_left"><span id="A2.T7.1.1.1.1.1.1.3.1.1" class="ltx_text ltx_font_italic">– style in context</span></span>
<span id="A2.T7.1.1.1.1.1.1.3.2" class="ltx_td ltx_align_center">0.82</span>
<span id="A2.T7.1.1.1.1.1.1.3.3" class="ltx_td ltx_align_center">0.27</span>
<span id="A2.T7.1.1.1.1.1.1.3.4" class="ltx_td ltx_align_center">0.72</span>
<span id="A2.T7.1.1.1.1.1.1.3.5" class="ltx_td ltx_align_center">0.06</span>
<span id="A2.T7.1.1.1.1.1.1.3.6" class="ltx_td ltx_align_center">0.58</span>
<span id="A2.T7.1.1.1.1.1.1.3.7" class="ltx_td ltx_align_center">0.13</span>
<span id="A2.T7.1.1.1.1.1.1.3.8" class="ltx_td ltx_align_center">0.85</span></span>
<span id="A2.T7.1.1.1.1.1.1.4" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.4.1" class="ltx_td ltx_align_left"><span id="A2.T7.1.1.1.1.1.1.4.1.1" class="ltx_text ltx_font_italic">– audio input</span></span>
<span id="A2.T7.1.1.1.1.1.1.4.2" class="ltx_td ltx_align_center">0.81</span>
<span id="A2.T7.1.1.1.1.1.1.4.3" class="ltx_td ltx_align_center">0.28</span>
<span id="A2.T7.1.1.1.1.1.1.4.4" class="ltx_td ltx_align_center">0.70</span>
<span id="A2.T7.1.1.1.1.1.1.4.5" class="ltx_td ltx_align_center">0.05</span>
<span id="A2.T7.1.1.1.1.1.1.4.6" class="ltx_td ltx_align_center"><span id="A2.T7.1.1.1.1.1.1.4.6.1" class="ltx_text ltx_font_bold">0.60</span></span>
<span id="A2.T7.1.1.1.1.1.1.4.7" class="ltx_td ltx_align_center">0.17</span>
<span id="A2.T7.1.1.1.1.1.1.4.8" class="ltx_td ltx_align_center"><span id="A2.T7.1.1.1.1.1.1.4.8.1" class="ltx_text ltx_font_bold">0.86</span></span></span>
<span id="A2.T7.1.1.1.1.1.1.5" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.5.1" class="ltx_td ltx_align_left"><span id="A2.T7.1.1.1.1.1.1.5.1.1" class="ltx_text ltx_font_italic">+ ASR &amp; style</span></span>
<span id="A2.T7.1.1.1.1.1.1.5.2" class="ltx_td ltx_align_center"><span id="A2.T7.1.1.1.1.1.1.5.2.1" class="ltx_text ltx_font_bold">0.84</span></span>
<span id="A2.T7.1.1.1.1.1.1.5.3" class="ltx_td ltx_align_center"><span id="A2.T7.1.1.1.1.1.1.5.3.1" class="ltx_text ltx_font_bold">0.33</span></span>
<span id="A2.T7.1.1.1.1.1.1.5.4" class="ltx_td ltx_align_center">0.70</span>
<span id="A2.T7.1.1.1.1.1.1.5.5" class="ltx_td ltx_align_center">0.06</span>
<span id="A2.T7.1.1.1.1.1.1.5.6" class="ltx_td ltx_align_center">0.57</span>
<span id="A2.T7.1.1.1.1.1.1.5.7" class="ltx_td ltx_align_center">0.22</span>
<span id="A2.T7.1.1.1.1.1.1.5.8" class="ltx_td ltx_align_center">0.85</span></span>
<span id="A2.T7.1.1.1.1.1.1.6" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_4"><span id="A2.T7.1.1.1.1.1.1.6.1.1" class="ltx_text">PodcastFillers</span></span>
<span id="A2.T7.1.1.1.1.1.1.6.2" class="ltx_td ltx_align_left ltx_border_t">Proposed</span>
<span id="A2.T7.1.1.1.1.1.1.6.3" class="ltx_td ltx_align_center ltx_border_t">0.68</span>
<span id="A2.T7.1.1.1.1.1.1.6.4" class="ltx_td ltx_align_center ltx_border_t">0.28</span>
<span id="A2.T7.1.1.1.1.1.1.6.5" class="ltx_td ltx_align_center ltx_border_t">0.82</span>
<span id="A2.T7.1.1.1.1.1.1.6.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T7.1.1.1.1.1.1.6.6.1" class="ltx_text ltx_font_bold">0.58</span></span>
<span id="A2.T7.1.1.1.1.1.1.6.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T7.1.1.1.1.1.1.6.7.1" class="ltx_text ltx_font_bold">0.51</span></span>
<span id="A2.T7.1.1.1.1.1.1.6.8" class="ltx_td ltx_align_center ltx_border_t">0.20</span>
<span id="A2.T7.1.1.1.1.1.1.6.9" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T7.1.1.1.1.1.1.6.9.1" class="ltx_text ltx_font_bold">0.84</span></span></span>
<span id="A2.T7.1.1.1.1.1.1.7" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.7.1" class="ltx_td ltx_align_left"><span id="A2.T7.1.1.1.1.1.1.7.1.1" class="ltx_text ltx_font_italic">– style in context</span></span>
<span id="A2.T7.1.1.1.1.1.1.7.2" class="ltx_td ltx_align_center">0.66</span>
<span id="A2.T7.1.1.1.1.1.1.7.3" class="ltx_td ltx_align_center">0.26</span>
<span id="A2.T7.1.1.1.1.1.1.7.4" class="ltx_td ltx_align_center">0.82</span>
<span id="A2.T7.1.1.1.1.1.1.7.5" class="ltx_td ltx_align_center">0.43</span>
<span id="A2.T7.1.1.1.1.1.1.7.6" class="ltx_td ltx_align_center">0.48</span>
<span id="A2.T7.1.1.1.1.1.1.7.7" class="ltx_td ltx_align_center">0.19</span>
<span id="A2.T7.1.1.1.1.1.1.7.8" class="ltx_td ltx_align_center">0.83</span></span>
<span id="A2.T7.1.1.1.1.1.1.8" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.8.1" class="ltx_td ltx_align_left"><span id="A2.T7.1.1.1.1.1.1.8.1.1" class="ltx_text ltx_font_italic">– audio input</span></span>
<span id="A2.T7.1.1.1.1.1.1.8.2" class="ltx_td ltx_align_center">0.70</span>
<span id="A2.T7.1.1.1.1.1.1.8.3" class="ltx_td ltx_align_center">0.27</span>
<span id="A2.T7.1.1.1.1.1.1.8.4" class="ltx_td ltx_align_center"><span id="A2.T7.1.1.1.1.1.1.8.4.1" class="ltx_text ltx_font_bold">0.83</span></span>
<span id="A2.T7.1.1.1.1.1.1.8.5" class="ltx_td ltx_align_center">0.49</span>
<span id="A2.T7.1.1.1.1.1.1.8.6" class="ltx_td ltx_align_center">0.48</span>
<span id="A2.T7.1.1.1.1.1.1.8.7" class="ltx_td ltx_align_center">0.22</span>
<span id="A2.T7.1.1.1.1.1.1.8.8" class="ltx_td ltx_align_center">0.82</span></span>
<span id="A2.T7.1.1.1.1.1.1.9" class="ltx_tr">
<span id="A2.T7.1.1.1.1.1.1.9.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="A2.T7.1.1.1.1.1.1.9.1.1" class="ltx_text ltx_font_italic">+ ASR &amp; style</span></span>
<span id="A2.T7.1.1.1.1.1.1.9.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T7.1.1.1.1.1.1.9.2.1" class="ltx_text ltx_font_bold">0.75</span></span>
<span id="A2.T7.1.1.1.1.1.1.9.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T7.1.1.1.1.1.1.9.3.1" class="ltx_text ltx_font_bold">0.28</span></span>
<span id="A2.T7.1.1.1.1.1.1.9.4" class="ltx_td ltx_align_center ltx_border_bb">0.82</span>
<span id="A2.T7.1.1.1.1.1.1.9.5" class="ltx_td ltx_align_center ltx_border_bb">0.48</span>
<span id="A2.T7.1.1.1.1.1.1.9.6" class="ltx_td ltx_align_center ltx_border_bb">0.49</span>
<span id="A2.T7.1.1.1.1.1.1.9.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T7.1.1.1.1.1.1.9.7.1" class="ltx_text ltx_font_bold">0.28</span></span>
<span id="A2.T7.1.1.1.1.1.1.9.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T7.1.1.1.1.1.1.9.8.1" class="ltx_text ltx_font_bold">0.84</span></span></span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Acoustic evaluation results for the proposed and ablated models for ablation study. The results for the proposed model are copied from Table <a href="#S5.T3" title="Table 3 ‣ 5 Results ‣ Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. </figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Subjective Evaluation Details</h2>

<div id="A3.p1" class="ltx_para ltx_noindent">
<p id="A3.p1.1" class="ltx_p">To ensure high-quality evaluation from MTurk, we followed <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib18" title="" class="ltx_ref">2024</a>)</cite> by enabling the following filters:</p>
<ul id="A3.I1" class="ltx_itemize">
<li id="A3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i1.p1" class="ltx_para">
<p id="A3.I1.i1.p1.1" class="ltx_p">HIT Approval Rate (%) for all Requesters’ HITS: <code id="A3.I1.i1.p1.1.1" class="ltx_verbatim ltx_font_typewriter">greater than 95</code>.</p>
</div>
</li>
<li id="A3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i2.p1" class="ltx_para">
<p id="A3.I1.i2.p1.1" class="ltx_p">Location: <code id="A3.I1.i2.p1.1.1" class="ltx_verbatim ltx_font_typewriter">is UNITED STATES (US)</code>.</p>
</div>
</li>
<li id="A3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="A3.I1.i3.p1.1" class="ltx_p">Number of HITs Approved: <code id="A3.I1.i3.p1.1.1" class="ltx_verbatim ltx_font_typewriter">greater than 50</code>.</p>
</div>
</li>
</ul>
</div>
<div id="A3.p2" class="ltx_para ltx_noindent">
<p id="A3.p2.1" class="ltx_p">We provided the following instructions for rating the naturalness and coherence:</p>
</div>
<div id="A3.p3" class="ltx_para ltx_noindent">
<ul id="A3.I2" class="ltx_itemize">
<li id="A3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I2.i1.p1" class="ltx_para ltx_noindent">
<p id="A3.I2.i1.p1.1" class="ltx_p">Naturalness:</p>
<pre id="A3.I2.i1.p1.2" class="ltx_verbatim ltx_font_typewriter">
    Rate how natural it is like being produced by human, compared to
    a computer or robot, evaluate based on the content spoken and
    speech quality.
</pre>
</div>
</li>
<li id="A3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A3.I2.i2.p1" class="ltx_para ltx_noindent">
<p id="A3.I2.i2.p1.1" class="ltx_p">Coherence:</p>
<pre id="A3.I2.i2.p1.2" class="ltx_verbatim ltx_font_typewriter">
    Rate how coherent the response is to the previous conversation,
    e.g., whether the person speaking is in the same emotion or the
    same person, or the response is relevant to previous
    conversation.
</pre>
</div>
</li>
</ul>
</div>
<div id="A3.p4" class="ltx_para ltx_noindent">
<p id="A3.p4.1" class="ltx_p">Additionally, for the evaluation of the DailyTalk dataset, we specifically informed the raters that the speakers are not native speakers and should be taken into consideration while rating naturalness.</p>
</div>
<div id="A3.p5" class="ltx_para ltx_noindent">
<p id="A3.p5.1" class="ltx_p">An example survey used for our subjective evaluation can be found at <a target="_blank" href="https://survey.alchemer.com/s3/7782786/this-colm2024-b2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://survey.alchemer.com/s3/7782786/this-colm2024-b2</a>. Each rating was paid $5 for completing this 15-minute survey.</p>
<div class="ltx_pagination ltx_role_newpage"></div>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.11848" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.11849" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.11849">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.11849" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.11850" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 13:31:13 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
