<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.15151] Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing</title><meta property="og:description" content="In visual speech processing, context modeling capability is one of the most important requirements due to the ambiguous nature of lip movements. For example, homophenes, words that share identical lip movements but pro…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.15151">

<!--Generated on Tue Mar  5 12:54:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Where Visual Speech Meets Language: VSP-LLM Framework 
<br class="ltx_break">for Efficient and Context-Aware Visual Speech Processing</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jeong Hun Yeo<sup id="4.4.1" class="ltx_sup">∗</sup>, Seunghee Han<sup id="5.5.2" class="ltx_sup">∗</sup>, Minsu Kim, Yong Man Ro<sup id="6.6.3" class="ltx_sup">†</sup>

<br class="ltx_break">Integrated Vision and Language Lab, KAIST, South Korea
<br class="ltx_break"><span id="7.7.4" class="ltx_text ltx_font_typewriter">{sedne246,gkstmdgml211,ms.k,ymro}@kaist.ac.kr</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="8.1" class="ltx_p">In visual speech processing, context modeling capability is one of the most important requirements due to the ambiguous nature of lip movements. For example, homophenes, words that share identical lip movements but produce different sounds, can be distinguished by considering the context. In this paper, we propose a novel framework, namely Visual Speech Processing incorporated with LLMs (VSP-LLM), to maximize the context modeling ability by bringing the overwhelming power of LLMs. Specifically, VSP-LLM is designed to perform multi-tasks of visual speech recognition and translation, where the given instructions control the type of task. The input video is mapped to the input latent space of a LLM by employing a self-supervised visual speech model. Focused on the fact that there is redundant information in input frames, we propose a novel deduplication method that reduces the embedded visual features by employing visual speech units. Through the proposed deduplication and Low Rank Adaptors (LoRA), VSP-LLM can be trained in a computationally efficient manner. In the translation dataset, the MuAViC benchmark, we demonstrate that VSP-LLM can more effectively recognize and translate lip movements with just 15 hours of labeled data, compared to the recent translation model <cite class="ltx_cite ltx_citemacro_cite">Anwar et al. (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> trained with 433 hours of labeld data. The code and demo are available at: <a target="_blank" href="https://github.com/Sally-SH/VSP-LLM" title="" class="ltx_ref ltx_href">https://github.com/Sally-SH/VSP-LLM</a></p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><sup id="footnotex1.1" class="ltx_sup">∗</sup>Equal Contribution. <sup id="footnotex1.2" class="ltx_sup">†</sup>Corresponding Author.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Along with audio, visual speech (<span id="S1.p1.1.1" class="ltx_text ltx_font_italic">e</span>.<span id="S1.p1.1.2" class="ltx_text ltx_font_italic">g</span>., lip movements) plays a critical role in human communication. With the increasing acknowledgment of the importance of visual speech, a diverse range of visual-based speech processing technologies <cite class="ltx_cite ltx_citemacro_cite">Assael et al. (<a href="#bib.bib4" title="" class="ltx_ref">2016</a>); Petridis and Pantic (<a href="#bib.bib37" title="" class="ltx_ref">2016</a>); Chung and Zisserman (<a href="#bib.bib10" title="" class="ltx_ref">2017a</a>); Ma et al. (<a href="#bib.bib29" title="" class="ltx_ref">2021a</a>, <a href="#bib.bib32" title="" class="ltx_ref">2022b</a>)</cite> is emerging. For instance, Visual Speech Recognition (VSR) <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib24" title="" class="ltx_ref">2021</a>); Ma et al. (<a href="#bib.bib31" title="" class="ltx_ref">2022a</a>); Yeo et al. (<a href="#bib.bib51" title="" class="ltx_ref">2023a</a>)</cite> allows for the identification of spoken words through the observation of lip movements alone, without the need for audio access. Most recently, the exploration has begun into Visual Speech Translation (VST) <cite class="ltx_cite ltx_citemacro_cite">Cheng et al. (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>, which directly generates translated text in the target language from the input lip movements of the source language.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">One key challenge in visual speech processing is to distinguish homophenes <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>. Homophenes refer to the words having different sounds but showing the same lip movements. Therefore, a crucial aspect of developing visual speech processing systems is in the modeling of context so that the same lip movements can be mapped into correct different pronunciations (that is distinguishing homophenes). Recently, Large Language Models (LLMs) <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib53" title="" class="ltx_ref">2022a</a>); Brown et al. (<a href="#bib.bib5" title="" class="ltx_ref">2020</a>); Workshop et al. (<a href="#bib.bib48" title="" class="ltx_ref">2022</a>)</cite> are attracting significant attention across various fields <cite class="ltx_cite ltx_citemacro_cite">Han et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>); Wu et al. (<a href="#bib.bib50" title="" class="ltx_ref">2023b</a>); Fathullah et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite>, thanks to their versatility and strong ability to model context. Motivated by the recent success of LLMs, we try to investigate whether the rich context modeling ability of LLMs can be employed in visual speech processing and can mitigate the ambiguity of homophenes, especially focusing on two tasks, VSR and VST.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To this end, in this paper, we propose a new framework named Visual Speech Processing incorporated with LLM (VSP-LLM) that learns the seamless embedding of visual speech into the learned text space of LLMs. VSP-LLM employs a self-supervised visual speech model to embed the input visual speech into phoneme-level representations, where the derived phonetic information can be effectively associated with text <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib54" title="" class="ltx_ref">2022b</a>)</cite>. Moreover, to reduce the computational burden in training along with LLMs, we propose a novel deduplication method that reduces the input sequence lengths of LLMs. Concretely, we employ visual speech units, the discretized representations of the features from a self-supervised model, as indicators for overlapped information between sequences. As the visual speech units can be regarded as pseudo-text <cite class="ltx_cite ltx_citemacro_cite">Lakhotia et al. (<a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>, the visual speech features assigned to the same visual speech units are averaged to reduce the processing of redundant information and improve computational efficiency. Through analysis, we show that we can reduce the sequence length by about 50% with the proposed deduplication without any performance degradation. Finally, the proposed VSP-LLM is jointly trained to perform VSR and VST with a single model which is the first explored in this paper. We show that by bringing the powerful context modeling ability into visual speech processing, we achieve state-of-the-art performances in both VSR and VST, and comparable performances with the previous methods even if we utilize just 15 hours of supervised data.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The key contributions of this paper can be summarized as follows: 1) To the best of our knowledge, this is the first work to incorporate visual speech modeling with LLMs and achieve state-of-the-art performances in VSR and VST.
2) This is the first to work to develop a unified visual speech processing model that can perform both VSR and VST with a single trained model.
3) We propose a novel visual speech deduplication that significantly improves computational efficiency.
4) We show that the proposed VSP-LLM can perform multi-tasks with superior performances even in limited training resource situations, just with 15 hours of labeled data by outperforming the recent translation model <cite class="ltx_cite ltx_citemacro_cite">Anwar et al. (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Visual Speech Processing</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Visual speech processing technologies are mainly comprised of two parts, VSR and VST. VSR is a task to recognize the language content by watching lip movements, without any sound. The VSR technologies have greatly progressed with the development of deep learning. Early works <cite class="ltx_cite ltx_citemacro_cite">Chung and Zisserman (<a href="#bib.bib11" title="" class="ltx_ref">2017b</a>); Stafylakis and Tzimiropoulos (<a href="#bib.bib45" title="" class="ltx_ref">2017</a>); Petridis et al. (<a href="#bib.bib36" title="" class="ltx_ref">2017</a>, <a href="#bib.bib38" title="" class="ltx_ref">2018</a>)</cite> utilize the CNN <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib21" title="" class="ltx_ref">2016</a>)</cite> and the RNN <cite class="ltx_cite ltx_citemacro_cite">Chung et al. (<a href="#bib.bib12" title="" class="ltx_ref">2014</a>); Hochreiter and Schmidhuber (<a href="#bib.bib22" title="" class="ltx_ref">1997</a>)</cite> to devise a word-level VSR system. To expand the VSR systems into sentence-level, <cite class="ltx_cite ltx_citemacro_cite">Chung et al. (<a href="#bib.bib9" title="" class="ltx_ref">2017</a>); Afouras et al. (<a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite> have utilized a multi-stage pipeline to automatically collect large-scale VSR data. Based on the large-scale VSR datasets, researchers <cite class="ltx_cite ltx_citemacro_cite">Serdyuk et al. (<a href="#bib.bib42" title="" class="ltx_ref">2022</a>); Ma et al. (<a href="#bib.bib30" title="" class="ltx_ref">2021b</a>)</cite> have developed the VSR systems from the perspective of architecture, especially the Transformer <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a href="#bib.bib47" title="" class="ltx_ref">2017</a>)</cite> have greatly improved the performance of VSR by enabling to capture of the context between any two positions of lip sequences. Moreover, the multimodal learning strategies <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib55" title="" class="ltx_ref">2020</a>); Afouras et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>); Ren et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>); Ma et al. (<a href="#bib.bib29" title="" class="ltx_ref">2021a</a>); Kim et al. (<a href="#bib.bib24" title="" class="ltx_ref">2021</a>, <a href="#bib.bib25" title="" class="ltx_ref">2022</a>); Yeo et al. (<a href="#bib.bib52" title="" class="ltx_ref">2023b</a>)</cite> have attempted to complement the insufficient visual speech representations by utilizing audio information. A recent self-supervised model known as AV-HuBERT <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite>, has significantly improved the visual speech representations by predicting the pseudo-label assigned from clustering audio-visual features, with a mask-prediction task like BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite>. According to the advancement of the VSR system, now we can recognize lip movements quite accurately through state-of-the-art VSR models such as AV-HuBERT. Building upon this, the exploration for VST has begun by introducing a Multilingual Audio-Visual Corpus (MuAViC) <cite class="ltx_cite ltx_citemacro_cite">Anwar et al. (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> dataset and constructing a VST <cite class="ltx_cite ltx_citemacro_cite">Cheng et al. (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Despite these research efforts, the development of visual speech processing systems enabling multi-task via a unified model, such as VSR and VST, has been never explored in the previous visual speech processing literature. Hence, the objective of this paper is to develop a unified model to perform multi-tasks, including VSR and VST, by utilizing a rich context modeling ability of LLMs.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<p id="S2.F1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S2.F1.1.1" class="ltx_text"><img src="/html/2402.15151/assets/x1.png" id="S2.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="484" height="206" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of our VSP-LLM framework. Visual speech representations encoded from the visual encoder are mapped to visual speech units. Then the visual speech representations are reduced through averaging based on the mapped visual speech units. These reduced representations are fed into the Large Language Model (LLM) along with text instructions. Throughout this process, the LLM is kept frozen, with only a small number of QLoRA parameters being fine-tuned.
</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Integration of speech models and LLMs</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">LLMs have shown remarkable success in various tasks due to their extensive linguistic knowledge and contextual understanding. While leveraging such inherent advantages of LLMs, several studies have tried to seamlessly integrate text-based knowledge with other modalities, particularly in the audio speech domain. For example, AudioPaLM <cite class="ltx_cite ltx_citemacro_cite">Rubenstein et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite> has been proposed to build a unified model interacting between text language and audio speech. To naturally bridge the gap between the two modalities, AudioPaLM has developed a multimodal vocabulary composed of discrete tokens representing both text and speech. Fathullah <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">Fathullah et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite> have employed LLaMA as a speech recognition decoder, so that the speech sequence features obtained from a conformer encoder were designed to be directly mapped into text tokens, the domain of LLaMA. Moreover, Wu <span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_italic">et al</span>. <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib49" title="" class="ltx_ref">2023a</a>)</cite> have tried to address the inherent problem of mismatched sequence lengths between speech signals and text, while taking LLaMA as a speech translation decoder. So, they have compressed the speech sequence feature and matched its sequence length with that of the text.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">However, while the existing studies have primarily focused on incorporating LLMs with the audio speech modality, the exploration of such integration for visual speech processing remains unexplored. In this paper, we propose a novel framework that integrates visual speech processing with LLMs. Specifically, we attempt to mitigate the homophenes problem, one of the key challenges in the field of visual speech processing, by leveraging the rich context modeling capabilities of LLMs. Additionally, to address the training load issues arising from the integration of the visual speech model and the Language Model (LLM), we introduce the concept of a visual speech unit. Through the implementation of visual speech units, we propose a novel visual speech deduplication method that compresses redundant representations while preserving contextual information.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Visual Speech Processing ‣ 2 Related Work ‣ Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the overall framework of the proposed Visual Speech Processing incorporated with LLM (VSP-LLM). It includes a visual encoder that embeds the input video into the input space of a pre-trained LLM, a visual speech unit based deduplication module that discards redundant information in contiguous frames, and an instruction embedding component that serves as a task specifier. In the following, we describe each component in detail.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Visual-to-Text Space Mapping</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our primary objective is to employ the rich context modeling capability of LLM in our visual speech modeling. To accomplish this, we need to represent the input video in a manner that aligns closely with linguistic information, thereby facilitating the association between visual inputs and the text space of the pre-trained LLM. Motivated by the recent success of the self-supervised speech models <cite class="ltx_cite ltx_citemacro_cite">Hsu et al. (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>); Shi et al. (<a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite> that showed the learned representations are highly correlated with phonetic information (<span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">e</span>.<span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">g</span>., phoneme) <cite class="ltx_cite ltx_citemacro_cite">Pasad et al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite>, we employ AV-HuBERT <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite> for our base visual encoder. Then, a learnable visual-to-text embedding layer is introduced to map the visual representations into the input space of LLM. We name this process as visual-to-text space mapping.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">To investigate how well the visual representation aligns with the text embedding space of the LLM, we compute the cosine similarity between the visual speech representation and the token embeddings of the LLM, mapping it to the text token with the highest similarity. Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Visual Speech Unit based Deduplication ‣ 3 Method ‣ Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>a shows an example of a textualized visual speech representation. An intriguing observation is that, with well-structured visual-text space mapping, textualized visual speech representations can exhibit pronunciation resembling real words. However, we observe redundant information when mapping entire video frames to text due to the similarity of adjacent frames. For instance, words like ’is’ and ’a’ are repeated multiple times, and the word ’social’ is mapped as a long stretch. This redundancy increases computational load when visual speech representations are fed into LLM. To address this, we propose a novel method called "Visual Speech Unit-based Deduplication" to remove redundancy while retaining semantic content.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Visual Speech Unit based Deduplication</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Compared to the length of the input video, the length of the text is much shorter. This is similar to the relationships between speech and text in Automatic Speech Recognition (ASR) <cite class="ltx_cite ltx_citemacro_cite">Graves and Graves (<a href="#bib.bib18" title="" class="ltx_ref">2012</a>)</cite>, where the input speech is almost always longer than the output text. Therefore, when we map visual speech representations into text space through visual-to-text space mapping, the resulting embedded output matches the length of the input video frames. If we directly provide it to the LLM, a large computational burden is inevitable. Here, we note that the video is smooth in temporal and the contiguous frames contain overlapped information, and propose to reduce the length of the embedded representation before feeding it to the LLM.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">To this end, we first extract the pronunciation cue from the visual representations through discretization. Recent literature <cite class="ltx_cite ltx_citemacro_cite">Lakhotia et al. (<a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite> shows that discretized self-supervised speech features, termed speech units, contain phonetic information while suppressing non-linguistic variations. Motivated by this, we propose to extract a visual version of speech units, namely visual speech units, which can be obtained by performing K-means clustering on the self-supervised visual speech representations. By doing this, we can access the pronunciation information for each video frame without requiring any text input <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>. Then, by employing the visual speech units as pseudo text, we investigate the overlapped contiguous frames. Finally, the corresponding visual features are averaged out. For instance, if the obtained visual speech units are <math id="S3.SS2.p2.1.m1.6" class="ltx_Math" alttext="\{7,7,7,16,9,9\}" display="inline"><semantics id="S3.SS2.p2.1.m1.6a"><mrow id="S3.SS2.p2.1.m1.6.7.2" xref="S3.SS2.p2.1.m1.6.7.1.cmml"><mo stretchy="false" id="S3.SS2.p2.1.m1.6.7.2.1" xref="S3.SS2.p2.1.m1.6.7.1.cmml">{</mo><mn id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">7</mn><mo id="S3.SS2.p2.1.m1.6.7.2.2" xref="S3.SS2.p2.1.m1.6.7.1.cmml">,</mo><mn id="S3.SS2.p2.1.m1.2.2" xref="S3.SS2.p2.1.m1.2.2.cmml">7</mn><mo id="S3.SS2.p2.1.m1.6.7.2.3" xref="S3.SS2.p2.1.m1.6.7.1.cmml">,</mo><mn id="S3.SS2.p2.1.m1.3.3" xref="S3.SS2.p2.1.m1.3.3.cmml">7</mn><mo id="S3.SS2.p2.1.m1.6.7.2.4" xref="S3.SS2.p2.1.m1.6.7.1.cmml">,</mo><mn id="S3.SS2.p2.1.m1.4.4" xref="S3.SS2.p2.1.m1.4.4.cmml">16</mn><mo id="S3.SS2.p2.1.m1.6.7.2.5" xref="S3.SS2.p2.1.m1.6.7.1.cmml">,</mo><mn id="S3.SS2.p2.1.m1.5.5" xref="S3.SS2.p2.1.m1.5.5.cmml">9</mn><mo id="S3.SS2.p2.1.m1.6.7.2.6" xref="S3.SS2.p2.1.m1.6.7.1.cmml">,</mo><mn id="S3.SS2.p2.1.m1.6.6" xref="S3.SS2.p2.1.m1.6.6.cmml">9</mn><mo stretchy="false" id="S3.SS2.p2.1.m1.6.7.2.7" xref="S3.SS2.p2.1.m1.6.7.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.6b"><set id="S3.SS2.p2.1.m1.6.7.1.cmml" xref="S3.SS2.p2.1.m1.6.7.2"><cn type="integer" id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">7</cn><cn type="integer" id="S3.SS2.p2.1.m1.2.2.cmml" xref="S3.SS2.p2.1.m1.2.2">7</cn><cn type="integer" id="S3.SS2.p2.1.m1.3.3.cmml" xref="S3.SS2.p2.1.m1.3.3">7</cn><cn type="integer" id="S3.SS2.p2.1.m1.4.4.cmml" xref="S3.SS2.p2.1.m1.4.4">16</cn><cn type="integer" id="S3.SS2.p2.1.m1.5.5.cmml" xref="S3.SS2.p2.1.m1.5.5">9</cn><cn type="integer" id="S3.SS2.p2.1.m1.6.6.cmml" xref="S3.SS2.p2.1.m1.6.6">9</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.6c">\{7,7,7,16,9,9\}</annotation></semantics></math> as illustrated in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Visual Speech Processing ‣ 2 Related Work ‣ Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, then the visual features at positions 1, 2, and 3 are averaged together, and those at positions 5 and 6 are averaged, resulting in 3 frames. We find that the proposed visual speech unit based deduplication reduces the sequence lengths by about 46.62% compared to the input video lengths. Most importantly, we observed that the deduplication process does not result in any drop in performance. The reduced visual features, when converted into text (Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Visual Speech Unit based Deduplication ‣ 3 Method ‣ Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>b), maintain the meaning of each word while the duplication of each word has been removed. For instance, the recurrence of ’is’ and ’a’, which appeared multiple times in the original feature, is reduced, and the length of ’social’, which has a long stretch, is also drastically reduced.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<p id="S3.F2.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S3.F2.1.1" class="ltx_text"><img src="/html/2402.15151/assets/x2.png" id="S3.F2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="470" height="265" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Textulaization results of the visual speech representations. GT, (a), and (b) indicate the ground truth, textualization without deduplication, and textualization with deduplication, respectively.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Multi-task Learning with Instruction</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">One advantage of bridging LLMs into visual speech processing is that we can leverage the versatility of LLMs as well. To investigate this, we train the proposed VSP-LLM with two tasks, VSR and VST. VSR aims to recognize the input silent speech while VST aims not only to predict the recognized speech but also to translate it into the target language. We design the system so that tasks can be controlled by inputting instructions directly into the LLM. When performing the VSR task the instruction is set to as below,</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<pre id="S3.SS3.p2.1" class="ltx_verbatim ltx_font_typewriter">
Recognize this speech in English.
Input: ${Dedupped_Visual_Feature}
</pre>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">where the deduplicated visual features are inserted after the instruction.
Otherwise, to perform VST, the following instruction is employed.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<pre id="S3.SS3.p4.1" class="ltx_verbatim ltx_font_typewriter">
Translate this English speech to ${TGT LANG}.
Input: ${Dedupped_Visual_Feature}
</pre>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.5" class="ltx_p">where the target language is used for the position of <span id="S3.SS3.p5.5.1" class="ltx_text ltx_font_typewriter">TGT LANG</span>. The objective function for each task can be written as follows,</p>
<table id="A2.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.3" class="ltx_Math" alttext="\displaystyle\mathcal{L}=-\sum^{L}_{l=1}\ \log p(y^{l}|X,I,y^{&lt;l})," display="inline"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.3.1.1.3" xref="S3.E1.m1.3.3.1.1.3.cmml">ℒ</mi><mo id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.1" xref="S3.E1.m1.3.3.1.1.1.cmml"><mo id="S3.E1.m1.3.3.1.1.1a" xref="S3.E1.m1.3.3.1.1.1.cmml">−</mo><mrow id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E1.m1.3.3.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml"><munderover id="S3.E1.m1.3.3.1.1.1.1.2a" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E1.m1.3.3.1.1.1.1.2.2.2" xref="S3.E1.m1.3.3.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.2.3" xref="S3.E1.m1.3.3.1.1.1.1.2.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.2.3.2" xref="S3.E1.m1.3.3.1.1.1.1.2.3.2.cmml">l</mi><mo id="S3.E1.m1.3.3.1.1.1.1.2.3.1" xref="S3.E1.m1.3.3.1.1.1.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.3.3.1.1.1.1.2.3.3" xref="S3.E1.m1.3.3.1.1.1.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.3.3.1.1.1.1.2.2.3" xref="S3.E1.m1.3.3.1.1.1.1.2.2.3.cmml">L</mi></munderover></mstyle><mrow id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.3.1" xref="S3.E1.m1.3.3.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E1.m1.3.3.1.1.1.1.1.3a" xref="S3.E1.m1.3.3.1.1.1.1.1.3.cmml">⁡</mo><mi id="S3.E1.m1.3.3.1.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.1.1.3.2.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml"><msup id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml">y</mi><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml">l</mi></msup><mo fence="false" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">X</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">I</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><msup id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">y</mi><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">l</mi></mrow></msup></mrow></mrow><mo stretchy="false" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1"><eq id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2"></eq><ci id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.3">ℒ</ci><apply id="S3.E1.m1.3.3.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1"><minus id="S3.E1.m1.3.3.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1"></minus><apply id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1"><apply id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2">subscript</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2">superscript</csymbol><sum id="S3.E1.m1.3.3.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.2.2"></sum><ci id="S3.E1.m1.3.3.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.2.3">𝐿</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.3"><eq id="S3.E1.m1.3.3.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.3.1"></eq><ci id="S3.E1.m1.3.3.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.3.2">𝑙</ci><cn type="integer" id="S3.E1.m1.3.3.1.1.1.1.2.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1"><times id="S3.E1.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.2"></times><apply id="S3.E1.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.3"><log id="S3.E1.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.3.1"></log><ci id="S3.E1.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.3.2">𝑝</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2">conditional</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.2">𝑦</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.3">𝑙</ci></apply><list id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑋</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝐼</ci><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2">𝑦</ci><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3"><lt id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑙</ci></apply></apply></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\displaystyle\mathcal{L}=-\sum^{L}_{l=1}\ \log p(y^{l}|X,I,y^{&lt;l}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p5.4" class="ltx_p">where <math id="S3.SS3.p5.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS3.p5.1.m1.1a"><mi id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.1b"><ci id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.1c">X</annotation></semantics></math> is input video, <math id="S3.SS3.p5.2.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS3.p5.2.m2.1a"><mi id="S3.SS3.p5.2.m2.1.1" xref="S3.SS3.p5.2.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.1b"><ci id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.1c">I</annotation></semantics></math> is instruction used, <math id="S3.SS3.p5.3.m3.1" class="ltx_Math" alttext="y^{&lt;l}" display="inline"><semantics id="S3.SS3.p5.3.m3.1a"><msup id="S3.SS3.p5.3.m3.1.1" xref="S3.SS3.p5.3.m3.1.1.cmml"><mi id="S3.SS3.p5.3.m3.1.1.2" xref="S3.SS3.p5.3.m3.1.1.2.cmml">y</mi><mrow id="S3.SS3.p5.3.m3.1.1.3" xref="S3.SS3.p5.3.m3.1.1.3.cmml"><mi id="S3.SS3.p5.3.m3.1.1.3.2" xref="S3.SS3.p5.3.m3.1.1.3.2.cmml"></mi><mo id="S3.SS3.p5.3.m3.1.1.3.1" xref="S3.SS3.p5.3.m3.1.1.3.1.cmml">&lt;</mo><mi id="S3.SS3.p5.3.m3.1.1.3.3" xref="S3.SS3.p5.3.m3.1.1.3.3.cmml">l</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.3.m3.1b"><apply id="S3.SS3.p5.3.m3.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.3.m3.1.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1">superscript</csymbol><ci id="S3.SS3.p5.3.m3.1.1.2.cmml" xref="S3.SS3.p5.3.m3.1.1.2">𝑦</ci><apply id="S3.SS3.p5.3.m3.1.1.3.cmml" xref="S3.SS3.p5.3.m3.1.1.3"><lt id="S3.SS3.p5.3.m3.1.1.3.1.cmml" xref="S3.SS3.p5.3.m3.1.1.3.1"></lt><csymbol cd="latexml" id="S3.SS3.p5.3.m3.1.1.3.2.cmml" xref="S3.SS3.p5.3.m3.1.1.3.2">absent</csymbol><ci id="S3.SS3.p5.3.m3.1.1.3.3.cmml" xref="S3.SS3.p5.3.m3.1.1.3.3">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.3.m3.1c">y^{&lt;l}</annotation></semantics></math> is the previous predictions, and <math id="S3.SS3.p5.4.m4.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS3.p5.4.m4.1a"><mi id="S3.SS3.p5.4.m4.1.1" xref="S3.SS3.p5.4.m4.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.4.m4.1b"><ci id="S3.SS3.p5.4.m4.1.1.cmml" xref="S3.SS3.p5.4.m4.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.4.m4.1c">L</annotation></semantics></math> is the length of ground truth. Please note that this is the first work exploring a unified framework of VSR and VST. For training, we employ the recently proposed QLoRA <cite class="ltx_cite ltx_citemacro_cite">Dettmers et al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite> to further relieve the computational load in training LLM.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Lip Reading Sentences 3 (LRS3)</span> <cite class="ltx_cite ltx_citemacro_cite">Afouras et al. (<a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite> is the most widely-used dataset for VSR, which comprises 433 hours of English audio-visual speech corpus with transcription data. These corpora are collected from the TED and TEDx talks. We utilize the LRS3 dataset to and measure the VSR performance of the proposed unified model.
<span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_bold">Multilingual Audio-Visual Corpus (MuAViC)</span> <cite class="ltx_cite ltx_citemacro_cite">Anwar et al. (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> is a multilingual audio-visual dataset designed for speech recognition and speech-to-text translation. It includes 1200 hours of audio-visual corpus in 9 languages, providing full transcriptions and covering 6 English-to-X translations, as well as 6 X-to-English translation directions. To evaluate the VST performance of our model, we utilize English-to-X translation data from MuAViC dataset, where X can be among four languages, Spanish (Es), French (Fr), Portuguese (Pt), and Italian (It).
For training our model, we combine LRS3 <cite class="ltx_cite ltx_citemacro_cite">Afouras et al. (<a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite> and English-to-X translation data of MuAViC <cite class="ltx_cite ltx_citemacro_cite">Anwar et al. (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:229.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-154.6pt,86.0pt) scale(0.57129698497238,0.57129698497238) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_nopad_l ltx_align_center" style="padding:2.5pt 0.0pt;" colspan="2">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>  Method</td>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">
<span id="S4.T1.1.1.1.2.1" class="ltx_text"></span> <span id="S4.T1.1.1.1.2.2" class="ltx_text">
<span id="S4.T1.1.1.1.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.1.1.1.2.2.1.1" class="ltx_tr">
<span id="S4.T1.1.1.1.2.2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;"><span id="S4.T1.1.1.1.2.2.1.1.1.1" class="ltx_text ltx_font_bold">Pre-training</span></span></span>
<span id="S4.T1.1.1.1.2.2.1.2" class="ltx_tr">
<span id="S4.T1.1.1.1.2.2.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;"><span id="S4.T1.1.1.1.2.2.1.2.1.1" class="ltx_text ltx_font_bold">Data (hrs)</span></span></span>
</span></span><span id="S4.T1.1.1.1.2.3" class="ltx_text"></span></td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">
<span id="S4.T1.1.1.1.3.1" class="ltx_text"></span> <span id="S4.T1.1.1.1.3.2" class="ltx_text">
<span id="S4.T1.1.1.1.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.1.1.1.3.2.1.1" class="ltx_tr">
<span id="S4.T1.1.1.1.3.2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;"><span id="S4.T1.1.1.1.3.2.1.1.1.1" class="ltx_text ltx_font_bold">Labeled</span></span></span>
<span id="S4.T1.1.1.1.3.2.1.2" class="ltx_tr">
<span id="S4.T1.1.1.1.3.2.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;"><span id="S4.T1.1.1.1.3.2.1.2.1.1" class="ltx_text ltx_font_bold">Training Data (hrs)</span></span></span>
</span></span><span id="S4.T1.1.1.1.3.3" class="ltx_text"></span></td>
<td id="S4.T1.1.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">
<span id="S4.T1.1.1.1.4.1" class="ltx_text"></span> <span id="S4.T1.1.1.1.4.2" class="ltx_text">
<span id="S4.T1.1.1.1.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.1.1.1.4.2.1.1" class="ltx_tr">
<span id="S4.T1.1.1.1.4.2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;"><span id="S4.T1.1.1.1.4.2.1.1.1.1" class="ltx_text ltx_font_bold">Recognition</span></span></span>
<span id="S4.T1.1.1.1.4.2.1.2" class="ltx_tr">
<span id="S4.T1.1.1.1.4.2.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;"><span id="S4.T1.1.1.1.4.2.1.2.1.1" class="ltx_text ltx_font_bold">Task</span></span></span>
</span></span><span id="S4.T1.1.1.1.4.3" class="ltx_text"></span></td>
<td id="S4.T1.1.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">
<span id="S4.T1.1.1.1.5.1" class="ltx_text"></span> <span id="S4.T1.1.1.1.5.2" class="ltx_text">
<span id="S4.T1.1.1.1.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.1.1.1.5.2.1.1" class="ltx_tr">
<span id="S4.T1.1.1.1.5.2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;"><span id="S4.T1.1.1.1.5.2.1.1.1.1" class="ltx_text ltx_font_bold">Translation</span></span></span>
<span id="S4.T1.1.1.1.5.2.1.2" class="ltx_tr">
<span id="S4.T1.1.1.1.5.2.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;"><span id="S4.T1.1.1.1.5.2.1.2.1.1" class="ltx_text ltx_font_bold">Task</span></span></span>
</span></span><span id="S4.T1.1.1.1.5.3" class="ltx_text"></span></td>
<td id="S4.T1.1.1.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;"><span id="S4.T1.1.1.1.6.1" class="ltx_text ltx_font_bold">WER(%)</span></td>
</tr>
<tr id="S4.T1.1.1.2" class="ltx_tr">
<td id="S4.T1.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:2.5pt 0.0pt;" rowspan="8"><span id="S4.T1.1.1.2.1.1" class="ltx_text ltx_font_bold">Supervised</span></td>
<td id="S4.T1.1.1.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:2.5pt 0.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Afouras et al. (<a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite></td>
<td id="S4.T1.1.1.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:2.5pt 0.0pt;">-</td>
<td id="S4.T1.1.1.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:2.5pt 0.0pt;">1,519</td>
<td id="S4.T1.1.1.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:2.5pt 0.0pt;">58.9</td>
</tr>
<tr id="S4.T1.1.1.3" class="ltx_tr">
<td id="S4.T1.1.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Shillingford et al. (<a href="#bib.bib44" title="" class="ltx_ref">2019</a>)</cite></td>
<td id="S4.T1.1.1.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">-</td>
<td id="S4.T1.1.1.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">3,886</td>
<td id="S4.T1.1.1.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">55.1</td>
</tr>
<tr id="S4.T1.1.1.4" class="ltx_tr">
<td id="S4.T1.1.1.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Makino et al. (<a href="#bib.bib33" title="" class="ltx_ref">2019</a>)</cite></td>
<td id="S4.T1.1.1.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">-</td>
<td id="S4.T1.1.1.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">31,000</td>
<td id="S4.T1.1.1.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">33.6</td>
</tr>
<tr id="S4.T1.1.1.5" class="ltx_tr">
<td id="S4.T1.1.1.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Prajwal et al. (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="S4.T1.1.1.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">-</td>
<td id="S4.T1.1.1.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">2,676</td>
<td id="S4.T1.1.1.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">30.7</td>
</tr>
<tr id="S4.T1.1.1.6" class="ltx_tr">
<td id="S4.T1.1.1.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Ma et al. (<a href="#bib.bib30" title="" class="ltx_ref">2021b</a>)</cite></td>
<td id="S4.T1.1.1.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">-</td>
<td id="S4.T1.1.1.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">595</td>
<td id="S4.T1.1.1.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">30.4</td>
</tr>
<tr id="S4.T1.1.1.7" class="ltx_tr">
<td id="S4.T1.1.1.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Ma et al. (<a href="#bib.bib28" title="" class="ltx_ref">2023</a>)</cite></td>
<td id="S4.T1.1.1.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">-</td>
<td id="S4.T1.1.1.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">3,448</td>
<td id="S4.T1.1.1.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.7.5" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.7.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">19.1</td>
</tr>
<tr id="S4.T1.1.1.8" class="ltx_tr">
<td id="S4.T1.1.1.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Serdyuk et al. (<a href="#bib.bib42" title="" class="ltx_ref">2022</a>)</cite></td>
<td id="S4.T1.1.1.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">-</td>
<td id="S4.T1.1.1.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">90,000</td>
<td id="S4.T1.1.1.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.8.5" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.8.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">17.0</td>
</tr>
<tr id="S4.T1.1.1.9" class="ltx_tr">
<td id="S4.T1.1.1.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Chang et al. (<a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite></td>
<td id="S4.T1.1.1.9.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">-</td>
<td id="S4.T1.1.1.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">100,000</td>
<td id="S4.T1.1.1.9.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.9.5" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.9.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">12.8</td>
</tr>
<tr id="S4.T1.1.1.10" class="ltx_tr">
<td id="S4.T1.1.1.10.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:2.5pt 0.0pt;" rowspan="4"><span id="S4.T1.1.1.10.1.1" class="ltx_text ltx_font_bold">Self-supervised</span></td>
<td id="S4.T1.1.1.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:2.5pt 0.0pt;">AV-HuBERT <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S4.T1.1.1.10.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:2.5pt 0.0pt;">1,759</td>
<td id="S4.T1.1.1.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:2.5pt 0.0pt;">30</td>
<td id="S4.T1.1.1.10.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.10.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.10.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:2.5pt 0.0pt;">32.5</td>
</tr>
<tr id="S4.T1.1.1.11" class="ltx_tr">
<td id="S4.T1.1.1.11.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">VATLM <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib56" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S4.T1.1.1.11.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">1,759</td>
<td id="S4.T1.1.1.11.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">30</td>
<td id="S4.T1.1.1.11.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.11.5" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.11.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">31.6</td>
</tr>
<tr id="S4.T1.1.1.12" class="ltx_tr">
<td id="S4.T1.1.1.12.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">RAVen <cite class="ltx_cite ltx_citemacro_cite">Haliassos et al. (<a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S4.T1.1.1.12.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">1,759</td>
<td id="S4.T1.1.1.12.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">30</td>
<td id="S4.T1.1.1.12.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.12.5" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.12.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">32.5</td>
</tr>
<tr id="S4.T1.1.1.13" class="ltx_tr">
<td id="S4.T1.1.1.13.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">AKVSR <cite class="ltx_cite ltx_citemacro_cite">Yeo et al. (<a href="#bib.bib51" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S4.T1.1.1.13.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">1,759</td>
<td id="S4.T1.1.1.13.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">30</td>
<td id="S4.T1.1.1.13.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.13.5" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.13.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">29.1</td>
</tr>
<tr id="S4.T1.1.1.14" class="ltx_tr">
<td id="S4.T1.1.1.14.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">
<span id="S4.T1.1.1.14.1.1" class="ltx_ERROR undefined">\cdashline</span>2-7</td>
<td id="S4.T1.1.1.14.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;"><span id="S4.T1.1.1.14.2.1" class="ltx_text ltx_font_bold">Proposed Method</span></td>
<td id="S4.T1.1.1.14.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">1,759</td>
<td id="S4.T1.1.1.14.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">30</td>
<td id="S4.T1.1.1.14.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.14.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.14.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">29.6</td>
</tr>
<tr id="S4.T1.1.1.15" class="ltx_tr">
<td id="S4.T1.1.1.15.1" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.15.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:2.5pt 0.0pt;">AV-HuBERT <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S4.T1.1.1.15.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:2.5pt 0.0pt;">1,759</td>
<td id="S4.T1.1.1.15.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:2.5pt 0.0pt;">433</td>
<td id="S4.T1.1.1.15.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.15.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_t" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.15.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:2.5pt 0.0pt;">28.6</td>
</tr>
<tr id="S4.T1.1.1.16" class="ltx_tr">
<td id="S4.T1.1.1.16.1" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.16.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">VATLM <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib56" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S4.T1.1.1.16.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">1,759</td>
<td id="S4.T1.1.1.16.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">433</td>
<td id="S4.T1.1.1.16.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.16.6" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.16.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">28.4</td>
</tr>
<tr id="S4.T1.1.1.17" class="ltx_tr">
<td id="S4.T1.1.1.17.1" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.17.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">RAVen <cite class="ltx_cite ltx_citemacro_cite">Haliassos et al. (<a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S4.T1.1.1.17.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">1,759</td>
<td id="S4.T1.1.1.17.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">433</td>
<td id="S4.T1.1.1.17.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.17.6" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.17.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">27.8</td>
</tr>
<tr id="S4.T1.1.1.18" class="ltx_tr">
<td id="S4.T1.1.1.18.1" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.18.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">AKVSR <cite class="ltx_cite ltx_citemacro_cite">Yeo et al. (<a href="#bib.bib51" title="" class="ltx_ref">2023a</a>)</cite>
</td>
<td id="S4.T1.1.1.18.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">1,759</td>
<td id="S4.T1.1.1.18.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">433</td>
<td id="S4.T1.1.1.18.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.18.6" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.18.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">27.6</td>
</tr>
<tr id="S4.T1.1.1.19" class="ltx_tr">
<td id="S4.T1.1.1.19.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">
<span id="S4.T1.1.1.19.1.1" class="ltx_ERROR undefined">\cdashline</span>2-7</td>
<td id="S4.T1.1.1.19.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;" rowspan="2"><span id="S4.T1.1.1.19.2.1" class="ltx_text ltx_font_bold">Proposed Method</span></td>
<td id="S4.T1.1.1.19.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">1,759</td>
<td id="S4.T1.1.1.19.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">433</td>
<td id="S4.T1.1.1.19.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.19.6" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.19.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">27.5</td>
</tr>
<tr id="S4.T1.1.1.20" class="ltx_tr">
<td id="S4.T1.1.1.20.1" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.20.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">1,759</td>
<td id="S4.T1.1.1.20.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">433</td>
<td id="S4.T1.1.1.20.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.20.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">✓</td>
<td id="S4.T1.1.1.20.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;">26.9</td>
</tr>
<tr id="S4.T1.1.1.21" class="ltx_tr">
<td id="S4.T1.1.1.21.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:2.5pt 0.0pt;"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td id="S4.T1.1.1.21.2" class="ltx_td" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.21.3" class="ltx_td" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.21.4" class="ltx_td" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.21.5" class="ltx_td" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.21.6" class="ltx_td" style="padding:2.5pt 0.0pt;"></td>
<td id="S4.T1.1.1.21.7" class="ltx_td" style="padding:2.5pt 0.0pt;"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The performance comparisons with state-of-the-art supervised and self-supvervised VSR methods. Compared to the self-supervised methods, the proposed unified model, which can perform both VSR and VST, achieves state-of-the-art recognition performances. Furthermore, our method obtains comparable performances even with much less amount of training data. Please note that the only method which can perform recogntion and translation at once is the proposed model.</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.2pt;height:157.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-4.1pt,1.5pt) scale(0.981284083548599,0.981284083548599) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;" rowspan="2">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
<span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Method</span>
</td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;" rowspan="2"><span id="S4.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Modality</span></td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;" rowspan="2"><span id="S4.T2.1.1.1.4.1" class="ltx_text"><span id="S4.T2.1.1.1.4.1.1" class="ltx_text"></span> <span id="S4.T2.1.1.1.4.1.2" class="ltx_text">
<span id="S4.T2.1.1.1.4.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.1.4.1.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.1.4.1.2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;"><span id="S4.T2.1.1.1.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Labeled</span></span></span>
<span id="S4.T2.1.1.1.4.1.2.1.2" class="ltx_tr">
<span id="S4.T2.1.1.1.4.1.2.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;"><span id="S4.T2.1.1.1.4.1.2.1.2.1.1" class="ltx_text ltx_font_bold">data(hrs)</span></span></span>
</span></span> <span id="S4.T2.1.1.1.4.1.3" class="ltx_text"></span></span></td>
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_nopad_l ltx_align_center" style="padding:3pt 0.0pt;" colspan="5"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">BLEU <math id="S4.T2.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T2.1.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
</tr>
<tr id="S4.T2.1.1.2" class="ltx_tr">
<td id="S4.T2.1.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:3pt 0.0pt;"><span id="S4.T2.1.1.2.1.1" class="ltx_text ltx_font_bold">En-It</span></td>
<td id="S4.T2.1.1.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:3pt 0.0pt;"><span id="S4.T2.1.1.2.2.1" class="ltx_text ltx_font_bold">En-Fr</span></td>
<td id="S4.T2.1.1.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:3pt 0.0pt;"><span id="S4.T2.1.1.2.3.1" class="ltx_text ltx_font_bold">En-Pt</span></td>
<td id="S4.T2.1.1.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:3pt 0.0pt;"><span id="S4.T2.1.1.2.4.1" class="ltx_text ltx_font_bold">En-Es</span></td>
<td id="S4.T2.1.1.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:3pt 0.0pt;"><span id="S4.T2.1.1.2.5.1" class="ltx_text ltx_font_bold">Avg</span></td>
</tr>
<tr id="S4.T2.1.1.3" class="ltx_tr">
<td id="S4.T2.1.1.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:3pt 0.0pt;"><cite class="ltx_cite ltx_citemacro_citet">Anwar et al. (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite></td>
<td id="S4.T2.1.1.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:3pt 0.0pt;">AV</td>
<td id="S4.T2.1.1.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:3pt 0.0pt;">433</td>
<td id="S4.T2.1.1.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:3pt 0.0pt;">20.7</td>
<td id="S4.T2.1.1.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:3pt 0.0pt;">25.3</td>
<td id="S4.T2.1.1.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:3pt 0.0pt;">20.5</td>
<td id="S4.T2.1.1.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:3pt 0.0pt;">26.6</td>
<td id="S4.T2.1.1.3.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:3pt 0.0pt;">23.3</td>
</tr>
<tr id="S4.T2.1.1.4" class="ltx_tr">
<td id="S4.T2.1.1.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">
<span id="S4.T2.1.1.4.1.1" class="ltx_text"></span> <span id="S4.T2.1.1.4.1.2" class="ltx_text">
<span id="S4.T2.1.1.4.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.4.1.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.4.1.2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">AV-HuBERT</span></span>
</span></span><span id="S4.T2.1.1.4.1.3" class="ltx_text"></span></td>
<td id="S4.T2.1.1.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">V</td>
<td id="S4.T2.1.1.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">433</td>
<td id="S4.T2.1.1.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">21.5</td>
<td id="S4.T2.1.1.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">26.5</td>
<td id="S4.T2.1.1.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">26.7</td>
<td id="S4.T2.1.1.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">24.5</td>
<td id="S4.T2.1.1.4.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">24.8</td>
</tr>
<tr id="S4.T2.1.1.5" class="ltx_tr">
<td id="S4.T2.1.1.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">
<span id="S4.T2.1.1.5.1.1" class="ltx_text"></span> <span id="S4.T2.1.1.5.1.2" class="ltx_text">
<span id="S4.T2.1.1.5.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.5.1.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.5.1.2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;"><span id="S4.T2.1.1.5.1.2.1.1.1.1" class="ltx_text"></span> <span id="S4.T2.1.1.5.1.2.1.1.1.2" class="ltx_text">
<span id="S4.T2.1.1.5.1.2.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.5.1.2.1.1.1.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.5.1.2.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">Cascaded (AV-HuBERT + MT)</span></span>
</span></span><span id="S4.T2.1.1.5.1.2.1.1.1.3" class="ltx_text"></span></span></span>
</span></span><span id="S4.T2.1.1.5.1.3" class="ltx_text"></span></td>
<td id="S4.T2.1.1.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">V</td>
<td id="S4.T2.1.1.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">433</td>
<td id="S4.T2.1.1.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">22.2</td>
<td id="S4.T2.1.1.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">23.9</td>
<td id="S4.T2.1.1.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">24.4</td>
<td id="S4.T2.1.1.5.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">23.5</td>
<td id="S4.T2.1.1.5.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">23.5</td>
</tr>
<tr id="S4.T2.1.1.6" class="ltx_tr">
<td id="S4.T2.1.1.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">
<span id="S4.T2.1.1.6.1.1" class="ltx_ERROR undefined">\hdashline</span><span id="S4.T2.1.1.6.1.2" class="ltx_text ltx_font_bold">Proposed Method</span>
</td>
<td id="S4.T2.1.1.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">V</td>
<td id="S4.T2.1.1.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">15</td>
<td id="S4.T2.1.1.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;"><span id="S4.T2.1.1.6.4.1" class="ltx_text ltx_font_bold">20.3</span></td>
<td id="S4.T2.1.1.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;"><span id="S4.T2.1.1.6.5.1" class="ltx_text ltx_font_bold">24.4</span></td>
<td id="S4.T2.1.1.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;"><span id="S4.T2.1.1.6.6.1" class="ltx_text ltx_font_bold">24.7</span></td>
<td id="S4.T2.1.1.6.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;"><span id="S4.T2.1.1.6.7.1" class="ltx_text ltx_font_bold">24.7</span></td>
<td id="S4.T2.1.1.6.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;"><span id="S4.T2.1.1.6.8.1" class="ltx_text ltx_font_bold">23.5</span></td>
</tr>
<tr id="S4.T2.1.1.7" class="ltx_tr">
<td id="S4.T2.1.1.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;"><span id="S4.T2.1.1.7.1.1" class="ltx_text ltx_font_bold">Proposed Method</span></td>
<td id="S4.T2.1.1.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">V</td>
<td id="S4.T2.1.1.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;">433</td>
<td id="S4.T2.1.1.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;"><span id="S4.T2.1.1.7.4.1" class="ltx_text ltx_font_bold">29.4</span></td>
<td id="S4.T2.1.1.7.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;"><span id="S4.T2.1.1.7.5.1" class="ltx_text ltx_font_bold">33.0</span></td>
<td id="S4.T2.1.1.7.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;"><span id="S4.T2.1.1.7.6.1" class="ltx_text ltx_font_bold">33.7</span></td>
<td id="S4.T2.1.1.7.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;"><span id="S4.T2.1.1.7.7.1" class="ltx_text ltx_font_bold">33.8</span></td>
<td id="S4.T2.1.1.7.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;"><span id="S4.T2.1.1.7.8.1" class="ltx_text ltx_font_bold">32.5</span></td>
</tr>
<tr id="S4.T2.1.1.8" class="ltx_tr">
<td id="S4.T2.1.1.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:3pt 0.0pt;"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td id="S4.T2.1.1.8.2" class="ltx_td" style="padding:3pt 0.0pt;"></td>
<td id="S4.T2.1.1.8.3" class="ltx_td" style="padding:3pt 0.0pt;"></td>
<td id="S4.T2.1.1.8.4" class="ltx_td" style="padding:3pt 0.0pt;"></td>
<td id="S4.T2.1.1.8.5" class="ltx_td" style="padding:3pt 0.0pt;"></td>
<td id="S4.T2.1.1.8.6" class="ltx_td" style="padding:3pt 0.0pt;"></td>
<td id="S4.T2.1.1.8.7" class="ltx_td" style="padding:3pt 0.0pt;"></td>
<td id="S4.T2.1.1.8.8" class="ltx_td" style="padding:3pt 0.0pt;"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The experimental results showing the effectiveness on VST, English to target language (En-X) speech translation tasks, on MuAViC.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p"><span id="S4.SS2.p1.2.1" class="ltx_text ltx_font_bold">Preprocessing.</span> The video is resampled at 25 fps, and facial landmarks are detected using RetinaFace <cite class="ltx_cite ltx_citemacro_cite">Deng et al. (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>. Mouth regions are cropped using bounding boxes of size <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="96\times 96" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">96</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">96</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">96</cn><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">96</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">96\times 96</annotation></semantics></math> and converted to grayscale. During training, we apply data augmentation by randomly cropping the video to <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="88\times 88" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">88</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">88</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><times id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">88</cn><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">88</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">88\times 88</annotation></semantics></math> and horizontally flipping it.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Architecture.</span> We use the AV-HuBERT <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_italic">large</span> <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite> pre-trained on LRS3 <cite class="ltx_cite ltx_citemacro_cite">Afouras et al. (<a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite> and VoxCeleb2 English <cite class="ltx_cite ltx_citemacro_cite">Chung et al. (<a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite> as our visual encoder. In all experiments, except the ablation part, we utilize 200 clustered visual speech units. For the LLM, we adopt LLaMA2-7B <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib46" title="" class="ltx_ref">2023</a>)</cite> and fine-tune it using QLoRA <cite class="ltx_cite ltx_citemacro_cite">Dettmers et al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite> with the rank value of 16 and a dropout rate of 5%. To align the dimensions of the visual representation from the visual encoder to the LLaMA input embedding, we use a single linear layer as our visual-to-text embedding layer.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.4" class="ltx_p"><span id="S4.SS2.p3.4.1" class="ltx_text ltx_font_bold">Training and evaluation.</span> We follow AV-HuBERT <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite> except for the number of updates and learning rate. We conduct training with 15K updates and a learning rate of <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="1e^{-3}" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mn id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.cmml">​</mo><msup id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml"><mi id="S4.SS2.p3.1.m1.1.1.3.2" xref="S4.SS2.p3.1.m1.1.1.3.2.cmml">e</mi><mrow id="S4.SS2.p3.1.m1.1.1.3.3" xref="S4.SS2.p3.1.m1.1.1.3.3.cmml"><mo id="S4.SS2.p3.1.m1.1.1.3.3a" xref="S4.SS2.p3.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS2.p3.1.m1.1.1.3.3.2" xref="S4.SS2.p3.1.m1.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><times id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">1</cn><apply id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.3.1.cmml" xref="S4.SS2.p3.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS2.p3.1.m1.1.1.3.2.cmml" xref="S4.SS2.p3.1.m1.1.1.3.2">𝑒</ci><apply id="S4.SS2.p3.1.m1.1.1.3.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3.3"><minus id="S4.SS2.p3.1.m1.1.1.3.3.1.cmml" xref="S4.SS2.p3.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.SS2.p3.1.m1.1.1.3.3.2.cmml" xref="S4.SS2.p3.1.m1.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">1e^{-3}</annotation></semantics></math> for LRS3 1h, 5h, 10h, and 30K updates with a learning rate of <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="5e^{-4}" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mrow id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml"><mn id="S4.SS2.p3.2.m2.1.1.2" xref="S4.SS2.p3.2.m2.1.1.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p3.2.m2.1.1.1" xref="S4.SS2.p3.2.m2.1.1.1.cmml">​</mo><msup id="S4.SS2.p3.2.m2.1.1.3" xref="S4.SS2.p3.2.m2.1.1.3.cmml"><mi id="S4.SS2.p3.2.m2.1.1.3.2" xref="S4.SS2.p3.2.m2.1.1.3.2.cmml">e</mi><mrow id="S4.SS2.p3.2.m2.1.1.3.3" xref="S4.SS2.p3.2.m2.1.1.3.3.cmml"><mo id="S4.SS2.p3.2.m2.1.1.3.3a" xref="S4.SS2.p3.2.m2.1.1.3.3.cmml">−</mo><mn id="S4.SS2.p3.2.m2.1.1.3.3.2" xref="S4.SS2.p3.2.m2.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><apply id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1"><times id="S4.SS2.p3.2.m2.1.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.p3.2.m2.1.1.2.cmml" xref="S4.SS2.p3.2.m2.1.1.2">5</cn><apply id="S4.SS2.p3.2.m2.1.1.3.cmml" xref="S4.SS2.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p3.2.m2.1.1.3.1.cmml" xref="S4.SS2.p3.2.m2.1.1.3">superscript</csymbol><ci id="S4.SS2.p3.2.m2.1.1.3.2.cmml" xref="S4.SS2.p3.2.m2.1.1.3.2">𝑒</ci><apply id="S4.SS2.p3.2.m2.1.1.3.3.cmml" xref="S4.SS2.p3.2.m2.1.1.3.3"><minus id="S4.SS2.p3.2.m2.1.1.3.3.1.cmml" xref="S4.SS2.p3.2.m2.1.1.3.3"></minus><cn type="integer" id="S4.SS2.p3.2.m2.1.1.3.3.2.cmml" xref="S4.SS2.p3.2.m2.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">5e^{-4}</annotation></semantics></math> for LRS3 30h and 433h. Adam optimizer is employed for training with <math id="S4.SS2.p3.3.m3.1" class="ltx_Math" alttext="\beta_{1}=0.9" display="inline"><semantics id="S4.SS2.p3.3.m3.1a"><mrow id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml"><msub id="S4.SS2.p3.3.m3.1.1.2" xref="S4.SS2.p3.3.m3.1.1.2.cmml"><mi id="S4.SS2.p3.3.m3.1.1.2.2" xref="S4.SS2.p3.3.m3.1.1.2.2.cmml">β</mi><mn id="S4.SS2.p3.3.m3.1.1.2.3" xref="S4.SS2.p3.3.m3.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS2.p3.3.m3.1.1.1" xref="S4.SS2.p3.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS2.p3.3.m3.1.1.3" xref="S4.SS2.p3.3.m3.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><apply id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1"><eq id="S4.SS2.p3.3.m3.1.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1.1"></eq><apply id="S4.SS2.p3.3.m3.1.1.2.cmml" xref="S4.SS2.p3.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p3.3.m3.1.1.2.1.cmml" xref="S4.SS2.p3.3.m3.1.1.2">subscript</csymbol><ci id="S4.SS2.p3.3.m3.1.1.2.2.cmml" xref="S4.SS2.p3.3.m3.1.1.2.2">𝛽</ci><cn type="integer" id="S4.SS2.p3.3.m3.1.1.2.3.cmml" xref="S4.SS2.p3.3.m3.1.1.2.3">1</cn></apply><cn type="float" id="S4.SS2.p3.3.m3.1.1.3.cmml" xref="S4.SS2.p3.3.m3.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">\beta_{1}=0.9</annotation></semantics></math> and <math id="S4.SS2.p3.4.m4.1" class="ltx_Math" alttext="\beta_{2}=0.98" display="inline"><semantics id="S4.SS2.p3.4.m4.1a"><mrow id="S4.SS2.p3.4.m4.1.1" xref="S4.SS2.p3.4.m4.1.1.cmml"><msub id="S4.SS2.p3.4.m4.1.1.2" xref="S4.SS2.p3.4.m4.1.1.2.cmml"><mi id="S4.SS2.p3.4.m4.1.1.2.2" xref="S4.SS2.p3.4.m4.1.1.2.2.cmml">β</mi><mn id="S4.SS2.p3.4.m4.1.1.2.3" xref="S4.SS2.p3.4.m4.1.1.2.3.cmml">2</mn></msub><mo id="S4.SS2.p3.4.m4.1.1.1" xref="S4.SS2.p3.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS2.p3.4.m4.1.1.3" xref="S4.SS2.p3.4.m4.1.1.3.cmml">0.98</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.1b"><apply id="S4.SS2.p3.4.m4.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1"><eq id="S4.SS2.p3.4.m4.1.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1.1"></eq><apply id="S4.SS2.p3.4.m4.1.1.2.cmml" xref="S4.SS2.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p3.4.m4.1.1.2.1.cmml" xref="S4.SS2.p3.4.m4.1.1.2">subscript</csymbol><ci id="S4.SS2.p3.4.m4.1.1.2.2.cmml" xref="S4.SS2.p3.4.m4.1.1.2.2">𝛽</ci><cn type="integer" id="S4.SS2.p3.4.m4.1.1.2.3.cmml" xref="S4.SS2.p3.4.m4.1.1.2.3">2</cn></apply><cn type="float" id="S4.SS2.p3.4.m4.1.1.3.cmml" xref="S4.SS2.p3.4.m4.1.1.3">0.98</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.1c">\beta_{2}=0.98</annotation></semantics></math>, utilizing a tri-stage learning rate scheduler. The training process is executed on 8 3090 RTX GPUs. For decoding, we use a beam search with a beam width of 20 and a length penalty of 0. We assess the performance of our model using Word Error Rate (WER) for the VSR task and BLEU score <cite class="ltx_cite ltx_citemacro_cite">Papineni et al. (<a href="#bib.bib34" title="" class="ltx_ref">2002</a>)</cite> for the VST task. We use total FLOPs per epoch as a metric to measure the model operation count during training.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Experimental Results</h3>

<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Comparison with State-of-the-arts</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">In this subsection, we compare the proposed unified model with state-of-the-art VSR methods and VST methods. Please note that the proposed model can perform multi-tasks VSR and VST with a single trained model while the other models need a single model per specific task.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Dataset ‣ 4 Experiment ‣ Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the performance comparisons of the proposed method with state-of-the-art VSR methods on the LRS3 dataset. The top section of Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Dataset ‣ 4 Experiment ‣ Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> outlines the performance of current supervised approaches that depend on extensive labeled training data, while the lower section presents a comparison with other self-supervised methods. Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Dataset ‣ 4 Experiment ‣ Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> demonstrates that our approach achieves performance on par with others by employing just 30 hours of labeled data, despite the proposed unified model’s ability to handle multiple tasks—VSR and VST—simultaneously. Additionally, with the use of 433 hours of data, our method reaches state-of-the-art performance, achieving a WER of 26.9%, surpassing other self-supervised approaches. Moreover, Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Dataset ‣ 4 Experiment ‣ Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>’s upper part shows that the existing supervised methods record exceptional performance using (tens of) thousands of labeled data. However, it is important to highlight that the proposed unified model can obtain comparable performances to several supervised methods.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para">
<p id="S4.SS3.SSS1.p3.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Dataset ‣ 4 Experiment ‣ Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the comparison results of VST performance. We construct two baseline models for comparison. The first, AV-HuBERT, is trained similarly to our approach, utilizing both VSR and VST datasets. The second model is a cascaded system that incorporates a pre-trained AV-HuBERT for VSR with a neural machine translation model <cite class="ltx_cite ltx_citemacro_cite">Fan et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>. Through this comparison, our proposed unified model demonstrates superior VST performance across four English-to-X translation tasks, achieving BLEU scores of 29.4, 33.0, 33.7, and 33.8 for English to Italian, French, Portuguese, and Spanish, respectively. Moreover, it is worth noting that the proposed method achieves a 23.5 BLEU score on average with only 15 hours of labeled data, outperforming the audio-visual speech translation model <cite class="ltx_cite ltx_citemacro_cite">Anwar et al. (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> trained with 433 hours of labeled data.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<p id="S4.F3.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S4.F3.1.1" class="ltx_text"><img src="/html/2402.15151/assets/x3.png" id="S4.F3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="484" height="241" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The qualitative results showing that the contextual modeling ability of LLM, which is adopted in our method, can improve the homophene problem and other confusing cases. The <span id="S4.F3.7.1" class="ltx_text" style="color:#E60000;">red</span> and <span id="S4.F3.8.2" class="ltx_text" style="color:#0033E6;">blue</span> words indicate the wrong predictions from AV-HuBERT. However, as shown in the examples, the proposed method can generate correct words by considering the entire context (<span id="S4.F3.9.3" class="ltx_text ltx_font_italic">e.g.,</span> ‘<span id="S4.F3.10.4" class="ltx_text ltx_font_italic">barrow</span>’ to ‘<span id="S4.F3.11.5" class="ltx_text ltx_font_italic">marrow</span>’).</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure">
<p id="S4.F4.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S4.F4.1.1" class="ltx_text"><img src="/html/2402.15151/assets/x4.png" id="S4.F4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="460" height="268" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>VSR performance analysis on LRS3 with varying video length of test samples. Due to the strength of contextual understanding ability of LLM, the proposed method shows superior performance with longer videos.</figcaption>
</figure>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Effectiveness of Rich Context Modeling</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">We have developed a unified model incorporating Large Language Models (LLMs) to leverage their advanced context modeling capabilities. Therefore, in this section, we conduct a qualitative experiment to demonstrate the effectiveness of the proposed VSP-LLM in handling homophenes, a challenging problem that requires substantial context understanding to accurately identify homophenes. Figure <a href="#S4.F3" title="Figure 3 ‣ 4.3.1 Comparison with State-of-the-arts ‣ 4.3 Experimental Results ‣ 4 Experiment ‣ Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows several transcription examples obtained from AV-HuBERT and our model, illustrating how our proposed method accurately generates words by considering the entire context of a sentence. For instance, in a homophene case, AV-HuBERT incorrectly transcribes "barrow", a word that looks similar on the lips visually but differs in meaning from “marrow”. On the other hand, our method correctly generates "marrow", leading to complete the accurate phrase "bone marrow" in the context of describing a cancer treatment process. Similarly, AV-HuBERT’s transcription of "hair" is contextually inappropriate for a sentence discussing considerations regarding building systems. Our method, however, accurately outputs "air," resulting in the correct phrase "air system". Also, we can observe similar results in the other cases, not the homophene problem only. For example, the proposed method can generate the word “planetary” in the context of asking about the relationship between greenhouse gases and “planetary”. These results corroborate that our approach can more effectively comprehend contextual clues and generate more precise and natural answers, due to the integration of LLM.</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.1" class="ltx_p">Additionally, we evaluate the VSR performance across various video length segments to explore the effectiveness of LLM in handling long speech. Figure <a href="#S4.F4" title="Figure 4 ‣ 4.3.1 Comparison with State-of-the-arts ‣ 4.3 Experimental Results ‣ 4 Experiment ‣ Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows that WER decreases as video length increases. Notably, our proposed method exhibits outstanding recognition performance, with a WER of 13.9% on videos longer than 6 seconds. Furthermore, our method demonstrates consistent performance improvements as the length of the video increases, compared to other methods. It indicates the effectiveness of LLM’s context modeling in longer video utterances, which demand a more comprehensive understanding of context.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.2pt;height:137.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(17.7pt,-5.6pt) scale(1.08900686218801,1.08900686218801) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;" rowspan="2">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
<span id="S4.T3.1.1.1.2.1" class="ltx_text"><span id="S4.T3.1.1.1.2.1.1" class="ltx_text"></span> <span id="S4.T3.1.1.1.2.1.2" class="ltx_text">
<span id="S4.T3.1.1.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.1.1.1.2.1.2.1.1" class="ltx_tr">
<span id="S4.T3.1.1.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T3.1.1.1.2.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Number of</span></span></span>
<span id="S4.T3.1.1.1.2.1.2.1.2" class="ltx_tr">
<span id="S4.T3.1.1.1.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T3.1.1.1.2.1.2.1.2.1.1" class="ltx_text ltx_font_bold">Clusters</span></span></span>
</span></span> <span id="S4.T3.1.1.1.2.1.3" class="ltx_text"></span></span>
</td>
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;" colspan="5"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">BLEU <math id="S4.T3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T3.1.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;" rowspan="2"><span id="S4.T3.1.1.1.3.1" class="ltx_text"><span id="S4.T3.1.1.1.3.1.1" class="ltx_text"></span> <span id="S4.T3.1.1.1.3.1.2" class="ltx_text">
<span id="S4.T3.1.1.1.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.1.1.1.3.1.2.1.1" class="ltx_tr">
<span id="S4.T3.1.1.1.3.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T3.1.1.1.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Length of</span></span></span>
<span id="S4.T3.1.1.1.3.1.2.1.2" class="ltx_tr">
<span id="S4.T3.1.1.1.3.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T3.1.1.1.3.1.2.1.2.1.1" class="ltx_text ltx_font_bold">sequence</span></span></span>
</span></span> <span id="S4.T3.1.1.1.3.1.3" class="ltx_text"></span></span></td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;" rowspan="2"><span id="S4.T3.1.1.1.4.1" class="ltx_text"><span id="S4.T3.1.1.1.4.1.1" class="ltx_text"></span> <span id="S4.T3.1.1.1.4.1.2" class="ltx_text">
<span id="S4.T3.1.1.1.4.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T3.1.1.1.4.1.2.1.1" class="ltx_tr">
<span id="S4.T3.1.1.1.4.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T3.1.1.1.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">FLOPs (P)</span></span></span>
</span></span> <span id="S4.T3.1.1.1.4.1.3" class="ltx_text"></span></span></td>
</tr>
<tr id="S4.T3.1.1.2" class="ltx_tr">
<td id="S4.T3.1.1.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T3.1.1.2.1.1" class="ltx_text ltx_font_bold">En-It</span></td>
<td id="S4.T3.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T3.1.1.2.2.1" class="ltx_text ltx_font_bold">En-Fr</span></td>
<td id="S4.T3.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T3.1.1.2.3.1" class="ltx_text ltx_font_bold">En-Pt</span></td>
<td id="S4.T3.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T3.1.1.2.4.1" class="ltx_text ltx_font_bold">En-Es</span></td>
<td id="S4.T3.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T3.1.1.2.5.1" class="ltx_text ltx_font_bold">Avg</span></td>
</tr>
<tr id="S4.T3.1.1.3" class="ltx_tr">
<td id="S4.T3.1.1.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">-</td>
<td id="S4.T3.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">18.4</td>
<td id="S4.T3.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">23.7</td>
<td id="S4.T3.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">22.4</td>
<td id="S4.T3.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">23.6</td>
<td id="S4.T3.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">22.0</td>
<td id="S4.T3.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">1.00</td>
<td id="S4.T3.1.1.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">27.2</td>
</tr>
<tr id="S4.T3.1.1.4" class="ltx_tr">
<td id="S4.T3.1.1.4.1" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S4.T3.1.1.4.1.1" class="ltx_ERROR undefined">\cdashline</span>1-8
2000</td>
<td id="S4.T3.1.1.4.2" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">18.1</td>
<td id="S4.T3.1.1.4.3" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">22.9</td>
<td id="S4.T3.1.1.4.4" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">22.3</td>
<td id="S4.T3.1.1.4.5" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">21.4</td>
<td id="S4.T3.1.1.4.6" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">21.2</td>
<td id="S4.T3.1.1.4.7" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">0.70</td>
<td id="S4.T3.1.1.4.8" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">21.3 (21.7%)</td>
</tr>
<tr id="S4.T3.1.1.5" class="ltx_tr">
<td id="S4.T3.1.1.5.1" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">200</td>
<td id="S4.T3.1.1.5.2" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">20.3</td>
<td id="S4.T3.1.1.5.3" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">22.8</td>
<td id="S4.T3.1.1.5.4" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">23.3</td>
<td id="S4.T3.1.1.5.5" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">22.0</td>
<td id="S4.T3.1.1.5.6" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">22.1</td>
<td id="S4.T3.1.1.5.7" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">0.53</td>
<td id="S4.T3.1.1.5.8" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">17.9 (34.2%)</td>
</tr>
<tr id="S4.T3.1.1.6" class="ltx_tr">
<td id="S4.T3.1.1.6.1" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">50</td>
<td id="S4.T3.1.1.6.2" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">18.5</td>
<td id="S4.T3.1.1.6.3" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">22.2</td>
<td id="S4.T3.1.1.6.4" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">22.9</td>
<td id="S4.T3.1.1.6.5" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">21.5</td>
<td id="S4.T3.1.1.6.6" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">21.3</td>
<td id="S4.T3.1.1.6.7" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">0.45</td>
<td id="S4.T3.1.1.6.8" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">16.1 (40.8%)</td>
</tr>
<tr id="S4.T3.1.1.7" class="ltx_tr">
<td id="S4.T3.1.1.7.1" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td id="S4.T3.1.1.7.2" class="ltx_td" style="padding-top:3pt;padding-bottom:3pt;"></td>
<td id="S4.T3.1.1.7.3" class="ltx_td" style="padding-top:3pt;padding-bottom:3pt;"></td>
<td id="S4.T3.1.1.7.4" class="ltx_td" style="padding-top:3pt;padding-bottom:3pt;"></td>
<td id="S4.T3.1.1.7.5" class="ltx_td" style="padding-top:3pt;padding-bottom:3pt;"></td>
<td id="S4.T3.1.1.7.6" class="ltx_td" style="padding-top:3pt;padding-bottom:3pt;"></td>
<td id="S4.T3.1.1.7.7" class="ltx_td" style="padding-top:3pt;padding-bottom:3pt;"></td>
<td id="S4.T3.1.1.7.8" class="ltx_td" style="padding-top:3pt;padding-bottom:3pt;"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Analysis on computational efficiency with varying number of visual speech unit clusters. When the deduplication strategy is adopted, the proposed method obtains comparable performances with greatly reduced sequence length and training FLOPs.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure">
<p id="S4.F5.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S4.F5.1.1" class="ltx_text"><img src="/html/2402.15151/assets/x5.png" id="S4.F5.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="236" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Visualization results showing how video frame features are deduplicated and mapped into visual speech units. By doing so, the redundant frame features can be reduced efficiently.</figcaption>
</figure>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Effectiveness of Deduplication</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">We conduct experiments to assess the effectiveness of our deduplication strategy. For the deduplication process, the number of clusters for visual speech units is required to be determined, and we show the effectiveness according to the number of clusters. Table <a href="#S4.T3" title="Table 3 ‣ 4.3.2 Effectiveness of Rich Context Modeling ‣ 4.3 Experimental Results ‣ 4 Experiment ‣ Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents these results, and the first row shows the performance of the baseline which does not utilize the deduplication. The baseline obtains an average BLEU score of 22.0 with 27.2 peta FLOPs per training epoch. In contrast, our method acquires comparable performance, while significantly reducing the sequence length and computational resources (FLOPs). Specifically, with 200 clusters for visual speech units, our method not only maintains a similar performance level with a 22.1 average BLEU score but also cuts the sequence length by 53%. Consequently, the FLOPs are greatly reduced to 17.9, marking a 34.2% decrease. These experiments confirm that deduplication, applied to visual speech units, effectively eliminates redundant information.</p>
</div>
<div id="S4.SS3.SSS3.p2" class="ltx_para">
<p id="S4.SS3.SSS3.p2.1" class="ltx_p">Moreover, we delve into the deduplication process by examining it at the video frame level to check whether consecutive visual features, characterized by similar lip movements, are grouped into the same visual speech unit. Figure <a href="#S4.F5" title="Figure 5 ‣ 4.3.2 Effectiveness of Rich Context Modeling ‣ 4.3 Experimental Results ‣ 4 Experiment ‣ Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> provides several visual examples alongside their corresponding phrases and video frames. In Figure
<a href="#S4.F5" title="Figure 5 ‣ 4.3.2 Effectiveness of Rich Context Modeling ‣ 4.3 Experimental Results ‣ 4 Experiment ‣ Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (a), as a speaker articulates “<span id="S4.SS3.SSS3.p2.1.1" class="ltx_text ltx_font_italic">What do you</span>”, it’s noted that 11 video frames can be expressed by 5 visual speech units. For instance, the visual sequences for the sound “wha” belong to the same 43rd unit. Similarly, Figure <a href="#S4.F5" title="Figure 5 ‣ 4.3.2 Effectiveness of Rich Context Modeling ‣ 4.3 Experimental Results ‣ 4 Experiment ‣ Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (c) illustrates that the four frames corresponding to “I” can be efficiently represented by the 46th and 171st visual speech units. Through this analysis, we confirm that visual features with similar lip shapes can be effectively deduplicated, significantly reducing the visual sequence’s length.</p>
</div>
</section>
<section id="S4.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.4 </span>VSP-LLM in Data-limited Situation</h4>

<div id="S4.SS3.SSS4.p1" class="ltx_para">
<p id="S4.SS3.SSS4.p1.1" class="ltx_p">Leveraging the contextual understanding capabilities of LLM, which are pre-trained on vast text corpora, we suppose that a small amount of labeled data is sufficient for constructing a unified VSR and VST model. This is because the proposed VSP-LLM endeavors to establish the visual-to-text mapping while entrusting the task of language modeling to the LLM. To validate it, we train VSP-LLM with different amounts of labeled data; <span id="S4.SS3.SSS4.p1.1.1" class="ltx_text ltx_font_bold">1 hour</span>, <span id="S4.SS3.SSS4.p1.1.2" class="ltx_text ltx_font_bold">5 hours</span>, <span id="S4.SS3.SSS4.p1.1.3" class="ltx_text ltx_font_bold">10 hours</span>, and <span id="S4.SS3.SSS4.p1.1.4" class="ltx_text ltx_font_bold">15 hours</span>. For comparison, we also develop AV-HuBERT <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite> on the same data. Table <a href="#S4.T4" title="Table 4 ‣ 4.3.4 VSP-LLM in Data-limited Situation ‣ 4.3 Experimental Results ‣ 4 Experiment ‣ Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> displays the VSR and VST performances. In all experimental conditions, regardless of the amount of data used, our proposed method significantly outperforms AV-HuBERT. Moreover, when using only 15 hours of labeled data, our unified method achieves a WER of 33.3%. This is a noteworthy achievement, particularly when compared to the previous VSR <cite class="ltx_cite ltx_citemacro_cite">Makino et al. (<a href="#bib.bib33" title="" class="ltx_ref">2019</a>)</cite> model achieving a WER of 33.6%, by using 31k hours of labeled data for training.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.2pt;height:211.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(14.2pt,-6.9pt) scale(1.07010067406792,1.07010067406792) ;">
<table id="S4.T4.2.2" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.2.2.2" class="ltx_tr">
<td id="S4.T4.2.2.2.3" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;" rowspan="2">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
<span id="S4.T4.2.2.2.3.1" class="ltx_text ltx_font_bold">Method</span>
</td>
<td id="S4.T4.2.2.2.4" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;" rowspan="2"><span id="S4.T4.2.2.2.4.1" class="ltx_text"><span id="S4.T4.2.2.2.4.1.1" class="ltx_text"></span> <span id="S4.T4.2.2.2.4.1.2" class="ltx_text">
<span id="S4.T4.2.2.2.4.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T4.2.2.2.4.1.2.1.1" class="ltx_tr">
<span id="S4.T4.2.2.2.4.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T4.2.2.2.4.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Labeled</span></span></span>
<span id="S4.T4.2.2.2.4.1.2.1.2" class="ltx_tr">
<span id="S4.T4.2.2.2.4.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T4.2.2.2.4.1.2.1.2.1.1" class="ltx_text ltx_font_bold">Data(hrs)</span></span></span>
</span></span> <span id="S4.T4.2.2.2.4.1.3" class="ltx_text"></span></span></td>
<td id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;" colspan="5"><span id="S4.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">BLEU <math id="S4.T4.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T4.1.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
<td id="S4.T4.2.2.2.2" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;" rowspan="2"><span id="S4.T4.2.2.2.2.1" class="ltx_text ltx_font_bold">WER(%) <math id="S4.T4.2.2.2.2.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T4.2.2.2.2.1.1.m1.1a"><mo stretchy="false" id="S4.T4.2.2.2.2.1.1.m1.1.1" xref="S4.T4.2.2.2.2.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.2.1.1.m1.1b"><ci id="S4.T4.2.2.2.2.1.1.m1.1.1.cmml" xref="S4.T4.2.2.2.2.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.2.1.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
</tr>
<tr id="S4.T4.2.2.3" class="ltx_tr">
<td id="S4.T4.2.2.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T4.2.2.3.1.1" class="ltx_text ltx_font_bold">En-It</span></td>
<td id="S4.T4.2.2.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T4.2.2.3.2.1" class="ltx_text ltx_font_bold">En-Fr</span></td>
<td id="S4.T4.2.2.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T4.2.2.3.3.1" class="ltx_text ltx_font_bold">En-Pt</span></td>
<td id="S4.T4.2.2.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T4.2.2.3.4.1" class="ltx_text ltx_font_bold">En-Es</span></td>
<td id="S4.T4.2.2.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T4.2.2.3.5.1" class="ltx_text ltx_font_bold">Avg</span></td>
</tr>
<tr id="S4.T4.2.2.4" class="ltx_tr">
<td id="S4.T4.2.2.4.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T4.2.2.4.1.1" class="ltx_text ltx_font_bold">AV-HuBERT</span></td>
<td id="S4.T4.2.2.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">1</td>
<td id="S4.T4.2.2.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">0.1</td>
<td id="S4.T4.2.2.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">0.1</td>
<td id="S4.T4.2.2.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">0.1</td>
<td id="S4.T4.2.2.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">0.1</td>
<td id="S4.T4.2.2.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">0.1</td>
<td id="S4.T4.2.2.4.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;">97.1</td>
</tr>
<tr id="S4.T4.2.2.5" class="ltx_tr">
<td id="S4.T4.2.2.5.1" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T4.2.2.5.1.1" class="ltx_text ltx_font_bold">Proposed Method</span></td>
<td id="S4.T4.2.2.5.2" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">1</td>
<td id="S4.T4.2.2.5.3" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">4.5</td>
<td id="S4.T4.2.2.5.4" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">7.8</td>
<td id="S4.T4.2.2.5.5" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">8.0</td>
<td id="S4.T4.2.2.5.6" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">7.1</td>
<td id="S4.T4.2.2.5.7" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">6.9</td>
<td id="S4.T4.2.2.5.8" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">62.0</td>
</tr>
<tr id="S4.T4.2.2.6" class="ltx_tr">
<td id="S4.T4.2.2.6.1" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S4.T4.2.2.6.1.1" class="ltx_ERROR undefined">\cdashline</span>1-8
<span id="S4.T4.2.2.6.1.2" class="ltx_text ltx_font_bold">AV-HuBERT</span>
</td>
<td id="S4.T4.2.2.6.2" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">5</td>
<td id="S4.T4.2.2.6.3" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">3.2</td>
<td id="S4.T4.2.2.6.4" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">6.2</td>
<td id="S4.T4.2.2.6.5" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">4.2</td>
<td id="S4.T4.2.2.6.6" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">4.4</td>
<td id="S4.T4.2.2.6.7" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">4.5</td>
<td id="S4.T4.2.2.6.8" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">71.7</td>
</tr>
<tr id="S4.T4.2.2.7" class="ltx_tr">
<td id="S4.T4.2.2.7.1" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T4.2.2.7.1.1" class="ltx_text ltx_font_bold">Proposed Method</span></td>
<td id="S4.T4.2.2.7.2" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">5</td>
<td id="S4.T4.2.2.7.3" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">15.4</td>
<td id="S4.T4.2.2.7.4" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">19.4</td>
<td id="S4.T4.2.2.7.5" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">18.6</td>
<td id="S4.T4.2.2.7.6" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">18.6</td>
<td id="S4.T4.2.2.7.7" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">18.0</td>
<td id="S4.T4.2.2.7.8" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">36.6</td>
</tr>
<tr id="S4.T4.2.2.8" class="ltx_tr">
<td id="S4.T4.2.2.8.1" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S4.T4.2.2.8.1.1" class="ltx_ERROR undefined">\cdashline</span>1-8
<span id="S4.T4.2.2.8.1.2" class="ltx_text ltx_font_bold">AV-HuBERT</span>
</td>
<td id="S4.T4.2.2.8.2" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">10</td>
<td id="S4.T4.2.2.8.3" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">5.1</td>
<td id="S4.T4.2.2.8.4" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">8.3</td>
<td id="S4.T4.2.2.8.5" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">8.0</td>
<td id="S4.T4.2.2.8.6" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">6.1</td>
<td id="S4.T4.2.2.8.7" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">6.9</td>
<td id="S4.T4.2.2.8.8" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">59.7</td>
</tr>
<tr id="S4.T4.2.2.9" class="ltx_tr">
<td id="S4.T4.2.2.9.1" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T4.2.2.9.1.1" class="ltx_text ltx_font_bold">Proposed Method</span></td>
<td id="S4.T4.2.2.9.2" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">10</td>
<td id="S4.T4.2.2.9.3" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">20.3</td>
<td id="S4.T4.2.2.9.4" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">22.8</td>
<td id="S4.T4.2.2.9.5" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">23.3</td>
<td id="S4.T4.2.2.9.6" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">21.9</td>
<td id="S4.T4.2.2.9.7" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">22.1</td>
<td id="S4.T4.2.2.9.8" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">34.4</td>
</tr>
<tr id="S4.T4.2.2.10" class="ltx_tr">
<td id="S4.T4.2.2.10.1" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">
<span id="S4.T4.2.2.10.1.1" class="ltx_ERROR undefined">\cdashline</span>1-8
<span id="S4.T4.2.2.10.1.2" class="ltx_text ltx_font_bold">AV-HuBERT</span>
</td>
<td id="S4.T4.2.2.10.2" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">15</td>
<td id="S4.T4.2.2.10.3" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">5.5</td>
<td id="S4.T4.2.2.10.4" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">9.2</td>
<td id="S4.T4.2.2.10.5" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">8.2</td>
<td id="S4.T4.2.2.10.6" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">6.7</td>
<td id="S4.T4.2.2.10.7" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">7.4</td>
<td id="S4.T4.2.2.10.8" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">57.3</td>
</tr>
<tr id="S4.T4.2.2.11" class="ltx_tr">
<td id="S4.T4.2.2.11.1" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T4.2.2.11.1.1" class="ltx_text ltx_font_bold">Proposed Method</span></td>
<td id="S4.T4.2.2.11.2" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">15</td>
<td id="S4.T4.2.2.11.3" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">20.3</td>
<td id="S4.T4.2.2.11.4" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">24.4</td>
<td id="S4.T4.2.2.11.5" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">24.7</td>
<td id="S4.T4.2.2.11.6" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">24.7</td>
<td id="S4.T4.2.2.11.7" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">23.5</td>
<td id="S4.T4.2.2.11.8" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;">33.3</td>
</tr>
<tr id="S4.T4.2.2.12" class="ltx_tr">
<td id="S4.T4.2.2.12.1" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td id="S4.T4.2.2.12.2" class="ltx_td" style="padding-top:3pt;padding-bottom:3pt;"></td>
<td id="S4.T4.2.2.12.3" class="ltx_td" style="padding-top:3pt;padding-bottom:3pt;"></td>
<td id="S4.T4.2.2.12.4" class="ltx_td" style="padding-top:3pt;padding-bottom:3pt;"></td>
<td id="S4.T4.2.2.12.5" class="ltx_td" style="padding-top:3pt;padding-bottom:3pt;"></td>
<td id="S4.T4.2.2.12.6" class="ltx_td" style="padding-top:3pt;padding-bottom:3pt;"></td>
<td id="S4.T4.2.2.12.7" class="ltx_td" style="padding-top:3pt;padding-bottom:3pt;"></td>
<td id="S4.T4.2.2.12.8" class="ltx_td" style="padding-top:3pt;padding-bottom:3pt;"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Impact of the amount of labeled data. It shows that a small amount of labeled data is sufficient to construct a unified VSR and VST model by leveraging contextual understanding capabilities of LLM.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we proposed a novel framework, Visual Speech Processing with LLMs (VSP-LLM), designed to leverage the context modeling ability of LLMs. Through this framework, we built a unified model that can perform multi-tasks, VSR and VST, with a single model. Moreover, the proposed deduplication strategy reduces the redundant information of visual speech representations based on pronunciation information modeled from visual speech units. Through extensive experiments, we verified that the proposed deduplication method can reduce the visual sequence length by about 50% without dropping the performance. In addition, we validated the effectiveness of the VSP-LLM by achieving a superior performance in the MuAViC benchmark with only 15 hours of labeled data.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Afouras et al. (2018)</span>
<span class="ltx_bibblock">
Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. 2018.

</span>
<span class="ltx_bibblock">Lrs3-ted: a large-scale dataset for visual speech recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.00496</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Afouras et al. (2020)</span>
<span class="ltx_bibblock">
Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. 2020.

</span>
<span class="ltx_bibblock">Asr is all you need: Cross-modal distillation for lip reading.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 2143–2147. IEEE.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anwar et al. (2023)</span>
<span class="ltx_bibblock">
Mohamed Anwar, Bowen Shi, Vedanuj Goswami, Wei-Ning Hsu, Juan Pino, and Changhan Wang. 2023.

</span>
<span class="ltx_bibblock">Muavic: A multilingual audio-visual corpus for robust speech recognition and robust speech-to-text translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.00628</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Assael et al. (2016)</span>
<span class="ltx_bibblock">
Yannis M Assael, Brendan Shillingford, Shimon Whiteson, and Nando De Freitas. 2016.

</span>
<span class="ltx_bibblock">Lipnet: End-to-end sentence-level lipreading.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1611.01599</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:1877–1901.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. (2023)</span>
<span class="ltx_bibblock">
Oscar Chang, Hank Liao, Dmitriy Serdyuk, Ankit Shah, and Olivier Siohan. 2023.

</span>
<span class="ltx_bibblock">Conformers are all you need for visual speech recogntion.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.10915</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. (2023)</span>
<span class="ltx_bibblock">
Xize Cheng, Tao Jin, Rongjie Huang, Linjun Li, Wang Lin, Zehan Wang, Ye Wang, Huadai Liu, Aoxiong Yin, and Zhou Zhao. 2023.

</span>
<span class="ltx_bibblock">Mixspeech: Cross-modality self-learning with audio-visual stream mixup for visual speech translation and recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pages 15735–15745.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. (2018)</span>
<span class="ltx_bibblock">
Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. 2018.

</span>
<span class="ltx_bibblock">Voxceleb2: Deep speaker recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.05622</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. (2017)</span>
<span class="ltx_bibblock">
Joon Son Chung, Andrew Senior, Oriol Vinyals, and Andrew Zisserman. 2017.

</span>
<span class="ltx_bibblock">Lip reading sentences in the wild.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>. IEEE.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung and Zisserman (2017a)</span>
<span class="ltx_bibblock">
Joon Son Chung and Andrew Zisserman. 2017a.

</span>
<span class="ltx_bibblock">Lip reading in the wild.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ACCV 2016: 13th Asian Conference on Computer Vision, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part II 13</em>, pages 87–103. Springer.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung and Zisserman (2017b)</span>
<span class="ltx_bibblock">
Joon Son Chung and Andrew Zisserman. 2017b.

</span>
<span class="ltx_bibblock">Lip reading in the wild.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ACCV 2016: 13th Asian Conference on Computer Vision, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part II 13</em>, pages 87–103. Springer.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. (2014)</span>
<span class="ltx_bibblock">
Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. 2014.

</span>
<span class="ltx_bibblock">Empirical evaluation of gated recurrent neural networks on sequence modeling.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">NIPS 2014 Workshop on Deep Learning, December 2014</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2020)</span>
<span class="ltx_bibblock">
Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. 2020.

</span>
<span class="ltx_bibblock">Retinaface: Single-shot multi-level face localisation in the wild.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 5203–5212.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et al. (2023)</span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.

</span>
<span class="ltx_bibblock">Qlora: Efficient finetuning of quantized llms.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.14314</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N19-1423" title="" class="ltx_ref ltx_href">BERT: Pre-training of deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2021)</span>
<span class="ltx_bibblock">
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. 2021.

</span>
<span class="ltx_bibblock">Beyond english-centric multilingual machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 22(107):1–48.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fathullah et al. (2023)</span>
<span class="ltx_bibblock">
Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, et al. 2023.

</span>
<span class="ltx_bibblock">Prompting large language models with speech recognition abilities.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.11795</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves and Graves (2012)</span>
<span class="ltx_bibblock">
Alex Graves and Alex Graves. 2012.

</span>
<span class="ltx_bibblock">Connectionist temporal classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Supervised sequence labelling with recurrent neural networks</em>, pages 61–93.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haliassos et al. (2022)</span>
<span class="ltx_bibblock">
Alexandros Haliassos, Pingchuan Ma, Rodrigo Mira, Stavros Petridis, and Maja Pantic. 2022.

</span>
<span class="ltx_bibblock">Jointly learning visual and auditory speech representations from raw data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. (2023)</span>
<span class="ltx_bibblock">
Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et al. 2023.

</span>
<span class="ltx_bibblock">Imagebind-llm: Multi-modality instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.03905</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 770–778.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hochreiter and Schmidhuber (1997)</span>
<span class="ltx_bibblock">
Sepp Hochreiter and Jürgen Schmidhuber. 1997.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Neural computation</em>, 9(8):1735–1780.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2021)</span>
<span class="ltx_bibblock">
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021.

</span>
<span class="ltx_bibblock">Hubert: Self-supervised speech representation learning by masked prediction of hidden units.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 29:3451–3460.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2021)</span>
<span class="ltx_bibblock">
Minsu Kim, Joanna Hong, Se Jin Park, and Yong Man Ro. 2021.

</span>
<span class="ltx_bibblock">Cromm-vsr: Cross-modal memory augmented visual speech recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Multimedia</em>, 24:4342–4355.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2022)</span>
<span class="ltx_bibblock">
Minsu Kim, Jeong Hun Yeo, and Yong Man Ro. 2022.

</span>
<span class="ltx_bibblock">Distinguishing homophenes using multi-head visual-audio memory for lip reading.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 36, pages 1174–1182.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lakhotia et al. (2021)</span>
<span class="ltx_bibblock">
Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, and Emmanuel Dupoux. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00430" title="" class="ltx_ref ltx_href">On generative spoken language modeling from raw audio</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 9:1336–1354.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2022)</span>
<span class="ltx_bibblock">
Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, et al. 2022.

</span>
<span class="ltx_bibblock">Textless speech-to-speech translation on real data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 860–872.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2023)</span>
<span class="ltx_bibblock">
Pingchuan Ma, Alexandros Haliassos, Adriana Fernandez-Lopez, Honglie Chen, Stavros Petridis, and Maja Pantic. 2023.

</span>
<span class="ltx_bibblock">Auto-avsr: Audio-visual speech recognition with automatic labels.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 1–5. IEEE.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2021a)</span>
<span class="ltx_bibblock">
Pingchuan Ma, Brais Martinez, Stavros Petridis, and Maja Pantic. 2021a.

</span>
<span class="ltx_bibblock">Towards practical lipreading with distilled and efficient models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 7608–7612. IEEE.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2021b)</span>
<span class="ltx_bibblock">
Pingchuan Ma, Stavros Petridis, and Maja Pantic. 2021b.

</span>
<span class="ltx_bibblock">End-to-end audio-visual speech recognition with conformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 7613–7617. IEEE.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2022a)</span>
<span class="ltx_bibblock">
Pingchuan Ma, Stavros Petridis, and Maja Pantic. 2022a.

</span>
<span class="ltx_bibblock">Visual speech recognition for multiple languages in the wild.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Nature Machine Intelligence</em>, 4(11):930–939.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2022b)</span>
<span class="ltx_bibblock">
Pingchuan Ma, Yujiang Wang, Stavros Petridis, Jie Shen, and Maja Pantic. 2022b.

</span>
<span class="ltx_bibblock">Training strategies for improved lip-reading.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 8472–8476. IEEE.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Makino et al. (2019)</span>
<span class="ltx_bibblock">
Takaki Makino, Hank Liao, Yannis Assael, Brendan Shillingford, Basilio Garcia, Otavio Braga, and Olivier Siohan. 2019.

</span>
<span class="ltx_bibblock">Recurrent neural network transducer for audio-visual speech recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">2019 IEEE automatic speech recognition and understanding workshop (ASRU)</em>, pages 905–912. IEEE.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</em>, pages 311–318.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pasad et al. (2023)</span>
<span class="ltx_bibblock">
Ankita Pasad, Bowen Shi, and Karen Livescu. 2023.

</span>
<span class="ltx_bibblock">Comparative layer-wise analysis of self-supervised speech models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 1–5. IEEE.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petridis et al. (2017)</span>
<span class="ltx_bibblock">
Stavros Petridis, Zuwei Li, and Maja Pantic. 2017.

</span>
<span class="ltx_bibblock">End-to-end visual speech recognition with lstms.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>, pages 2592–2596. IEEE.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petridis and Pantic (2016)</span>
<span class="ltx_bibblock">
Stavros Petridis and Maja Pantic. 2016.

</span>
<span class="ltx_bibblock">Deep complementary bottleneck features for visual speech recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 2304–2308. IEEE.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petridis et al. (2018)</span>
<span class="ltx_bibblock">
Stavros Petridis, Themos Stafylakis, Pingehuan Ma, Feipeng Cai, Georgios Tzimiropoulos, and Maja Pantic. 2018.

</span>
<span class="ltx_bibblock">End-to-end audiovisual speech recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>, pages 6548–6552. IEEE.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prajwal et al. (2022)</span>
<span class="ltx_bibblock">
KR Prajwal, Triantafyllos Afouras, and Andrew Zisserman. 2022.

</span>
<span class="ltx_bibblock">Sub-word level lip reading with visual attention.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</em>, pages 5162–5172.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2021)</span>
<span class="ltx_bibblock">
Sucheng Ren, Yong Du, Jianming Lv, Guoqiang Han, and Shengfeng He. 2021.

</span>
<span class="ltx_bibblock">Learning from the master: Distilling cross-modal advanced knowledge for lip reading.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 13325–13333.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rubenstein et al. (2023)</span>
<span class="ltx_bibblock">
Paul K Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al. 2023.

</span>
<span class="ltx_bibblock">Audiopalm: A large language model that can speak and listen.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.12925</em>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Serdyuk et al. (2022)</span>
<span class="ltx_bibblock">
Dmitriy Serdyuk, Otavio Braga, and Olivier Siohan. 2022.

</span>
<span class="ltx_bibblock">Transformer-based video front-ends for audio-visual speech recognition for single and multi-person video.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.10439</em>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2022)</span>
<span class="ltx_bibblock">
Bowen Shi, Wei-Ning Hsu, Kushal Lakhotia, and Abdelrahman Mohamed. 2022.

</span>
<span class="ltx_bibblock">Learning audio-visual speech representation by masked multimodal cluster prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.02184</em>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shillingford et al. (2019)</span>
<span class="ltx_bibblock">
Brendan Shillingford, Yannis M. Assael, Matthew W. Hoffman, Thomas Paine, Cían Hughes, Utsav Prabhu, Hank Liao, Hasim Sak, Kanishka Rao, Lorrayne Bennett, Marie Mulville, Misha Denil, Ben Coppin, Ben Laurie, Andrew W. Senior, and Nando de Freitas. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.21437/INTERSPEECH.2019-1669" title="" class="ltx_ref ltx_href">Large-scale visual speech recognition</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Interspeech 2019, 20th Annual Conference of the International Speech Communication Association, Graz, Austria, 15-19 September 2019</em>, pages 4135–4139. ISCA.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stafylakis and Tzimiropoulos (2017)</span>
<span class="ltx_bibblock">
Themos Stafylakis and Georgios Tzimiropoulos. 2017.

</span>
<span class="ltx_bibblock">Combining residual networks with lstms for lipreading.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Workshop et al. (2022)</span>
<span class="ltx_bibblock">
BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. 2022.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.05100</em>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023a)</span>
<span class="ltx_bibblock">
Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, et al. 2023a.

</span>
<span class="ltx_bibblock">On decoder-only architecture for speech-to-text and large language model integration.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>, pages 1–8. IEEE.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023b)</span>
<span class="ltx_bibblock">
Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2023b.

</span>
<span class="ltx_bibblock">Next-gpt: Any-to-any multimodal llm.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.05519</em>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yeo et al. (2023a)</span>
<span class="ltx_bibblock">
Jeong Hun Yeo, Minsu Kim, Jeongsoo Choi, Dae Hoe Kim, and Yong Man Ro. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.07593" title="" class="ltx_ref ltx_href">Akvsr: Audio knowledge empowered visual speech recognition by compressing audio knowledge of a pretrained model</a>.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yeo et al. (2023b)</span>
<span class="ltx_bibblock">
Jeong Hun Yeo, Minsu Kim, and Yong Man Ro. 2023b.

</span>
<span class="ltx_bibblock">Multi-temporal lip-audio memory for visual speech recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 1–5. IEEE.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022a)</span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022a.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.01068</em>.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022b)</span>
<span class="ltx_bibblock">
Ziqiang Zhang, Long Zhou, Junyi Ao, Shujie Liu, Lirong Dai, Jinyu Li, and Furu Wei. 2022b.

</span>
<span class="ltx_bibblock">Speechut: Bridging speech and text with hidden-unit for encoder-decoder based speech-text pre-training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pages 1663–1676.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2020)</span>
<span class="ltx_bibblock">
Ya Zhao, Rui Xu, Xinchao Wang, Peng Hou, Haihong Tang, and Mingli Song. 2020.

</span>
<span class="ltx_bibblock">Hearing lips: Improving lip reading by distilling speech recognizers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 34, pages 6917–6924.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2023)</span>
<span class="ltx_bibblock">
Qiushi Zhu, Long Zhou, Ziqiang Zhang, Shujie Liu, Binxing Jiao, Jie Zhang, Lirong Dai, Daxin Jiang, Jinyu Li, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock">Vatlm: Visual-audio-text pre-training with unified masked prediction for speech representation learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Multimedia</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Visualization of Visual Speech Units</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">The visualization results of the visual speech units are shown in Figure <a href="#A2.F6" title="Figure 6 ‣ Appendix B Examples of Predicted Sentences ‣ Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. In this paper, we use 200 clusters in order to generate visual speech units. Through analyzing the results, we verify that the video frames assigned the same visual speech unit have similar lip movement.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Examples of Predicted Sentences</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">The examples of recognized and translated transcription by the proposed unified model are shown in Figure <a href="#A2.F7" title="Figure 7 ‣ Appendix B Examples of Predicted Sentences ‣ Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. For generating transcription, we use a single-trained model that performs both VSR and VST tasks.</p>
</div>
<figure id="A2.F6" class="ltx_figure">
<p id="A2.F6.1" class="ltx_p ltx_align_center ltx_align_center"><span id="A2.F6.1.1" class="ltx_text"><img src="/html/2402.15151/assets/x6.png" id="A2.F6.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="247" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Visualization of video frames corresponding to visual speech units. Each number indicates an index of visual speech unit.</figcaption>
</figure>
<figure id="A2.F7" class="ltx_figure">
<p id="A2.F7.1" class="ltx_p ltx_align_center ltx_align_center"><span id="A2.F7.1.1" class="ltx_text"><img src="/html/2402.15151/assets/x7.png" id="A2.F7.1.1.g1" class="ltx_graphics ltx_img_square" width="423" height="411" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Examples of VSR predictions produced by our proposed model on LRS3 test set. Deletions from the ground-truth text are highlighted in <span id="A2.F7.4.1" class="ltx_text" style="color:#E60000;">Red</span>, while substitutions are shown in <span id="A2.F7.5.2" class="ltx_text" style="color:#0033E6;">Blue</span>.</figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2402.15150" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2402.15151" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2402.15151">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.15151" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2402.15152" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 12:54:24 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
