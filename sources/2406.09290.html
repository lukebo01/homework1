<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.09290] Exploring Spoken Language Identification Strategies for Automatic Transcription of Multilingual Broadcast and Institutional Speech</title><meta property="og:description" content="This paper addresses spoken language identification (SLI) and speech recognition of multilingual broadcast and institutional speech, real application scenarios that have been rarely addressed in the SLI literature.
Obs…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Exploring Spoken Language Identification Strategies for Automatic Transcription of Multilingual Broadcast and Institutional Speech">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Exploring Spoken Language Identification Strategies for Automatic Transcription of Multilingual Broadcast and Institutional Speech">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.09290">

<!--Generated on Sat Jul  6 00:46:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=1]MartinaValente
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=1]FabioBrugnara
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=1]GiovanniMorrone
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>[affiliation=1]EnricoZovato
<span id="p1.3.4" class="ltx_ERROR undefined">\name</span>[affiliation=1]LeonardoBadino




</p>
</div>
<h1 class="ltx_title ltx_title_document">Exploring Spoken Language Identification Strategies for Automatic Transcription of Multilingual Broadcast and Institutional Speech</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">This paper addresses spoken language identification (SLI) and speech recognition of multilingual broadcast and institutional speech, real application scenarios that have been rarely addressed in the SLI literature.
Observing that in these domains language changes are mostly associated with speaker changes, we propose a cascaded system consisting of speaker diarization and language identification and compare it with more traditional language identification and language diarization systems.
Results show that the proposed system often achieves lower language classification and language diarization error rates (up to 10% relative language diarization error reduction and 60% relative language confusion reduction) and
leads to lower WERs on multilingual test sets (more than 8% relative WER reduction), while at the same time does not negatively affect speech recognition on monolingual audio (with an absolute WER increase between 0.1% and 0.7% w.r.t. monolingual ASR).</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>spoken language identification, language diarization, speaker diarization, speech recognition
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Spoken Language Identification (SLI) refers to the task of automatically recognizing the language spoken in a given utterance and is an important preprocessing step for various applications, including Automatic Speech Recognition (ASR).</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Most of the SLI systems that have been proposed over the years focus on automatically recognizing the language of short, monolingual audio clips <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, and most publicly available multilingual datasets adhere to this specification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Only few studies have approached the challenge of language identification of authentic multilingual audios, most of which target code-switching speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, which is a type of verbal communication in which multiple languages are alternated within a single sentence. In addition, existing works rarely consider the impact that a SLI front-end might have on subsequent processing stages, such as ASR.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">This paper focuses on SLI and its impact on speech recognition of multilingual audio, specifically targeting real application scenarios of broadcast (i.e., speech from TV and radio channels) and institutional speech. In these particular scenarios, which are rather unexplored in the literature, language changes happen at a relatively long timescale and typically match speaker changes. We propose a speaker-informed approach to SLI that combines speaker change detection and SLI and compare it against speaker-agnostic systems that are by design more suitable for more popular scenarios in the SLI literature (e.g. segment-based language classification or code-switching). Other than a simple addition of information, speaker change detection might represent a more accurate proxy for language change detection, given that speaker information requires less context to be extracted with respect to language information. Please note that the proposed technique is not intended to address code-switching by the same speaker.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Stemming from the NeMo Titanet-LID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> trained on VoxLingua107 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, we train two different SLI architectures: one that assigns language labels to segments identified by previous processing blocks (segment-based) and another one that predicts language labels at fixed time steps (frame-based).</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">For the first model, we re-use the same architecture as the pretrained model, and fine-tune on domain-specific data to improve the SLI performance on the domain and languages of interest. At inference time, the model classifies the segments identified by either a voice activity detection (VAD) or a speaker diarization (SD) system.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">For the second model, we extend and finetune the pretrained model architecture to produce frame-based language predictions. We aim at performing both language segmentation and identification from the raw audio, a task known in the code-switching literature as Language Diarization (LD)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Recent works have shown that end-to-end LD systems that combine local language information extraction with a contextual language classifier are able to outperform more traditional systems based on two-step processing of embedding extraction and classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Additionally, the use of pretrained models as feature extractors do increase the identification performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Inspired by this type of considerations, we develop an end-to-end system for language diarization that combines a pretrained convolutional front-end for the extraction of local information with a back-end based on the Long Short-Term Memory (LSTM) topology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> to take into account the contextual information.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">We report extensive evaluations of the different language identification systems on test sets from broadcast and institutional domains and compare our results with a baseline system composed by publicly available VAD and SLI provided by SpeechBrain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
In addition, we assess the impact of language identification front-end on the transcription accuracy of a multilingual system composed by SLI followed by monolingual speech recognition engines.
We further address the question of whether a SLI is worthwhile even when the input signal is mostly monolingual: this is a relevant question especially in broadcast speech where multilingual speech is present but most of the audio is monolingual.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>System Architecture</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We test different combinations of segmentation and language identification models.
More specifically, we compare the performance of three different cascaded systems:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Voice Activity Detection followed by segment-based SLI.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Speaker Diarization followed by segment-based SLI.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">Voice Activity Detection followed by frame-based SLI.</p>
</div>
</li>
</ul>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In addition, we combine the language identification systems with monolingual ASR engines to build multilingual speech recognition systems, and test the impact of such a configuration on transcription accuracy.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In the following, we provide the implementation details of each of the components mentioned above.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Segment-based and frame-based SLI models</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The segment-based SLI model is a fine-tuned version of the TitaNet-LID<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/langid_ambernet" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/langid_ambernet</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> trained on VoxLingua107 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The TitaNet-LID architecture consists of a 1D depth-wise channel separable convolutional encoder with ContextNet-like architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and a decoder with a statistic pooling
layer followed by two linear layers. This model shows state-of-the-art results on the VoxLingua107 test set while being 10 times smaller (29M parameters) than competitor models.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">For the frame-based SLI model, the TitaNet-LID architecture has been adapted to produce frame-based predictions, by retaining the encoder and replacing the decoder with a bi-LSTM decoder.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Speaker Diarization Model</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">We employ an improved version of the end-to-end neural diarization (EEND)-vector clustering model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, a hybrid diarization approach that combines neural and clustering-based diarization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. The input recording is split into short chunks (e.g., 30 seconds), for which we can assume that a maximum number of active speakers can occur (e.g., 2 or 3). Then, local neural diarization is applied on each chunk independently and outputs speech activity probabilities for each active speaker. For local diarization we use the same multi-layer Transformer-based architecture as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. In the original EEND-VC implementation, the same network is exploited to compute speaker embeddings for each active speaker along with diarization results.
Finally, a constrained clustering algorithm is applied on speaker embeddings to estimate which diarization outputs across chunks belong to the same speaker.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">We found that the original EEND-VC performed poorly on broadcast audio, suffering the presence of many speakers (e.g., <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mo id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><geq id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">\geq</annotation></semantics></math> 5) in long input recordings. In such a case, speaker embeddings quality is not good enough to assure decent performance for the following clustering step, thus we employ an external pre-trained embedding extractor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
This variant of EEND-VC produced a diarization error rate reduction of 49.3% and 55.9% on VoxConverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> dev and test sets, respectively.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">In the proposed SLI systems, the SD model is only used to detect speaker changes. Some preliminary experiments using speaker identity to assign language labels led to worse results, but we will explore this aspect more thoroughly in future experiments.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Voice Activity Detection Model</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">To perform a fair comparison, the VAD model is derived from the local neural diarization model. We discard speaker information and only retain speech/non-speech segmentation. The overall VAD results is simply obtained by stitching the local VAD outputs without any clustering.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Broadcast Datasets</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">The broadcast training dataset is composed of 300 hours of audio extracted from TV, radio channels and YouTube.
It covers 3 languages: Danish, Swedish and English, with approximately 100 hours each. Three different varieties of English were included: British (30 hours), American (30 hours), and L2 English (40 hours). For SLI model training, the audio was split into speech segments of duration between 1 and 20 seconds, and 5% of the data was used as validation set.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">A monolingual test set with approximately 1 hour of material per language was extracted from similar sources as the training and for the same set of languages. The test set was manually reviewed to exclude any foreign language material and transcribed. No annotation of speech/non speech was provided.</p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p">A synthetic multilingual test sets was built by extracting monolingual segments of random length from the monolingual test set and concatenating them in random order. Danish, Swedish and English were sampled with uniform probability. We created two versions of the test: <em id="S3.SS1.SSS1.p3.1.1" class="ltx_emph ltx_font_italic">ts-mix-short</em>, with segment duration from 5 to 15 seconds, and <em id="S3.SS1.SSS1.p3.1.2" class="ltx_emph ltx_font_italic">ts-mix-long</em> with segments from 15 to 45 seconds, and with two different concatenation modalities: with a gap of one second or without any gap. For each version and concatenation modality, 60 files of around 1 minute were created, resulting in one hour of audio per each version.</p>
</div>
<div id="S3.SS1.SSS1.p4" class="ltx_para">
<p id="S3.SS1.SSS1.p4.1" class="ltx_p">The real multilingual test set is a proprietary test set containing extracts from multilingual Swedish TV programs.
In Scandinavian broadcasting, it is common to find programs that are primarily in one language, e.g. Swedish, featuring interviews or reports in a different language, e.g. English. We collected around 1 hour of material, which was manually annotated with both language labels and verbatim transcripts, for a total of around 40 minutes of Swedish, 10 minutes of English and 1 minute of Danish. The test set counts 76 language changes with monolingual segments length ranging from 0.83 to 362.04 seconds (average: 44.31s).</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Institutional Datasets</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">The institutional dataset amounts to approximately 1000 hours of material collected from 2008-2020 plenary sessions of the European Parliament<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://www.europarl.europa.eu/plenary/en/debates-video.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.europarl.europa.eu/plenary/en/debates-video.html</a></span></span></span>. Sessions consist of interventions from multiple speakers in different European languages.
The transcripts include information about the language of the intervention. We select the 10 most represented languages in the dataset (i.e., English, German, Italian, French, Polish, Spanish, Romanian, Dutch, Greek and Portuguese).
Around 100 hours per language were used for SLI model training, and audio was split into speech segments of duration between 1 and 20 seconds. 3% of data was used as validation set.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">The monolingual test set consists of approximately 10 hours taken from late 2020 plenary sessions of the European Parliament, excluded from the training material. Language coverage matches that of the training set.
The test set was manually reviewed to exclude non-target language content and to adjust the official transcripts to verbatim standard.</p>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.1" class="ltx_p">The multilingual test set
contains the 10 target languages only. It counts 32 variable-length recordings, containing from 1 to 6 languages each, for an overall 2.6 hours.
Language segmentation and transcription boundaries were manually adjusted. Transcripts were taken from the official source, and are not verbatim. The test set is publicly available <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://awdelivery.blob.core.windows.net/spoken-language-identification/tst_is24.zip" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://awdelivery.blob.core.windows.net/spoken-language-identification/tst_is24.zip</a></span></span></span>.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Speaker Diarization Datasets</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">Since large broadcast or institutional datasets with diarization labels are not available, we used the <em id="S3.SS1.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">Fisher Corpus Part 1</em> and <em id="S3.SS1.SSS3.p1.1.2" class="ltx_emph ltx_font_italic">Part 2</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> to train the local diarization network.
After removing problematic annotations, the final training set resulted in 10653 examples, totaling about 1762 hours of English speech sampled at 8 kHz.
We augmented the dataset following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and using
noises from MUSAN corpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, and artificial reverberation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Through augmentation we generated 4 subsets, each containing 50000 diarization-style simulated mixtures, totaling 8089 hours of speech. In each subset the number of speakers involved is fixed (i.e., from 1 to 4).</p>
</div>
<div id="S3.SS1.SSS3.p2" class="ltx_para">
<p id="S3.SS1.SSS3.p2.1" class="ltx_p">For validation and hyperparameters tuning, we used the development set of the VoxConverse dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, which includes broadcast recordings and covers acoustic and linguistic domains very similar to the ones covered in the present work. In particular, the number of speakers involved can range from 1 to 20.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Architecture, Training and Inference Details</h3>

<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Segment-based and frame-based SLI</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">To train SLI models, we used the NeMo toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. We followed the original implementation for feature extraction. For both models, the encoder weights were initialized from the pretrained model. For segment based SLI, the decoder outputs were set to 3 or 10 depending on the experimental domain. Fine-tuning was performed with a learning rate of 5e-5 for 3 epochs and the best checkpoint in terms of validation accuracy was selected. For the frame-based SLI, the decoder was replaced with a 1-layer, 1024 nodes bidirectional LSTM. We tried other configurations, e.g. adding more LSTM layers or removing the squeeze-and-excitation mechanism from the encoder, but these produced slightly worse results. In order to train the model to perform language segmentation, artificial multilingual samples were created on-the-fly by concatenating monolingual samples of length <math id="S3.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mo id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><lt id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">&lt;</annotation></semantics></math> 10s. The models were trained with a learning rate of 5e-6 for 10 epochs on a Quadro RTX 8000 GPU, and the best checkpoint in terms of validation accuracy was selected.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">At inference time, segments shorter than 1s were discarded and segments longer than 20s were split to better match the training conditions. For the frame-based SLI model, we applied a moving average to smooth posterior probabilities, with a window size of 200 frames (i.e., 2 seconds).</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Speaker Diarization</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Our SD system is built on the official implementation<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/nttcslab-sp/EEND-vector-clustering" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/nttcslab-sp/EEND-vector-clustering</a></span></span></span> of the EEND-vector clustering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> model. The local diarization neural network consists of self-attentive 6-layer Transformer encoder with 8 attention heads. We basically followed the original paper for feature extraction, network architecture and training protocol with the following exception: the model was trained for 200 epochs on 30 s long chunks of the augmented Fisher training set. The model parameters of the last 10 epochs were averaged to obtain the final model.
We replaced the speaker embedding estimation layer with an external pretrained speaker embedding extractor available in the Wespeaker framework<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/wenet-e2e/wespeaker/tree/master/examples/voxceleb/v2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/wenet-e2e/wespeaker/tree/master/examples/voxceleb/v2</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. In particular, we used the 34-layer ResNet based <em id="S3.SS2.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">"r-vector"</em> architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> trained on VoxCeleb2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. As clustering algorithm we employed the constrained Agglomerative Hierarchical Clustering (AHC) as it reaches the best performance in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
We tuned the speech activity threshold and the AHC linkage threshold on the VoxConverse dev set.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Evaluation metrics and baselines</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">For the evaluation of SLI systems, we employ two different performance metrics:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Language Diarization Error Rate (LDER), as defined in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. It is computed as the sum of language confusion (LC), false alarms (FA) and missed speech (MS) normalized by the total audio duration. It takes into account the system-level language identification and segmentation performance. We used this metric for datasets for which we have both language identity and speech/non-speech annotations.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Language Error Rate (LER), here defined as the ratio of time with incorrect language label to total time recognized as speech. It accounts for language confusion error only. We used this metric for datasets where speech/nonspeech annotation is not available.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">We compute 95% confidence intervals on the LDER measure with bootstrapping approach using the ConfidenceIntervals Python toolkit<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://github.com/luferrer/ConfidenceIntervals" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/luferrer/ConfidenceIntervals</a></span></span></span> applied to frame-level labels.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">We compare our results with two baselines: the first one was obtained by applying SpeechBrain VAD<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://huggingface.co/speechbrain/vad-crdnn-libriparty" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/speechbrain/vad-crdnn-libriparty</a></span></span></span> followed by SpeechBrain ECAPA-TDNN SLI<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="https://huggingface.co/speechbrain/lang-id-voxlingua107-ecapahttps://huggingface.co/speechbrain/lang-id-voxlingua107-ecapa" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/speechbrain/lang-id-voxlingua107-ecapahttps://huggingface.co/speechbrain/lang-id-voxlingua107-ecapa</a></span></span></span> trained on VoxLingua107 dataset; the second one is the NeMo TitaNet-LID model, that was used to initialize our systems, combined with our in-house VAD (cfr. Section <a href="#S2.SS3" title="2.3 Voice Activity Detection Model ‣ 2 System Architecture ‣ Exploring Spoken Language Identification Strategies for Automatic Transcription of Multilingual Broadcast and Institutional Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>). To ensure a fair comparison, we limit both the SpeechBrain and NeMo SLI outputs during inference to the relevant classes by applying a binary mask on the networks' output nodes. For SpeechBrain VAD, we use the default configuration for all parameters.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Monolingual Speech</h3>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Average LER and WER on monolingual test sets.</figcaption>
<table id="S4.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.3.1.1" class="ltx_tr">
<th id="S4.T1.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T1.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Method</span></th>
<th id="S4.T1.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T1.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Broadcast</span></th>
<th id="S4.T1.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T1.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Institutional</span></th>
</tr>
<tr id="S4.T1.3.2.2" class="ltx_tr">
<th id="S4.T1.3.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T1.3.2.2.1.1" class="ltx_text" style="font-size:70%;">LER</span></th>
<th id="S4.T1.3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T1.3.2.2.2.1" class="ltx_text" style="font-size:70%;">WER</span></th>
<th id="S4.T1.3.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T1.3.2.2.3.1" class="ltx_text" style="font-size:70%;">LER</span></th>
<th id="S4.T1.3.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T1.3.2.2.4.1" class="ltx_text" style="font-size:70%;">WER</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.3.3.1" class="ltx_tr">
<th id="S4.T1.3.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.3.3.1.1.1" class="ltx_text" style="font-size:70%;">ECAPA-TDNN 107L</span></th>
<td id="S4.T1.3.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.3.1.2.1" class="ltx_text" style="font-size:70%;">17.08</span></td>
<td id="S4.T1.3.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.3.1.3.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T1.3.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.3.1.4.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T1.3.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.3.1.5.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S4.T1.3.4.2" class="ltx_tr">
<th id="S4.T1.3.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.3.4.2.1.1" class="ltx_text" style="font-size:70%;">ECAPA-TDNN 3L</span></th>
<td id="S4.T1.3.4.2.2" class="ltx_td ltx_align_center"><span id="S4.T1.3.4.2.2.1" class="ltx_text" style="font-size:70%;">1.47</span></td>
<td id="S4.T1.3.4.2.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.4.2.3.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T1.3.4.2.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.4.2.4.1" class="ltx_text" style="font-size:70%;">1.99</span></td>
<td id="S4.T1.3.4.2.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.4.2.5.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S4.T1.3.5.3" class="ltx_tr">
<th id="S4.T1.3.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.3.5.3.1.1" class="ltx_text" style="font-size:70%;">Monolingual ASR</span></th>
<td id="S4.T1.3.5.3.2" class="ltx_td ltx_align_center"><span id="S4.T1.3.5.3.2.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T1.3.5.3.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.5.3.3.1" class="ltx_text" style="font-size:70%;">10.96</span></td>
<td id="S4.T1.3.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.5.3.4.1" class="ltx_text" style="font-size:70%;">-</span></td>
<td id="S4.T1.3.5.3.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.5.3.5.1" class="ltx_text" style="font-size:70%;">13.23</span></td>
</tr>
<tr id="S4.T1.3.6.4" class="ltx_tr">
<th id="S4.T1.3.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.3.6.4.1.1" class="ltx_text" style="font-size:70%;">TitaNet-LID 3L</span></th>
<td id="S4.T1.3.6.4.2" class="ltx_td ltx_align_center"><span id="S4.T1.3.6.4.2.1" class="ltx_text" style="font-size:70%;">0.75</span></td>
<td id="S4.T1.3.6.4.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.6.4.3.1" class="ltx_text" style="font-size:70%;">11.42</span></td>
<td id="S4.T1.3.6.4.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.6.4.4.1" class="ltx_text" style="font-size:70%;">2.39</span></td>
<td id="S4.T1.3.6.4.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.6.4.5.1" class="ltx_text" style="font-size:70%;">15.06</span></td>
</tr>
<tr id="S4.T1.3.7.5" class="ltx_tr">
<th id="S4.T1.3.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.3.7.5.1.1" class="ltx_text" style="font-size:70%;">Seg-SLI + VAD</span></th>
<td id="S4.T1.3.7.5.2" class="ltx_td ltx_align_center"><span id="S4.T1.3.7.5.2.1" class="ltx_text" style="font-size:70%;">0.21</span></td>
<td id="S4.T1.3.7.5.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.7.5.3.1" class="ltx_text" style="font-size:70%;">11.02</span></td>
<td id="S4.T1.3.7.5.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.7.5.4.1" class="ltx_text" style="font-size:70%;">0.21</span></td>
<td id="S4.T1.3.7.5.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.7.5.5.1" class="ltx_text" style="font-size:70%;">13.26</span></td>
</tr>
<tr id="S4.T1.3.8.6" class="ltx_tr">
<th id="S4.T1.3.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.3.8.6.1.1" class="ltx_text" style="font-size:70%;">Seg-SLI + SD</span></th>
<td id="S4.T1.3.8.6.2" class="ltx_td ltx_align_center"><span id="S4.T1.3.8.6.2.1" class="ltx_text" style="font-size:70%;">0.37</span></td>
<td id="S4.T1.3.8.6.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.8.6.3.1" class="ltx_text" style="font-size:70%;">11.69</span></td>
<td id="S4.T1.3.8.6.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.8.6.4.1" class="ltx_text" style="font-size:70%;">0.25</span></td>
<td id="S4.T1.3.8.6.5" class="ltx_td ltx_align_center"><span id="S4.T1.3.8.6.5.1" class="ltx_text" style="font-size:70%;">13.35</span></td>
</tr>
<tr id="S4.T1.3.9.7" class="ltx_tr">
<th id="S4.T1.3.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T1.3.9.7.1.1" class="ltx_text" style="font-size:70%;">Frm-SLI + VAD</span></th>
<td id="S4.T1.3.9.7.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.3.9.7.2.1" class="ltx_text" style="font-size:70%;">0.26</span></td>
<td id="S4.T1.3.9.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.3.9.7.3.1" class="ltx_text" style="font-size:70%;">11.06</span></td>
<td id="S4.T1.3.9.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.3.9.7.4.1" class="ltx_text" style="font-size:70%;">0.35</span></td>
<td id="S4.T1.3.9.7.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.3.9.7.5.1" class="ltx_text" style="font-size:70%;">13.40</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We use the monolingual test sets to evaluate identification accuracy of the different SLI systems on domain-specific monolingual speech. A very good performance on monolingual data is crucial for applications where language changes are present but monolingual speech is predominant. For what concerns the broadcast data, we observe a decrease in performance for systems trained on VoxLingua when applied to L2 English. This loss is recovered with fine-tuning on the broadcast data, with all in-house systems having LER below 1%. We observe a similar pattern for the institutional domain, with English (mostly L2 in this case) being the most critical language for the pretrained models, but not for the fine-tuned ones. We further explore the impact on transcription accuracy of the multilingual system composed by SLI and ASR, observing that monolingual ASR performance is only slightly degraded, thus proving the multilingual system to be a valid candidate for our use case.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Multilingual Synthetic Speech</h3>

<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>LER of SLI systems on synthetic multilingual test set.</figcaption>
<table id="S4.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.3.1.1" class="ltx_tr">
<th id="S4.T2.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T2.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Method</span></th>
<th id="S4.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T2.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">ts-mix-short</span></th>
<th id="S4.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T2.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">ts-mix-long</span></th>
</tr>
<tr id="S4.T2.3.2.2" class="ltx_tr">
<th id="S4.T2.3.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.3.2.2.1.1" class="ltx_text" style="font-size:70%;">1s gap</span></th>
<th id="S4.T2.3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.3.2.2.2.1" class="ltx_text" style="font-size:70%;">no gap</span></th>
<th id="S4.T2.3.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.3.2.2.3.1" class="ltx_text" style="font-size:70%;">1s gap</span></th>
<th id="S4.T2.3.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.3.2.2.4.1" class="ltx_text" style="font-size:70%;">no gap</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.3.3.1" class="ltx_tr">
<th id="S4.T2.3.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T2.3.3.1.1.1" class="ltx_text" style="font-size:70%;">ECAPA-TDNN 3L</span></th>
<td id="S4.T2.3.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.3.1.2.1" class="ltx_text" style="font-size:70%;">36.91</span></td>
<td id="S4.T2.3.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.3.1.3.1" class="ltx_text" style="font-size:70%;">43.41</span></td>
<td id="S4.T2.3.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.3.1.4.1" class="ltx_text" style="font-size:70%;">5.88</span></td>
<td id="S4.T2.3.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.3.1.5.1" class="ltx_text" style="font-size:70%;">6.57</span></td>
</tr>
<tr id="S4.T2.3.4.2" class="ltx_tr">
<th id="S4.T2.3.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.3.4.2.1.1" class="ltx_text" style="font-size:70%;">TitaNet-LID 3L</span></th>
<td id="S4.T2.3.4.2.2" class="ltx_td ltx_align_center"><span id="S4.T2.3.4.2.2.1" class="ltx_text" style="font-size:70%;">1.69</span></td>
<td id="S4.T2.3.4.2.3" class="ltx_td ltx_align_center"><span id="S4.T2.3.4.2.3.1" class="ltx_text" style="font-size:70%;">36.90</span></td>
<td id="S4.T2.3.4.2.4" class="ltx_td ltx_align_center"><span id="S4.T2.3.4.2.4.1" class="ltx_text" style="font-size:70%;">0.20</span></td>
<td id="S4.T2.3.4.2.5" class="ltx_td ltx_align_center"><span id="S4.T2.3.4.2.5.1" class="ltx_text" style="font-size:70%;">5.72</span></td>
</tr>
<tr id="S4.T2.3.5.3" class="ltx_tr">
<th id="S4.T2.3.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.3.5.3.1.1" class="ltx_text" style="font-size:70%;">Seg-SLI + VAD</span></th>
<td id="S4.T2.3.5.3.2" class="ltx_td ltx_align_center"><span id="S4.T2.3.5.3.2.1" class="ltx_text" style="font-size:70%;">1.56</span></td>
<td id="S4.T2.3.5.3.3" class="ltx_td ltx_align_center"><span id="S4.T2.3.5.3.3.1" class="ltx_text" style="font-size:70%;">37.15</span></td>
<td id="S4.T2.3.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T2.3.5.3.4.1" class="ltx_text" style="font-size:70%;">0.21</span></td>
<td id="S4.T2.3.5.3.5" class="ltx_td ltx_align_center"><span id="S4.T2.3.5.3.5.1" class="ltx_text" style="font-size:70%;">5.33</span></td>
</tr>
<tr id="S4.T2.3.6.4" class="ltx_tr">
<th id="S4.T2.3.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.3.6.4.1.1" class="ltx_text" style="font-size:70%;">Seg-SLI + SD</span></th>
<td id="S4.T2.3.6.4.2" class="ltx_td ltx_align_center"><span id="S4.T2.3.6.4.2.1" class="ltx_text" style="font-size:70%;">1.91</span></td>
<td id="S4.T2.3.6.4.3" class="ltx_td ltx_align_center"><span id="S4.T2.3.6.4.3.1" class="ltx_text" style="font-size:70%;">13.59</span></td>
<td id="S4.T2.3.6.4.4" class="ltx_td ltx_align_center"><span id="S4.T2.3.6.4.4.1" class="ltx_text" style="font-size:70%;">0.47</span></td>
<td id="S4.T2.3.6.4.5" class="ltx_td ltx_align_center"><span id="S4.T2.3.6.4.5.1" class="ltx_text" style="font-size:70%;">1.12</span></td>
</tr>
<tr id="S4.T2.3.7.5" class="ltx_tr">
<th id="S4.T2.3.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T2.3.7.5.1.1" class="ltx_text" style="font-size:70%;">Frm-SLI + VAD</span></th>
<td id="S4.T2.3.7.5.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.7.5.2.1" class="ltx_text" style="font-size:70%;">0.73</span></td>
<td id="S4.T2.3.7.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.7.5.3.1" class="ltx_text" style="font-size:70%;">4.57</span></td>
<td id="S4.T2.3.7.5.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.7.5.4.1" class="ltx_text" style="font-size:70%;">0.08</span></td>
<td id="S4.T2.3.7.5.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.7.5.5.1" class="ltx_text" style="font-size:70%;">0.51</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">On the multilingual synthetic datasets, the ECAPA-TDNN model has the highest LER on both short and long segments versions. This is mostly a consequence of the behaviour of the SpeechBrain VAD, that tends to create long segments containing multiple languages, that necessarily result in larger language confusion given the segment-based nature of the ECAPA-TDNN. We observe a similar performance for the TitaNet-LID and our segment-based model, when applied on VAD-extracted segments. In this case, the major impact on performance happens when there is no pause at the language change ("no gap" columns), because the VAD lacks cues to split the audio. This shortcoming was greatly reduced when using SD-based segmentation, because in this artificial scenario, similarly to our real use cases, language changes are associated to speaker changes. The best performing system on this synthetic testset is the frame-base SLI, that, given the same VAD, reduces the segment-based system error from 37.15% to 4.57% on the short test set and from 5.33% to 0.51% on the long test set. However, we notice that the test conditions (i.e. short segments of different languages stitched one after the other) are very close to the training conditions for the frame-based SLI, thus this system might be implicitly advantaged.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Real Multilingual Speech</h3>

<figure id="S4.T3" class="ltx_table">

<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>LDER and WER of SLI systems on multilingual test sets. 95% confidence intervals are within squared brackets. Only WERs of fully integrated SLI+ASR systems are reported.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T3.3" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.3.1.1" class="ltx_tr">
<th id="S4.T3.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" colspan="6"><span id="S4.T3.3.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:70%;">(a) Broadcast test set.</span></th>
</tr>
<tr id="S4.T3.3.2.2" class="ltx_tr">
<th id="S4.T3.3.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T3.3.2.2.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Method</span></th>
<th id="S4.T3.3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.3.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">LDER</span></th>
<th id="S4.T3.3.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.3.2.2.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">LC</span></th>
<th id="S4.T3.3.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.3.2.2.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">MS</span></th>
<th id="S4.T3.3.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.3.2.2.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">FA</span></th>
<th id="S4.T3.3.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.3.2.2.6.1" class="ltx_text ltx_font_bold" style="font-size:70%;">WER</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.3.3.1" class="ltx_tr">
<th id="S4.T3.3.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.3.3.1.1.1" class="ltx_text" style="font-size:70%;">ECAPA-TDNN 3L</span></th>
<td id="S4.T3.3.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.3.1.2.1" class="ltx_text" style="font-size:70%;">12.13 [12.11-12.15]</span></td>
<td id="S4.T3.3.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.3.1.3.1" class="ltx_text" style="font-size:70%;">5.91</span></td>
<td id="S4.T3.3.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.3.1.4.1" class="ltx_text" style="font-size:70%;">1.26</span></td>
<td id="S4.T3.3.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.3.1.5.1" class="ltx_text" style="font-size:70%;">4.96</span></td>
<td id="S4.T3.3.3.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.3.1.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S4.T3.3.4.2" class="ltx_tr">
<th id="S4.T3.3.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.3.4.2.1.1" class="ltx_text" style="font-size:70%;">TitaNet-LID 3L</span></th>
<td id="S4.T3.3.4.2.2" class="ltx_td ltx_align_center"><span id="S4.T3.3.4.2.2.1" class="ltx_text" style="font-size:70%;">14.09 [14.06-14.11]</span></td>
<td id="S4.T3.3.4.2.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.4.2.3.1" class="ltx_text" style="font-size:70%;">4.98</span></td>
<td id="S4.T3.3.4.2.4" class="ltx_td ltx_align_center"><span id="S4.T3.3.4.2.4.1" class="ltx_text" style="font-size:70%;">1.10</span></td>
<td id="S4.T3.3.4.2.5" class="ltx_td ltx_align_center"><span id="S4.T3.3.4.2.5.1" class="ltx_text" style="font-size:70%;">8.01</span></td>
<td id="S4.T3.3.4.2.6" class="ltx_td ltx_align_center"><span id="S4.T3.3.4.2.6.1" class="ltx_text" style="font-size:70%;">23.20</span></td>
</tr>
<tr id="S4.T3.3.5.3" class="ltx_tr">
<th id="S4.T3.3.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.3.5.3.1.1" class="ltx_text" style="font-size:70%;">Seg-SLI + VAD</span></th>
<td id="S4.T3.3.5.3.2" class="ltx_td ltx_align_center"><span id="S4.T3.3.5.3.2.1" class="ltx_text" style="font-size:70%;">13.83 [13.80-13.85]</span></td>
<td id="S4.T3.3.5.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.5.3.3.1" class="ltx_text" style="font-size:70%;">4.72</span></td>
<td id="S4.T3.3.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T3.3.5.3.4.1" class="ltx_text" style="font-size:70%;">1.10</span></td>
<td id="S4.T3.3.5.3.5" class="ltx_td ltx_align_center"><span id="S4.T3.3.5.3.5.1" class="ltx_text" style="font-size:70%;">8.01</span></td>
<td id="S4.T3.3.5.3.6" class="ltx_td ltx_align_center"><span id="S4.T3.3.5.3.6.1" class="ltx_text" style="font-size:70%;">23.09</span></td>
</tr>
<tr id="S4.T3.3.6.4" class="ltx_tr">
<th id="S4.T3.3.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.3.6.4.1.1" class="ltx_text" style="font-size:70%;">Seg-SLI + SD</span></th>
<td id="S4.T3.3.6.4.2" class="ltx_td ltx_align_center"><span id="S4.T3.3.6.4.2.1" class="ltx_text" style="font-size:70%;">10.87 [10.84-10.89]</span></td>
<td id="S4.T3.3.6.4.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.6.4.3.1" class="ltx_text" style="font-size:70%;">1.57</span></td>
<td id="S4.T3.3.6.4.4" class="ltx_td ltx_align_center"><span id="S4.T3.3.6.4.4.1" class="ltx_text" style="font-size:70%;">1.43</span></td>
<td id="S4.T3.3.6.4.5" class="ltx_td ltx_align_center"><span id="S4.T3.3.6.4.5.1" class="ltx_text" style="font-size:70%;">7.86</span></td>
<td id="S4.T3.3.6.4.6" class="ltx_td ltx_align_center"><span id="S4.T3.3.6.4.6.1" class="ltx_text" style="font-size:70%;">21.19</span></td>
</tr>
<tr id="S4.T3.3.7.5" class="ltx_tr">
<th id="S4.T3.3.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T3.3.7.5.1.1" class="ltx_text" style="font-size:70%;">Frm-SLI + VAD</span></th>
<td id="S4.T3.3.7.5.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.3.7.5.2.1" class="ltx_text" style="font-size:70%;">13.15 [13.13-13.17]</span></td>
<td id="S4.T3.3.7.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.3.7.5.3.1" class="ltx_text" style="font-size:70%;">4.13</span></td>
<td id="S4.T3.3.7.5.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.3.7.5.4.1" class="ltx_text" style="font-size:70%;">1.00</span></td>
<td id="S4.T3.3.7.5.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.3.7.5.5.1" class="ltx_text" style="font-size:70%;">8.02</span></td>
<td id="S4.T3.3.7.5.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.3.7.5.6.1" class="ltx_text" style="font-size:70%;">23.12</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T3.4" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.4.1.1" class="ltx_tr">
<th id="S4.T3.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" colspan="6"><span id="S4.T3.4.1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:70%;">(b) Institutional test set.</span></th>
</tr>
<tr id="S4.T3.4.2.2" class="ltx_tr">
<th id="S4.T3.4.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T3.4.2.2.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Method</span></th>
<th id="S4.T3.4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.4.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">LDER</span></th>
<th id="S4.T3.4.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.4.2.2.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">LC</span></th>
<th id="S4.T3.4.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.4.2.2.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">MS</span></th>
<th id="S4.T3.4.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.4.2.2.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">FA</span></th>
<th id="S4.T3.4.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.4.2.2.6.1" class="ltx_text ltx_font_bold" style="font-size:70%;">WER</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.4.3.1" class="ltx_tr">
<th id="S4.T3.4.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.4.3.1.1.1" class="ltx_text" style="font-size:70%;">ECAPA-TDNN 3L</span></th>
<td id="S4.T3.4.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.3.1.2.1" class="ltx_text" style="font-size:70%;">13.40 [13.32-13.47]</span></td>
<td id="S4.T3.4.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.3.1.3.1" class="ltx_text" style="font-size:70%;">7.46</span></td>
<td id="S4.T3.4.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.3.1.4.1" class="ltx_text" style="font-size:70%;">2.52</span></td>
<td id="S4.T3.4.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.3.1.5.1" class="ltx_text" style="font-size:70%;">3.42</span></td>
<td id="S4.T3.4.3.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.3.1.6.1" class="ltx_text" style="font-size:70%;">-</span></td>
</tr>
<tr id="S4.T3.4.4.2" class="ltx_tr">
<th id="S4.T3.4.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.4.4.2.1.1" class="ltx_text" style="font-size:70%;">TitaNet-LID 3L</span></th>
<td id="S4.T3.4.4.2.2" class="ltx_td ltx_align_center"><span id="S4.T3.4.4.2.2.1" class="ltx_text" style="font-size:70%;">13.57 [13.50-13.57]</span></td>
<td id="S4.T3.4.4.2.3" class="ltx_td ltx_align_center"><span id="S4.T3.4.4.2.3.1" class="ltx_text" style="font-size:70%;">8.32</span></td>
<td id="S4.T3.4.4.2.4" class="ltx_td ltx_align_center"><span id="S4.T3.4.4.2.4.1" class="ltx_text" style="font-size:70%;">2.86</span></td>
<td id="S4.T3.4.4.2.5" class="ltx_td ltx_align_center"><span id="S4.T3.4.4.2.5.1" class="ltx_text" style="font-size:70%;">2.39</span></td>
<td id="S4.T3.4.4.2.6" class="ltx_td ltx_align_center"><span id="S4.T3.4.4.2.6.1" class="ltx_text" style="font-size:70%;">26.29</span></td>
</tr>
<tr id="S4.T3.4.5.3" class="ltx_tr">
<th id="S4.T3.4.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.4.5.3.1.1" class="ltx_text" style="font-size:70%;">Seg-SLI + VAD</span></th>
<td id="S4.T3.4.5.3.2" class="ltx_td ltx_align_center"><span id="S4.T3.4.5.3.2.1" class="ltx_text" style="font-size:70%;">7.00 [6.95-7.05]</span></td>
<td id="S4.T3.4.5.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.4.5.3.3.1" class="ltx_text" style="font-size:70%;">1.75</span></td>
<td id="S4.T3.4.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T3.4.5.3.4.1" class="ltx_text" style="font-size:70%;">2.86</span></td>
<td id="S4.T3.4.5.3.5" class="ltx_td ltx_align_center"><span id="S4.T3.4.5.3.5.1" class="ltx_text" style="font-size:70%;">2.39</span></td>
<td id="S4.T3.4.5.3.6" class="ltx_td ltx_align_center"><span id="S4.T3.4.5.3.6.1" class="ltx_text" style="font-size:70%;">21.18</span></td>
</tr>
<tr id="S4.T3.4.6.4" class="ltx_tr">
<th id="S4.T3.4.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.4.6.4.1.1" class="ltx_text" style="font-size:70%;">Seg-SLI + SD</span></th>
<td id="S4.T3.4.6.4.2" class="ltx_td ltx_align_center"><span id="S4.T3.4.6.4.2.1" class="ltx_text" style="font-size:70%;">6.47 [6.42-6.53]</span></td>
<td id="S4.T3.4.6.4.3" class="ltx_td ltx_align_center"><span id="S4.T3.4.6.4.3.1" class="ltx_text" style="font-size:70%;">1.61</span></td>
<td id="S4.T3.4.6.4.4" class="ltx_td ltx_align_center"><span id="S4.T3.4.6.4.4.1" class="ltx_text" style="font-size:70%;">2.67</span></td>
<td id="S4.T3.4.6.4.5" class="ltx_td ltx_align_center"><span id="S4.T3.4.6.4.5.1" class="ltx_text" style="font-size:70%;">2.19</span></td>
<td id="S4.T3.4.6.4.6" class="ltx_td ltx_align_center"><span id="S4.T3.4.6.4.6.1" class="ltx_text" style="font-size:70%;">21.50</span></td>
</tr>
<tr id="S4.T3.4.7.5" class="ltx_tr">
<th id="S4.T3.4.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T3.4.7.5.1.1" class="ltx_text" style="font-size:70%;">Frm-SLI + VAD</span></th>
<td id="S4.T3.4.7.5.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.4.7.5.2.1" class="ltx_text" style="font-size:70%;">6.84 [6.79-6.89]</span></td>
<td id="S4.T3.4.7.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.4.7.5.3.1" class="ltx_text" style="font-size:70%;">1.70</span></td>
<td id="S4.T3.4.7.5.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.4.7.5.4.1" class="ltx_text" style="font-size:70%;">2.73</span></td>
<td id="S4.T3.4.7.5.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.4.7.5.5.1" class="ltx_text" style="font-size:70%;">2.40</span></td>
<td id="S4.T3.4.7.5.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.4.7.5.6.1" class="ltx_text" style="font-size:70%;">21.17</span></td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‣ 4.3 Real Multilingual Speech ‣ 4 Results ‣ Exploring Spoken Language Identification Strategies for Automatic Transcription of Multilingual Broadcast and Institutional Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the performance of the SLI systems on the authentic multilingual test sets.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Concerning the broadcast test set, the system combining SD and SLI reaches the lowest LDER. This is mostly due to a LC value (1.57%) much lower than the other systems. The ECAPA-TDNN shows the highest LC, most likely due to the SpeechBrain VAD doing fewer splits than our in-house VAD. On the other hand, the SpeechBrain VAD performs best in terms of false alarms.
Despite being the best system on synthetic multilingual data, the frame-based SLI does not significantly improve the performance of a segment-based SLI, neither in terms of LDER nor WER, significantly underperforming the system combining SD and SLI on the broadcast testset. By qualitative evaluations of the SLI systems predictions, we observe that the language boundaries are detected more sharply by the system combining SD and SLI than from the frame-based SLI. We hypothesize that this is a consequence of language-distinctive features happening at a longer timescale with respect to speaker-distinctive features, that result in a more accurate detection of speaker boundaries with respect to language boundaries. In addition, we observe that the frame-based system tends to over-segment when output language probabilities are very close to one another, having an impact on both LDER and WER. Lastly, the presence of English words in the Swedish speech might also be a source of errors for the frame-based SLI.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Regarding the institutional test set, we observe a big difference between LDER of the pretrained systems and our in-house systems. This is mostly due to much higher language confusion, that directly relates to the higher LER on the monolingual testset.
The best performing system in terms of LDER is again the system combining SD and SLI, even though, in this particular scenario, by a small margin w.r.t. the system employing a VAD. This is mostly due to the specific characteristics of the test set. Here, language (speaker) turns are well separated, often by relatively long pauses, and overlaps are very uncommon. In this case, the VAD successfully detects language boundaries, so there is not much room for improvement for other systems.
We report WER on this dataset for thoroughness, but values should be considered only as indicative, because the official transcripts of this dataset are not verbatim.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We analyzed the performance of different spoken language identification systems on multilingual broadcast and institutional speech. These domains are characterized by language changes that happen at relatively long timescales and usually match speaker changes. We found that, under these circumstances, a cascaded system composed of speaker diarization and segment-based SLI is particularly accurate in detecting language changes, outperforming systems that are agnostic to speaker information.
Furthermore, we analyzed the impact on transcription accuracy when the spoken language identifier is combined with monolingual speech recognition engines. We found that such a system has the lowest word error rate on multilingual data, with negligible degradation of monolingual transcription accuracy.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
H. Li, B. Ma, and K. A. Lee, ``Spoken Language Recognition: From Fundamentals to Practice,'' <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE</em>, vol. 101, no. 5, pp. 1136–1159, 2013.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Valk and T. Alumäe, ``VOXLINGUA107: A Dataset for Spoken Language Recognition,'' in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proc. of Spoken Language Technology Workshop</em>.   IEEE, 2021, pp. 652–658.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber, ``Common Voice: A Massively-Multilingual Speech Corpus,'' in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proc. of Language Resources and Evaluation Conference</em>.   European Language Resources Association, 2020, pp. 4211–4215.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A. Conneau, M. Ma, S. Khanuja, Y. Zhang, V. Axelrod, S. Dalmia, J. Riesa, C. Rivera, and A. Bapna, ``FLEURS: FEW-Shot Learning Evaluation of Universal Representations of Speech,'' in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proc. of Spoken Language Technology Workshop</em>.   IEEE, 2023, pp. 798–805.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
D.-C. Lyu, E.-S. Chng, and H. Li, ``Language Diarization for Code-Switch Conversational Speech,'' in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proc. of International Conference on Acoustics, Speech and Signal Processing</em>.   IEEE, 2013, pp. 7314–7318.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
C. Barras, V.-B. Le, and J.-L. Gauvain, ``Vocapia-LIMSI System for 2020 Shared Task on Code-switched Spoken Language Identification,'' in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proc. of Speech Technologies for Code-Switching in Multilingual Communities Workshop</em>, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
H. Liu, L. P. G. Perera, X. Zhang, J. Dauwels, A. W. Khong, S. Khudanpur, and S. J. Styles, ``End-to-End Language Diarization for Bilingual Code-Switching Speech,'' in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proc. of Interspeech</em>.   ISCA, 2021, pp. 1489–1493.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
F. Jia, N. R. Koluguri, J. Balam, and B. Ginsburg, ``A Compact End-to-End Model with Local and Global Context for Spoken Language Identification,'' in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proc. of Interspeech</em>.   ISCA, 2023, pp. 5321–5325.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Mishra, J. N. Patil, A. Chowdhury, and M. Prasanna, ``End to End Spoken Language Diarization with Wav2vec Embeddings,'' in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proc. of Interspeech</em>.   ISCA, 2023, pp. 501–505.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Graves and J. Schmidhuber, ``Framewise phoneme classification with bidirectional lstm and other neural network architectures,'' <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Neural networks</em>, vol. 18, no. 5-6, pp. 602–610, 2005.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
M. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. Cornell, L. Lugosch, C. Subakan, N. Dawalatabad, A. Heba, J. Zhong, J.-C. Chou, S.-L. Yeh, S.-W. Fu, C.-F. Liao, E. Rastorgueva, F. Grondin, W. Aris, H. Na, Y. Gao, R. D. Mori, and Y. Bengio, ``SpeechBrain: A general-purpose speech toolkit,'' <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv:2106.04624</em>, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
W. Han, Z. Zhang, Y. Zhang, J. Yu, C.-C. Chiu, J. Qin, A. Gulati, R. Pang, and Y. Wu, ``ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context,'' in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proc. of Interspeech</em>.   ISCA, 2020, pp. 3610–3614.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
K. Kinoshita, M. Delcroix, and N. Tawara, ``Advances in Integration of End-to-End Neural and Clustering-Based Diarization for Real Conversational Speech,'' in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proc. of Interspeech</em>.   ISCA, 2021, pp. 3565–3569.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
T. J. Park, N. Kanda, D. Dimitriadis, K. J. Han, S. Watanabe, and S. Narayanan, ``A review of speaker diarization: Recent advances with deep learning,'' <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Computer Speech &amp; Language</em>, vol. 72, p. 101317, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and S. Watanabe, ``End-to-end neural speaker diarization with self-attention,'' in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proc. of Automatic Speech Recognition and Understanding Workshop</em>.   IEEE, 2019, pp. 296–303.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
H. Zeinali, S. Wang, A. Silnova, P. Matějka, and O. Plchot, ``BUT System Description to VoxCeleb Speaker Recognition Challenge 2019,'' <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv:1910.12592</em>, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J. S. Chung, J. Huh, A. Nagrani, T. Afouras, and A. Zisserman, ``Spot the conversation: speaker diarisation in the wild,'' in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proc. of Interspeech</em>.   ISCA, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
C. Cieri, D. Miller, and K. Walker, ``The Fisher corpus: A resource for the next generations of speech-to-text.'' in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proc. of Language Resources and Evaluation Conference</em>, vol. 4.   European Language Resources Association, 2004, pp. 69–71.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and S. Watanabe, ``End-to-end neural speaker diarization with permutation-free objectives,'' in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proc. of Interspeech</em>, vol. 2019.   ISCA, 2019, pp. 4300–4304.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
D. Snyder, G. Chen, and D. Povey, ``Musan: A music, speech, and noise corpus,'' <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv:1510.08484</em>, 2015.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur, ``A study on data augmentation of reverberant speech for robust speech recognition,'' in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proc. of International Conference on Acoustics, Speech and Signal Processing</em>.   IEEE, 2017, pp. 5220–5224.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
O. Kuchaiev, J. Li, H. Nguyen, O. Hrinchuk, R. Leary, B. Ginsburg, S. Kriman, S. Beliaev, V. Lavrukhin, J. Cook, P. Castonguay, M. Popova, J. Huang, and J. M. Cohen, ``NeMo: a toolkit for building AI applications using Neural Modules,'' <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv:1909.09577</em>, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
H. Wang, C. Liang, S. Wang, Z. Chen, B. Zhang, X. Xiang, Y. Deng, and Y. Qian, ``Wespeaker: A research and production oriented speaker embedding learning toolkit,'' in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proc. of International Conference on Acoustics, Speech and Signal Processing</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J. S. Chung, A. Nagrani, and A. Zisserman, ``VoxCeleb2: Deep speaker recognition,'' in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proc. of Interspeech</em>.   ISCA, 2018.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
V. Y. H. Chua, H. Liu, L. P. Garcia, F. T. Woon, J. Wong, X. Zhang, S. Khudanpur, A. W. H. Khong, J. Dauwels, and S. J. Styles, ``MERLIon CCS Challenge: A English-Mandarin code-switching child-directed speech corpus for language identification and diarization,'' in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proc. of Interspeech</em>.   ISCA, 2023, pp. 4109–4113.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.09289" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.09290" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.09290">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.09290" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.09291" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Jul  6 00:46:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
