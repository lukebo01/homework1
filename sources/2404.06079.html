<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.06079] The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge</title><meta property="og:description" content="Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS).
In this paper, we descr…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.06079">

<!--Generated on Sun May  5 17:21:53 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.7" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.8" class="ltx_ERROR undefined">\name</span>
<p id="p1.6" class="ltx_p">[affiliation=1]YiweiGuo<sup id="p1.6.1" class="ltx_sup"><span id="p1.6.1.1" class="ltx_text ltx_font_italic">∗♠</span></sup>
<span id="p1.6.2" class="ltx_ERROR undefined">\name</span>[affiliation=1]ChenrunWang<sup id="p1.6.3" class="ltx_sup"><span id="p1.6.3.1" class="ltx_text ltx_font_italic">∗♣</span></sup>
<span id="p1.6.4" class="ltx_ERROR undefined">\name</span>[affiliation=1]YifanYang<sup id="p1.6.5" class="ltx_sup"><span id="p1.6.5.1" class="ltx_text ltx_font_italic">∗♡</span></sup>
<span id="p1.6.6" class="ltx_ERROR undefined">\name</span>[affiliation=1]HankunWang<sup id="p1.6.7" class="ltx_sup"><span id="p1.6.7.1" class="ltx_text ltx_font_italic">♣</span></sup>
<span id="p1.6.8" class="ltx_ERROR undefined">\name</span>[affiliation=1]ZiyangMa<sup id="p1.6.9" class="ltx_sup"><span id="p1.6.9.1" class="ltx_text ltx_font_italic">♠♡</span></sup>
<span id="p1.6.10" class="ltx_ERROR undefined">\name</span>[affiliation=1]ChenpengDu<sup id="p1.6.11" class="ltx_sup"><span id="p1.6.11.1" class="ltx_text ltx_font_italic">♠♣</span></sup>
<span id="p1.6.12" class="ltx_ERROR undefined">\name</span>[affiliation=2]ShuaiWang
<span id="p1.6.13" class="ltx_ERROR undefined">\name</span>[affiliation=3]HanzhengLi
<span id="p1.6.14" class="ltx_ERROR undefined">\name</span>[affiliation=3]XuLi
<span id="p1.6.15" class="ltx_ERROR undefined">\name</span>[affiliation=3]ShuaiFan
<span id="p1.6.16" class="ltx_ERROR undefined">\name</span>[affiliation=3]HuiZhang
<span id="p1.6.17" class="ltx_ERROR undefined">\name</span>[affiliation=1]XieChen
<span id="p1.6.18" class="ltx_ERROR undefined">\name</span>[affiliation=1,3]KaiYu</p>
</div>
<h1 class="ltx_title ltx_title_document">The X-LANCE Technical Report for Interspeech 2024 
<br class="ltx_break">Speech Processing Using Discrete Speech Unit Challenge</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS).
In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge.
Notably, we achieved 1st rank on the leaderboard<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Submitted on March 15th 2024 due to a non-technical issue, thus still evaluated by the organizers.</span></span></span>
in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>automatic speech recognition, text-to-speech, singing voice synthesis, discrete token, challenge
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><sup id="footnotex1.1" class="ltx_sup">∗</sup>Equal contribution</span></span></span><span id="footnotex2" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><sup id="footnotex2.1" class="ltx_sup">♠</sup>Text-to-speech (TTS) track</span></span></span><span id="footnotex3" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><sup id="footnotex3.1" class="ltx_sup">♣</sup>Singing voice synthesis (SVS) track</span></span></span><span id="footnotex4" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><sup id="footnotex4.1" class="ltx_sup">♡</sup>Automatic speech recognition (ASR) track</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>TTS Track: The VQTTS System</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The TTS track requires participants to build a discrete token-based TTS on the LJSpeech dataset, with a favorably low bitrate and high naturalness score.
Our best submission used FunCodec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> as the discrete tokens and a modified VQTTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> system as the TTS pipeline.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Discrete Tokens</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">In TTS, two kinds of speech discretization are both favored: the semantic tokens and the acoustic tokens <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
Semantic tokens capture most of the content-relevant information by clustering on self-supervised learning models, while the acoustic tokens aim to reconstruct speech signals as perfectly as possible.
Both two types exhibit their own pros and cons, and have been successfully used in speech synthesis tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">In this TTS track, we considered a semantic token, wav2vec2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, and an acoustic token, FunCodec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
Specifically, for wav2vec2.0, we used the official wav2vec2-large-lv60 model<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://huggingface.co/facebook/wav2vec2-large-lv60</span></span></span> which was pretrained on 60k hours of LibriLight <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
We did not use a finetuned version of wav2vec2.0 because the quantizer is only trained in the pretraining stage.
This led to a codebook of 2 groups, each with 320 vocabulary size and 384-dimensional code-vectors in 50Hz frame rate.
In our preliminary study, the discrete codes from wav2vec2.0 contain more prosodic information than vq-wav2vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and Kmeans clusters on HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
The 50Hz frame rate also benefits the reduction of bitrate.
After extracting the 2 groups of codebook indexes, we identified 24686 unique pairs that occurred at least once in the training set, hence the original 2 groups can be transformed to one group of 24686 integer indexes on the provided corpus with a bitrate at about 729bps.
The 2 groups of 320 codes contain <math id="S1.SS1.p2.1.m1.1" class="ltx_Math" alttext="320^{2}=102400" display="inline"><semantics id="S1.SS1.p2.1.m1.1a"><mrow id="S1.SS1.p2.1.m1.1.1" xref="S1.SS1.p2.1.m1.1.1.cmml"><msup id="S1.SS1.p2.1.m1.1.1.2" xref="S1.SS1.p2.1.m1.1.1.2.cmml"><mn id="S1.SS1.p2.1.m1.1.1.2.2" xref="S1.SS1.p2.1.m1.1.1.2.2.cmml">320</mn><mn id="S1.SS1.p2.1.m1.1.1.2.3" xref="S1.SS1.p2.1.m1.1.1.2.3.cmml">2</mn></msup><mo id="S1.SS1.p2.1.m1.1.1.1" xref="S1.SS1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S1.SS1.p2.1.m1.1.1.3" xref="S1.SS1.p2.1.m1.1.1.3.cmml">102400</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.SS1.p2.1.m1.1b"><apply id="S1.SS1.p2.1.m1.1.1.cmml" xref="S1.SS1.p2.1.m1.1.1"><eq id="S1.SS1.p2.1.m1.1.1.1.cmml" xref="S1.SS1.p2.1.m1.1.1.1"></eq><apply id="S1.SS1.p2.1.m1.1.1.2.cmml" xref="S1.SS1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S1.SS1.p2.1.m1.1.1.2.1.cmml" xref="S1.SS1.p2.1.m1.1.1.2">superscript</csymbol><cn type="integer" id="S1.SS1.p2.1.m1.1.1.2.2.cmml" xref="S1.SS1.p2.1.m1.1.1.2.2">320</cn><cn type="integer" id="S1.SS1.p2.1.m1.1.1.2.3.cmml" xref="S1.SS1.p2.1.m1.1.1.2.3">2</cn></apply><cn type="integer" id="S1.SS1.p2.1.m1.1.1.3.cmml" xref="S1.SS1.p2.1.m1.1.1.3">102400</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p2.1.m1.1c">320^{2}=102400</annotation></semantics></math> possible combinations, so compressing them into one group considerably reduced the bitrate.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">For FunCodec, we used an open-sourced checkpoint<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://huggingface.co/alibaba-damo/audio_codec-encodec-zh_en-general-16k-nq32ds640-pytorch</span></span></span> trained on a large in-house corpus containing both English and Chinese data with an total about 25k hours.
It adopts a similar architecture with EnCodec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, with the following advantages:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">The frame rate is 25Hz, i.e. 640 times downsampling on 16kHz waveform. This reduces the bitrate significantly, since the necessary vocabulary size can be relaxed in a squared sense when the sequence length is halved.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Some techniques are introduced into training, such as an additional magnitude spectrum loss, structured dropout, and codebook learning strategies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. This improves the reconstruction quality of the quantized codes.</p>
</div>
</li>
</ul>
<p id="S1.SS1.p3.2" class="ltx_p">We only used the first codebook with 1024-size and 128-dimensional code vectors, and trained a customized vocoder to better adapt to the given corpus.
The bitrate is <span id="S1.SS1.p3.2.1" class="ltx_text ltx_font_bold">250bps</span> then, which is the lowest among all submissions in this track.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Model Architecture</h3>

<section id="S1.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.2.1 </span>Acoustic Model</h4>

<div id="S1.SS2.SSS1.p1" class="ltx_para">
<p id="S1.SS2.SSS1.p1.1" class="ltx_p">The acoustic model in this challenge inherits the txt2vec acoustic model in VQTTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
The model architecture is visualized in Fig.<a href="#S1.F1" title="Figure 1 ‣ 1.2.2 Discrete Unit-based Vocoder ‣ 1.2 Model Architecture ‣ 1 TTS Track: The VQTTS System ‣ The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
The input phoneme sequence is fed into a conformer-based text encoder, and then a phoneme-level prosody controller.
This prosody controller predicts the Kmeans clustering index of phoneme-averaged dynamic prosodic features<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Pitch, probability of voice, energy, and their first and second-order differences.</span></span></span> via an LSTM.
Then, a length regulator repeats the phone-level sequence to frame-level.
After a conformer decoder, we design another causal decoder that predicts the vector-quantized (VQ) indexes.
Although this is an autoregressive decoder, the decoding timesteps are fixed to be the durations.
In every decoding step, the input to the causal VQ decoder is the corresponding hidden state concatenated with the code vector of the code index from the last step.
In this practice, we favor the transformer decoder structure rather than the LSTM in VQTTS.
The training criterion consists of the cross entropy of VQ prediction, the duration prediction loss and the prosody prediction loss.
</p>
</div>
</section>
<section id="S1.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.2.2 </span>Discrete Unit-based Vocoder</h4>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2404.06079/assets/figure/vqtts.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="374" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Architecture of the acoustic model (left) and vocoder (right) in the TTS track. ``C" in the circle means to concatenate along dimensions.</figcaption>
</figure>
<div id="S1.SS2.SSS2.p1" class="ltx_para">
<p id="S1.SS2.SSS2.p1.1" class="ltx_p">Our vocoder resembles the CTX-vec2wav vocoder introduced in UniCATS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. We discard the ``context" part since this track only considers single-speaker TTS.
As is shown in Fig.<a href="#S1.F1" title="Figure 1 ‣ 1.2.2 Discrete Unit-based Vocoder ‣ 1.2 Model Architecture ‣ 1 TTS Track: The VQTTS System ‣ The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the code vectors are first fed into a frontend consisting of conformer encoders and an auxiliary feature adaptor, before entering the generator of HifiGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
The auxiliary feature adaptor predicts the prosody features.
To further mitigate the pronunciation errors brought by speech discretization, we implement a trick that provides the aligned phoneme sequence to the vocoder by embedding it and concatenating it with the code vectors.
As the phonemes are the input to the whole TTS system, this trick more resembles a ``residual connection" and thus does not increase the bitrate.
In inference, the aligned phoneme sequence can be directly obtained from the length regulator in the acoustic model.
This relaxes the burden of the vocoder since phonemes already provide strong information of articulation.
</p>
</div>
<div id="S1.SS2.SSS2.p2" class="ltx_para">
<p id="S1.SS2.SSS2.p2.1" class="ltx_p">The training criterion of this vocoder includes the HifiGAN loss (i.e. mel loss, feature matching loss and discriminator loss), the frontend mel loss and the auxiliary feature prediction loss.</p>
</div>
</section>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Data Preparation and Training</h3>

<section id="S1.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.3.1 </span>Data Preparation</h4>

<div id="S1.SS3.SSS1.p1" class="ltx_para">
<p id="S1.SS3.SSS1.p1.1" class="ltx_p">The provided dataset is LJSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, a single-speaker English corpus with about 24 hours.
We downsampled all speech data to 16kHz.
We used Kaldi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> to extract the 80-dimensional mel-spectrograms and prosodic features in 20ms frame shift for training the vocoder.
These features were cepstral mean-normalized with statistics computed on the training set.</p>
</div>
<div id="S1.SS3.SSS1.p2" class="ltx_para">
<p id="S1.SS3.SSS1.p2.1" class="ltx_p">For texts, we obtained optional silence marks by performing alignment in 10ms by Kaldi.
We then needed to have the phoneme duration in 20ms frame shift, but this appeared to be too large for Kaldi since every phoneme must be aligned to at least 3 consecutive frames there.
Hence we resorted to RAD-TTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, a flow-based TTS model with the capability of learning robust alignment paths.
We trained a RAD-TTS model on LibriTTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> in 20ms frame shift and then performed inference on the LJSpeech dataset.
However, the FunCodec tokens are in 40ms frame shift, which might still be too large for alignment.
Hence for FunCodec, we repeated each token for two times to match the 20ms duration both in the acoustic model and vocoder.
This had no effect on the bitrate either, because folding and repeating can both be done inside the models.</p>
</div>
</section>
<section id="S1.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.3.2 </span>Training</h4>

<div id="S1.SS3.SSS2.p1" class="ltx_para">
<p id="S1.SS3.SSS2.p1.2" class="ltx_p">We trained the acoustic model for 160 epochs using the Adam optimizer at a learning rate of <math id="S1.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="10^{-3}" display="inline"><semantics id="S1.SS3.SSS2.p1.1.m1.1a"><msup id="S1.SS3.SSS2.p1.1.m1.1.1" xref="S1.SS3.SSS2.p1.1.m1.1.1.cmml"><mn id="S1.SS3.SSS2.p1.1.m1.1.1.2" xref="S1.SS3.SSS2.p1.1.m1.1.1.2.cmml">10</mn><mrow id="S1.SS3.SSS2.p1.1.m1.1.1.3" xref="S1.SS3.SSS2.p1.1.m1.1.1.3.cmml"><mo id="S1.SS3.SSS2.p1.1.m1.1.1.3a" xref="S1.SS3.SSS2.p1.1.m1.1.1.3.cmml">−</mo><mn id="S1.SS3.SSS2.p1.1.m1.1.1.3.2" xref="S1.SS3.SSS2.p1.1.m1.1.1.3.2.cmml">3</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S1.SS3.SSS2.p1.1.m1.1b"><apply id="S1.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S1.SS3.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S1.SS3.SSS2.p1.1.m1.1.1.1.cmml" xref="S1.SS3.SSS2.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S1.SS3.SSS2.p1.1.m1.1.1.2.cmml" xref="S1.SS3.SSS2.p1.1.m1.1.1.2">10</cn><apply id="S1.SS3.SSS2.p1.1.m1.1.1.3.cmml" xref="S1.SS3.SSS2.p1.1.m1.1.1.3"><minus id="S1.SS3.SSS2.p1.1.m1.1.1.3.1.cmml" xref="S1.SS3.SSS2.p1.1.m1.1.1.3"></minus><cn type="integer" id="S1.SS3.SSS2.p1.1.m1.1.1.3.2.cmml" xref="S1.SS3.SSS2.p1.1.m1.1.1.3.2">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS3.SSS2.p1.1.m1.1c">10^{-3}</annotation></semantics></math> and a decay at <math id="S1.SS3.SSS2.p1.2.m2.1" class="ltx_Math" alttext="10^{-6}" display="inline"><semantics id="S1.SS3.SSS2.p1.2.m2.1a"><msup id="S1.SS3.SSS2.p1.2.m2.1.1" xref="S1.SS3.SSS2.p1.2.m2.1.1.cmml"><mn id="S1.SS3.SSS2.p1.2.m2.1.1.2" xref="S1.SS3.SSS2.p1.2.m2.1.1.2.cmml">10</mn><mrow id="S1.SS3.SSS2.p1.2.m2.1.1.3" xref="S1.SS3.SSS2.p1.2.m2.1.1.3.cmml"><mo id="S1.SS3.SSS2.p1.2.m2.1.1.3a" xref="S1.SS3.SSS2.p1.2.m2.1.1.3.cmml">−</mo><mn id="S1.SS3.SSS2.p1.2.m2.1.1.3.2" xref="S1.SS3.SSS2.p1.2.m2.1.1.3.2.cmml">6</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S1.SS3.SSS2.p1.2.m2.1b"><apply id="S1.SS3.SSS2.p1.2.m2.1.1.cmml" xref="S1.SS3.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S1.SS3.SSS2.p1.2.m2.1.1.1.cmml" xref="S1.SS3.SSS2.p1.2.m2.1.1">superscript</csymbol><cn type="integer" id="S1.SS3.SSS2.p1.2.m2.1.1.2.cmml" xref="S1.SS3.SSS2.p1.2.m2.1.1.2">10</cn><apply id="S1.SS3.SSS2.p1.2.m2.1.1.3.cmml" xref="S1.SS3.SSS2.p1.2.m2.1.1.3"><minus id="S1.SS3.SSS2.p1.2.m2.1.1.3.1.cmml" xref="S1.SS3.SSS2.p1.2.m2.1.1.3"></minus><cn type="integer" id="S1.SS3.SSS2.p1.2.m2.1.1.3.2.cmml" xref="S1.SS3.SSS2.p1.2.m2.1.1.3.2">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS3.SSS2.p1.2.m2.1c">10^{-6}</annotation></semantics></math>.
The text encoder and decoder contains 6 and 3 conformer blocks respectively, each with 384 attention dimensions and 2 heads.
The causal VQ decoder is a transformer decoder with 6 layers, 768 attention dimensions and 12 heads.
The three losses in the acoustic model were equally weighted.
</p>
</div>
<div id="S1.SS3.SSS2.p2" class="ltx_para">
<p id="S1.SS3.SSS2.p2.1" class="ltx_p">The implementation and training of the vocoder follows that of UniCATS<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://github.com/X-LANCE/UniCATS-CTX-vec2wav</span></span></span>, only with the difference of input dimensions.</p>
</div>
</section>
</section>
<section id="S1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.4 </span>Experimental Results</h3>

<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Resynthesis and TTS results in the TTS track. WER is measured by Whisper ``medium.en" version, and ``w/ phn." means to provide aligned phonemes in the vocoder. The official baseline as a bitrate of 448.3bps with UTMOS of 3.73 on the whole training set, and 2.48 on the 1h training set.
</figcaption>
<table id="S1.T1.13" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.T1.3.3" class="ltx_tr">
<th id="S1.T1.3.3.4" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S1.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S1.T1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1.1.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Bitrate<math id="S1.T1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S1.T1.1.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S1.T1.1.1.1.1.1.1.1.m1.1.1" xref="S1.T1.1.1.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S1.T1.1.1.1.1.1.1.1.m1.1b"><ci id="S1.T1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S1.T1.1.1.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.1.1.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
</tr>
<tr id="S1.T1.1.1.1.1.2" class="ltx_tr">
<td id="S1.T1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.1.1.1.1.2.1.1" class="ltx_text" style="font-size:80%;">(bps)</span></td>
</tr>
</table>
</td>
<td id="S1.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S1.T1.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.2.2.2.1.1" class="ltx_tr">
<td id="S1.T1.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.2.2.2.1.1.1.1" class="ltx_text ltx_font_bold">WER<math id="S1.T1.2.2.2.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S1.T1.2.2.2.1.1.1.1.m1.1a"><mo stretchy="false" id="S1.T1.2.2.2.1.1.1.1.m1.1.1" xref="S1.T1.2.2.2.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S1.T1.2.2.2.1.1.1.1.m1.1b"><ci id="S1.T1.2.2.2.1.1.1.1.m1.1.1.cmml" xref="S1.T1.2.2.2.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.2.2.2.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
</tr>
<tr id="S1.T1.2.2.2.1.2" class="ltx_tr">
<td id="S1.T1.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.2.2.2.1.2.1.1" class="ltx_text" style="font-size:80%;">(Whisper)</span></td>
</tr>
</table>
</td>
<td id="S1.T1.3.3.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt"><span id="S1.T1.3.3.3.1" class="ltx_text ltx_font_bold">UTMOS<math id="S1.T1.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S1.T1.3.3.3.1.m1.1a"><mo stretchy="false" id="S1.T1.3.3.3.1.m1.1.1" xref="S1.T1.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S1.T1.3.3.3.1.m1.1b"><ci id="S1.T1.3.3.3.1.m1.1.1.cmml" xref="S1.T1.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math></span></td>
</tr>
<tr id="S1.T1.4.4" class="ltx_tr">
<th id="S1.T1.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Ground truth</th>
<td id="S1.T1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S1.T1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">2.31</td>
<td id="S1.T1.4.4.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">4.43<math id="S1.T1.4.4.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T1.4.4.1.m1.1a"><mo id="S1.T1.4.4.1.m1.1.1" xref="S1.T1.4.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T1.4.4.1.m1.1b"><csymbol cd="latexml" id="S1.T1.4.4.1.m1.1.1.cmml" xref="S1.T1.4.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.4.4.1.m1.1c">\pm</annotation></semantics></math>0.07</td>
</tr>
<tr id="S1.T1.13.14.1" class="ltx_tr">
<th id="S1.T1.13.14.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S1.T1.13.14.1.1.1" class="ltx_text ltx_font_bold">Resynthesis</span></th>
<td id="S1.T1.13.14.1.2" class="ltx_td ltx_border_tt"></td>
<td id="S1.T1.13.14.1.3" class="ltx_td ltx_border_tt"></td>
<td id="S1.T1.13.14.1.4" class="ltx_td ltx_nopad_r ltx_border_tt"></td>
</tr>
<tr id="S1.T1.5.5" class="ltx_tr">
<th id="S1.T1.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">wav2vec2.0</th>
<td id="S1.T1.5.5.3" class="ltx_td ltx_border_t"></td>
<td id="S1.T1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">2.84</td>
<td id="S1.T1.5.5.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">4.38<math id="S1.T1.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T1.5.5.1.m1.1a"><mo id="S1.T1.5.5.1.m1.1.1" xref="S1.T1.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T1.5.5.1.m1.1b"><csymbol cd="latexml" id="S1.T1.5.5.1.m1.1.1.cmml" xref="S1.T1.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.5.5.1.m1.1c">\pm</annotation></semantics></math>0.15</td>
</tr>
<tr id="S1.T1.6.6" class="ltx_tr">
<th id="S1.T1.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">wav2vec2.0 w/ phn.</th>
<td id="S1.T1.6.6.3" class="ltx_td ltx_align_center"><span id="S1.T1.6.6.3.1" class="ltx_text">729</span></td>
<td id="S1.T1.6.6.4" class="ltx_td ltx_align_center"><span id="S1.T1.6.6.4.1" class="ltx_text ltx_font_bold">2.51</span></td>
<td id="S1.T1.6.6.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S1.T1.6.6.1.1" class="ltx_text ltx_font_bold">4.44<math id="S1.T1.6.6.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T1.6.6.1.1.m1.1a"><mo id="S1.T1.6.6.1.1.m1.1.1" xref="S1.T1.6.6.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T1.6.6.1.1.m1.1b"><csymbol cd="latexml" id="S1.T1.6.6.1.1.m1.1.1.cmml" xref="S1.T1.6.6.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.6.6.1.1.m1.1c">\pm</annotation></semantics></math>0.10</span></td>
</tr>
<tr id="S1.T1.7.7" class="ltx_tr">
<th id="S1.T1.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S1.T1.7.7.2.1" class="ltx_ERROR undefined">\rowcolor</span>[HTML]EFEFEF
FunCodec w/ phn.</th>
<td id="S1.T1.7.7.3" class="ltx_td ltx_align_center"><span id="S1.T1.7.7.3.1" class="ltx_text ltx_font_bold">250</span></td>
<td id="S1.T1.7.7.4" class="ltx_td ltx_align_center">4.14</td>
<td id="S1.T1.7.7.1" class="ltx_td ltx_nopad_r ltx_align_center">4.43<math id="S1.T1.7.7.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T1.7.7.1.m1.1a"><mo id="S1.T1.7.7.1.m1.1.1" xref="S1.T1.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T1.7.7.1.m1.1b"><csymbol cd="latexml" id="S1.T1.7.7.1.m1.1.1.cmml" xref="S1.T1.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.7.7.1.m1.1c">\pm</annotation></semantics></math>0.13</td>
</tr>
<tr id="S1.T1.13.15.2" class="ltx_tr">
<th id="S1.T1.13.15.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S1.T1.13.15.2.1.1" class="ltx_text ltx_font_bold">Resynthesis-1h</span></th>
<td id="S1.T1.13.15.2.2" class="ltx_td ltx_border_tt"></td>
<td id="S1.T1.13.15.2.3" class="ltx_td ltx_border_tt"></td>
<td id="S1.T1.13.15.2.4" class="ltx_td ltx_nopad_r ltx_border_tt"></td>
</tr>
<tr id="S1.T1.8.8" class="ltx_tr">
<th id="S1.T1.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">wav2vec2.0 w/ phn.</th>
<td id="S1.T1.8.8.3" class="ltx_td ltx_align_center ltx_border_t">729</td>
<td id="S1.T1.8.8.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.8.8.4.1" class="ltx_text ltx_font_bold">3.72</span></td>
<td id="S1.T1.8.8.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S1.T1.8.8.1.1" class="ltx_text ltx_font_bold">3.47<math id="S1.T1.8.8.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T1.8.8.1.1.m1.1a"><mo id="S1.T1.8.8.1.1.m1.1.1" xref="S1.T1.8.8.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T1.8.8.1.1.m1.1b"><csymbol cd="latexml" id="S1.T1.8.8.1.1.m1.1.1.cmml" xref="S1.T1.8.8.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.8.8.1.1.m1.1c">\pm</annotation></semantics></math>0.34</span></td>
</tr>
<tr id="S1.T1.9.9" class="ltx_tr">
<th id="S1.T1.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S1.T1.9.9.2.1" class="ltx_ERROR undefined">\rowcolor</span>[HTML]EFEFEF
FunCodec w/ phn.</th>
<td id="S1.T1.9.9.3" class="ltx_td ltx_align_center"><span id="S1.T1.9.9.3.1" class="ltx_text ltx_font_bold">250</span></td>
<td id="S1.T1.9.9.4" class="ltx_td ltx_align_center">6.73</td>
<td id="S1.T1.9.9.1" class="ltx_td ltx_nopad_r ltx_align_center">2.84<math id="S1.T1.9.9.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T1.9.9.1.m1.1a"><mo id="S1.T1.9.9.1.m1.1.1" xref="S1.T1.9.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T1.9.9.1.m1.1b"><csymbol cd="latexml" id="S1.T1.9.9.1.m1.1.1.cmml" xref="S1.T1.9.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.9.9.1.m1.1c">\pm</annotation></semantics></math>0.35</td>
</tr>
<tr id="S1.T1.13.16.3" class="ltx_tr">
<th id="S1.T1.13.16.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S1.T1.13.16.3.1.1" class="ltx_text ltx_font_bold">TTS</span></th>
<td id="S1.T1.13.16.3.2" class="ltx_td ltx_border_tt"></td>
<td id="S1.T1.13.16.3.3" class="ltx_td ltx_border_tt"></td>
<td id="S1.T1.13.16.3.4" class="ltx_td ltx_nopad_r ltx_border_tt"></td>
</tr>
<tr id="S1.T1.10.10" class="ltx_tr">
<th id="S1.T1.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">wav2vec2.0</th>
<td id="S1.T1.10.10.3" class="ltx_td ltx_align_center ltx_border_t">729</td>
<td id="S1.T1.10.10.4" class="ltx_td ltx_align_center ltx_border_t">2.21</td>
<td id="S1.T1.10.10.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S1.T1.10.10.1.1" class="ltx_text ltx_font_bold">4.42<math id="S1.T1.10.10.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T1.10.10.1.1.m1.1a"><mo id="S1.T1.10.10.1.1.m1.1.1" xref="S1.T1.10.10.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T1.10.10.1.1.m1.1b"><csymbol cd="latexml" id="S1.T1.10.10.1.1.m1.1.1.cmml" xref="S1.T1.10.10.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.10.10.1.1.m1.1c">\pm</annotation></semantics></math>0.11</span></td>
</tr>
<tr id="S1.T1.11.11" class="ltx_tr">
<th id="S1.T1.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S1.T1.11.11.2.1" class="ltx_ERROR undefined">\rowcolor</span>[HTML]EFEFEF
FunCodec</th>
<td id="S1.T1.11.11.3" class="ltx_td ltx_align_center"><span id="S1.T1.11.11.3.1" class="ltx_text ltx_font_bold">250</span></td>
<td id="S1.T1.11.11.4" class="ltx_td ltx_align_center"><span id="S1.T1.11.11.4.1" class="ltx_text ltx_font_bold">2.11</span></td>
<td id="S1.T1.11.11.1" class="ltx_td ltx_nopad_r ltx_align_center">4.41<math id="S1.T1.11.11.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T1.11.11.1.m1.1a"><mo id="S1.T1.11.11.1.m1.1.1" xref="S1.T1.11.11.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T1.11.11.1.m1.1b"><csymbol cd="latexml" id="S1.T1.11.11.1.m1.1.1.cmml" xref="S1.T1.11.11.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.11.11.1.m1.1c">\pm</annotation></semantics></math>0.11</td>
</tr>
<tr id="S1.T1.13.17.4" class="ltx_tr">
<th id="S1.T1.13.17.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S1.T1.13.17.4.1.1" class="ltx_text ltx_font_bold">TTS-1h</span></th>
<td id="S1.T1.13.17.4.2" class="ltx_td ltx_border_tt"></td>
<td id="S1.T1.13.17.4.3" class="ltx_td ltx_border_tt"></td>
<td id="S1.T1.13.17.4.4" class="ltx_td ltx_nopad_r ltx_border_tt"></td>
</tr>
<tr id="S1.T1.12.12" class="ltx_tr">
<th id="S1.T1.12.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">wav2vec2.0</th>
<td id="S1.T1.12.12.3" class="ltx_td ltx_align_center ltx_border_t">729</td>
<td id="S1.T1.12.12.4" class="ltx_td ltx_align_center ltx_border_t">41.99</td>
<td id="S1.T1.12.12.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">2.77<math id="S1.T1.12.12.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T1.12.12.1.m1.1a"><mo id="S1.T1.12.12.1.m1.1.1" xref="S1.T1.12.12.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T1.12.12.1.m1.1b"><csymbol cd="latexml" id="S1.T1.12.12.1.m1.1.1.cmml" xref="S1.T1.12.12.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.12.12.1.m1.1c">\pm</annotation></semantics></math>0.37</td>
</tr>
<tr id="S1.T1.13.13" class="ltx_tr">
<th id="S1.T1.13.13.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">
<span id="S1.T1.13.13.2.1" class="ltx_ERROR undefined">\rowcolor</span>[HTML]EFEFEF
FunCodec</th>
<td id="S1.T1.13.13.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S1.T1.13.13.3.1" class="ltx_text ltx_font_bold">250</span></td>
<td id="S1.T1.13.13.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S1.T1.13.13.4.1" class="ltx_text ltx_font_bold">8.87</span></td>
<td id="S1.T1.13.13.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S1.T1.13.13.1.1" class="ltx_text ltx_font_bold">2.77<math id="S1.T1.13.13.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S1.T1.13.13.1.1.m1.1a"><mo id="S1.T1.13.13.1.1.m1.1.1" xref="S1.T1.13.13.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S1.T1.13.13.1.1.m1.1b"><csymbol cd="latexml" id="S1.T1.13.13.1.1.m1.1.1.cmml" xref="S1.T1.13.13.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.13.13.1.1.m1.1c">\pm</annotation></semantics></math>0.28</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S1.SS4.p1" class="ltx_para">
<p id="S1.SS4.p1.1" class="ltx_p">Table <a href="#S1.T1" title="Table 1 ‣ 1.4 Experimental Results ‣ 1 TTS Track: The VQTTS System ‣ The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the vocoder resynthesis and the TTS results. In this table, the TTS results were obtained by the phoneme-augmented (``w/ phn.") version of vocoders. We used Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> to evaluate the intelligibility and UTMOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> to measure naturalness in a objective manner.
It can thus be seen that providing aligned phonemes in vocoders decreases the WER while also increases the UTMOS score when reconstructed from wav2vec2.0 tokens, which is the reason we only trained the FunCodec vocoder with phonemes.
Although FunCodec exhibits worse WER in resynthesis, we found it somehow improved the WER compared to wav2vec2.0 in the TTS scenario while keeping almost the same UTMOS.
Note that both tokens outperform ground truth in terms of WER. The reason might be that there are more tokenization errors from the real waveforms, but fewer from synthesized ones, and the vocoder still learns the right pronunciation from most of the correctly generated tokens.
Both wav2vec2.0 and FunCodec resulted in a TTS model that performed much better than the official baseline (HuBERT + FastSpeech) in terms of UTMOS.</p>
</div>
<div id="S1.SS4.p2" class="ltx_para">
<p id="S1.SS4.p2.1" class="ltx_p">For versions trained on only 1 hour data, we found FunCodec exhibited more stable decoding compared to wav2vec2.0 though the latter obtained better resynthesis quality. This can be explained by the drastical difference between the two vocabulary sizes.
FunCodec's 1024 tokens made prediction process much simpler.
From Table <a href="#S1.T1" title="Table 1 ‣ 1.4 Experimental Results ‣ 1 TTS Track: The VQTTS System ‣ The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we conclude that the first layer of FunCodec is the better choice for this single-speaker TTS challenge, since it has a much lower bitrate, better WER and still highly competitive UTMOS.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>SVS Track: The DOVSinger System</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The SVS track requires participants to build a discrete token-based SVS on the Opencpop  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> dataset, with a favorably low bit-rate and high naturalness score.
Our best submission used DAC as the discrete tokens and a modified VALL-E <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> system as the SVS pipeline.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Discrete Tokens</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">For singing voice synthesis (SVS), discrete tokens can also be categorized into semantic tokens and acoustic tokens, but discrete tokens specifically designed and trained for SVS are not very common. Recent work Make-A-Voice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> utilized HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> as the semantic tokens and SoundStream <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> as the acoustic tokens and designed a unit-based vocoder to generate speech and singing voices. However, solely for SVS, we do not need to employ such complex network architectures. Through experimentation, we found that the Descript Audio Codec (DAC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> can faithfully restore the original audio in the Opencpop dataset.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">DAC is a universal neural audio compression algorithm renowned for its high-fidelity performance, achieving compression of 44.1 kHz audio into tokens. The sampling rate of Opencpop is also 44.1kHz, avoiding information loss during downsampling. In contrast, other acoustic token models like SoundStream <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, tailored for audio with a sampling rate of 24kHz, necessitate downsampling, leading to potential information loss.
Moreover, DAC endeavors to compress audio across all domains and benefits from training on datasets abundant in music, such as the MUSDB dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, consequently yielding outstanding reconstruction results for singing voices.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">In consideration of the excessive information load and significantly high bitrate carried by the 18-layer DAC, we ultimately opted to utilize the the initial layer of DAC features as discrete tokens to facilitate information transmission.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Model Architecture</h3>

<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Acoustic Model</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">The acoustic model has been refined through adjustments grounded in VALL-E <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>'s framework. The model architecture is visualized in Fig.<a href="#S2.F2" title="Figure 2 ‣ 2.2.1 Acoustic Model ‣ 2.2 Model Architecture ‣ 2 SVS Track: The DOVSinger System ‣ The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Within the existing framework, VALL-E employs a decoder-only neural network architecture to decode input information directly into discrete tokens. Specifically, it utilizes a single-layer autoregressive (AR) structure to decode the first-layer tokens, and employs a seven-layer non-autoregressive (NAR) structure to decode the second to eighth layer.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p">In the AR architecture, VALL-E performs prediction in the temporal axis. However, in the context of the SVS task, as duration is explicitly provided in the MIDI input, there is no requirement for supplementary mechanisms to predict duration. Therefore, after augmenting the phone, pitch, slur, and other information with phone duration from the MIDI, we directly employed the NAR structure to predict the first-layer DAC codes.</p>
</div>
<div id="S2.SS2.SSS1.p3" class="ltx_para">
<p id="S2.SS2.SSS1.p3.1" class="ltx_p">After experimentation, we observed that the stability of the reconstructed waveform was compromised when employing the NAR structure for predicting the first-layer codes directly. To facilitate the learning of contextual information from neighboring frames more effectively, we introduced an additional layer of LSTM following the transformer layer. The results demonstrated that the waveform reconstructed in this manner exhibited improved temporal stability and coherence. The training criterion is the cross entropy loss of DAC's first layer prediction.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2404.06079/assets/figure/svs.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="527" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Architecture of the acoustic model (left) and vocoder (right) in the SVS track. </figcaption>
</figure>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Discrete Unit-based Vocoder</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">The discrete code information from the first layer of the DAC obtained after the acoustic model is combined with the MIDI information as inputs to the vocoder.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">The primary architecture of the vocoder involves the continuation of inferring subsequent DAC codes using input information, followed by waveform reconstruction using a DAC decoder. During the inference of each layer of code, additional information from preceding layers is also incorporated.To ensure both the speed and quality of vocoder generation, we adopted a fully NAR structure to further infer four layers of DAC code, replacing the LSTM structure with a Linear one to enhance inference speed.The training criterion is the cross entropy loss of DAC's 2nd to 5th layer prediction.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Data Preparation and Training</h3>

<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Data Preparation</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para">
<p id="S2.SS3.SSS1.p1.1" class="ltx_p">The provided dataset is Opencpop <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, consisting of 100 unique Mandarin songs, which were recorded by a professional female singer. Furthermore, we leveraged the M4Singer dataset during training to augment the model's capacity for robust fitting. It is essential to underscore that the incorporation of supplementary datasets mandates the inclusion of speaker attributes within the MIDI input. Prior to training, we performed DAC extraction on the entire training corpus to obtain discrete codes. Additionally, we converted phone durations into frame counts using DAC's 1/86-second frame shift and temporally extended other MIDI attributes to match the frame count length, enabling direct integration into the NAR model.</p>
</div>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Training</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para">
<p id="S2.SS3.SSS2.p1.1" class="ltx_p">We trained the acoustic model for 160 epochs using the Adam optimizer at a base learning rate of 0.05.
The transformer encoder contains 6 layers, each with 512 attention dimensions and 8 heads.</p>
</div>
</section>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Experimental Results</h3>

<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>SVS results in the SVS track. All data is sourced from the leaderboard. However, the leaderboard did not provide the bitrate and MOS of baseline system.</figcaption>
<table id="S2.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T2.3.3" class="ltx_tr">
<td id="S2.T2.3.3.4" class="ltx_td ltx_border_tt"></td>
<th id="S2.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T2.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.1.1.1.1" class="ltx_tr">
<td id="S2.T2.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T2.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Bitrate<math id="S2.T2.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T2.1.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S2.T2.1.1.1.1.1.1.1.m1.1.1" xref="S2.T2.1.1.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.1.1.1.1.m1.1b"><ci id="S2.T2.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T2.1.1.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
</tr>
<tr id="S2.T2.1.1.1.1.2" class="ltx_tr">
<td id="S2.T2.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T2.1.1.1.1.2.1.1" class="ltx_text" style="font-size:80%;">(bps)</span></td>
</tr>
</table>
</th>
<th id="S2.T2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S2.T2.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.2.2.2.1.1" class="ltx_tr">
<td id="S2.T2.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S2.T2.2.2.2.1.1.1.1" class="ltx_text ltx_font_bold">Log F0 RMSE<math id="S2.T2.2.2.2.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T2.2.2.2.1.1.1.1.m1.1a"><mo stretchy="false" id="S2.T2.2.2.2.1.1.1.1.m1.1.1" xref="S2.T2.2.2.2.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S2.T2.2.2.2.1.1.1.1.m1.1b"><ci id="S2.T2.2.2.2.1.1.1.1.m1.1.1.cmml" xref="S2.T2.2.2.2.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.2.2.2.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
</tr>
</table>
</th>
<th id="S2.T2.3.3.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T2.3.3.3.1" class="ltx_text ltx_font_bold">MOS<math id="S2.T2.3.3.3.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T2.3.3.3.1.m1.1a"><mo stretchy="false" id="S2.T2.3.3.3.1.m1.1.1" xref="S2.T2.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S2.T2.3.3.3.1.m1.1b"><ci id="S2.T2.3.3.3.1.m1.1.1.cmml" xref="S2.T2.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.3.3.3.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
</tr>
<tr id="S2.T2.3.4.1" class="ltx_tr">
<td id="S2.T2.3.4.1.1" class="ltx_td ltx_align_left ltx_border_t">baseline</td>
<td id="S2.T2.3.4.1.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S2.T2.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t">0.19</td>
<td id="S2.T2.3.4.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S2.T2.3.5.2" class="ltx_tr">
<td id="S2.T2.3.5.2.1" class="ltx_td ltx_align_left ltx_border_bb">DOVSinger</td>
<td id="S2.T2.3.5.2.2" class="ltx_td ltx_align_center ltx_border_bb">725.9</td>
<td id="S2.T2.3.5.2.3" class="ltx_td ltx_align_center ltx_border_bb">0.19</td>
<td id="S2.T2.3.5.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">2.63</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Table <a href="#S2.T2" title="Table 2 ‣ 2.4 Experimental Results ‣ 2 SVS Track: The DOVSinger System ‣ The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the SVS results and all the data is sourced from the leaderboard. From the table we could found that, at the expense of reducing bitrate while simultaneously ensuring accuracy in pitch, DOVSinger sacrifices perceptual quality. One reason is the utilization of only five layers of DAC code for reconstruction, resulting in noticeable hoarseness in the final vocal output. Another factor is the missed opportunity for improvement by directly employing the DAC decoder for reconstruction, since employing a vocoder for decoding, while incorporating the loss of Mel-spectrogram reconstruction during training, could enhance the quality of the generated vocals.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>ASR Track</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Discrete Tokens for ASR</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Speech discrete tokens for ASR can be broadly categorized into semantic tokens and acoustic tokens based on linguistic information or acoustic details of the speech.
Specifically, models including HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> generate semantic tokens, which are trained for discrimination tasks or masking predictions while audio neural codec models like Encodec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> yield acoustic tokens aimed at speech reconstruction.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>ASR with Discretized Input</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2404.06079/assets/figure/ASR.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="340" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Illustration of the pipeline for speech discrete tokens in the ASR track.</figcaption>
</figure>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Speech Discretization</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">In the ASR track, we used WavLM-large to extract features, and then applied k-means with 2000 classes to obtain discrete tokens.
Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, we performed the data augmentation techniques for discretized inputs to enhance robustness for discrete token representations.
Along with their corresponding texts, we leveraged Zipformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> as the encoder and RNN-T loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> for optimization to train an end-to-end ASR model.
These tokens were projected to 80 dimensions through a linear embedding layer.
Then, these features were duplicated twice frame by frame to a uniform 100 Hz rate before feeding into the ASR model.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Model Architecture</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">The neural Transducer architecture was adopted for ASR. Pruned RNN-T loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> was used as the training objective function, implemented within the k2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> framework<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://github.com/k2-fsa/k2</span></span></span>. The encoder employed a 6-stack Zipformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> with downsampling factors of (1,2,4,8,4,2). The label decoder employed a stateless decoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, which consists of an embedding layer followed by a 512-dim Conv1D layer.
A convolution subsampling module with a stride of 2 was placed to reduce the frame rate to 50 Hz before being fed into the encoder. Overall, the model had 65.5M parameters.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Training</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">We trained the acoustic model for 100 epochs using the ScaledAdam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> optimizer at a base learning rate of 0.045. We utilized 500-class Byte Pair Encoding (BPE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> word pieces as the classification units.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Experimental Results</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The ASR track used LibriSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and ML-SUPERB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> corpora to evaluate.
Performance metrics include character error rate (CER) and bitrate.
As illustrated in Table <a href="#S3.T3" title="Table 3 ‣ 3.3 Experimental Results ‣ 3 ASR Track ‣ The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, our system achieved a relative CER reduction of up to 13.0% compared to the baseline, albeit at a higher bitrate of 550 bps.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>CER and bitrate of our system in the ASR track.</figcaption>
<table id="S3.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.2.2" class="ltx_tr">
<th id="S3.T3.2.2.3" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S3.T3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S3.T3.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T3.1.1.1.1.1" class="ltx_tr">
<td id="S3.T3.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T3.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Bitrate (bps)<math id="S3.T3.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T3.1.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T3.1.1.1.1.1.1.1.m1.1.1" xref="S3.T3.1.1.1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
</tr>
</table>
</th>
<th id="S3.T3.2.2.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<table id="S3.T3.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T3.2.2.2.1.1" class="ltx_tr">
<td id="S3.T3.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T3.2.2.2.1.1.1.1" class="ltx_text ltx_font_bold">EN_LibriSpeech CER (%)<math id="S3.T3.2.2.2.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T3.2.2.2.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T3.2.2.2.1.1.1.1.m1.1.1" xref="S3.T3.2.2.2.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.1.1.1.1.m1.1b"><ci id="S3.T3.2.2.2.1.1.1.1.m1.1.1.cmml" xref="S3.T3.2.2.2.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.2.3.1" class="ltx_tr">
<th id="S3.T3.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">baseline</th>
<td id="S3.T3.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">335.86</td>
<td id="S3.T3.2.3.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">2.31</td>
</tr>
<tr id="S3.T3.2.4.2" class="ltx_tr">
<th id="S3.T3.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Zipformer</th>
<td id="S3.T3.2.4.2.2" class="ltx_td ltx_align_center ltx_border_bb">550.00</td>
<td id="S3.T3.2.4.2.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S3.T3.2.4.2.3.1" class="ltx_text ltx_font_bold">2.01</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We present the systems for TTS, ASR and SVS track in the discrete speech unit challenge.
Detailed model designs and choices of discrete tokens are illustrated.
Specifically, we use FunCodec and an improved VQTTS model for TTS to achieve a low bitrate yet highly competitive TTS system that ranks the first.
DAC and VALL-E are adopted in the SVS track, while k-means clusters of WavLM and Zipformer are utilized in ASR track.
We hope our experimental findings could promote better understanding and utilization of the discrete speech tokens among the speech community.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Z. Du, S. Zhang, K. Hu, and S. Zheng, ``Funcodec: A fundamental, reproducible and integrable open-source toolkit for neural speech codec,'' <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.07405</em>, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
C. Du, Y. Guo, X. Chen, and K. Yu, ``VQTTS: high-fidelity text-to-speech synthesis with self-supervised VQ acoustic feature,'' in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Interspeech 2022</em>.   ISCA, 2022, pp. 1596–1600.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi, and N. Zeghidour, ``AudioLM: A language modeling approach to audio generation,'' <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 31, pp. 2523–2533, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
E. Kharitonov, D. Vincent, Z. Borsos, R. Marinier, S. Girgin, O. Pietquin, M. Sharifi, M. Tagliasacchi, and N. Zeghidour, ``Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision,'' <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, vol. 11, pp. 1703–1718, 12 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Neural codec language models are zero-shot text to speech synthesizers,'' <em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.02111</em>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
C. Du, Y. Guo, F. Shen, Z. Liu, Z. Liang, X. Chen, S. Wang, H. Zhang, and K. Yu, ``UniCATS: A unified context-aware text-to-speech framework with contextual vq-diffusion and vocoding,'' in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">AAAI Conference on Artificial Intelligence</em>, 2024.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
X. Zhu, Y. Lv, Y. Lei, T. Li, W. He, H. Zhou, H. Lu, and L. Xie, ``Vec-Tok Speech: speech vectorization and tokenization for neural speech generation,'' <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.07246</em>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y. Song, Z. Chen, X. Wang, Z. Ma, and X. Chen, ``ELLA-V: Stable neural codec language modeling with alignment-guided sequence reordering,'' <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.07333</em>, 2024.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, ``wav2vec 2.0: A framework for self-supervised learning of speech representations,'' in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 33.   Curran Associates, Inc., 2020, pp. 12 449–12 460.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P. Mazaré, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux, ``Libri-Light: A benchmark for ASR with limited or no supervision,'' in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>.   IEEE, 2020, pp. 7669–7673.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. Baevski, S. Schneider, and M. Auli, ``vq-wav2vec: Self-supervised learning of discrete speech representations,'' in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, ``HuBERT: Self-supervised speech representation learning by masked prediction of hidden units,'' <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 29, pp. 3451–3460, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Défossez, J. Copet, G. Synnaeve, and Y. Adi, ``High fidelity neural audio compression,'' in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.13438</em>, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J. Kong, J. Kim, and J. Bae, ``HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis,'' in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proc. NeurIPS</em>, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
K. Ito and L. Johnson, ``The LJ Speech Dataset,'' <a target="_blank" href="https://keithito.com/LJ-Speech-Dataset/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://keithito.com/LJ-Speech-Dataset/</a>, 2017.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``The Kaldi speech recognition toolkit,'' in <em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic">IEEE 2011 workshop on automatic speech recognition and understanding</em>.   IEEE Signal Processing Society, 2011.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
K. J. Shih, R. Valle, R. Badlani, A. Lancucki, W. Ping, and B. Catanzaro, ``RAD-TTS: Parallel flow-based TTS with robust alignment learning and diverse synthesis,'' in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models</em>, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and Y. Wu, ``LibriTTS: A corpus derived from librispeech for text-to-speech,'' in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Interspeech 2019</em>.   ISCA, 2019, pp. 1526–1530.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. Mcleavey, and I. Sutskever, ``Robust speech recognition via large-scale weak supervision,'' in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine Learning</em>, ser. Proceedings of Machine Learning Research.   PMLR, 23–29 Jul 2023, pp. 28 492–28 518.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
T. Saeki, D. Xin, W. Nakata, T. Koriyama, S. Takamichi, and H. Saruwatari, ``UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022,'' in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2022</em>, 2022, pp. 4521–4525.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Y. Wang, X. Wang, P. Zhu, J. Wu, H. Li, H. Xue, Y. Zhang, L. Xie, and M. Bi, ``Opencpop: A high-quality open source chinese popular song corpus for singing voice synthesis,'' in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Interspeech 2022</em>.   ISCA, 2022, pp. 4242–4246.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
R. Huang, C. Zhang, Y. Wang, D. Yang, L. Liu, Z. Ye, Z. Jiang, C. Weng, Z. Zhao, and D. Yu, ``Make-A-Voice: Unified voice synthesis with discrete representation,'' <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.19269</em>, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi, ``SoundStream: An end-to-end neural audio codec,'' <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 30, pp. 495–507, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
R. Kumar, P. Seetharaman, A. Luebs, I. Kumar, and K. Kumar, ``High-fidelity audio compression with improved RVQGAN,'' <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 36, 2024.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Z. Rafii, A. Liutkus, F.-R. Stöter, S. I. Mimilakis, and R. M. Bittner, ``MUSDB18 - a corpus for music separation,'' 2017. [Online]. Available: <a target="_blank" href="https://api.semanticscholar.org/CorpusID:199578935" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:199578935</a>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``WavLM: Large-scale self-supervised pre-training for full stack speech processing,'' <em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, vol. 16, 2022.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Y. Yang, F. Shen, C. Du, Z. Ma, K. Yu, D. Povey, and X. Chen, ``Towards universal speech discrete tokens: A case study for asr and tts,'' in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2024, pp. 10 401–10 405.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Z. Yao, L. Guo, X. Yang, W. Kang, F. Kuang, Y. Yang, Z. Jin, L. Lin, and D. Povey, ``Zipformer: A faster and better encoder for automatic speech recognition,'' in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>, 2024.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A. Graves, A. Mohamed, and G. E. Hinton, ``Speech recognition with deep recurrent neural networks,'' in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, Vancouver, 2013.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
F. Kuang, L. Guo, W. Kang, L. Lin, M. Luo, Z. Yao, and D. Povey, ``Pruned RNN-T for fast, memory-efficient asr training,'' in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, Incheon, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
D. Povey, P. Zelasko, and S. Khudanpur, ``Speech recognition with next-generation kaldi (k2, lhotse, icefall),'' in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Interspeech: tutorials</em>, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
M. Ghodsi, X. Liu, J. Apfel, R. Cabrera, and E. Weinstein, ``Rnn-Transducer with stateless prediction network,'' in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, Barcelona, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
R. Sennrich, B. Haddow, and A. Birch, ``Neural machine translation of rare words with subword units,'' in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proc. ACL</em>, Berlin, 2016.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, ``LibriSpeech: an asr corpus based on public domain audio books,'' in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, South Brisbane, 2015.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
J. Shi, D. Berrebbi, W. Chen, H.-L. Chung, E.-P. Hu, W. P. Huang, X. Chang, S.-W. Li, A. Mohamed, H.-y. Lee <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``ML-SUPERB: Multilingual speech universal performance benchmark,'' <em id="bib.bib35.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.10615</em>, 2023.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.06078" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.06079" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.06079">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.06079" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.06081" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 17:21:53 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
