<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.08403] Ethics of Generating Synthetic MRI Vocal Tract Views from the Face CVPR Responsible Generative AI Workshop</title><meta property="og:description" content="Forming oral models capable of understanding the complete dynamics of the oral cavity is vital across research areas such as speech correction, designing foods for the aging population, and dentistry. Magnetic resonanc…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Ethics of Generating Synthetic MRI Vocal Tract Views from the Face CVPR Responsible Generative AI Workshop">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Ethics of Generating Synthetic MRI Vocal Tract Views from the Face CVPR Responsible Generative AI Workshop">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.08403">

<!--Generated on Mon Aug  5 14:05:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Ethics of Generating Synthetic MRI Vocal Tract Views from the Face
<br class="ltx_break">CVPR Responsible Generative AI Workshop</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Muhammad Suhaib Shahid
<br class="ltx_break">University of Nottingham
<br class="ltx_break">NG8 1BB, UK 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Muhammad.Shahid@nottingham.ac.uk</span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gleb E. Yakubov
<br class="ltx_break">University of Nottingham
<br class="ltx_break">LE12 5RD, UK 
<br class="ltx_break">
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andrew P. French
<br class="ltx_break">University of Nottingham
<br class="ltx_break">NG8 1BB, UK
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Forming oral models capable of understanding the complete dynamics of the oral cavity is vital across research areas such as speech correction, designing foods for the aging population, and dentistry. Magnetic resonance imaging (MRI) technologies, capable of capturing oral data essential for creating such detailed representations, offer a powerful tool for illustrating articulatory dynamics. However, its real-time application is hindered by expense and expertise requirements. Ever advancing generative AI approaches present themselves as a way to address this barrier by leveraging multi-modal approaches for generating pseudo-MRI views. Nonetheless, this immediately sparks ethical concerns regarding the utilisation of a technology with the capability to produce MRIs from facial observations.</p>
<p id="id3.id2" class="ltx_p">This paper explores the ethical implications of external-to-internal correlation modeling (E2ICM). E2ICM utilises facial movements to infer internal configurations and provides a cost-effective supporting technology for MRI. In this preliminary work, we employ Pix2PixGAN to generate pseudo-MRI views from external articulatory data, demonstrating the feasibility of this approach. Ethical considerations concerning privacy, consent, and potential misuse, which are fundamental to our examination of this innovative methodology, are discussed as a result of this experimentation.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The ability to model the complete oral cavity holds significant utility across various domains, notably in dentistry, where understanding how dental prosthetics impact speech and mastication is crucial in forming personalised dental devices. However, achieving comprehensive oral cavity modeling presents challenges, including limitations in technique availability, articulator capture capacity, and associated costs. Researchers often face the dilemma of selecting a suitable technique tailored to their specific objectives. Among the available methods, magnetic resonance imaging (MRI) stands out for its capability to provide detailed representations of all articulators, particularly when augmented with real-time functionality, offering insights into dynamic movements. Nonetheless, the practicality of real-time MRI is hindered by its costliness and the need for specialised expertise, rendering it less feasible for routine use.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">This raises the question, is it possible to use generative AI approaches to achieve a complete model of the oral cavity, encompassing all articulators in motion, without incurring excessive expenses? This is where the concept of external-to-internal correlation modeling (E2ICM) emerges as a potential solution. By observing the external facial movements, particularly those of visible articulators such as the lips, and jaw, we investigate if we can in any way reconstruct the internal configurations of the oral cavity. This approach leverages the inherent relationship between external facial gestures and internal vocal tract configurations. Such an approach aims to address the cost and complexity concerns associated with MRI and other experimental techniques. Clearly there are limitations to this approach, but here we consider exploring the feasibility of such a technology, and bring to the fore the ethical questions such an approach might raise.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">As advancements in AI-based approaches continue to progress, questions regarding ethical implications become increasingly pertinent. The ability to record or photograph individuals during articulation and mastication, followed by the generation of MRI-like images of the internal oral cavity, raises several ethical considerations and potential concerns regarding privacy, consent, and misuse.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">This paper explores the application of generative deep learning models to create pseudo-MRI views of the oral cavity. Specifically, it employs the Pix2PixGAN network to transform external views of a participant during articulation into predicted MRI representations, in a limited speech-reconstruction scenario. For the purpose of demonstrating the feasibility of the proposed approach we briefly evaluate the challenges associated with determining the quality of the generated images. This is followed by a discussion focused on possible ethical dilemmas associated with the use of such generated data.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Real time MRI (RtMRI) of the vocal tract is one of the very few techniques capable of displaying, frame by frame, the movements of all articulators during speech<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>. The method allows researchers to explore a wide range of applications from articulatory studies, to oral health and food oral processing. Despite the prospects RtMRI presents, there are some underpinning issues that hinder its widespread use. These limitations are a result of the cost and expertise requirements for collecting RtMRI data on an individual subject basis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>. One possible generative solution to this is by forming predictive models capable of using external observations of the face to synthesise a representation of the vocal tract MRI view.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The feasibility of such an approach relies on investigating the interrelationship between the internal vocal tract and external face views. Such research has explored correlations between the two views by linking facial movements, captured via video, with vocal tract dynamics, captured through rtMRI <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>. The main focus is to identify whether there is sufficient mutual information between the forward coronal view of the face and the sagittal MRI to make reconstruction procedures possible. Employing Principal Component Analysis (PCA), Scholes et al. (2020) simplified the data and identified key patterns of change in both modalities. Through this process, they uncovered connections between facial gestures and vocal tract configurations, showcasing the potential for mutual reconstruction between the two modalities. The findings concluded that facial information may hold sufficient data to recover certain vocal tract shapes during speech production.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">While the PCA-based analysis-by-synthesis technique showcases an interrelationship between the two modalities, it comes short of addressing key barriers that prevent the widespread use of MRI. In order to reconstruct an MRI representation of the vocal tract, a corresponding PCA matrix must accompany each specific external view. However, the PCA representation is derived from the MRI image, and consequently, the MRI data are still necessary each time the representation is created.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Paving the way to addressing this problem are generative machine learning models capable of performing cross-modality synthesis of unseen MRI configurations when presented with a novel face view for a specific individual <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>. This technique is commonly used in computer vision and machine learning to create mappings between different visual styles, attributes, or characteristics. It is the process of transforming an input image from one domain into a corresponding output image in another domain, while preserving meaningful content and maintaining consistency between the two domains; it involves changing how an image looks while keeping its underlying meaning intact. In the application of this task, it would involve shifting from a face view to a MRI vocal tract view for any two paired frames; this pairing being key to the approach.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">The Pix2PixGaN framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> serves as the translation network chosen for this task. The architecture comprises two key components: a generator and a discriminator. The generator works to produce a realistic mapping from the input domain to the desired output, while the discriminator’s role is to determine whether an image is real or synthesised. The generator and discriminator train in an adversarial fashion, each trying to optimise ahead of the other. This approach drives the mapping of images from one domain to another in a supervised manner. Once trained, the system has the potential to operate in an autonomous manner and predict internal views based on the outside image or video only. If successful, such approaches can enable generating synthesised views without specialised equipment and a person’s consent, which raises important ethical questions and considerations that need to be addressed.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Existing research has explored the (bio)ethical considerations surrounding the use of Generative Adversarial Networks (GANs) for generating medical images. The integration of AI technologies in healthcare raises complex legal, ethical, and technical challenges. In their work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>, the authors underscore the necessity for a regulatory framework to ensure the safe integration of generative technologies in medical contexts. A systematic review conducted by <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> examined recent GAN architectures utilised in medical image analysis, revealing imbalances in their capabilities, particularly with smaller datasets. These findings align with the observations of <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>, regarding the imbalanced class distributions often observed in datasets, thereby raising ethical concerns.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Framework and Implementation</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this study we used a dual-modal dataset used for this study, comprising registered videos captured during speech, this has been previously published<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>. Initial data collection involved 13 participants articulating a predetermined set of 10 sentences. Participants underwent two recording sessions: first, speaking the sentences in front of a camera, and second, repeating the same sentences during MRI scans. Subsequently, these video sets were then aligned. Initially, data from 13 participants were collected for the study. However, only data from 11 participants were ultimately included in the published datasets as the study focused on British English speakers. The dataset encompasses videos providing a frontal view of the face alongside sagittal MRI views. For the purposes of this preliminary study, only data from one subject was utilised, as they were the only participant for whom all 10 videos were available across all sentences. Across these 10 videos, a total of 461 frames were available, considering the videos were recorded at a frame rate of 15 frames per second (fps). The shortest video contained 30 frames, while the longest comprised 59 frames.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.4" class="ltx_p">An implementation of Pix2PixGaN framework was used as the image-to-image translation network. Based on the conditional generative adversarial network (CGaN), the architecture consists of a generator and a discriminator. The generator aims to produce realistic images based on the input, while the discriminator’s job is to distinguish between real and generated images. The generator employs a U-Net-inspired encoder-decoder architecture with skip connections. The encoder module is formed of only convolutional layers, omitting dropout. This structure forms the following sequence of layers: C64-C128-C256-C512-C512-C512-C512-C512. The decoder integrates dropout layers with a dropout rate of 0.5 in the first, second, and third layers. The decoder’s structure is as follows: CD512-CD512-CD512-C512-C256-C128-C64. This combination establishes a proficient generator capable of producing coherent translations for this dataset. The model was optimised using the Adam optimiser with hyperparameters <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">\alpha</annotation></semantics></math> = 0.0002, <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="\beta_{1}" display="inline"><semantics id="S3.p2.2.m2.1a"><msub id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><mi id="S3.p2.2.m2.1.1.2" xref="S3.p2.2.m2.1.1.2.cmml">β</mi><mn id="S3.p2.2.m2.1.1.3" xref="S3.p2.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.p2.2.m2.1.1.2.cmml" xref="S3.p2.2.m2.1.1.2">𝛽</ci><cn type="integer" id="S3.p2.2.m2.1.1.3.cmml" xref="S3.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">\beta_{1}</annotation></semantics></math> = 0.5, <math id="S3.p2.3.m3.1" class="ltx_Math" alttext="\beta_{2}" display="inline"><semantics id="S3.p2.3.m3.1a"><msub id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml"><mi id="S3.p2.3.m3.1.1.2" xref="S3.p2.3.m3.1.1.2.cmml">β</mi><mn id="S3.p2.3.m3.1.1.3" xref="S3.p2.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><apply id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.1.cmml" xref="S3.p2.3.m3.1.1">subscript</csymbol><ci id="S3.p2.3.m3.1.1.2.cmml" xref="S3.p2.3.m3.1.1.2">𝛽</ci><cn type="integer" id="S3.p2.3.m3.1.1.3.cmml" xref="S3.p2.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">\beta_{2}</annotation></semantics></math> = 0.999, and <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.p2.4.m4.1a"><mi id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><ci id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">\epsilon</annotation></semantics></math> = 1e-08. The training was done with a batch size of 16, for 200 epochs. Tanh activation function was used.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The Fréchet Inception Distance (FID) metric and Structural Similarity Index Measure (SSIM) were used to accesses the quality of image examination. FID provides a quantitative measure of similarity between the distribution of generated and authentic images, with lower scores indicating higher quality. SSIM considers the structural information of images, accounting for spatial relationships beyond pixel values. Additionally, a qualitative evaluation was conducted by observing the movements of each articulator frame by frame, drawing conclusions regarding which articulators are constructed most effectively.
The FID score for generated images compared to ground truth is 30.80, though establishing an understanding of what a ”good” FID score is when transitioning from RGB to MRI domains remains challenging. To gain some insight, an FID was calculated for various ground truth frames to assess how well the FID performs with real images but of different vocal tract views, yielding a score of 19.75. While FID offers valuable insight into image similarity, it doesn’t directly consider the spatial representation of vocal tract structure. Therefore, SSIM might be more suitable for this task. The average SSIM score for all 46 test images was 0.7961, with higher scores indicating better image quality on a scale from -1 to 1. We recognise, and highlight, that interpreting these scores in this application domain is challenging.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">As illustrated in Figure 1, the generative models demonstrate some proficiency in generating images with realistic appearances, particularly showcasing discernible movements in the jaw regions. However, upon closer examination, specific articulatory details are challenging to determine. Despite reasonable FID and SSIM scores indicating overall good image similarity, inconsistencies between generated articulators and ground truth are apparent in certain frames. These discrepancies could potentially lead to misleading interpretations in clinical applications, where images resembling plausible MRI scans but with incorrect articulator configurations may pose risks. Moving forward, vocal tract segmentation could serve as a promising avenue for enhancing the clinical relevance in assessing the quality of generated vocal tract views <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>.</p>
</div>
<figure id="S4.F1" class="ltx_figure"><img src="/html/2407.08403/assets/combined_grid_image_3.png" id="S4.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S4.F1.3.2" class="ltx_text" style="font-size:90%;">Still frames sample the external view (left), ground truth MRI frame (middle) and generated frame (right).</span></figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion of Ethics and Responsible Use</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The ethical dimensions surrounding the potential of such generative medical approaches demand scrutiny. There are concerns associated with the generation and utilisation of synthetic views, the accuracy and reliability of the data, and potential misuse.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Synthetic dataset enlargement</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The fundamental principle of informed consent and participant autonomy is central. Ethical protocols governing the collection of MRI data are universally stringent, dictating both the type of data collected and its storage practices. Participants are provided with comprehensive information about the study, including potential risks and the intended utilisation of their data, to make informed decisions regarding consent. The integration of generative AI raises logistical and ethical considerations perhaps not originally conceived. In a practical application, MRI data might initially be collected for a limited set of sentences, such as the 10 here. Subsequently, using generative techniques, additional MRI data could be synthesised for sentences that were not originally captured via MRI. When using a trained model, it’s possible to employ readily available facial data, especially from public spaces where recording may be allowed by law. However, the ethical dilemma arises: Is it acceptable and responsible to generate new data modalities without obtaining explicit consent? While enlarging datasets using generative AI techniques is not novel and has been applied in various domains, the unique aspect here lies in the translation from an external view to an internal view. Accessing and utilising facial data for such a task, even if publicly available, must be carefully assessed to uphold principles of privacy.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Accuracy of generated images</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Responsible use of generative AI necessitates addressing concerns surrounding the accuracy and integrity of synthesised data. As demonstrated here, methods such as FID are employed to help assess the ”quality” of images. However, it is evident from the outset that these methods do not adequately evaluate specific spatial information in the generated images. In other words, the morphology of clinically-relevant structures is not captured well by these metrics. Often, it is also hard to identify subtle features even directly in the dataset (see Figure 1), so interpreting the quality of synthesised data in this domain is a challenge.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Questions will arise regarding the potential misuse or misinterpretation of inaccurate synthetic information. Poorly performing models could lead to misdiagnosis or misinterpretation. Therefore, it is imperative to remain vigilant and implement rigorous validation procedures to ensure the reliability and accuracy of synthesised data. More work is needed to develop approaches to assess the usefulness and trustworthiness of generated images.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Data storage of generated images</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">The stringent protocols governing the storage and anonymisation of MRI data are imperative to safeguard individuals’ sensitive health information. However, the creation of <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_italic">additional</span> synthetic MRI data, which may still contain identifiable features or morphology, may not always undergo comparable protocol scrutiny as the original data. While large scale MRI datasets could potentially advance medical research and clinical applications, relaxed protocols for synthesised data may compromise privacy and data security. Thus, careful ethical consideration is warranted here for future research.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Furthermore, there are broader societal implications to consider, particularly regarding the potential impact of synthesised medical views on areas such as identity verification. If such technology were to be deployed in contexts such as security or law enforcement, there could be implications for individuals’ rights and freedoms, including the risk of discrimination or misuse of biometric data.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Dataset and model biases</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">The publicly available dataset used in this study exhibited a bias towards speakers of British English. Though it is understandable from the associated papers that this is likely an attempt to standardise an already small and challenging data set, it nevertheless highlights concerns regarding potential biases in future datasets and the models trained upon them. Certain demographic groups may be favoured in inferences, while for others the model could perform poorly. Likewise, the application of the model would likely be limited to the application studied in the dataset (e.g. speech versus chewing, for example); wrong application would lead to misleading results.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">The concern extends beyond data representation to the broader implications for societal equity and fairness. Though this is a problem not only relevant to this application, occurrence of such a scenario could also exacerbate existing disparities in access to resources and opportunities, with models not being tailored to regional use. Proactive measures must be implemented to mitigate bias and promote inclusivity in dataset curation and model development. Strategies may include diversifying dataset sources to encompass a broader spectrum of linguistic and cultural backgrounds, and implementing robust validation techniques to identify and mitigate bias in model predictions.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">A demonstration of an exploratory method for generating MRI images of the vocal tract has been presented. Leveraging the Pix2PixGAN architecture, this study demonstrates the application of Generative AI to synthesise previously-unseen vocal tract configurations from external facial views. The quality of the generated images has been evaluated using the Fréchet Inception Distance (FID) metric, alongside the observation of distinct articulator movements. Results are by no means conclusive at this stage, but certainly raise the question of whether this is a valid line of research in generative AI for future researchers.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Furthermore, an initial discussion regarding the responsible use of generative AI in such applications has been provided. This discussion presents considerations that must be taken into account when employing such techniques. These encompass various aspects, including the enlargement of synthetic datasets, the accuracy of generated images, the storage protocols employed, and the potential biases inherent in both the dataset and the models utilised.


<span id="S6.p2.1.1" class="ltx_text" style="font-size:90%;"></span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Isola et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Image-to-Image Translation with Conditional Adversarial Networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib1.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib1.11.3" class="ltx_text" style="font-size:90%;">, pages 5967–5976, Honolulu, HI, 2017. IEEE.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Jeong et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Jiwoong J. Jeong, Amara Tariq, Tobiloba Adejumo, Hari Trivedi, Judy W. Gichoya, and Imon Banerjee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">Systematic Review of Generative Adversarial Networks (GANs) for Medical Image Classification and Segmentation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib2.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Digital Imaging</em><span id="bib.bib2.10.2" class="ltx_text" style="font-size:90%;">, 35(2):137–152, 2022.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.4.4.1" class="ltx_text" style="font-size:90%;">Kochetov [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.6.1" class="ltx_text" style="font-size:90%;">
Alexei Kochetov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">Research methods in articulatory phonetics I: Introduction and studying oral gestures.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib3.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Language and Linguistics Compass</em><span id="bib.bib3.9.2" class="ltx_text" style="font-size:90%;">, 14(4):e12368, 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.10.1" class="ltx_text" style="font-size:90%;">_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/lnc3.12368.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Makhlouf et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Ahmed Makhlouf, Marina Maayah, Nada Abughanam, and Cagatay Catal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">The use of generative adversarial networks in medical image augmentation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib4.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Neural Computing and Applications</em><span id="bib.bib4.10.2" class="ltx_text" style="font-size:90%;">, 35(34):24055–24068, 2023.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Paladugu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Phani Srivatsav Paladugu, Joshua Ong, Nicolas Nelson, Sharif Amit Kamran, Ethan Waisberg, Nasif Zaman, Rahul Kumar, Roger Daglius Dias, Andrew Go Lee, and Alireza Tavakkoli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Generative Adversarial Networks in Medicine: Important Considerations for this Emerging Innovation in Artificial Intelligence.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib5.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Annals of Biomedical Engineering</em><span id="bib.bib5.10.2" class="ltx_text" style="font-size:90%;">, 51(10):2130–2142, 2023.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Ramanarayanan et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Vikram Ramanarayanan, Sam Tilsen, Michael Proctor, Johannes Töger, Louis Goldstein, Krishna S. Nayak, and Shrikanth Narayanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">Analysis of speech production real-time MRI.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib6.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Speech and Language</em><span id="bib.bib6.10.2" class="ltx_text" style="font-size:90%;">, 52:1–22, 2018.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.4.4.1" class="ltx_text" style="font-size:90%;">Scholes and Skipper [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.6.1" class="ltx_text" style="font-size:90%;">
Chris Scholes and Jeremy I. Skipper.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">The inter-relationship between the face and vocal-tract configuration during audio-visual speech.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.9.1" class="ltx_text" style="font-size:90%;">Publisher: OSF.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Scholes et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Chris Scholes, Jeremy I. Skipper, and Alan Johnston.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">The interrelationship between the face and vocal tract configuration during audiovisual speech.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib8.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the National Academy of Sciences</em><span id="bib.bib8.10.2" class="ltx_text" style="font-size:90%;">, 117(51):32791–32798, 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.11.1" class="ltx_text" style="font-size:90%;">Publisher: Proceedings of the National Academy of Sciences.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Shahid et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Muhammad Suhaib Shahid, Andrew P French, Michel F. Valstar, and Gleb E. Yakubov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">Research in Methodologies for Modelling the Oral Cavity.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib9.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Biomedical Physics &amp; Engineering Express</em><span id="bib.bib9.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Tiede et al. [2000]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
M.K. Tiede, Shinobu Masaki, and Eric Vatikiotis-Bateson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">Contrasts in speech articulation observed in sitting and supine conditions.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib10.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 5th Seminar on Speech Production</em><span id="bib.bib10.10.2" class="ltx_text" style="font-size:90%;">, pages 25–28, 2000.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Xie et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Guoyang Xie, Yawen Huang, Jinbao Wang, Jiayi Lyu, Feng Zheng, Yefeng Zheng, and Yaochu Jin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">Cross-modality Neuroimage Synthesis: A Survey.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib11.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM Computing Surveys</em><span id="bib.bib11.10.2" class="ltx_text" style="font-size:90%;">, 56(3):80:1–80:28, 2023.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.08402" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.08403" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.08403">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.08403" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.08404" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 14:05:24 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
