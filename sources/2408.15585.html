<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.15585] Whisper-PMFA: Partial Multi-Scale Feature Aggregation for Speaker Verification using Whisper Models</title><meta property="og:description" content="In this paper, Whisper, a large-scale pre-trained model for automatic speech recognition, is proposed to apply to speaker verification.
A partial multi-scale feature aggregation (PMFA) approach is proposed based on a sâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Whisper-PMFA: Partial Multi-Scale Feature Aggregation for Speaker Verification using Whisper Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Whisper-PMFA: Partial Multi-Scale Feature Aggregation for Speaker Verification using Whisper Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.15585">

<!--Generated on Thu Sep  5 12:26:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=1]YiyangZhao
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=2]ShuaiWang
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=3]GuangzhiSun
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>[affiliation=1]ZehuaChen
<span id="p1.3.4" class="ltx_ERROR undefined">\name</span>[affiliation=1]ChaoZhang
<span id="p1.3.5" class="ltx_ERROR undefined">\name</span>[affiliation=1]
<br class="ltx_break">MingxingXu
<span id="p1.3.6" class="ltx_ERROR undefined">\name</span>[affiliation=1*]Thomas FangZheng




</p>
</div>
<h1 class="ltx_title ltx_title_document">Whisper-PMFA: Partial Multi-Scale Feature Aggregation for 
<br class="ltx_break">Speaker Verification using Whisper Models</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">In this paper, Whisper, a large-scale pre-trained model for automatic speech recognition, is proposed to apply to speaker verification.
A partial multi-scale feature aggregation (PMFA) approach is proposed based on a subset of Whisper encoder blocks to derive highly discriminative speaker embeddings.
Experimental results demonstrate that using the middle to later blocks of the Whisper encoder keeps more speaker information. On the VoxCeleb1 and CN-Celeb1 datasets, our system achieves 1.42% and 8.23% equal error rates (EERs) respectively,
receiving 0.58% and 1.81% absolute EER reductions over the ECAPA-TDNN baseline, and 0.46% and 0.97% over the ResNet34 baseline. Furthermore, our results indicate that using Whisper models trained on multilingual data can effectively enhance the model's robustness across languages. Finally, the low-rank adaptation approach is evaluated, which reduces the trainable model parameters by approximately 45 times while only slightly increasing EER by 0.2%.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Our source code will be released in Wespeaker.</span></span></span></p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>speaker verification, Whisper, LoRA, speaker recognition, multilingual
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Speaker verification (SV) is crucial for biometric authentication, aiming to confirm a person's identity based on their voice characteristics. In the past decade, the advent of deep learning has led to significant advancements in speaker verification technology<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Models such as the convolutional neural network-based Residual Network (ResNet) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, the time-delay neural network-based ECAPA-TDNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, and their diverse variants<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, alongside multi-scale feature fusion models like MFA-Conformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, have significantly contributed to the development of the field. Meanwhile, novel training methods, including loss functions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, strategic training approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, and score normalization techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, have also considerably enhanced the performance of speaker verification systems.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, the increasing complexity of model architectures intensifies the demand for training data<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Acquiring this data, particularly labelled datasets, is far from trivial, this challenge is further exacerbated in the realm of low-resource languages. To address these challenges, researchers have proposed numerous solutions, a widely used strategy involves leveraging large pre-trained models trained on extensive corpora(e.g. Whisper<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, HuBERT<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, WavLM<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>). The integration of pre-trained models provides a robust foundation for feature extraction and representation learning, thereby alleviating some of the constraints imposed by data scarcity. Berns et al. implemented a speaker change detection task on Whisper and Wav2vec2 by innovatively adding speaker change labels to the training data<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Further, Cai et al. explored the feasibility of applying automatic speech recognition (ASR) models to speaker verification tasks by testing an ASR-pretrained Conformer model in speaker verification scenarios<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">As a large pre-trained ASR model trained on extensive corpora, Whisper has been extensively trained on multilingual and diverse situational audio datasets<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. This extensive training grants it impressive performance and robust cross-linguistic features. Accordingly, we adapted it as a pre-trained model for speaker recognition tasks.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">However, due to the fine-tuning paradigm's intensive memory and computational resource requirements, leveraging large-scale models trained on extensive datasets introduces a significant challenge. Addressing this challenge, Sang et al. introduce an adapter approach for adapting self-supervised speech models to speaker verification tasks<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Concurrently, Peng et al. explore parameter-efficient transfer learning methods for adapting pre-trained Transformer models to speaker verification tasks, aiming to reduce the computational burden<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this paper, we propose an effective Whisper-based partial multi-scale feature aggregation model (Whisper-PMFA). Compared to the widely used MFA architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, our approach benefits from partial layer selection, effectively reducing the performance degradation caused by the integration of excessive irrelevant information. This selective process also mitigates the computational and memory overhead caused by full concatenation. Additionally, to address the issue of computational overhead, we explored the use of the low-rank adaptation (LoRA) approach<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> as an alternative to fine-tuning. The primary contributions of this paper can be summarized as follows:</p>
</div>
<div id="S1.p6" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We developed a partial multi-scale feature aggregation module, adapting the Whisper model for the speaker verification task through selective aggregation of Whisper layers.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">In our enhanced model, by incorporating LoRA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> in place of comprehensive fine-tuning, we significantly reduced the model's trainable parameters by approximately 45 times while only incurring a marginal increase in EER of 0.2%.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Our experiments confirmed Whisper's potential for cross-lingual speaker recognition applications.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">Our evaluations on the VoxCeleb1 and CN-Celeb1 datasets have conclusively demonstrated that the proposed model achieves significant improvements over the baseline models.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2408.15585/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="186" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The overall architecture of Whisper-PMFA, where <math id="S1.F1.3.m1.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S1.F1.3.m1.1b"><mi id="S1.F1.3.m1.1.1" xref="S1.F1.3.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S1.F1.3.m1.1c"><ci id="S1.F1.3.m1.1.1.cmml" xref="S1.F1.3.m1.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.3.m1.1d">S</annotation></semantics></math> denotes the index of the initial Whisper block selected for feature aggregation, and <math id="S1.F1.4.m2.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S1.F1.4.m2.1b"><mi id="S1.F1.4.m2.1.1" xref="S1.F1.4.m2.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S1.F1.4.m2.1c"><ci id="S1.F1.4.m2.1.1.cmml" xref="S1.F1.4.m2.1.1">ğ¸</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.4.m2.1d">E</annotation></semantics></math> represents the index of the final Whisper block selected.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Whisper-PMFA</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, the Whisper-PMFA approach is introduced, aimed at using the rich and diverse speech knowledge obtained from a large amount of training data and embedded in a pre-trained ASR model to assist in the enhancement of speaker verification tasks. It combines the Whisper pre-trained model and aggregation model, with architectural details in Figure 1.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Preliminary of Whisper</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Whisper<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> is a sophisticated, multilingual speech recognition model trained on a vast corpus of 680,000 hours of multilingual and diverse acoustic data. It combines a classical encoder-decoder Transformer<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> architecture, with its encoder comprising two convolutional layers, sinusoidal positional encoding, and a series of Transformer blocks, to effectively handle diverse linguistic and acoustic challenges. In this paper, we utilize the encoder from the Whisper Large-v2 as our pre-trained model, which comprises 32 Transformer blocks, each equipped with an attention mechanism that consists of 20 heads.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Partial Multi-scale Feature Aggregation Model</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Multi-scale feature aggregation (MFA), involves the concatenation of output features from various frame-level modules within a speaker embedding architecture, before pooling at the utterance level. In previous research, MFA-Conformer has already demonstrated its effectiveness on Conformer models<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. However, as the number of layers and the output size increase, concatenating the outputs of all blocks results in significant computational and memory overhead. Moreover, performing full concatenation may also introduce a substantial amount of non-speaker-related information, potentially leading to a degradation in model performance.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Therefore, unlike MFA, we replace the full concatenation operation with the partially selected layer concatenation:</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<table id="S2.E1" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S2.E1X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle\mathbf{H}^{\prime}" display="inline"><semantics id="S2.E1X.2.1.1.m1.1a"><msup id="S2.E1X.2.1.1.m1.1.1" xref="S2.E1X.2.1.1.m1.1.1.cmml"><mi id="S2.E1X.2.1.1.m1.1.1.2" xref="S2.E1X.2.1.1.m1.1.1.2.cmml">ğ‡</mi><mo id="S2.E1X.2.1.1.m1.1.1.3" xref="S2.E1X.2.1.1.m1.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S2.E1X.2.1.1.m1.1b"><apply id="S2.E1X.2.1.1.m1.1.1.cmml" xref="S2.E1X.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.E1X.2.1.1.m1.1.1.1.cmml" xref="S2.E1X.2.1.1.m1.1.1">superscript</csymbol><ci id="S2.E1X.2.1.1.m1.1.1.2.cmml" xref="S2.E1X.2.1.1.m1.1.1.2">ğ‡</ci><ci id="S2.E1X.2.1.1.m1.1.1.3.cmml" xref="S2.E1X.2.1.1.m1.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1X.2.1.1.m1.1c">\displaystyle\mathbf{H}^{\prime}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E1X.3.2.2.m1.4" class="ltx_Math" alttext="\displaystyle=\textrm{Concat}(h_{s},h_{s+1},\cdots,h_{e})" display="inline"><semantics id="S2.E1X.3.2.2.m1.4a"><mrow id="S2.E1X.3.2.2.m1.4.4" xref="S2.E1X.3.2.2.m1.4.4.cmml"><mi id="S2.E1X.3.2.2.m1.4.4.5" xref="S2.E1X.3.2.2.m1.4.4.5.cmml"></mi><mo id="S2.E1X.3.2.2.m1.4.4.4" xref="S2.E1X.3.2.2.m1.4.4.4.cmml">=</mo><mrow id="S2.E1X.3.2.2.m1.4.4.3" xref="S2.E1X.3.2.2.m1.4.4.3.cmml"><mtext id="S2.E1X.3.2.2.m1.4.4.3.5" xref="S2.E1X.3.2.2.m1.4.4.3.5a.cmml">Concat</mtext><mo lspace="0em" rspace="0em" id="S2.E1X.3.2.2.m1.4.4.3.4" xref="S2.E1X.3.2.2.m1.4.4.3.4.cmml">â€‹</mo><mrow id="S2.E1X.3.2.2.m1.4.4.3.3.3" xref="S2.E1X.3.2.2.m1.4.4.3.3.4.cmml"><mo stretchy="false" id="S2.E1X.3.2.2.m1.4.4.3.3.3.4" xref="S2.E1X.3.2.2.m1.4.4.3.3.4.cmml">(</mo><msub id="S2.E1X.3.2.2.m1.2.2.1.1.1.1" xref="S2.E1X.3.2.2.m1.2.2.1.1.1.1.cmml"><mi id="S2.E1X.3.2.2.m1.2.2.1.1.1.1.2" xref="S2.E1X.3.2.2.m1.2.2.1.1.1.1.2.cmml">h</mi><mi id="S2.E1X.3.2.2.m1.2.2.1.1.1.1.3" xref="S2.E1X.3.2.2.m1.2.2.1.1.1.1.3.cmml">s</mi></msub><mo id="S2.E1X.3.2.2.m1.4.4.3.3.3.5" xref="S2.E1X.3.2.2.m1.4.4.3.3.4.cmml">,</mo><msub id="S2.E1X.3.2.2.m1.3.3.2.2.2.2" xref="S2.E1X.3.2.2.m1.3.3.2.2.2.2.cmml"><mi id="S2.E1X.3.2.2.m1.3.3.2.2.2.2.2" xref="S2.E1X.3.2.2.m1.3.3.2.2.2.2.2.cmml">h</mi><mrow id="S2.E1X.3.2.2.m1.3.3.2.2.2.2.3" xref="S2.E1X.3.2.2.m1.3.3.2.2.2.2.3.cmml"><mi id="S2.E1X.3.2.2.m1.3.3.2.2.2.2.3.2" xref="S2.E1X.3.2.2.m1.3.3.2.2.2.2.3.2.cmml">s</mi><mo id="S2.E1X.3.2.2.m1.3.3.2.2.2.2.3.1" xref="S2.E1X.3.2.2.m1.3.3.2.2.2.2.3.1.cmml">+</mo><mn id="S2.E1X.3.2.2.m1.3.3.2.2.2.2.3.3" xref="S2.E1X.3.2.2.m1.3.3.2.2.2.2.3.3.cmml">1</mn></mrow></msub><mo id="S2.E1X.3.2.2.m1.4.4.3.3.3.6" xref="S2.E1X.3.2.2.m1.4.4.3.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.E1X.3.2.2.m1.1.1" xref="S2.E1X.3.2.2.m1.1.1.cmml">â‹¯</mi><mo id="S2.E1X.3.2.2.m1.4.4.3.3.3.7" xref="S2.E1X.3.2.2.m1.4.4.3.3.4.cmml">,</mo><msub id="S2.E1X.3.2.2.m1.4.4.3.3.3.3" xref="S2.E1X.3.2.2.m1.4.4.3.3.3.3.cmml"><mi id="S2.E1X.3.2.2.m1.4.4.3.3.3.3.2" xref="S2.E1X.3.2.2.m1.4.4.3.3.3.3.2.cmml">h</mi><mi id="S2.E1X.3.2.2.m1.4.4.3.3.3.3.3" xref="S2.E1X.3.2.2.m1.4.4.3.3.3.3.3.cmml">e</mi></msub><mo stretchy="false" id="S2.E1X.3.2.2.m1.4.4.3.3.3.8" xref="S2.E1X.3.2.2.m1.4.4.3.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1X.3.2.2.m1.4b"><apply id="S2.E1X.3.2.2.m1.4.4.cmml" xref="S2.E1X.3.2.2.m1.4.4"><eq id="S2.E1X.3.2.2.m1.4.4.4.cmml" xref="S2.E1X.3.2.2.m1.4.4.4"></eq><csymbol cd="latexml" id="S2.E1X.3.2.2.m1.4.4.5.cmml" xref="S2.E1X.3.2.2.m1.4.4.5">absent</csymbol><apply id="S2.E1X.3.2.2.m1.4.4.3.cmml" xref="S2.E1X.3.2.2.m1.4.4.3"><times id="S2.E1X.3.2.2.m1.4.4.3.4.cmml" xref="S2.E1X.3.2.2.m1.4.4.3.4"></times><ci id="S2.E1X.3.2.2.m1.4.4.3.5a.cmml" xref="S2.E1X.3.2.2.m1.4.4.3.5"><mtext id="S2.E1X.3.2.2.m1.4.4.3.5.cmml" xref="S2.E1X.3.2.2.m1.4.4.3.5">Concat</mtext></ci><vector id="S2.E1X.3.2.2.m1.4.4.3.3.4.cmml" xref="S2.E1X.3.2.2.m1.4.4.3.3.3"><apply id="S2.E1X.3.2.2.m1.2.2.1.1.1.1.cmml" xref="S2.E1X.3.2.2.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1X.3.2.2.m1.2.2.1.1.1.1.1.cmml" xref="S2.E1X.3.2.2.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S2.E1X.3.2.2.m1.2.2.1.1.1.1.2.cmml" xref="S2.E1X.3.2.2.m1.2.2.1.1.1.1.2">â„</ci><ci id="S2.E1X.3.2.2.m1.2.2.1.1.1.1.3.cmml" xref="S2.E1X.3.2.2.m1.2.2.1.1.1.1.3">ğ‘ </ci></apply><apply id="S2.E1X.3.2.2.m1.3.3.2.2.2.2.cmml" xref="S2.E1X.3.2.2.m1.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1X.3.2.2.m1.3.3.2.2.2.2.1.cmml" xref="S2.E1X.3.2.2.m1.3.3.2.2.2.2">subscript</csymbol><ci id="S2.E1X.3.2.2.m1.3.3.2.2.2.2.2.cmml" xref="S2.E1X.3.2.2.m1.3.3.2.2.2.2.2">â„</ci><apply id="S2.E1X.3.2.2.m1.3.3.2.2.2.2.3.cmml" xref="S2.E1X.3.2.2.m1.3.3.2.2.2.2.3"><plus id="S2.E1X.3.2.2.m1.3.3.2.2.2.2.3.1.cmml" xref="S2.E1X.3.2.2.m1.3.3.2.2.2.2.3.1"></plus><ci id="S2.E1X.3.2.2.m1.3.3.2.2.2.2.3.2.cmml" xref="S2.E1X.3.2.2.m1.3.3.2.2.2.2.3.2">ğ‘ </ci><cn type="integer" id="S2.E1X.3.2.2.m1.3.3.2.2.2.2.3.3.cmml" xref="S2.E1X.3.2.2.m1.3.3.2.2.2.2.3.3">1</cn></apply></apply><ci id="S2.E1X.3.2.2.m1.1.1.cmml" xref="S2.E1X.3.2.2.m1.1.1">â‹¯</ci><apply id="S2.E1X.3.2.2.m1.4.4.3.3.3.3.cmml" xref="S2.E1X.3.2.2.m1.4.4.3.3.3.3"><csymbol cd="ambiguous" id="S2.E1X.3.2.2.m1.4.4.3.3.3.3.1.cmml" xref="S2.E1X.3.2.2.m1.4.4.3.3.3.3">subscript</csymbol><ci id="S2.E1X.3.2.2.m1.4.4.3.3.3.3.2.cmml" xref="S2.E1X.3.2.2.m1.4.4.3.3.3.3.2">â„</ci><ci id="S2.E1X.3.2.2.m1.4.4.3.3.3.3.3.cmml" xref="S2.E1X.3.2.2.m1.4.4.3.3.3.3.3">ğ‘’</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1X.3.2.2.m1.4c">\displaystyle=\textrm{Concat}(h_{s},h_{s+1},\cdots,h_{e})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
<tr id="S2.E1Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1Xa.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle\mathbf{H}" display="inline"><semantics id="S2.E1Xa.2.1.1.m1.1a"><mi id="S2.E1Xa.2.1.1.m1.1.1" xref="S2.E1Xa.2.1.1.m1.1.1.cmml">ğ‡</mi><annotation-xml encoding="MathML-Content" id="S2.E1Xa.2.1.1.m1.1b"><ci id="S2.E1Xa.2.1.1.m1.1.1.cmml" xref="S2.E1Xa.2.1.1.m1.1.1">ğ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.E1Xa.2.1.1.m1.1c">\displaystyle\mathbf{H}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E1Xa.3.2.2.m1.1" class="ltx_Math" alttext="\displaystyle=\textrm{LayerNorm}(\mathbf{H}^{\prime})" display="inline"><semantics id="S2.E1Xa.3.2.2.m1.1a"><mrow id="S2.E1Xa.3.2.2.m1.1.1" xref="S2.E1Xa.3.2.2.m1.1.1.cmml"><mi id="S2.E1Xa.3.2.2.m1.1.1.3" xref="S2.E1Xa.3.2.2.m1.1.1.3.cmml"></mi><mo id="S2.E1Xa.3.2.2.m1.1.1.2" xref="S2.E1Xa.3.2.2.m1.1.1.2.cmml">=</mo><mrow id="S2.E1Xa.3.2.2.m1.1.1.1" xref="S2.E1Xa.3.2.2.m1.1.1.1.cmml"><mtext id="S2.E1Xa.3.2.2.m1.1.1.1.3" xref="S2.E1Xa.3.2.2.m1.1.1.1.3a.cmml">LayerNorm</mtext><mo lspace="0em" rspace="0em" id="S2.E1Xa.3.2.2.m1.1.1.1.2" xref="S2.E1Xa.3.2.2.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E1Xa.3.2.2.m1.1.1.1.1.1" xref="S2.E1Xa.3.2.2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1Xa.3.2.2.m1.1.1.1.1.1.2" xref="S2.E1Xa.3.2.2.m1.1.1.1.1.1.1.cmml">(</mo><msup id="S2.E1Xa.3.2.2.m1.1.1.1.1.1.1" xref="S2.E1Xa.3.2.2.m1.1.1.1.1.1.1.cmml"><mi id="S2.E1Xa.3.2.2.m1.1.1.1.1.1.1.2" xref="S2.E1Xa.3.2.2.m1.1.1.1.1.1.1.2.cmml">ğ‡</mi><mo id="S2.E1Xa.3.2.2.m1.1.1.1.1.1.1.3" xref="S2.E1Xa.3.2.2.m1.1.1.1.1.1.1.3.cmml">â€²</mo></msup><mo stretchy="false" id="S2.E1Xa.3.2.2.m1.1.1.1.1.1.3" xref="S2.E1Xa.3.2.2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1Xa.3.2.2.m1.1b"><apply id="S2.E1Xa.3.2.2.m1.1.1.cmml" xref="S2.E1Xa.3.2.2.m1.1.1"><eq id="S2.E1Xa.3.2.2.m1.1.1.2.cmml" xref="S2.E1Xa.3.2.2.m1.1.1.2"></eq><csymbol cd="latexml" id="S2.E1Xa.3.2.2.m1.1.1.3.cmml" xref="S2.E1Xa.3.2.2.m1.1.1.3">absent</csymbol><apply id="S2.E1Xa.3.2.2.m1.1.1.1.cmml" xref="S2.E1Xa.3.2.2.m1.1.1.1"><times id="S2.E1Xa.3.2.2.m1.1.1.1.2.cmml" xref="S2.E1Xa.3.2.2.m1.1.1.1.2"></times><ci id="S2.E1Xa.3.2.2.m1.1.1.1.3a.cmml" xref="S2.E1Xa.3.2.2.m1.1.1.1.3"><mtext id="S2.E1Xa.3.2.2.m1.1.1.1.3.cmml" xref="S2.E1Xa.3.2.2.m1.1.1.1.3">LayerNorm</mtext></ci><apply id="S2.E1Xa.3.2.2.m1.1.1.1.1.1.1.cmml" xref="S2.E1Xa.3.2.2.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1Xa.3.2.2.m1.1.1.1.1.1.1.1.cmml" xref="S2.E1Xa.3.2.2.m1.1.1.1.1.1">superscript</csymbol><ci id="S2.E1Xa.3.2.2.m1.1.1.1.1.1.1.2.cmml" xref="S2.E1Xa.3.2.2.m1.1.1.1.1.1.1.2">ğ‡</ci><ci id="S2.E1Xa.3.2.2.m1.1.1.1.1.1.1.3.cmml" xref="S2.E1Xa.3.2.2.m1.1.1.1.1.1.1.3">â€²</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1Xa.3.2.2.m1.1c">\displaystyle=\textrm{LayerNorm}(\mathbf{H}^{\prime})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
<div id="S2.SS2.p4" class="ltx_para ltx_noindent">
<p id="S2.SS2.p4.7" class="ltx_p">where <math id="S2.SS2.p4.1.m1.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S2.SS2.p4.1.m1.1a"><mi id="S2.SS2.p4.1.m1.1.1" xref="S2.SS2.p4.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.1.m1.1b"><ci id="S2.SS2.p4.1.m1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.1.m1.1c">s</annotation></semantics></math> is the first Whisper block number to be selected, and <math id="S2.SS2.p4.2.m2.1" class="ltx_Math" alttext="e" display="inline"><semantics id="S2.SS2.p4.2.m2.1a"><mi id="S2.SS2.p4.2.m2.1.1" xref="S2.SS2.p4.2.m2.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.2.m2.1b"><ci id="S2.SS2.p4.2.m2.1.1.cmml" xref="S2.SS2.p4.2.m2.1.1">ğ‘’</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.2.m2.1c">e</annotation></semantics></math> is the last Whisper block number to be selected. <math id="S2.SS2.p4.3.m3.1" class="ltx_Math" alttext="\mathbf{h}_{i}\in\mathbb{R}^{d\times T}" display="inline"><semantics id="S2.SS2.p4.3.m3.1a"><mrow id="S2.SS2.p4.3.m3.1.1" xref="S2.SS2.p4.3.m3.1.1.cmml"><msub id="S2.SS2.p4.3.m3.1.1.2" xref="S2.SS2.p4.3.m3.1.1.2.cmml"><mi id="S2.SS2.p4.3.m3.1.1.2.2" xref="S2.SS2.p4.3.m3.1.1.2.2.cmml">ğ¡</mi><mi id="S2.SS2.p4.3.m3.1.1.2.3" xref="S2.SS2.p4.3.m3.1.1.2.3.cmml">i</mi></msub><mo id="S2.SS2.p4.3.m3.1.1.1" xref="S2.SS2.p4.3.m3.1.1.1.cmml">âˆˆ</mo><msup id="S2.SS2.p4.3.m3.1.1.3" xref="S2.SS2.p4.3.m3.1.1.3.cmml"><mi id="S2.SS2.p4.3.m3.1.1.3.2" xref="S2.SS2.p4.3.m3.1.1.3.2.cmml">â„</mi><mrow id="S2.SS2.p4.3.m3.1.1.3.3" xref="S2.SS2.p4.3.m3.1.1.3.3.cmml"><mi id="S2.SS2.p4.3.m3.1.1.3.3.2" xref="S2.SS2.p4.3.m3.1.1.3.3.2.cmml">d</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p4.3.m3.1.1.3.3.1" xref="S2.SS2.p4.3.m3.1.1.3.3.1.cmml">Ã—</mo><mi id="S2.SS2.p4.3.m3.1.1.3.3.3" xref="S2.SS2.p4.3.m3.1.1.3.3.3.cmml">T</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.3.m3.1b"><apply id="S2.SS2.p4.3.m3.1.1.cmml" xref="S2.SS2.p4.3.m3.1.1"><in id="S2.SS2.p4.3.m3.1.1.1.cmml" xref="S2.SS2.p4.3.m3.1.1.1"></in><apply id="S2.SS2.p4.3.m3.1.1.2.cmml" xref="S2.SS2.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p4.3.m3.1.1.2.1.cmml" xref="S2.SS2.p4.3.m3.1.1.2">subscript</csymbol><ci id="S2.SS2.p4.3.m3.1.1.2.2.cmml" xref="S2.SS2.p4.3.m3.1.1.2.2">ğ¡</ci><ci id="S2.SS2.p4.3.m3.1.1.2.3.cmml" xref="S2.SS2.p4.3.m3.1.1.2.3">ğ‘–</ci></apply><apply id="S2.SS2.p4.3.m3.1.1.3.cmml" xref="S2.SS2.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p4.3.m3.1.1.3.1.cmml" xref="S2.SS2.p4.3.m3.1.1.3">superscript</csymbol><ci id="S2.SS2.p4.3.m3.1.1.3.2.cmml" xref="S2.SS2.p4.3.m3.1.1.3.2">â„</ci><apply id="S2.SS2.p4.3.m3.1.1.3.3.cmml" xref="S2.SS2.p4.3.m3.1.1.3.3"><times id="S2.SS2.p4.3.m3.1.1.3.3.1.cmml" xref="S2.SS2.p4.3.m3.1.1.3.3.1"></times><ci id="S2.SS2.p4.3.m3.1.1.3.3.2.cmml" xref="S2.SS2.p4.3.m3.1.1.3.3.2">ğ‘‘</ci><ci id="S2.SS2.p4.3.m3.1.1.3.3.3.cmml" xref="S2.SS2.p4.3.m3.1.1.3.3.3">ğ‘‡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.3.m3.1c">\mathbf{h}_{i}\in\mathbb{R}^{d\times T}</annotation></semantics></math> is the output of <math id="S2.SS2.p4.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS2.p4.4.m4.1a"><mi id="S2.SS2.p4.4.m4.1.1" xref="S2.SS2.p4.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.4.m4.1b"><ci id="S2.SS2.p4.4.m4.1.1.cmml" xref="S2.SS2.p4.4.m4.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.4.m4.1c">i</annotation></semantics></math>-th Whisper block, <math id="S2.SS2.p4.5.m5.2" class="ltx_Math" alttext="\mathbf{H},\mathbf{H}^{\prime}\in\mathbb{R}^{D\times T}" display="inline"><semantics id="S2.SS2.p4.5.m5.2a"><mrow id="S2.SS2.p4.5.m5.2.2" xref="S2.SS2.p4.5.m5.2.2.cmml"><mrow id="S2.SS2.p4.5.m5.2.2.1.1" xref="S2.SS2.p4.5.m5.2.2.1.2.cmml"><mi id="S2.SS2.p4.5.m5.1.1" xref="S2.SS2.p4.5.m5.1.1.cmml">ğ‡</mi><mo id="S2.SS2.p4.5.m5.2.2.1.1.2" xref="S2.SS2.p4.5.m5.2.2.1.2.cmml">,</mo><msup id="S2.SS2.p4.5.m5.2.2.1.1.1" xref="S2.SS2.p4.5.m5.2.2.1.1.1.cmml"><mi id="S2.SS2.p4.5.m5.2.2.1.1.1.2" xref="S2.SS2.p4.5.m5.2.2.1.1.1.2.cmml">ğ‡</mi><mo id="S2.SS2.p4.5.m5.2.2.1.1.1.3" xref="S2.SS2.p4.5.m5.2.2.1.1.1.3.cmml">â€²</mo></msup></mrow><mo id="S2.SS2.p4.5.m5.2.2.2" xref="S2.SS2.p4.5.m5.2.2.2.cmml">âˆˆ</mo><msup id="S2.SS2.p4.5.m5.2.2.3" xref="S2.SS2.p4.5.m5.2.2.3.cmml"><mi id="S2.SS2.p4.5.m5.2.2.3.2" xref="S2.SS2.p4.5.m5.2.2.3.2.cmml">â„</mi><mrow id="S2.SS2.p4.5.m5.2.2.3.3" xref="S2.SS2.p4.5.m5.2.2.3.3.cmml"><mi id="S2.SS2.p4.5.m5.2.2.3.3.2" xref="S2.SS2.p4.5.m5.2.2.3.3.2.cmml">D</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p4.5.m5.2.2.3.3.1" xref="S2.SS2.p4.5.m5.2.2.3.3.1.cmml">Ã—</mo><mi id="S2.SS2.p4.5.m5.2.2.3.3.3" xref="S2.SS2.p4.5.m5.2.2.3.3.3.cmml">T</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.5.m5.2b"><apply id="S2.SS2.p4.5.m5.2.2.cmml" xref="S2.SS2.p4.5.m5.2.2"><in id="S2.SS2.p4.5.m5.2.2.2.cmml" xref="S2.SS2.p4.5.m5.2.2.2"></in><list id="S2.SS2.p4.5.m5.2.2.1.2.cmml" xref="S2.SS2.p4.5.m5.2.2.1.1"><ci id="S2.SS2.p4.5.m5.1.1.cmml" xref="S2.SS2.p4.5.m5.1.1">ğ‡</ci><apply id="S2.SS2.p4.5.m5.2.2.1.1.1.cmml" xref="S2.SS2.p4.5.m5.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p4.5.m5.2.2.1.1.1.1.cmml" xref="S2.SS2.p4.5.m5.2.2.1.1.1">superscript</csymbol><ci id="S2.SS2.p4.5.m5.2.2.1.1.1.2.cmml" xref="S2.SS2.p4.5.m5.2.2.1.1.1.2">ğ‡</ci><ci id="S2.SS2.p4.5.m5.2.2.1.1.1.3.cmml" xref="S2.SS2.p4.5.m5.2.2.1.1.1.3">â€²</ci></apply></list><apply id="S2.SS2.p4.5.m5.2.2.3.cmml" xref="S2.SS2.p4.5.m5.2.2.3"><csymbol cd="ambiguous" id="S2.SS2.p4.5.m5.2.2.3.1.cmml" xref="S2.SS2.p4.5.m5.2.2.3">superscript</csymbol><ci id="S2.SS2.p4.5.m5.2.2.3.2.cmml" xref="S2.SS2.p4.5.m5.2.2.3.2">â„</ci><apply id="S2.SS2.p4.5.m5.2.2.3.3.cmml" xref="S2.SS2.p4.5.m5.2.2.3.3"><times id="S2.SS2.p4.5.m5.2.2.3.3.1.cmml" xref="S2.SS2.p4.5.m5.2.2.3.3.1"></times><ci id="S2.SS2.p4.5.m5.2.2.3.3.2.cmml" xref="S2.SS2.p4.5.m5.2.2.3.3.2">ğ·</ci><ci id="S2.SS2.p4.5.m5.2.2.3.3.3.cmml" xref="S2.SS2.p4.5.m5.2.2.3.3.3">ğ‘‡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.5.m5.2c">\mathbf{H},\mathbf{H}^{\prime}\in\mathbb{R}^{D\times T}</annotation></semantics></math> with <math id="S2.SS2.p4.6.m6.1" class="ltx_Math" alttext="D=k*d" display="inline"><semantics id="S2.SS2.p4.6.m6.1a"><mrow id="S2.SS2.p4.6.m6.1.1" xref="S2.SS2.p4.6.m6.1.1.cmml"><mi id="S2.SS2.p4.6.m6.1.1.2" xref="S2.SS2.p4.6.m6.1.1.2.cmml">D</mi><mo id="S2.SS2.p4.6.m6.1.1.1" xref="S2.SS2.p4.6.m6.1.1.1.cmml">=</mo><mrow id="S2.SS2.p4.6.m6.1.1.3" xref="S2.SS2.p4.6.m6.1.1.3.cmml"><mi id="S2.SS2.p4.6.m6.1.1.3.2" xref="S2.SS2.p4.6.m6.1.1.3.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p4.6.m6.1.1.3.1" xref="S2.SS2.p4.6.m6.1.1.3.1.cmml">âˆ—</mo><mi id="S2.SS2.p4.6.m6.1.1.3.3" xref="S2.SS2.p4.6.m6.1.1.3.3.cmml">d</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.6.m6.1b"><apply id="S2.SS2.p4.6.m6.1.1.cmml" xref="S2.SS2.p4.6.m6.1.1"><eq id="S2.SS2.p4.6.m6.1.1.1.cmml" xref="S2.SS2.p4.6.m6.1.1.1"></eq><ci id="S2.SS2.p4.6.m6.1.1.2.cmml" xref="S2.SS2.p4.6.m6.1.1.2">ğ·</ci><apply id="S2.SS2.p4.6.m6.1.1.3.cmml" xref="S2.SS2.p4.6.m6.1.1.3"><times id="S2.SS2.p4.6.m6.1.1.3.1.cmml" xref="S2.SS2.p4.6.m6.1.1.3.1"></times><ci id="S2.SS2.p4.6.m6.1.1.3.2.cmml" xref="S2.SS2.p4.6.m6.1.1.3.2">ğ‘˜</ci><ci id="S2.SS2.p4.6.m6.1.1.3.3.cmml" xref="S2.SS2.p4.6.m6.1.1.3.3">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.6.m6.1c">D=k*d</annotation></semantics></math>, <math id="S2.SS2.p4.7.m7.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS2.p4.7.m7.1a"><mi id="S2.SS2.p4.7.m7.1.1" xref="S2.SS2.p4.7.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.7.m7.1b"><ci id="S2.SS2.p4.7.m7.1.1.cmml" xref="S2.SS2.p4.7.m7.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.7.m7.1c">k</annotation></semantics></math> is the sum of the chosen Whipser block numbers.
Thereafter, we use attentive statistics pooling<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> to extract speaker cues from frame-level features that are helpful for the speaker verification task.
Finally, the speech vector is passed through batch normalization and a fully connected layer to obtain a low-dimensional speaker embedding representation.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>LoRA for model adaptation</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Compared to full fine-tuning, LoRA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> optimizes storage and computational efficiency by modulating low-rank subspace parameters. In this paper, we apply LoRA to the Q (query), K (key), V (value), and O (output) weights of the Whisper model's multi-head attention, freezing the model's remaining parameters. For the model's weight matrix <math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="W\in\mathbb{R}^{d\times k}" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><mrow id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml"><mi id="S2.SS3.p1.1.m1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.2.cmml">W</mi><mo id="S2.SS3.p1.1.m1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S2.SS3.p1.1.m1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.3.cmml"><mi id="S2.SS3.p1.1.m1.1.1.3.2" xref="S2.SS3.p1.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S2.SS3.p1.1.m1.1.1.3.3" xref="S2.SS3.p1.1.m1.1.1.3.3.cmml"><mi id="S2.SS3.p1.1.m1.1.1.3.3.2" xref="S2.SS3.p1.1.m1.1.1.3.3.2.cmml">d</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS3.p1.1.m1.1.1.3.3.1" xref="S2.SS3.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S2.SS3.p1.1.m1.1.1.3.3.3" xref="S2.SS3.p1.1.m1.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><apply id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1"><in id="S2.SS3.p1.1.m1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1"></in><ci id="S2.SS3.p1.1.m1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.2">ğ‘Š</ci><apply id="S2.SS3.p1.1.m1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p1.1.m1.1.1.3.1.cmml" xref="S2.SS3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S2.SS3.p1.1.m1.1.1.3.2.cmml" xref="S2.SS3.p1.1.m1.1.1.3.2">â„</ci><apply id="S2.SS3.p1.1.m1.1.1.3.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3.3"><times id="S2.SS3.p1.1.m1.1.1.3.3.1.cmml" xref="S2.SS3.p1.1.m1.1.1.3.3.1"></times><ci id="S2.SS3.p1.1.m1.1.1.3.3.2.cmml" xref="S2.SS3.p1.1.m1.1.1.3.3.2">ğ‘‘</ci><ci id="S2.SS3.p1.1.m1.1.1.3.3.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3.3.3">ğ‘˜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">W\in\mathbb{R}^{d\times k}</annotation></semantics></math>, the update mechanism is delineated as follows:</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<table id="S2.E2" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S2.E2X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E2X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle W+\Delta W=W+BA" display="inline"><semantics id="S2.E2X.2.1.1.m1.1a"><mrow id="S2.E2X.2.1.1.m1.1.1" xref="S2.E2X.2.1.1.m1.1.1.cmml"><mrow id="S2.E2X.2.1.1.m1.1.1.2" xref="S2.E2X.2.1.1.m1.1.1.2.cmml"><mi id="S2.E2X.2.1.1.m1.1.1.2.2" xref="S2.E2X.2.1.1.m1.1.1.2.2.cmml">W</mi><mo id="S2.E2X.2.1.1.m1.1.1.2.1" xref="S2.E2X.2.1.1.m1.1.1.2.1.cmml">+</mo><mrow id="S2.E2X.2.1.1.m1.1.1.2.3" xref="S2.E2X.2.1.1.m1.1.1.2.3.cmml"><mi mathvariant="normal" id="S2.E2X.2.1.1.m1.1.1.2.3.2" xref="S2.E2X.2.1.1.m1.1.1.2.3.2.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="S2.E2X.2.1.1.m1.1.1.2.3.1" xref="S2.E2X.2.1.1.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S2.E2X.2.1.1.m1.1.1.2.3.3" xref="S2.E2X.2.1.1.m1.1.1.2.3.3.cmml">W</mi></mrow></mrow><mo id="S2.E2X.2.1.1.m1.1.1.1" xref="S2.E2X.2.1.1.m1.1.1.1.cmml">=</mo><mrow id="S2.E2X.2.1.1.m1.1.1.3" xref="S2.E2X.2.1.1.m1.1.1.3.cmml"><mi id="S2.E2X.2.1.1.m1.1.1.3.2" xref="S2.E2X.2.1.1.m1.1.1.3.2.cmml">W</mi><mo id="S2.E2X.2.1.1.m1.1.1.3.1" xref="S2.E2X.2.1.1.m1.1.1.3.1.cmml">+</mo><mrow id="S2.E2X.2.1.1.m1.1.1.3.3" xref="S2.E2X.2.1.1.m1.1.1.3.3.cmml"><mi id="S2.E2X.2.1.1.m1.1.1.3.3.2" xref="S2.E2X.2.1.1.m1.1.1.3.3.2.cmml">B</mi><mo lspace="0em" rspace="0em" id="S2.E2X.2.1.1.m1.1.1.3.3.1" xref="S2.E2X.2.1.1.m1.1.1.3.3.1.cmml">â€‹</mo><mi id="S2.E2X.2.1.1.m1.1.1.3.3.3" xref="S2.E2X.2.1.1.m1.1.1.3.3.3.cmml">A</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2X.2.1.1.m1.1b"><apply id="S2.E2X.2.1.1.m1.1.1.cmml" xref="S2.E2X.2.1.1.m1.1.1"><eq id="S2.E2X.2.1.1.m1.1.1.1.cmml" xref="S2.E2X.2.1.1.m1.1.1.1"></eq><apply id="S2.E2X.2.1.1.m1.1.1.2.cmml" xref="S2.E2X.2.1.1.m1.1.1.2"><plus id="S2.E2X.2.1.1.m1.1.1.2.1.cmml" xref="S2.E2X.2.1.1.m1.1.1.2.1"></plus><ci id="S2.E2X.2.1.1.m1.1.1.2.2.cmml" xref="S2.E2X.2.1.1.m1.1.1.2.2">ğ‘Š</ci><apply id="S2.E2X.2.1.1.m1.1.1.2.3.cmml" xref="S2.E2X.2.1.1.m1.1.1.2.3"><times id="S2.E2X.2.1.1.m1.1.1.2.3.1.cmml" xref="S2.E2X.2.1.1.m1.1.1.2.3.1"></times><ci id="S2.E2X.2.1.1.m1.1.1.2.3.2.cmml" xref="S2.E2X.2.1.1.m1.1.1.2.3.2">Î”</ci><ci id="S2.E2X.2.1.1.m1.1.1.2.3.3.cmml" xref="S2.E2X.2.1.1.m1.1.1.2.3.3">ğ‘Š</ci></apply></apply><apply id="S2.E2X.2.1.1.m1.1.1.3.cmml" xref="S2.E2X.2.1.1.m1.1.1.3"><plus id="S2.E2X.2.1.1.m1.1.1.3.1.cmml" xref="S2.E2X.2.1.1.m1.1.1.3.1"></plus><ci id="S2.E2X.2.1.1.m1.1.1.3.2.cmml" xref="S2.E2X.2.1.1.m1.1.1.3.2">ğ‘Š</ci><apply id="S2.E2X.2.1.1.m1.1.1.3.3.cmml" xref="S2.E2X.2.1.1.m1.1.1.3.3"><times id="S2.E2X.2.1.1.m1.1.1.3.3.1.cmml" xref="S2.E2X.2.1.1.m1.1.1.3.3.1"></times><ci id="S2.E2X.2.1.1.m1.1.1.3.3.2.cmml" xref="S2.E2X.2.1.1.m1.1.1.3.3.2">ğµ</ci><ci id="S2.E2X.2.1.1.m1.1.1.3.3.3.cmml" xref="S2.E2X.2.1.1.m1.1.1.3.3.3">ğ´</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2X.2.1.1.m1.1c">\displaystyle W+\Delta W=W+BA</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(2)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.p3.3" class="ltx_p">where <math id="S2.SS3.p3.1.m1.1" class="ltx_Math" alttext="B\in\mathbb{R}^{d\times r}" display="inline"><semantics id="S2.SS3.p3.1.m1.1a"><mrow id="S2.SS3.p3.1.m1.1.1" xref="S2.SS3.p3.1.m1.1.1.cmml"><mi id="S2.SS3.p3.1.m1.1.1.2" xref="S2.SS3.p3.1.m1.1.1.2.cmml">B</mi><mo id="S2.SS3.p3.1.m1.1.1.1" xref="S2.SS3.p3.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S2.SS3.p3.1.m1.1.1.3" xref="S2.SS3.p3.1.m1.1.1.3.cmml"><mi id="S2.SS3.p3.1.m1.1.1.3.2" xref="S2.SS3.p3.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S2.SS3.p3.1.m1.1.1.3.3" xref="S2.SS3.p3.1.m1.1.1.3.3.cmml"><mi id="S2.SS3.p3.1.m1.1.1.3.3.2" xref="S2.SS3.p3.1.m1.1.1.3.3.2.cmml">d</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS3.p3.1.m1.1.1.3.3.1" xref="S2.SS3.p3.1.m1.1.1.3.3.1.cmml">Ã—</mo><mi id="S2.SS3.p3.1.m1.1.1.3.3.3" xref="S2.SS3.p3.1.m1.1.1.3.3.3.cmml">r</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.1.m1.1b"><apply id="S2.SS3.p3.1.m1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1"><in id="S2.SS3.p3.1.m1.1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1.1"></in><ci id="S2.SS3.p3.1.m1.1.1.2.cmml" xref="S2.SS3.p3.1.m1.1.1.2">ğµ</ci><apply id="S2.SS3.p3.1.m1.1.1.3.cmml" xref="S2.SS3.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p3.1.m1.1.1.3.1.cmml" xref="S2.SS3.p3.1.m1.1.1.3">superscript</csymbol><ci id="S2.SS3.p3.1.m1.1.1.3.2.cmml" xref="S2.SS3.p3.1.m1.1.1.3.2">â„</ci><apply id="S2.SS3.p3.1.m1.1.1.3.3.cmml" xref="S2.SS3.p3.1.m1.1.1.3.3"><times id="S2.SS3.p3.1.m1.1.1.3.3.1.cmml" xref="S2.SS3.p3.1.m1.1.1.3.3.1"></times><ci id="S2.SS3.p3.1.m1.1.1.3.3.2.cmml" xref="S2.SS3.p3.1.m1.1.1.3.3.2">ğ‘‘</ci><ci id="S2.SS3.p3.1.m1.1.1.3.3.3.cmml" xref="S2.SS3.p3.1.m1.1.1.3.3.3">ğ‘Ÿ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.1.m1.1c">B\in\mathbb{R}^{d\times r}</annotation></semantics></math> is initialised with zeros, <math id="S2.SS3.p3.2.m2.1" class="ltx_Math" alttext="A\in\mathbb{R}^{r\times k}" display="inline"><semantics id="S2.SS3.p3.2.m2.1a"><mrow id="S2.SS3.p3.2.m2.1.1" xref="S2.SS3.p3.2.m2.1.1.cmml"><mi id="S2.SS3.p3.2.m2.1.1.2" xref="S2.SS3.p3.2.m2.1.1.2.cmml">A</mi><mo id="S2.SS3.p3.2.m2.1.1.1" xref="S2.SS3.p3.2.m2.1.1.1.cmml">âˆˆ</mo><msup id="S2.SS3.p3.2.m2.1.1.3" xref="S2.SS3.p3.2.m2.1.1.3.cmml"><mi id="S2.SS3.p3.2.m2.1.1.3.2" xref="S2.SS3.p3.2.m2.1.1.3.2.cmml">â„</mi><mrow id="S2.SS3.p3.2.m2.1.1.3.3" xref="S2.SS3.p3.2.m2.1.1.3.3.cmml"><mi id="S2.SS3.p3.2.m2.1.1.3.3.2" xref="S2.SS3.p3.2.m2.1.1.3.3.2.cmml">r</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS3.p3.2.m2.1.1.3.3.1" xref="S2.SS3.p3.2.m2.1.1.3.3.1.cmml">Ã—</mo><mi id="S2.SS3.p3.2.m2.1.1.3.3.3" xref="S2.SS3.p3.2.m2.1.1.3.3.3.cmml">k</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.2.m2.1b"><apply id="S2.SS3.p3.2.m2.1.1.cmml" xref="S2.SS3.p3.2.m2.1.1"><in id="S2.SS3.p3.2.m2.1.1.1.cmml" xref="S2.SS3.p3.2.m2.1.1.1"></in><ci id="S2.SS3.p3.2.m2.1.1.2.cmml" xref="S2.SS3.p3.2.m2.1.1.2">ğ´</ci><apply id="S2.SS3.p3.2.m2.1.1.3.cmml" xref="S2.SS3.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p3.2.m2.1.1.3.1.cmml" xref="S2.SS3.p3.2.m2.1.1.3">superscript</csymbol><ci id="S2.SS3.p3.2.m2.1.1.3.2.cmml" xref="S2.SS3.p3.2.m2.1.1.3.2">â„</ci><apply id="S2.SS3.p3.2.m2.1.1.3.3.cmml" xref="S2.SS3.p3.2.m2.1.1.3.3"><times id="S2.SS3.p3.2.m2.1.1.3.3.1.cmml" xref="S2.SS3.p3.2.m2.1.1.3.3.1"></times><ci id="S2.SS3.p3.2.m2.1.1.3.3.2.cmml" xref="S2.SS3.p3.2.m2.1.1.3.3.2">ğ‘Ÿ</ci><ci id="S2.SS3.p3.2.m2.1.1.3.3.3.cmml" xref="S2.SS3.p3.2.m2.1.1.3.3.3">ğ‘˜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.2.m2.1c">A\in\mathbb{R}^{r\times k}</annotation></semantics></math> is initialised with random Gaussian initialisation, with the rank <math id="S2.SS3.p3.3.m3.2" class="ltx_Math" alttext="r\ll\mathrm{min}(d,k)" display="inline"><semantics id="S2.SS3.p3.3.m3.2a"><mrow id="S2.SS3.p3.3.m3.2.3" xref="S2.SS3.p3.3.m3.2.3.cmml"><mi id="S2.SS3.p3.3.m3.2.3.2" xref="S2.SS3.p3.3.m3.2.3.2.cmml">r</mi><mo id="S2.SS3.p3.3.m3.2.3.1" xref="S2.SS3.p3.3.m3.2.3.1.cmml">â‰ª</mo><mrow id="S2.SS3.p3.3.m3.2.3.3" xref="S2.SS3.p3.3.m3.2.3.3.cmml"><mi id="S2.SS3.p3.3.m3.2.3.3.2" xref="S2.SS3.p3.3.m3.2.3.3.2.cmml">min</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p3.3.m3.2.3.3.1" xref="S2.SS3.p3.3.m3.2.3.3.1.cmml">â€‹</mo><mrow id="S2.SS3.p3.3.m3.2.3.3.3.2" xref="S2.SS3.p3.3.m3.2.3.3.3.1.cmml"><mo stretchy="false" id="S2.SS3.p3.3.m3.2.3.3.3.2.1" xref="S2.SS3.p3.3.m3.2.3.3.3.1.cmml">(</mo><mi id="S2.SS3.p3.3.m3.1.1" xref="S2.SS3.p3.3.m3.1.1.cmml">d</mi><mo id="S2.SS3.p3.3.m3.2.3.3.3.2.2" xref="S2.SS3.p3.3.m3.2.3.3.3.1.cmml">,</mo><mi id="S2.SS3.p3.3.m3.2.2" xref="S2.SS3.p3.3.m3.2.2.cmml">k</mi><mo stretchy="false" id="S2.SS3.p3.3.m3.2.3.3.3.2.3" xref="S2.SS3.p3.3.m3.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.3.m3.2b"><apply id="S2.SS3.p3.3.m3.2.3.cmml" xref="S2.SS3.p3.3.m3.2.3"><csymbol cd="latexml" id="S2.SS3.p3.3.m3.2.3.1.cmml" xref="S2.SS3.p3.3.m3.2.3.1">much-less-than</csymbol><ci id="S2.SS3.p3.3.m3.2.3.2.cmml" xref="S2.SS3.p3.3.m3.2.3.2">ğ‘Ÿ</ci><apply id="S2.SS3.p3.3.m3.2.3.3.cmml" xref="S2.SS3.p3.3.m3.2.3.3"><times id="S2.SS3.p3.3.m3.2.3.3.1.cmml" xref="S2.SS3.p3.3.m3.2.3.3.1"></times><ci id="S2.SS3.p3.3.m3.2.3.3.2.cmml" xref="S2.SS3.p3.3.m3.2.3.3.2">min</ci><interval closure="open" id="S2.SS3.p3.3.m3.2.3.3.3.1.cmml" xref="S2.SS3.p3.3.m3.2.3.3.3.2"><ci id="S2.SS3.p3.3.m3.1.1.cmml" xref="S2.SS3.p3.3.m3.1.1">ğ‘‘</ci><ci id="S2.SS3.p3.3.m3.2.2.cmml" xref="S2.SS3.p3.3.m3.2.2">ğ‘˜</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.3.m3.2c">r\ll\mathrm{min}(d,k)</annotation></semantics></math>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2408.15585/assets/fig2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>LoRA. The pretrained weight parameters <math id="S2.F2.4.m1.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S2.F2.4.m1.1b"><mi id="S2.F2.4.m1.1.1" xref="S2.F2.4.m1.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S2.F2.4.m1.1c"><ci id="S2.F2.4.m1.1.1.cmml" xref="S2.F2.4.m1.1.1">ğ‘Š</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.4.m1.1d">W</annotation></semantics></math> are frozen, with only <math id="S2.F2.5.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.F2.5.m2.1b"><mi id="S2.F2.5.m2.1.1" xref="S2.F2.5.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.F2.5.m2.1c"><ci id="S2.F2.5.m2.1.1.cmml" xref="S2.F2.5.m2.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.5.m2.1d">A</annotation></semantics></math> and <math id="S2.F2.6.m3.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S2.F2.6.m3.1b"><mi id="S2.F2.6.m3.1.1" xref="S2.F2.6.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S2.F2.6.m3.1c"><ci id="S2.F2.6.m3.1.1.cmml" xref="S2.F2.6.m3.1.1">ğµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.6.m3.1d">B</annotation></semantics></math> being updated.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental setup</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To investigate the effectiveness of the proposed methods, we conduct experiments on the VoxCeleb1<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and CN-Celeb1<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> datasets.
The VoxCeleb1 dataset is a large-scale audio-visual collection designed for speaker recognition tasks, containing over 150,000 utterances from 1,251 celebrities sourced from YouTube videos.
The CN-Celeb1 dataset is a comprehensive audio dataset tailored for speaker recognition research. It consists of approximately 130,000 utterances by 1,000 Chinese celebrities from various domains, including but not limited to entertainment, sports, and politics.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">During the training phase, we adhered to the official dataset partitions provided, training two distinct models on VoxCeleb1 and CN-Celeb1, respectively. In the testing phase for VoxCeleb1, we utilized the original trial list associated with VoxCeleb1 (VoxCeleb1-O) for evaluation. Similarly, for CN-Celeb1, we conducted tests using the official trial lists provided.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Model configuration</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In our experiments, we selected two baseline models: ResNet34 and ECAPA-TDNN. Both baseline models, along with our proposed model, were implemented within the Wespeaker framework<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. The specifications and parameter settings for each model are as follows:</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">ECAPA-TDNN<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.p2.1.1.1.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S3.SS2.p2.1.1.2.2" class="ltx_text ltx_font_medium">]</span></cite></span>: ECAPA-TDNN is a modified version of the Time Delay Neural Network (TDNN), it features an enhanced Channel-wise Correlation Matrix Attention mechanism and time-domain x-vectors. Our implementation adopted the recommended parameters from Wespeaker.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">ResNet34<cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.p3.1.1.1.1" class="ltx_text ltx_font_medium">[</span><a href="#bib.bib30" title="" class="ltx_ref">30</a><span id="S3.SS2.p3.1.1.2.2" class="ltx_text ltx_font_medium">]</span></cite></span>: ResNet-based r-vector is the winning system of VoxSRC 2019. It utilizes residual connections to alleviate the vanishing gradient problem during training, enabling the effective training of very deep neural networks. The parameter settings also follow recipes in Wespeaker.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Whisper-PMFA</span>:
Our proposed model Whisper-PMFA is built upon the pretrained Whisper large-v2 model, which consists of 32 Transformer blocks. For our implementation, we selected partial blocks from the Whisper large-v2 model to obtain the speaker embedding. The details of the experiments and results will be presented in Section 4.2. The dimensionality of the speaker embedding was set to 192.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Intra-Language and Cross-Language Performance Evaluation for Systems Trained on VoxCeleb1 and Cn-Celeb1</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="3"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" rowspan="3"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Score Norm</span></td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="4"><span id="S3.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Training Dataset:VoxCeleb1</span></td>
<td id="S3.T1.1.1.1.4" class="ltx_td ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="4"><span id="S3.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">Training Dataset:CN-Celeb1</span></td>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="2"><span id="S3.T1.1.2.2.1.1" class="ltx_text ltx_font_bold">VoxCeleb1-O</span></td>
<td id="S3.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="2"><span id="S3.T1.1.2.2.2.1" class="ltx_text ltx_font_bold">CN-Celeb1-T</span></td>
<td id="S3.T1.1.2.2.3" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="2"><span id="S3.T1.1.2.2.4.1" class="ltx_text ltx_font_bold">VoxCeleb1-O</span></td>
<td id="S3.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;" colspan="2"><span id="S3.T1.1.2.2.5.1" class="ltx_text ltx_font_bold">CN-Celeb1-T</span></td>
</tr>
<tr id="S3.T1.1.3.3" class="ltx_tr">
<td id="S3.T1.1.3.3.1" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">EER(%)</td>
<td id="S3.T1.1.3.3.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">minDCF</td>
<td id="S3.T1.1.3.3.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">EER(%)</td>
<td id="S3.T1.1.3.3.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">minDCF</td>
<td id="S3.T1.1.3.3.5" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T1.1.3.3.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">EER(%)</td>
<td id="S3.T1.1.3.3.7" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">minDCF</td>
<td id="S3.T1.1.3.3.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">EER(%)</td>
<td id="S3.T1.1.3.3.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">minDCF</td>
</tr>
<tr id="S3.T1.1.4.4" class="ltx_tr">
<td id="S3.T1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">ECAPA-TDNN</td>
<td id="S3.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S3.T1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">2.23</td>
<td id="S3.T1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.157</td>
<td id="S3.T1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">11.97</td>
<td id="S3.T1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.457</td>
<td id="S3.T1.1.4.4.7" class="ltx_td ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">8.22</td>
<td id="S3.T1.1.4.4.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.463</td>
<td id="S3.T1.1.4.4.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">10.43</td>
<td id="S3.T1.1.4.4.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.453</td>
</tr>
<tr id="S3.T1.1.5.5" class="ltx_tr">
<td id="S3.T1.1.5.5.1" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">ResNet34</td>
<td id="S3.T1.1.5.5.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S3.T1.1.5.5.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">1.99</td>
<td id="S3.T1.1.5.5.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.154</td>
<td id="S3.T1.1.5.5.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">11.90</td>
<td id="S3.T1.1.5.5.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.458</td>
<td id="S3.T1.1.5.5.7" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T1.1.5.5.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8.53</td>
<td id="S3.T1.1.5.5.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.501</td>
<td id="S3.T1.1.5.5.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">9.49</td>
<td id="S3.T1.1.5.5.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.419</td>
</tr>
<tr id="S3.T1.1.6.6" class="ltx_tr">
<td id="S3.T1.1.6.6.1" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-PMFA(ours)</td>
<td id="S3.T1.1.6.6.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">-</td>
<td id="S3.T1.1.6.6.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">1.62</td>
<td id="S3.T1.1.6.6.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.144</td>
<td id="S3.T1.1.6.6.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">11.41</td>
<td id="S3.T1.1.6.6.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.500</td>
<td id="S3.T1.1.6.6.7" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T1.1.6.6.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4.31</td>
<td id="S3.T1.1.6.6.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.307</td>
<td id="S3.T1.1.6.6.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">9.00</td>
<td id="S3.T1.1.6.6.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.399</td>
</tr>
<tr id="S3.T1.1.7.7" class="ltx_tr">
<td id="S3.T1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">ECAPA-TDNN</td>
<td id="S3.T1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">AS-Norm</td>
<td id="S3.T1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">2.00</td>
<td id="S3.T1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.144</td>
<td id="S3.T1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">12.12</td>
<td id="S3.T1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.405</td>
<td id="S3.T1.1.7.7.7" class="ltx_td ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">7.73</td>
<td id="S3.T1.1.7.7.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.432</td>
<td id="S3.T1.1.7.7.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">10.10</td>
<td id="S3.T1.1.7.7.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">0.403</td>
</tr>
<tr id="S3.T1.1.8.8" class="ltx_tr">
<td id="S3.T1.1.8.8.1" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">ResNet34</td>
<td id="S3.T1.1.8.8.2" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">AS-Norm</td>
<td id="S3.T1.1.8.8.3" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">1.88</td>
<td id="S3.T1.1.8.8.4" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.154</td>
<td id="S3.T1.1.8.8.5" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">11.42</td>
<td id="S3.T1.1.8.8.6" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.8.8.6.1" class="ltx_text ltx_font_bold">0.374</span></td>
<td id="S3.T1.1.8.8.7" class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T1.1.8.8.8" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7.87</td>
<td id="S3.T1.1.8.8.9" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.454</td>
<td id="S3.T1.1.8.8.10" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">9.27</td>
<td id="S3.T1.1.8.8.11" class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">0.385</td>
</tr>
<tr id="S3.T1.1.9.9" class="ltx_tr">
<td id="S3.T1.1.9.9.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-PMFA(ours)</td>
<td id="S3.T1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">AS-Norm</td>
<td id="S3.T1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.9.9.3.1" class="ltx_text ltx_font_bold">1.42</span></td>
<td id="S3.T1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.9.9.4.1" class="ltx_text ltx_font_bold">0.121</span></td>
<td id="S3.T1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.9.9.5.1" class="ltx_text ltx_font_bold">11.24</span></td>
<td id="S3.T1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">0.440</td>
<td id="S3.T1.1.9.9.7" class="ltx_td ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td id="S3.T1.1.9.9.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.9.9.8.1" class="ltx_text ltx_font_bold">3.91</span></td>
<td id="S3.T1.1.9.9.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.9.9.9.1" class="ltx_text ltx_font_bold">0.270</span></td>
<td id="S3.T1.1.9.9.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.9.9.10.1" class="ltx_text ltx_font_bold">8.30</span></td>
<td id="S3.T1.1.9.9.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span id="S3.T1.1.9.9.11.1" class="ltx_text ltx_font_bold">0.358</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Implementation details</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To enhance the robustness of the systems, we applied three data augmentation techniques across all systems: additive noise augmentation from the MUSAN dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, reverberation noise augmentation from the RIRs dataset<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, and speed perturbation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> with 0.9 and 1.1 times speed changes.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">During the feature extraction phase for the two baseline models, we used Wespeaker's standard method. This process entails selecting random 2-second clips from each speech sample and extracting 80-dimensional FBank features from these segments. The window length was set to 25 milliseconds with a frameshift of 10 milliseconds, and no voice activity detection was performed.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">For our proposed model, named Whisper-PMFA, we utilized an 80-channel log magnitude Mel spectrogram consistent with the training of Whisper. During the training phase, we initially froze the parameters of the Whisper model and fine-tuned the remaining parameters for 4 epochs. This strategy was employed to prevent the pre-trained model from being fine-tuned in the wrong direction due to the random initialization of other parts. Subsequently, we conducted an overall fine-tuning of the entire model.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">All models were trained using the AAM-Softmax loss<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, with a margin of 0.2 and a scaling factor of 30. None of the models underwent large-margin fine-tuning during the training process.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Evaluation</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.2" class="ltx_p">We use cosine distance with AS-Norm for scoring. We report the system performance using two evaluation metrics: Equal Error Rate (EER) and Minimum Detection Cost Function (minDCF) with <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="P_{target}=0.05" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mrow id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><msub id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml"><mi id="S3.SS4.p1.1.m1.1.1.2.2" xref="S3.SS4.p1.1.m1.1.1.2.2.cmml">P</mi><mrow id="S3.SS4.p1.1.m1.1.1.2.3" xref="S3.SS4.p1.1.m1.1.1.2.3.cmml"><mi id="S3.SS4.p1.1.m1.1.1.2.3.2" xref="S3.SS4.p1.1.m1.1.1.2.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.1.m1.1.1.2.3.1" xref="S3.SS4.p1.1.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS4.p1.1.m1.1.1.2.3.3" xref="S3.SS4.p1.1.m1.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.1.m1.1.1.2.3.1a" xref="S3.SS4.p1.1.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS4.p1.1.m1.1.1.2.3.4" xref="S3.SS4.p1.1.m1.1.1.2.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.1.m1.1.1.2.3.1b" xref="S3.SS4.p1.1.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS4.p1.1.m1.1.1.2.3.5" xref="S3.SS4.p1.1.m1.1.1.2.3.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.1.m1.1.1.2.3.1c" xref="S3.SS4.p1.1.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS4.p1.1.m1.1.1.2.3.6" xref="S3.SS4.p1.1.m1.1.1.2.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.1.m1.1.1.2.3.1d" xref="S3.SS4.p1.1.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS4.p1.1.m1.1.1.2.3.7" xref="S3.SS4.p1.1.m1.1.1.2.3.7.cmml">t</mi></mrow></msub><mo id="S3.SS4.p1.1.m1.1.1.1" xref="S3.SS4.p1.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><eq id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1"></eq><apply id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.2.1.cmml" xref="S3.SS4.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.2.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2.2">ğ‘ƒ</ci><apply id="S3.SS4.p1.1.m1.1.1.2.3.cmml" xref="S3.SS4.p1.1.m1.1.1.2.3"><times id="S3.SS4.p1.1.m1.1.1.2.3.1.cmml" xref="S3.SS4.p1.1.m1.1.1.2.3.1"></times><ci id="S3.SS4.p1.1.m1.1.1.2.3.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2.3.2">ğ‘¡</ci><ci id="S3.SS4.p1.1.m1.1.1.2.3.3.cmml" xref="S3.SS4.p1.1.m1.1.1.2.3.3">ğ‘</ci><ci id="S3.SS4.p1.1.m1.1.1.2.3.4.cmml" xref="S3.SS4.p1.1.m1.1.1.2.3.4">ğ‘Ÿ</ci><ci id="S3.SS4.p1.1.m1.1.1.2.3.5.cmml" xref="S3.SS4.p1.1.m1.1.1.2.3.5">ğ‘”</ci><ci id="S3.SS4.p1.1.m1.1.1.2.3.6.cmml" xref="S3.SS4.p1.1.m1.1.1.2.3.6">ğ‘’</ci><ci id="S3.SS4.p1.1.m1.1.1.2.3.7.cmml" xref="S3.SS4.p1.1.m1.1.1.2.3.7">ğ‘¡</ci></apply></apply><cn type="float" id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">P_{target}=0.05</annotation></semantics></math> and <math id="S3.SS4.p1.2.m2.1" class="ltx_Math" alttext="C_{FA}=C_{Miss}=1" display="inline"><semantics id="S3.SS4.p1.2.m2.1a"><mrow id="S3.SS4.p1.2.m2.1.1" xref="S3.SS4.p1.2.m2.1.1.cmml"><msub id="S3.SS4.p1.2.m2.1.1.2" xref="S3.SS4.p1.2.m2.1.1.2.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2.2" xref="S3.SS4.p1.2.m2.1.1.2.2.cmml">C</mi><mrow id="S3.SS4.p1.2.m2.1.1.2.3" xref="S3.SS4.p1.2.m2.1.1.2.3.cmml"><mi id="S3.SS4.p1.2.m2.1.1.2.3.2" xref="S3.SS4.p1.2.m2.1.1.2.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.2.m2.1.1.2.3.1" xref="S3.SS4.p1.2.m2.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS4.p1.2.m2.1.1.2.3.3" xref="S3.SS4.p1.2.m2.1.1.2.3.3.cmml">A</mi></mrow></msub><mo id="S3.SS4.p1.2.m2.1.1.3" xref="S3.SS4.p1.2.m2.1.1.3.cmml">=</mo><msub id="S3.SS4.p1.2.m2.1.1.4" xref="S3.SS4.p1.2.m2.1.1.4.cmml"><mi id="S3.SS4.p1.2.m2.1.1.4.2" xref="S3.SS4.p1.2.m2.1.1.4.2.cmml">C</mi><mrow id="S3.SS4.p1.2.m2.1.1.4.3" xref="S3.SS4.p1.2.m2.1.1.4.3.cmml"><mi id="S3.SS4.p1.2.m2.1.1.4.3.2" xref="S3.SS4.p1.2.m2.1.1.4.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.2.m2.1.1.4.3.1" xref="S3.SS4.p1.2.m2.1.1.4.3.1.cmml">â€‹</mo><mi id="S3.SS4.p1.2.m2.1.1.4.3.3" xref="S3.SS4.p1.2.m2.1.1.4.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.2.m2.1.1.4.3.1a" xref="S3.SS4.p1.2.m2.1.1.4.3.1.cmml">â€‹</mo><mi id="S3.SS4.p1.2.m2.1.1.4.3.4" xref="S3.SS4.p1.2.m2.1.1.4.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS4.p1.2.m2.1.1.4.3.1b" xref="S3.SS4.p1.2.m2.1.1.4.3.1.cmml">â€‹</mo><mi id="S3.SS4.p1.2.m2.1.1.4.3.5" xref="S3.SS4.p1.2.m2.1.1.4.3.5.cmml">s</mi></mrow></msub><mo id="S3.SS4.p1.2.m2.1.1.5" xref="S3.SS4.p1.2.m2.1.1.5.cmml">=</mo><mn id="S3.SS4.p1.2.m2.1.1.6" xref="S3.SS4.p1.2.m2.1.1.6.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><apply id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1"><and id="S3.SS4.p1.2.m2.1.1a.cmml" xref="S3.SS4.p1.2.m2.1.1"></and><apply id="S3.SS4.p1.2.m2.1.1b.cmml" xref="S3.SS4.p1.2.m2.1.1"><eq id="S3.SS4.p1.2.m2.1.1.3.cmml" xref="S3.SS4.p1.2.m2.1.1.3"></eq><apply id="S3.SS4.p1.2.m2.1.1.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.2.1.cmml" xref="S3.SS4.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.2.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2.2">ğ¶</ci><apply id="S3.SS4.p1.2.m2.1.1.2.3.cmml" xref="S3.SS4.p1.2.m2.1.1.2.3"><times id="S3.SS4.p1.2.m2.1.1.2.3.1.cmml" xref="S3.SS4.p1.2.m2.1.1.2.3.1"></times><ci id="S3.SS4.p1.2.m2.1.1.2.3.2.cmml" xref="S3.SS4.p1.2.m2.1.1.2.3.2">ğ¹</ci><ci id="S3.SS4.p1.2.m2.1.1.2.3.3.cmml" xref="S3.SS4.p1.2.m2.1.1.2.3.3">ğ´</ci></apply></apply><apply id="S3.SS4.p1.2.m2.1.1.4.cmml" xref="S3.SS4.p1.2.m2.1.1.4"><csymbol cd="ambiguous" id="S3.SS4.p1.2.m2.1.1.4.1.cmml" xref="S3.SS4.p1.2.m2.1.1.4">subscript</csymbol><ci id="S3.SS4.p1.2.m2.1.1.4.2.cmml" xref="S3.SS4.p1.2.m2.1.1.4.2">ğ¶</ci><apply id="S3.SS4.p1.2.m2.1.1.4.3.cmml" xref="S3.SS4.p1.2.m2.1.1.4.3"><times id="S3.SS4.p1.2.m2.1.1.4.3.1.cmml" xref="S3.SS4.p1.2.m2.1.1.4.3.1"></times><ci id="S3.SS4.p1.2.m2.1.1.4.3.2.cmml" xref="S3.SS4.p1.2.m2.1.1.4.3.2">ğ‘€</ci><ci id="S3.SS4.p1.2.m2.1.1.4.3.3.cmml" xref="S3.SS4.p1.2.m2.1.1.4.3.3">ğ‘–</ci><ci id="S3.SS4.p1.2.m2.1.1.4.3.4.cmml" xref="S3.SS4.p1.2.m2.1.1.4.3.4">ğ‘ </ci><ci id="S3.SS4.p1.2.m2.1.1.4.3.5.cmml" xref="S3.SS4.p1.2.m2.1.1.4.3.5">ğ‘ </ci></apply></apply></apply><apply id="S3.SS4.p1.2.m2.1.1c.cmml" xref="S3.SS4.p1.2.m2.1.1"><eq id="S3.SS4.p1.2.m2.1.1.5.cmml" xref="S3.SS4.p1.2.m2.1.1.5"></eq><share href="#S3.SS4.p1.2.m2.1.1.4.cmml" id="S3.SS4.p1.2.m2.1.1d.cmml" xref="S3.SS4.p1.2.m2.1.1"></share><cn type="integer" id="S3.SS4.p1.2.m2.1.1.6.cmml" xref="S3.SS4.p1.2.m2.1.1.6">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">C_{FA}=C_{Miss}=1</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation results and analysis</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Performance evaluation and analysis</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Experimental results are shown in Table 1. In the experiments, we conducted hierarchical fusion on layers 17-24, and the details regarding the selection of layers will be discussed in the next section.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">As shown in Table 1, even though the two baseline models implemented within the Wespeaker framework achieved good results, our proposed model significantly outperforms the two baseline models on both the CN-Celeb1 and VoxCeleb1 datasets. On the VoxCeleb1 dataset, our model achieves an EER of 1.42%, representing a reduction of 24.3% compared to ResNet34 and a reduction of 29.0% compared to ECAPA-TDNN. On the CN-Celeb1 dataset, our model achieves an EER of 8.30%, representing a reduction of 10.4% compared to ResNet34 and a reduction of 17.9% compared to ECAPA-TDNN. MinDCF also shows corresponding improvements across all datasets.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">The experimental results on multiple datasets demonstrate that although Whisper has not been optimized for speaker verification, our proposed method effectively utilizes the information learned and filtered from the pre-trained Whisper model. Whisper-PMFA leverages this information to capture distinctive speaker embeddings better, thereby significantly enhancing the effectiveness of speaker recognition tasks.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Layer selection experiment</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The results of the layer selection experiment are presented in Table 2. We explored two different layer aggregation approaches: one using 8 layers and the other using 16 layers. In the 8-layer experiment, we divided the model into four parts: the front (layers 1-8), the front-middle (layers 9-16), the middle-back (layers 17-24), and the back (layers 25-32). In the 16-layer experiment, we selected adjacent parts based on the previous division, resulting in three groups of experiments: layers 1-16, layers 9-24, and layers 17-32Â <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Due to the constraint of GPU memory, a maximum of 16 layers is supported in our experiments</span></span></span>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The experimental results for both the 8-layer and 16-layer experiments indicate that models utilizing 16 layers generally perform worse than those utilizing 8 layers. This suggests that increasing the number of layers does not necessarily improve model performance. Instead, the inclusion of excessively irrelevant information with an increased number of layers can hinder the model's pooling layers from effectively extracting relevant speaker-related cues. This ultimately leads to a degradation in performance.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Additionally, the experimental results indicate that compared to other parts, the mid-back part (layers 17-24) of the Whisper model contains more speaker-related cues. Fusion models based on these layers achieved the best performance.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Layer selection experiment result</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Selected Layers</span></td>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">VoxCeleb1-O</span></td>
</tr>
<tr id="S4.T2.1.2.2" class="ltx_tr">
<td id="S4.T2.1.2.2.1" class="ltx_td ltx_align_center"><span id="S4.T2.1.2.2.1.1" class="ltx_text ltx_font_bold">EER(%)</span></td>
<td id="S4.T2.1.2.2.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.2.2.2.1" class="ltx_text ltx_font_bold">minDCF</span></td>
</tr>
<tr id="S4.T2.1.3.3" class="ltx_tr">
<td id="S4.T2.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t">1-8</td>
<td id="S4.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">3.93</td>
<td id="S4.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">0.287</td>
</tr>
<tr id="S4.T2.1.4.4" class="ltx_tr">
<td id="S4.T2.1.4.4.1" class="ltx_td ltx_align_center">9-16</td>
<td id="S4.T2.1.4.4.2" class="ltx_td ltx_align_center">1.66</td>
<td id="S4.T2.1.4.4.3" class="ltx_td ltx_align_center">0.135</td>
</tr>
<tr id="S4.T2.1.5.5" class="ltx_tr">
<td id="S4.T2.1.5.5.1" class="ltx_td ltx_align_center">17-24</td>
<td id="S4.T2.1.5.5.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.5.2.1" class="ltx_text ltx_font_bold">1.42</span></td>
<td id="S4.T2.1.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.5.5.3.1" class="ltx_text ltx_font_bold">0.121</span></td>
</tr>
<tr id="S4.T2.1.6.6" class="ltx_tr">
<td id="S4.T2.1.6.6.1" class="ltx_td ltx_align_center">25-32</td>
<td id="S4.T2.1.6.6.2" class="ltx_td ltx_align_center">1.65</td>
<td id="S4.T2.1.6.6.3" class="ltx_td ltx_align_center">0.148</td>
</tr>
<tr id="S4.T2.1.7.7" class="ltx_tr">
<td id="S4.T2.1.7.7.1" class="ltx_td ltx_align_center ltx_border_t">1-16</td>
<td id="S4.T2.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t">2.07</td>
<td id="S4.T2.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t">0.144</td>
</tr>
<tr id="S4.T2.1.8.8" class="ltx_tr">
<td id="S4.T2.1.8.8.1" class="ltx_td ltx_align_center">9-24</td>
<td id="S4.T2.1.8.8.2" class="ltx_td ltx_align_center">1.74</td>
<td id="S4.T2.1.8.8.3" class="ltx_td ltx_align_center">0.134</td>
</tr>
<tr id="S4.T2.1.9.9" class="ltx_tr">
<td id="S4.T2.1.9.9.1" class="ltx_td ltx_align_center ltx_border_bb">17-32</td>
<td id="S4.T2.1.9.9.2" class="ltx_td ltx_align_center ltx_border_bb">2.03</td>
<td id="S4.T2.1.9.9.3" class="ltx_td ltx_align_center ltx_border_bb">0.163</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Cross-Language performance analysis</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The experiments in the second and third columns of the table demonstrate the cross-linguistic performance of our proposed model compared to the baselines. When trained on Chinese (CN-Celeb1) and tested on English (VoxCeleb1), as well as the reverse scenario, our proposed model consistently outperforms both baseline models. This phenomenon is particularly notable when training in Chinese and testing in English, where our model achieves performance improvements of nearly 50% compared to the baselines.
A critical factor contributing to this significant enhancement is utilizing the Whisper pre-trained model as the foundation of our proposed architecture. The Whisper model has been pre-trained on a diverse corpus that spans multiple languages, endowing it with robust cross-linguistic features. Our model, built upon this pre-trained base, inherits and optimizes these features for cross-lingual tasks. This advancement underscores the potential of pre-trained models in improving the adaptability and generalization of language processing systems across diverse linguistic contexts.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">While our model achieves notable cross-lingual performance improvements in certain scenarios, its performance uplift is less marked when training in English (VoxCeleb1) and testing in Chinese (CN-Celeb1). This could be attributed to the complex nature of the CN-Celeb1 test set, which encompasses various domains such as drama, singing, and speeches. These diverse conditions likely impede the model's generalization in this context. Addressing this challenge will form the focus of our next research phase, aiming to enhance the model's adaptability and performance across varied linguistic domains.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Comparison with results in the literature</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We compared our Whisper-PMFA with several state-of-the-art models, all models are trained on the VoxCeleb1 dataset. The experimental outcomes demonstrated that our proposed model achieved the highest performance among the models tested, this indicates the effectiveness of our method.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison with published systems on VoxCeleb1</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.2pt;padding-right:5.2pt;" rowspan="2"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.2pt;padding-right:5.2pt;" colspan="2"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">VoxCeleb1-O</span></th>
</tr>
<tr id="S4.T3.1.2.2" class="ltx_tr">
<th id="S4.T3.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.2pt;padding-right:5.2pt;"><span id="S4.T3.1.2.2.1.1" class="ltx_text ltx_font_bold">EER(%)</span></th>
<th id="S4.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:5.2pt;padding-right:5.2pt;"><span id="S4.T3.1.2.2.2.1" class="ltx_text ltx_font_bold">minDCF</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.3.1" class="ltx_tr">
<td id="S4.T3.1.3.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.2pt;padding-right:5.2pt;">M-sv<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>
</td>
<td id="S4.T3.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.2pt;padding-right:5.2pt;">3.61</td>
<td id="S4.T3.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.2pt;padding-right:5.2pt;">-</td>
</tr>
<tr id="S4.T3.1.4.2" class="ltx_tr">
<td id="S4.T3.1.4.2.1" class="ltx_td ltx_align_center" style="padding-left:5.2pt;padding-right:5.2pt;">Inter-layer Adapter WavLM<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</td>
<td id="S4.T3.1.4.2.2" class="ltx_td ltx_align_center" style="padding-left:5.2pt;padding-right:5.2pt;">2.58</td>
<td id="S4.T3.1.4.2.3" class="ltx_td ltx_align_center" style="padding-left:5.2pt;padding-right:5.2pt;">0.187</td>
</tr>
<tr id="S4.T3.1.5.3" class="ltx_tr">
<td id="S4.T3.1.5.3.1" class="ltx_td ltx_align_center" style="padding-left:5.2pt;padding-right:5.2pt;">DROP-TDNN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
</td>
<td id="S4.T3.1.5.3.2" class="ltx_td ltx_align_center" style="padding-left:5.2pt;padding-right:5.2pt;">2.15</td>
<td id="S4.T3.1.5.3.3" class="ltx_td ltx_align_center" style="padding-left:5.2pt;padding-right:5.2pt;">-</td>
</tr>
<tr id="S4.T3.1.6.4" class="ltx_tr">
<td id="S4.T3.1.6.4.1" class="ltx_td ltx_align_center" style="padding-left:5.2pt;padding-right:5.2pt;">HuBERT-Base ECAPA-TDNN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>
</td>
<td id="S4.T3.1.6.4.2" class="ltx_td ltx_align_center" style="padding-left:5.2pt;padding-right:5.2pt;">1.86</td>
<td id="S4.T3.1.6.4.3" class="ltx_td ltx_align_center" style="padding-left:5.2pt;padding-right:5.2pt;">-</td>
</tr>
<tr id="S4.T3.1.7.5" class="ltx_tr">
<td id="S4.T3.1.7.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:5.2pt;padding-right:5.2pt;">Whisper-PMFA(ours)</td>
<td id="S4.T3.1.7.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:5.2pt;padding-right:5.2pt;"><span id="S4.T3.1.7.5.2.1" class="ltx_text ltx_font_bold">1.42</span></td>
<td id="S4.T3.1.7.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-left:5.2pt;padding-right:5.2pt;"><span id="S4.T3.1.7.5.3.1" class="ltx_text ltx_font_bold">0.121</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Investigation of more effective adaptation method</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">To address the efficiency reduction caused by the Whisper model's size, we integrated LoRA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> as an alternative to full fine-tuning. According to the results shown in Table 4, this method significantly reduced the trainable parameters by approximately 45 times with only a minimal increase in EER by 0.2%. The Whisper-PMFA model, enhanced with LoRA, maintained high performance while achieving a parameter count comparable to other models, demonstrating a more effective adaptation method for large-scale models.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison with the Integration of LoRA</figcaption>
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.8pt;padding-right:4.8pt;"><span id="S4.T4.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.8pt;padding-right:4.8pt;"><span id="S4.T4.1.1.1.2.1" class="ltx_text ltx_font_bold"># Params</span></th>
<th id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.8pt;padding-right:4.8pt;"><span id="S4.T4.1.1.1.3.1" class="ltx_text ltx_font_bold">EER(%)</span></th>
<th id="S4.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.8pt;padding-right:4.8pt;"><span id="S4.T4.1.1.1.4.1" class="ltx_text ltx_font_bold">minDCF</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.2.1" class="ltx_tr">
<td id="S4.T4.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.8pt;padding-right:4.8pt;">Whisper-PMFA</td>
<td id="S4.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.8pt;padding-right:4.8pt;">487.7M</td>
<td id="S4.T4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.8pt;padding-right:4.8pt;">1.42</td>
<td id="S4.T4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.8pt;padding-right:4.8pt;">0.121</td>
</tr>
<tr id="S4.T4.1.3.2" class="ltx_tr">
<td id="S4.T4.1.3.2.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.8pt;padding-right:4.8pt;">Whisper-PMFA(LoRA)</td>
<td id="S4.T4.1.3.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.8pt;padding-right:4.8pt;">10.9M</td>
<td id="S4.T4.1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.8pt;padding-right:4.8pt;">1.62</td>
<td id="S4.T4.1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.8pt;padding-right:4.8pt;">0.150</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, the Whisper-PMFA framework is proposed, which leverages the rich speech knowledge embedded in the pre-trained Whisper ASR model to achieve high-quality speaker embedding extraction through selective feature aggregation. Experimental results on the widely used VoxCeleb1 and CN-Celeb1 datasets show that Whisper-PMFA can achieve notably lower EERs than the competing models and high cross-linguistic robustness. In addition, the LoRA adaptation approach is also investigated as a trial adaptation method, achieving a significant reduction in the number of trainable model parameters while maintaining competitive performance.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgement</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Shuai Wang is supported by Internal Project of Shenzhen Research Institute of Big Data under grant No.J00220230014.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
D.Â Snyder, D.Â Garcia-Romero, G.Â Sell, D.Â Povey, and S.Â Khudanpur, ``X-vectors: Robust dnn embeddings for speaker recognition,'' in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, Calgary, 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
E.Â Variani, X.Â Lei, E.Â McDermott, I.Â L. Moreno, and J.Â Gonzalez-Dominguez, ``Deep neural networks for small footprint text-dependent speaker verification,'' in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, Florence, 2014.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
N.Â Dehak, P.Â J. Kenny, R.Â Dehak, P.Â Dumouchel, and P.Â Ouellet, ``Front-end factor analysis for speaker verification,'' <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Audio, Speech, and Language Processing</em>, vol.Â 19, no.Â 4, pp. 788â€“798, 2010.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
W.Â Fang, Z.Â Yu, Y.Â Chen, T.Â Huang, T.Â Masquelier, and Y.Â Tian, ``Deep residual learning in spiking neural networks,'' in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proc. Neur IPS</em>, Montreal, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
B.Â Desplanques, J.Â Thienpondt, and K.Â Demuynck, ``Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification,'' in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, Shanghai, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
T.Â Zhou, Y.Â Zhao, and J.Â Wu, ``Resnext and res2net structures for speaker verification,'' in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proc. SLT</em>, virtual event, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
B.Â Gu, W.Â Guo, and J.Â Zhang, ``Memory storable network based feature aggregation for speaker representation learning,'' <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.Â 31, pp. 643â€“655, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y.Â Tang, G.Â Ding, J.Â Huang, X.Â He, and B.Â Zhou, ``Deep speaker embedding learning with multi-level pooling for text-independent speaker verification,'' in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, Brighton, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Y.Â Zhang, Z.Â Lv, H.Â Wu, S.Â Zhang, P.Â Hu, Z.Â Wu, H.-y. Lee, and H.Â Meng, ``Mfa-conformer: Multi-scale feature aggregation conformer for automatic speaker verification,'' in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, Inchon, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
F.Â Wang, J.Â Cheng, W.Â Liu, and H.Â Liu, ``Additive margin softmax for face verification,'' <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Letters</em>, vol.Â 25, no.Â 7, pp. 926â€“930, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J.Â Deng, J.Â Guo, N.Â Xue, and S.Â Zafeiriou, ``Arcface: Additive angular margin loss for deep face recognition,'' in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proc. CVPR</em>, Long Beach, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
W.Â Liu, Y.Â Wen, Z.Â Yu, M.Â Li, B.Â Raj, and L.Â Song, ``Sphereface: Deep hypersphere embedding for face recognition,'' in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proc. CVPR</em>, Hawaii, 2017.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J.Â Thienpondt, B.Â Desplanques, and K.Â Demuynck, ``The idlab voxsrc-20 submission: Large margin fine-tuning and quality-aware score calibration in dnn based speaker verification,'' in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, Toronto, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
P.Â Matejka, O.Â Novotná»³, O.Â Plchot, L.Â Burget, M.Â D. SÃ¡nchez, and J.Â Cernocká»³, ``Analysis of score normalization in multilingual speaker recognition.'' in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, Stockholm, 2017.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
X.Â Hu, L.Â Chu, J.Â Pei, W.Â Liu, and J.Â Bian, ``Model complexity of deep learning: A survey,'' <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Knowledge and Information Systems</em>, vol.Â 63, pp. 2585â€“2619, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
A.Â Radford, J.Â W. Kim, T.Â Xu, G.Â Brockman, C.Â McLeavey, and I.Â Sutskever, ``Robust speech recognition via large-scale weak supervision,'' in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proc. ICML</em>, Hawaii, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
W.-N. Hsu, B.Â Bolte, Y.-H.Â H. Tsai, K.Â Lakhotia, R.Â Salakhutdinov, and A.Â Mohamed, ``Hubert: Self-supervised speech representation learning by masked prediction of hidden units,'' <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.Â 29, pp. 3451â€“3460, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S.Â Chen, C.Â Wang, Z.Â Chen, Y.Â Wu, S.Â Liu, Z.Â Chen, J.Â Li, N.Â Kanda, T.Â Yoshioka, X.Â Xiao <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Wavlm: Large-scale self-supervised pre-training for full stack speech processing,'' <em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, vol.Â 16, no.Â 6, pp. 1505â€“1518, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
T.Â Berns, N.Â Vaessen, and D.Â A. van Leeuwen, ``Speaker and language change detection using wav2vec2 and whisper,'' <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.09381</em>, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
D.Â Cai, W.Â Wang, M.Â Li, R.Â Xia, and C.Â Huang, ``Pretraining conformer with asr for speaker verification,'' in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, Rhodes, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
D.Â Liao, T.Â Jiang, F.Â Wang, L.Â Li, and Q.Â Hong, ``Towards a unified conformer structure: from asr to asv task,'' in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, Rhodes, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
M.Â Sang and J.Â H. Hansen, ``Efficient adapter tuning of pre-trained speech models for automatic speaker verification,'' <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.00293</em>, 2024.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J.Â Peng, T.Â Stafylakis, R.Â Gu, O.Â Plchot, L.Â MoÅ¡ner, L.Â Burget, and J.Â ÄŒernocká»³, ``Parameter-efficient transfer learning of pre-trained transformer models for speaker verification using adapters,'' in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, Rhodes, 2023.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
E.Â J. Hu, Y.Â Shen, P.Â Wallis, Z.Â Allen-Zhu, Y.Â Li, S.Â Wang, L.Â Wang, and W.Â Chen, ``Lora: Low-rank adaptation of large language models,'' in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proc. ICLR</em>, Vienna, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
A.Â Vaswani, N.Â Shazeer, N.Â Parmar, J.Â Uszkoreit, L.Â Jones, A.Â N. Gomez, Å.Â Kaiser, and I.Â Polosukhin, ``Attention is all you need,'' in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proc. Neur IPS</em>, Long Beach, 2017.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
K.Â Okabe, T.Â Koshinaka, and K.Â Shinoda, ``Attentive statistics pooling for deep speaker embedding,'' in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, Hyderabad, 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
A.Â Nagrani, J.Â S. Chung, W.Â Xie, and A.Â Zisserman, ``Voxceleb: Large-scale speaker verification in the wild,'' <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Computer Speech &amp; Language</em>, vol.Â 60, p. 101027, 2020.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Y.Â Fan, J.Â Kang, L.Â Li, K.Â Li, H.Â Chen, S.Â Cheng, P.Â Zhang, Z.Â Zhou, Y.Â Cai, and D.Â Wang, ``Cn-celeb: a challenging chinese speaker recognition dataset,'' in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, Barcelona, 2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
H.Â Wang, C.Â Liang, S.Â Wang, Z.Â Chen, B.Â Zhang, X.Â Xiang, Y.Â Deng, and Y.Â Qian, ``Wespeaker: A research and production oriented speaker embedding learning toolkit,'' in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, Rhodes, 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
H.Â Zeinali, S.Â Wang, A.Â Silnova, P.Â MatÄ›jka, and O.Â Plchot, ``But system description to voxceleb speaker recognition challenge 2019,'' <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.12592</em>, 2019.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
D.Â Snyder, G.Â Chen, and D.Â Povey, ``Musan: A music, speech, and noise corpus,'' <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1510.08484</em>, 2015.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
T.Â Ko, V.Â Peddinti, D.Â Povey, M.Â L. Seltzer, and S.Â Khudanpur, ``A study on data augmentation of reverberant speech for robust speech recognition,'' in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, New Orleans, 2017.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
S.Â Yang and M.Â Liu, ``Data augmentation for speaker verification,'' in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proc. EITCE</em>, Xiamen, 2022.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
X.Â Xiang, S.Â Wang, H.Â Huang, Y.Â Qian, and K.Â Yu, ``Margin matters: Towards more discriminative deep neural network embeddings for speaker recognition,'' in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proc. APSIPA ASC</em>, Lanzhou, 2019.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Z.Â Fan, M.Â Li, S.Â Zhou, and B.Â Xu, ``Exploring wav2vec 2.0 on speaker verification and language identification,'' in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, Brno, 2021.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Q.-B. Hong, C.-H. Wu, and H.-M. Wang, ``Decomposition and reorganization of phonetic information for speaker embedding learning,'' <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, pp. 1745â€“1757, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Z.Â Chen, S.Â Chen, Y.Â Wu, Y.Â Qian, C.Â Wang, S.Â Liu, Y.Â Qian, and M.Â Zeng, ``Large-scale self-supervised speech representation learning for automatic speaker verification,'' in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, Singapore, 2022.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.15584" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.15585" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.15585">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.15585" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.15586" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 12:26:13 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
