<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.13891] SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning</title><meta property="og:description" content="Instruction-based speech processing is becoming popular.
Studies show that training with multiple tasks boosts performance, but collecting diverse, large-scale tasks and datasets is expensive.
Thus, it is highly desira…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.13891">

<!--Generated on Thu Sep  5 18:01:23 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=1]Chien-yuHuang
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=2]Min-HanShih
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=1]Ke-HanLu
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>[affiliation=1]Chi-YuanHsiao
<span id="p1.3.4" class="ltx_ERROR undefined">\name</span>[affiliation=1]Hung-yiLee</p>
</div>
<h1 class="ltx_title ltx_title_document">
<span id="id3.id1" class="ltx_text ltx_font_smallcaps">SpeechCaps</span>: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id6.id1" class="ltx_p">Instruction-based speech processing is becoming popular.
Studies show that training with multiple tasks boosts performance, but collecting diverse, large-scale tasks and datasets is expensive.
Thus, it is highly desirable to design a fundamental task that benefits other downstream tasks.
This paper introduces a multi-talker speaking style captioning task to enhance the understanding of speaker and prosodic information.
We used large language models to generate descriptions for multi-talker speech.
Then, we trained our model with pre-training on this captioning task followed by instruction tuning.
Evaluation on Dynamic-SUPERB shows our model outperforming the baseline pre-trained only on single-talker tasks, particularly in speaker and emotion recognition.
Additionally, tests on a multi-talker QA task reveal that current models struggle with attributes such as gender, pitch, and speaking rate.
The code and dataset are available at <a target="_blank" href="https://github.com/cyhuang-tw/speechcaps" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/cyhuang-tw/speechcaps</a>.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>speech captioning, speaking style, instruction tuning, large language model
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The advancement of large language models (LLMs) in natural language processing (NLP) has significantly impacted speech processing research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, particularly in developing instruction-based speech models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
Unlike conventional task-specific models trained for fixed tasks, instruction-based models use user prompts to perform various tasks, offering greater flexibility.
A key goal is to achieve emergent capabilities for handling unseen tasks effectively, as done in NLP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
LTU-AS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> enhances performance across audio and speech tasks with open-ended question-answering data.
SALMONN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> uses ASR and audio captioning, adopting activation tuning to mitigate overfitting.
Qwen-Audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> employs a multi-task framework to enhance general audio understanding.
WavLLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> uses curriculum training, starting with elementary tasks and progressing to more complex ones.
DeSTA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> learns speech-text alignment by describing a talker's speaking style before instruction tuning.
These models integrate speech features into LLMs, which are responsible for understanding speech and describing them with natural language.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Dynamic-SUPERB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> provides a comprehensive set of 55 tasks designed to assess instruction-based speech models, covering dimensions such as content, semantics, and speaker characteristics.
Surprisingly, existing models perform poorly in speaker and emotion tasks, showing an insufficient understanding of these aspects <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/dynamic-superb/dynamic-superb/blob/main/docs/leaderboard.md" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://github.com/dynamic-superb/dynamic-superb/blob/main/docs/leaderboard.md</a></span></span></span>.
This capability is crucial for tasks like speaker verification and emotion recognition, which involve processing multiple talkers or expressive speech, and is essential for developing advanced conversation-related applications.
While multi-task training can improve performance, gathering sufficient data for all task types is often costly or infeasible.
Alternatively, we aim to investigate whether training models on fundamental tasks can enable them to learn general knowledge benefiting several downstream applications.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we propose a novel task called multi-talker speaking style captioning as a fundamental task to enhance models' general speech understanding capabilities.
Speaking style captioning uses natural language to describe how a speaker talks, focusing on speaker-specific and prosodic information rather than content.
DeSTA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> is the first model to learn general speech knowledge by pre-training on speaking style captioning tasks, but it only involves single-talker captioning, limiting its potential.
Our proposed task aims to describe each speaker's style, including overlapping talkers, making it more challenging.
Consequently, we created <span id="S1.p3.1.1" class="ltx_text ltx_font_smallcaps">SpeechCaps</span>, the first multi-talker speaking style captioning dataset, synthetically generated from PromptSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
Then, we developed DeSTA+ using a two-stage pre-training approach that extends from single-talker to multi-talker captioning with <span id="S1.p3.1.2" class="ltx_text ltx_font_smallcaps">SpeechCaps</span>, enhancing its understanding of speaker and prosodic information.
Evaluation results on Dynamic-SUPERB show that DeSTA+ significantly improves performance in speaker and emotion tasks, achieves state-of-the-art results, and demonstrates competitive performance in content and semantic tasks compared to DeSTA, highlighting the effectiveness of the proposed task as a pre-training approach.
Besides, we created a test set in <span id="S1.p3.1.3" class="ltx_text ltx_font_smallcaps">SpeechCaps</span> to directly assess models' capabilities in capturing the speaking styles of different talkers.
The poor performance of baseline models on this test set reflects their weaknesses in speaker and emotion tasks on Dynamic-SUPERB.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Pre-Training Task Generation</h2>

<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.3.2" class="ltx_text" style="font-size:90%;">An example of metadata along with its corresponding description and question-answer pairs generated by GPT-4o. Descriptions are generated only for the training set, and question-answer pairs are generated only for the testing set.</span></figcaption>
<div id="S2.T1.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:420.6pt;height:236.8pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-13.5pt,7.6pt) scale(0.939738419032193,0.939738419032193) ;">
<table id="S2.T1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.4.1.1" class="ltx_tr">
<td id="S2.T1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt">
<span id="S2.T1.4.1.1.1.1" class="ltx_ERROR undefined">\phantomsubcaption</span>(a) <span id="S2.T1.4.1.1.1.2" class="ltx_text ltx_font_bold">Metadata</span>
</td>
</tr>
<tr id="S2.T1.4.1.2" class="ltx_tr">
<td id="S2.T1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">
<span id="S2.T1.4.1.2.1.1" class="ltx_text ltx_font_bold">Speaker 1:</span> {
gender: female,
emotion: sad,
pitch: low,
speed: slow,
energy: low,
start: 0.0,
end: 3.744
}</td>
</tr>
<tr id="S2.T1.4.1.3" class="ltx_tr">
<td id="S2.T1.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">
<span id="S2.T1.4.1.3.1.1" class="ltx_text ltx_font_bold">Speaker 2:</span> {
gender: female,
emotion: shouting,
pitch: high,
speed: medium,
energy: medium,
start: 1.176,
end: 5.106
}</td>
</tr>
<tr id="S2.T1.4.1.4" class="ltx_tr">
<td id="S2.T1.4.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left">
<span id="S2.T1.4.1.4.1.1" class="ltx_text ltx_font_bold">Speaker 3:</span> {
gender: male,
emotion: cheerful,
pitch: medium,
speed: fast,
energy: high,
start: 5.562,
end: 9.546
}</td>
</tr>
<tr id="S2.T1.4.1.5" class="ltx_tr">
<td id="S2.T1.4.1.5.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt">
<span id="S2.T1.4.1.5.1.1" class="ltx_ERROR undefined">\phantomsubcaption</span>(b) <span id="S2.T1.4.1.5.1.2" class="ltx_text ltx_font_bold">Description (Only in training)</span>
</td>
</tr>
<tr id="S2.T1.4.1.6" class="ltx_tr">
<td id="S2.T1.4.1.6.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">
<span id="S2.T1.4.1.6.1.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:433.6pt;">
<span id="S2.T1.4.1.6.1.1.1" class="ltx_p">The audio starts with a female speaker expressing sadness, her voice low and slow, conveying sorrow.
Overlapping slightly, another female voice interjects, shouting with a high pitch and medium speed, her tone filled with intensity.
Following this, a male speaker chimes in, his cheerful and medium-pitched voice moving quickly with high energy, bringing a lively and upbeat atmosphere.</span>
</span>
</td>
</tr>
<tr id="S2.T1.4.1.7" class="ltx_tr">
<td id="S2.T1.4.1.7.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt">
<span id="S2.T1.4.1.7.1.1" class="ltx_ERROR undefined">\phantomsubcaption</span>(c) <span id="S2.T1.4.1.7.1.2" class="ltx_text ltx_font_bold">Question-Answer Pairs (Only in testing)</span>
</td>
</tr>
<tr id="S2.T1.4.1.8" class="ltx_tr">
<td id="S2.T1.4.1.8.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">Q: Among the 3 speakers in the audio, what is the emotion of the speaker who is first in the sequence? A: sad</td>
</tr>
<tr id="S2.T1.4.1.9" class="ltx_tr">
<td id="S2.T1.4.1.9.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">Q: In this audio, there are 3 speakers. Who, according to their speaking order, speaks at the highest speed? A: 3 (the third)</td>
</tr>
</table>
</span></div>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Dataset: PromptSpeech</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p"><span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">SpeechCaps</span> utilizes data from PromptSpeech, which features a variety of expressive utterances from different speakers, specifically designed for training and evaluating PromptTTS, an expressive text-to-speech (TTS) model.
Although the audio files are not publicly available, PromptSpeech provides comprehensive metadata, including transcriptions, biological gender, speaker identity, pitch, speaking rate, energy (loudness), and style prompts.
To generate audio, we used a commercial TTS API<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://azure.microsoft.com/en-us/products/ai-services/text-to-speech/#overview" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:70%;">https://azure.microsoft.com/en-us/products/ai-services/text-to-speech/#overview</a></span></span></span>, which is also used in the official version, to synthesize utterances based on the official metadata.
This process resulted in approximately 50k utterances for the training set, each spoken by a single talker.
We then constructed <span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_smallcaps">SpeechCaps</span> from PromptSpeech as described in subsequent sections.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>multi-talker audio generation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">For each audio clip, we randomly determined the number of speakers (2 or 3) and sampled one utterance per speaker.
We considered two scenarios for concatenation.
The first scenario involves inserting silence between utterances, with the duration of silence uniformly sampled from the range [0, 1] seconds.
The second scenario involves overlapping utterances, where the overlap duration is uniformly sampled from the range [0.8, 2.4] seconds.
In the overlapping scenario, we simulate a natural conversation where different speakers talk simultaneously.
Finally, we generated 30k audio clips for training data.
The data, including prosodic attributes, starting times, and ending times, were recorded and used to generate text descriptions (Sec. <a href="#S2.SS3" title="2.3 Description generation ‣ 2 Pre-Training Task Generation ‣ SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>).</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Description generation</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Utilizing language models to generate data and evaluate model output has become a widespread practice in NLP research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
However, to the best of our knowledge, no large speech models currently exist that can generate high-quality descriptions of audio.
As an alternative, we used the metadata collected in Sec. <a href="#S2.SS2" title="2.2 multi-talker audio generation ‣ 2 Pre-Training Task Generation ‣ SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>, formatted it with a crafted prompt, and asked GPT-4o <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and Claude3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> to generate a text description for each audio, producing 20k descriptions with GPT-4o and 10k with Claude3.
Table <a href="#S2.T1" title="Table 1 ‣ 2 Pre-Training Task Generation ‣ SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents an example of the metadata and its corresponding description.
The description fluently captures each speaker's emotion, pitch, energy, and speaking rate, effectively illustrating their characteristics.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Training Overview</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">The training framework of DeSTA+ consists of two pre-training stages and one instruction-tuning stage.
In the first stage, <span id="S2.SS4.p1.1.1" class="ltx_text ltx_font_italic">single-talker</span> speaking style captioning, DeSTA+ learns the basics of speaker and prosodic information.
The second stage, <span id="S2.SS4.p1.1.2" class="ltx_text ltx_font_italic">multi-talker</span> speaking style captioning, enhances DeSTA+'s ability to identify different talkers and prosodic variations in mixed audios (Sec. <a href="#S2.SS2" title="2.2 multi-talker audio generation ‣ 2 Pre-Training Task Generation ‣ SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>).
Finally, in the instruction-tuning stage, we use a dataset with various speech tasks and task-specific instructions to develop DeSTA+'s instruction-following capabilities.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluation</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dynamic-SUPERB</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Dynamic-SUPERB offers a variety of speech tasks that necessitate an understanding of diverse speech information, requiring models to leverage their capabilities for various applications, such as speaker verification.
Specifically, Dynamic-SUPERB includes 55 tasks, and each task consists of text instructions, speech, and labels.
A model receives both the text instructions and speech as input and then performs the task based on the instructions.
Based on the specific speech information involved, these tasks are categorized into six dimensions: (1) content, (2) degradation, (3) paralinguistics, (4) semantics, (5) speaker, and (6) audio.
In this paper, we exclude the audio dimension since we focus on enhancing models' understanding of speaker and prosodic information rather than audio understanding.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span id="S3.SS2.1.1" class="ltx_text ltx_font_smallcaps">SpeechCaps</span> testing set</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">While Dynamic-SUPERB includes a wide range of tasks, it lacks those that directly assess specific prosodic attributes.
To address this, we created the <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">SpeechCaps</span> test set to evaluate models' understanding of prosodic information.
We built the <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_smallcaps">SpeechCaps</span> test set from the PromptSpeech test set.
Although PromptSpeech provides useful metadata for TTS development, it is not ideal for speech captioning tasks.
Figure <a href="#S3.F1" title="Figure 1 ‣ 3.2 SpeechCaps testing set ‣ 3 Evaluation ‣ SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the speaking rate distribution for utterances of a specific speaker in PromptSpeech, with colors indicating the original labels.
We observe overlaps in the distributions of speaking rate levels labeled as low, medium, and high, which can cause ambiguity and complicate evaluation.
Thus, constructing a unified pipeline to relabel the test data is essential.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2408.13891/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="403" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.3.2" class="ltx_text" style="font-size:90%;">Speaking rate distribution of utterances from a specific speaker (Jenny) in PromptSpeech.</span></figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">To relabel the PromptSpeech test set, we used Data-Speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and SoX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> to calculate speaking rate (phonemes per second), pitch, and energy.
These distributions are continuous, and we can define two thresholds to divide each attribute into three categories.
However, this also introduces ambiguity for utterances near the thresholds, leading to inconsistent descriptions by different models and complicating evaluation.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">To address this, we implemented a filtering process.
For each attribute, we selected data in the lowest 15%, middle 15%, and highest 15%, labeling them as low, medium, and high, respectively.
This filtering was applied to all three attributes: pitch, speaking rate, and energy.
We retained only the utterances falling into these regions across all attributes.
Besides, for pitch, we split utterances by biological gender due to significant differences in pitch distributions between males and females.
After filtering, we got 501 utterances for the test set.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Evaluating speaking style captioning during testing is challenging because a model's output may not include descriptions for all speakers or attributes, and descriptions may be inconsistent across different models.
To reduce ambiguity, we introduced question-answer (QA) pairs as a simplified form to facilitate evaluation.
We devised questions prompting models to identify speakers based on specific attributes.
Table <a href="#S2.T1" title="Table 1 ‣ 2 Pre-Training Task Generation ‣ SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows an example of question-answering for speaking style captioning.
In the question, we ask models to identify the speaker with a particular attribute based on their order or to directly inquire about a specific speaker's emotion.
For pitch-related questions, we specify the speaker's biological gender to mitigate the influence of varying pitch distributions across genders.
Using this approach, we generated 3813 QA pairs for testing.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Evaluation metrics</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">For each question in the test sets, a model can generate one of three responses: (1) an irrelevant answer, (2) a question-related but incorrect answer, or (3) a correct answer.
The model's ability to follow instructions determines whether it produces irrelevant content (case 1) or a question-relevant answer (cases 2 and 3).
Using the first QA pair in Table <a href="#S2.T1" title="Table 1 ‣ 2 Pre-Training Task Generation ‣ SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> as an example, if a model outputs ``the first speaker,'' it is irrelevant (case 1) because the question asks for emotion, not the speaker's order.
If the response is ``happy,'' it is question-relevant but incorrect (case 2).
While both cases (1) and (2) are incorrect, identifying the reason is crucial.
Therefore, we define the <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_bold">instruction-following rate</span> as the proportion of cases (2) and (3) in the model outputs.
Using LLMs for evaluation has been widely adopted in NLP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
Here, we use GPT-4o, providing it with a prompt that includes the question and the model-generated response.
GPT-4o evaluates whether the output is related to the question.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">By calculating the instruction-following rate, we can eliminate irrelevant outputs and focus on evaluating relevant ones.
Exact Match and F1 scores are common metrics in QA but fail to assess semantically similar answers accurately.
Thus, we use GPT-4o to check if a model output aligns with the ground truth.
We design a prompt that includes the question, its ground truth label, and the model output, and GPT-4o then assesses alignment.
We define two types of accuracy: <span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">overall accuracy</span>, the percentage of aligned answers in the entire dataset, and <span id="S3.SS3.p2.1.2" class="ltx_text ltx_font_bold">conditional accuracy</span>, the percentage of aligned answers among all relevant answers (cases 2 and 3).
In the following sections, accuracy refers to overall accuracy unless specified otherwise.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Settings</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Models</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We utilized four instruction-based speech models: LTU-AS, SALMONN, Qwen-Audio, and DeSTA.
Each model is primarily constructed with a speech encoder and an LLM and trained with parameter-efficient fine-tuning techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
The speech features serve as soft prompts that provide various speech information for the LLMs.
LTU-AS and DeSTA extract speech representations and transcriptions from Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, which are subsequently integrated into LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> for further reasoning.
SALMONN adopts a window-level Q-Former <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> to generate soft embeddings that fuse speech and audio representations from Whisper and BEATs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.
Qwen-Audio introduces several speech-task-specific tags to encourage knowledge sharing and minimize interference among different tasks.
Based on the size of the LLM, SALMONN is categorized into 7B and 13B versions, and we used the 7B version in the experiments.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">For LTU-AS, SALMONN, Qwen-Audio, and DeSTA, we used the official pre-trained models.
To train DeSTA+, we first applied PromptSpeech for single-talker speaking style captioning, followed by <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">SpeechCaps</span> for multi-talker speaking style captioning, and used the Dynamic-SUPERB training set for instruction tuning.
The only difference between DeSTA and DeSTA+ is the multi-talker speaking style captioning in pre-training.
In the multi-talker speaking style captioning stage, we set the learning rate to 1e-4, used a batch size of 12, and trained the model for 5 epochs.
All other hyperparameters matched the official DeSTA implementation.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Dynamic-SUPERB results</h3>

<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S5.T2.4.2" class="ltx_text" style="font-size:90%;">Comparison of <span id="S5.T2.4.2.1" class="ltx_text ltx_font_bold">overall accuracy</span> for various models on the Dynamic-SUPERB dataset, evaluated using GPT-4o.</span></figcaption>
<div id="S5.T2.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:193.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(75.5pt,-33.7pt) scale(1.53463324641961,1.53463324641961) ;">
<table id="S5.T2.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.5.1.1" class="ltx_tr">
<td id="S5.T2.5.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S5.T2.5.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="5"><span id="S5.T2.5.1.1.2.1" class="ltx_text ltx_font_bold">Model</span></td>
</tr>
<tr id="S5.T2.5.1.2" class="ltx_tr">
<td id="S5.T2.5.1.2.1" class="ltx_td ltx_align_left"><span id="S5.T2.5.1.2.1.1" class="ltx_text ltx_font_bold">Dimension</span></td>
<td id="S5.T2.5.1.2.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t"><span id="S5.T2.5.1.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">LTU-AS</span></td>
<td id="S5.T2.5.1.2.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t"><span id="S5.T2.5.1.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">SALMONN</span></td>
<td id="S5.T2.5.1.2.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t"><span id="S5.T2.5.1.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Qwen-Audio</span></td>
<td id="S5.T2.5.1.2.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t"><span id="S5.T2.5.1.2.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">DeSTA</span></td>
<td id="S5.T2.5.1.2.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t"><span id="S5.T2.5.1.2.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">DeSTA+</span></td>
</tr>
<tr id="S5.T2.5.1.3" class="ltx_tr">
<td id="S5.T2.5.1.3.1" class="ltx_td ltx_align_left ltx_border_t">(a) Content</td>
<td id="S5.T2.5.1.3.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">0.445</td>
<td id="S5.T2.5.1.3.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">0.521</td>
<td id="S5.T2.5.1.3.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">0.622</td>
<td id="S5.T2.5.1.3.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">0.913</td>
<td id="S5.T2.5.1.3.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">0.877</td>
</tr>
<tr id="S5.T2.5.1.4" class="ltx_tr">
<td id="S5.T2.5.1.4.1" class="ltx_td ltx_align_left">(b) Degradation</td>
<td id="S5.T2.5.1.4.2" class="ltx_td ltx_nopad_l ltx_align_center">0.366</td>
<td id="S5.T2.5.1.4.3" class="ltx_td ltx_nopad_l ltx_align_center">0.283</td>
<td id="S5.T2.5.1.4.4" class="ltx_td ltx_nopad_l ltx_align_center">0.320</td>
<td id="S5.T2.5.1.4.5" class="ltx_td ltx_nopad_l ltx_align_center">0.619</td>
<td id="S5.T2.5.1.4.6" class="ltx_td ltx_nopad_l ltx_align_center">0.673</td>
</tr>
<tr id="S5.T2.5.1.5" class="ltx_tr">
<td id="S5.T2.5.1.5.1" class="ltx_td ltx_align_left">(c) Paralinguistics</td>
<td id="S5.T2.5.1.5.2" class="ltx_td ltx_nopad_l ltx_align_center">0.250</td>
<td id="S5.T2.5.1.5.3" class="ltx_td ltx_nopad_l ltx_align_center">0.294</td>
<td id="S5.T2.5.1.5.4" class="ltx_td ltx_nopad_l ltx_align_center">0.268</td>
<td id="S5.T2.5.1.5.5" class="ltx_td ltx_nopad_l ltx_align_center">0.513</td>
<td id="S5.T2.5.1.5.6" class="ltx_td ltx_nopad_l ltx_align_center">0.476</td>
</tr>
<tr id="S5.T2.5.1.6" class="ltx_tr">
<td id="S5.T2.5.1.6.1" class="ltx_td ltx_align_left">(d) Semantics</td>
<td id="S5.T2.5.1.6.2" class="ltx_td ltx_nopad_l ltx_align_center">0.362</td>
<td id="S5.T2.5.1.6.3" class="ltx_td ltx_nopad_l ltx_align_center">0.509</td>
<td id="S5.T2.5.1.6.4" class="ltx_td ltx_nopad_l ltx_align_center">0.480</td>
<td id="S5.T2.5.1.6.5" class="ltx_td ltx_nopad_l ltx_align_center">0.743</td>
<td id="S5.T2.5.1.6.6" class="ltx_td ltx_nopad_l ltx_align_center">0.729</td>
</tr>
<tr id="S5.T2.5.1.7" class="ltx_tr">
<td id="S5.T2.5.1.7.1" class="ltx_td ltx_align_left ltx_border_bb">(e) Speaker</td>
<td id="S5.T2.5.1.7.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">0.407</td>
<td id="S5.T2.5.1.7.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">0.332</td>
<td id="S5.T2.5.1.7.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">0.422</td>
<td id="S5.T2.5.1.7.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">0.540</td>
<td id="S5.T2.5.1.7.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">0.671</td>
</tr>
</table>
</span></div>
</figure>
<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We evaluated the proposed pre-training approach's enhancement of model capabilities across a broader range of tasks beyond speech captioning.
Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Dynamic-SUPERB results ‣ 5 Results ‣ SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">overall accuracy</span> of the models on Dynamic-SUPERB, using GPT-4o for semantic alignment between model outputs and the ground truth.
Due to space constraints, we do not detail all tasks individually.
Instead, we categorized accuracy results into the following task categories: (a) content, (b) degradation, (c) paralinguistics, (d) semantics, and (e) speaker.
It is important to note that direct comparisons across different dimensions are not feasible due to varying task settings and difficulties.
Alternatively, we compare the performance of different models within the same dimension.
We begin with performance comparisons among LTU-AS, SALMONN, and Qwen-Audio, then shift to DeSTA and DeSTA+. This is because the DeSTA models used an instruction-tuning dataset closely aligned with Dynamic-SUPERB, which could bias comparisons.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">From Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Dynamic-SUPERB results ‣ 5 Results ‣ SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we see that among LTU-AS, SALMONN, and Qwen-Audio, no single model dominated all dimensions, and each had distinct strengths.
Qwen-Audio excelled in content tasks, outperforming LTU-AS and SALMONN, and showed moderate performance in other dimensions except paralinguistics.
LTU-AS, while lagging in content and semantic tasks, was competitive in degradation and speaker tasks.
SALMONN showed superior performance in paralinguistics and semantics but struggled with degradation tasks.
The DeSTA models demonstrated superior performance across all dimensions compared to the above three models, likely due to their instruction-tuning dataset being closely aligned with Dynamic-SUPERB.
Notably, DeSTA+ significantly outperformed in the speaker dimension, indicating an enhanced understanding of speaker information with the proposed approach.
Surprisingly, DeSTA+ also showed improved accuracy in degradation tasks, suggesting that learning from more complex audio scenarios can boost performance across various aspects.
For other dimensions, DeSTA+ performed similarly to DeSTA, indicating that the proposed approach did not degrade performance.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">To examine the models' understanding of speaker and prosodic information, we show the performance of speaker verification and emotion recognition in Table <a href="#S5.T3" title="Table 3 ‣ 5.1 Dynamic-SUPERB results ‣ 5 Results ‣ SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
In speaker verification (Table <a href="#S5.T3.st1" title="In Table 3 ‣ 5.1 Dynamic-SUPERB results ‣ 5 Results ‣ SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>), LTU-AS and SALMONN achieved high instruction-following rates (over 85%) but their accuracies were close to random guesses (50%).
Qwen-Audio had a lower instruction-following rate (50%), impacting its overall accuracy on the task.
For emotion recognition (Table <a href="#S5.T3.st2" title="In Table 3 ‣ 5.1 Dynamic-SUPERB results ‣ 5 Results ‣ SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a>), Qwen-Audio had higher instruction-following rates and accuracy than LTU-AS and SALMONN, but its accuracy was still quite low (around 34%).
Conversely, though LTU-AS maintained a following rate of around 60%, its accuracy dropped significantly.
These results show that LTU-AS, SALMONN, and Qwen-Audio have poor capabilities in understanding speakers and emotions.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">DeSTA and DeSTA+ achieved much higher instruction-following rates than the other models.
In speaker verification, DeSTA+ outperformed DeSTA by around 10% accuracy, showing its enhanced ability to distinguish different speakers.
A similar trend was observed in emotion recognition.
DeSTA+ achieved the highest accuracy, demonstrating the effectiveness of the proposed approach.
Although the use of instruction-tuning datasets brought some bias, DeSTA+'s superior performance over DeSTA justifies the effectiveness of the proposed approach, as no new utterances were introduced in training, and we only combined them to create more complex data.</p>
</div>
<figure id="S5.T3" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.2.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.3.2" class="ltx_text" style="font-size:90%;">Performance comparison of models on speaker verification and emotion recognition tasks in Dynamic-SUPERB.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.T3.st1" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.st1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.T3.st1.3.2" class="ltx_text" style="font-size:90%;">speaker verification</span></figcaption>
<div id="S5.T3.st1.4" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:145.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(82.8pt,-27.8pt) scale(1.61814960167663,1.61814960167663) ;">
<table id="S5.T3.st1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.st1.4.1.1" class="ltx_tr">
<td id="S5.T3.st1.4.1.1.1" class="ltx_td ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S5.T3.st1.4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="5"><span id="S5.T3.st1.4.1.1.2.1" class="ltx_text ltx_font_bold">Model</span></td>
</tr>
<tr id="S5.T3.st1.4.1.2" class="ltx_tr">
<td id="S5.T3.st1.4.1.2.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.st1.4.1.2.1.1" class="ltx_text ltx_font_bold">Metric</span></td>
<td id="S5.T3.st1.4.1.2.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.st1.4.1.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">LTU-AS</span></td>
<td id="S5.T3.st1.4.1.2.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.st1.4.1.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">SALMONN</span></td>
<td id="S5.T3.st1.4.1.2.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.st1.4.1.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Qwen-Audio</span></td>
<td id="S5.T3.st1.4.1.2.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.st1.4.1.2.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">DeSTA</span></td>
<td id="S5.T3.st1.4.1.2.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.st1.4.1.2.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">DeSTA+</span></td>
</tr>
<tr id="S5.T3.st1.4.1.3" class="ltx_tr">
<td id="S5.T3.st1.4.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">(a) IF Rate</td>
<td id="S5.T3.st1.4.1.3.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.865</td>
<td id="S5.T3.st1.4.1.3.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.930</td>
<td id="S5.T3.st1.4.1.3.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.509</td>
<td id="S5.T3.st1.4.1.3.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.950</td>
<td id="S5.T3.st1.4.1.3.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.943</td>
</tr>
<tr id="S5.T3.st1.4.1.4" class="ltx_tr">
<td id="S5.T3.st1.4.1.4.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(b) Overall Acc.</td>
<td id="S5.T3.st1.4.1.4.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.434</td>
<td id="S5.T3.st1.4.1.4.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.465</td>
<td id="S5.T3.st1.4.1.4.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.212</td>
<td id="S5.T3.st1.4.1.4.5" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.580</td>
<td id="S5.T3.st1.4.1.4.6" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.645</td>
</tr>
<tr id="S5.T3.st1.4.1.5" class="ltx_tr">
<td id="S5.T3.st1.4.1.5.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">(c) Cond. Acc.</td>
<td id="S5.T3.st1.4.1.5.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">0.501</td>
<td id="S5.T3.st1.4.1.5.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">0.500</td>
<td id="S5.T3.st1.4.1.5.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">0.418</td>
<td id="S5.T3.st1.4.1.5.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">0.611</td>
<td id="S5.T3.st1.4.1.5.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">0.684</td>
</tr>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S5.T3.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\justify</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.T3.st2" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.st2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.T3.st2.3.2" class="ltx_text" style="font-size:90%;">emotion recognition</span></figcaption>
<div id="S5.T3.st2.4" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:145.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(82.8pt,-27.8pt) scale(1.61814960167663,1.61814960167663) ;">
<table id="S5.T3.st2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.st2.4.1.1" class="ltx_tr">
<td id="S5.T3.st2.4.1.1.1" class="ltx_td ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S5.T3.st2.4.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="5"><span id="S5.T3.st2.4.1.1.2.1" class="ltx_text ltx_font_bold">Model</span></td>
</tr>
<tr id="S5.T3.st2.4.1.2" class="ltx_tr">
<td id="S5.T3.st2.4.1.2.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.st2.4.1.2.1.1" class="ltx_text ltx_font_bold">Metric</span></td>
<td id="S5.T3.st2.4.1.2.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.st2.4.1.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">LTU-AS</span></td>
<td id="S5.T3.st2.4.1.2.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.st2.4.1.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">SALMONN</span></td>
<td id="S5.T3.st2.4.1.2.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.st2.4.1.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Qwen-Audio</span></td>
<td id="S5.T3.st2.4.1.2.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.st2.4.1.2.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">DeSTA</span></td>
<td id="S5.T3.st2.4.1.2.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.st2.4.1.2.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">DeSTA+</span></td>
</tr>
<tr id="S5.T3.st2.4.1.3" class="ltx_tr">
<td id="S5.T3.st2.4.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">(a) IF Rate</td>
<td id="S5.T3.st2.4.1.3.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.593</td>
<td id="S5.T3.st2.4.1.3.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.670</td>
<td id="S5.T3.st2.4.1.3.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.852</td>
<td id="S5.T3.st2.4.1.3.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.905</td>
<td id="S5.T3.st2.4.1.3.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.938</td>
</tr>
<tr id="S5.T3.st2.4.1.4" class="ltx_tr">
<td id="S5.T3.st2.4.1.4.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(b) Overall Acc.</td>
<td id="S5.T3.st2.4.1.4.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.055</td>
<td id="S5.T3.st2.4.1.4.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.200</td>
<td id="S5.T3.st2.4.1.4.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.341</td>
<td id="S5.T3.st2.4.1.4.5" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.598</td>
<td id="S5.T3.st2.4.1.4.6" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.658</td>
</tr>
<tr id="S5.T3.st2.4.1.5" class="ltx_tr">
<td id="S5.T3.st2.4.1.5.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">(c) Cond. Acc.</td>
<td id="S5.T3.st2.4.1.5.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">0.094</td>
<td id="S5.T3.st2.4.1.5.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">0.299</td>
<td id="S5.T3.st2.4.1.5.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">0.400</td>
<td id="S5.T3.st2.4.1.5.5" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">0.660</td>
<td id="S5.T3.st2.4.1.5.6" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">0.701</td>
</tr>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S5.T3.5" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\justify</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.T3.6" class="ltx_p ltx_figure_panel ltx_align_center">IF: instruction-following, Cond.: conditional.</p>
</div>
</div>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span><span id="S5.SS2.1.1" class="ltx_text ltx_font_smallcaps">SpeechCaps</span> results</h3>

<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S5.T4.4.2" class="ltx_text" style="font-size:90%;">Evaluation of instruction-following rate and accuracy across various models on <span id="S5.T4.4.2.1" class="ltx_text ltx_font_smallcaps">SpeechCaps</span> test set using GPT-4o.</span></figcaption>
<div id="S5.T4.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:137.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(74.8pt,-23.7pt) scale(1.52697749510248,1.52697749510248) ;">
<table id="S5.T4.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.5.1.1" class="ltx_tr">
<td id="S5.T4.5.1.1.1" class="ltx_td ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S5.T4.5.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="5"><span id="S5.T4.5.1.1.2.1" class="ltx_text ltx_font_bold">Model</span></td>
</tr>
<tr id="S5.T4.5.1.2" class="ltx_tr">
<td id="S5.T4.5.1.2.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T4.5.1.2.1.1" class="ltx_text ltx_font_bold">Metric</span></td>
<td id="S5.T4.5.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T4.5.1.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">LTU-AS</span></td>
<td id="S5.T4.5.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T4.5.1.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">SALMONN</span></td>
<td id="S5.T4.5.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T4.5.1.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Qwen-Audio</span></td>
<td id="S5.T4.5.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T4.5.1.2.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">DeSTA</span></td>
<td id="S5.T4.5.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T4.5.1.2.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">DeSTA+</span></td>
</tr>
<tr id="S5.T4.5.1.3" class="ltx_tr">
<td id="S5.T4.5.1.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">(a) IF Rate</td>
<td id="S5.T4.5.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.379</td>
<td id="S5.T4.5.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.519</td>
<td id="S5.T4.5.1.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.783</td>
<td id="S5.T4.5.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.607</td>
<td id="S5.T4.5.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.483</td>
</tr>
<tr id="S5.T4.5.1.4" class="ltx_tr">
<td id="S5.T4.5.1.4.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">(b) Overall Acc.</td>
<td id="S5.T4.5.1.4.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.066</td>
<td id="S5.T4.5.1.4.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.101</td>
<td id="S5.T4.5.1.4.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.191</td>
<td id="S5.T4.5.1.4.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.159</td>
<td id="S5.T4.5.1.4.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.127</td>
</tr>
<tr id="S5.T4.5.1.5" class="ltx_tr">
<td id="S5.T4.5.1.5.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">(c) Cond. Acc.</td>
<td id="S5.T4.5.1.5.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">0.174</td>
<td id="S5.T4.5.1.5.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">0.194</td>
<td id="S5.T4.5.1.5.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">0.268</td>
<td id="S5.T4.5.1.5.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">0.315</td>
<td id="S5.T4.5.1.5.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">0.330</td>
</tr>
</table>
</span></div>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In addition to Dynamic-SUPERB, we tested models on the <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">SpeechCaps</span> test set to assess their understanding of prosodic information more directly.
Table <a href="#S5.T4" title="Table 4 ‣ 5.2 SpeechCaps results ‣ 5 Results ‣ SpeechCaps: Advancing Instruction-Based Universal Speech Models with Multi-Talker Speaking Style Captioning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the performance of each model.
There were significant differences in instruction-following rates (row (a)).
Qwen-Audio achieved the highest following rate (about 80%), while LTU-AS lagged significantly behind.
SALMONN and DeSTA had similar rates around 50%, with DeSTA slightly higher.
Different instruction-following rates directly impacted performance on the test set.
For accurate output, a model must correctly understand the instruction, and a low instruction-following rate leads to lower accuracy.
In row (b), all models showed poor <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_bold">overall accuracy</span>, with Qwen-Audio achieving the highest at about 20%, and others lagging significantly.
This low performance could result from poor instruction-following or a lack of understanding of speaker and prosodic information.
To this end, we introduced <span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_bold">conditioned accuracy</span> in row (c), evaluating only instances where instructions were followed correctly.
Higher conditioned accuracy means a better understanding of speaker and prosodic information.
Although Qwen-Audio had the highest overall accuracy, DeSTA outperformed it in conditioned accuracy by about 5%.
This suggests DeSTA better understands speaker and prosodic information, likely because it uses the speech captioning task for pre-training.
However, DeSTA's lower instruction-following rate led to its lower overall accuracy.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Last, DeSTA+ had a dropped instruction-following rate than DeSTA but outperformed in conditioned accuracy, and both surpassed the other three models.
This indicates that training with multi-talker data enhanced DeSTA+'s ability to capture speaker and prosodic information, though the different forms of captioning tasks (description in training and QA in testing) impacted its instruction-following capability.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Instruction-based speech models are gaining popularity across various applications, yet enhancing their fundamental capabilities to benefit several downstream tasks remains unexplored.
This paper introduces a novel task, multi-talker speaking style captioning, as a pre-training approach to enhance model capabilities.
Evaluation on Dynamic-SUPERB shows that this approach significantly improves understanding of speaker and prosodic information, achieving state-of-the-art performance.
Besides, we built a question-answering dataset for prosodic attributes, revealing that the four baseline models cannot capture prosodic information among different talkers.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y. Gong, H. Luo, A. H. Liu, L. Karlinsky, and J. R. Glass, ``Listen, think, and understand,'' in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2024.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Wu, Y. Gaur, Z. Chen, L. Zhou, Y. Zhu, T. Wang, J. Li, S. Liu, B. Ren, L. Liu <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``On decoder-only architecture for speech-to-text and large language model integration,'' in <em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.   IEEE, 2023, pp. 1–8.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
M. Wang, W. Han, I. Shafran, Z. Wu, C.-C. Chiu, Y. Cao, N. Chen, Y. Zhang, H. Soltau, P. K. Rubenstein <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Slm: Bridge the thin gap between speech and text foundation models,'' in <em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.   IEEE, 2023, pp. 1–8.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
E. Nachmani, A. Levkovitch, R. Hirsch, J. Salazar, C. Asawaroengchai, S. Mariooryad, E. Rivlin, R. Skerry-Ryan, and M. T. Ramanovich, ``Spoken question answering and speech continuation using spectrogram-powered llm,'' in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2024.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Z. Kong, A. Goel, R. Badlani, W. Ping, R. Valle, and B. Catanzaro, ``Audio flamingo: A novel audio language model with few-shot learning and dialogue abilities,'' in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, 2024.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Y. Gong, A. H. Liu, H. Luo, L. Karlinsky, and J. Glass, ``Joint audio and speech understanding,'' in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
C. Tang, W. Yu, G. Sun, X. Chen, T. Tan, W. Li, L. Lu, M. Zejun, and C. Zhang, ``Salmonn: Towards generic hearing abilities for large language models,'' in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">The Twelfth International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y. Chu, J. Xu, X. Zhou, Q. Yang, S. Zhang, Z. Yan, C. Zhou, and J. Zhou, ``Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models,'' <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.07919</em>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Y. Shu, S. Dong, G. Chen, W. Huang, R. Zhang, D. Shi, Q. Xiang, and Y. Shi, ``Llasm: Large language and speech model,'' <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.15930</em>, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S. Hu, L. Zhou, S. Liu, S. Chen, H. Hao, J. Pan, X. Liu, J. Li, S. Sivasankaran, L. Liu <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Wavllm: Towards robust and adaptive speech large language model,'' <em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.00656</em>, 2024.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
K.-H. Lu, Z. Chen, S.-W. Fu, H. Huang, B. Ginsburg, Y.-C. F. Wang, and H. yi Lee, ``Desta: Enhancing speech language models through descriptive speech-text alignment,'' in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH 2024</em>, 2024.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, ``Finetuned language models are zero-shot learners,'' in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
C.-y. Huang, K.-H. Lu, S.-H. Wang, C.-Y. Hsiao, C.-Y. Kuan, H. Wu, S. Arora, K.-W. Chang, J. Shi, Y. Peng <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Dynamic-superb: Towards a dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech,'' in <em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2024, pp. 12 136–12 140.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Z. Guo, Y. Leng, Y. Wu, S. Zhao, and X. Tan, ``Prompttts: Controllable text-to-speech with text descriptions,'' in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
V. Kumar, A. Choudhary, and E. Cho, ``Data augmentation using pre-trained transformer models,'' in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems</em>, W. M. Campbell, A. Waibel, D. Hakkani-Tur, T. J. Hazen, K. Kilgour, E. Cho, V. Kumar, and H. Glaude, Eds.   Suzhou, China: Association for Computational Linguistics, Dec. 2020, pp. 18–26. [Online]. Available: <a target="_blank" href="https://aclanthology.org/2020.lifelongnlp-1.3" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2020.lifelongnlp-1.3</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
K. M. Yoo, D. Park, J. Kang, S.-W. Lee, and W. Park, ``GPT3Mix: Leveraging large-scale language models for text augmentation,'' in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2021</em>, M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds.   Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 2225–2239. [Online]. Available: <a target="_blank" href="https://aclanthology.org/2021.findings-emnlp.192" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.findings-emnlp.192</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Gpt-4 technical report,'' <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Anthropic, ``Claude,'' <a target="_blank" href="https://www.anthropic.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.anthropic.com</a>, 2024, large language model.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Y. Lacombe, V. Srivastav, and S. Gandhi, ``Data-speech,'' <a target="_blank" href="https://github.com/ylacombe/dataspeech" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ylacombe/dataspeech</a>, 2024.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
SoX Contributors, ``Sox - sound exchange,'' 2024, accessed: 2024-06-16. [Online]. Available: <a target="_blank" href="http://sox.sourceforge.net/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://sox.sourceforge.net/</a>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J. Wang, Y. Liang, F. Meng, Z. Sun, H. Shi, Z. Li, J. Xu, J. Qu, and J. Zhou, ``Is chatgpt a good nlg evaluator? a preliminary study,'' in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of EMNLP Workshop</em>, 2023, p. 1.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Y. Liu, D. Iter, Y. Xu, S. Wang, R. Xu, and C. Zhu, ``G-eval: Nlg evaluation using gpt-4 with better human alignment,'' in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, 2023, pp. 2511–2522.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
C.-H. Chiang and H.-Y. Lee, ``Can large language models be an alternative to human evaluations?'' in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 2023, pp. 15 607–15 631.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
E. J. Hu, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Lora: Low-rank adaptation of large language models,'' in <em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
R. He, L. Liu, H. Ye, Q. Tan, B. Ding, L. Cheng, J. Low, L. Bing, and L. Si, ``On the effectiveness of adapter-based tuning for pretrained language model adaptation,'' in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, 2021, pp. 2208–2222.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, ``Robust speech recognition via large-scale weak supervision,'' in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2023, pp. 28 492–28 518.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Llama: Open and efficient foundation language models,'' <em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Llama 2: Open foundation and fine-tuned chat models,'' <em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>, 2023.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J. Li, D. Li, S. Savarese, and S. Hoi, ``Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,'' in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.   PMLR, 2023, pp. 19 730–19 742.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
S. Chen, Y. Wu, C. Wang, S. Liu, D. Tompkins, Z. Chen, W. Che, X. Yu, and F. Wei, ``Beats: audio pre-training with acoustic tokenizers,'' in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine Learning</em>, 2023, pp. 5178–5193.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.13890" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.13891" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.13891">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.13891" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.13892" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 18:01:23 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
