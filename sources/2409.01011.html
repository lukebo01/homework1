<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.01011] Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts</title><meta property="og:description" content="This study presents a multi-modal multi-granularity tokenizer specifically designed for analyzing ancient Chinese scripts, focusing on the Chu bamboo slip (CBS) script used during the Spring and Autumn and Warring Stat…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.01011">

<!--Generated on Sat Oct  5 22:46:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yingfa Chen, Chenlong Hu, Cong Feng, Chenyang Song 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_bold">Shi Yu</span>, <span id="id2.2.id2" class="ltx_text ltx_font_bold">Xu Han</span>, <span id="id3.3.id3" class="ltx_text ltx_font_bold">Zhiyuan Liu</span>, <span id="id4.4.id4" class="ltx_text ltx_font_bold">Maosong Sun
<br class="ltx_break"></span>NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing, China 
<br class="ltx_break"><span id="id5.5.id5" class="ltx_text ltx_font_typewriter">yingfa-c24@mails.tsinghua.edu.cn</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id6.id1" class="ltx_p">This study presents a multi-modal multi-granularity tokenizer specifically designed for analyzing ancient Chinese scripts, focusing on the Chu bamboo slip (CBS) script used during the Spring and Autumn and Warring States period (771-256 BCE) in Ancient China. Considering the complex hierarchical structure of ancient Chinese scripts, where a single character may be a combination of multiple sub-characters, our tokenizer first adopts character detection to locate character boundaries, and then conducts character recognition at both the character and sub-character levels. Moreover, to support the academic community, we have also assembled the first large-scale dataset of CBSs with over 100K annotated character image scans. On the part-of-speech tagging task built on our dataset, using our tokenizer gives a 5.5% relative improvement in F1-score compared to mainstream sub-word tokenizers. Our work not only aids in further investigations of the specific script but also has the potential to advance research on other forms of ancient Chinese scripts.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The code and data are available at: <a target="_blank" href="https://www.github.com/THUNLP/Chujian" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.github.com/THUNLP/Chujian</a></span></span></span></p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.1" class="ltx_block ltx_align_bottom">
<p id="p1.1.1" class="ltx_p"><span id="p1.1.1.1" class="ltx_text ltx_font_bold">Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.2" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.2.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_top">
<span class="ltx_thead">
<span id="p1.1.2.1.1.1.1" class="ltx_tr">
<span id="p1.1.2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="p1.1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Yingfa Chen, Chenlong Hu, Cong Feng, Chenyang Song</span></span></span>
<span id="p1.1.2.1.1.2.2" class="ltx_tr">
<span id="p1.1.2.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="p1.1.2.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Shi Yu</span>, <span id="p1.1.2.1.1.2.2.1.2" class="ltx_text ltx_font_bold">Xu Han</span>, <span id="p1.1.2.1.1.2.2.1.3" class="ltx_text ltx_font_bold">Zhiyuan Liu</span>, <span id="p1.1.2.1.1.2.2.1.4" class="ltx_text ltx_font_bold">Maosong Sun</span></span></span>
</span>
<span class="ltx_tbody">
<span id="p1.1.2.1.1.3.1" class="ltx_tr">
<span id="p1.1.2.1.1.3.1.1" class="ltx_td ltx_align_center">NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing, China</span></span>
<span id="p1.1.2.1.1.4.2" class="ltx_tr">
<span id="p1.1.2.1.1.4.2.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.4.2.1.1" class="ltx_text ltx_font_typewriter">yingfa-c24@mails.tsinghua.edu.cn</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Deep neural networks have demonstrated remarkable success in various natural language processing tasks <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib15" title="" class="ltx_ref">2023</a>; Touvron et al., <a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite> as well as the analyses of ancient languages <cite class="ltx_cite ltx_citemacro_citep">(Sommerschield et al., <a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>. Inspired by these former works, we aim to apply deep learning to the analysis of ancient Chinese scripts. However, this application faces three challenges: (1) Most of these ancient scripts are stored as images, which are more difficult to analyze than texts. (2) A large proportion of the characters is rare or undeciphered, making it challenging to train data-driven neural networks. This also implies that the widely-used subword tokenizers such as BPE <cite class="ltx_cite ltx_citemacro_citep">(Sennrich et al., <a href="#bib.bib18" title="" class="ltx_ref">2016</a>)</cite> and SentencePiece <cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson, <a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite> fall short because the neural networks struggle to learn informative representations of the rare and undeciphered characters. (3) Current tokenizers struggle to generalize to unseen materials, in which there is a considerable ratio of out-of-vocabulary (OOV) characters.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.01011/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="456" height="448" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of our proposed tokenizer on an example. Each ancient character is mapped to a modern character if possible. Otherwise, the tokenizer rolls back to decomposing the character into sub-character units, potentially containing useful information. One possible deciphering of the text is “At first, action is not simple”. The slip shown is the 14th slip in Zhonggong document from the Shanghai Museum Slips.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To overcome these challenges, we propose a novel multi-modal multi-granularity tokenizer tailored for ancient Chinese scripts, focusing on the 2000-year-old Chu bamboo slip (CBS) script from ancient China. The tokenization pipeline begins by detecting and ordering the characters in image scans of the raw materials into a sequence of character images. Next, each character is recognized within a pre-defined vocabulary. If the recognition confidence is low, the tokenizer rolls back to tokenizing the character into <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">sub-character components</span> (components that make up Chinese characters and are larger than a stroke, and smaller than a character) which may contain rich information about the semantics or phonetics of the text <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a href="#bib.bib23" title="" class="ltx_ref">2014</a>); Nguyen et al. (<a href="#bib.bib14" title="" class="ltx_ref">2017</a>); Si et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023a</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To demonstrate the effectiveness of our tokenizer, we collect and release the first dataset of CBS texts. It contains 102,722 annotated CBS character images, from 5,033 slips and 164 documents. To facilitate further investigation, we have developed a user-friendly platform where researchers with different expertise can access and analyze the dataset with ease.
The proposed tokenizer significantly outperforms the existing baselines, especially on the task of part-of-speech tagging.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The main contributions of this study can be summarized as follows:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We collect, process, and release CHUBS, the first large-scale dataset on Chu Bamboo Slip script in a format that is convenient for typical NLP workflows.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose an annotation scheme for provided useful information about the sub-character features of CBS scripts to address the large proportion of out-of-vocabulary characters prevalent in CBS.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Based on the sub-character annotations, we propose a multi-granularity tokenizer that outperform ordinary character-based tokenizers on downstream tasks.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We build a platform for easy access to the data for researchers of all background to facilitate future research.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Tokenization</h5>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Tokenization is the process of splitting a sentence into units. It is essential to current natural language processing techniques and have an integral impact on downstream performance <cite class="ltx_cite ltx_citemacro_citep">(Mielke et al., <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>. Current NLP tokenizers accept text sequences as inputs and split them into pieces that are then turned into integers to be handled by neural networks. In this work, although the tokenization process start from the image scan of text inscriptions, the goal is to convert the raw representation into a sequence of simple representations that are easy for the pipeline to handle. Therefore, we call our method a tokenization pipeline.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Chinese Tokenization</h5>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">Regarding Chinese characters, most existing tokenization methods operate on the character level. Each token is either once character or a combination of character <cite class="ltx_cite ltx_citemacro_citep">(Si et al., <a href="#bib.bib19" title="" class="ltx_ref">2023a</a>)</cite>. Such method disregard the fact that each Chinese character is composed of components that encode information that may be useful for analyzing the language. Numerous works have shown that tokenizing characters at the sub-character level can improve the downstream performance of Chinese, Japanese, and Korean neural models. Some notable works include <cite class="ltx_cite ltx_citemacro_citet">Sun et al. (<a href="#bib.bib23" title="" class="ltx_ref">2014</a>); Li et al. (<a href="#bib.bib11" title="" class="ltx_ref">2015</a>); Song et al. (<a href="#bib.bib22" title="" class="ltx_ref">2018</a>); Si et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023a</a>)</cite>, which have shown that utilizing sub-character components can improve the quality of learned embeddings, as measured by improved performance or efficiency in a wide range of language understanding tasks compared to conventional tokenizers. For pre-trained language models, <cite class="ltx_cite ltx_citemacro_citet">Si et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023a</a>)</cite> show that converting Chinese characters to sub-character sequences can improve the efficiency and robustness in general language understanding. In language generation,  <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite> have showed that using stroke information can improve English-Chinese translations.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Deep Learning Applications in Ancient Scripts</h5>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">As a result of the recent advances in the capabilities of deep neural networks in computer vision and natural language processing, there have been numerous works that utilize deep learning methods to assist research in ancient scripts <cite class="ltx_cite ltx_citemacro_citep">(Sommerschield et al., <a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>. Some examples include ancient Greek <cite class="ltx_cite ltx_citemacro_citep">(Assael et al., <a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite>, Devanagari <cite class="ltx_cite ltx_citemacro_citep">(Narang et al., <a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite>, ancient Chinese <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Liu, <a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite>, ancient Japanese <cite class="ltx_cite ltx_citemacro_citep">(Clanuwat et al., <a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite>, etc. To the best of our knowledge, our work represent the first attempt to apply deep learning methods in the processing of Chu bamboo slips.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We begin with a brief introduction to the background of the CBSs (Section <a href="#S3.SS1" title="3.1 Chu Bamboo Slips ‣ 3 Dataset ‣ Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>). Then, we describe the collection process of our dataset called CHUBS (<span id="S3.p1.1.1" class="ltx_text ltx_font_bold">CHU</span> <span id="S3.p1.1.2" class="ltx_text ltx_font_bold">B</span>amboo <span id="S3.p1.1.3" class="ltx_text ltx_font_bold">S</span>lips) (Section <a href="#S3.SS2" title="3.2 CHUBS ‣ 3 Dataset ‣ Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>). Finally, we present an open platform for convenient access to our data, especially for researchers of different backgrounds (Section <a href="#S3.SS3" title="3.3 Open Platform ‣ 3 Dataset ‣ Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Chu Bamboo Slips</h3>

<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Source name</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Chinese name</span></th>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.1.1.1.3.1" class="ltx_text ltx_font_bold"># documents</span></td>
<td id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.1.1.1.4.1" class="ltx_text ltx_font_bold"># slips</span></td>
<td id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.1.1.1.5.1" class="ltx_text ltx_font_bold"># characters</span></td>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<th id="S3.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Tsinghua University Slips</th>
<th id="S3.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">清华简</th>
<td id="S3.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">50</td>
<td id="S3.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">1,402</td>
<td id="S3.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">31,468</td>
</tr>
<tr id="S3.T1.1.3.3" class="ltx_tr">
<th id="S3.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Shanghai Museum Slips</th>
<th id="S3.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">上博简</th>
<td id="S3.T1.1.3.3.3" class="ltx_td ltx_align_center">60</td>
<td id="S3.T1.1.3.3.4" class="ltx_td ltx_align_center">881</td>
<td id="S3.T1.1.3.3.5" class="ltx_td ltx_align_center">25,795</td>
</tr>
<tr id="S3.T1.1.4.4" class="ltx_tr">
<th id="S3.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Baoshan Slips</th>
<th id="S3.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">包山简</th>
<td id="S3.T1.1.4.4.3" class="ltx_td ltx_align_center">4</td>
<td id="S3.T1.1.4.4.4" class="ltx_td ltx_align_center">337</td>
<td id="S3.T1.1.4.4.5" class="ltx_td ltx_align_center">12,647</td>
</tr>
<tr id="S3.T1.1.5.5" class="ltx_tr">
<th id="S3.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Guodian Slips</th>
<th id="S3.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">郭店简</th>
<td id="S3.T1.1.5.5.3" class="ltx_td ltx_align_center">18</td>
<td id="S3.T1.1.5.5.4" class="ltx_td ltx_align_center">705</td>
<td id="S3.T1.1.5.5.5" class="ltx_td ltx_align_center">11,865</td>
</tr>
<tr id="S3.T1.1.6.6" class="ltx_tr">
<th id="S3.T1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Geling Slips</th>
<th id="S3.T1.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">葛陵简</th>
<td id="S3.T1.1.6.6.3" class="ltx_td ltx_align_center">8</td>
<td id="S3.T1.1.6.6.4" class="ltx_td ltx_align_center">743</td>
<td id="S3.T1.1.6.6.5" class="ltx_td ltx_align_center">6,209</td>
</tr>
<tr id="S3.T1.1.7.7" class="ltx_tr">
<th id="S3.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Zenghouyi Slips</th>
<th id="S3.T1.1.7.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">曾侯乙简</th>
<td id="S3.T1.1.7.7.3" class="ltx_td ltx_align_center">4</td>
<td id="S3.T1.1.7.7.4" class="ltx_td ltx_align_center">198</td>
<td id="S3.T1.1.7.7.5" class="ltx_td ltx_align_center">6,016</td>
</tr>
<tr id="S3.T1.1.8.8" class="ltx_tr">
<th id="S3.T1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Jiudian Slips</th>
<th id="S3.T1.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">九店简</th>
<td id="S3.T1.1.8.8.3" class="ltx_td ltx_align_center">2</td>
<td id="S3.T1.1.8.8.4" class="ltx_td ltx_align_center">232</td>
<td id="S3.T1.1.8.8.5" class="ltx_td ltx_align_center">2,956</td>
</tr>
<tr id="S3.T1.1.9.9" class="ltx_tr">
<th id="S3.T1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Wangshan Slips</th>
<th id="S3.T1.1.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">望山简</th>
<td id="S3.T1.1.9.9.3" class="ltx_td ltx_align_center">3</td>
<td id="S3.T1.1.9.9.4" class="ltx_td ltx_align_center">273</td>
<td id="S3.T1.1.9.9.5" class="ltx_td ltx_align_center">2,218</td>
</tr>
<tr id="S3.T1.1.10.10" class="ltx_tr">
<th id="S3.T1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Changtaiguan Slips</th>
<th id="S3.T1.1.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">长台关简</th>
<td id="S3.T1.1.10.10.3" class="ltx_td ltx_align_center">3</td>
<td id="S3.T1.1.10.10.4" class="ltx_td ltx_align_center">148</td>
<td id="S3.T1.1.10.10.5" class="ltx_td ltx_align_center">1,504</td>
</tr>
<tr id="S3.T1.1.11.11" class="ltx_tr">
<th id="S3.T1.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Zidanku Silk</th>
<th id="S3.T1.1.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">子弹库帛</th>
<td id="S3.T1.1.11.11.3" class="ltx_td ltx_align_center">7</td>
<td id="S3.T1.1.11.11.4" class="ltx_td ltx_align_center">7</td>
<td id="S3.T1.1.11.11.5" class="ltx_td ltx_align_center">1,471</td>
</tr>
<tr id="S3.T1.1.12.12" class="ltx_tr">
<th id="S3.T1.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Yangtianhu Slips</th>
<th id="S3.T1.1.12.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">仰天湖简</th>
<td id="S3.T1.1.12.12.3" class="ltx_td ltx_align_center">1</td>
<td id="S3.T1.1.12.12.4" class="ltx_td ltx_align_center">42</td>
<td id="S3.T1.1.12.12.5" class="ltx_td ltx_align_center">335</td>
</tr>
<tr id="S3.T1.1.13.13" class="ltx_tr">
<th id="S3.T1.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Wulipai Slips</th>
<th id="S3.T1.1.13.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">五里牌简</th>
<td id="S3.T1.1.13.13.3" class="ltx_td ltx_align_center">1</td>
<td id="S3.T1.1.13.13.4" class="ltx_td ltx_align_center">18</td>
<td id="S3.T1.1.13.13.5" class="ltx_td ltx_align_center">109</td>
</tr>
<tr id="S3.T1.1.14.14" class="ltx_tr">
<th id="S3.T1.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Xiyangpo Slips</th>
<th id="S3.T1.1.14.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">夕阳坡简</th>
<td id="S3.T1.1.14.14.3" class="ltx_td ltx_align_center">1</td>
<td id="S3.T1.1.14.14.4" class="ltx_td ltx_align_center">2</td>
<td id="S3.T1.1.14.14.5" class="ltx_td ltx_align_center">54</td>
</tr>
<tr id="S3.T1.1.15.15" class="ltx_tr">
<th id="S3.T1.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Ynagjiawan Slips</th>
<th id="S3.T1.1.15.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">杨家湾简</th>
<td id="S3.T1.1.15.15.3" class="ltx_td ltx_align_center">1</td>
<td id="S3.T1.1.15.15.4" class="ltx_td ltx_align_center">38</td>
<td id="S3.T1.1.15.15.5" class="ltx_td ltx_align_center">41</td>
</tr>
<tr id="S3.T1.1.16.16" class="ltx_tr">
<th id="S3.T1.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Caojiagang Slips</th>
<th id="S3.T1.1.16.16.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">曹家岗简</th>
<td id="S3.T1.1.16.16.3" class="ltx_td ltx_align_center">1</td>
<td id="S3.T1.1.16.16.4" class="ltx_td ltx_align_center">7</td>
<td id="S3.T1.1.16.16.5" class="ltx_td ltx_align_center">34</td>
</tr>
<tr id="S3.T1.1.17.17" class="ltx_tr">
<th id="S3.T1.1.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">Total</th>
<th id="S3.T1.1.17.17.2" class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_t"></th>
<td id="S3.T1.1.17.17.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">164</td>
<td id="S3.T1.1.17.17.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">5,033</td>
<td id="S3.T1.1.17.17.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">102,722</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The amount of data from different sources of our collection of CBSs.</figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">CBSs are the writing materials used in ancient China during the Warring States period over two thousand years ago, and the earliest known large-scale form of calligraphic writing<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Some Oracle Bone Script were formed by brushes, but only in extremely small amounts.</span></span></span>. The study of it holds great linguistic, historical, and cultural value, especially for East Asian scripts.
The content includes, for instance, the oldest known records of ancient classics such as the <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">Book of Documents</span> (also the <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">Classic of History</span>, Chinese: 尚书) and <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_italic">Classic of Poetry</span> (Shijing, Chinese: 诗经).</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The slips survived over two thousand years mainly because when they were submerged in water until excavation, protecting them from oxidation. For the same reason, most current slips are found along the Yangtze River. As shown in Figure <a href="#A1.F2" title="Figure 2 ‣ Appendix A An example of a CBS Material ‣ Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the form of the slips is highly regular, most are 45cm long and 0.6cm wide. The longer slips typically carry between 27 to 38 characters. Multiple slips are tied together to form documents. A real example of a CBS is given in Appendix <a href="#A1" title="Appendix A An example of a CBS Material ‣ Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>CHUBS</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Digitizing and understanding CBSs, especially in the view of natural language processing, are of great value in promoting history, culture, and art research studies.
However, to the best of our knowledge, there is no public large-scale collection of CBS dataset prepared in an accessible format that is convenient for usage in typical workflows within the machine learning community.
Thus, to facilitate the application of machine learning to aid research in CBS, we collect and publish the first dataset of CBS inscriptions, called CHUBS. It includes high-quality scanned images of the slips and their text annotations.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Data Source</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">All data is extracted and processed from publicly released textbooks or records by paleographers, containing image scans and transcriptions of a set of bamboo slips from certain excavation projects. We supplemented the materials with some missing transcriptions and extracted the images of the characters from the slip images.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">These materials are widely known in the community of paleographers in ancient Chinese scripts. Our contribution is that we are the first to compile these materials into an easily accessible format for the application of machine learning methods. We have been careful to ensure that there is no restriction on the use of these materials, and the data will be released under a permissive license.</p>
</div>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.1" class="ltx_p">Since all image scans are extracted and processed from publicly released textbooks containing unearthed materials from various sources and different periods, variations between scans produced by different teams are inevitable. For example, some scans are black, while others are in color.</p>
</div>
<div id="S3.SS2.SSS1.p4" class="ltx_para">
<p id="S3.SS2.SSS1.p4.1" class="ltx_p">Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Chu Bamboo Slips ‣ 3 Dataset ‣ Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> lists each of the data sources as well as the number of documents, slips, and characters from each source.
It is worth noting that many of the sources do not have an official English name. Therefore, we only give the pinyin transcription of the Chinese name. We highly suggest interested readers use the Chinese name when possible for future research.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Annotating Sub-Character Components</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Each character is annotated with modern Chinese text.
However, manual inspection reveals that at least 27% of the characters in our dataset are not within the set of modern Chinese words<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>A word may consist of multiple characters.</span></span></span> (these characters do not have a UTF-8 encoding). In other words, 27% of the detected characters are out of vocabulary (OOV) if we tokenize them on character-level granularity. The upper two characters in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> are examples of such OOV characters.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p">There are two reasons for this high proportion of OOV CBS characters:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">The CBS character has not yet been deciphered due to drastic changes in character forms or material degradation.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">The CBS character does not have a modern Chinese equivalent (but experts believe that they know the meaning of the character).</p>
</div>
</li>
</ol>
<p id="S3.SS2.SSS2.p2.2" class="ltx_p">Such CBS characters are annotated with a set of <span id="S3.SS2.SSS2.p2.2.1" class="ltx_text ltx_font_italic">sub-character components</span> such as radicals or <span id="S3.SS2.SSS2.p2.2.2" class="ltx_text ltx_font_italic">pianpang<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note"><span id="footnote4.1.1.1" class="ltx_text ltx_font_upright">4</span></span><span id="footnote4.5" class="ltx_text ltx_font_upright">Components of Chinese characters traditionally used for indexing in dictionaries.</span></span></span></span></span>. For instance, assuming the character “想” (pronunciation: <span id="S3.SS2.SSS2.p2.2.3" class="ltx_text ltx_font_italic">xiang</span>) does not have a modern equivalence, it may be labeled as “相心”<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>We have refrained from using a more advanced encoding system (such as including the positioning of the components) to keep the annotation cost low.</span></span></span> (pronunciation: <span id="S3.SS2.SSS2.p2.2.4" class="ltx_text ltx_font_italic">xiang xin</span>).
If even such sub-character components are unrecognizable, it is annotated with a placeholder to indicate that the character is unrecognizable.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p">However, there is no common consensus on how to split Chinese characters into sub-character components. Our approach is based on the philosophy that each unit shoudl be semantically or phonetically meaningful (i.e., it is a morpheme or a phoneme). This is because we hypothesize that further splitting such units does not provide additional useful information about the text, but may introduce noise or result in unnecessarily lengthy token sequences.</p>
</div>
<div id="S3.SS2.SSS2.p4" class="ltx_para">
<p id="S3.SS2.SSS2.p4.1" class="ltx_p">Concretely, we request an expert in the field (with a Ph.D. degree studying CBS) to annotate each CBS character with the corresponding sub-character components. One possibility is to label the pianpang. However, this has two main limitations when applied to CBS scripts. Firstly, CBS characters are very different from modern Chinese and not every CBS character has a pianpang. Secondly, we want to retain as much information about the character as possible, so we need a method for annotating the semantics or phonetics of the part of the characters that is not the pianpang as well.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Sub-Character Component Annotation Scheme</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">Addressing the above limitations, our final annotation procedure is as follows. For a given CBS character, if it is already labeled with a modern Chinese character (i.e., it is not OOV), we keep it as it is. Otherwise, we first identify it as one of the three types of Chinese characters: <span id="S3.SS2.SSS3.p1.1.1" class="ltx_text ltx_font_bold">logograms</span> (<span id="S3.SS2.SSS3.p1.1.2" class="ltx_text ltx_font_italic">xiangxing</span> characters, Chinese: 象形字), <span id="S3.SS2.SSS3.p1.1.3" class="ltx_text ltx_font_bold">semantic-phonetic compound characters</span> (<span id="S3.SS2.SSS3.p1.1.4" class="ltx_text ltx_font_italic">xingsheng</span> characters, Chinese: 形声字), and <span id="S3.SS2.SSS3.p1.1.5" class="ltx_text ltx_font_bold">phonograms</span> (<span id="S3.SS2.SSS3.p1.1.6" class="ltx_text ltx_font_italic">jiajie</span> characters, Chinese: 假借字). Such classification of Chinese characters was first introduced by <cite class="ltx_cite ltx_citemacro_cite">Chen (<a href="#bib.bib2" title="" class="ltx_ref">1956</a>)</cite>, and is commonly taught in Chinese schools<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>This categorization scheme is called “three category theory” (<span id="footnote6.1" class="ltx_text ltx_font_italic">san shu shuo</span>, Chinese: 三书说), but there are also other categorization methods. Two notable instances are “four category theory” and “six category theory”.</span></span></span>.
Then, we start with a sub-character vocabulary with 540 items introduced by <span id="S3.SS2.SSS3.p1.1.7" class="ltx_text ltx_font_italic">Shuowen Jiezi</span> <cite class="ltx_cite ltx_citemacro_cite">Xu (<a href="#bib.bib27" title="" class="ltx_ref">1963</a>)</cite>, a well-known Chinese dictionary released around 100 CE during the Eastern Han dynasty.</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">For <span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">semantic-phonetic compound characters</span>, we split them into the semantic and phonetic parts (the former is always a logogram), and apply the following rules.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">For <span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">logograms</span> and <span id="S3.I2.i2.p1.1.2" class="ltx_text ltx_font_bold">phonograms</span>: we try to split it into components of the current sub-character vocabulary. If there exists a part of the character that is not and does not include any of the current sub-character components, we add that part as a sub-character component into the vocabulary.</p>
</div>
</li>
</ul>
<p id="S3.SS2.SSS3.p1.2" class="ltx_p">Repeating this process for all characters in our library results in 798 sub-character components in total, which makes up our final sub-character vocabulary.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">We emphasize that the vocabulary construction may have considerable impact on the downstream performance, but it is out of the scope of this thesis work.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Open Platform</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To better foster future research in CBS scripts, we build and release a platform to make accessing our data more convenient for researchers from different backgrounds. The platform allows the download of the entire collection as well as searching particular images based on the text annotation, origin, and character appearance (searching by hand-written strokes), which is essential for searching for characters without modern Chinese equivalents.
Further, this platform also features pipeline processing capabilities for CBS, including detecting, recognizing, and retrieving characters, significantly reducing both time and human resources for experts. Specifically, for a CBS image, it can detect each character and recognize it with our multi-modal tokenizer. Appendix <a href="#A2" title="Appendix B Open Platform ‣ Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> displays a screenshot of this platform.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Multi-Modal Multi-Granularity Tokenizer</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In summary, our tokenizer consists of multiple neural networks that perform object detection and classification in a pipeline. The input is the image of the material containing the Ancient inscriptions. The pipeline consists of the following steps:</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">The characters in the bamboo slip are detected using an object detection model, cropped then ordered into a sequence based on their location.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Each image is fed to a character recognition that maps the CBS characters into a modern Chinese character/word.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">If the classification confidence is lower than a certain threshold, the tokenizer falls back to sub-character analysis by recognizing the sub-character components of the character.</p>
</div>
</li>
</ol>
<p id="S4.p1.2" class="ltx_p">The output is a sequence where each element is either a single character or a set of sub-character components. The classification confidence threshold is typically determined using a validation set of examples from the downstream task.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Sub-Character Recognition</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">As mentioned in Section <a href="#S3.SS2.SSS2" title="3.2.2 Annotating Sub-Character Components ‣ 3.2 CHUBS ‣ 3 Dataset ‣ Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>, many characters in our dataset are not within the set of modern Chinese words. For such characters, assigning a unique class would not be conducive, because the class label may not help us better understand the ancient text. Therefore, we propose to recognize the sub-character components<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>We use “components” to refer to any consistent and frequent set of strokes smaller than or equal to a character.</span></span></span> of the characters instead. This may be beneficial for downstream tasks because Chinese character components may represent rich information about the phonetics and semantics of the character.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">This is done with a multi-label classifier whose vocabulary is simply the set of 798 sub-character components we have annotated in CHUBS.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Details</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Models</h3>

<section id="S5.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Character Detection</h5>

<div id="S5.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px1.p1.1" class="ltx_p">Specifically, we employ the YOLOv5 model <cite class="ltx_cite ltx_citemacro_citep">(Jocher et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite>, one of the most used versions in the YOLO series <cite class="ltx_cite ltx_citemacro_citep">(Redmon et al., <a href="#bib.bib17" title="" class="ltx_ref">2016</a>)</cite>.
We train this model on the CBS images annotated by domain experts.</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Character and Sub-Character Recognition</h5>

<div id="S5.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px2.p1.1" class="ltx_p">For both character and sub-character recognition, we try both ResNet <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib6" title="" class="ltx_ref">2016</a>)</cite> and Visual Transformer (ViT) <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite>, which are two strong models with great capabilities in image classification. We use roughly the same number of parameters for both architectures. The difference between character and sub-character recognition is the number of classes and that the former is an ordinary multi-class classification while the latter is a multi-label classification.</p>
</div>
<div id="S5.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S5.SS1.SSS0.Px2.p2.1" class="ltx_p">Specifically, we start from commonly used public checkpoints, the official <span id="S5.SS1.SSS0.Px2.p2.1.1" class="ltx_text ltx_font_typewriter">resnet152</span> model of PyTorch and the ViT by <cite class="ltx_cite ltx_citemacro_citet">Wu et al. (<a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite><span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="https://huggingface.co/google/vit-base-patch16-224" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/google/vit-base-patch16-224</a></span></span></span>. These model checkpoints are pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a href="#bib.bib4" title="" class="ltx_ref">2009</a>)</cite>, and we finetune them on CHUBS.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Training Data</h3>

<section id="S5.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Detector Training Data</h5>

<div id="S5.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p1.1" class="ltx_p">To train the CBS character detector, an expert paleographer is asked to manually annotate a small number of CBS.
The annotations are then quality-checked by other authors. In total, 177 image scans of bamboo slips from Tsinghua University Slips were annotated, of which 141 were used as training data, and 36 for validation. We emphasize that this annotation process is rather simple because most CBS characters are very easy to identify in the image scans.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Classifier Training Data</h5>

<div id="S5.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px2.p1.3" class="ltx_p">The character and sub-character recognizer are simply trained on CHUBS, since the data already contains all supervision needed.
The frequency distribution of the characters follows a Zipfian distribution, so approximately half of the characters only appear once in the dataset. To ensure that each class contains enough data for both training and testing, we discard characters that have less than <math id="S5.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS2.SSS0.Px2.p1.1.m1.1a"><mi id="S5.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px2.p1.1.m1.1b"><ci id="S5.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS0.Px2.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px2.p1.1.m1.1c">k</annotation></semantics></math> images (we use <math id="S5.SS2.SSS0.Px2.p1.2.m2.2" class="ltx_Math" alttext="k=3,10" display="inline"><semantics id="S5.SS2.SSS0.Px2.p1.2.m2.2a"><mrow id="S5.SS2.SSS0.Px2.p1.2.m2.2.3" xref="S5.SS2.SSS0.Px2.p1.2.m2.2.3.cmml"><mi id="S5.SS2.SSS0.Px2.p1.2.m2.2.3.2" xref="S5.SS2.SSS0.Px2.p1.2.m2.2.3.2.cmml">k</mi><mo id="S5.SS2.SSS0.Px2.p1.2.m2.2.3.1" xref="S5.SS2.SSS0.Px2.p1.2.m2.2.3.1.cmml">=</mo><mrow id="S5.SS2.SSS0.Px2.p1.2.m2.2.3.3.2" xref="S5.SS2.SSS0.Px2.p1.2.m2.2.3.3.1.cmml"><mn id="S5.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S5.SS2.SSS0.Px2.p1.2.m2.1.1.cmml">3</mn><mo id="S5.SS2.SSS0.Px2.p1.2.m2.2.3.3.2.1" xref="S5.SS2.SSS0.Px2.p1.2.m2.2.3.3.1.cmml">,</mo><mn id="S5.SS2.SSS0.Px2.p1.2.m2.2.2" xref="S5.SS2.SSS0.Px2.p1.2.m2.2.2.cmml">10</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px2.p1.2.m2.2b"><apply id="S5.SS2.SSS0.Px2.p1.2.m2.2.3.cmml" xref="S5.SS2.SSS0.Px2.p1.2.m2.2.3"><eq id="S5.SS2.SSS0.Px2.p1.2.m2.2.3.1.cmml" xref="S5.SS2.SSS0.Px2.p1.2.m2.2.3.1"></eq><ci id="S5.SS2.SSS0.Px2.p1.2.m2.2.3.2.cmml" xref="S5.SS2.SSS0.Px2.p1.2.m2.2.3.2">𝑘</ci><list id="S5.SS2.SSS0.Px2.p1.2.m2.2.3.3.1.cmml" xref="S5.SS2.SSS0.Px2.p1.2.m2.2.3.3.2"><cn type="integer" id="S5.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S5.SS2.SSS0.Px2.p1.2.m2.1.1">3</cn><cn type="integer" id="S5.SS2.SSS0.Px2.p1.2.m2.2.2.cmml" xref="S5.SS2.SSS0.Px2.p1.2.m2.2.2">10</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px2.p1.2.m2.2c">k=3,10</annotation></semantics></math> in character recognition and <math id="S5.SS2.SSS0.Px2.p1.3.m3.2" class="ltx_Math" alttext="k=2,20" display="inline"><semantics id="S5.SS2.SSS0.Px2.p1.3.m3.2a"><mrow id="S5.SS2.SSS0.Px2.p1.3.m3.2.3" xref="S5.SS2.SSS0.Px2.p1.3.m3.2.3.cmml"><mi id="S5.SS2.SSS0.Px2.p1.3.m3.2.3.2" xref="S5.SS2.SSS0.Px2.p1.3.m3.2.3.2.cmml">k</mi><mo id="S5.SS2.SSS0.Px2.p1.3.m3.2.3.1" xref="S5.SS2.SSS0.Px2.p1.3.m3.2.3.1.cmml">=</mo><mrow id="S5.SS2.SSS0.Px2.p1.3.m3.2.3.3.2" xref="S5.SS2.SSS0.Px2.p1.3.m3.2.3.3.1.cmml"><mn id="S5.SS2.SSS0.Px2.p1.3.m3.1.1" xref="S5.SS2.SSS0.Px2.p1.3.m3.1.1.cmml">2</mn><mo id="S5.SS2.SSS0.Px2.p1.3.m3.2.3.3.2.1" xref="S5.SS2.SSS0.Px2.p1.3.m3.2.3.3.1.cmml">,</mo><mn id="S5.SS2.SSS0.Px2.p1.3.m3.2.2" xref="S5.SS2.SSS0.Px2.p1.3.m3.2.2.cmml">20</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px2.p1.3.m3.2b"><apply id="S5.SS2.SSS0.Px2.p1.3.m3.2.3.cmml" xref="S5.SS2.SSS0.Px2.p1.3.m3.2.3"><eq id="S5.SS2.SSS0.Px2.p1.3.m3.2.3.1.cmml" xref="S5.SS2.SSS0.Px2.p1.3.m3.2.3.1"></eq><ci id="S5.SS2.SSS0.Px2.p1.3.m3.2.3.2.cmml" xref="S5.SS2.SSS0.Px2.p1.3.m3.2.3.2">𝑘</ci><list id="S5.SS2.SSS0.Px2.p1.3.m3.2.3.3.1.cmml" xref="S5.SS2.SSS0.Px2.p1.3.m3.2.3.3.2"><cn type="integer" id="S5.SS2.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S5.SS2.SSS0.Px2.p1.3.m3.1.1">2</cn><cn type="integer" id="S5.SS2.SSS0.Px2.p1.3.m3.2.2.cmml" xref="S5.SS2.SSS0.Px2.p1.3.m3.2.2">20</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px2.p1.3.m3.2c">k=2,20</annotation></semantics></math> in sub-character recognition). We then split the data into training, validation, and test sets by an 8:1:1 ratio, while ensuring that the test set has at least one example from every class.</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Training Details</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">All training experiments are conducted on an A100 GPU, and implemented with PyTorch. We use the Adam optimizer <cite class="ltx_cite ltx_citemacro_citep">(Kingma et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite> and a learning rate scheduler that decays by 0.9 after every epoch. We only search different batch sizes and maximum learning rates during the hyperparameter search to keep the computational cost low.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Since the tokenization pipeline has three steps, we first show the empirical performance of each part. Then, we apply the tokenizer on an example downstream task, part-of-speech (POS) tagging, to demonstrate its effectiveness over character-based tokenizers (one CBS character per token).</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Character Detection</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">The performance of the character detector is shown in Table <a href="#S6.T2" title="Table 2 ‣ 6.1 Character Detection ‣ 6 Results ‣ Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
The <span id="S6.SS1.p1.1.1" class="ltx_text ltx_font_italic">near-perfect</span> F1-score implies that the model is well-suited and robust for CBS characters and that it introduces minimal noise to our tokenization pipeline. Based on these detection results, we then conduct character recognition.</p>
</div>
<figure id="S6.T2" class="ltx_table">
<table id="S6.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T2.1.1.1" class="ltx_tr">
<td id="S6.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Precision</span></td>
<td id="S6.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Recall</span></td>
<td id="S6.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">F1</span></td>
</tr>
<tr id="S6.T2.1.2.2" class="ltx_tr">
<td id="S6.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.998</td>
<td id="S6.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.996</td>
<td id="S6.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.997</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Character Detection Results with YOLOv5.</figcaption>
</figure>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Character Recognition</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">The result of the character recognizer on the test set is shown in Table <a href="#S6.T3" title="Table 3 ‣ 6.2 Character Recognition ‣ 6 Results ‣ Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, in which we can see that ViT consistently outperforms ResNet, which is consistent with the results by the authors of ViT.
The high accuracy indicates that the application of such deep learning offers great practical value.</p>
</div>
<figure id="S6.T3" class="ltx_table">
<table id="S6.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T3.2.3.1" class="ltx_tr">
<th id="S6.T3.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S6.T3.2.3.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<td id="S6.T3.2.3.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T3.2.3.1.2.1" class="ltx_text ltx_font_bold">Top-1</span></td>
<td id="S6.T3.2.3.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T3.2.3.1.3.1" class="ltx_text ltx_font_bold">Top-3</span></td>
<td id="S6.T3.2.3.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T3.2.3.1.4.1" class="ltx_text ltx_font_bold">Top-5</span></td>
<td id="S6.T3.2.3.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T3.2.3.1.5.1" class="ltx_text ltx_font_bold">Top-10</span></td>
</tr>
<tr id="S6.T3.1.1" class="ltx_tr">
<th id="S6.T3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="5"><math id="S6.T3.1.1.1.m1.1" class="ltx_Math" alttext="k=3" display="inline"><semantics id="S6.T3.1.1.1.m1.1a"><mrow id="S6.T3.1.1.1.m1.1.1" xref="S6.T3.1.1.1.m1.1.1.cmml"><mi id="S6.T3.1.1.1.m1.1.1.2" xref="S6.T3.1.1.1.m1.1.1.2.cmml">k</mi><mo id="S6.T3.1.1.1.m1.1.1.1" xref="S6.T3.1.1.1.m1.1.1.1.cmml">=</mo><mn id="S6.T3.1.1.1.m1.1.1.3" xref="S6.T3.1.1.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.1.1.1.m1.1b"><apply id="S6.T3.1.1.1.m1.1.1.cmml" xref="S6.T3.1.1.1.m1.1.1"><eq id="S6.T3.1.1.1.m1.1.1.1.cmml" xref="S6.T3.1.1.1.m1.1.1.1"></eq><ci id="S6.T3.1.1.1.m1.1.1.2.cmml" xref="S6.T3.1.1.1.m1.1.1.2">𝑘</ci><cn type="integer" id="S6.T3.1.1.1.m1.1.1.3.cmml" xref="S6.T3.1.1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.1.1.1.m1.1c">k=3</annotation></semantics></math></th>
</tr>
<tr id="S6.T3.2.4.2" class="ltx_tr">
<th id="S6.T3.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">ResNet</th>
<td id="S6.T3.2.4.2.2" class="ltx_td ltx_align_center ltx_border_t">61.23</td>
<td id="S6.T3.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t">65.48</td>
<td id="S6.T3.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t">70.84</td>
<td id="S6.T3.2.4.2.5" class="ltx_td ltx_align_center ltx_border_t">72.33</td>
</tr>
<tr id="S6.T3.2.5.3" class="ltx_tr">
<th id="S6.T3.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ViT</th>
<td id="S6.T3.2.5.3.2" class="ltx_td ltx_align_center">73.48</td>
<td id="S6.T3.2.5.3.3" class="ltx_td ltx_align_center">84.65</td>
<td id="S6.T3.2.5.3.4" class="ltx_td ltx_align_center">87.45</td>
<td id="S6.T3.2.5.3.5" class="ltx_td ltx_align_center">89.95</td>
</tr>
<tr id="S6.T3.2.2" class="ltx_tr">
<th id="S6.T3.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="5"><math id="S6.T3.2.2.1.m1.1" class="ltx_Math" alttext="k=10" display="inline"><semantics id="S6.T3.2.2.1.m1.1a"><mrow id="S6.T3.2.2.1.m1.1.1" xref="S6.T3.2.2.1.m1.1.1.cmml"><mi id="S6.T3.2.2.1.m1.1.1.2" xref="S6.T3.2.2.1.m1.1.1.2.cmml">k</mi><mo id="S6.T3.2.2.1.m1.1.1.1" xref="S6.T3.2.2.1.m1.1.1.1.cmml">=</mo><mn id="S6.T3.2.2.1.m1.1.1.3" xref="S6.T3.2.2.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T3.2.2.1.m1.1b"><apply id="S6.T3.2.2.1.m1.1.1.cmml" xref="S6.T3.2.2.1.m1.1.1"><eq id="S6.T3.2.2.1.m1.1.1.1.cmml" xref="S6.T3.2.2.1.m1.1.1.1"></eq><ci id="S6.T3.2.2.1.m1.1.1.2.cmml" xref="S6.T3.2.2.1.m1.1.1.2">𝑘</ci><cn type="integer" id="S6.T3.2.2.1.m1.1.1.3.cmml" xref="S6.T3.2.2.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.2.2.1.m1.1c">k=10</annotation></semantics></math></th>
</tr>
<tr id="S6.T3.2.6.4" class="ltx_tr">
<th id="S6.T3.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">ResNet</th>
<td id="S6.T3.2.6.4.2" class="ltx_td ltx_align_center ltx_border_t">72.60</td>
<td id="S6.T3.2.6.4.3" class="ltx_td ltx_align_center ltx_border_t">83.70</td>
<td id="S6.T3.2.6.4.4" class="ltx_td ltx_align_center ltx_border_t">87.18</td>
<td id="S6.T3.2.6.4.5" class="ltx_td ltx_align_center ltx_border_t">90.57</td>
</tr>
<tr id="S6.T3.2.7.5" class="ltx_tr">
<th id="S6.T3.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">ViT</th>
<td id="S6.T3.2.7.5.2" class="ltx_td ltx_align_center ltx_border_bb">90.11</td>
<td id="S6.T3.2.7.5.3" class="ltx_td ltx_align_center ltx_border_bb">95.03</td>
<td id="S6.T3.2.7.5.4" class="ltx_td ltx_align_center ltx_border_bb">96.06</td>
<td id="S6.T3.2.7.5.5" class="ltx_td ltx_align_center ltx_border_bb">97.16</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Accuracy (in %) of character recognition models on the test set. <math id="S6.T3.4.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.T3.4.m1.1b"><mi id="S6.T3.4.m1.1.1" xref="S6.T3.4.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.T3.4.m1.1c"><ci id="S6.T3.4.m1.1.1.cmml" xref="S6.T3.4.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.4.m1.1d">k</annotation></semantics></math> indicates the minimum occurrence of a character in the dataset.</figcaption>
</figure>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Sub-Character Recognition</h3>

<figure id="S6.T4" class="ltx_table">
<table id="S6.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T4.2.3.1" class="ltx_tr">
<th id="S6.T4.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S6.T4.2.3.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<td id="S6.T4.2.3.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T4.2.3.1.2.1" class="ltx_text ltx_font_bold">Recall</span></td>
<td id="S6.T4.2.3.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T4.2.3.1.3.1" class="ltx_text ltx_font_bold">Precision</span></td>
<td id="S6.T4.2.3.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T4.2.3.1.4.1" class="ltx_text ltx_font_bold">F1</span></td>
</tr>
<tr id="S6.T4.1.1" class="ltx_tr">
<th id="S6.T4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4"><math id="S6.T4.1.1.1.m1.1" class="ltx_Math" alttext="k=2" display="inline"><semantics id="S6.T4.1.1.1.m1.1a"><mrow id="S6.T4.1.1.1.m1.1.1" xref="S6.T4.1.1.1.m1.1.1.cmml"><mi id="S6.T4.1.1.1.m1.1.1.2" xref="S6.T4.1.1.1.m1.1.1.2.cmml">k</mi><mo id="S6.T4.1.1.1.m1.1.1.1" xref="S6.T4.1.1.1.m1.1.1.1.cmml">=</mo><mn id="S6.T4.1.1.1.m1.1.1.3" xref="S6.T4.1.1.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T4.1.1.1.m1.1b"><apply id="S6.T4.1.1.1.m1.1.1.cmml" xref="S6.T4.1.1.1.m1.1.1"><eq id="S6.T4.1.1.1.m1.1.1.1.cmml" xref="S6.T4.1.1.1.m1.1.1.1"></eq><ci id="S6.T4.1.1.1.m1.1.1.2.cmml" xref="S6.T4.1.1.1.m1.1.1.2">𝑘</ci><cn type="integer" id="S6.T4.1.1.1.m1.1.1.3.cmml" xref="S6.T4.1.1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.1.1.1.m1.1c">k=2</annotation></semantics></math></th>
</tr>
<tr id="S6.T4.2.4.2" class="ltx_tr">
<th id="S6.T4.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">ResNet</th>
<td id="S6.T4.2.4.2.2" class="ltx_td ltx_align_center ltx_border_t">84.79</td>
<td id="S6.T4.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t">77.32</td>
<td id="S6.T4.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t">80.88</td>
</tr>
<tr id="S6.T4.2.5.3" class="ltx_tr">
<th id="S6.T4.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ViT</th>
<td id="S6.T4.2.5.3.2" class="ltx_td ltx_align_center">22.48</td>
<td id="S6.T4.2.5.3.3" class="ltx_td ltx_align_center">26.31</td>
<td id="S6.T4.2.5.3.4" class="ltx_td ltx_align_center">24.24</td>
</tr>
<tr id="S6.T4.2.2" class="ltx_tr">
<th id="S6.T4.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4"><math id="S6.T4.2.2.1.m1.1" class="ltx_Math" alttext="k=20" display="inline"><semantics id="S6.T4.2.2.1.m1.1a"><mrow id="S6.T4.2.2.1.m1.1.1" xref="S6.T4.2.2.1.m1.1.1.cmml"><mi id="S6.T4.2.2.1.m1.1.1.2" xref="S6.T4.2.2.1.m1.1.1.2.cmml">k</mi><mo id="S6.T4.2.2.1.m1.1.1.1" xref="S6.T4.2.2.1.m1.1.1.1.cmml">=</mo><mn id="S6.T4.2.2.1.m1.1.1.3" xref="S6.T4.2.2.1.m1.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T4.2.2.1.m1.1b"><apply id="S6.T4.2.2.1.m1.1.1.cmml" xref="S6.T4.2.2.1.m1.1.1"><eq id="S6.T4.2.2.1.m1.1.1.1.cmml" xref="S6.T4.2.2.1.m1.1.1.1"></eq><ci id="S6.T4.2.2.1.m1.1.1.2.cmml" xref="S6.T4.2.2.1.m1.1.1.2">𝑘</ci><cn type="integer" id="S6.T4.2.2.1.m1.1.1.3.cmml" xref="S6.T4.2.2.1.m1.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.2.2.1.m1.1c">k=20</annotation></semantics></math></th>
</tr>
<tr id="S6.T4.2.6.4" class="ltx_tr">
<th id="S6.T4.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">ResNet</th>
<td id="S6.T4.2.6.4.2" class="ltx_td ltx_align_center ltx_border_t">85.70</td>
<td id="S6.T4.2.6.4.3" class="ltx_td ltx_align_center ltx_border_t">78.31</td>
<td id="S6.T4.2.6.4.4" class="ltx_td ltx_align_center ltx_border_t">80.19</td>
</tr>
<tr id="S6.T4.2.7.5" class="ltx_tr">
<th id="S6.T4.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">ViT</th>
<td id="S6.T4.2.7.5.2" class="ltx_td ltx_align_center ltx_border_bb">28.57</td>
<td id="S6.T4.2.7.5.3" class="ltx_td ltx_align_center ltx_border_bb">28.23</td>
<td id="S6.T4.2.7.5.4" class="ltx_td ltx_align_center ltx_border_bb">28.40</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Recognition result (in %) of sub-character components of our model.</figcaption>
</figure>
<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">Table <a href="#S6.T4" title="Table 4 ‣ 6.3 Sub-Character Recognition ‣ 6 Results ‣ Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the performance of the sub-character recognition module. Perhaps surprisingly, ResNet beats ViT by a large margin, which differs from the observation in the character recognition experiments. One possible explanation for this is that each head in the multi-head attention module is responsible for recognizing a certain set of components (or their corresponding features), but the number of classes is too great for the architecture. Further investigations are outside this work’s scope.</p>
</div>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Downstream Task: Part-of-Speech Tagging</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">To demonstrate the effectiveness of our multi-granularity tokenizer, we apply it to a part-of-speech (POS) tagging task in the CBS script.</p>
</div>
<div id="S6.SS4.p2" class="ltx_para">
<p id="S6.SS4.p2.1" class="ltx_p">We create a POS tagging dataset for CBS by manually annotating 1,109 randomly sampled sentences using the BIO (Beginning, Inside, and Outside) format <cite class="ltx_cite ltx_citemacro_citep">(Ramshaw and Marcus, <a href="#bib.bib16" title="" class="ltx_ref">1999</a>)</cite>. This annotation is conducted by an expert in CBS scripts.
Then, we apply our multi-granularity tokenizer and a character-based tokenizer (each character is one token).</p>
</div>
<div id="S6.SS4.p3" class="ltx_para">
<p id="S6.SS4.p3.1" class="ltx_p">Our annotations include the following ten part-of-speeches that are commonly found and analyzed in ancient Chinese:</p>
</div>
<div id="S6.SS4.p4" class="ltx_para">
<ol id="S6.I1" class="ltx_enumerate">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p">Noun (Chinese: 名词, <span id="S6.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">mingci</span>)</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p">Verb (Chinese: 动词, <span id="S6.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">dongci</span>)</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p">Conjunction (Chinese: 连词, <span id="S6.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">lianci</span>)</p>
</div>
</li>
<li id="S6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S6.I1.i4.p1" class="ltx_para">
<p id="S6.I1.i4.p1.1" class="ltx_p">Adjective (Chinese: 形容词, <span id="S6.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">xingrongci</span>)</p>
</div>
</li>
<li id="S6.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S6.I1.i5.p1" class="ltx_para">
<p id="S6.I1.i5.p1.1" class="ltx_p">Adverb (Chinese: 副词, <span id="S6.I1.i5.p1.1.1" class="ltx_text ltx_font_italic">fuci</span>)</p>
</div>
</li>
<li id="S6.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S6.I1.i6.p1" class="ltx_para">
<p id="S6.I1.i6.p1.1" class="ltx_p">Numeral (Chinese: 数量词, <span id="S6.I1.i6.p1.1.1" class="ltx_text ltx_font_italic">shuliangci</span>)</p>
</div>
</li>
<li id="S6.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="S6.I1.i7.p1" class="ltx_para">
<p id="S6.I1.i7.p1.1" class="ltx_p">Modal Particle (Chinese: 语气词, <span id="S6.I1.i7.p1.1.1" class="ltx_text ltx_font_italic">yuqici</span>)</p>
</div>
</li>
<li id="S6.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">8.</span> 
<div id="S6.I1.i8.p1" class="ltx_para">
<p id="S6.I1.i8.p1.1" class="ltx_p">Pronoun (Chinese: 代词, <span id="S6.I1.i8.p1.1.1" class="ltx_text ltx_font_italic">daici</span>)</p>
</div>
</li>
<li id="S6.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9.</span> 
<div id="S6.I1.i9.p1" class="ltx_para">
<p id="S6.I1.i9.p1.1" class="ltx_p">Preposition (Chinese: 介词, <span id="S6.I1.i9.p1.1.1" class="ltx_text ltx_font_italic">jieci</span>)</p>
</div>
</li>
<li id="S6.I1.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">10.</span> 
<div id="S6.I1.i10.p1" class="ltx_para">
<p id="S6.I1.i10.p1.1" class="ltx_p">Auxiliary Word (Chinese: 助词, <span id="S6.I1.i10.p1.1.1" class="ltx_text ltx_font_italic">zhuci</span>)</p>
</div>
</li>
</ol>
</div>
<div id="S6.SS4.p5" class="ltx_para">
<p id="S6.SS4.p5.1" class="ltx_p">This dataset will be publicly released along side with our CHUBS dataset and training code.</p>
</div>
<div id="S6.SS4.p6" class="ltx_para">
<p id="S6.SS4.p6.1" class="ltx_p">When splitting characters into sub-character components, the label corresponding to the components is the same as the label for the original character. Then, a special token representing the boundary between each character is added to the sides of the sequence of components for each character. The predictions for these special tokens are ignored.</p>
</div>
<div id="S6.SS4.p7" class="ltx_para">
<p id="S6.SS4.p7.1" class="ltx_p">For the downstream model, we tune a large language model for this task using in-context learning. Specifically, we randomly sample 10 examples from the training data to use as in-context demonstrations and prompt the LLM to generate the predicted entities and the types as a Markdown list. The actual prompt template will be given along with the code after the review period. We use GPT-3-Turbo with default hyperparameters and repeat the experiments with 10 random seeds to ensure reproducibility.</p>
</div>
<div id="S6.SS4.p8" class="ltx_para">
<p id="S6.SS4.p8.1" class="ltx_p">The result is shown in Table <a href="#S6.T5" title="Table 5 ‣ 6.4 Downstream Task: Part-of-Speech Tagging ‣ 6 Results ‣ Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We observe that using our multi-granularity tokenizer can significantly (with a p-value of <math id="S6.SS4.p8.1.m1.1" class="ltx_Math" alttext="0.0079" display="inline"><semantics id="S6.SS4.p8.1.m1.1a"><mn id="S6.SS4.p8.1.m1.1.1" xref="S6.SS4.p8.1.m1.1.1.cmml">0.0079</mn><annotation-xml encoding="MathML-Content" id="S6.SS4.p8.1.m1.1b"><cn type="float" id="S6.SS4.p8.1.m1.1.1.cmml" xref="S6.SS4.p8.1.m1.1.1">0.0079</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p8.1.m1.1c">0.0079</annotation></semantics></math> in a t-test) improve the POS tagging performance of the downstream model, as we have expected.</p>
</div>
<figure id="S6.T5" class="ltx_table">
<table id="S6.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T5.1.1.1" class="ltx_tr">
<th id="S6.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><span id="S6.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">Tokenizer</span></th>
<th id="S6.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">Recall</span></th>
<th id="S6.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T5.1.1.1.3.1" class="ltx_text ltx_font_bold">Prec.</span></th>
<th id="S6.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T5.1.1.1.4.1" class="ltx_text ltx_font_bold">F1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T5.1.2.1" class="ltx_tr">
<th id="S6.T5.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Character-based</th>
<td id="S6.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">47.9</td>
<td id="S6.T5.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">43.8</td>
<td id="S6.T5.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">45.3</td>
</tr>
<tr id="S6.T5.1.3.2" class="ltx_tr">
<th id="S6.T5.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Multi-granularity</th>
<td id="S6.T5.1.3.2.2" class="ltx_td ltx_align_center ltx_border_bb">50.2</td>
<td id="S6.T5.1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb">46.1</td>
<td id="S6.T5.1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb">47.8</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>The part-of-speech performance (in %) when using a conventional character-level tokenizer (Char-Tokenizer) and our multi-granularity tokenizer.</figcaption>
</figure>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion and Discussions</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We have proposed a multi-modal multi-granularity tokenizer for better analyzing ancient Chinese scripts than the existing more popular sub-word tokenizers. We have also collected the first large-scale multi-modal dataset of CBS text with an open platform targeted at audiences of different backgrounds. We believe this work is an important step in leveraging deep learning methods in the research of East Asian scripts.</p>
</div>
<section id="S7.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">What are the differences between multi-granularity tokenizers and mainstream tokenizers?</h5>

<div id="S7.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S7.SS0.SSS0.Px1.p1.1" class="ltx_p">Currently, most tokenizers are a kind of “subword tokenizer”. This includes Byte-Pair Encoding (BPE) <cite class="ltx_cite ltx_citemacro_citep">(Sennrich et al., <a href="#bib.bib18" title="" class="ltx_ref">2016</a>)</cite> and SentencePiece <cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson, <a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite>, used in GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite> and LLaMa <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite>, respectively. The tokens in these tokenizers are often called <span id="S7.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">sub-words</span> (sequences of characters that are smaller than space-delimited words but larger than letters). For Chinese, mainstream tokenizers usually treat each Chinese character as an atomic unit. In contrast, our multi-granularity tokenizer splits each Chinese character into smaller sub-character components and provides this information to the downstream neural network.</p>
</div>
</section>
<section id="S7.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Why is identifying sub-character components conducive to downstream tasks?</h5>

<div id="S7.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S7.SS0.SSS0.Px2.p1.1" class="ltx_p">Tokenizers that treat each character as an atomic unit generally work for phonetic languages such as Indo-European languages because splitting phonetic letters into smaller components typically provides little to no additional information<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>One possible example task that we hypothesize might benefit from splitting Latin characters into multiple tokens is for answering questions about the shape of the letters.</span></span></span>.
However, for ideographic languages such as Chinese, components of a character may encode rich information about the semantics or phonetics of the characters. For common characters, the model may be able to learn such information automatically from that distribution of co-occurring characters.
However, for infrequent characters or unknown characters, the sample size of co-occurring characters is too small. Although some works have shown that language models can implicitly learn the letter composition (which is a kind of sub-token information) of tokens <cite class="ltx_cite ltx_citemacro_citep">(Si et al., <a href="#bib.bib20" title="" class="ltx_ref">2023b</a>; Hiraoka and Okazaki, <a href="#bib.bib7" title="" class="ltx_ref">2024</a>)</cite>, it is reasonable to hypothesize that such information requires large amounts of training data and tokenization at the sub-character level can provide conductive bias that either enhances performance or reduces the amount of data required to achieve the same performance.</p>
</div>
</section>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">In terms of the performance of the tokenizer, there are many possible methods for improving the effectiveness of the components of our tokenizer, such as pre-training on a corpus of modern text, larger/better model architectures, and better data pre- or post-processing. Moreover, augmenting the tokenizer with more knowledge about the history may help. But, we have not employed more tricks to keep our analysis simple.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">Also, although we have demonstrated the effectiveness of our tokenizer on the CBS script, it may be less effective on other scripts due to variations between scripts. However, since many other scripts face the same challenges highlighted in the introduction, our method should still have a performance advantage over conventional tokenizers. Additionally, due to the annotation cost, we have only investigated our tokenizer’s effectiveness on one downstream task.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Ethical Concerns</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">This work presents a new dataset on the Chu bamboo slips, a writing material from ancient China over two thousand years ago. We also introduce a new tokenizer for better processing ancient Chinese scripts with a large number of characters that do not have modern Chinese correspondence. The goal is to advance research in this ancient script as well as other forms of ancient Chinese scripts, which should not have significant ethical implications. However, the original content from these raw materials may have ethical implications for certain groups, but since these are existing historic materials, we do not make efforts to censor any content.</p>
</div>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">This work is supported by the National Key R&amp;D Program of China (No. 2022ZD0116312), National Natural Science Foundation of China (No. 62236004), and Institute Guo Qiang at Tsinghua University.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Assael et al. (2022)</span>
<span class="ltx_bibblock">
Yannis Assael, Thea Sommerschield, Brendan Shillingford, Mahyar Bordbar, John Pavlopoulos, Marita Chatzipanagiotou, Ion Androutsopoulos, Jonathan Prag, and Nando de Freitas. 2022.

</span>
<span class="ltx_bibblock">Restoring and attributing ancient texts using deep neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Nature</em>, 603(7900):280–283.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen (1956)</span>
<span class="ltx_bibblock">
Mengjia Chen. 1956.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Yin Xu Pu Ci Survey</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clanuwat et al. (2019)</span>
<span class="ltx_bibblock">
Tarin Clanuwat, Alex Lamb, and Asanobu Kitamoto. 2019.

</span>
<span class="ltx_bibblock">Kuronet: pre-modern japanese kuzushiji character recognition with deep learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">2019 International Conference on Document Analysis and Recognition (ICDAR)</em>, pages 607–614. IEEE.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2009)</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/5206848/" title="" class="ltx_ref ltx_href">Imagenet: A large-scale hierarchical image database</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on</em>, pages 248–255. IEEE.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2020)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.11929</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 770–778.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hiraoka and Okazaki (2024)</span>
<span class="ltx_bibblock">
Tatsuya Hiraoka and Naoaki Okazaki. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2402.09808" title="" class="ltx_ref ltx_href">Knowledge of pretrained language models on surface information of tokens</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jocher et al. (2020)</span>
<span class="ltx_bibblock">
Glenn Jocher, Alex Stoken, Jirka Borovec, NanoCode012, ChristopherSTAN, Liu Changyu, Laughing, tkianai, Adam Hogan, lorenzomammana, yxNONG, AlexWang1900, Laurentiu Diaconu, Marc, wanghaoyang0106, ml5ah, Doug, Francisco Ingham, Frederik, Guilhen, Hatovix, Jake Poznanski, Jiacong Fang, Lijun Yu, changyu98, Mingyu Wang, Naman Gupta, Osama Akhtar, PetrDvoracek, and Prashant Rai. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.5281/zenodo.4154370" title="" class="ltx_ref ltx_href">ultralytics/yolov5: v3.1 - Bug Fixes and Performance Improvements</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma et al. (2020)</span>
<span class="ltx_bibblock">
Diederik P Kingma, J Adam Ba, and J Adam. 2020.

</span>
<span class="ltx_bibblock">A method for stochastic optimization. arxiv 2014.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.6980</em>, 106.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:52051958" title="" class="ltx_ref ltx_href">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2015)</span>
<span class="ltx_bibblock">
Yanran Li, Wenjie Li, Fei Sun, and Sujian Li. 2015.

</span>
<span class="ltx_bibblock">Component-enhanced chinese character embeddings.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1508.06669</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mielke et al. (2021)</span>
<span class="ltx_bibblock">
Sabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey, Matthias Gallé, Arun Raja, Chenglei Si, Wilson Y. Lee, Benoît Sagot, and Samson Tan. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:245335281" title="" class="ltx_ref ltx_href">Between words and characters: A brief history of open-vocabulary modeling and tokenization in nlp</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2112.10508.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narang et al. (2021)</span>
<span class="ltx_bibblock">
Sonika Rani Narang, Munish Kumar, and Manish Kumar Jindal. 2021.

</span>
<span class="ltx_bibblock">Deepnetdevanagari: a deep learning model for devanagari ancient character recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Multimedia Tools and Applications</em>, 80:20671–20686.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2017)</span>
<span class="ltx_bibblock">
Nguyen, Julian Brooke, and Timothy Baldwin. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:36468734" title="" class="ltx_ref ltx_href">Sub-character neural language modelling in japanese</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">SWCN@EMNLP</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:257532815" title="" class="ltx_ref ltx_href">Gpt-4 technical report</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramshaw and Marcus (1999)</span>
<span class="ltx_bibblock">
Lance A Ramshaw and Mitchell P Marcus. 1999.

</span>
<span class="ltx_bibblock">Text chunking using transformation-based learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Natural language processing using very large corpora</em>, pages 157–176. Springer.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Redmon et al. (2016)</span>
<span class="ltx_bibblock">
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016.

</span>
<span class="ltx_bibblock">You only look once: Unified, real-time object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 779–788.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et al. (2016)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P16-1162" title="" class="ltx_ref ltx_href">Neural machine translation of rare words with subword units</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1715–1725, Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Si et al. (2023a)</span>
<span class="ltx_bibblock">
Chenglei Si, Zhengyan Zhang, Yingfa Chen, Fanchao Qi, Xiaozhi Wang, Zhiyuan Liu, Yasheng Wang, Qun Liu, and Maosong Sun. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00560" title="" class="ltx_ref ltx_href">Sub-character tokenization for Chinese pretrained language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 11:469–487.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Si et al. (2023b)</span>
<span class="ltx_bibblock">
Chenglei Si, Zhengyan Zhang, Yingfa Chen, Fanchao Qi, Xiaozhi Wang, Zhiyuan Liu, Yasheng Wang, Qun Liu, and Maosong Sun. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00560" title="" class="ltx_ref ltx_href">Sub-character tokenization for Chinese pretrained language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 11:469–487.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sommerschield et al. (2023)</span>
<span class="ltx_bibblock">
Thea Sommerschield, Yannis Assael, John Pavlopoulos, Vanessa Stefanak, Andrew Senior, Chris Dyer, John Bodel, Jonathan Prag, Ion Androutsopoulos, and Nando de Freitas. 2023.

</span>
<span class="ltx_bibblock">Machine learning for ancient languages: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Computational Linguistics</em>, pages 1–44.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2018)</span>
<span class="ltx_bibblock">
Yan Song, Shuming Shi, and Jing Li. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:51609660" title="" class="ltx_ref ltx_href">Joint learning embeddings for chinese words and their components via ladder structured networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">International Joint Conference on Artificial Intelligence</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2014)</span>
<span class="ltx_bibblock">
Yaming Sun, Lei Lin, Nan Yang, Zhenzhou Ji, and Xiaolong Wang. 2014.

</span>
<span class="ltx_bibblock">Radical-enhanced chinese character embedding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Neural Information Processing: 21st International Conference, ICONIP 2014, Kuching, Malaysia, November 3-6, 2014. Proceedings, Part II 21</em>, pages 279–286. Springer.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:257219404" title="" class="ltx_ref ltx_href">Llama: Open and efficient foundation language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2302.13971.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Zhijun Wang, Xuebo Liu, and Min Zhang. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:253801742" title="" class="ltx_ref ltx_href">Breaking the representation bottleneck of chinese characters: Neural machine translation with stroke sequence modeling</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2211.12781.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2020)</span>
<span class="ltx_bibblock">
Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter Vajda. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2006.03677" title="" class="ltx_ref ltx_href">Visual transformers: Token-based image representation and processing for computer vision</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu (1963)</span>
<span class="ltx_bibblock">
Shen Xu. 1963.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Shuowen jiezi</em>.

</span>
<span class="ltx_bibblock">Zhonghua Book Company.

</span>
<span class="ltx_bibblock">In Chinese.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Liu (2021)</span>
<span class="ltx_bibblock">
Cheng Zhang and Xingjun Liu. 2021.

</span>
<span class="ltx_bibblock">Feature extraction of ancient chinese characters based on deep convolution neural network and big data analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Computational Intelligence and Neuroscience</em>, 2021.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>An example of a CBS Material</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">To better understand the nature of CBS, we give an example of a CBS material in our dataset in Figure <a href="#A1.F2" title="Figure 2 ‣ Appendix A An example of a CBS Material ‣ Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The text is read from top to bottom and right to left.</p>
</div>
<figure id="A1.F2" class="ltx_figure"><img src="/html/2409.01011/assets/figs/chujian_example.jpg" id="A1.F2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="105" height="614" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An example of a CBS material. The slip shown is the 98th slip of the “Wu Ji” from Tsinghua University Slips.</figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Open Platform</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">The platform described in Section <a href="#S3.SS3" title="3.3 Open Platform ‣ 3 Dataset ‣ Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> will be launched after the anonymous review process. A screenshot of it is shown in Figure <a href="#A2.F3" title="Figure 3 ‣ Appendix B Open Platform ‣ Multi-Modal Multi-Granularity Tokenizer for Chu Bamboo Slip Scripts" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The platform is a website, and the interaction system was implemented using the Gradio library.</p>
</div>
<figure id="A2.F3" class="ltx_figure"><img src="/html/2409.01011/assets/figs/CBS_platform_20240216.png" id="A2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="592" height="444" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A screenshot of our platform for accessing the dataset and a demo of our tokenizer.</figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>AI-Assistant-Related Statement</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">AI-assisted tools were used for error-checking in writing this paper, and for code-completion during the implementation of the experiments.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.01010" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.01011" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.01011">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.01011" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.01012" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 22:46:34 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
