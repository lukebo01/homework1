<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.00223] \system: A Visual Analytics Solution for Automated Speech Recognition Analysis and Exploration</title><meta property="og:description" content="Confidence scores of automatic speech recognition (ASR) outputs are often inadequately communicated, preventing its seamless integration into analytical workflows. In this paper, we introduce \system, a visual analytic…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="\system: A Visual Analytics Solution for Automated Speech Recognition Analysis and Exploration">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="\system: A Visual Analytics Solution for Automated Speech Recognition Analysis and Exploration">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.00223">

<!--Generated on Wed Jun  5 17:06:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.2" class="ltx_ERROR undefined">\onlineid</span>
<p id="p1.1" class="ltx_p">1100
<span id="p1.1.1" class="ltx_ERROR undefined">\vgtccategory</span>Research
<span id="p1.1.2" class="ltx_ERROR undefined">\vgtcpapertype</span>system

 <span id="p1.1.3" class="ltx_ERROR undefined">\authorfooter</span>
S. Ha, C. Lim, and A. Ottley are with Washington University. E-mails: sha@wustl.edu, chaelim@wustl.edu, alvitta@wustl.edu.
R. J. Crouser is with Smith College. 
<br class="ltx_break">E-mail: jcrouser@smith.edu.





<span id="p1.1.4" class="ltx_ERROR undefined">\teaser</span>
<img src="/html/2405.00223/assets/figures/teaser.png" id="p1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="327" alt="[Uncaptioned image]">
<span id="p1.1.5" class="ltx_text ltx_caption ltx_align_center">An overview of <span id="p1.1.5.1" class="ltx_ERROR undefined">\system</span>: (a) The collapsible side menu contains controls for login, selecting, uploading, and transcribing audio files via AWS Transcribe. (b) At the top of the dashboard are the audio player and search bar. (c) The confidence overview displays the length and average confidence value of each line segment in the transcription (encoded by the width and opacity of each rectangle, respectively). (d) The word tree provides context to a specific search term and shows which words most often follow or precede it. (e) The user can view and edit the transcription; each word is underlined where its opacity indicates the confidence score.
</span></p>
</div>
<h1 class="ltx_title ltx_title_document">
<span id="1.1" class="ltx_ERROR undefined">\system</span>: A Visual Analytics Solution for 
<br class="ltx_break">Automated Speech Recognition Analysis and Exploration</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sunwoo Ha
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Chaehun Lim
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> R. Jordan Crouser
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> and Alvitta Ottley
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="2.1" class="ltx_p">Confidence scores of automatic speech recognition (ASR) outputs are often inadequately communicated, preventing its seamless integration into analytical workflows. In this paper, we introduce <span id="2.1.1" class="ltx_ERROR undefined">\system</span>, a visual analytic system developed in collaboration with intelligence analysts to address this issue. <span id="2.1.2" class="ltx_ERROR undefined">\system</span> aims to aid exploration and post-AI-transcription editing by visually representing the confidence associated with the transcription.
We demonstrate how our tool can assist intelligence analysts who use ASR outputs in their analytical and exploratory tasks and how it can help mitigate misinterpretation of crucial information. We also discuss opportunities for improving textual data cleaning and model transparency for human-machine collaboration.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Visual analytics, confidence visualization, automatic speech recognition
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The volume of audio data grows exponentially with each passing hour. However, turning raw audio data into actionable intelligence can be complex, as it relies heavily on the accuracy of the algorithms, transcription services employed, and the quality of the original audio files. Despite significant advances in Artificial Intelligence (AI) and Automatic Speech Recognition (ASR), the analysis of audio data remains fraught with inaccuracies and is notably time-consuming <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. As a result, fields such as intelligence analysis urgently require solutions that enable them to efficiently and accurately harness this vast and rapidly expanding resource.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Existing solutions that provide automatic speech-to-text and editing capabilities of the transcription (i.e., Rev, Happy Scribe) fall short as they often inadequately or rarely highlight and convey metrics that may bring forward potential inaccuracies to the user. The lack of transparency between the machine and the analyst within these existing services hinders the complete integration and effective utilization of speech-to-text technology within their workflows and analytical processes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Relying on AI, especially in human-machine collaborations, without awareness of these uncertainties can be detrimental, as the quality of an analyst’s work is a direct result of the trust <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> in and accuracy of the information presented to them.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address this challenge, we present a new visual analytics system called <span id="S1.p3.1.1" class="ltx_ERROR undefined">\system</span>, which we developed in collaboration with intelligence analysts. Our system enhances understanding of speech-to-text results by showing how confident the model is in its transcription. It uses the speech-to-text service from Amazon Web Services (AWS) and aims to make exploring and editing post-AI transcription easier by providing visual representations of confidence levels. By using interactive visualizations and multiple views, we demonstrate how our tool could improve the analysis of speech-to-text output and foster trust in human-machine collaborations. We also discuss how our system could support common analytical and exploratory tasks for intelligence analysts working with audio transcriptions. Finally, we explore opportunities to improve model transparency and textual data cleaning to encourage more effective human-machine collaboration. Through this work, our main contributions include:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We outline specific design goals derived from tasks that intelligence analysts perform within their workflow when inspecting audio transcriptions.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We develop a visual analytics system, <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">ConFides</span>, that allows analysts to easily transcribe their audio files and explore the transcription while being aware of uncertainties within the data.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">To demonstrate the applicability and usefulness of the system, we present a realistic use case scenario that forages for information from the Nixon White House tapes.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We briefly overview the prior work on transparency in human-machine collaborations in visual analytics and AI-assisted decision-making settings and visualizing confidence in speech-to-text outputs.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Communicating Model Uncertainty</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">From the point of view of mixed-initiative systems, Sacha et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> discussed the role of uncertainty, awareness, and trust in visual analytics and argued that users’ confidence in the machine teammate’s results depends on their degree of awareness of the different types of uncertainty that are present or generated in the system. Additionally, research has shown that visually communicating uncertainty can support users’ interpretation of data and support decision-making  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Therefore, communicating uncertainty is vital, especially when an AI agent assists in a decision-making process as it can potentially mitigate unwanted behaviors from the user such as underutilization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> or overreliance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> on AI suggestions. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> explored various explanation methods for recidivism prediction and forest cover prediction tasks. Their study found supportive evidence suggesting that providing information about feature contribution allowed participants to be aware of the uncertainties within the model and appropriately calibrate their trust.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Visualizing Confidence in Speech-To-Text Outputs</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Various methods change the appearance of text to convey speech-to-text confidence (e.g., alternating the font size <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, font color <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, font opacity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and underlining <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>). Vertanen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> utilized underlining, color, and opacity to visually embed confidence in speech recognition outputs, where the opacity of the colored underline of the text is based on the degree of confidence that the predicted word is accurate. In addition to embedding confidence in the transcription text, Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> developed a prototype visualization system that provides an overview of the confidence of speech-to-text outputs with a bar chart. In this visualization, each bar is mapped to a segment in the transcription, and all the bars of one transcribed text are sequentially visualized. We take inspiration from these prior works to communicate confidence to analysts and design the interactive visualizations used in our system.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Design Goals</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Numerous applications exist for text-to-speech data, including legal environments where depositions are recorded <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, healthcare settings where doctors record audio notes for medical records <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, automatic captioning for online videos, and academic research involving interview studies. Each use case scenario has different and unique tasks. Therefore, to create an ecologically valid solution, we ground our work in government intelligence analysis settings where analysts often analyze transcribed audio data for matters related to national security.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">We partnered with the Laboratory of Analytical Sciences, which facilitates collaborations between the Department of Defense’s (DoD) intelligence community and academia. Our tasks and design goals were derived from an initial one-hour, semi-structured interview with a language specialist from the DoD. We inquired about current tools, tasks, and goals that are supported, typical analysis workflows, collaborative activities, pain points, and features that are missing. This initial interview provided context and grounded the design process. We then adopted an interactive design methodology with bi-weekly feedback meetings from January 2023 to December 2023. Attendees included academic partners, DoD-affiliated analysts, and the projects’ program manager. Below we summarize the design goals (<span id="S3.p2.1.1" class="ltx_text ltx_font_bold">DG</span>) for the visual analytics system:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">DG1: Interactively visualize transcription and assess the quality of the transcription output</span>. Current transcription services lack transparency and rarely communicate the uncertainties and confidence values of automatic transcriptions. In many tasks, especially ones with consequential decisions, the analyst must determine when to trust the automatic transcription versus taking the time to listen to the source audio. Analysts must also determine whether additional data cleaning is necessary.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">DG2: Correct and perform basic textual data cleaning</span>. The system should allow the analyst to easily explore and playback audio segments, and make corrections to the transcription (e.g., adding, deleting, viewing alternative words, and replacing the text if desired).</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">DG3: Discern patterns and extract actionable intelligence within the transcription</span>. The system should facilitate the extraction of insights and allow the analyst to identify patterns and trends across the audio transcription data. It should enable the analyst to explore the context and frequency of spoken content, enhancing the ability to track discourse over time.</p>
</div>
</li>
</ul>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">To support the analyst in performing these tasks, ConFides offers three main interactive views, as detailed in Section <a href="#S4.SS2" title="4.2 Views ‣ 4 Visual Analytics System Design ‣ \system: A Visual Analytics Solution for Automated Speech Recognition Analysis and Exploration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>. Additionally, the backend of the system is connected to AWS Transcribe, enabling effortless transcription of audio files and exploration of the outputs in a single interface.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Visual Analytics System Design</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Through several iterative designs and feedback from our collaborators, we developed <span id="S4.p1.1.1" class="ltx_ERROR undefined">\system</span>. The following sections describe the framework of the system and how each view will support the analyst with the design goals outlined in Section <a href="#S3" title="3 Design Goals ‣ \system: A Visual Analytics Solution for Automated Speech Recognition Analysis and Exploration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Framework</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The backend of our system utilizes the service offered by AWS Transcribe to automatically transcribe audio files uploaded by the analyst. Despite the vast amount of cloud-based speech-to-text services available, we chose the service provided by Amazon as the transcription output contains valuable insights, such as confidence scores, speaker labels, and alternative texts. Once the analyst uploads the audio file with our tool and the transcription of the file is received from AWS Transcribe, the transcription is available for exploration and analysis by the analyst. Figure <a href="#S4.F1" title="Figure 1 ‣ 4.1 Framework ‣ 4 Visual Analytics System Design ‣ \system: A Visual Analytics Solution for Automated Speech Recognition Analysis and Exploration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the framework of the system.</p>
</div>
<figure id="S4.F1" class="ltx_figure"><img src="/html/2405.00223/assets/figures/framework.png" id="S4.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="146" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The framework of <span id="S4.F1.2.1" class="ltx_ERROR undefined">\system</span>. The audio files are uploaded and sent to AWS for automatic transcription. Users can select which transcriptions to explore and analyze.</figcaption>
</figure>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>AWS Transcribe Output</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">AWS Transcribe offers a line segmentation feature in their transcription output that will divide the transcript if it detects a pause or change in speaker. The service offers up to a maximum of 10 different speakers and will use its speaker diarization algorithm to determine the speaker at the current time in the audio. AWS Transcribe also supports word-level confidence, which is defined as a value between 0 and 1. Based on their documentation, “a larger [confidence] value indicates a higher probability that the identified item correctly matches the item spoken” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Due to confidentiality, AWS does not release any information on how this confidence score is calculated. Therefore, we stress that the confidence score should not be treated as absolute truth but rather as a metric that offers the analyst a way to gauge the potential flaws within the transcription.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Views</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To promote transparency and encourage calibration of trust in the machine’s output, each view of the system is embedded with the machine’s confidence in the transcription, whether that is term or segment-based.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Confidence Overview</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">To provide analysts with a visual overview of the transcription data, we designed the following view shown in Figure <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title"><span class="ltx_ERROR undefined">\system</span>: A Visual Analytics Solution for <span class="ltx_text"> </span>Automated Speech Recognition Analysis and Exploration</span></span>(c). In this view, each rectangle element corresponds to a segment in the transcript, with the width representing the audio length of the segment. Similar to the bar chart in the prototype proposed by Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, the rectangle segments are sequentially ordered from left to right. Each segment’s average confidence determines its corresponding rectangle’s opacity, meaning the lower the confidence, the more transparent the rectangle will be. This opacity value is calculated by averaging the confidence score associated with every word within the segment.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">Within the confidence overview, analysts can rapidly gauge the confidence of segments and playback certain portions of the audio by clicking on the rectangle element associated with the segment <span id="S4.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_bold">(DG1)</span>. Clicking on the rectangle element prompts the audio player to move and play the corresponding segment instantly. When the analyst hovers over a rectangle element within the view, a tooltip dynamically updates to provide the analyst with specific details on demand about the corresponding segment. This includes information such as the segment’s line number, the rolling average of confidence values within the segment, and the segment’s text.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Transcription Editor</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">The transcription view, as seen in Figure <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title"><span class="ltx_ERROR undefined">\system</span>: A Visual Analytics Solution for <span class="ltx_text"> </span>Automated Speech Recognition Analysis and Exploration</span></span>(f), is designed to assist the analyst in all the tasks listed in Section <a href="#S3" title="3 Design Goals ‣ \system: A Visual Analytics Solution for Automated Speech Recognition Analysis and Exploration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. This view displays the automatic transcription of a singular audio file from AWS. Each line represents a segment of the audio spoken by a specific speaker. We utilize color coding to distinguish speakers.
To promote transparency, we provided both a visual representation (shown by the opacity of the underline – inspired by Vertanen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>) and a textual representation (the tooltip of each word shows the confidence score) of the confidence for each word within the text (<span id="S4.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_bold">DG1</span>). As the audio plays, the transcription will follow along and automatically scroll to the current segment of the audio being played. The segment currently being played will also be indicated by the boldness of the text within the transcription. By utilizing the search bar shown in Figure <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title"><span class="ltx_ERROR undefined">\system</span>: A Visual Analytics Solution for <span class="ltx_text"> </span>Automated Speech Recognition Analysis and Exploration</span></span>(b), the analyst can query for specific keywords and traverse through all instances of the word within the transcription (<span id="S4.SS2.SSS2.p1.1.2" class="ltx_text ltx_font_bold">DG3</span>). The search word is highlighted in yellow within the transcription for easy viewing. The last actionable item the analyst can perform within the transcription view is editing the output. If and when the analyst finds errors within the transcription, the analyst can add or delete text, or even replace a word within the transcription with an alternative suggestion from AWS (<span id="S4.SS2.SSS2.p1.1.3" class="ltx_text ltx_font_bold">DG2</span>).</p>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Context Word Tree</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">Unlike the other two views above, the word tree view will only be populated when the analyst provides a specific keyword they are interested in exploring with the search bar. This view, as seen in Figure <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title"><span class="ltx_ERROR undefined">\system</span>: A Visual Analytics Solution for <span class="ltx_text"> </span>Automated Speech Recognition Analysis and Exploration</span></span>(d), depicts multiple parallel sequences of words. Based on visualizations such as the Word Tree <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and Sententree <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, we visualize a node-link diagram where nodes are words and links indicate word co-occurrence within the same segment. The size of the words in this visualization is proportional to the number of occurrences of the word observed. The average confidence of the keyword is also communicated to the analyst within this view.</p>
</div>
<div id="S4.SS2.SSS3.p2" class="ltx_para">
<p id="S4.SS2.SSS3.p2.1" class="ltx_p">In addition to the visualization, this view provides the analyst with a list of phonetically similar words – also known as homophones – to the current search keyword. The goal of presenting homophones in this view is to bring awareness of the nuances of the English language, allow the analyst to discern between similar-sounding words, and reduce misunderstandings in spoken and written discourse. The list of homophones shown to the analyst is gathered from an online platform that has curated an extensive collection of phonetically similar words <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS3.p3" class="ltx_para">
<p id="S4.SS2.SSS3.p3.1" class="ltx_p">With the word tree visualization, we aim to show which words most often follow or precede the specific keyword indicated by the analyst (<span id="S4.SS2.SSS3.p3.1.1" class="ltx_text ltx_font_bold">DG3</span>). This view will allow the analyst to explore and understand the context in which the specified word is being said. The analyst can also click on any neighboring word to navigate the tree visualization.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Case Study: Panda Diplomacy and the Nixon Tapes</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">During the Nixon Administration, China gifted two pandas – Ling-Ling and Hsing-Hsing – to the US. This case study will use the <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">Nixon White House Tapes</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> to learn about the <span id="S5.p1.1.2" class="ltx_text ltx_font_italic">Panda Diplomacy</span>.
We demonstrate how analysts can use <span id="S5.p1.1.3" class="ltx_ERROR undefined">\system</span> to find relevant data and enable them to answer their key intelligence questions more efficiently. We focus on two questions: “<span id="S5.p1.1.4" class="ltx_text ltx_font_italic">When were the pandas expected to arrive in the US?</span>” and “<span id="S5.p1.1.5" class="ltx_text ltx_font_italic">Which locations were considered for housing the pandas</span>?” We provide a video <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://youtu.be/hbeDn5D-GCg" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://youtu.be/hbeDn5D-GCg</a></span></span></span> walking through the case study. The reader can also click on the <img src="/html/2405.00223/assets/figures/link.png" id="S5.p1.1.g1" class="ltx_graphics ltx_img_square" width="8" height="8" alt="[Uncaptioned image]"> icons in subsections <a href="#S5.SS3" title="5.3 Extracting Intelligence and Deciding When to Rely on the AI’s Output ‣ 5 Case Study: Panda Diplomacy and the Nixon Tapes ‣ \system: A Visual Analytics Solution for Automated Speech Recognition Analysis and Exploration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a> to view the walk-through for that specific question.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>About the Nixon White House Tapes</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">U.S. President Richard Nixon’s administration secretly recorded conversations held in the White House from 1971 to 1973. These recordings, infamously as the Nixon White House Tapes surfaced during the Watergate Scandal and ultimately led to Nixon’s resignation. The administration installed recording devices in the Oval Office and other locations in the White House, intending to document meetings and conversations for historical purposes and to aid in decision-making. As these tapes were captured with concealed microphones, the audio quality is often poor, making the task of transcribing and extracting information after transcription strenuous and overwhelming for analysts. This is the ideal scenario for leveraging the strengths of <span id="S5.SS1.p1.1.1" class="ltx_ERROR undefined">\system</span> and visual analytics. The analysis of this data can reveal pivotal information, but ASR outputs are fraught with inaccuracies due to the poor recording qualities. Since manually transcribing and analyzing the data is tedious, the analyst can offload the computationally heavy work to the machine and utilize their knowledge and perception to generate insights from the transcription output.</p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2405.00223/assets/figures/q1.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="304" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Searching for “pandas” in the current transcription revealed two instances. We can observe that the first instance of this search term has a confidence score of 52%.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Reconnaissance, Quality Assessment, and Editing</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The analyst begins by searching for instances of “pandas” within the transcription (See Figure <a href="#S5.F2" title="Figure 2 ‣ 5.1 About the Nixon White House Tapes ‣ 5 Case Study: Panda Diplomacy and the Nixon Tapes ‣ \system: A Visual Analytics Solution for Automated Speech Recognition Analysis and Exploration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). The query results indicate that “pandas” is believed to be said only twice within this hour-and-a-half-long audio file. Puzzled, the analyst reads the first segment (line 707) that contains the term “pandas” and observes that the individual confidence in the term “pandas” (52%) is quite low compared to the overall average confidence of this line segment (80%). However, the terms “pan” and “panther” also appear in the segment. Considering that “pan” is a subword/substring of the desired search term and “panther” is phonetically similar to “pandas,” the analyst plays the source audio and confirms that “pandas” is indeed inaccurately labeled as “pan” or “panther.” The analyst edits the transcription to reflect the audio. We note that there is no search-and-replace feature as a precaution, so the analysts should listen to the audio before making changes to the transcription.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Extracting Intelligence and Deciding When to Rely on the AI’s Output</h3>

<section id="S5.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">
<span id="S5.SS3.SSS0.Px1.2.2" class="ltx_text ltx_font_bold">When were the pandas expected to arrive in the US?</span> <a target="_blank" href="https://youtu.be/hbeDn5D-GCg?si=56DGu--fjOJqB_gn&amp;t=77" title="" class="ltx_ref ltx_href"><img src="/html/2405.00223/assets/figures/link.png" id="S5.SS3.SSS0.Px1.1.1.g1" class="ltx_graphics ltx_img_square" width="8" height="8" alt="[Uncaptioned image]"></a>
</h5>

<div id="S5.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS3.SSS0.Px1.p1.1" class="ltx_p">While updating the transcription, the analyst discovered that line 709 refers to “pans” and “April 1st,” the latter with two instances and a confidence score of 100% for both. The analyst decided the source audio was unnecessary as this line segment showed the pandas arriving in the United States with high confidence on April 1st.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2405.00223/assets/figures/q2.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="297" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>After searching for “pan,” we observe “zoo” in the word tree. This indicates that “panda” was misclassified as “pan” and hints that a zoo may be where the pandas will be kept.</figcaption>
</figure>
</section>
<section id="S5.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">
<span id="S5.SS3.SSS0.Px2.2.2" class="ltx_text ltx_font_bold">Which locations were considered for housing the pandas?</span> <a target="_blank" href="https://youtu.be/hbeDn5D-GCg?si=DZ5w2Ojq3AfjIYqz&amp;t=187" title="" class="ltx_ref ltx_href"><img src="/html/2405.00223/assets/figures/link.png" id="S5.SS3.SSS0.Px2.1.1.g1" class="ltx_graphics ltx_img_square" width="8" height="8" alt="[Uncaptioned image]"></a>
</h5>

<div id="S5.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS3.SSS0.Px2.p1.1" class="ltx_p">As the analyst searches for more instances of “pan,” (See Figure <a href="#S5.F3" title="Figure 3 ‣ When were the pandas expected to arrive in the US? ‣ 5.3 Extracting Intelligence and Deciding When to Rely on the AI’s Output ‣ 5 Case Study: Panda Diplomacy and the Nixon Tapes ‣ \system: A Visual Analytics Solution for Automated Speech Recognition Analysis and Exploration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), they observe that word tree mentions “national zoo.” Additionally, line 704 lists several cities (San Diego, St. Louis, New York, and Chicago) known for their zoos. The transcription shows high confidence for the listed cities (ranging from 93% to 100%). Even though the confidence level for this particular segment is relatively high, the analyst decides to cross-check the source audio due to grammatical errors in the transcription. They discovered that despite considering the four listed cities, the pandas will be kept at the National Zoo in Washington, D.C., as one speaker reasoned that it is a tradition that all animals gifted to the U.S. are homed to this zoo. This can be uncovered by searching for “zoo” and observing the first instance of the term, shown by line 631 in Figure <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title"><span class="ltx_ERROR undefined">\system</span>: A Visual Analytics Solution for <span class="ltx_text"> </span>Automated Speech Recognition Analysis and Exploration</span></span>.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion and Future Work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Through this work, we aim to underscore the importance of AI transparency in fostering and calibrating appropriate trust in visual analytics, especially in scenarios involving domain experts making high-risk decisions.
Research has shown that domain experts tend to perform tasks on their own <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> despite the presence of AI assistance. To promote effective usage of AI tools, designers should carefully iterate through intuitive visual representations of uncertainty that do not add cognitive load to the analyst when performing a task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. However, even with the communication of model confidence, analysts are likely to engage with AI suggestions differently <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, influenced by their own definition/interpretation of confidence and their criteria for reliability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Further research is needed to explore and develop design guidelines for visual presentations of confidence that are responsive to individual analysts’ needs within decision-making tools such as <span id="S6.p1.1.1" class="ltx_ERROR undefined">\system</span>.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">In the transcription view of our system, we utilized underlining and color opacity to highlight uncertainty and possible errors within the output. As prior works have found inconclusive findings on the effectiveness of highlighting potential errors to aid in the correction of automatic speech-to-text data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, future work involves conducting user studies to validate the system’s ability to assist analysts in data cleaning and also uncovering relevant data more efficiently. In the future, we would also like to expand upon this work by providing additional assistance to analysts through guided suggestions. Through the observation of the analyst’s interactions and their analytical provenance, we can leverage user modeling algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> to offer the analyst relevant data points of interest in real-time for enhanced exploration and analysis.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We developed <span id="S7.p1.1.1" class="ltx_ERROR undefined">\system</span> alongside intelligence analysts to enhance the analyst’s understanding of speech-to-text results by providing visual representations of uncertainty in several views of our system. In this paper, we identify clear design goals that guided the development of this system and explore a realistic use case scenario investigating the Nixon White House tapes to demonstrate the system’s applicability. Finally, we discuss lessons learned from the development of this system and opportunities for improving textual data cleaning, promoting transparency, and fostering trust. Moving forward, continued efforts to improve the communication of model uncertainty and build appropriate reliance will further enhance the efficacy of human-machine collaborations within visual analytics.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
Thank you to Syrine Matoussi, Lan Kung, Christine Brugh, and our LAS collaborators for their valuable feedback. This work is partly supported by the National Science Foundation under Grant Nos. OAC-2118201 and IIS-2142977.
It is also based upon work done, in whole or in part, in coordination with the Department of Defense (DoD). Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the DoD and/or any agency or entity of the United States Government.


</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
National Archives, White House Tapes — Richard Nixon Museum and Library, 1999.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Amazon Web Services Inc., AWS Transcribe Documentation, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A. Aloisi.

</span>
<span class="ltx_bibblock">Homophone Dictionary, 2008.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
L. Berke, C. Caulfield, and M. Huenerfauth.

</span>
<span class="ltx_bibblock">Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility</span>, pp. 155–164. ACM, 2017.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
K. Brodlie, R. Allendes Osorio, and A. Lopes.

</span>
<span class="ltx_bibblock">A Review of Uncertainty in Data Visualization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Expanding the Frontiers of Visual Analytics and Visualization</span>, pp. 81–109. Springer London, London, 2012. doi: 10 . 1007/978-1-4471-2804-5_6

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Z. Buçinca, M. B. Malaya, and K. Z. Gajos.

</span>
<span class="ltx_bibblock">To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings of the ACM on Human-Computer Interaction</span>, 5(CSCW1):1–21, 2021. doi: 10 . 1145/3449287

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
F. Dabek and J. J. Caban.

</span>
<span class="ltx_bibblock">A Grammar-based Approach for Modeling User Interactions and Generating Suggestions During the Data Exploration Process.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>, 23(1):41–50, 2017.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. Dasgupta, J.-Y. Lee, R. Wilson, R. A. Lafrance, N. Cramer, K. Cook, and S. Payne.

</span>
<span class="ltx_bibblock">Familiarity Vs Trust: A Comparative Study of Domain Scientists’ Trust in Visual Analytics and Conventional Analysis Methods.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>, 23(1):271–280, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
B. J. Dietvorst, J. P. Simmons, and C. Massey.

</span>
<span class="ltx_bibblock">Algorithm aversion: People erroneously avoid algorithms after seeing them err.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Journal of Experimental Psychology: General</span>, 144(1):114–126, 2015. doi: 10 . 1037/xge0000033

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M. Fernandes, L. Walls, S. Munson, J. Hullman, and M. Kay.

</span>
<span class="ltx_bibblock">Uncertainty Displays Using Quantile Dotplots or CDFs Improve Transit Decision-Making.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</span>, pp. 1–12. ACM, Montreal QC Canada, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
S. Ha, S. Monadjemi, R. Garnett, and A. Ottley.

</span>
<span class="ltx_bibblock">A Unified Comparison of User Modeling Techniques for Predicting Data Interaction and Detecting Exploration Bias.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>, 29:483–492, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Ha, S. Monadjemi, and A. Ottley.

</span>
<span class="ltx_bibblock">Guided By AI: Navigating Trust, Bias, and Data Exploration in AI-Guided Visual Analytics.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Computer Graphics Forum</span>, 2024.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M. Hu, K. Wongsuphasawat, and J. Stasko.

</span>
<span class="ltx_bibblock">Visualizing Social Media Content with SentenTree.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>, 23(1):621–630, 2017. doi: 10 . 1109/TVCG . 2016 . 2598590

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. Jacobs, M. F. Pradier, T. H. McCoy, R. H. Perlis, F. Doshi-Velez, and K. Z. Gajos.

</span>
<span class="ltx_bibblock">How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Translational Psychiatry</span>, 11(1):108, 2021. doi: 10 . 1038/s41398-021-01224-x

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M. F. Jung, D. Sirkin, T. M. Gür, and M. Steinert.

</span>
<span class="ltx_bibblock">Displayed Uncertainty Improves Driving Experience and Behavior: The Case of Range Anxiety in an Electric Car.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</span>, pp. 2201–2210. ACM, Seoul Republic of Korea, 2015. doi: 10 . 1145/2702123 . 2702479

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
M. Kay, T. Kola, J. R. Hullman, and S. A. Munson.

</span>
<span class="ltx_bibblock">When (ish) is My Bus?: User-centered Visualizations of Uncertainty in Everyday, Mobile Predictive Systems.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</span>, pp. 5092–5103. ACM, San Jose California USA, 2016. doi: 10 . 1145/2858036 . 2858558

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A. Kim, M. Yang, and J. Zhang.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">When Algorithms Err</span> : Differential Impact of Early vs. Late Errors on Users’ Reliance on Algorithms.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text ltx_font_italic">ACM Transactions on Computer-Human Interaction</span>, 30(1):1–36, 2023. doi: 10 . 1145/3557889

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S. Latif, J. Qadir, A. Qayyum, M. Usama, and S. Younis.

</span>
<span class="ltx_bibblock">Speech Technology for Healthcare: Opportunities, Challenges, and State of the Art.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">IEEE Reviews in Biomedical Engineering</span>, 14:342–356, 2021. doi: 10 . 1109/RBME . 2020 . 3006860

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
S. Monadjemi, R. Garnett, and A. Ottley.

</span>
<span class="ltx_bibblock">Competing models: Inferring exploration patterns and information relevance via bayesian model selection.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>, 27(2):412–421, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S. Monadjemi, S. Ha, Q. Nguyen, H. Chai, R. Garnett, and A. Ottley.

</span>
<span class="ltx_bibblock">Guided data discovery in interactive visualizations via active search.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">2022 IEEE Visualization and Visual Analytics (VIS)</span>, pp. 70–74, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. Ottley.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Adaptive and personalized visualization</span>.

</span>
<span class="ltx_bibblock">Springer Nature, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A. Piquard-Kipffer, O. Mella, J. Miranda, D. Jouvet, and L. Orosanu.

</span>
<span class="ltx_bibblock">Qualitative investigation of the display of speech recognition results for communication with deaf people.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proceedings of SLPAT 2015: 6th Workshop on Speech and Language Processing for Assistive Technologies</span>, pp. 36–41. Association for Computational Linguistics, Dresden, Germany, 2015. doi: 10 . 18653/v1/W15-5107

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
R. Prasad, L. Nguyen, R. Schwartz, and J. Makhoul.

</span>
<span class="ltx_bibblock">Automatic transcription of courtroom speech.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">7th International Conference on Spoken Language Processing (ICSLP 2002)</span>, pp. 1745–1748. ISCA, 2002. doi: 10 . 21437/ICSLP . 2002-520

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
D. Sacha, H. Senaratne, B. C. Kwon, G. Ellis, and D. A. Keim.

</span>
<span class="ltx_bibblock">The Role of Uncertainty, Awareness, and Trust in Visual Analytics.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>, 22(1):240–249, 2016.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
T. B. Sheridan.

</span>
<span class="ltx_bibblock">Individual Differences in Attributes of Trust in Automation: Measurement and Application to System Design.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Frontiers in Psychology</span>, 10, 2019. doi: 10 . 3389/fpsyg . 2019 . 01117

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
T. H. Soe, F. Guribye, and M. Slavkovik.

</span>
<span class="ltx_bibblock">Evaluating AI assisted subtitling.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">ACM International Conference on Interactive Media Experiences</span>, pp. 96–107. ACM, Virtual Event USA, 2021. doi: 10 . 1145/3452918 . 3458792

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
B. Suhm, B. Myers, and A. Waibel.

</span>
<span class="ltx_bibblock">Multimodal error correction for speech user interfaces.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Computer-Human Interaction</span>, 8(1):60–98, 2001. doi: 10 . 1145/371127 . 371166

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
K. Vertanen and P. O. Kristensson.

</span>
<span class="ltx_bibblock">On the benefits of confidence visualization in speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</span>, pp. 1497–1500. ACM, Florence Italy, 2008. doi: 10 . 1145/1357054 . 1357288

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
X. Wang and M. Yin.

</span>
<span class="ltx_bibblock">Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">26th International Conference on Intelligent User Interfaces</span>, pp. 318–328. ACM, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
M. Wattenberg and F. Viegas.

</span>
<span class="ltx_bibblock">The Word Tree, an Interactive Visual Concordance.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Visualization and Computer Graphics</span>, 14(6):1221–1228, 2008. doi: 10 . 1109/TVCG . 2008 . 172

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
T. H. Wu, Y. Zhao, and M. Amiruzzaman.

</span>
<span class="ltx_bibblock">Interactive Visualization of AI-based Speech Recognition Texts.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">EuroVis Workshop on Visual Analytics (EuroVA)</span>, 2020. doi: 10 . 2312/EUROVA . 20201091

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
J. Zhou, S. Z. Arshad, S. Luo, and F. Chen.

</span>
<span class="ltx_bibblock">Effects of uncertainty and cognitive load on user trust in predictive decision making.

</span>
<span class="ltx_bibblock">In <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">IFIP conference on human-computer interaction</span>, pp. 23–39. Springer, 2017.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.00222" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.00223" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.00223">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.00223" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.00224" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 17:06:47 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
