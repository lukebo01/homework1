<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.05554] Improving Whisper's Recognition Performance for Under-Represented Language Kazakh Leveraging Unpaired Speech and Text</title><meta property="og:description" content="Whisper and other large-scale automatic speech recognition models have made significant progress in performance. However, their performance on many low-resource languages, such as Kazakh, is not satisfactory. It is wor‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Improving Whisper's Recognition Performance for Under-Represented Language Kazakh Leveraging Unpaired Speech and Text">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Improving Whisper's Recognition Performance for Under-Represented Language Kazakh Leveraging Unpaired Speech and Text">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.05554">

<!--Generated on Thu Sep  5 12:57:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">JinpengLi
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>YuPu
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>QiSun
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>[affiliation=*]Wei-QiangZhang




</p>
</div>
<h1 class="ltx_title ltx_title_document">Improving Whisper's Recognition Performance for Under-Represented Language Kazakh Leveraging Unpaired Speech and Text</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">Whisper and other large-scale automatic speech recognition models have made significant progress in performance. However, their performance on many low-resource languages, such as Kazakh, is not satisfactory. It is worth researching how to utilize low-cost data to improve the performance of Whisper on under-represented languages. In this study, we utilized easily accessible unpaired speech and text data and combined the language model GPT with Whisper on Kazakh. We implemented end of transcript (EOT) judgment modification and hallucination penalty to improve the performance of speech recognition. Further, we employed the decoding average token log probability as a criterion to select samples from unlabeled speech data and used pseudo-labeled data to fine-tune the model to further improve its performance. Ultimately, we achieved more than 10% absolute WER reduction in multiple experiments, and the whole process has the potential to be generalized to other under-represented languages.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>under-represented language, speech recognition, unpaired data, pseudo-label fine-tuning
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The development of end-to-end (E2E) automatic speech recognition (ASR) systems has seen significant advancements in the field of speech recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. However, training E2E models to achieve satisfactory recognition results requires large amounts of high-quality labeled speech data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. This poses a substantial bottleneck for low-resource languages that lack adequate labeled speech data. Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> is a universal multilingual speech recognition model trained on 680,000 hours of supervised data. However, it has not been as effective for many low-resource languages. Since only a few languages have sufficient annotated speech data, while most languages are resource-scarce in this regard, it is crucial to investigate how limited low-cost data can be leveraged to improve low-resource ASR systems.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Various strategies have been proposed by researchers to address this challenge. Multilingual transfer learning and multilingual meta-learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> are two approaches that utilize labeled data to pre-train a foundational model that can be applied across multiple languages. However, both methods require paired labeled data in both the source and target languages for pre-training and fine-tuning, respectively. Unfortunately, paired data for under-represented languages is scarce.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">A promising alternative lies in leveraging unlabeled data through self-supervised or semi-supervised learning techniques. Self-supervised learning (SSL) leverages readily available unpaired speech data. Inspired by masked language models in text, masked acoustic models are trained to predict masked segments of speech, learning representations without labels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. These SSL models, when fine-tuned with a small amount of labeled data, have shown significant improvements in low-resource ASR systems. The success of models like wav2vec2 XLSR-53 and HuBERT, pre-trained on vast amounts of unlabeled data, exemplifies the effectiveness of SSL in this domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. However, self-supervised methods are difficult to apply to the trained Whisper model due to the fact that its encoder already has excellent representation capabilities through large-scale supervised training. In this case, semi-supervised methods are more suitable, such as iterative pseudo-labeling, which utilizes language models to create pseudo-labels for unlabeled data and combines them with a small amount of labeled data to expand the training set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we select Kazakh as an example of under-represented languages for study.
Kazakh is the official language of Kazakhstan and belongs to the Turkic language family, but it is still under-represented in speech recognition. Whisper does not perform well in Kazakh, specifically, the word error rate (WER) for Kazakh is over 40% on the Fleurs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> test set and over 55% on the KSC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> test set. In this work, we only utilize low-cost unpaired speech and text data that can be easily accessed online without requiring manual labeling. This appears to be related to approximate unsupervised learning methods, such as wav2vec-U <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. However, due to the requirement of pronunciation lexicon information and the difficulty in achieving convergence during training of GAN models, we chose to use this data to improve the performance of Whisper.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Specifically, we leveraged easily accessible text data and integrated language model GPT with Whisper, implementing improvements such as end of transcript (EOT) judgment modification and hallucination penalty. We found that decoding with GPT leads to a more significant reduction in WER for samples with higher average token log probability (ALP). Therefore, we employed ALP as a criterion to select samples from unlabeled speech data, thus fine-tuning the model using pseudo-labels. In multiple experiments, we achieved more than 10% absolute WER reduction, and the pipeline is scalable to other under-represented languages.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Leveraging text data</h3>

<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Integrate GPT with Whisper</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">We first trained a language model using text data and then integrated it as an additional decoder in the Whisper framework, working in conjunction with its original decoder. Through this approach, our goal is to harness the complementary advantages of the two decoders to improve Whisper's recognition performance on under-represented languages. Similar attempts have been made previously by integrating the official GPT-2 model into the decoding process of Whisper and applying it to English <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. However, since Whisper already performs well in English, the improvement was relatively slight. In our study, we utilized mGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> with 1.3 billion parameters, sharing the same architecture as GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. After adjusting the tokenizer to be consistent with Whisper and retraining it with text data, we applied this model to under-represented languages to improve recognition performance.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2408.05554/assets/figures/gpt-all.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="592" height="471" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Integrating GPT into the decoding process of Whisper.</figcaption>
</figure>
<div id="S2.SS1.SSS1.p2" class="ltx_para">
<p id="S2.SS1.SSS1.p2.7" class="ltx_p">Figure <a href="#S2.F1" title="Figure 1 ‚Ä£ 2.1.1 Integrate GPT with Whisper ‚Ä£ 2.1 Leveraging text data ‚Ä£ 2 Methods ‚Ä£ Improving Whisper's Recognition Performance for Under-Represented Language Kazakh Leveraging Unpaired Speech and Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the decoding process after integrating GPT into Whisper. Let the audio input be denoted as <math id="S2.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.SS1.SSS1.p2.1.m1.1a"><mi id="S2.SS1.SSS1.p2.1.m1.1.1" xref="S2.SS1.SSS1.p2.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.1.m1.1b"><ci id="S2.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S2.SS1.SSS1.p2.1.m1.1.1">ùëã</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.1.m1.1c">X</annotation></semantics></math>, and in a certain iteration of the autoregressive decoding, the token sequence of the preceding context is denoted as <math id="S2.SS1.SSS1.p2.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.SS1.SSS1.p2.2.m2.1a"><mi id="S2.SS1.SSS1.p2.2.m2.1.1" xref="S2.SS1.SSS1.p2.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.2.m2.1b"><ci id="S2.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S2.SS1.SSS1.p2.2.m2.1.1">ùëá</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.2.m2.1c">T</annotation></semantics></math>. The probabilities for the next decoding token <math id="S2.SS1.SSS1.p2.3.m3.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.SS1.SSS1.p2.3.m3.1a"><mi id="S2.SS1.SSS1.p2.3.m3.1.1" xref="S2.SS1.SSS1.p2.3.m3.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.3.m3.1b"><ci id="S2.SS1.SSS1.p2.3.m3.1.1.cmml" xref="S2.SS1.SSS1.p2.3.m3.1.1">ùëå</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.3.m3.1c">Y</annotation></semantics></math> for Whisper decoder and GPT are <math id="S2.SS1.SSS1.p2.4.m4.3" class="ltx_Math" alttext="P_{\text{Whisper}}(Y|X,T)" display="inline"><semantics id="S2.SS1.SSS1.p2.4.m4.3a"><mrow id="S2.SS1.SSS1.p2.4.m4.3.3" xref="S2.SS1.SSS1.p2.4.m4.3.3.cmml"><msub id="S2.SS1.SSS1.p2.4.m4.3.3.3" xref="S2.SS1.SSS1.p2.4.m4.3.3.3.cmml"><mi id="S2.SS1.SSS1.p2.4.m4.3.3.3.2" xref="S2.SS1.SSS1.p2.4.m4.3.3.3.2.cmml">P</mi><mtext id="S2.SS1.SSS1.p2.4.m4.3.3.3.3" xref="S2.SS1.SSS1.p2.4.m4.3.3.3.3a.cmml">Whisper</mtext></msub><mo lspace="0em" rspace="0em" id="S2.SS1.SSS1.p2.4.m4.3.3.2" xref="S2.SS1.SSS1.p2.4.m4.3.3.2.cmml">‚Äã</mo><mrow id="S2.SS1.SSS1.p2.4.m4.3.3.1.1" xref="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.SSS1.p2.4.m4.3.3.1.1.2" xref="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1.cmml">(</mo><mrow id="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1" xref="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1.cmml"><mi id="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1.2" xref="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1.2.cmml">Y</mi><mo fence="false" id="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1.1" xref="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1.1.cmml">|</mo><mrow id="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1.3.2" xref="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1.3.1.cmml"><mi id="S2.SS1.SSS1.p2.4.m4.1.1" xref="S2.SS1.SSS1.p2.4.m4.1.1.cmml">X</mi><mo id="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1.3.2.1" xref="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1.3.1.cmml">,</mo><mi id="S2.SS1.SSS1.p2.4.m4.2.2" xref="S2.SS1.SSS1.p2.4.m4.2.2.cmml">T</mi></mrow></mrow><mo stretchy="false" id="S2.SS1.SSS1.p2.4.m4.3.3.1.1.3" xref="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.4.m4.3b"><apply id="S2.SS1.SSS1.p2.4.m4.3.3.cmml" xref="S2.SS1.SSS1.p2.4.m4.3.3"><times id="S2.SS1.SSS1.p2.4.m4.3.3.2.cmml" xref="S2.SS1.SSS1.p2.4.m4.3.3.2"></times><apply id="S2.SS1.SSS1.p2.4.m4.3.3.3.cmml" xref="S2.SS1.SSS1.p2.4.m4.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p2.4.m4.3.3.3.1.cmml" xref="S2.SS1.SSS1.p2.4.m4.3.3.3">subscript</csymbol><ci id="S2.SS1.SSS1.p2.4.m4.3.3.3.2.cmml" xref="S2.SS1.SSS1.p2.4.m4.3.3.3.2">ùëÉ</ci><ci id="S2.SS1.SSS1.p2.4.m4.3.3.3.3a.cmml" xref="S2.SS1.SSS1.p2.4.m4.3.3.3.3"><mtext mathsize="70%" id="S2.SS1.SSS1.p2.4.m4.3.3.3.3.cmml" xref="S2.SS1.SSS1.p2.4.m4.3.3.3.3">Whisper</mtext></ci></apply><apply id="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1.cmml" xref="S2.SS1.SSS1.p2.4.m4.3.3.1.1"><csymbol cd="latexml" id="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1.1.cmml" xref="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1.1">conditional</csymbol><ci id="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1.2.cmml" xref="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1.2">ùëå</ci><list id="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1.3.1.cmml" xref="S2.SS1.SSS1.p2.4.m4.3.3.1.1.1.3.2"><ci id="S2.SS1.SSS1.p2.4.m4.1.1.cmml" xref="S2.SS1.SSS1.p2.4.m4.1.1">ùëã</ci><ci id="S2.SS1.SSS1.p2.4.m4.2.2.cmml" xref="S2.SS1.SSS1.p2.4.m4.2.2">ùëá</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.4.m4.3c">P_{\text{Whisper}}(Y|X,T)</annotation></semantics></math> and <math id="S2.SS1.SSS1.p2.5.m5.1" class="ltx_Math" alttext="P_{\text{GPT}}(Y|T)" display="inline"><semantics id="S2.SS1.SSS1.p2.5.m5.1a"><mrow id="S2.SS1.SSS1.p2.5.m5.1.1" xref="S2.SS1.SSS1.p2.5.m5.1.1.cmml"><msub id="S2.SS1.SSS1.p2.5.m5.1.1.3" xref="S2.SS1.SSS1.p2.5.m5.1.1.3.cmml"><mi id="S2.SS1.SSS1.p2.5.m5.1.1.3.2" xref="S2.SS1.SSS1.p2.5.m5.1.1.3.2.cmml">P</mi><mtext id="S2.SS1.SSS1.p2.5.m5.1.1.3.3" xref="S2.SS1.SSS1.p2.5.m5.1.1.3.3a.cmml">GPT</mtext></msub><mo lspace="0em" rspace="0em" id="S2.SS1.SSS1.p2.5.m5.1.1.2" xref="S2.SS1.SSS1.p2.5.m5.1.1.2.cmml">‚Äã</mo><mrow id="S2.SS1.SSS1.p2.5.m5.1.1.1.1" xref="S2.SS1.SSS1.p2.5.m5.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.SSS1.p2.5.m5.1.1.1.1.2" xref="S2.SS1.SSS1.p2.5.m5.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.SSS1.p2.5.m5.1.1.1.1.1" xref="S2.SS1.SSS1.p2.5.m5.1.1.1.1.1.cmml"><mi id="S2.SS1.SSS1.p2.5.m5.1.1.1.1.1.2" xref="S2.SS1.SSS1.p2.5.m5.1.1.1.1.1.2.cmml">Y</mi><mo fence="false" id="S2.SS1.SSS1.p2.5.m5.1.1.1.1.1.1" xref="S2.SS1.SSS1.p2.5.m5.1.1.1.1.1.1.cmml">|</mo><mi id="S2.SS1.SSS1.p2.5.m5.1.1.1.1.1.3" xref="S2.SS1.SSS1.p2.5.m5.1.1.1.1.1.3.cmml">T</mi></mrow><mo stretchy="false" id="S2.SS1.SSS1.p2.5.m5.1.1.1.1.3" xref="S2.SS1.SSS1.p2.5.m5.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.5.m5.1b"><apply id="S2.SS1.SSS1.p2.5.m5.1.1.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1"><times id="S2.SS1.SSS1.p2.5.m5.1.1.2.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.2"></times><apply id="S2.SS1.SSS1.p2.5.m5.1.1.3.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p2.5.m5.1.1.3.1.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.3">subscript</csymbol><ci id="S2.SS1.SSS1.p2.5.m5.1.1.3.2.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.3.2">ùëÉ</ci><ci id="S2.SS1.SSS1.p2.5.m5.1.1.3.3a.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.3.3"><mtext mathsize="70%" id="S2.SS1.SSS1.p2.5.m5.1.1.3.3.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.3.3">GPT</mtext></ci></apply><apply id="S2.SS1.SSS1.p2.5.m5.1.1.1.1.1.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.1.1"><csymbol cd="latexml" id="S2.SS1.SSS1.p2.5.m5.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.1.1.1.1">conditional</csymbol><ci id="S2.SS1.SSS1.p2.5.m5.1.1.1.1.1.2.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.1.1.1.2">ùëå</ci><ci id="S2.SS1.SSS1.p2.5.m5.1.1.1.1.1.3.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.1.1.1.3">ùëá</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.5.m5.1c">P_{\text{GPT}}(Y|T)</annotation></semantics></math>, respectively. The weight of GPT is defined as <math id="S2.SS1.SSS1.p2.6.m6.1" class="ltx_Math" alttext="\lambda_{\text{GPT}}" display="inline"><semantics id="S2.SS1.SSS1.p2.6.m6.1a"><msub id="S2.SS1.SSS1.p2.6.m6.1.1" xref="S2.SS1.SSS1.p2.6.m6.1.1.cmml"><mi id="S2.SS1.SSS1.p2.6.m6.1.1.2" xref="S2.SS1.SSS1.p2.6.m6.1.1.2.cmml">Œª</mi><mtext id="S2.SS1.SSS1.p2.6.m6.1.1.3" xref="S2.SS1.SSS1.p2.6.m6.1.1.3a.cmml">GPT</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.6.m6.1b"><apply id="S2.SS1.SSS1.p2.6.m6.1.1.cmml" xref="S2.SS1.SSS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p2.6.m6.1.1.1.cmml" xref="S2.SS1.SSS1.p2.6.m6.1.1">subscript</csymbol><ci id="S2.SS1.SSS1.p2.6.m6.1.1.2.cmml" xref="S2.SS1.SSS1.p2.6.m6.1.1.2">ùúÜ</ci><ci id="S2.SS1.SSS1.p2.6.m6.1.1.3a.cmml" xref="S2.SS1.SSS1.p2.6.m6.1.1.3"><mtext mathsize="70%" id="S2.SS1.SSS1.p2.6.m6.1.1.3.cmml" xref="S2.SS1.SSS1.p2.6.m6.1.1.3">GPT</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.6.m6.1c">\lambda_{\text{GPT}}</annotation></semantics></math>. The selection criterion for <math id="S2.SS1.SSS1.p2.7.m7.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.SS1.SSS1.p2.7.m7.1a"><mi id="S2.SS1.SSS1.p2.7.m7.1.1" xref="S2.SS1.SSS1.p2.7.m7.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.7.m7.1b"><ci id="S2.SS1.SSS1.p2.7.m7.1.1.cmml" xref="S2.SS1.SSS1.p2.7.m7.1.1">ùëå</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.7.m7.1c">Y</annotation></semantics></math>, i.e., the calculation formula for the log probability (LP) of this token, is given by:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.3" class="ltx_Math" alttext="\mathrm{LP}=\frac{1}{1+\lambda_{\text{GPT}}}(\log P_{\text{Whisper}}(Y|X,T)+\lambda_{\text{GPT}}\log P_{\text{GPT}}(Y|T))" display="block"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml"><mi id="S2.E1.m1.3.3.3" xref="S2.E1.m1.3.3.3.cmml">LP</mi><mo id="S2.E1.m1.3.3.2" xref="S2.E1.m1.3.3.2.cmml">=</mo><mrow id="S2.E1.m1.3.3.1" xref="S2.E1.m1.3.3.1.cmml"><mfrac id="S2.E1.m1.3.3.1.3" xref="S2.E1.m1.3.3.1.3.cmml"><mn id="S2.E1.m1.3.3.1.3.2" xref="S2.E1.m1.3.3.1.3.2.cmml">1</mn><mrow id="S2.E1.m1.3.3.1.3.3" xref="S2.E1.m1.3.3.1.3.3.cmml"><mn id="S2.E1.m1.3.3.1.3.3.2" xref="S2.E1.m1.3.3.1.3.3.2.cmml">1</mn><mo id="S2.E1.m1.3.3.1.3.3.1" xref="S2.E1.m1.3.3.1.3.3.1.cmml">+</mo><msub id="S2.E1.m1.3.3.1.3.3.3" xref="S2.E1.m1.3.3.1.3.3.3.cmml"><mi id="S2.E1.m1.3.3.1.3.3.3.2" xref="S2.E1.m1.3.3.1.3.3.3.2.cmml">Œª</mi><mtext id="S2.E1.m1.3.3.1.3.3.3.3" xref="S2.E1.m1.3.3.1.3.3.3.3a.cmml">GPT</mtext></msub></mrow></mfrac><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.1.2" xref="S2.E1.m1.3.3.1.2.cmml">‚Äã</mo><mrow id="S2.E1.m1.3.3.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.3.3.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.cmml"><mrow id="S2.E1.m1.3.3.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.3.1" xref="S2.E1.m1.3.3.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S2.E1.m1.3.3.1.1.1.1.1.3a" xref="S2.E1.m1.3.3.1.1.1.1.1.3.cmml">‚Å°</mo><msub id="S2.E1.m1.3.3.1.1.1.1.1.3.2" xref="S2.E1.m1.3.3.1.1.1.1.1.3.2.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.3.2.2" xref="S2.E1.m1.3.3.1.1.1.1.1.3.2.2.cmml">P</mi><mtext id="S2.E1.m1.3.3.1.1.1.1.1.3.2.3" xref="S2.E1.m1.3.3.1.1.1.1.1.3.2.3a.cmml">Whisper</mtext></msub></mrow><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.2.cmml">‚Äã</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">Y</mi><mo fence="false" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml">|</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.3.1.cmml"><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">X</mi><mo id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.3.2.1" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.3.1.cmml">,</mo><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">T</mi></mrow></mrow><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.3.3.1.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.3.cmml">+</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.2.cmml"><msub id="S2.E1.m1.3.3.1.1.1.1.2.3" xref="S2.E1.m1.3.3.1.1.1.1.2.3.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.2.3.2" xref="S2.E1.m1.3.3.1.1.1.1.2.3.2.cmml">Œª</mi><mtext id="S2.E1.m1.3.3.1.1.1.1.2.3.3" xref="S2.E1.m1.3.3.1.1.1.1.2.3.3a.cmml">GPT</mtext></msub><mo lspace="0.167em" rspace="0em" id="S2.E1.m1.3.3.1.1.1.1.2.2" xref="S2.E1.m1.3.3.1.1.1.1.2.2.cmml">‚Äã</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.2.4" xref="S2.E1.m1.3.3.1.1.1.1.2.4.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.2.4.1" xref="S2.E1.m1.3.3.1.1.1.1.2.4.1.cmml">log</mi><mo lspace="0.167em" id="S2.E1.m1.3.3.1.1.1.1.2.4a" xref="S2.E1.m1.3.3.1.1.1.1.2.4.cmml">‚Å°</mo><msub id="S2.E1.m1.3.3.1.1.1.1.2.4.2" xref="S2.E1.m1.3.3.1.1.1.1.2.4.2.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.2.4.2.2" xref="S2.E1.m1.3.3.1.1.1.1.2.4.2.2.cmml">P</mi><mtext id="S2.E1.m1.3.3.1.1.1.1.2.4.2.3" xref="S2.E1.m1.3.3.1.1.1.1.2.4.2.3a.cmml">GPT</mtext></msub></mrow><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.1.1.1.1.2.2a" xref="S2.E1.m1.3.3.1.1.1.1.2.2.cmml">‚Äã</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.2.1.1" xref="S2.E1.m1.3.3.1.1.1.1.2.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.1.2.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.2.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.3.3.1.1.1.1.2.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.2.1.1.1.cmml"><mi id="S2.E1.m1.3.3.1.1.1.1.2.1.1.1.2" xref="S2.E1.m1.3.3.1.1.1.1.2.1.1.1.2.cmml">Y</mi><mo fence="false" id="S2.E1.m1.3.3.1.1.1.1.2.1.1.1.1" xref="S2.E1.m1.3.3.1.1.1.1.2.1.1.1.1.cmml">|</mo><mi id="S2.E1.m1.3.3.1.1.1.1.2.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.2.1.1.1.3.cmml">T</mi></mrow><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.1.2.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S2.E1.m1.3.3.1.1.1.3" xref="S2.E1.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3"><eq id="S2.E1.m1.3.3.2.cmml" xref="S2.E1.m1.3.3.2"></eq><ci id="S2.E1.m1.3.3.3.cmml" xref="S2.E1.m1.3.3.3">LP</ci><apply id="S2.E1.m1.3.3.1.cmml" xref="S2.E1.m1.3.3.1"><times id="S2.E1.m1.3.3.1.2.cmml" xref="S2.E1.m1.3.3.1.2"></times><apply id="S2.E1.m1.3.3.1.3.cmml" xref="S2.E1.m1.3.3.1.3"><divide id="S2.E1.m1.3.3.1.3.1.cmml" xref="S2.E1.m1.3.3.1.3"></divide><cn type="integer" id="S2.E1.m1.3.3.1.3.2.cmml" xref="S2.E1.m1.3.3.1.3.2">1</cn><apply id="S2.E1.m1.3.3.1.3.3.cmml" xref="S2.E1.m1.3.3.1.3.3"><plus id="S2.E1.m1.3.3.1.3.3.1.cmml" xref="S2.E1.m1.3.3.1.3.3.1"></plus><cn type="integer" id="S2.E1.m1.3.3.1.3.3.2.cmml" xref="S2.E1.m1.3.3.1.3.3.2">1</cn><apply id="S2.E1.m1.3.3.1.3.3.3.cmml" xref="S2.E1.m1.3.3.1.3.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.3.3.3.1.cmml" xref="S2.E1.m1.3.3.1.3.3.3">subscript</csymbol><ci id="S2.E1.m1.3.3.1.3.3.3.2.cmml" xref="S2.E1.m1.3.3.1.3.3.3.2">ùúÜ</ci><ci id="S2.E1.m1.3.3.1.3.3.3.3a.cmml" xref="S2.E1.m1.3.3.1.3.3.3.3"><mtext mathsize="70%" id="S2.E1.m1.3.3.1.3.3.3.3.cmml" xref="S2.E1.m1.3.3.1.3.3.3.3">GPT</mtext></ci></apply></apply></apply><apply id="S2.E1.m1.3.3.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1"><plus id="S2.E1.m1.3.3.1.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.3"></plus><apply id="S2.E1.m1.3.3.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1"><times id="S2.E1.m1.3.3.1.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.2"></times><apply id="S2.E1.m1.3.3.1.1.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.3"><log id="S2.E1.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.3.1"></log><apply id="S2.E1.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.1.3.2.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.3.2.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.3.2.2">ùëÉ</ci><ci id="S2.E1.m1.3.3.1.1.1.1.1.3.2.3a.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.3.2.3"><mtext mathsize="70%" id="S2.E1.m1.3.3.1.1.1.1.1.3.2.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.3.2.3">Whisper</mtext></ci></apply></apply><apply id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.2">ùëå</ci><list id="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.1.1.1.1.3.2"><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">ùëã</ci><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">ùëá</ci></list></apply></apply><apply id="S2.E1.m1.3.3.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2"><times id="S2.E1.m1.3.3.1.1.1.1.2.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2.2"></times><apply id="S2.E1.m1.3.3.1.1.1.1.2.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.2.3.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2.3">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.2.3.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2.3.2">ùúÜ</ci><ci id="S2.E1.m1.3.3.1.1.1.1.2.3.3a.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2.3.3"><mtext mathsize="70%" id="S2.E1.m1.3.3.1.1.1.1.2.3.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2.3.3">GPT</mtext></ci></apply><apply id="S2.E1.m1.3.3.1.1.1.1.2.4.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2.4"><log id="S2.E1.m1.3.3.1.1.1.1.2.4.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2.4.1"></log><apply id="S2.E1.m1.3.3.1.1.1.1.2.4.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2.4.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.2.4.2.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2.4.2">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.2.4.2.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2.4.2.2">ùëÉ</ci><ci id="S2.E1.m1.3.3.1.1.1.1.2.4.2.3a.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2.4.2.3"><mtext mathsize="70%" id="S2.E1.m1.3.3.1.1.1.1.2.4.2.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2.4.2.3">GPT</mtext></ci></apply></apply><apply id="S2.E1.m1.3.3.1.1.1.1.2.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2.1.1"><csymbol cd="latexml" id="S2.E1.m1.3.3.1.1.1.1.2.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2.1.1.1.1">conditional</csymbol><ci id="S2.E1.m1.3.3.1.1.1.1.2.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2.1.1.1.2">ùëå</ci><ci id="S2.E1.m1.3.3.1.1.1.1.2.1.1.1.3.cmml" xref="S2.E1.m1.3.3.1.1.1.1.2.1.1.1.3">ùëá</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">\mathrm{LP}=\frac{1}{1+\lambda_{\text{GPT}}}(\log P_{\text{Whisper}}(Y|X,T)+\lambda_{\text{GPT}}\log P_{\text{GPT}}(Y|T))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.SSS1.p2.8" class="ltx_p">In the equation, we found that taking the logarithm of the two probabilities separately and then adding them weightedly yields better results than taking the weighted sum of probabilities and then taking the logarithm. In addition, when determining the end of transcription, it is important to depend on the speech content and not allow the language model to generate redundant content. Therefore, we made end of transcription (EOT) judgment modification: based on the output probability of Whisper, if the current token's probability of being EOT is the highest, then <math id="S2.SS1.SSS1.p2.8.m1.1" class="ltx_Math" alttext="\lambda_{\text{GPT}}" display="inline"><semantics id="S2.SS1.SSS1.p2.8.m1.1a"><msub id="S2.SS1.SSS1.p2.8.m1.1.1" xref="S2.SS1.SSS1.p2.8.m1.1.1.cmml"><mi id="S2.SS1.SSS1.p2.8.m1.1.1.2" xref="S2.SS1.SSS1.p2.8.m1.1.1.2.cmml">Œª</mi><mtext id="S2.SS1.SSS1.p2.8.m1.1.1.3" xref="S2.SS1.SSS1.p2.8.m1.1.1.3a.cmml">GPT</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.8.m1.1b"><apply id="S2.SS1.SSS1.p2.8.m1.1.1.cmml" xref="S2.SS1.SSS1.p2.8.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p2.8.m1.1.1.1.cmml" xref="S2.SS1.SSS1.p2.8.m1.1.1">subscript</csymbol><ci id="S2.SS1.SSS1.p2.8.m1.1.1.2.cmml" xref="S2.SS1.SSS1.p2.8.m1.1.1.2">ùúÜ</ci><ci id="S2.SS1.SSS1.p2.8.m1.1.1.3a.cmml" xref="S2.SS1.SSS1.p2.8.m1.1.1.3"><mtext mathsize="70%" id="S2.SS1.SSS1.p2.8.m1.1.1.3.cmml" xref="S2.SS1.SSS1.p2.8.m1.1.1.3">GPT</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.8.m1.1c">\lambda_{\text{GPT}}</annotation></semantics></math> is set to 0. Experiments have confirmed the importance of this improvement.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Hallucination penalty</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.2" class="ltx_p">During the decoding process, instances where the interruption occurs due to the number of tokens surpassing the predetermined upper limit often indicate potential issues such as hallucinations or excessively lengthy audio segments. Such occurrences pose a significant risk of diminishing the credibility of the resultant transcript. To address this concern, we introduce a penalty mechanism aimed at mitigating the adverse effects of excessively long transcripts. Specifically, when this situation occurs, we impose the following penalty on the sum of token log probabilities (SLP):</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.2" class="ltx_Math" alttext="\mathrm{SLP}:=\mathrm{SLP}-N\cdot\log(2)" display="block"><semantics id="S2.E2.m1.2a"><mrow id="S2.E2.m1.2.3" xref="S2.E2.m1.2.3.cmml"><mi id="S2.E2.m1.2.3.2" xref="S2.E2.m1.2.3.2.cmml">SLP</mi><mo lspace="0.278em" rspace="0.278em" id="S2.E2.m1.2.3.1" xref="S2.E2.m1.2.3.1.cmml">:=</mo><mrow id="S2.E2.m1.2.3.3" xref="S2.E2.m1.2.3.3.cmml"><mi id="S2.E2.m1.2.3.3.2" xref="S2.E2.m1.2.3.3.2.cmml">SLP</mi><mo id="S2.E2.m1.2.3.3.1" xref="S2.E2.m1.2.3.3.1.cmml">‚àí</mo><mrow id="S2.E2.m1.2.3.3.3" xref="S2.E2.m1.2.3.3.3.cmml"><mi id="S2.E2.m1.2.3.3.3.2" xref="S2.E2.m1.2.3.3.3.2.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S2.E2.m1.2.3.3.3.1" xref="S2.E2.m1.2.3.3.3.1.cmml">‚ãÖ</mo><mrow id="S2.E2.m1.2.3.3.3.3.2" xref="S2.E2.m1.2.3.3.3.3.1.cmml"><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">log</mi><mo id="S2.E2.m1.2.3.3.3.3.2a" xref="S2.E2.m1.2.3.3.3.3.1.cmml">‚Å°</mo><mrow id="S2.E2.m1.2.3.3.3.3.2.1" xref="S2.E2.m1.2.3.3.3.3.1.cmml"><mo stretchy="false" id="S2.E2.m1.2.3.3.3.3.2.1.1" xref="S2.E2.m1.2.3.3.3.3.1.cmml">(</mo><mn id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">2</mn><mo stretchy="false" id="S2.E2.m1.2.3.3.3.3.2.1.2" xref="S2.E2.m1.2.3.3.3.3.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.2b"><apply id="S2.E2.m1.2.3.cmml" xref="S2.E2.m1.2.3"><csymbol cd="latexml" id="S2.E2.m1.2.3.1.cmml" xref="S2.E2.m1.2.3.1">assign</csymbol><ci id="S2.E2.m1.2.3.2.cmml" xref="S2.E2.m1.2.3.2">SLP</ci><apply id="S2.E2.m1.2.3.3.cmml" xref="S2.E2.m1.2.3.3"><minus id="S2.E2.m1.2.3.3.1.cmml" xref="S2.E2.m1.2.3.3.1"></minus><ci id="S2.E2.m1.2.3.3.2.cmml" xref="S2.E2.m1.2.3.3.2">SLP</ci><apply id="S2.E2.m1.2.3.3.3.cmml" xref="S2.E2.m1.2.3.3.3"><ci id="S2.E2.m1.2.3.3.3.1.cmml" xref="S2.E2.m1.2.3.3.3.1">‚ãÖ</ci><ci id="S2.E2.m1.2.3.3.3.2.cmml" xref="S2.E2.m1.2.3.3.3.2">ùëÅ</ci><apply id="S2.E2.m1.2.3.3.3.3.1.cmml" xref="S2.E2.m1.2.3.3.3.3.2"><log id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1"></log><cn type="integer" id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.2c">\mathrm{SLP}:=\mathrm{SLP}-N\cdot\log(2)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">where <math id="S2.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.SSS2.p1.1.m1.1a"><mi id="S2.SS1.SSS2.p1.1.m1.1.1" xref="S2.SS1.SSS2.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.1.m1.1b"><ci id="S2.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1">ùëÅ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.1.m1.1c">N</annotation></semantics></math> denotes the number of decoding tokens. SLP is typically used to divide the number of decoding tokens to get the average log probability (ALP) as the criterion for final selection in beam search. The penalty is intended to approximate the halving of the probabilities of all tokens when decoding is interrupted due to the token limit being exceeded.</p>
</div>
<div id="S2.SS1.SSS2.p2" class="ltx_para">
<p id="S2.SS1.SSS2.p2.4" class="ltx_p">On the other hand, hallucinations are often presented in the form of sentence repetition. Therefore, for all beam search candidates, we identify token strings in the sequence that exhibit cyclic behavior. The maximum length of the cyclic substring is denoted as <math id="S2.SS1.SSS2.p2.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S2.SS1.SSS2.p2.1.m1.1a"><mi id="S2.SS1.SSS2.p2.1.m1.1.1" xref="S2.SS1.SSS2.p2.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.1.m1.1b"><ci id="S2.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p2.1.m1.1.1">ùêø</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.1.m1.1c">L</annotation></semantics></math>, and the number of repetitions is denoted as <math id="S2.SS1.SSS2.p2.2.m2.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S2.SS1.SSS2.p2.2.m2.1a"><mi id="S2.SS1.SSS2.p2.2.m2.1.1" xref="S2.SS1.SSS2.p2.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.2.m2.1b"><ci id="S2.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S2.SS1.SSS2.p2.2.m2.1.1">ùê∂</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.2.m2.1c">C</annotation></semantics></math>. For instance, in the sequence <em id="S2.SS1.SSS2.p2.4.1" class="ltx_emph ltx_font_italic">ABCDABCD</em>, <math id="S2.SS1.SSS2.p2.3.m3.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S2.SS1.SSS2.p2.3.m3.1a"><mi id="S2.SS1.SSS2.p2.3.m3.1.1" xref="S2.SS1.SSS2.p2.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.3.m3.1b"><ci id="S2.SS1.SSS2.p2.3.m3.1.1.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1">ùêø</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.3.m3.1c">L</annotation></semantics></math> is 4 and <math id="S2.SS1.SSS2.p2.4.m4.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S2.SS1.SSS2.p2.4.m4.1a"><mi id="S2.SS1.SSS2.p2.4.m4.1.1" xref="S2.SS1.SSS2.p2.4.m4.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.4.m4.1b"><ci id="S2.SS1.SSS2.p2.4.m4.1.1.cmml" xref="S2.SS1.SSS2.p2.4.m4.1.1">ùê∂</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.4.m4.1c">C</annotation></semantics></math> is 1. Based on this, we introduce an additional penalty mechanism for SLP:</p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.2" class="ltx_Math" alttext="\mathrm{SLP}:=\mathrm{SLP}-L\cdot C\cdot\log(2)" display="block"><semantics id="S2.E3.m1.2a"><mrow id="S2.E3.m1.2.3" xref="S2.E3.m1.2.3.cmml"><mi id="S2.E3.m1.2.3.2" xref="S2.E3.m1.2.3.2.cmml">SLP</mi><mo lspace="0.278em" rspace="0.278em" id="S2.E3.m1.2.3.1" xref="S2.E3.m1.2.3.1.cmml">:=</mo><mrow id="S2.E3.m1.2.3.3" xref="S2.E3.m1.2.3.3.cmml"><mi id="S2.E3.m1.2.3.3.2" xref="S2.E3.m1.2.3.3.2.cmml">SLP</mi><mo id="S2.E3.m1.2.3.3.1" xref="S2.E3.m1.2.3.3.1.cmml">‚àí</mo><mrow id="S2.E3.m1.2.3.3.3" xref="S2.E3.m1.2.3.3.3.cmml"><mi id="S2.E3.m1.2.3.3.3.2" xref="S2.E3.m1.2.3.3.3.2.cmml">L</mi><mo lspace="0.222em" rspace="0.222em" id="S2.E3.m1.2.3.3.3.1" xref="S2.E3.m1.2.3.3.3.1.cmml">‚ãÖ</mo><mi id="S2.E3.m1.2.3.3.3.3" xref="S2.E3.m1.2.3.3.3.3.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S2.E3.m1.2.3.3.3.1a" xref="S2.E3.m1.2.3.3.3.1.cmml">‚ãÖ</mo><mrow id="S2.E3.m1.2.3.3.3.4.2" xref="S2.E3.m1.2.3.3.3.4.1.cmml"><mi id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml">log</mi><mo id="S2.E3.m1.2.3.3.3.4.2a" xref="S2.E3.m1.2.3.3.3.4.1.cmml">‚Å°</mo><mrow id="S2.E3.m1.2.3.3.3.4.2.1" xref="S2.E3.m1.2.3.3.3.4.1.cmml"><mo stretchy="false" id="S2.E3.m1.2.3.3.3.4.2.1.1" xref="S2.E3.m1.2.3.3.3.4.1.cmml">(</mo><mn id="S2.E3.m1.2.2" xref="S2.E3.m1.2.2.cmml">2</mn><mo stretchy="false" id="S2.E3.m1.2.3.3.3.4.2.1.2" xref="S2.E3.m1.2.3.3.3.4.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.2b"><apply id="S2.E3.m1.2.3.cmml" xref="S2.E3.m1.2.3"><csymbol cd="latexml" id="S2.E3.m1.2.3.1.cmml" xref="S2.E3.m1.2.3.1">assign</csymbol><ci id="S2.E3.m1.2.3.2.cmml" xref="S2.E3.m1.2.3.2">SLP</ci><apply id="S2.E3.m1.2.3.3.cmml" xref="S2.E3.m1.2.3.3"><minus id="S2.E3.m1.2.3.3.1.cmml" xref="S2.E3.m1.2.3.3.1"></minus><ci id="S2.E3.m1.2.3.3.2.cmml" xref="S2.E3.m1.2.3.3.2">SLP</ci><apply id="S2.E3.m1.2.3.3.3.cmml" xref="S2.E3.m1.2.3.3.3"><ci id="S2.E3.m1.2.3.3.3.1.cmml" xref="S2.E3.m1.2.3.3.3.1">‚ãÖ</ci><ci id="S2.E3.m1.2.3.3.3.2.cmml" xref="S2.E3.m1.2.3.3.3.2">ùêø</ci><ci id="S2.E3.m1.2.3.3.3.3.cmml" xref="S2.E3.m1.2.3.3.3.3">ùê∂</ci><apply id="S2.E3.m1.2.3.3.3.4.1.cmml" xref="S2.E3.m1.2.3.3.3.4.2"><log id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1"></log><cn type="integer" id="S2.E3.m1.2.2.cmml" xref="S2.E3.m1.2.2">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.2c">\mathrm{SLP}:=\mathrm{SLP}-L\cdot C\cdot\log(2)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.SSS2.p2.5" class="ltx_p">This penalty is aimed at reducing the probability of additional tokens falling into a cycle, thus biasing the final selection in beam search towards candidates without hallucinations.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Leveraging speech data</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">After completing the decoding process, we recorded the text of each sample and its average token log probability (ALP), and found a certain correlation between the individual sample's WER and ALP. Furthermore, by incorporating GPT into the decoding process, we have a greater opportunity to predict sample decoding quality through ALP, leveraging the rich linguistic information provided by GPT. Therefore, we utilized unlabeled target domain speech data, decoded it with GPT to generate pseudo-labels, and then recorded the ALP. Subsequently, we sorted the samples based on ALP, and selected a subset with higher ALP values for fine-tuning the Whisper model. Through this pseudo-label fine-tuning approach, the Whisper model can better adapt to the target domain, learn from the knowledge of the language model, and improve speech recognition capability in under-represented languages.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Details of leveraging text data</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The text dataset consists of the Kazakh portion from the Leipzig<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://huggingface.co/datasets/kz-transformers/multidomain-kazakh-dataset/blob/main/leipzig.csv</span></span></span> and Uzbek-Kazakh parallel corpora<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://huggingface.co/datasets/Sanatbek/uzbek-kazakh-parallel-corpora</span></span></span>, comprising approximately 1.7 million entries, utilized to train a 1.3B GPT-3 model. The model is initialized with the parameters of mGPT-Kazakh <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, setting the tokenizer to be consistent with Whisper's multilingual model. The model is trained for one epoch using the text dataset, with the optimizer set to AdamW and the scheduler set to WarmupLR. During decoding, we set the beam search size to 5, consistent with the default value of the transcribe interface. In the decoding loop, we use key-value cache to record intermediate key-value pairs generated during the decoding process to accelerate the decoding speed.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">To explore the effect of model size on the results, we employed two Whisper models of different scales: Whisper-base with 74 million parameters and Whisper-large with 1.5 billion parameters. As the former exhibited a WER over 100% on Kazakh, we fine-tuned it using the Kazakh dataset from Fleurs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, resulting in the model called Whisper-base-KF. This fine-tuned model achieved a WER on Kazakh similar to that of the original Whisper-large model. Whisper-base-KF and Whisper-large have GPT weights of 0.3 and 0.1 at decoding, respectively.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Details of leveraging speech data</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">For the unlabeled speech data, approximately 500 hours of the crowdsourced portion (KSC) from the KSC2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> were employed to generate pseudo-labels for Whisper-base-KF, while around 10 hours of the Kazakh training set from Fleurs were utilized to generate pseudo-labels for Whisper-large. During the fine-tuning of Whisper with pseudo-labels, we froze the encoder and only fine-tuned the decoder. We used the cross-entropy loss function and employed the AdamW optimizer. The hyperparameters were set as follows: epoch: 5, batch size: 16, learning rate: 0.0001, and weight decay: 0.01.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Leveraging text data</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">After training the GPT model using text data, we conducted tests to measure its perplexity (ppl) on the Fleurs and KSC test sets. The perplexity results for the Fleurs and KSC test sets were 2.61 and 6.20, respectively. Furthermore, the trained GPT model was integrated into Whisper, and the decoding results of the Whisper-base-KF and whisper-large models on the Fleurs and KSC test sets are shown in Table <a href="#S4.T1" title="Table 1 ‚Ä£ 4.1 Leveraging text data ‚Ä£ 4 Results ‚Ä£ Improving Whisper's Recognition Performance for Under-Represented Language Kazakh Leveraging Unpaired Speech and Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Summary of Word Error Rate (WER) of the two models with/without GPT in Fleurs and KSC test sets.</figcaption>
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:261.3pt;height:85.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.9pt,2.3pt) scale(0.95,0.95) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Fleurs WER(%)</span></th>
<th id="S4.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">KSC WER(%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Whisper-base-KF</th>
<td id="S4.T1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">37.31</td>
<td id="S4.T1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">61.51</td>
</tr>
<tr id="S4.T1.1.1.3.2" class="ltx_tr">
<th id="S4.T1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">¬†¬†‚ÄÉ+GPT for decoding</th>
<td id="S4.T1.1.1.3.2.2" class="ltx_td ltx_align_center">28.60</td>
<td id="S4.T1.1.1.3.2.3" class="ltx_td ltx_align_center">50.53</td>
</tr>
<tr id="S4.T1.1.1.4.3" class="ltx_tr">
<th id="S4.T1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Whisper-large</th>
<td id="S4.T1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">43.58</td>
<td id="S4.T1.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">56.18</td>
</tr>
<tr id="S4.T1.1.1.5.4" class="ltx_tr">
<th id="S4.T1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">¬†¬†‚ÄÉ+GPT for decoding</th>
<td id="S4.T1.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">36.64</td>
<td id="S4.T1.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">49.24</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">It can be seen that integrating a well-trained GPT for the Kazakh language into the decoding process of Whisper, along with the utilization of EOT modification and hallucination penalty, can significantly improve the performance of both Whisper models. The gain brought by GPT for the speech recognition results is dependent on the scale of the Whisper model. For the relatively smaller Whisper model, Whisper-base-KF, the combination with the 1.3B GPT yields greater benefits, achieving a relative WER reduction of 23.3% and 18.9% on the Fleurs and KSC test sets, respectively. For the larger Whisper model, whisper-large, the combination with GPT results in a relative WER reduction of 15.9% and 12.4% on the Fleurs and KSC test sets, respectively.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Impact of the modifications on the results</h4>

<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results of models decoding with GPT, EOT Judgment Modification (EOT-JM), and Hallucination Penalty (HP) on Fleurs-test.</figcaption>
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:246.5pt;height:153.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.5pt,4.1pt) scale(0.95,0.95) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.2.1" class="ltx_text ltx_font_bold">GPT</span></th>
<th id="S4.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.3.1" class="ltx_text ltx_font_bold">EOT-JM</span></th>
<th id="S4.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.4.1" class="ltx_text ltx_font_bold">HP</span></th>
<th id="S4.T2.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.5.1" class="ltx_text ltx_font_bold">WER(%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.2.1" class="ltx_tr">
<th id="S4.T2.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="S4.T2.1.1.2.1.1.1" class="ltx_text">Whisper-base-KF</span></th>
<td id="S4.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">‚úó</td>
<td id="S4.T2.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">‚úó</td>
<td id="S4.T2.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">‚úó</td>
<td id="S4.T2.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">37.31</td>
</tr>
<tr id="S4.T2.1.1.3.2" class="ltx_tr">
<td id="S4.T2.1.1.3.2.1" class="ltx_td ltx_align_center">‚úì</td>
<td id="S4.T2.1.1.3.2.2" class="ltx_td ltx_align_center">‚úó</td>
<td id="S4.T2.1.1.3.2.3" class="ltx_td ltx_align_center">‚úó</td>
<td id="S4.T2.1.1.3.2.4" class="ltx_td ltx_align_center">34.49</td>
</tr>
<tr id="S4.T2.1.1.4.3" class="ltx_tr">
<td id="S4.T2.1.1.4.3.1" class="ltx_td ltx_align_center">‚úì</td>
<td id="S4.T2.1.1.4.3.2" class="ltx_td ltx_align_center">‚úì</td>
<td id="S4.T2.1.1.4.3.3" class="ltx_td ltx_align_center">‚úó</td>
<td id="S4.T2.1.1.4.3.4" class="ltx_td ltx_align_center">28.78</td>
</tr>
<tr id="S4.T2.1.1.5.4" class="ltx_tr">
<td id="S4.T2.1.1.5.4.1" class="ltx_td ltx_align_center">‚úì</td>
<td id="S4.T2.1.1.5.4.2" class="ltx_td ltx_align_center">‚úì</td>
<td id="S4.T2.1.1.5.4.3" class="ltx_td ltx_align_center">‚úì</td>
<td id="S4.T2.1.1.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.5.4.4.1" class="ltx_text ltx_font_bold">28.60</span></td>
</tr>
<tr id="S4.T2.1.1.6.5" class="ltx_tr">
<th id="S4.T2.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="4"><span id="S4.T2.1.1.6.5.1.1" class="ltx_text">Whisper-large</span></th>
<td id="S4.T2.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_t">‚úó</td>
<td id="S4.T2.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t">‚úó</td>
<td id="S4.T2.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t">‚úó</td>
<td id="S4.T2.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_t">43.58</td>
</tr>
<tr id="S4.T2.1.1.7.6" class="ltx_tr">
<td id="S4.T2.1.1.7.6.1" class="ltx_td ltx_align_center">‚úì</td>
<td id="S4.T2.1.1.7.6.2" class="ltx_td ltx_align_center">‚úó</td>
<td id="S4.T2.1.1.7.6.3" class="ltx_td ltx_align_center">‚úó</td>
<td id="S4.T2.1.1.7.6.4" class="ltx_td ltx_align_center">36.75</td>
</tr>
<tr id="S4.T2.1.1.8.7" class="ltx_tr">
<td id="S4.T2.1.1.8.7.1" class="ltx_td ltx_align_center">‚úì</td>
<td id="S4.T2.1.1.8.7.2" class="ltx_td ltx_align_center">‚úì</td>
<td id="S4.T2.1.1.8.7.3" class="ltx_td ltx_align_center">‚úó</td>
<td id="S4.T2.1.1.8.7.4" class="ltx_td ltx_align_center">36.68</td>
</tr>
<tr id="S4.T2.1.1.9.8" class="ltx_tr">
<td id="S4.T2.1.1.9.8.1" class="ltx_td ltx_align_center ltx_border_bb">‚úì</td>
<td id="S4.T2.1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_bb">‚úì</td>
<td id="S4.T2.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_bb">‚úì</td>
<td id="S4.T2.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.1.9.8.4.1" class="ltx_text ltx_font_bold">36.64</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 ‚Ä£ 4.1.1 Impact of the modifications on the results ‚Ä£ 4.1 Leveraging text data ‚Ä£ 4 Results ‚Ä£ Improving Whisper's Recognition Performance for Under-Represented Language Kazakh Leveraging Unpaired Speech and Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the Word Error Rate (WER) of two models with each improvement step on Fleurs-test. For the EOT Judgment Modification (EOT-JM) and Hallucination Penalty (HP), the smaller-scale Whisper-base-KF model with a higher language model weight exhibits a greater decrease in WER, particularly for the former improvement. This suggests that smaller-scale models rely more on larger language models during decoding, even for judging endings. EOT-JM ensures that the ending of transcription relies on audio information, reducing the generation of nonexistent information in the audio by GPT, resulting in a significant reduction in WER. The overall effect of HP on WER is not significant because HP affects only the candidate options and average token log probability (ALP) in the final beam search after decoding all tokens. However, for high-priority samples with higher ALP, HP can have a substantial impact.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Impact of hallucination penalty on high-priority data</h4>

<figure id="S4.F2" class="ltx_figure"><img src="/html/2408.05554/assets/figures/subplot_ablation.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="459" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Decoded sample distribution of models on Fleurs-test. The X-axis represents the negative average log probability of the sample's tokens (-ALP), and the Y-axis represents the Word Error Rate (WER) for each sample. The red dashed line separates the samples into two halves based on -ALP.</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Summary of WER for the Fleurs-test subset with a high average token log probability (ALP). The values highlighted in red represent the difference compared to the case without GPT.</figcaption>
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:253.5pt;height:205.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-17.3pt,14.0pt) scale(0.88,0.88) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Samples WER(%)</span></td>
<td id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.1.1.2.1" class="ltx_text ltx_font_bold">All</span></td>
<td id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Top 20% ALP</span></td>
<td id="S4.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Top 50% ALP</span></td>
</tr>
<tr id="S4.T3.1.1.2.2" class="ltx_tr">
<td id="S4.T3.1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_tt" colspan="4"><span id="S4.T3.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Whisper-base-KF:</span></td>
</tr>
<tr id="S4.T3.1.1.3.3" class="ltx_tr">
<td id="S4.T3.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t">w/o GPT</td>
<td id="S4.T3.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">37.31</td>
<td id="S4.T3.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">22.17</td>
<td id="S4.T3.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">29.32</td>
</tr>
<tr id="S4.T3.1.1.4.4" class="ltx_tr">
<td id="S4.T3.1.1.4.4.1" class="ltx_td ltx_align_center" rowspan="2"><span id="S4.T3.1.1.4.4.1.1" class="ltx_text">w/o HP</span></td>
<td id="S4.T3.1.1.4.4.2" class="ltx_td ltx_align_center">28.78</td>
<td id="S4.T3.1.1.4.4.3" class="ltx_td ltx_align_center">20.34</td>
<td id="S4.T3.1.1.4.4.4" class="ltx_td ltx_align_center">21.18</td>
</tr>
<tr id="S4.T3.1.1.5.5" class="ltx_tr">
<td id="S4.T3.1.1.5.5.1" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.5.5.1.1" class="ltx_text" style="color:#FF0000;">(-8.53)</span></td>
<td id="S4.T3.1.1.5.5.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.5.5.2.1" class="ltx_text" style="color:#FF0000;">(-1.73)</span></td>
<td id="S4.T3.1.1.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.5.5.3.1" class="ltx_text" style="color:#FF0000;">(-8.14)</span></td>
</tr>
<tr id="S4.T3.1.1.6.6" class="ltx_tr">
<td id="S4.T3.1.1.6.6.1" class="ltx_td ltx_align_center" rowspan="2"><span id="S4.T3.1.1.6.6.1.1" class="ltx_text">with HP</span></td>
<td id="S4.T3.1.1.6.6.2" class="ltx_td ltx_align_center">28.60</td>
<td id="S4.T3.1.1.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.6.6.3.1" class="ltx_text ltx_font_bold">12.42</span></td>
<td id="S4.T3.1.1.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.6.6.4.1" class="ltx_text ltx_font_bold">18.07</span></td>
</tr>
<tr id="S4.T3.1.1.7.7" class="ltx_tr">
<td id="S4.T3.1.1.7.7.1" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.7.7.1.1" class="ltx_text" style="color:#FF0000;">(-8.71)</span></td>
<td id="S4.T3.1.1.7.7.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.7.7.2.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">(-9.75)</span></td>
<td id="S4.T3.1.1.7.7.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.7.7.3.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">(-11.25)</span></td>
</tr>
<tr id="S4.T3.1.1.8.8" class="ltx_tr">
<td id="S4.T3.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_tt" colspan="4"><span id="S4.T3.1.1.8.8.1.1" class="ltx_text ltx_font_bold">Whisper-large:</span></td>
</tr>
<tr id="S4.T3.1.1.9.9" class="ltx_tr">
<td id="S4.T3.1.1.9.9.1" class="ltx_td ltx_align_center ltx_border_t">w/o GPT</td>
<td id="S4.T3.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_t">43.58</td>
<td id="S4.T3.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_t">34.55</td>
<td id="S4.T3.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_t">38.14</td>
</tr>
<tr id="S4.T3.1.1.10.10" class="ltx_tr">
<td id="S4.T3.1.1.10.10.1" class="ltx_td ltx_align_center" rowspan="2"><span id="S4.T3.1.1.10.10.1.1" class="ltx_text">w/o HP</span></td>
<td id="S4.T3.1.1.10.10.2" class="ltx_td ltx_align_center">36.68</td>
<td id="S4.T3.1.1.10.10.3" class="ltx_td ltx_align_center">28.96</td>
<td id="S4.T3.1.1.10.10.4" class="ltx_td ltx_align_center">30.94</td>
</tr>
<tr id="S4.T3.1.1.11.11" class="ltx_tr">
<td id="S4.T3.1.1.11.11.1" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.11.11.1.1" class="ltx_text" style="color:#FF0000;">(-6.90)</span></td>
<td id="S4.T3.1.1.11.11.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.11.11.2.1" class="ltx_text" style="color:#FF0000;">(-5.59)</span></td>
<td id="S4.T3.1.1.11.11.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.11.11.3.1" class="ltx_text" style="color:#FF0000;">(-7.20)</span></td>
</tr>
<tr id="S4.T3.1.1.12.12" class="ltx_tr">
<td id="S4.T3.1.1.12.12.1" class="ltx_td ltx_align_center ltx_border_bb" rowspan="2"><span id="S4.T3.1.1.12.12.1.1" class="ltx_text">with HP</span></td>
<td id="S4.T3.1.1.12.12.2" class="ltx_td ltx_align_center">36.64</td>
<td id="S4.T3.1.1.12.12.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.12.12.3.1" class="ltx_text ltx_font_bold">26.83</span></td>
<td id="S4.T3.1.1.12.12.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.12.12.4.1" class="ltx_text ltx_font_bold">30.39</span></td>
</tr>
<tr id="S4.T3.1.1.13.13" class="ltx_tr">
<td id="S4.T3.1.1.13.13.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.1.1.13.13.1.1" class="ltx_text" style="color:#FF0000;">(-6.94)</span></td>
<td id="S4.T3.1.1.13.13.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.1.1.13.13.2.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">(-7.72)</span></td>
<td id="S4.T3.1.1.13.13.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.1.1.13.13.3.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">(-7.75)</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">The system is able to calculate the average token log probability (ALP) for each sample during decoding, and ALP values are usually statistically correlated with the WER of the sample, as shown in Figure <a href="#S4.F2" title="Figure 2 ‚Ä£ 4.1.2 Impact of hallucination penalty on high-priority data ‚Ä£ 4.1 Leveraging text data ‚Ä£ 4 Results ‚Ä£ Improving Whisper's Recognition Performance for Under-Represented Language Kazakh Leveraging Unpaired Speech and Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. However, when combined with GPT during decoding, there are some ``outliers" in the left half, which corresponds to higher ALP values, indicating that these samples have a significantly higher WER. Upon examination, we found that these samples were trapped in hallucination, where a portion of the content was repeated incorrectly multiple times. However, after applying Hallucination Penalty (HP), the phenomenon of ``outliers" is significantly mitigated. Table <a href="#S4.T3" title="Table 3 ‚Ä£ 4.1.2 Impact of hallucination penalty on high-priority data ‚Ä£ 4.1 Leveraging text data ‚Ä£ 4 Results ‚Ä£ Improving Whisper's Recognition Performance for Under-Represented Language Kazakh Leveraging Unpaired Speech and Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> provides a summary of the WER on the high ALP test subset, where high ALP samples are given higher priority in speech pseudo-label training. It can be seen that compared to not using GPT, selecting the high ALP subset results in a significant additional decrease in WER compared to selecting all samples, and HP plays a significant role in this improvement.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Leveraging speech data</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We used two data scales of unlabeled speech data for two models, which were first decoded with GPT to generate pseudo-labels and then used for model fine-tuning. Figure <a href="#S4.F3" title="Figure 3 ‚Ä£ 4.2 Leveraging speech data ‚Ä£ 4 Results ‚Ä£ Improving Whisper's Recognition Performance for Under-Represented Language Kazakh Leveraging Unpaired Speech and Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> depicts the relationship between the proportion of data selected based on average token log probability and the corresponding WER of the corresponding domain test set. The left graph shows the results of fine-tuning on whisper-large with approximately 10 hours of FLEURS-train pseudo-labels. The right graph illustrates the results of fine-tuning on Whisper-base-KF with approximately 500 hours of KSC-train pseudo-labels. The results are presented with and without incorporating GPT decoding. Additionally, we conducted fine-tuning with manually annotated labels under the same configuration for comparative experiments to explore the difference between pseudo-labels and manually annotated labels.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2408.05554/assets/figures/relation.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="231" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Relationship between the proportion of data selected based on the average token log probability and the WER of the corresponding domain test set.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">From Figure <a href="#S4.F3" title="Figure 3 ‚Ä£ 4.2 Leveraging speech data ‚Ä£ 4 Results ‚Ä£ Improving Whisper's Recognition Performance for Under-Represented Language Kazakh Leveraging Unpaired Speech and Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we observe that when performing fine-tuning with a smaller amount of data using pseudo-labels, there is a certain gain in WER reduction. However, as the selection proportion exceeds 0.5, fluctuations occur, and the WER no longer decreases significantly. Nonetheless, incorporating GPT decoding still provides some improvement. On the other hand, when fine-tuning with a larger amount of high-frequency data using pseudo-labels, there is a clear decreasing-then-increasing trend in WER after fine-tuning with varying proportions of data. This trend indicates the effectiveness of using ALP as a criterion for data selection. Furthermore, due to a sufficiently large amount of data, this pseudo-label fine-tuning method enables the model to learn language model information, leading to almost no additional improvement when decoding with GPT. This approach allows for the integration of language model knowledge into Whisper, eliminating the need for external language models during decoding and accelerating the decoding speed.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Summary of the overall WER for systems leveraging unpaired speech and text data.</figcaption>
<div id="S4.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:261.3pt;height:118.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.7pt,13.0pt) scale(0.82,0.82) ;">
<table id="S4.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Systems</span></th>
<td id="S4.T4.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Whisper-large</span></td>
<td id="S4.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T4.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Whisper-base-KF</span></td>
</tr>
<tr id="S4.T4.1.1.2.2" class="ltx_tr">
<td id="S4.T4.1.1.2.2.1" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Fleurs-test WER</span></td>
<td id="S4.T4.1.1.2.2.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.2.2.2.1" class="ltx_text ltx_font_bold">KSC-test WER</span></td>
</tr>
<tr id="S4.T4.1.1.3.3" class="ltx_tr">
<th id="S4.T4.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">1.Origin (baseline)</th>
<td id="S4.T4.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">43.58%</td>
<td id="S4.T4.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">61.51%</td>
</tr>
<tr id="S4.T4.1.1.4.4" class="ltx_tr">
<th id="S4.T4.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">2.(1)+GPT for decoding</th>
<td id="S4.T4.1.1.4.4.2" class="ltx_td ltx_align_center">36.64%</td>
<td id="S4.T4.1.1.4.4.3" class="ltx_td ltx_align_center">50.53%</td>
</tr>
<tr id="S4.T4.1.1.5.5" class="ltx_tr">
<th id="S4.T4.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">3.Pseudo-label fine-tuning</th>
<td id="S4.T4.1.1.5.5.2" class="ltx_td ltx_align_center">35.79%</td>
<td id="S4.T4.1.1.5.5.3" class="ltx_td ltx_align_center">48.66%</td>
</tr>
<tr id="S4.T4.1.1.6.6" class="ltx_tr">
<th id="S4.T4.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">4.(3)+GPT for decoding</th>
<td id="S4.T4.1.1.6.6.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.6.6.2.1" class="ltx_text ltx_font_bold">32.36%</span></td>
<td id="S4.T4.1.1.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.1.6.6.3.1" class="ltx_text ltx_font_bold">48.23%</span></td>
</tr>
<tr id="S4.T4.1.1.7.7" class="ltx_tr">
<th id="S4.T4.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">5.Reference label fine-tuning</th>
<td id="S4.T4.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t">23.24%</td>
<td id="S4.T4.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t">25.26%</td>
</tr>
<tr id="S4.T4.1.1.8.8" class="ltx_tr">
<th id="S4.T4.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Gap-filling Ratio</th>
<td id="S4.T4.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_bb">0.552</td>
<td id="S4.T4.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_bb">0.366</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Table <a href="#S4.T4" title="Table 4 ‚Ä£ 4.2 Leveraging speech data ‚Ä£ 4 Results ‚Ä£ Improving Whisper's Recognition Performance for Under-Represented Language Kazakh Leveraging Unpaired Speech and Text" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> summarizes the WER of systems leveraging unpaired speech and text data. The selection of system (3) corresponds to the best result among all data selection proportions. Reference label fine-tuning is performed using the same amount of data as in system (3). The Gap-filling Ratio is the ratio of the reduction in difference between system (4) and system (5) compared to system (1), which is calculated as (WER.(1)-WER.(4))/(WER.(1)-WER.(5)). In Whisper-large, by leveraging text data and unlabeled Fleurs-train speech data, we achieved an absolute WER reduction of 11.24% on the in-domain test set. This method can achieve more than half the efficacy of reference labels, without incurring the associated human labor costs. Similarly, in Whisper-base-KF, by utilizing text data and unlabeled KSC-train speech data, we observed an absolute WER reduction of 13.28% on the in-domain test set. Even at higher scales of data, more than one-third of the performance of the reference label can be achieved using this pipeline, resulting in a significant reduction in WER for Whisper on Kazakh.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we explore how to leverage low-cost unpaired speech and text data to improve the performance of the multilingual speech recognition model Whisper on the under-represented language Kazakh. By integrating the language model GPT into Whisper's decoding process and implementing EOT judgment modification and hallucination penalty, we significantly reduce WER, particularly for samples with higher decoding average token log probability. Furthermore, we utilize this criterion to select samples for model pseudo-label fine-tuning, further improving performance. The whole process is foundational but effective in bringing low-resource languages into the wave of large speech models, and it is entirely possible to generalize it to other under-represented languages, with the potential to combine it with more novel techniques.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J.¬†Li, ``Recent advances in end-to-end automatic speech recognition,'' <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">APSIPA Transactions on Signal and Information Processing</em>, vol.¬†11, no.¬†1, pp. e8: 1‚Äì27, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A.¬†Radford, J.¬†W. Kim, T.¬†Xu, G.¬†Brockman, C.¬†McLeavey, and I.¬†Sutskever, ``Robust speech recognition via large-scale weak supervision,'' in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proc. International Conference on Machine Learning (ICML)</em>.¬†¬†¬†PMLR, 2023, pp. 28‚Äâ492‚Äì28‚Äâ518.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Y.¬†Zhang, W.¬†Han, J.¬†Qin, Y.¬†Wang, A.¬†Bapna, Z.¬†Chen, N.¬†Chen, B.¬†Li, V.¬†Axelrod, G.¬†Wang <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ``Google USM: Scaling automatic speech recognition beyond 100 languages,'' <em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.01037</em>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Z.¬†Zhao and W.-Q. Zhang, ``End-to-end keyword search system based on attention mechanism and energy scorer for low resource languages,'' <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Neural Networks</em>, vol. 139, pp. 326‚Äì334, 7 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J.¬†Kunze, L.¬†Kirsch, I.¬†Kurenkov, A.¬†Krug, J.¬†Johannsmeier, and S.¬†Stober, ``Transfer learning for speech recognition on a budget,'' in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proc. Workshop on Representation Learning for NLP (RepL4NLP)</em>, 2017, pp. 168‚Äì177.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
B.¬†Li, R.¬†Pang, T.¬†N. Sainath, A.¬†Gulati, Y.¬†Zhang, J.¬†Qin, P.¬†Haghani, W.¬†R. Huang, M.¬†Ma, and J.¬†Bai, ``Scaling end-to-end models for large-scale multilingual ASR,'' in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.¬†¬†¬†IEEE, 2021, pp. 1011‚Äì1018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S.¬†Dalmia, R.¬†Sanabria, F.¬†Metze, and A.¬†W. Black, ``Sequence-based multi-lingual low resource speech recognition,'' in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.¬†¬†¬†IEEE, 2018, pp. 4909‚Äì4913.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y.¬†Qian and Z.¬†Zhou, ``Optimizing data usage for low-resource speech recognition,'' <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.¬†30, pp. 394‚Äì403, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J.¬†Devlin, M.-W. Chang, K.¬†Lee, and K.¬†Toutanova, ``BERT: Pre-training of deep bidirectional transformers for language understanding,'' in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proc. Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT)</em>.¬†¬†¬†ACL, 2019, pp. 4171‚Äì4186.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A.¬†T. Liu, S.-w. Yang, P.-H. Chi, P.-c. Hsu, and H.-y. Lee, ``Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders,'' in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.¬†¬†¬†IEEE, 2020, pp. 6419‚Äì6423.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A.¬†T. Liu, S.-W. Li, and H.-y. Lee, ``Tera: Self-supervised learning of transformer encoder representation for speech,'' <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.¬†29, pp. 2351‚Äì2366, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A.¬†Conneau, A.¬†Baevski, R.¬†Collobert, A.¬†Mohamed, and M.¬†Auli, ``Unsupervised cross-lingual representation learning for speech recognition,'' in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>.¬†¬†¬†ISCA, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
W.-N. Hsu, B.¬†Bolte, Y.-H.¬†H. Tsai, K.¬†Lakhotia, R.¬†Salakhutdinov, and A.¬†Mohamed, ``HuBERT: Self-supervised speech representation learning by masked prediction of hidden units,'' <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.¬†29, pp. 3451‚Äì3460, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J.¬†Zhao and W.-Q. Zhang, ``Improving automatic speech recognition performance for low-resource languages with self-supervised models,'' <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, vol.¬†16, pp. 1227‚Äì1241, 10 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Q.¬†Xu, T.¬†Likhomanenko, J.¬†Kahn, A.¬†Hannun, G.¬†Synnaeve, and R.¬†Collobert, ``Iterative pseudo-labeling for speech recognition,'' in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>.¬†¬†¬†ISCA, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
D.¬†S. Park, Y.¬†Zhang, Y.¬†Jia, W.¬†Han, C.-C. Chiu, B.¬†Li, Y.¬†Wu, and Q.¬†V. Le, ``Improved noisy student training for automatic speech recognition,'' in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>.¬†¬†¬†ISCA, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A.¬†Conneau, M.¬†Ma, S.¬†Khanuja, Y.¬†Zhang, V.¬†Axelrod, S.¬†Dalmia, J.¬†Riesa, C.¬†Rivera, and A.¬†Bapna, ``Fleurs: Few-shot learning evaluation of universal representations of speech,'' in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Spoken Language Technology Workshop (SLT)</em>.¬†¬†¬†IEEE, 2023, pp. 798‚Äì805.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S.¬†Mussakhojayeva, Y.¬†Khassanov, and H.¬†A. Varol, ``KSC2: An industrial-scale open-source Kazakh speech corpus,'' in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>.¬†¬†¬†ISCA, 2022, pp. 1367‚Äì1371.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
A.¬†Baevski, W.-N. Hsu, A.¬†Conneau, and M.¬†Auli, ``Unsupervised speech recognition,'' <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol.¬†34, pp. 27‚Äâ826‚Äì27‚Äâ839, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A.¬†H. Liu, W.-N. Hsu, M.¬†Auli, and A.¬†Baevski, ``Towards end-to-end unsupervised speech recognition,'' in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Spoken Language Technology Workshop (SLT)</em>.¬†¬†¬†IEEE, 2023, pp. 221‚Äì228.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
G.¬†Sun, X.¬†Zheng, C.¬†Zhang, and P.¬†C. Woodland, ``Can contextual biasing remain effective with whisper and GPT-2?'' in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>.¬†¬†¬†ISCA, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
O.¬†Shliazhko, A.¬†Fenogenova, M.¬†Tikhonova, A.¬†Kozlova, V.¬†Mikhailov, and T.¬†Shavrina, ``mGPT: Few-shot learners go multilingual,'' <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, vol.¬†12, pp. 58‚Äì79, 2024.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
T.¬†Brown, B.¬†Mann, N.¬†Ryder, M.¬†Subbiah, J.¬†D. Kaplan, P.¬†Dhariwal, A.¬†Neelakantan, P.¬†Shyam, G.¬†Sastry, A.¬†Askell <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">et¬†al.</em>, ``Language models are few-shot learners,'' <em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol.¬†33, pp. 1877‚Äì1901, 2020.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.05552" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.05554" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.05554">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.05554" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.05555" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 12:57:48 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
