<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.09221] Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?</title><meta property="og:description" content="Decoder-only discrete-token language models have recently achieved significant success in automatic speech recognition. However, systematic analyses of how different modalities impact performance in specific scenarios …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.09221">

<!--Generated on Sat Oct  5 21:10:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
automatic speech recognition,  large language model,  multi-modal processing
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Yiwen Guan,
Viet Anh Trinh,
Vivek Voleti, and
Jacob Whitehill
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">Worcester Polytechnic Institute
<br class="ltx_break"></span>yguan2@wpi.edu, vtrinh@wpi.edu, vsvoleti@wpi.edu, jrwhitehill@wpi.edu

</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Decoder-only discrete-token language models have recently achieved significant success in automatic speech recognition. However, systematic analyses of how different modalities impact performance in specific scenarios remain limited. In this paper, we investigate the effects of multiple modalities on recognition accuracy on both synthetic and real-world datasets. Our experiments suggest that: (1) Integrating more modalities can increase accuracy; in particular, our paper is, to our best knowledge, the first to show the benefit of combining audio, image context, and lip information; (2) Images as a supplementary modality for speech recognition provide the greatest benefit at moderate noise levels, moreover, they exhibit a different trend compared to inherently synchronized modalities like lip movements; (3) Performance improves on both synthetic and real-world datasets when the most relevant visual information is filtered as a preprocessing step.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
automatic speech recognition, large language model, multi-modal processing

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The past few years have seen rapidly growing interest in automatic speech recognition (ASR) systems based on decoder-only discrete-token language models (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>). Such models are attractive in part due to their ability to accept multi-modal inputs (e.g., audio, text, images) and generate multi-modal outputs (e.g., text tokens for ASR, audio tokens for speech-to-speech translation, etc.).
The ability to process multiple input streams, such as audio that was spoken, lip movements of the speaker, an image of what was spoken about, etc., can help to make ASR more robust to difficult conditions.
Advantages of these models include training on large-scale audio or audio-visual datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>, along with the
ability to leverage advanced language understanding capabilities from LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> to perform multi-modal tasks. Considerable research on multi-modal large language models (MM-LLMs) has exploited the power of LLMs to understand multi-modal information. Despite extensive exploration in multi-modal areas, studies that systematically investigate how different modalities impact ASR performance are scarce.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this work, we extend the MM-LLM model in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> to support more input modalities (audio, image, lip movements, OCR text). We aim to deepen the understanding of how to take advantage of multiple modalities in specific contexts. To this end, we introduce a synthesized, multi-modal dataset (3-Equations) that is highly controllable, on which we can conveniently simulate various tailored situations. We further experiment on the real-world SlideAVSR dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Audio-visual speech recognition</span>:  Despite the tremendous success of ASR systems over the past decade, quality and reliability issues still exist in acoustically noisy environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>. One strategy to mitigate such noise is to harness complementary visual information, e.g., of the speaker’s lips <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>, <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>.
A state-of-the-art approach is AV-HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>: it learns audio-visual speech representations by feature clustering and masked prediction. By extending noise augmentation to AV-HuBERT pre-training, downstream models attain better robustness to acoustic noise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>. Other works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> explore the performance gap between audio-only and audio-visual models in noisy conditions and show that the gap becomes larger as the noise levels increase. Instead of harnessing visual representations of the lips, some works such as SlideSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> and SlideAVSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> have explored using optical character recognition (OCR) to extract information from the subject matter (lecture slides). To leverage the extracted text, SlideSpeech uses cross-attention to combine speech embeddings and contextual phrase embeddings for contextual ASR; SlideAVSR proposes DocWhisper which provides the texts to Whisper as prompts. Both works demonstrate performance improvement by integrating OCR texts.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Multi-modal large language models</span>:  Recognizing the powerful language generation, zero-shot transfer, and contextual learning capabilities of large language models (LLMs), significant efforts have been made to harness the knowledge from LLM pre-training to empower multi-modal tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>. Research works such as Qwen-Audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite> and LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> have focused on multi-modal comprehension. Meanwhile, the outstanding generative capability has also inspired many works to extend unimodal LLMs to perform multi-modal generation, such as MiniGPT-5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> and NExT-GPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>, leading to the emergence of multi-modal large language models (MM-LLMs). Even so, only a few works incorporate visual speech modeling with LLMs, leaving this a relatively unexplored area. VSP-LLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> integrates LLMs into visual speech processing, enhancing context modeling for tasks like visual speech recognition (VSR) and translation (VST). However, VSP-LLM is primarily concentrated on visual speech processing and does not explore combinations with other modalities.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2409.09221/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="213" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An overview of Discrete Multimodal Language Model (DMLM). The inputs are encoded by modality-specific encoders, and the encoded multi-modal tokens are concatenated to a task prompt, and then passed to DMLM for processing in next-token prediction style.</figcaption>
</figure>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2409.09221/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="61" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An example of the 3-Equations dataset. From left to right: the image sample shows 3 mathematical equations; the OCR texts are extracted with EasyOCR and cleaned to retain only numbers, letters, and operators; the audio contains randomly reading 2 out of the 3 equations aloud; the lip movement video displays a lip region reading the corresponding equation sentences; the ground-truth transcription is the plain-text translation (label) of the speech. In this example, the speech reads the third and the second equations in order.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We carry out experiments to investigate: (1) How do additional modalities help improve ASR accuracy on average? (2) How does the performance of each modality or their combinations vary across noise levels? (3) How does irrelevant visual information affect performance?</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Experimental Settings</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">Model</span>  Our multi-modal speech recognition experiments are based on a Discrete Multi-modal Language Model (DMLM, depicted in Fig. <a href="#S3.F1" title="Figure 1 ‣ III Experiments ‣ Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>, which is a discrete token-based Transformer decoder model using OPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> as the backbone. DMLM tokenizes the input data by modality-specific encoders to form a discrete token sequence. In particular, DMLM uses the Seamless codec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> to convert audio waveforms to discrete speech tokens, and the DALL-E encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite> to convert images to discrete image tokens. In addition, to extend DMLM to accept lip movement input, we employ AV-HuBERT to obtain lip tokens as well as text tokens representing the lip-reading hypothesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>. The input token sequence is concatenated to a task description (e.g. ASR, lip-to-text) and then passed to DMLM for processing.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Model Configurations and Training Details</span>  Following prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>, we employ the OPT model with 125M parameters as the backbone LLM of DMLM. To enable DMLM to solve multi-modal problems, we fine-tune DMLM on a mixture of multi-modal tasks, including speech recognition, speech translation, image generation, and image captioning, using LibriSpeech-960 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>, CVSS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>, CoVoST2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>, and COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>. We use this fine-tuned DMLM as a pre-trained model, and further fine-tune it on desired tasks such as multi-modal ASR. We extend the length-normalized tri-modal loss function proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> to accept more modalities. The model is trained using AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> with <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\beta</annotation></semantics></math>=(0.9, 0.999), weight decay of 1e-4, lr=1e-6, on a single NVIDIA A100 GPU with a batch size of 4 and patience of 5.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.3" class="ltx_p"><span id="S3.SS1.p3.3.1" class="ltx_text ltx_font_bold">Evaluation Metrics</span>  We report the Word Error Rates (WER) for each dataset, using the Whisper English text normalizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite> to clean and standardize the text by removing extraneous characters, normalizing spaces, and converting text to lowercase. We also report a relative WER benefit of adding extra modalities to the audio task, which is calculated by <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="(\textrm{WER}_{A}-\textrm{WER}_{X+A})/\textrm{WER}_{A}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mrow id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mrow id="S3.SS1.p3.1.m1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p3.1.m1.1.1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p3.1.m1.1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.1.cmml"><msub id="S3.SS1.p3.1.m1.1.1.1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.1.1.1.2.cmml"><mtext id="S3.SS1.p3.1.m1.1.1.1.1.1.2.2" xref="S3.SS1.p3.1.m1.1.1.1.1.1.2.2a.cmml">WER</mtext><mi id="S3.SS1.p3.1.m1.1.1.1.1.1.2.3" xref="S3.SS1.p3.1.m1.1.1.1.1.1.2.3.cmml">A</mi></msub><mo id="S3.SS1.p3.1.m1.1.1.1.1.1.1" xref="S3.SS1.p3.1.m1.1.1.1.1.1.1.cmml">−</mo><msub id="S3.SS1.p3.1.m1.1.1.1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3.cmml"><mtext id="S3.SS1.p3.1.m1.1.1.1.1.1.3.2" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3.2a.cmml">WER</mtext><mrow id="S3.SS1.p3.1.m1.1.1.1.1.1.3.3" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3.3.cmml"><mi id="S3.SS1.p3.1.m1.1.1.1.1.1.3.3.2" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3.3.2.cmml">X</mi><mo id="S3.SS1.p3.1.m1.1.1.1.1.1.3.3.1" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3.3.1.cmml">+</mo><mi id="S3.SS1.p3.1.m1.1.1.1.1.1.3.3.3" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3.3.3.cmml">A</mi></mrow></msub></mrow><mo stretchy="false" id="S3.SS1.p3.1.m1.1.1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">/</mo><msub id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml"><mtext id="S3.SS1.p3.1.m1.1.1.3.2" xref="S3.SS1.p3.1.m1.1.1.3.2a.cmml">WER</mtext><mi id="S3.SS1.p3.1.m1.1.1.3.3" xref="S3.SS1.p3.1.m1.1.1.3.3.cmml">A</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><divide id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2"></divide><apply id="S3.SS1.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1"><minus id="S3.SS1.p3.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.1"></minus><apply id="S3.SS1.p3.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.1.1.1.2.2a.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.2.2"><mtext id="S3.SS1.p3.1.m1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.2.2">WER</mtext></ci><ci id="S3.SS1.p3.1.m1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.2.3">𝐴</ci></apply><apply id="S3.SS1.p3.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.1.1.1.3.2a.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3.2"><mtext id="S3.SS1.p3.1.m1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3.2">WER</mtext></ci><apply id="S3.SS1.p3.1.m1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3.3"><plus id="S3.SS1.p3.1.m1.1.1.1.1.1.3.3.1.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3.3.1"></plus><ci id="S3.SS1.p3.1.m1.1.1.1.1.1.3.3.2.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3.3.2">𝑋</ci><ci id="S3.SS1.p3.1.m1.1.1.1.1.1.3.3.3.cmml" xref="S3.SS1.p3.1.m1.1.1.1.1.1.3.3.3">𝐴</ci></apply></apply></apply><apply id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.3.1.cmml" xref="S3.SS1.p3.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.3.2a.cmml" xref="S3.SS1.p3.1.m1.1.1.3.2"><mtext id="S3.SS1.p3.1.m1.1.1.3.2.cmml" xref="S3.SS1.p3.1.m1.1.1.3.2">WER</mtext></ci><ci id="S3.SS1.p3.1.m1.1.1.3.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3">𝐴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">(\textrm{WER}_{A}-\textrm{WER}_{X+A})/\textrm{WER}_{A}</annotation></semantics></math>, where <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mi id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><ci id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">A</annotation></semantics></math> denotes audio and <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mi id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">X</annotation></semantics></math> stands for any additional modalities. This metric indicates better performance with a larger value.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Datasets</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">3-Equations<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1.1.1.1" class="ltx_text ltx_font_medium">1</span></span><span id="footnote1.4" class="ltx_text ltx_font_medium">This dataset will be released for free academic use.</span></span></span></span></span>: To systematically explore the interaction between modalities, we synthesized a multi-modal dataset consisting of images, audio clips, and lip movements. Each example contains multiple randomly generated mathematical equations – reminiscent of the kinds of visual content that appear in lecture slides. The motivation is to design a dataset that forces the model to exploit information across modalities while allowing us to introduce complexity in a controlled manner. In this way, we can study how the model relies on information from each modality separately, especially when some modalities are insufficient or corrupted. An example from this dataset is illustrated in Fig. <a href="#S3.F2" title="Figure 2 ‣ III Experiments ‣ Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. To simulate an insufficient auditory modality scenario, each audio sample only reads out two of the three equations randomly from the image. This setting encourages the model to rely on both visual and auditory modalities: without auditory information, the model cannot find the correct spoken equations; without visual information, the model will fail in a noisy environment.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The dataset consists of 10,000 examples, each containing one image sample with its OCR text, one audio sample, and one lip movement video sample. Specifically, the images depict three mathematical equations, involving operations such as addition, subtraction, logarithms, fractions and exponentiation, with each image sized <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="450\times 200" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mn id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">450</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">200</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><times id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">450</cn><cn type="integer" id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">200</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">450\times 200</annotation></semantics></math>. The audio part contains 25.2 hours of synthesized speech, averaging 9 seconds per sample, with 20.2 hours for training and 2.5 hours each for development and testing. The lip movement videos are generated from the synthesized audio at 25 FPS using a static portrait image. To create the dataset, we employ pyttsx3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite> to generate speech utterances, latex command to produce equation images, EasyOCR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite> to obtain OCR texts, and Wav2Lip <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> to generate lip-synced videos.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">One advantage of creating this dataset is that, we can freely simulate anticipated situations, such as when multi-modal fusion is important. Hence, we added noise from the MUSAN dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite> to the second half of each equation utterance at varying signal-to-noise ratios (SNR). This process produces a dataset we refer to as “2-noise”. The first half of each utterance remains clean, allowing the model to leverage this “incomplete” clean auditory information to locate the correct equation in the image, and subsequently complete the speech transcription with clean visual information.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Another advantage of using this dataset is that, given that our image data consists of mathematical symbols, the visual information could be difficult for standard image codecs to extract. This allows us to study the impact of an imperfect image codec on the ASR accuracy. In particular, if we consider an “oracle OCR” that perfectly transcribes the 3 equation sentences in the image (but does not select which 2 out of the 3 were actually spoken), then the sequence: no vision, image encoding, real OCR, oracle OCR can be considered as an ascending order of image representation quality. Therefore, we can systematically explore how the quality of the visual modality, which is a supplementary modality for the task, affects speech recognition performance.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">Interestingly, we identify a particular failure mode of Gemini when feeding 3-Equations 2-noise data into Gemini-1.5 Flash for transcription. Examples of Gemini’s output are shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ III-D Experiment II: How does the performance of each modality or their combinations vary across noise levels? ‣ III Experiments ‣ Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. It appears to overly rely on a single modality, whereby prone to generate transcriptions for 1 or 3 equations instead of the correct 2 spoken equations. This finding also illustrates how this synthetic dataset can be useful for analyzing even very powerful models.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p"><span id="S3.SS2.p6.1.1" class="ltx_text ltx_font_bold">SlideAVSR</span>: To generalize our findings to the real world, we leverage SlideAVSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>, an audio-visual dataset of paper explanation videos collected from YouTube. The dataset provides manually transcribed speech, synchronized visual slides, and preprocessed OCR keywords. Since the videos contain many AI technical terms, accurate transcription is difficult without referring to the slides, which makes the dataset useful for multi-modal experiments. The dataset comprises 245 hours of audio data, with 195 hours allocated for training, 20 hours for development, and 30 hours for testing.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Experiment I: How much do additional modalities improve ASR accuracy on average?</span>
</h3>

<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Evaluation of WER (%) and benefit of adding image(I), lip hypothesis(L), and different types of OCR(O) on the 3-equations 2-noise test set, at different noise levels based on SNR. +<math id="S3.T1.3.m1.1" class="ltx_Math" alttext="\infty" display="inline"><semantics id="S3.T1.3.m1.1b"><mi mathvariant="normal" id="S3.T1.3.m1.1.1" xref="S3.T1.3.m1.1.1.cmml">∞</mi><annotation-xml encoding="MathML-Content" id="S3.T1.3.m1.1c"><infinity id="S3.T1.3.m1.1.1.cmml" xref="S3.T1.3.m1.1.1"></infinity></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.m1.1d">\infty</annotation></semantics></math> represents clean audio and <math id="S3.T1.4.m2.1" class="ltx_Math" alttext="-\infty" display="inline"><semantics id="S3.T1.4.m2.1b"><mrow id="S3.T1.4.m2.1.1" xref="S3.T1.4.m2.1.1.cmml"><mo id="S3.T1.4.m2.1.1b" xref="S3.T1.4.m2.1.1.cmml">−</mo><mi mathvariant="normal" id="S3.T1.4.m2.1.1.2" xref="S3.T1.4.m2.1.1.2.cmml">∞</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.4.m2.1c"><apply id="S3.T1.4.m2.1.1.cmml" xref="S3.T1.4.m2.1.1"><minus id="S3.T1.4.m2.1.1.1.cmml" xref="S3.T1.4.m2.1.1"></minus><infinity id="S3.T1.4.m2.1.1.2.cmml" xref="S3.T1.4.m2.1.1.2"></infinity></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.m2.1d">-\infty</annotation></semantics></math> means completely noise.</figcaption>
<div id="S3.T1.18.14" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:137.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-210.1pt,66.4pt) scale(0.507866222594179,0.507866222594179) ;">
<table id="S3.T1.18.14.14" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.18.14.14.15.1" class="ltx_tr">
<th id="S3.T1.18.14.14.15.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" colspan="11"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.18.14.14.16.1" class="ltx_tr">
<th id="S3.T1.18.14.14.16.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S3.T1.18.14.14.16.1.2" class="ltx_td ltx_align_center">(clean)</td>
<td id="S3.T1.18.14.14.16.1.3" class="ltx_td ltx_align_center" colspan="7">2-noise, SNR(dB) =</td>
<td id="S3.T1.18.14.14.16.1.4" class="ltx_td ltx_align_center">(noise)</td>
<td id="S3.T1.18.14.14.16.1.5" class="ltx_td ltx_border_l"></td>
</tr>
<tr id="S3.T1.6.2.2.2" class="ltx_tr">
<th id="S3.T1.6.2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Task</th>
<td id="S3.T1.5.1.1.1.1" class="ltx_td ltx_align_center">+<math id="S3.T1.5.1.1.1.1.m1.1" class="ltx_Math" alttext="\infty" display="inline"><semantics id="S3.T1.5.1.1.1.1.m1.1a"><mi mathvariant="normal" id="S3.T1.5.1.1.1.1.m1.1.1" xref="S3.T1.5.1.1.1.1.m1.1.1.cmml">∞</mi><annotation-xml encoding="MathML-Content" id="S3.T1.5.1.1.1.1.m1.1b"><infinity id="S3.T1.5.1.1.1.1.m1.1.1.cmml" xref="S3.T1.5.1.1.1.1.m1.1.1"></infinity></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.1.1.1.1.m1.1c">\infty</annotation></semantics></math>
</td>
<td id="S3.T1.6.2.2.2.4" class="ltx_td ltx_align_center">20</td>
<td id="S3.T1.6.2.2.2.5" class="ltx_td ltx_align_center">10</td>
<td id="S3.T1.6.2.2.2.6" class="ltx_td ltx_align_center">5</td>
<td id="S3.T1.6.2.2.2.7" class="ltx_td ltx_align_center">0</td>
<td id="S3.T1.6.2.2.2.8" class="ltx_td ltx_align_center">-5</td>
<td id="S3.T1.6.2.2.2.9" class="ltx_td ltx_align_center">-10</td>
<td id="S3.T1.6.2.2.2.10" class="ltx_td ltx_align_center">-20</td>
<td id="S3.T1.6.2.2.2.2" class="ltx_td ltx_align_center"><math id="S3.T1.6.2.2.2.2.m1.1" class="ltx_Math" alttext="-\infty" display="inline"><semantics id="S3.T1.6.2.2.2.2.m1.1a"><mrow id="S3.T1.6.2.2.2.2.m1.1.1" xref="S3.T1.6.2.2.2.2.m1.1.1.cmml"><mo id="S3.T1.6.2.2.2.2.m1.1.1a" xref="S3.T1.6.2.2.2.2.m1.1.1.cmml">−</mo><mi mathvariant="normal" id="S3.T1.6.2.2.2.2.m1.1.1.2" xref="S3.T1.6.2.2.2.2.m1.1.1.2.cmml">∞</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.6.2.2.2.2.m1.1b"><apply id="S3.T1.6.2.2.2.2.m1.1.1.cmml" xref="S3.T1.6.2.2.2.2.m1.1.1"><minus id="S3.T1.6.2.2.2.2.m1.1.1.1.cmml" xref="S3.T1.6.2.2.2.2.m1.1.1"></minus><infinity id="S3.T1.6.2.2.2.2.m1.1.1.2.cmml" xref="S3.T1.6.2.2.2.2.m1.1.1.2"></infinity></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.2.2.2.2.m1.1c">-\infty</annotation></semantics></math></td>
<td id="S3.T1.6.2.2.2.11" class="ltx_td ltx_align_center ltx_border_l"><span id="S3.T1.6.2.2.2.11.1" class="ltx_text ltx_font_bold">Average</span></td>
</tr>
<tr id="S3.T1.8.4.4.4" class="ltx_tr">
<th id="S3.T1.8.4.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="background-color:#DEDCDC;" colspan="11"><span id="S3.T1.8.4.4.4.2.2" class="ltx_text" style="background-color:#DEDCDC;">WER <math id="S3.T1.7.3.3.3.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.7.3.3.3.1.1.m1.1a"><mo mathbackground="#DEDCDC" stretchy="false" id="S3.T1.7.3.3.3.1.1.m1.1.1" xref="S3.T1.7.3.3.3.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.7.3.3.3.1.1.m1.1b"><ci id="S3.T1.7.3.3.3.1.1.m1.1.1.cmml" xref="S3.T1.7.3.3.3.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.3.3.3.1.1.m1.1c">\downarrow</annotation></semantics></math> (relative benefit <math id="S3.T1.8.4.4.4.2.2.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.8.4.4.4.2.2.m2.1a"><mo mathbackground="#DEDCDC" stretchy="false" id="S3.T1.8.4.4.4.2.2.m2.1.1" xref="S3.T1.8.4.4.4.2.2.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.8.4.4.4.2.2.m2.1b"><ci id="S3.T1.8.4.4.4.2.2.m2.1.1.cmml" xref="S3.T1.8.4.4.4.2.2.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.4.4.4.2.2.m2.1c">\uparrow</annotation></semantics></math>)</span></th>
</tr>
<tr id="S3.T1.18.14.14.17.2" class="ltx_tr">
<th id="S3.T1.18.14.14.17.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Whisper-base.en</th>
<td id="S3.T1.18.14.14.17.2.2" class="ltx_td ltx_align_center">3.88</td>
<td id="S3.T1.18.14.14.17.2.3" class="ltx_td ltx_align_center">4.23</td>
<td id="S3.T1.18.14.14.17.2.4" class="ltx_td ltx_align_center">4.71</td>
<td id="S3.T1.18.14.14.17.2.5" class="ltx_td ltx_align_center">5.32</td>
<td id="S3.T1.18.14.14.17.2.6" class="ltx_td ltx_align_center">6.89</td>
<td id="S3.T1.18.14.14.17.2.7" class="ltx_td ltx_align_center">17.55</td>
<td id="S3.T1.18.14.14.17.2.8" class="ltx_td ltx_align_center">49.78</td>
<td id="S3.T1.18.14.14.17.2.9" class="ltx_td ltx_align_center">92.01</td>
<td id="S3.T1.18.14.14.17.2.10" class="ltx_td ltx_align_center">123.4</td>
<td id="S3.T1.18.14.14.17.2.11" class="ltx_td ltx_align_center ltx_border_l">34.2</td>
</tr>
<tr id="S3.T1.18.14.14.18.3" class="ltx_tr">
<th id="S3.T1.18.14.14.18.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Whisper-small.en</th>
<td id="S3.T1.18.14.14.18.3.2" class="ltx_td ltx_align_center">1.46</td>
<td id="S3.T1.18.14.14.18.3.3" class="ltx_td ltx_align_center">1.36</td>
<td id="S3.T1.18.14.14.18.3.4" class="ltx_td ltx_align_center">1.48</td>
<td id="S3.T1.18.14.14.18.3.5" class="ltx_td ltx_align_center">1.96</td>
<td id="S3.T1.18.14.14.18.3.6" class="ltx_td ltx_align_center">2.95</td>
<td id="S3.T1.18.14.14.18.3.7" class="ltx_td ltx_align_center">7.96</td>
<td id="S3.T1.18.14.14.18.3.8" class="ltx_td ltx_align_center">26.77</td>
<td id="S3.T1.18.14.14.18.3.9" class="ltx_td ltx_align_center">55.57</td>
<td id="S3.T1.18.14.14.18.3.10" class="ltx_td ltx_align_center">135.2</td>
<td id="S3.T1.18.14.14.18.3.11" class="ltx_td ltx_align_center ltx_border_l">26.1</td>
</tr>
<tr id="S3.T1.9.5.5.5" class="ltx_tr">
<th id="S3.T1.9.5.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">A<math id="S3.T1.9.5.5.5.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T1.9.5.5.5.1.m1.1a"><mo stretchy="false" id="S3.T1.9.5.5.5.1.m1.1.1" xref="S3.T1.9.5.5.5.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T1.9.5.5.5.1.m1.1b"><ci id="S3.T1.9.5.5.5.1.m1.1.1.cmml" xref="S3.T1.9.5.5.5.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.5.5.5.1.m1.1c">\rightarrow</annotation></semantics></math>T</th>
<td id="S3.T1.9.5.5.5.2" class="ltx_td ltx_align_center ltx_border_t">0.21</td>
<td id="S3.T1.9.5.5.5.3" class="ltx_td ltx_align_center ltx_border_t">0.33</td>
<td id="S3.T1.9.5.5.5.4" class="ltx_td ltx_align_center ltx_border_t">0.56</td>
<td id="S3.T1.9.5.5.5.5" class="ltx_td ltx_align_center ltx_border_t">1.11</td>
<td id="S3.T1.9.5.5.5.6" class="ltx_td ltx_align_center ltx_border_t">3.08</td>
<td id="S3.T1.9.5.5.5.7" class="ltx_td ltx_align_center ltx_border_t">12.75</td>
<td id="S3.T1.9.5.5.5.8" class="ltx_td ltx_align_center ltx_border_t">26.22</td>
<td id="S3.T1.9.5.5.5.9" class="ltx_td ltx_align_center ltx_border_t">37.67</td>
<td id="S3.T1.9.5.5.5.10" class="ltx_td ltx_align_center ltx_border_t">227.0</td>
<td id="S3.T1.9.5.5.5.11" class="ltx_td ltx_align_center ltx_border_l ltx_border_t">34.3</td>
</tr>
<tr id="S3.T1.10.6.6.6" class="ltx_tr">
<th id="S3.T1.10.6.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">I+A<math id="S3.T1.10.6.6.6.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T1.10.6.6.6.1.m1.1a"><mo stretchy="false" id="S3.T1.10.6.6.6.1.m1.1.1" xref="S3.T1.10.6.6.6.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T1.10.6.6.6.1.m1.1b"><ci id="S3.T1.10.6.6.6.1.m1.1.1.cmml" xref="S3.T1.10.6.6.6.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.6.6.6.1.m1.1c">\rightarrow</annotation></semantics></math>T</th>
<td id="S3.T1.10.6.6.6.2" class="ltx_td ltx_align_center">0.21 (0.0%)</td>
<td id="S3.T1.10.6.6.6.3" class="ltx_td ltx_align_center">0.34 (-3.0%)</td>
<td id="S3.T1.10.6.6.6.4" class="ltx_td ltx_align_center">0.59 (-5.4%)</td>
<td id="S3.T1.10.6.6.6.5" class="ltx_td ltx_align_center">1.1 <span id="S3.T1.10.6.6.6.5.1" class="ltx_text" style="color:#009503;">(+0.9%)</span>
</td>
<td id="S3.T1.10.6.6.6.6" class="ltx_td ltx_align_center">2.97 <span id="S3.T1.10.6.6.6.6.1" class="ltx_text" style="color:#009503;">(+3.6%)</span>
</td>
<td id="S3.T1.10.6.6.6.7" class="ltx_td ltx_align_center">13.05 (-2.4%)</td>
<td id="S3.T1.10.6.6.6.8" class="ltx_td ltx_align_center">26.78 (-2.1%)</td>
<td id="S3.T1.10.6.6.6.9" class="ltx_td ltx_align_center">38.07 (-1.1%)</td>
<td id="S3.T1.10.6.6.6.10" class="ltx_td ltx_align_center">238.1 (-4.9%)</td>
<td id="S3.T1.10.6.6.6.11" class="ltx_td ltx_align_center ltx_border_l">35.7 (-1.6%)</td>
</tr>
<tr id="S3.T1.11.7.7.7" class="ltx_tr">
<th id="S3.T1.11.7.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">L+A<math id="S3.T1.11.7.7.7.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T1.11.7.7.7.1.m1.1a"><mo stretchy="false" id="S3.T1.11.7.7.7.1.m1.1.1" xref="S3.T1.11.7.7.7.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T1.11.7.7.7.1.m1.1b"><ci id="S3.T1.11.7.7.7.1.m1.1.1.cmml" xref="S3.T1.11.7.7.7.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.7.7.7.1.m1.1c">\rightarrow</annotation></semantics></math>T</th>
<td id="S3.T1.11.7.7.7.2" class="ltx_td ltx_align_center">0.27 (-28.6%)</td>
<td id="S3.T1.11.7.7.7.3" class="ltx_td ltx_align_center">0.45 (-36.4%)</td>
<td id="S3.T1.11.7.7.7.4" class="ltx_td ltx_align_center">0.76 (-35.7%)</td>
<td id="S3.T1.11.7.7.7.5" class="ltx_td ltx_align_center">1.24 (-11.7%)</td>
<td id="S3.T1.11.7.7.7.6" class="ltx_td ltx_align_center">3.12 (-1.3%)</td>
<td id="S3.T1.11.7.7.7.7" class="ltx_td ltx_align_center">12.43 <span id="S3.T1.11.7.7.7.7.1" class="ltx_text" style="color:#009503;">(+2.5%)</span>
</td>
<td id="S3.T1.11.7.7.7.8" class="ltx_td ltx_align_center">24.71 <span id="S3.T1.11.7.7.7.8.1" class="ltx_text" style="color:#009503;">(+5.8%)</span>
</td>
<td id="S3.T1.11.7.7.7.9" class="ltx_td ltx_align_center">35.14 <span id="S3.T1.11.7.7.7.9.1" class="ltx_text" style="color:#009503;">(+6.7%)</span>
</td>
<td id="S3.T1.11.7.7.7.10" class="ltx_td ltx_align_center">191.8 <span id="S3.T1.11.7.7.7.10.1" class="ltx_text" style="color:#009503;">(+15.5%)</span>
</td>
<td id="S3.T1.11.7.7.7.11" class="ltx_td ltx_align_center ltx_border_l">30.0 (-9.2%)</td>
</tr>
<tr id="S3.T1.12.8.8.8" class="ltx_tr">
<th id="S3.T1.12.8.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">I+L+A<math id="S3.T1.12.8.8.8.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T1.12.8.8.8.1.m1.1a"><mo stretchy="false" id="S3.T1.12.8.8.8.1.m1.1.1" xref="S3.T1.12.8.8.8.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T1.12.8.8.8.1.m1.1b"><ci id="S3.T1.12.8.8.8.1.m1.1.1.cmml" xref="S3.T1.12.8.8.8.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.8.8.8.1.m1.1c">\rightarrow</annotation></semantics></math>T</th>
<td id="S3.T1.12.8.8.8.2" class="ltx_td ltx_align_center">0.19 <span id="S3.T1.12.8.8.8.2.1" class="ltx_text" style="color:#009503;">(+9.5%)</span>
</td>
<td id="S3.T1.12.8.8.8.3" class="ltx_td ltx_align_center">0.35 (-6.1%)</td>
<td id="S3.T1.12.8.8.8.4" class="ltx_td ltx_align_center">0.62 (-10.7%)</td>
<td id="S3.T1.12.8.8.8.5" class="ltx_td ltx_align_center">1.18 (-6.3%)</td>
<td id="S3.T1.12.8.8.8.6" class="ltx_td ltx_align_center">3.05 <span id="S3.T1.12.8.8.8.6.1" class="ltx_text" style="color:#009503;">(+1.0%)</span>
</td>
<td id="S3.T1.12.8.8.8.7" class="ltx_td ltx_align_center">12.69 <span id="S3.T1.12.8.8.8.7.1" class="ltx_text" style="color:#009503;">(+0.5%)</span>
</td>
<td id="S3.T1.12.8.8.8.8" class="ltx_td ltx_align_center">25.21 <span id="S3.T1.12.8.8.8.8.1" class="ltx_text" style="color:#009503;">(+3.9%)</span>
</td>
<td id="S3.T1.12.8.8.8.9" class="ltx_td ltx_align_center">35.61 <span id="S3.T1.12.8.8.8.9.1" class="ltx_text" style="color:#009503;">(+5.5%)</span>
</td>
<td id="S3.T1.12.8.8.8.10" class="ltx_td ltx_align_center">142.6 <span id="S3.T1.12.8.8.8.10.1" class="ltx_text" style="color:#009503;">(+37.2%)</span>
</td>
<td id="S3.T1.12.8.8.8.11" class="ltx_td ltx_align_center ltx_border_l"><span id="S3.T1.12.8.8.8.11.1" class="ltx_text ltx_font_bold">24.6 <span id="S3.T1.12.8.8.8.11.1.1" class="ltx_text" style="color:#009503;">(+3.8%)</span></span></td>
</tr>
<tr id="S3.T1.13.9.9.9" class="ltx_tr">
<th id="S3.T1.13.9.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">O+A<math id="S3.T1.13.9.9.9.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T1.13.9.9.9.1.m1.1a"><mo stretchy="false" id="S3.T1.13.9.9.9.1.m1.1.1" xref="S3.T1.13.9.9.9.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T1.13.9.9.9.1.m1.1b"><ci id="S3.T1.13.9.9.9.1.m1.1.1.cmml" xref="S3.T1.13.9.9.9.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.13.9.9.9.1.m1.1c">\rightarrow</annotation></semantics></math>T</th>
<td id="S3.T1.13.9.9.9.2" class="ltx_td ltx_align_center ltx_border_t">0.24 (-14.3%)</td>
<td id="S3.T1.13.9.9.9.3" class="ltx_td ltx_align_center ltx_border_t">0.28 <span id="S3.T1.13.9.9.9.3.1" class="ltx_text" style="color:#009503;">(+15.2%)</span>
</td>
<td id="S3.T1.13.9.9.9.4" class="ltx_td ltx_align_center ltx_border_t">0.51 <span id="S3.T1.13.9.9.9.4.1" class="ltx_text" style="color:#009503;">(+8.9%)</span>
</td>
<td id="S3.T1.13.9.9.9.5" class="ltx_td ltx_align_center ltx_border_t">0.86 <span id="S3.T1.13.9.9.9.5.1" class="ltx_text" style="color:#009503;">(+22.5%)</span>
</td>
<td id="S3.T1.13.9.9.9.6" class="ltx_td ltx_align_center ltx_border_t">2.4 <span id="S3.T1.13.9.9.9.6.1" class="ltx_text" style="color:#009503;">(+22.1%)</span>
</td>
<td id="S3.T1.13.9.9.9.7" class="ltx_td ltx_align_center ltx_border_t">11.68 <span id="S3.T1.13.9.9.9.7.1" class="ltx_text" style="color:#009503;">(+8.4%)</span>
</td>
<td id="S3.T1.13.9.9.9.8" class="ltx_td ltx_align_center ltx_border_t">24.33 <span id="S3.T1.13.9.9.9.8.1" class="ltx_text" style="color:#009503;">(+7.2%)</span>
</td>
<td id="S3.T1.13.9.9.9.9" class="ltx_td ltx_align_center ltx_border_t">34.66 <span id="S3.T1.13.9.9.9.9.1" class="ltx_text" style="color:#009503;">(+8.0%)</span>
</td>
<td id="S3.T1.13.9.9.9.10" class="ltx_td ltx_align_center ltx_border_t">172.4 <span id="S3.T1.13.9.9.9.10.1" class="ltx_text" style="color:#009503;">(+24.0%)</span>
</td>
<td id="S3.T1.13.9.9.9.11" class="ltx_td ltx_align_center ltx_border_l ltx_border_t">27.5 <span id="S3.T1.13.9.9.9.11.1" class="ltx_text" style="color:#009503;">(+11.3%)</span>
</td>
</tr>
<tr id="S3.T1.14.10.10.10" class="ltx_tr">
<th id="S3.T1.14.10.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">O+L+A<math id="S3.T1.14.10.10.10.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T1.14.10.10.10.1.m1.1a"><mo stretchy="false" id="S3.T1.14.10.10.10.1.m1.1.1" xref="S3.T1.14.10.10.10.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T1.14.10.10.10.1.m1.1b"><ci id="S3.T1.14.10.10.10.1.m1.1.1.cmml" xref="S3.T1.14.10.10.10.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.14.10.10.10.1.m1.1c">\rightarrow</annotation></semantics></math>T</th>
<td id="S3.T1.14.10.10.10.2" class="ltx_td ltx_align_center">0.22 (-4.8%)</td>
<td id="S3.T1.14.10.10.10.3" class="ltx_td ltx_align_center">0.29 <span id="S3.T1.14.10.10.10.3.1" class="ltx_text" style="color:#009503;">(+12.1%)</span>
</td>
<td id="S3.T1.14.10.10.10.4" class="ltx_td ltx_align_center">0.51 <span id="S3.T1.14.10.10.10.4.1" class="ltx_text" style="color:#009503;">(+8.9%)</span>
</td>
<td id="S3.T1.14.10.10.10.5" class="ltx_td ltx_align_center">1.01 <span id="S3.T1.14.10.10.10.5.1" class="ltx_text" style="color:#009503;">(+9.0%)</span>
</td>
<td id="S3.T1.14.10.10.10.6" class="ltx_td ltx_align_center">2.57 <span id="S3.T1.14.10.10.10.6.1" class="ltx_text" style="color:#009503;">(+16.6%)</span>
</td>
<td id="S3.T1.14.10.10.10.7" class="ltx_td ltx_align_center">11.72 <span id="S3.T1.14.10.10.10.7.1" class="ltx_text" style="color:#009503;">(+8.1%)</span>
</td>
<td id="S3.T1.14.10.10.10.8" class="ltx_td ltx_align_center">24.13 <span id="S3.T1.14.10.10.10.8.1" class="ltx_text" style="color:#009503;">(+8.0%)</span>
</td>
<td id="S3.T1.14.10.10.10.9" class="ltx_td ltx_align_center">34.03 <span id="S3.T1.14.10.10.10.9.1" class="ltx_text" style="color:#009503;">(+9.7%)</span>
</td>
<td id="S3.T1.14.10.10.10.10" class="ltx_td ltx_align_center">135.9 <span id="S3.T1.14.10.10.10.10.1" class="ltx_text" style="color:#009503;">(+40.2%)</span>
</td>
<td id="S3.T1.14.10.10.10.11" class="ltx_td ltx_align_center ltx_border_l"><span id="S3.T1.14.10.10.10.11.1" class="ltx_text ltx_font_bold">23.4 <span id="S3.T1.14.10.10.10.11.1.1" class="ltx_text" style="color:#009503;">(+12.0%)</span></span></td>
</tr>
<tr id="S3.T1.16.12.12.12" class="ltx_tr">
<th id="S3.T1.16.12.12.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">
<math id="S3.T1.15.11.11.11.1.m1.1" class="ltx_Math" alttext="\textrm{O}_{\textrm{oracle,3}}" display="inline"><semantics id="S3.T1.15.11.11.11.1.m1.1a"><msub id="S3.T1.15.11.11.11.1.m1.1.1" xref="S3.T1.15.11.11.11.1.m1.1.1.cmml"><mtext id="S3.T1.15.11.11.11.1.m1.1.1.2" xref="S3.T1.15.11.11.11.1.m1.1.1.2a.cmml">O</mtext><mtext id="S3.T1.15.11.11.11.1.m1.1.1.3" xref="S3.T1.15.11.11.11.1.m1.1.1.3a.cmml">oracle,3</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.T1.15.11.11.11.1.m1.1b"><apply id="S3.T1.15.11.11.11.1.m1.1.1.cmml" xref="S3.T1.15.11.11.11.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.15.11.11.11.1.m1.1.1.1.cmml" xref="S3.T1.15.11.11.11.1.m1.1.1">subscript</csymbol><ci id="S3.T1.15.11.11.11.1.m1.1.1.2a.cmml" xref="S3.T1.15.11.11.11.1.m1.1.1.2"><mtext id="S3.T1.15.11.11.11.1.m1.1.1.2.cmml" xref="S3.T1.15.11.11.11.1.m1.1.1.2">O</mtext></ci><ci id="S3.T1.15.11.11.11.1.m1.1.1.3a.cmml" xref="S3.T1.15.11.11.11.1.m1.1.1.3"><mtext mathsize="70%" id="S3.T1.15.11.11.11.1.m1.1.1.3.cmml" xref="S3.T1.15.11.11.11.1.m1.1.1.3">oracle,3</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.15.11.11.11.1.m1.1c">\textrm{O}_{\textrm{oracle,3}}</annotation></semantics></math>+A<math id="S3.T1.16.12.12.12.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T1.16.12.12.12.2.m2.1a"><mo stretchy="false" id="S3.T1.16.12.12.12.2.m2.1.1" xref="S3.T1.16.12.12.12.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T1.16.12.12.12.2.m2.1b"><ci id="S3.T1.16.12.12.12.2.m2.1.1.cmml" xref="S3.T1.16.12.12.12.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.16.12.12.12.2.m2.1c">\rightarrow</annotation></semantics></math>T</th>
<td id="S3.T1.16.12.12.12.3" class="ltx_td ltx_align_center ltx_border_t">0.04 <span id="S3.T1.16.12.12.12.3.1" class="ltx_text" style="color:#009503;">(+81%)</span>
</td>
<td id="S3.T1.16.12.12.12.4" class="ltx_td ltx_align_center ltx_border_t">0.06 <span id="S3.T1.16.12.12.12.4.1" class="ltx_text" style="color:#009503;">(+81.8%)</span>
</td>
<td id="S3.T1.16.12.12.12.5" class="ltx_td ltx_align_center ltx_border_t">0.1 <span id="S3.T1.16.12.12.12.5.1" class="ltx_text" style="color:#009503;">(+82.1%)</span>
</td>
<td id="S3.T1.16.12.12.12.6" class="ltx_td ltx_align_center ltx_border_t">0.14 <span id="S3.T1.16.12.12.12.6.1" class="ltx_text" style="color:#009503;">(+87.4%)</span>
</td>
<td id="S3.T1.16.12.12.12.7" class="ltx_td ltx_align_center ltx_border_t">0.31 <span id="S3.T1.16.12.12.12.7.1" class="ltx_text" style="color:#009503;">(+89.9%)</span>
</td>
<td id="S3.T1.16.12.12.12.8" class="ltx_td ltx_align_center ltx_border_t">1.42 <span id="S3.T1.16.12.12.12.8.1" class="ltx_text" style="color:#009503;">(+88.9%)</span>
</td>
<td id="S3.T1.16.12.12.12.9" class="ltx_td ltx_align_center ltx_border_t">2.12 <span id="S3.T1.16.12.12.12.9.1" class="ltx_text" style="color:#009503;">(+91.9%)</span>
</td>
<td id="S3.T1.16.12.12.12.10" class="ltx_td ltx_align_center ltx_border_t">3.14 <span id="S3.T1.16.12.12.12.10.1" class="ltx_text" style="color:#009503;">(+91.7%)</span>
</td>
<td id="S3.T1.16.12.12.12.11" class="ltx_td ltx_align_center ltx_border_t">91.95 <span id="S3.T1.16.12.12.12.11.1" class="ltx_text" style="color:#009503;">(+59.5%)</span>
</td>
<td id="S3.T1.16.12.12.12.12" class="ltx_td ltx_align_center ltx_border_l ltx_border_t"><span id="S3.T1.16.12.12.12.12.1" class="ltx_text ltx_font_bold">11.0 <span id="S3.T1.16.12.12.12.12.1.1" class="ltx_text" style="color:#009503;">(+83.8%)</span></span></td>
</tr>
<tr id="S3.T1.18.14.14.14" class="ltx_tr">
<th id="S3.T1.18.14.14.14.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<math id="S3.T1.17.13.13.13.1.m1.1" class="ltx_Math" alttext="\textrm{O}_{\textrm{oracle,10}}" display="inline"><semantics id="S3.T1.17.13.13.13.1.m1.1a"><msub id="S3.T1.17.13.13.13.1.m1.1.1" xref="S3.T1.17.13.13.13.1.m1.1.1.cmml"><mtext id="S3.T1.17.13.13.13.1.m1.1.1.2" xref="S3.T1.17.13.13.13.1.m1.1.1.2a.cmml">O</mtext><mtext id="S3.T1.17.13.13.13.1.m1.1.1.3" xref="S3.T1.17.13.13.13.1.m1.1.1.3a.cmml">oracle,10</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.T1.17.13.13.13.1.m1.1b"><apply id="S3.T1.17.13.13.13.1.m1.1.1.cmml" xref="S3.T1.17.13.13.13.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.17.13.13.13.1.m1.1.1.1.cmml" xref="S3.T1.17.13.13.13.1.m1.1.1">subscript</csymbol><ci id="S3.T1.17.13.13.13.1.m1.1.1.2a.cmml" xref="S3.T1.17.13.13.13.1.m1.1.1.2"><mtext id="S3.T1.17.13.13.13.1.m1.1.1.2.cmml" xref="S3.T1.17.13.13.13.1.m1.1.1.2">O</mtext></ci><ci id="S3.T1.17.13.13.13.1.m1.1.1.3a.cmml" xref="S3.T1.17.13.13.13.1.m1.1.1.3"><mtext mathsize="70%" id="S3.T1.17.13.13.13.1.m1.1.1.3.cmml" xref="S3.T1.17.13.13.13.1.m1.1.1.3">oracle,10</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.17.13.13.13.1.m1.1c">\textrm{O}_{\textrm{oracle,10}}</annotation></semantics></math>+A<math id="S3.T1.18.14.14.14.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T1.18.14.14.14.2.m2.1a"><mo stretchy="false" id="S3.T1.18.14.14.14.2.m2.1.1" xref="S3.T1.18.14.14.14.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T1.18.14.14.14.2.m2.1b"><ci id="S3.T1.18.14.14.14.2.m2.1.1.cmml" xref="S3.T1.18.14.14.14.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.18.14.14.14.2.m2.1c">\rightarrow</annotation></semantics></math>T</th>
<td id="S3.T1.18.14.14.14.3" class="ltx_td ltx_align_center">0.07 <span id="S3.T1.18.14.14.14.3.1" class="ltx_text" style="color:#009503;">(+66.7%)</span>
</td>
<td id="S3.T1.18.14.14.14.4" class="ltx_td ltx_align_center">0.08 <span id="S3.T1.18.14.14.14.4.1" class="ltx_text" style="color:#009503;">(+75.8%)</span>
</td>
<td id="S3.T1.18.14.14.14.5" class="ltx_td ltx_align_center">0.13 <span id="S3.T1.18.14.14.14.5.1" class="ltx_text" style="color:#009503;">(+76.8%)</span>
</td>
<td id="S3.T1.18.14.14.14.6" class="ltx_td ltx_align_center">0.32 <span id="S3.T1.18.14.14.14.6.1" class="ltx_text" style="color:#009503;">(+71.2%)</span>
</td>
<td id="S3.T1.18.14.14.14.7" class="ltx_td ltx_align_center">0.79 <span id="S3.T1.18.14.14.14.7.1" class="ltx_text" style="color:#009503;">(+74.4%)</span>
</td>
<td id="S3.T1.18.14.14.14.8" class="ltx_td ltx_align_center">5.78 <span id="S3.T1.18.14.14.14.8.1" class="ltx_text" style="color:#009503;">(+54.7%)</span>
</td>
<td id="S3.T1.18.14.14.14.9" class="ltx_td ltx_align_center">12.32 <span id="S3.T1.18.14.14.14.9.1" class="ltx_text" style="color:#009503;">(+53.0%)</span>
</td>
<td id="S3.T1.18.14.14.14.10" class="ltx_td ltx_align_center">18.79 <span id="S3.T1.18.14.14.14.10.1" class="ltx_text" style="color:#009503;">(+50.1%)</span>
</td>
<td id="S3.T1.18.14.14.14.11" class="ltx_td ltx_align_center">109.4 <span id="S3.T1.18.14.14.14.11.1" class="ltx_text" style="color:#009503;">(+51.8%)</span>
</td>
<td id="S3.T1.18.14.14.14.12" class="ltx_td ltx_align_center ltx_border_l">16.4 <span id="S3.T1.18.14.14.14.12.1" class="ltx_text" style="color:#009503;">(+63.8%)</span>
</td>
</tr>
</tbody>
<tfoot class="ltx_tfoot">
<tr id="S3.T1.18.14.14.19.1" class="ltx_tr">
<th id="S3.T1.18.14.14.19.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" colspan="11"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></th>
</tr>
</tfoot>
</table>
</span></div>
</figure>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Evaluation of WER (%) and benefit of adding OCR(O) on the SlideAVSR dataset, at different noise levels based on SNR and FQ Ranker <math id="S3.T2.3.m1.1" class="ltx_Math" alttext="K^{\mathrm{a}}" display="inline"><semantics id="S3.T2.3.m1.1b"><msup id="S3.T2.3.m1.1.1" xref="S3.T2.3.m1.1.1.cmml"><mi id="S3.T2.3.m1.1.1.2" xref="S3.T2.3.m1.1.1.2.cmml">K</mi><mi mathvariant="normal" id="S3.T2.3.m1.1.1.3" xref="S3.T2.3.m1.1.1.3.cmml">a</mi></msup><annotation-xml encoding="MathML-Content" id="S3.T2.3.m1.1c"><apply id="S3.T2.3.m1.1.1.cmml" xref="S3.T2.3.m1.1.1"><csymbol cd="ambiguous" id="S3.T2.3.m1.1.1.1.cmml" xref="S3.T2.3.m1.1.1">superscript</csymbol><ci id="S3.T2.3.m1.1.1.2.cmml" xref="S3.T2.3.m1.1.1.2">𝐾</ci><ci id="S3.T2.3.m1.1.1.3.cmml" xref="S3.T2.3.m1.1.1.3">a</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.m1.1d">K^{\mathrm{a}}</annotation></semantics></math> values. <math id="S3.T2.4.m2.1" class="ltx_Math" alttext="\textrm{O}_{\textrm{ALL}}" display="inline"><semantics id="S3.T2.4.m2.1b"><msub id="S3.T2.4.m2.1.1" xref="S3.T2.4.m2.1.1.cmml"><mtext id="S3.T2.4.m2.1.1.2" xref="S3.T2.4.m2.1.1.2a.cmml">O</mtext><mtext id="S3.T2.4.m2.1.1.3" xref="S3.T2.4.m2.1.1.3a.cmml">ALL</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.T2.4.m2.1c"><apply id="S3.T2.4.m2.1.1.cmml" xref="S3.T2.4.m2.1.1"><csymbol cd="ambiguous" id="S3.T2.4.m2.1.1.1.cmml" xref="S3.T2.4.m2.1.1">subscript</csymbol><ci id="S3.T2.4.m2.1.1.2a.cmml" xref="S3.T2.4.m2.1.1.2"><mtext id="S3.T2.4.m2.1.1.2.cmml" xref="S3.T2.4.m2.1.1.2">O</mtext></ci><ci id="S3.T2.4.m2.1.1.3a.cmml" xref="S3.T2.4.m2.1.1.3"><mtext mathsize="70%" id="S3.T2.4.m2.1.1.3.cmml" xref="S3.T2.4.m2.1.1.3">ALL</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.m2.1d">\textrm{O}_{\textrm{ALL}}</annotation></semantics></math> means using all OCR words.</figcaption>
<div id="S3.T2.15" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:162pt;vertical-align:-2.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.2pt,10.4pt) scale(0.88473436490263,0.88473436490263) ;">
<table id="S3.T2.15.11" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.15.11.12.1" class="ltx_tr">
<th id="S3.T2.15.11.12.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" colspan="6"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.15.11.13.1" class="ltx_tr">
<th id="S3.T2.15.11.13.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S3.T2.15.11.13.1.2" class="ltx_td ltx_align_center">(clean)</td>
<td id="S3.T2.15.11.13.1.3" class="ltx_td ltx_align_center" colspan="2">SNR(dB) =</td>
<td id="S3.T2.15.11.13.1.4" class="ltx_td"></td>
<td id="S3.T2.15.11.13.1.5" class="ltx_td ltx_border_l"></td>
</tr>
<tr id="S3.T2.5.1.1" class="ltx_tr">
<th id="S3.T2.5.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Task</th>
<td id="S3.T2.5.1.1.1" class="ltx_td ltx_align_center">+<math id="S3.T2.5.1.1.1.m1.1" class="ltx_Math" alttext="\infty" display="inline"><semantics id="S3.T2.5.1.1.1.m1.1a"><mi mathvariant="normal" id="S3.T2.5.1.1.1.m1.1.1" xref="S3.T2.5.1.1.1.m1.1.1.cmml">∞</mi><annotation-xml encoding="MathML-Content" id="S3.T2.5.1.1.1.m1.1b"><infinity id="S3.T2.5.1.1.1.m1.1.1.cmml" xref="S3.T2.5.1.1.1.m1.1.1"></infinity></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.1.1.1.m1.1c">\infty</annotation></semantics></math>
</td>
<td id="S3.T2.5.1.1.3" class="ltx_td ltx_align_center">10</td>
<td id="S3.T2.5.1.1.4" class="ltx_td ltx_align_center">0</td>
<td id="S3.T2.5.1.1.5" class="ltx_td ltx_align_center">-10</td>
<td id="S3.T2.5.1.1.6" class="ltx_td ltx_align_center ltx_border_l">Average</td>
</tr>
<tr id="S3.T2.7.3.3" class="ltx_tr">
<th id="S3.T2.7.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="background-color:#DEDCDC;" colspan="6"><span id="S3.T2.7.3.3.2.2" class="ltx_text" style="background-color:#DEDCDC;">WER <math id="S3.T2.6.2.2.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T2.6.2.2.1.1.m1.1a"><mo mathbackground="#DEDCDC" stretchy="false" id="S3.T2.6.2.2.1.1.m1.1.1" xref="S3.T2.6.2.2.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T2.6.2.2.1.1.m1.1b"><ci id="S3.T2.6.2.2.1.1.m1.1.1.cmml" xref="S3.T2.6.2.2.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.2.2.1.1.m1.1c">\downarrow</annotation></semantics></math> (relative benefit <math id="S3.T2.7.3.3.2.2.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T2.7.3.3.2.2.m2.1a"><mo mathbackground="#DEDCDC" stretchy="false" id="S3.T2.7.3.3.2.2.m2.1.1" xref="S3.T2.7.3.3.2.2.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.7.3.3.2.2.m2.1b"><ci id="S3.T2.7.3.3.2.2.m2.1.1.cmml" xref="S3.T2.7.3.3.2.2.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.7.3.3.2.2.m2.1c">\uparrow</annotation></semantics></math>)</span></th>
</tr>
<tr id="S3.T2.8.4.4" class="ltx_tr">
<th id="S3.T2.8.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">A<math id="S3.T2.8.4.4.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T2.8.4.4.1.m1.1a"><mo stretchy="false" id="S3.T2.8.4.4.1.m1.1.1" xref="S3.T2.8.4.4.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T2.8.4.4.1.m1.1b"><ci id="S3.T2.8.4.4.1.m1.1.1.cmml" xref="S3.T2.8.4.4.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.8.4.4.1.m1.1c">\rightarrow</annotation></semantics></math>T</th>
<td id="S3.T2.8.4.4.2" class="ltx_td ltx_align_center">33.8</td>
<td id="S3.T2.8.4.4.3" class="ltx_td ltx_align_center">42.5</td>
<td id="S3.T2.8.4.4.4" class="ltx_td ltx_align_center">44.8</td>
<td id="S3.T2.8.4.4.5" class="ltx_td ltx_align_center">70.6</td>
<td id="S3.T2.8.4.4.6" class="ltx_td ltx_align_center ltx_border_l">47.9</td>
</tr>
<tr id="S3.T2.10.6.6" class="ltx_tr">
<th id="S3.T2.10.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<math id="S3.T2.9.5.5.1.m1.1" class="ltx_Math" alttext="\textrm{O}_{\textrm{ALL}}" display="inline"><semantics id="S3.T2.9.5.5.1.m1.1a"><msub id="S3.T2.9.5.5.1.m1.1.1" xref="S3.T2.9.5.5.1.m1.1.1.cmml"><mtext id="S3.T2.9.5.5.1.m1.1.1.2" xref="S3.T2.9.5.5.1.m1.1.1.2a.cmml">O</mtext><mtext id="S3.T2.9.5.5.1.m1.1.1.3" xref="S3.T2.9.5.5.1.m1.1.1.3a.cmml">ALL</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.T2.9.5.5.1.m1.1b"><apply id="S3.T2.9.5.5.1.m1.1.1.cmml" xref="S3.T2.9.5.5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T2.9.5.5.1.m1.1.1.1.cmml" xref="S3.T2.9.5.5.1.m1.1.1">subscript</csymbol><ci id="S3.T2.9.5.5.1.m1.1.1.2a.cmml" xref="S3.T2.9.5.5.1.m1.1.1.2"><mtext id="S3.T2.9.5.5.1.m1.1.1.2.cmml" xref="S3.T2.9.5.5.1.m1.1.1.2">O</mtext></ci><ci id="S3.T2.9.5.5.1.m1.1.1.3a.cmml" xref="S3.T2.9.5.5.1.m1.1.1.3"><mtext mathsize="70%" id="S3.T2.9.5.5.1.m1.1.1.3.cmml" xref="S3.T2.9.5.5.1.m1.1.1.3">ALL</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.9.5.5.1.m1.1c">\textrm{O}_{\textrm{ALL}}</annotation></semantics></math>+A<math id="S3.T2.10.6.6.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T2.10.6.6.2.m2.1a"><mo stretchy="false" id="S3.T2.10.6.6.2.m2.1.1" xref="S3.T2.10.6.6.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T2.10.6.6.2.m2.1b"><ci id="S3.T2.10.6.6.2.m2.1.1.cmml" xref="S3.T2.10.6.6.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.10.6.6.2.m2.1c">\rightarrow</annotation></semantics></math>T</th>
<td id="S3.T2.10.6.6.3" class="ltx_td ltx_align_center">31.4 <span id="S3.T2.10.6.6.3.1" class="ltx_text" style="color:#009503;">(+7.0%)</span>
</td>
<td id="S3.T2.10.6.6.4" class="ltx_td ltx_align_center">37.2 <span id="S3.T2.10.6.6.4.1" class="ltx_text" style="color:#009503;">(+12.4%)</span>
</td>
<td id="S3.T2.10.6.6.5" class="ltx_td ltx_align_center">47.7 (-6.4%)</td>
<td id="S3.T2.10.6.6.6" class="ltx_td ltx_align_center">75.9 (-7.6%)</td>
<td id="S3.T2.10.6.6.7" class="ltx_td ltx_align_center ltx_border_l">48.0 <span id="S3.T2.10.6.6.7.1" class="ltx_text" style="color:#009503;">(+1.4%)</span>
</td>
</tr>
<tr id="S3.T2.12.8.8" class="ltx_tr">
<th id="S3.T2.12.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<math id="S3.T2.11.7.7.1.m1.1" class="ltx_Math" alttext="\textrm{O}_{K=30}" display="inline"><semantics id="S3.T2.11.7.7.1.m1.1a"><msub id="S3.T2.11.7.7.1.m1.1.1" xref="S3.T2.11.7.7.1.m1.1.1.cmml"><mtext id="S3.T2.11.7.7.1.m1.1.1.2" xref="S3.T2.11.7.7.1.m1.1.1.2a.cmml">O</mtext><mrow id="S3.T2.11.7.7.1.m1.1.1.3" xref="S3.T2.11.7.7.1.m1.1.1.3.cmml"><mi id="S3.T2.11.7.7.1.m1.1.1.3.2" xref="S3.T2.11.7.7.1.m1.1.1.3.2.cmml">K</mi><mo id="S3.T2.11.7.7.1.m1.1.1.3.1" xref="S3.T2.11.7.7.1.m1.1.1.3.1.cmml">=</mo><mn id="S3.T2.11.7.7.1.m1.1.1.3.3" xref="S3.T2.11.7.7.1.m1.1.1.3.3.cmml">30</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.T2.11.7.7.1.m1.1b"><apply id="S3.T2.11.7.7.1.m1.1.1.cmml" xref="S3.T2.11.7.7.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T2.11.7.7.1.m1.1.1.1.cmml" xref="S3.T2.11.7.7.1.m1.1.1">subscript</csymbol><ci id="S3.T2.11.7.7.1.m1.1.1.2a.cmml" xref="S3.T2.11.7.7.1.m1.1.1.2"><mtext id="S3.T2.11.7.7.1.m1.1.1.2.cmml" xref="S3.T2.11.7.7.1.m1.1.1.2">O</mtext></ci><apply id="S3.T2.11.7.7.1.m1.1.1.3.cmml" xref="S3.T2.11.7.7.1.m1.1.1.3"><eq id="S3.T2.11.7.7.1.m1.1.1.3.1.cmml" xref="S3.T2.11.7.7.1.m1.1.1.3.1"></eq><ci id="S3.T2.11.7.7.1.m1.1.1.3.2.cmml" xref="S3.T2.11.7.7.1.m1.1.1.3.2">𝐾</ci><cn type="integer" id="S3.T2.11.7.7.1.m1.1.1.3.3.cmml" xref="S3.T2.11.7.7.1.m1.1.1.3.3">30</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.11.7.7.1.m1.1c">\textrm{O}_{K=30}</annotation></semantics></math>+A<math id="S3.T2.12.8.8.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T2.12.8.8.2.m2.1a"><mo stretchy="false" id="S3.T2.12.8.8.2.m2.1.1" xref="S3.T2.12.8.8.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T2.12.8.8.2.m2.1b"><ci id="S3.T2.12.8.8.2.m2.1.1.cmml" xref="S3.T2.12.8.8.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.12.8.8.2.m2.1c">\rightarrow</annotation></semantics></math>T</th>
<td id="S3.T2.12.8.8.3" class="ltx_td ltx_align_center">30.5 <span id="S3.T2.12.8.8.3.1" class="ltx_text" style="color:#009503;">(+9.7%)</span>
</td>
<td id="S3.T2.12.8.8.4" class="ltx_td ltx_align_center">35.7 <span id="S3.T2.12.8.8.4.1" class="ltx_text" style="color:#009503;">(+15.8%)</span>
</td>
<td id="S3.T2.12.8.8.5" class="ltx_td ltx_align_center">46.9 (-4.7%)</td>
<td id="S3.T2.12.8.8.6" class="ltx_td ltx_align_center">75.5 (-6.9%)</td>
<td id="S3.T2.12.8.8.7" class="ltx_td ltx_align_center ltx_border_l">
<span id="S3.T2.12.8.8.7.1" class="ltx_text ltx_font_bold">47.2</span> <span id="S3.T2.12.8.8.7.2" class="ltx_text" style="color:#009503;">(+3.5%)</span>
</td>
</tr>
<tr id="S3.T2.14.10.10" class="ltx_tr">
<th id="S3.T2.14.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<math id="S3.T2.13.9.9.1.m1.1" class="ltx_Math" alttext="\textrm{O}_{K=10}" display="inline"><semantics id="S3.T2.13.9.9.1.m1.1a"><msub id="S3.T2.13.9.9.1.m1.1.1" xref="S3.T2.13.9.9.1.m1.1.1.cmml"><mtext id="S3.T2.13.9.9.1.m1.1.1.2" xref="S3.T2.13.9.9.1.m1.1.1.2a.cmml">O</mtext><mrow id="S3.T2.13.9.9.1.m1.1.1.3" xref="S3.T2.13.9.9.1.m1.1.1.3.cmml"><mi id="S3.T2.13.9.9.1.m1.1.1.3.2" xref="S3.T2.13.9.9.1.m1.1.1.3.2.cmml">K</mi><mo id="S3.T2.13.9.9.1.m1.1.1.3.1" xref="S3.T2.13.9.9.1.m1.1.1.3.1.cmml">=</mo><mn id="S3.T2.13.9.9.1.m1.1.1.3.3" xref="S3.T2.13.9.9.1.m1.1.1.3.3.cmml">10</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.T2.13.9.9.1.m1.1b"><apply id="S3.T2.13.9.9.1.m1.1.1.cmml" xref="S3.T2.13.9.9.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T2.13.9.9.1.m1.1.1.1.cmml" xref="S3.T2.13.9.9.1.m1.1.1">subscript</csymbol><ci id="S3.T2.13.9.9.1.m1.1.1.2a.cmml" xref="S3.T2.13.9.9.1.m1.1.1.2"><mtext id="S3.T2.13.9.9.1.m1.1.1.2.cmml" xref="S3.T2.13.9.9.1.m1.1.1.2">O</mtext></ci><apply id="S3.T2.13.9.9.1.m1.1.1.3.cmml" xref="S3.T2.13.9.9.1.m1.1.1.3"><eq id="S3.T2.13.9.9.1.m1.1.1.3.1.cmml" xref="S3.T2.13.9.9.1.m1.1.1.3.1"></eq><ci id="S3.T2.13.9.9.1.m1.1.1.3.2.cmml" xref="S3.T2.13.9.9.1.m1.1.1.3.2">𝐾</ci><cn type="integer" id="S3.T2.13.9.9.1.m1.1.1.3.3.cmml" xref="S3.T2.13.9.9.1.m1.1.1.3.3">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.13.9.9.1.m1.1c">\textrm{O}_{K=10}</annotation></semantics></math>+A<math id="S3.T2.14.10.10.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T2.14.10.10.2.m2.1a"><mo stretchy="false" id="S3.T2.14.10.10.2.m2.1.1" xref="S3.T2.14.10.10.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T2.14.10.10.2.m2.1b"><ci id="S3.T2.14.10.10.2.m2.1.1.cmml" xref="S3.T2.14.10.10.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.14.10.10.2.m2.1c">\rightarrow</annotation></semantics></math>T</th>
<td id="S3.T2.14.10.10.3" class="ltx_td ltx_align_center">30.6 <span id="S3.T2.14.10.10.3.1" class="ltx_text" style="color:#009503;">(+9.5%)</span>
</td>
<td id="S3.T2.14.10.10.4" class="ltx_td ltx_align_center">34.6 <span id="S3.T2.14.10.10.4.1" class="ltx_text" style="color:#009503;">(+18.5%)</span>
</td>
<td id="S3.T2.14.10.10.5" class="ltx_td ltx_align_center">46.2 (-3.1%)</td>
<td id="S3.T2.14.10.10.6" class="ltx_td ltx_align_center">77.9 (-10.3%)</td>
<td id="S3.T2.14.10.10.7" class="ltx_td ltx_align_center ltx_border_l">47.3 <span id="S3.T2.14.10.10.7.1" class="ltx_text" style="color:#009503;">(<span id="S3.T2.14.10.10.7.1.1" class="ltx_text ltx_font_bold">+3.6%</span>)</span>
</td>
</tr>
</tbody>
<tfoot class="ltx_tfoot">
<tr id="S3.T2.15.11.14.1" class="ltx_tr">
<th id="S3.T2.15.11.14.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" colspan="6"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></th>
</tr>
<tr id="S3.T2.15.11.11" class="ltx_tr">
<th id="S3.T2.15.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="6">
<sup id="S3.T2.15.11.11.1.1" class="ltx_sup"><span id="S3.T2.15.11.11.1.1.1" class="ltx_text ltx_font_italic">a</span></sup>Indicating maximum word counts for OCR.</th>
</tr>
</tfoot>
</table>
</span></div>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">On the one hand, using multiple modalities could improve accuracy by supplying the model with complementary information. On the other hand, it could conceivably hurt performance, as the additional modalities result in longer input sequences, which might prevent the model from finding relevant information. We thus conduct a modality ablation study to investigate whether fusing more modalities with audio can improve recognition performance. In this experiment we examine the potential benefits of incorporating multiple modalities by averaging across different noise levels.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.3" class="ltx_p"><span id="S3.SS3.p2.3.1" class="ltx_text ltx_font_bold">3-Equations</span>: In Table <a href="#S3.T1" title="TABLE I ‣ III-C Experiment I: How much do additional modalities improve ASR accuracy on average? ‣ III Experiments ‣ Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> we list the WER and relative benefit of adding each modality to the audio-only baseline (A<math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mo stretchy="false" id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\rightarrow</annotation></semantics></math>T). Models are fine-tuned on the 3-Equations 2-noise training set, such that each audio is augmented with random MUSAN noise at an SNR in [+<math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\infty" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mi mathvariant="normal" id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">∞</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><infinity id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"></infinity></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\infty</annotation></semantics></math>, 20, 10, 5, 0, -5, -10, -20, -<math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="\infty" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mi mathvariant="normal" id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">∞</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><infinity id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"></infinity></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">\infty</annotation></semantics></math>]. Then the models are evaluated on 2-noise constant SNR sets.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.5" class="ltx_p">Compared to an audio-only model, the average benefits of adding a single modality of either image (I+A<math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mo stretchy="false" id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\rightarrow</annotation></semantics></math>T), lip (L+A<math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mo stretchy="false" id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">\rightarrow</annotation></semantics></math>T), or OCR (O+A<math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><mo stretchy="false" id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><ci id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">\rightarrow</annotation></semantics></math>T) are -1.6%, -9.2%, and +11.3%, respectively. Hence, only the addition of OCR brings a consistent benefit across noise levels. However, when considering 3-modality combinations, we observe more consistent benefits: adding both image and lip (I+L+A<math id="S3.SS3.p3.4.m4.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.p3.4.m4.1a"><mo stretchy="false" id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><ci id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">\rightarrow</annotation></semantics></math>T), the model surpasses the audio-only model by +3.8%, and adding both OCR and lip (O+L+A<math id="S3.SS3.p3.5.m5.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.p3.5.m5.1a"><mo stretchy="false" id="S3.SS3.p3.5.m5.1.1" xref="S3.SS3.p3.5.m5.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m5.1b"><ci id="S3.SS3.p3.5.m5.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m5.1c">\rightarrow</annotation></semantics></math>T), the model achieves an average benefit of +12%.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.2" class="ltx_p">In addition to varying the input modalities, we explore different representations of visual modality, from implicit to explicit. We consider tokens generated by the image encoder (DALL-E) as <span id="S3.SS3.p4.2.1" class="ltx_text ltx_font_italic">implicit</span> visual representation, OCR as <span id="S3.SS3.p4.2.2" class="ltx_text ltx_font_italic">explicit</span>, and oracle OCR as <span id="S3.SS3.p4.2.3" class="ltx_text ltx_font_italic">perfect explicit</span>. As summarized in Table <a href="#S3.T1" title="TABLE I ‣ III-C Experiment I: How much do additional modalities improve ASR accuracy on average? ‣ III Experiments ‣ Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, the average benefits of these models follow the order: I+A<math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><mo id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><lt id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">&lt;</annotation></semantics></math>O+A<math id="S3.SS3.p4.2.m2.1" class="ltx_Math" alttext="&lt;\textrm{O}_{\textrm{oracle,3}}" display="inline"><semantics id="S3.SS3.p4.2.m2.1a"><mrow id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml"><mi id="S3.SS3.p4.2.m2.1.1.2" xref="S3.SS3.p4.2.m2.1.1.2.cmml"></mi><mo id="S3.SS3.p4.2.m2.1.1.1" xref="S3.SS3.p4.2.m2.1.1.1.cmml">&lt;</mo><msub id="S3.SS3.p4.2.m2.1.1.3" xref="S3.SS3.p4.2.m2.1.1.3.cmml"><mtext id="S3.SS3.p4.2.m2.1.1.3.2" xref="S3.SS3.p4.2.m2.1.1.3.2a.cmml">O</mtext><mtext id="S3.SS3.p4.2.m2.1.1.3.3" xref="S3.SS3.p4.2.m2.1.1.3.3a.cmml">oracle,3</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><apply id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1"><lt id="S3.SS3.p4.2.m2.1.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1.1"></lt><csymbol cd="latexml" id="S3.SS3.p4.2.m2.1.1.2.cmml" xref="S3.SS3.p4.2.m2.1.1.2">absent</csymbol><apply id="S3.SS3.p4.2.m2.1.1.3.cmml" xref="S3.SS3.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p4.2.m2.1.1.3.1.cmml" xref="S3.SS3.p4.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS3.p4.2.m2.1.1.3.2a.cmml" xref="S3.SS3.p4.2.m2.1.1.3.2"><mtext id="S3.SS3.p4.2.m2.1.1.3.2.cmml" xref="S3.SS3.p4.2.m2.1.1.3.2">O</mtext></ci><ci id="S3.SS3.p4.2.m2.1.1.3.3a.cmml" xref="S3.SS3.p4.2.m2.1.1.3.3"><mtext mathsize="70%" id="S3.SS3.p4.2.m2.1.1.3.3.cmml" xref="S3.SS3.p4.2.m2.1.1.3.3">oracle,3</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">&lt;\textrm{O}_{\textrm{oracle,3}}</annotation></semantics></math>+A. This can be attributed to the visual representation becoming more explicit and accurate, making it easier for the model to use, which suggests that better visual representation can lead to better supplementary performance.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">We also compare our model to two Whisper models that are of similar size as ours. Although our model is only trained on about 4k hours of audio (per-training and fine-tuning combined), which is less than 1% of Whisper’s training set, it shows better performance by harnessing multi-modal capabilities. This suggests leveraging more modalities can optimize performance even with a limited amount of data.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.2" class="ltx_p"><span id="S3.SS3.p6.2.1" class="ltx_text ltx_font_bold">SlideAVSR</span>: Based on previous 3-Equations experiments, image data with more text tends to perform better with explicit representation. Therefore, we similarly conduct OCR-based experiments on SlideAVSR. As shown in Table <a href="#S3.T2" title="TABLE II ‣ III-C Experiment I: How much do additional modalities improve ASR accuracy on average? ‣ III Experiments ‣ Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, adding extra OCR modality (<math id="S3.SS3.p6.1.m1.1" class="ltx_Math" alttext="\textrm{O}_{\textrm{ALL}}" display="inline"><semantics id="S3.SS3.p6.1.m1.1a"><msub id="S3.SS3.p6.1.m1.1.1" xref="S3.SS3.p6.1.m1.1.1.cmml"><mtext id="S3.SS3.p6.1.m1.1.1.2" xref="S3.SS3.p6.1.m1.1.1.2a.cmml">O</mtext><mtext id="S3.SS3.p6.1.m1.1.1.3" xref="S3.SS3.p6.1.m1.1.1.3a.cmml">ALL</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.1.m1.1b"><apply id="S3.SS3.p6.1.m1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p6.1.m1.1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p6.1.m1.1.1.2a.cmml" xref="S3.SS3.p6.1.m1.1.1.2"><mtext id="S3.SS3.p6.1.m1.1.1.2.cmml" xref="S3.SS3.p6.1.m1.1.1.2">O</mtext></ci><ci id="S3.SS3.p6.1.m1.1.1.3a.cmml" xref="S3.SS3.p6.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS3.p6.1.m1.1.1.3.cmml" xref="S3.SS3.p6.1.m1.1.1.3">ALL</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.1.m1.1c">\textrm{O}_{\textrm{ALL}}</annotation></semantics></math>+A<math id="S3.SS3.p6.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.p6.2.m2.1a"><mo stretchy="false" id="S3.SS3.p6.2.m2.1.1" xref="S3.SS3.p6.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.2.m2.1b"><ci id="S3.SS3.p6.2.m2.1.1.cmml" xref="S3.SS3.p6.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.2.m2.1c">\rightarrow</annotation></semantics></math>T) shows an average relative benefit of +1.4%, which confirms that adding OCR improves overall recognition performance.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Experiment II: How does the performance of each modality or their combinations vary across noise levels?</span>
</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2409.09221/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="85" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Examples of Gemini’s output on the 3-Equations 2-noise. For a fair comparison, we “help” Gemini spell out some words it recognizes as symbols. REF stands for reference, HYP stands for model’s hypothesis.</figcaption>
</figure>
<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">We explored the hypothesis that the benefits of multimodality would be largest when there is a “medium” amount of noise because (a) if the audio is very clean, the other modalities are unnecessary and (b) if the audio is very noisy, the model cannot find a correspondence between the auditory and visual channels. Results on both datasets are described below.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p"><span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_bold">3-Equations</span>: In Table <a href="#S3.T1" title="TABLE I ‣ III-C Experiment I: How much do additional modalities improve ASR accuracy on average? ‣ III Experiments ‣ Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, when audio is clean, neither image nor lip helps improve recognition performance. However, when looking into higher noise levels, the relative benefit of adding images exhibits a very different trend compared to adding lips. As shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ III-D Experiment II: How does the performance of each modality or their combinations vary across noise levels? ‣ III Experiments ‣ Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, with increasing noise levels, the benefit of lips in enhancing accuracy becomes more amplified, showing the same trend as discussed in previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>. Conversely, the benefit of images follows a trend of first increasing and then decreasing, peaking in the middle when SNR=0dB. This discrepancy could be attributed to image modality not being inherently synchronized with speech, unlike lip movements. Therefore, when above a “sweet spot”, the audio is too noisy for the image to establish a reliable correspondence, thus the relative benefit begins to decline. The benefit of OCR shows a similar trend as image modality, but the sweet spot is likely different from the image due to visual information quality (in this case, SNR=5dB).</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.3" class="ltx_p"><span id="S3.SS4.p3.3.1" class="ltx_text ltx_font_bold">SlideAVSR</span>: The results in Table <a href="#S3.T2" title="TABLE II ‣ III-C Experiment I: How much do additional modalities improve ASR accuracy on average? ‣ III Experiments ‣ Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> and Fig. <a href="#S3.F5" title="Figure 5 ‣ III-D Experiment II: How does the performance of each modality or their combinations vary across noise levels? ‣ III Experiments ‣ Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> show a similar phenomenon: Including the OCR modality (<math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="\textrm{O}_{\textrm{ALL}}" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><msub id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mtext id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2a.cmml">O</mtext><mtext id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3a.cmml">ALL</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.2a.cmml" xref="S3.SS4.p3.1.m1.1.1.2"><mtext id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">O</mtext></ci><ci id="S3.SS4.p3.1.m1.1.1.3a.cmml" xref="S3.SS4.p3.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS4.p3.1.m1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3">ALL</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">\textrm{O}_{\textrm{ALL}}</annotation></semantics></math>+A<math id="S3.SS4.p3.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS4.p3.2.m2.1a"><mo stretchy="false" id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><ci id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">\rightarrow</annotation></semantics></math>T) outperforms the audio-only baseline at low noise levels (<math id="S3.SS4.p3.3.m3.1" class="ltx_Math" alttext="\infty" display="inline"><semantics id="S3.SS4.p3.3.m3.1a"><mi mathvariant="normal" id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml">∞</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><infinity id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1"></infinity></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">\infty</annotation></semantics></math>, 10dB), but performs worse at higher noise levels. The relative benefit of adding OCR initially increases, then decreases, achieving the greatest benefit of +12.4% at SNR=10dB.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2409.09221/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="182" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Relative WER benefit (%) of adding image, OCR(real), or lip modalities on 3-Equations 2-noise test set. The noise levels vary from <math id="S3.F4.3.m1.1" class="ltx_Math" alttext="-\infty" display="inline"><semantics id="S3.F4.3.m1.1b"><mrow id="S3.F4.3.m1.1.1" xref="S3.F4.3.m1.1.1.cmml"><mo id="S3.F4.3.m1.1.1b" xref="S3.F4.3.m1.1.1.cmml">−</mo><mi mathvariant="normal" id="S3.F4.3.m1.1.1.2" xref="S3.F4.3.m1.1.1.2.cmml">∞</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F4.3.m1.1c"><apply id="S3.F4.3.m1.1.1.cmml" xref="S3.F4.3.m1.1.1"><minus id="S3.F4.3.m1.1.1.1.cmml" xref="S3.F4.3.m1.1.1"></minus><infinity id="S3.F4.3.m1.1.1.2.cmml" xref="S3.F4.3.m1.1.1.2"></infinity></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.3.m1.1d">-\infty</annotation></semantics></math>(completely noise) to <math id="S3.F4.4.m2.1" class="ltx_Math" alttext="\infty" display="inline"><semantics id="S3.F4.4.m2.1b"><mi mathvariant="normal" id="S3.F4.4.m2.1.1" xref="S3.F4.4.m2.1.1.cmml">∞</mi><annotation-xml encoding="MathML-Content" id="S3.F4.4.m2.1c"><infinity id="S3.F4.4.m2.1.1.cmml" xref="S3.F4.4.m2.1.1"></infinity></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.4.m2.1d">\infty</annotation></semantics></math>(clean).</figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2409.09221/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="369" height="199" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Relative WER benefit (%) of adding OCR with different K values for FQ Ranker on SlideAVSR test set.</figcaption>
</figure>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.4.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.5.2" class="ltx_text ltx_font_italic">Experiment III: How does irrelevant visual information affect performance?</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.2" class="ltx_p">Since 3-Equations dataset has 3 written equations but only 2 spoken equations in each example, the OCR inputs inherently contain 1/3 irrelevant information. Therefore, we are also interested in how the proportion of irrelevant visual inputs affects performance. On the 3-Equations dataset, we thus add 7 extra irrelevant oracle OCR sentences in addition to the 3 relevant sentences, making a dataset with 4/5 irrelevant inputs. The result is included in Table <a href="#S3.T1" title="TABLE I ‣ III-C Experiment I: How much do additional modalities improve ASR accuracy on average? ‣ III Experiments ‣ Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> (O<math id="S3.SS5.p1.1.m1.1" class="ltx_Math" alttext="{}_{\textrm{oracle,10}}" display="inline"><semantics id="S3.SS5.p1.1.m1.1a"><msub id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml"><mi id="S3.SS5.p1.1.m1.1.1a" xref="S3.SS5.p1.1.m1.1.1.cmml"></mi><mtext id="S3.SS5.p1.1.m1.1.1.1" xref="S3.SS5.p1.1.m1.1.1.1a.cmml">oracle,10</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><apply id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1"><ci id="S3.SS5.p1.1.m1.1.1.1a.cmml" xref="S3.SS5.p1.1.m1.1.1.1"><mtext mathsize="70%" id="S3.SS5.p1.1.m1.1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1.1">oracle,10</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">{}_{\textrm{oracle,10}}</annotation></semantics></math>+A<math id="S3.SS5.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS5.p1.2.m2.1a"><mo stretchy="false" id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><ci id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">\rightarrow</annotation></semantics></math>T). We observe that the overall benefit is much worse than using accurate oracle OCR. This trend confirms that adding more irrelevant visual information will hinder the model from finding the correct information, especially in noisy environments.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.2" class="ltx_p">In SlideAVSR, one slide sample can contain hundreds of OCR words, but only a small proportion is relevant to the speech. Following the work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>, we used FQ Ranker that calculates word ranks based on the frequency of word occurrences in English Wikipedia, and filters the OCR words based on word frequency. We use 10 and 30 as the maximum word count (<math id="S3.SS5.p2.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS5.p2.1.m1.1a"><mi id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><ci id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">K</annotation></semantics></math>) for prompts. This preprocessing step helps us filter the most relevant or long-tail words in OCR words, and conceivably helps the recognition performance. As shown in Table <a href="#S3.T2" title="TABLE II ‣ III-C Experiment I: How much do additional modalities improve ASR accuracy on average? ‣ III Experiments ‣ Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> and Fig. <a href="#S3.F5" title="Figure 5 ‣ III-D Experiment II: How does the performance of each modality or their combinations vary across noise levels? ‣ III Experiments ‣ Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, although the performance of adding OCR is worse than that of audio-only at some noise levels, the relative benefit of OCR generally increases as we filter more stringently (i.e., smaller <math id="S3.SS5.p2.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS5.p2.2.m2.1a"><mi id="S3.SS5.p2.2.m2.1.1" xref="S3.SS5.p2.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><ci id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">K</annotation></semantics></math>).</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Conclusions</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We investigated how multiple modalities impact the accuracy of speech recognition performed by decoder-only discrete speech models. Our experiments suggest that fusing multiple modalities generally enhances recognition performance, but with caveats:
Image information exhibits a different trend from lip movements. Typically, as the noise level increases, the accuracy benefit of the lip information grows larger, whereas images provide the greatest benefit at moderate noise levels. Also, we observe a steady performance improvement when relevant visual information is filtered in preprocessing.
To our knowledge, this paper is the first to show the benefit of combining audio, images, and lip movements in one model.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.2.2.1" class="ltx_text" style="font-size:90%;">[1]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.4.1" class="ltx_text" style="font-size:90%;">
X. Chang, B. Yan, K. Choi, et al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.5.1" class="ltx_text" style="font-size:90%;">“Exploring speech recognition, translation, and understanding with discrete speech units: A comparative study,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.6.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib1.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span id="bib.bib1.8.3" class="ltx_text" style="font-size:90%;">. IEEE, 2024, pp. 11481–11485.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.2.2.1" class="ltx_text" style="font-size:90%;">[2]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.4.1" class="ltx_text" style="font-size:90%;">
G. Team, R. Anil, S. Borgeaud, et al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.5.1" class="ltx_text" style="font-size:90%;">“Gemini: a family of highly capable multimodal models,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2312.11805</span><span id="bib.bib2.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.2.2.1" class="ltx_text" style="font-size:90%;">[3]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.4.1" class="ltx_text" style="font-size:90%;">
V. A. Trinh, R. Southwell, Y. Guan, X. He, Z. Wang, and J. Whitehill,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.5.1" class="ltx_text" style="font-size:90%;">“Discrete multimodal transformers with a pretrained large language model for mixed-supervision speech processing,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2406.06582</span><span id="bib.bib3.7.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.2.2.1" class="ltx_text" style="font-size:90%;">[4]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.4.1" class="ltx_text" style="font-size:90%;">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.5.1" class="ltx_text" style="font-size:90%;">“Robust speech recognition via large-scale weak supervision,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.6.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib4.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib4.8.3" class="ltx_text" style="font-size:90%;">. PMLR, 2023, pp. 28492–28518.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.2.2.1" class="ltx_text" style="font-size:90%;">[5]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.4.1" class="ltx_text" style="font-size:90%;">
T. Afouras, J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.5.1" class="ltx_text" style="font-size:90%;">“Deep audio-visual speech recognition,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span id="bib.bib5.7.2" class="ltx_text" style="font-size:90%;">, vol. 44, no. 12, pp. 8717–8727, 2018.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.2.2.1" class="ltx_text" style="font-size:90%;">[6]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.4.1" class="ltx_text" style="font-size:90%;">
B. Shillingford, Y. Assael, M. W. Hoffman, et al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.5.1" class="ltx_text" style="font-size:90%;">“Large-scale visual speech recognition,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1807.05162</span><span id="bib.bib6.7.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.2.2.1" class="ltx_text" style="font-size:90%;">[7]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.4.1" class="ltx_text" style="font-size:90%;">
J. Achiam, S. Adler, S. Agarwal, et al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.5.1" class="ltx_text" style="font-size:90%;">“Gpt-4 technical report,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.08774</span><span id="bib.bib7.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.2.2.1" class="ltx_text" style="font-size:90%;">[8]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.4.1" class="ltx_text" style="font-size:90%;">
H. Wang, S. Kurita, S. Shimizu, and D. Kawahara,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.5.1" class="ltx_text" style="font-size:90%;">“Slideavsr: A dataset of paper explanation videos for audio-visual speech recognition,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2401.09759</span><span id="bib.bib8.7.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.2.2.1" class="ltx_text" style="font-size:90%;">[9]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.4.1" class="ltx_text" style="font-size:90%;">
D. Ivanko, D. Ryumin, and A. Karpov,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.5.1" class="ltx_text" style="font-size:90%;">“A review of recent advances on deep learning methods for audio-visual speech recognition,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Mathematics</span><span id="bib.bib9.7.2" class="ltx_text" style="font-size:90%;">, vol. 11, no. 12, 2023.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.2.2.1" class="ltx_text" style="font-size:90%;">[10]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.4.1" class="ltx_text" style="font-size:90%;">
D. Ryumin, D. Ivanko, and E. Ryumina,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.5.1" class="ltx_text" style="font-size:90%;">“Audio-visual speech and gesture recognition by sensors of mobile devices,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib10.7.2" class="ltx_text" style="font-size:90%;">, vol. 23, no. 4, pp. 2284, 2023.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.2.2.1" class="ltx_text" style="font-size:90%;">[11]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.4.1" class="ltx_text" style="font-size:90%;">
P. Ma, S. Petridis, and M. Pantic,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.5.1" class="ltx_text" style="font-size:90%;">“End-to-end audio-visual speech recognition with conformers,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.6.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib11.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span id="bib.bib11.8.3" class="ltx_text" style="font-size:90%;">. IEEE, 2021, pp. 7613–7617.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.2.2.1" class="ltx_text" style="font-size:90%;">[12]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.4.1" class="ltx_text" style="font-size:90%;">
Q. Song, B. Sun, and S. Li,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.5.1" class="ltx_text" style="font-size:90%;">“Multimodal sparse transformer network for audio-visual speech recognition,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Neural Networks and Learning Systems</span><span id="bib.bib12.7.2" class="ltx_text" style="font-size:90%;">, vol. 34, no. 12, pp. 10028–10038, 2022.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.2.2.1" class="ltx_text" style="font-size:90%;">[13]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.4.1" class="ltx_text" style="font-size:90%;">
M. Burchi and R. Timofte,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.5.1" class="ltx_text" style="font-size:90%;">“Audio-visual efficient conformer for robust speech recognition,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.6.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib13.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</span><span id="bib.bib13.8.3" class="ltx_text" style="font-size:90%;">, 2023, pp. 2258–2267.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.2.2.1" class="ltx_text" style="font-size:90%;">[14]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.4.1" class="ltx_text" style="font-size:90%;">
B. Shi, W.-N. Hsu, K. Lakhotia, and A. Mohamed,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.5.1" class="ltx_text" style="font-size:90%;">“Learning audio-visual speech representation by masked multimodal cluster prediction,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.6.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib14.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib14.8.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.2.2.1" class="ltx_text" style="font-size:90%;">[15]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.4.1" class="ltx_text" style="font-size:90%;">
B. Shi, W.-N. Hsu, and A. Mohamed,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.5.1" class="ltx_text" style="font-size:90%;">“Robust self-supervised audio-visual speech recognition,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2201.01763</span><span id="bib.bib15.7.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.2.2.1" class="ltx_text" style="font-size:90%;">[16]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.4.1" class="ltx_text" style="font-size:90%;">
P. Ma, A. Haliassos, A. Fernandez-Lopez, H. Chen, S. Petridis, and M. Pantic,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.5.1" class="ltx_text" style="font-size:90%;">“Auto-avsr: Audio-visual speech recognition with automatic labels,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.6.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib16.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span id="bib.bib16.8.3" class="ltx_text" style="font-size:90%;">. IEEE, 2023, pp. 1–5.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.2.2.1" class="ltx_text" style="font-size:90%;">[17]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.4.1" class="ltx_text" style="font-size:90%;">
J. Hong, M. Kim, J. Choi, and Y. M. Ro,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.5.1" class="ltx_text" style="font-size:90%;">“Watch or listen: Robust audio-visual speech recognition with visual corruption modeling and reliability scoring,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.6.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib17.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span><span id="bib.bib17.8.3" class="ltx_text" style="font-size:90%;">. IEEE, 2023, pp. 18783–18794.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.2.2.1" class="ltx_text" style="font-size:90%;">[18]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.4.1" class="ltx_text" style="font-size:90%;">
H. Wang, F. Yu, X. Shi, Y. Wang, S. Zhang, and M. Li,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.5.1" class="ltx_text" style="font-size:90%;">“Slidespeech: A large scale slide-enriched audio-visual corpus,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.6.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib18.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</span><span id="bib.bib18.8.3" class="ltx_text" style="font-size:90%;">. IEEE, 2024, pp. 11076–11080.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.2.2.1" class="ltx_text" style="font-size:90%;">[19]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.4.1" class="ltx_text" style="font-size:90%;">
D. Zhang, Y. Yu, C. Li, et al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.5.1" class="ltx_text" style="font-size:90%;">“Mm-llms: Recent advances in multimodal large language models,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2401.13601</span><span id="bib.bib19.7.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.2.2.1" class="ltx_text" style="font-size:90%;">[20]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.4.1" class="ltx_text" style="font-size:90%;">
Y. Chu, J. Xu, X. Zhou, et al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.5.1" class="ltx_text" style="font-size:90%;">“Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2311.07919</span><span id="bib.bib20.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.2.2.1" class="ltx_text" style="font-size:90%;">[21]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.4.1" class="ltx_text" style="font-size:90%;">
H. Liu, C. Li, Q. Wu, and Y. J. Lee,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.5.1" class="ltx_text" style="font-size:90%;">“Visual instruction tuning,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib21.7.2" class="ltx_text" style="font-size:90%;">, vol. 36, 2024.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.2.2.1" class="ltx_text" style="font-size:90%;">[22]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.4.1" class="ltx_text" style="font-size:90%;">
K. Zheng, X. He, and X. E. Wang,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.5.1" class="ltx_text" style="font-size:90%;">“Minigpt-5: Interleaved vision-and-language generation via generative vokens,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2310.02239</span><span id="bib.bib22.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.2.2.1" class="ltx_text" style="font-size:90%;">[23]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.4.1" class="ltx_text" style="font-size:90%;">
S. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.5.1" class="ltx_text" style="font-size:90%;">“Next-gpt: Any-to-any multimodal llm,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.6.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib23.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Forty-first International Conference on Machine Learning</span><span id="bib.bib23.8.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.2.2.1" class="ltx_text" style="font-size:90%;">[24]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.4.1" class="ltx_text" style="font-size:90%;">
J. H. Yeo, S. Han, M. Kim, and Y. M. Ro,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.5.1" class="ltx_text" style="font-size:90%;">“Where visual speech meets language: Vsp-llm framework for efficient and context-aware visual speech processing,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2402.15151</span><span id="bib.bib24.7.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.2.2.1" class="ltx_text" style="font-size:90%;">[25]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.4.1" class="ltx_text" style="font-size:90%;">
S. Zhang, S. Roller, N. Goyal, et al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.5.1" class="ltx_text" style="font-size:90%;">“Opt: Open pre-trained transformer language models,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2205.01068</span><span id="bib.bib25.7.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.2.2.1" class="ltx_text" style="font-size:90%;">[26]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.4.1" class="ltx_text" style="font-size:90%;">
L. Barrault, Y.-A. Chung, M. C. Meglioli, et al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.5.1" class="ltx_text" style="font-size:90%;">“Seamlessm4t-massively multilingual &amp; multimodal machine translation,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2308.11596</span><span id="bib.bib26.7.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.2.2.1" class="ltx_text" style="font-size:90%;">[27]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.4.1" class="ltx_text" style="font-size:90%;">
A. Ramesh, M. Pavlov, G. Goh, et al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.5.1" class="ltx_text" style="font-size:90%;">“Zero-shot text-to-image generation,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.6.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib27.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib27.8.3" class="ltx_text" style="font-size:90%;">. Pmlr, 2021, pp. 8821–8831.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.2.2.1" class="ltx_text" style="font-size:90%;">[28]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.4.1" class="ltx_text" style="font-size:90%;">
V. Panayotov, G. Chen, D. Povey, and S. Khudanpur,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.5.1" class="ltx_text" style="font-size:90%;">“Librispeech: an asr corpus based on public domain audio books,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.6.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib28.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)</span><span id="bib.bib28.8.3" class="ltx_text" style="font-size:90%;">. IEEE, 2015, pp. 5206–5210.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.2.2.1" class="ltx_text" style="font-size:90%;">[29]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.4.1" class="ltx_text" style="font-size:90%;">
Y. Jia, M. T. Ramanovich, Q. Wang, and H. Zen,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.5.1" class="ltx_text" style="font-size:90%;">“Cvss corpus and massively multilingual speech-to-speech translation,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2201.03713</span><span id="bib.bib29.7.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.2.2.1" class="ltx_text" style="font-size:90%;">[30]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.4.1" class="ltx_text" style="font-size:90%;">
C. Wang, A. Wu, and J. Pino,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.5.1" class="ltx_text" style="font-size:90%;">“Covost 2 and massively multilingual speech-to-text translation,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2007.10310</span><span id="bib.bib30.7.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.2.2.1" class="ltx_text" style="font-size:90%;">[31]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.4.1" class="ltx_text" style="font-size:90%;">
T.-Y. Lin, M. Maire, S. Belongie, et al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.5.1" class="ltx_text" style="font-size:90%;">“Microsoft coco: Common objects in context,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.6.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib31.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</span><span id="bib.bib31.8.3" class="ltx_text" style="font-size:90%;">. Springer, 2014, pp. 740–755.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.2.2.1" class="ltx_text" style="font-size:90%;">[32]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.4.1" class="ltx_text" style="font-size:90%;">
I. Loshchilov and F. Hutter,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.5.1" class="ltx_text" style="font-size:90%;">“Decoupled weight decay regularization,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.6.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib32.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib32.8.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.2.2.1" class="ltx_text" style="font-size:90%;">[33]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.4.1" class="ltx_text" style="font-size:90%;">
“pyttsx3: Offline text to speech (tts) converter for python,” </span><span id="bib.bib33.5.2" class="ltx_ERROR undefined">\url</span><span id="bib.bib33.6.3" class="ltx_text" style="font-size:90%;">https://github.com/nateshmbhat/pyttsx3.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.2.2.1" class="ltx_text" style="font-size:90%;">[34]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.4.1" class="ltx_text" style="font-size:90%;">
“Easyocr: Ready-to-use ocr with 80+ supported languages,” </span><span id="bib.bib34.5.2" class="ltx_ERROR undefined">\url</span><span id="bib.bib34.6.3" class="ltx_text" style="font-size:90%;">https://github.com/JaidedAI/EasyOCR.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.2.2.1" class="ltx_text" style="font-size:90%;">[35]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.4.1" class="ltx_text" style="font-size:90%;">
K. Prajwal, R. Mukhopadhyay, V. P. Namboodiri, and C. Jawahar,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.5.1" class="ltx_text" style="font-size:90%;">“A lip sync expert is all you need for speech to lip generation in the wild,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.6.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib35.7.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 28th ACM international conference on multimedia</span><span id="bib.bib35.8.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 484–492.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.2.2.1" class="ltx_text" style="font-size:90%;">[36]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.4.1" class="ltx_text" style="font-size:90%;">
D. Snyder, G. Chen, and D. Povey,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.5.1" class="ltx_text" style="font-size:90%;">“Musan: A music, speech, and noise corpus,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1510.08484</span><span id="bib.bib36.7.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.09219" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.09221" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.09221">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.09221" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.09223" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 21:10:32 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
