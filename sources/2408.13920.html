<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.13920] Wav2Small: Distilling Wav2Vec2 to 72K parameters for Low-Resource Speech emotion recognition</title><meta property="og:description" content="Speech Emotion Recognition (SER) needs high computational resources to overcome the challenge of substantial annotator disagreement. Today SER is shifting towards dimensional annotations of arousal, dominance, and vale…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Wav2Small: Distilling Wav2Vec2 to 72K parameters for Low-Resource Speech emotion recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Wav2Small: Distilling Wav2Vec2 to 72K parameters for Low-Resource Speech emotion recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.13920">

<!--Generated on Thu Sep  5 15:56:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Wav2Small: Distilling Wav2Vec2 to 72K parameters for Low-Resource Speech emotion recognition</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Speech Emotion Recognition (SER) needs high computational resources to overcome the challenge of substantial annotator disagreement. Today SER is shifting towards dimensional annotations of arousal, dominance, and valence (A/D/V). Universal metrics as the L2 distance prove unsuitable for evaluating A/D/V accuracy due to non converging consensus of annotator opinions. However, Concordance Correlation Coefficient (CCC) arose as an alternative metric for A/D/V where a model’s output is evaluated to match a whole dataset’s CCC rather than L2 distances of individual audios. Recent studies have shown that wav2vec2.0 / wavLM architectures outputing a float value for each A/D/V dimension achieve today’s State-of-the-art (Sota) CCC on A/D/V. The Wav2Vec2.0 / WavLm family has a high computational footprint, but training small models using human annotations has been unsuccessful<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> trained a VGG14 of 4.9M params on MSP Podcast v1.7 using only human annotations. This resulted in very low CCC especially for valence: Their VGG14 achieved only 0.248 valence CCC vs 0.638 achieved by thier Wav2Vec2 (Dawn).</span></span></span>. In this paper we use a large Transformer Sota A/D/V model as Teacher/Annotator to train 5 student models: 4 MobileNets and our proposed Wav2Small, using only the Teacher’s A/D/V predictions instead of human annotations. The Teacher model we construct also sets a new Sota on the MSP Podcast dataset. We choose MobileNetV4 / MobileNet-V3 as students, as MobileNet has been designed for fast execution times. We also propose Wav2Small – an architecture designed for minimal parameter number and RAM consumption. Wav2Small with an .onnx (quantized) of only 120KB is a potential solution for A/D/V on hardware with low resources, having only 72K parameters<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Convolution followed by BatchNorm then ReLU is a fused op in .onnx.</span></span></span> vs 3.12M parameters for MobileNet-V4-Small. <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>© 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.</span></span></span></p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">— </span></span>
speech emotion recognition, MobileNetV4</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>INTRODUCTION</h2>

<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.26.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>A/D/V model’s resources at 32bit non quantized .onnx. All onnx take 16KHz audio as input and include all parameters, preprocessing and LogMel Spectrogram extraction. Values from Intel(R) Xeon(R) Gold 6226R CPU @ 2.90GHz.</figcaption>
<table id="S1.T1.24" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.T1.24.25.1" class="ltx_tr">
<td id="S1.T1.24.25.1.1" class="ltx_td ltx_nopad_r ltx_border_tt"></td>
<th id="S1.T1.24.25.1.2" class="ltx_td ltx_nopad_r ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S1.T1.24.25.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">5s audio input</th>
</tr>
<tr id="S1.T1.24.26.2" class="ltx_tr">
<td id="S1.T1.24.26.2.1" class="ltx_td ltx_nopad_r"></td>
<th id="S1.T1.24.26.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column">
<table id="S1.T1.24.26.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.24.26.2.2.1.1" class="ltx_tr">
<td id="S1.T1.24.26.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Param.</td>
</tr>
<tr id="S1.T1.24.26.2.2.1.2" class="ltx_tr">
<td id="S1.T1.24.26.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(M)</td>
</tr>
</table>
</th>
<th id="S1.T1.24.26.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">
<table id="S1.T1.24.26.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.24.26.2.3.1.1" class="ltx_tr">
<td id="S1.T1.24.26.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Peak</td>
</tr>
<tr id="S1.T1.24.26.2.3.1.2" class="ltx_tr">
<td id="S1.T1.24.26.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">RSS</td>
</tr>
<tr id="S1.T1.24.26.2.3.1.3" class="ltx_tr">
<td id="S1.T1.24.26.2.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">(MB)</td>
</tr>
</table>
</th>
<th id="S1.T1.24.26.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">
<table id="S1.T1.24.26.2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.24.26.2.4.1.1" class="ltx_tr">
<td id="S1.T1.24.26.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">MAC</td>
</tr>
</table>
</th>
<th id="S1.T1.24.26.2.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">
<table id="S1.T1.24.26.2.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.24.26.2.5.1.1" class="ltx_tr">
<td id="S1.T1.24.26.2.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Time CPU</td>
</tr>
<tr id="S1.T1.24.26.2.5.1.2" class="ltx_tr">
<td id="S1.T1.24.26.2.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(ms)</td>
</tr>
</table>
</th>
</tr>
<tr id="S1.T1.3.3" class="ltx_tr">
<td id="S1.T1.3.3.4" class="ltx_td ltx_align_left ltx_border_t">Teacher</td>
<td id="S1.T1.1.1.1" class="ltx_td ltx_align_left ltx_border_t"><math id="S1.T1.1.1.1.m1.1" class="ltx_Math" alttext="483.9" display="inline"><semantics id="S1.T1.1.1.1.m1.1a"><mn id="S1.T1.1.1.1.m1.1.1" xref="S1.T1.1.1.1.m1.1.1.cmml">483.9</mn><annotation-xml encoding="MathML-Content" id="S1.T1.1.1.1.m1.1b"><cn type="float" id="S1.T1.1.1.1.m1.1.1.cmml" xref="S1.T1.1.1.1.m1.1.1">483.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.1.1.1.m1.1c">483.9</annotation></semantics></math></td>
<td id="S1.T1.2.2.2" class="ltx_td ltx_align_left ltx_border_t"><math id="S1.T1.2.2.2.m1.1" class="ltx_Math" alttext="1929" display="inline"><semantics id="S1.T1.2.2.2.m1.1a"><mn id="S1.T1.2.2.2.m1.1.1" xref="S1.T1.2.2.2.m1.1.1.cmml">1929</mn><annotation-xml encoding="MathML-Content" id="S1.T1.2.2.2.m1.1b"><cn type="integer" id="S1.T1.2.2.2.m1.1.1.cmml" xref="S1.T1.2.2.2.m1.1.1">1929</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.2.2.2.m1.1c">1929</annotation></semantics></math></td>
<td id="S1.T1.3.3.5" class="ltx_td ltx_align_left ltx_border_t">149G</td>
<td id="S1.T1.3.3.3" class="ltx_td ltx_align_left ltx_border_t"><math id="S1.T1.3.3.3.m1.1" class="ltx_Math" alttext="1089" display="inline"><semantics id="S1.T1.3.3.3.m1.1a"><mn id="S1.T1.3.3.3.m1.1.1" xref="S1.T1.3.3.3.m1.1.1.cmml">1089</mn><annotation-xml encoding="MathML-Content" id="S1.T1.3.3.3.m1.1b"><cn type="integer" id="S1.T1.3.3.3.m1.1.1.cmml" xref="S1.T1.3.3.3.m1.1.1">1089</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.3.3.3.m1.1c">1089</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.6.6" class="ltx_tr">
<td id="S1.T1.6.6.4" class="ltx_td ltx_align_left">WavLM</td>
<td id="S1.T1.4.4.1" class="ltx_td ltx_align_left"><math id="S1.T1.4.4.1.m1.1" class="ltx_Math" alttext="318.6" display="inline"><semantics id="S1.T1.4.4.1.m1.1a"><mn id="S1.T1.4.4.1.m1.1.1" xref="S1.T1.4.4.1.m1.1.1.cmml">318.6</mn><annotation-xml encoding="MathML-Content" id="S1.T1.4.4.1.m1.1b"><cn type="float" id="S1.T1.4.4.1.m1.1.1.cmml" xref="S1.T1.4.4.1.m1.1.1">318.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.4.4.1.m1.1c">318.6</annotation></semantics></math></td>
<td id="S1.T1.5.5.2" class="ltx_td ltx_align_left"><math id="S1.T1.5.5.2.m1.1" class="ltx_Math" alttext="1284" display="inline"><semantics id="S1.T1.5.5.2.m1.1a"><mn id="S1.T1.5.5.2.m1.1.1" xref="S1.T1.5.5.2.m1.1.1.cmml">1284</mn><annotation-xml encoding="MathML-Content" id="S1.T1.5.5.2.m1.1b"><cn type="integer" id="S1.T1.5.5.2.m1.1.1.cmml" xref="S1.T1.5.5.2.m1.1.1">1284</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.5.5.2.m1.1c">1284</annotation></semantics></math></td>
<td id="S1.T1.6.6.5" class="ltx_td ltx_align_left">95G</td>
<td id="S1.T1.6.6.3" class="ltx_td ltx_align_left"><math id="S1.T1.6.6.3.m1.1" class="ltx_Math" alttext="688" display="inline"><semantics id="S1.T1.6.6.3.m1.1a"><mn id="S1.T1.6.6.3.m1.1.1" xref="S1.T1.6.6.3.m1.1.1.cmml">688</mn><annotation-xml encoding="MathML-Content" id="S1.T1.6.6.3.m1.1b"><cn type="integer" id="S1.T1.6.6.3.m1.1.1.cmml" xref="S1.T1.6.6.3.m1.1.1">688</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.6.6.3.m1.1c">688</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.9.9" class="ltx_tr">
<td id="S1.T1.9.9.4" class="ltx_td ltx_align_left">Dawn</td>
<td id="S1.T1.7.7.1" class="ltx_td ltx_align_left"><math id="S1.T1.7.7.1.m1.1" class="ltx_Math" alttext="165.3" display="inline"><semantics id="S1.T1.7.7.1.m1.1a"><mn id="S1.T1.7.7.1.m1.1.1" xref="S1.T1.7.7.1.m1.1.1.cmml">165.3</mn><annotation-xml encoding="MathML-Content" id="S1.T1.7.7.1.m1.1b"><cn type="float" id="S1.T1.7.7.1.m1.1.1.cmml" xref="S1.T1.7.7.1.m1.1.1">165.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.7.7.1.m1.1c">165.3</annotation></semantics></math></td>
<td id="S1.T1.8.8.2" class="ltx_td ltx_align_left"><math id="S1.T1.8.8.2.m1.1" class="ltx_Math" alttext="697" display="inline"><semantics id="S1.T1.8.8.2.m1.1a"><mn id="S1.T1.8.8.2.m1.1.1" xref="S1.T1.8.8.2.m1.1.1.cmml">697</mn><annotation-xml encoding="MathML-Content" id="S1.T1.8.8.2.m1.1b"><cn type="integer" id="S1.T1.8.8.2.m1.1.1.cmml" xref="S1.T1.8.8.2.m1.1.1">697</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.8.8.2.m1.1c">697</annotation></semantics></math></td>
<td id="S1.T1.9.9.5" class="ltx_td ltx_align_left">55G</td>
<td id="S1.T1.9.9.3" class="ltx_td ltx_align_left"><math id="S1.T1.9.9.3.m1.1" class="ltx_Math" alttext="372" display="inline"><semantics id="S1.T1.9.9.3.m1.1a"><mn id="S1.T1.9.9.3.m1.1.1" xref="S1.T1.9.9.3.m1.1.1.cmml">372</mn><annotation-xml encoding="MathML-Content" id="S1.T1.9.9.3.m1.1b"><cn type="integer" id="S1.T1.9.9.3.m1.1.1.cmml" xref="S1.T1.9.9.3.m1.1.1">372</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.9.9.3.m1.1c">372</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.12.12" class="ltx_tr">
<td id="S1.T1.12.12.4" class="ltx_td ltx_align_left">MobileNetV4-L</td>
<td id="S1.T1.10.10.1" class="ltx_td ltx_align_left"><math id="S1.T1.10.10.1.m1.1" class="ltx_Math" alttext="31.87" display="inline"><semantics id="S1.T1.10.10.1.m1.1a"><mn id="S1.T1.10.10.1.m1.1.1" xref="S1.T1.10.10.1.m1.1.1.cmml">31.87</mn><annotation-xml encoding="MathML-Content" id="S1.T1.10.10.1.m1.1b"><cn type="float" id="S1.T1.10.10.1.m1.1.1.cmml" xref="S1.T1.10.10.1.m1.1.1">31.87</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.10.10.1.m1.1c">31.87</annotation></semantics></math></td>
<td id="S1.T1.11.11.2" class="ltx_td ltx_align_left"><math id="S1.T1.11.11.2.m1.1" class="ltx_Math" alttext="257" display="inline"><semantics id="S1.T1.11.11.2.m1.1a"><mn id="S1.T1.11.11.2.m1.1.1" xref="S1.T1.11.11.2.m1.1.1.cmml">257</mn><annotation-xml encoding="MathML-Content" id="S1.T1.11.11.2.m1.1b"><cn type="integer" id="S1.T1.11.11.2.m1.1.1.cmml" xref="S1.T1.11.11.2.m1.1.1">257</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.11.11.2.m1.1c">257</annotation></semantics></math></td>
<td id="S1.T1.12.12.5" class="ltx_td ltx_align_left">2.2G</td>
<td id="S1.T1.12.12.3" class="ltx_td ltx_align_left"><math id="S1.T1.12.12.3.m1.1" class="ltx_Math" alttext="24" display="inline"><semantics id="S1.T1.12.12.3.m1.1a"><mn id="S1.T1.12.12.3.m1.1.1" xref="S1.T1.12.12.3.m1.1.1.cmml">24</mn><annotation-xml encoding="MathML-Content" id="S1.T1.12.12.3.m1.1b"><cn type="integer" id="S1.T1.12.12.3.m1.1.1.cmml" xref="S1.T1.12.12.3.m1.1.1">24</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.12.12.3.m1.1c">24</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.15.15" class="ltx_tr">
<td id="S1.T1.15.15.4" class="ltx_td ltx_align_left">MobileNetV4-M</td>
<td id="S1.T1.13.13.1" class="ltx_td ltx_align_left"><math id="S1.T1.13.13.1.m1.1" class="ltx_Math" alttext="10.38" display="inline"><semantics id="S1.T1.13.13.1.m1.1a"><mn id="S1.T1.13.13.1.m1.1.1" xref="S1.T1.13.13.1.m1.1.1.cmml">10.38</mn><annotation-xml encoding="MathML-Content" id="S1.T1.13.13.1.m1.1b"><cn type="float" id="S1.T1.13.13.1.m1.1.1.cmml" xref="S1.T1.13.13.1.m1.1.1">10.38</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.13.13.1.m1.1c">10.38</annotation></semantics></math></td>
<td id="S1.T1.14.14.2" class="ltx_td ltx_align_left"><math id="S1.T1.14.14.2.m1.1" class="ltx_Math" alttext="94" display="inline"><semantics id="S1.T1.14.14.2.m1.1a"><mn id="S1.T1.14.14.2.m1.1.1" xref="S1.T1.14.14.2.m1.1.1.cmml">94</mn><annotation-xml encoding="MathML-Content" id="S1.T1.14.14.2.m1.1b"><cn type="integer" id="S1.T1.14.14.2.m1.1.1.cmml" xref="S1.T1.14.14.2.m1.1.1">94</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.14.14.2.m1.1c">94</annotation></semantics></math></td>
<td id="S1.T1.15.15.5" class="ltx_td ltx_align_left">1.7G</td>
<td id="S1.T1.15.15.3" class="ltx_td ltx_align_left"><math id="S1.T1.15.15.3.m1.1" class="ltx_Math" alttext="13" display="inline"><semantics id="S1.T1.15.15.3.m1.1a"><mn id="S1.T1.15.15.3.m1.1.1" xref="S1.T1.15.15.3.m1.1.1.cmml">13</mn><annotation-xml encoding="MathML-Content" id="S1.T1.15.15.3.m1.1b"><cn type="integer" id="S1.T1.15.15.3.m1.1.1.cmml" xref="S1.T1.15.15.3.m1.1.1">13</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.15.15.3.m1.1c">13</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.18.18" class="ltx_tr">
<td id="S1.T1.18.18.4" class="ltx_td ltx_align_left">MobileNetV3-S</td>
<td id="S1.T1.16.16.1" class="ltx_td ltx_align_left"><math id="S1.T1.16.16.1.m1.1" class="ltx_Math" alttext="3.14" display="inline"><semantics id="S1.T1.16.16.1.m1.1a"><mn id="S1.T1.16.16.1.m1.1.1" xref="S1.T1.16.16.1.m1.1.1.cmml">3.14</mn><annotation-xml encoding="MathML-Content" id="S1.T1.16.16.1.m1.1b"><cn type="float" id="S1.T1.16.16.1.m1.1.1.cmml" xref="S1.T1.16.16.1.m1.1.1">3.14</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.16.16.1.m1.1c">3.14</annotation></semantics></math></td>
<td id="S1.T1.17.17.2" class="ltx_td ltx_align_left"><math id="S1.T1.17.17.2.m1.1" class="ltx_Math" alttext="22" display="inline"><semantics id="S1.T1.17.17.2.m1.1a"><mn id="S1.T1.17.17.2.m1.1.1" xref="S1.T1.17.17.2.m1.1.1.cmml">22</mn><annotation-xml encoding="MathML-Content" id="S1.T1.17.17.2.m1.1b"><cn type="integer" id="S1.T1.17.17.2.m1.1.1.cmml" xref="S1.T1.17.17.2.m1.1.1">22</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.17.17.2.m1.1c">22</annotation></semantics></math></td>
<td id="S1.T1.18.18.5" class="ltx_td ltx_align_left">400M</td>
<td id="S1.T1.18.18.3" class="ltx_td ltx_align_left"><math id="S1.T1.18.18.3.m1.1" class="ltx_Math" alttext="11" display="inline"><semantics id="S1.T1.18.18.3.m1.1a"><mn id="S1.T1.18.18.3.m1.1.1" xref="S1.T1.18.18.3.m1.1.1.cmml">11</mn><annotation-xml encoding="MathML-Content" id="S1.T1.18.18.3.m1.1b"><cn type="integer" id="S1.T1.18.18.3.m1.1.1.cmml" xref="S1.T1.18.18.3.m1.1.1">11</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.18.18.3.m1.1c">11</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.21.21" class="ltx_tr">
<td id="S1.T1.21.21.4" class="ltx_td ltx_align_left">MobileNetV4-S</td>
<td id="S1.T1.19.19.1" class="ltx_td ltx_align_left"><math id="S1.T1.19.19.1.m1.1" class="ltx_Math" alttext="3.12" display="inline"><semantics id="S1.T1.19.19.1.m1.1a"><mn id="S1.T1.19.19.1.m1.1.1" xref="S1.T1.19.19.1.m1.1.1.cmml">3.12</mn><annotation-xml encoding="MathML-Content" id="S1.T1.19.19.1.m1.1b"><cn type="float" id="S1.T1.19.19.1.m1.1.1.cmml" xref="S1.T1.19.19.1.m1.1.1">3.12</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.19.19.1.m1.1c">3.12</annotation></semantics></math></td>
<td id="S1.T1.20.20.2" class="ltx_td ltx_align_left"><math id="S1.T1.20.20.2.m1.1" class="ltx_Math" alttext="36" display="inline"><semantics id="S1.T1.20.20.2.m1.1a"><mn id="S1.T1.20.20.2.m1.1.1" xref="S1.T1.20.20.2.m1.1.1.cmml">36</mn><annotation-xml encoding="MathML-Content" id="S1.T1.20.20.2.m1.1b"><cn type="integer" id="S1.T1.20.20.2.m1.1.1.cmml" xref="S1.T1.20.20.2.m1.1.1">36</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.20.20.2.m1.1c">36</annotation></semantics></math></td>
<td id="S1.T1.21.21.5" class="ltx_td ltx_align_left">417M</td>
<td id="S1.T1.21.21.3" class="ltx_td ltx_align_left"><math id="S1.T1.21.21.3.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S1.T1.21.21.3.m1.1a"><mn id="S1.T1.21.21.3.m1.1.1" xref="S1.T1.21.21.3.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S1.T1.21.21.3.m1.1b"><cn type="integer" id="S1.T1.21.21.3.m1.1.1.cmml" xref="S1.T1.21.21.3.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.21.21.3.m1.1c">5</annotation></semantics></math></td>
</tr>
<tr id="S1.T1.24.24" class="ltx_tr">
<td id="S1.T1.24.24.4" class="ltx_td ltx_align_left ltx_border_bb">Wav2Small</td>
<td id="S1.T1.22.22.1" class="ltx_td ltx_align_left ltx_border_bb"><math id="S1.T1.22.22.1.m1.1" class="ltx_Math" alttext="0.072" display="inline"><semantics id="S1.T1.22.22.1.m1.1a"><mn id="S1.T1.22.22.1.m1.1.1" xref="S1.T1.22.22.1.m1.1.1.cmml">0.072</mn><annotation-xml encoding="MathML-Content" id="S1.T1.22.22.1.m1.1b"><cn type="float" id="S1.T1.22.22.1.m1.1.1.cmml" xref="S1.T1.22.22.1.m1.1.1">0.072</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.22.22.1.m1.1c">0.072</annotation></semantics></math></td>
<td id="S1.T1.23.23.2" class="ltx_td ltx_align_left ltx_border_bb"><math id="S1.T1.23.23.2.m1.1" class="ltx_Math" alttext="9" display="inline"><semantics id="S1.T1.23.23.2.m1.1a"><mn id="S1.T1.23.23.2.m1.1.1" xref="S1.T1.23.23.2.m1.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S1.T1.23.23.2.m1.1b"><cn type="integer" id="S1.T1.23.23.2.m1.1.1.cmml" xref="S1.T1.23.23.2.m1.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.23.23.2.m1.1c">9</annotation></semantics></math></td>
<td id="S1.T1.24.24.5" class="ltx_td ltx_align_left ltx_border_bb">416M</td>
<td id="S1.T1.24.24.3" class="ltx_td ltx_align_left ltx_border_bb"><math id="S1.T1.24.24.3.m1.1" class="ltx_Math" alttext="9" display="inline"><semantics id="S1.T1.24.24.3.m1.1a"><mn id="S1.T1.24.24.3.m1.1.1" xref="S1.T1.24.24.3.m1.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S1.T1.24.24.3.m1.1b"><cn type="integer" id="S1.T1.24.24.3.m1.1.1.cmml" xref="S1.T1.24.24.3.m1.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.24.24.3.m1.1c">9</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">A need for SER on low resource hardware, drives us to investigate small architectures for A/D/V. Neural Architecture Search has birthed useful architectures for categorical emotions SER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. However, Sota A/D/V is dominated by Transformer models, such as Wav2Vec2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> or WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, both having a VGG7 audio input extractor. A VGG architecture is chosen because the skip connection of ResNet has high RAM footprint.
Conventional components such as Convolution, BatchNorm, ReLU have hardware inter-compatibility <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. In this paper we use distillation to train 5 small student architectures for A/D/V. We train <math id="S1.p1.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S1.p1.1.m1.1a"><mn id="S1.p1.1.m1.1.1" xref="S1.p1.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.1b"><cn type="integer" id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.1c">3</annotation></semantics></math> MobileNetV4<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>We use the Top-1 pretrained models from https://huggingface.co/timm/ 
<br class="ltx_break">mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k 
<br class="ltx_break">mobilenetv4_hybrid_medium.ix_e550_r384_in1k 
<br class="ltx_break">mobilenetv4_conv_small.e1200_r224_in1k.</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> (-Large, -Medium, -Small), as well as a MobileNetV3-Small <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite><span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>pretrained on Audioset.</span></span></span>, as well as our proposed architecture named Wav2Small.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Wav2Small</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">The paradigm of a VGG feature extractor followed by transformer layers has shown great performance for Speech
Tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. We notice that MobileNets do not investigate re-purposing of tokens/time-frames as convolution channels. Transformer pretrained architectures have been found superior than LogMel (Spectrogram) for SER, yet LogMel is still
superior than time samples for small architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. We propose Wav2Small: a VGG7 with LogMel stem followed
by vectorisation of tokens into the convolution-channels dimension, shown in Listing <a href="#S1.F1" title="Figure 1 ‣ 1.1 Wav2Small ‣ 1 INTRODUCTION ‣ Wav2Small: Distilling Wav2Vec2 to 72K parameters for Low-Resource Speech emotion recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Wav2Small does not reduce
drastically time-frames as MobileNets thus it can be used
as inexpensive feature extractor for large transformer architectures, of the Wav2Vec2/WavLM family. Nonetheless
Wav2Small has equal computation as MobileNetV4-S albeit
only with 72K parameters and less RAM, as shown in Table <a href="#S1.T1" title="Table 1 ‣ 1 INTRODUCTION ‣ Wav2Small: Distilling Wav2Vec2 to 72K parameters for Low-Resource Speech emotion recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<p id="S1.F1.2" class="ltx_p"><span id="S1.F1.2.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;"><span id="S1.F1.2.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;"></span><span id="S1.F1.2.1.2" class="ltx_text ltx_inline-block" style="width:433.6pt;"><span id="S1.F1.2.1.2.1" class="ltx_text ltx_font_bold" style="color:#008000;">from</span> <span id="S1.F1.2.1.2.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">torch</span> <span id="S1.F1.2.1.2.3" class="ltx_text ltx_font_bold" style="color:#008000;">import</span> nn</span></span><span id="S1.F1.2.2" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;"><span id="S1.F1.2.2.1" class="ltx_text ltx_font_bold" style="color:#008000;">from</span> <span id="S1.F1.2.2.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">torchlibrosa</span> <span id="S1.F1.2.2.3" class="ltx_text ltx_font_bold" style="color:#008000;">import</span></span><span id="S1.F1.2.3" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">  Spectrogram, LogmelFilterBank</span><span id="S1.F1.2.4" class="ltx_text ltx_inline-block" style="width:433.6pt;"></span><span id="S1.F1.2.5" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;"><span id="S1.F1.2.5.1" class="ltx_text ltx_font_bold" style="color:#008000;">def</span> <span id="S1.F1.2.5.2" class="ltx_text" style="color:#0000FF;">conv</span>(<span id="S1.F1.2.5.3" class="ltx_text ltx_font_bold" style="color:#AB21FF;">in</span><span id="S1.F1.2.5.4" class="ltx_text" style="color:#666666;">=<span id="S1.F1.2.5.4.1" class="ltx_text">13</span></span>):</span><span id="S1.F1.2.6" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">  <span id="S1.F1.2.6.1" class="ltx_text ltx_font_bold" style="color:#008000;">return</span> nn<span id="S1.F1.2.6.2" class="ltx_text" style="color:#666666;">.</span>Sequential(nn<span id="S1.F1.2.6.3" class="ltx_text" style="color:#666666;">.</span>Conv2d(<span id="S1.F1.2.6.4" class="ltx_text ltx_font_bold" style="color:#AB21FF;">in</span>,<span id="S1.F1.2.6.5" class="ltx_text" style="color:#666666;">13</span>,k<span id="S1.F1.2.6.6" class="ltx_text" style="color:#666666;">=<span id="S1.F1.2.6.6.1" class="ltx_text">3</span></span>,pad<span id="S1.F1.2.6.7" class="ltx_text" style="color:#666666;">=<span id="S1.F1.2.6.7.1" class="ltx_text">1</span></span>,</span><span id="S1.F1.2.7" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">      bias<span id="S1.F1.2.7.1" class="ltx_text" style="color:#666666;">=<span id="S1.F1.2.7.1.1" class="ltx_text ltx_font_bold" style="color:#008000;">False</span></span>), nn<span id="S1.F1.2.7.2" class="ltx_text" style="color:#666666;">.</span>BatchNorm2d(<span id="S1.F1.2.7.3" class="ltx_text" style="color:#666666;">13</span>), nn<span id="S1.F1.2.7.4" class="ltx_text" style="color:#666666;">.</span>ReLU())</span><span id="S1.F1.2.8" class="ltx_text ltx_inline-block" style="width:433.6pt;"></span><span id="S1.F1.2.9" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;"><span id="S1.F1.2.9.1" class="ltx_text ltx_font_bold" style="color:#008000;">class</span> <span id="S1.F1.2.9.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">Wav2Small</span>():</span><span id="S1.F1.2.10" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">  <span id="S1.F1.2.10.1" class="ltx_text ltx_font_bold" style="color:#008000;">def</span> <span id="S1.F1.2.10.2" class="ltx_text" style="color:#0000FF;">__init__</span>():</span><span id="S1.F1.2.11" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">    <span id="S1.F1.2.11.1" class="ltx_text" style="color:#008000;">self<span id="S1.F1.2.11.1.1" class="ltx_text" style="color:#666666;">.</span></span>vgg7 <span id="S1.F1.2.11.2" class="ltx_text" style="color:#666666;">=</span> nn<span id="S1.F1.2.11.3" class="ltx_text" style="color:#666666;">.</span>Sequential(</span><span id="S1.F1.2.12" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">          Spectrogram(fft<span id="S1.F1.2.12.1" class="ltx_text" style="color:#666666;">=<span id="S1.F1.2.12.1.1" class="ltx_text">64</span></span>, hop<span id="S1.F1.2.12.2" class="ltx_text" style="color:#666666;">=<span id="S1.F1.2.12.2.1" class="ltx_text">32</span></span>),</span><span id="S1.F1.2.13" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">          LogmelFilterBank(fft<span id="S1.F1.2.13.1" class="ltx_text" style="color:#666666;">=<span id="S1.F1.2.13.1.1" class="ltx_text">64</span></span>, mels<span id="S1.F1.2.13.2" class="ltx_text" style="color:#666666;">=<span id="S1.F1.2.13.2.1" class="ltx_text">26</span></span>),</span><span id="S1.F1.2.14" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">          conv(<span id="S1.F1.2.14.1" class="ltx_text ltx_font_bold" style="color:#AB21FF;">in</span><span id="S1.F1.2.14.2" class="ltx_text" style="color:#666666;">=<span id="S1.F1.2.14.2.1" class="ltx_text">1</span></span>),</span><span id="S1.F1.2.15" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">          conv(),</span><span id="S1.F1.2.16" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">          conv(),</span><span id="S1.F1.2.17" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">          nn<span id="S1.F1.2.17.1" class="ltx_text" style="color:#666666;">.</span>MaxPool2d(<span id="S1.F1.2.17.2" class="ltx_text" style="color:#666666;">3</span>, stride<span id="S1.F1.2.17.3" class="ltx_text" style="color:#666666;">=<span id="S1.F1.2.17.3.1" class="ltx_text">2</span></span>, pad<span id="S1.F1.2.17.4" class="ltx_text" style="color:#666666;">=<span id="S1.F1.2.17.4.1" class="ltx_text">1</span></span>),</span><span id="S1.F1.2.18" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">          conv(),</span><span id="S1.F1.2.19" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">          conv(),</span><span id="S1.F1.2.20" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">          conv(),</span><span id="S1.F1.2.21" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">          conv(),</span><span id="S1.F1.2.22" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">          nn<span id="S1.F1.2.22.1" class="ltx_text" style="color:#666666;">.</span>Conv2d(<span id="S1.F1.2.22.2" class="ltx_text" style="color:#666666;">13</span>, <span id="S1.F1.2.22.3" class="ltx_text" style="color:#666666;">13</span>, k<span id="S1.F1.2.22.4" class="ltx_text" style="color:#666666;">=<span id="S1.F1.2.22.4.1" class="ltx_text">1</span></span>))</span><span id="S1.F1.2.23" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">    <span id="S1.F1.2.23.1" class="ltx_text" style="color:#008000;">self<span id="S1.F1.2.23.1.1" class="ltx_text" style="color:#666666;">.</span></span>lin <span id="S1.F1.2.23.2" class="ltx_text" style="color:#666666;">=</span> nn<span id="S1.F1.2.23.3" class="ltx_text" style="color:#666666;">.</span>Linear(<span id="S1.F1.2.23.4" class="ltx_text" style="color:#666666;">169</span>, <span id="S1.F1.2.23.5" class="ltx_text" style="color:#666666;">169</span>)</span><span id="S1.F1.2.24" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">    <span id="S1.F1.2.24.1" class="ltx_text" style="color:#008000;">self<span id="S1.F1.2.24.1.1" class="ltx_text" style="color:#666666;">.</span></span>sof <span id="S1.F1.2.24.2" class="ltx_text" style="color:#666666;">=</span> nn<span id="S1.F1.2.24.3" class="ltx_text" style="color:#666666;">.</span>Linear(<span id="S1.F1.2.24.4" class="ltx_text" style="color:#666666;">169</span>, <span id="S1.F1.2.24.5" class="ltx_text" style="color:#666666;">169</span>)</span><span id="S1.F1.2.25" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">    <span id="S1.F1.2.25.1" class="ltx_text" style="color:#008000;">self<span id="S1.F1.2.25.1.1" class="ltx_text" style="color:#666666;">.</span></span>adv <span id="S1.F1.2.25.2" class="ltx_text" style="color:#666666;">=</span> nn<span id="S1.F1.2.25.3" class="ltx_text" style="color:#666666;">.</span>Linear(<span id="S1.F1.2.25.4" class="ltx_text" style="color:#666666;">169</span>, <span id="S1.F1.2.25.5" class="ltx_text" style="color:#666666;">3</span>)</span><span id="S1.F1.2.26" class="ltx_text ltx_inline-block" style="width:433.6pt;"></span><span id="S1.F1.2.27" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">  <span id="S1.F1.2.27.1" class="ltx_text ltx_font_bold" style="color:#008000;">def</span> <span id="S1.F1.2.27.2" class="ltx_text" style="color:#0000FF;">forward</span>(x):</span><span id="S1.F1.2.28" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;color:#BABABA;">      <span id="S1.F1.2.28.1" class="ltx_text ltx_font_italic" style="color:#BA2121;">’’’x: (batch, time_samples)’’’</span></span><span id="S1.F1.2.29" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">      x <span id="S1.F1.2.29.1" class="ltx_text" style="color:#666666;">-=</span> x<span id="S1.F1.2.29.2" class="ltx_text" style="color:#666666;">.</span>mean(<span id="S1.F1.2.29.3" class="ltx_text" style="color:#666666;">1</span>)</span><span id="S1.F1.2.30" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">      variance <span id="S1.F1.2.30.1" class="ltx_text" style="color:#666666;">=</span> (x <span id="S1.F1.2.30.2" class="ltx_text" style="color:#666666;">*</span> x)<span id="S1.F1.2.30.3" class="ltx_text" style="color:#666666;">.</span>mean(<span id="S1.F1.2.30.4" class="ltx_text" style="color:#666666;">1</span>) <span id="S1.F1.2.30.5" class="ltx_text" style="color:#666666;">+</span> <span id="S1.F1.2.30.6" class="ltx_text" style="color:#666666;">1e-7</span></span><span id="S1.F1.2.31" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">      x <span id="S1.F1.2.31.1" class="ltx_text" style="color:#666666;">/=</span> variance<span id="S1.F1.2.31.2" class="ltx_text" style="color:#666666;">.</span>sqrt()</span><span id="S1.F1.2.32" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">      x <span id="S1.F1.2.32.1" class="ltx_text" style="color:#666666;">=</span> <span id="S1.F1.2.32.2" class="ltx_text" style="color:#008000;">self<span id="S1.F1.2.32.2.1" class="ltx_text" style="color:#666666;">.</span></span>vgg7(x)</span><span id="S1.F1.2.33" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">      batch, channel, token, mel <span id="S1.F1.2.33.1" class="ltx_text" style="color:#666666;">=</span> x<span id="S1.F1.2.33.2" class="ltx_text" style="color:#666666;">.</span>shape</span><span id="S1.F1.2.34" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">      <span id="S1.F1.2.34.1" class="ltx_text ltx_font_italic" style="color:#3D7A7A;"># channel is non contiguous to mel thus</span></span><span id="S1.F1.2.35" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">      <span id="S1.F1.2.35.1" class="ltx_text ltx_font_italic" style="color:#3D7A7A;"># reshapes tokens together as attention.</span></span><span id="S1.F1.2.36" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">      x <span id="S1.F1.2.36.1" class="ltx_text" style="color:#666666;">=</span> x<span id="S1.F1.2.36.2" class="ltx_text" style="color:#666666;">.</span>reshape(batch, token, channel <span id="S1.F1.2.36.3" class="ltx_text" style="color:#666666;">*</span> mel)</span><span id="S1.F1.2.37" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">      x <span id="S1.F1.2.37.1" class="ltx_text" style="color:#666666;">=</span> <span id="S1.F1.2.37.2" class="ltx_text" style="color:#008000;">self<span id="S1.F1.2.37.2.1" class="ltx_text" style="color:#666666;">.</span></span>sof(x)<span id="S1.F1.2.37.3" class="ltx_text" style="color:#666666;">.</span>softmax(<span id="S1.F1.2.37.4" class="ltx_text" style="color:#666666;">1</span>) <span id="S1.F1.2.37.5" class="ltx_text" style="color:#666666;">*</span> <span id="S1.F1.2.37.6" class="ltx_text" style="color:#008000;">self<span id="S1.F1.2.37.6.1" class="ltx_text" style="color:#666666;">.</span></span>lin(x)</span><span id="S1.F1.2.38" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">      x <span id="S1.F1.2.38.1" class="ltx_text" style="color:#666666;">=</span> x<span id="S1.F1.2.38.2" class="ltx_text" style="color:#666666;">.</span>sum(<span id="S1.F1.2.38.3" class="ltx_text" style="color:#666666;">1</span>) <span id="S1.F1.2.38.4" class="ltx_text ltx_font_italic" style="color:#3D7A7A;"># Bayesian pool</span></span><span id="S1.F1.2.39" class="ltx_text ltx_font_typewriter ltx_inline-block" style="font-size:80%;width:433.6pt;">      <span id="S1.F1.2.39.1" class="ltx_text ltx_font_bold" style="color:#008000;">return</span> <span id="S1.F1.2.39.2" class="ltx_text" style="color:#008000;">self<span id="S1.F1.2.39.2.1" class="ltx_text" style="color:#666666;">.</span></span>adv(x) <span id="S1.F1.2.39.3" class="ltx_text ltx_font_italic" style="color:#3D7A7A;"># arousal, domin., val.</span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text ltx_font_bold">Fig. 1</span>: </span>Wav2Small in PyTorch. Note that x.reshape vectorises neighbor tokens to a new token before Bayesian pool.
</figcaption>
</figure>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>A / D / V Distillation</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Inspired by two publicly available Sota A/D/V models: The 12x Transformer Layer Wav2Vec2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> named Dawn and the 12x Transformer layer WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> that won the 2024 MSP Podcast Challenge<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://lab-msp.com/MSP-Podcast_Competition/leaderboard.php</span></span></span> for A/D/V. We propose to use them as annotators of A/D/V to train small student models.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Teacher</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">We notice that running the WavLM and also the Dawn baselines on the same audio and averaging the output A/D/V we obtain a Teacher model that outperforms both WavLM and Dawn. We open source the weights of Teacher here<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>https://huggingface.co/dkounadis/wav2small</span></span></span> . To the best of our knowledge this Teacher defines a new Sota on MSP Podcast valence with a ccc = 0.676, as shown in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.2 Data / Augmentation ‣ 2 A / D / V Distillation ‣ Wav2Small: Distilling Wav2Vec2 to 72K parameters for Low-Resource Speech emotion recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We use Teacher frozen, as teacher for distillation.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data / Augmentation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Distillation for categorical emotions has been used for cross-lingual transfer learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> or as a pretrain / finetune paradigm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Here we opt for simultaneous distillation from MSP Podcast
and mixed-speaker-speech / non-speech data. We apply always-on,
mixup <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> thus clean MSP Podcast speech is never shown
to the models. This enables long trainings without overfitting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Extensive work on the most useful labels for SER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> has revealed that soft labels as produced by Teacher are on-a-par with human annotations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. During training we use only the Teacher output A/D/V as ground truth and ignore the MSP Podcast annotations.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.6" class="ltx_p">The distillation of a student runs for 50M steps (<math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mo id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><csymbol cd="latexml" id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">\sim</annotation></semantics></math> 2 weeks) with batch = 16, SGD of fixed learning rate (no
schedule) = <math id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="5\text{E}-5" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><mrow id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml"><mrow id="S2.SS2.p2.2.m2.1.1.2" xref="S2.SS2.p2.2.m2.1.1.2.cmml"><mn id="S2.SS2.p2.2.m2.1.1.2.2" xref="S2.SS2.p2.2.m2.1.1.2.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S2.SS2.p2.2.m2.1.1.2.1" xref="S2.SS2.p2.2.m2.1.1.2.1.cmml">​</mo><mtext id="S2.SS2.p2.2.m2.1.1.2.3" xref="S2.SS2.p2.2.m2.1.1.2.3a.cmml">E</mtext></mrow><mo id="S2.SS2.p2.2.m2.1.1.1" xref="S2.SS2.p2.2.m2.1.1.1.cmml">−</mo><mn id="S2.SS2.p2.2.m2.1.1.3" xref="S2.SS2.p2.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><apply id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1"><minus id="S2.SS2.p2.2.m2.1.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1.1"></minus><apply id="S2.SS2.p2.2.m2.1.1.2.cmml" xref="S2.SS2.p2.2.m2.1.1.2"><times id="S2.SS2.p2.2.m2.1.1.2.1.cmml" xref="S2.SS2.p2.2.m2.1.1.2.1"></times><cn type="integer" id="S2.SS2.p2.2.m2.1.1.2.2.cmml" xref="S2.SS2.p2.2.m2.1.1.2.2">5</cn><ci id="S2.SS2.p2.2.m2.1.1.2.3a.cmml" xref="S2.SS2.p2.2.m2.1.1.2.3"><mtext id="S2.SS2.p2.2.m2.1.1.2.3.cmml" xref="S2.SS2.p2.2.m2.1.1.2.3">E</mtext></ci></apply><cn type="integer" id="S2.SS2.p2.2.m2.1.1.3.cmml" xref="S2.SS2.p2.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">5\text{E}-5</annotation></semantics></math>, weight decay = 0, momentum = 0,
on Nvidida RTX A4000 GPU, on PyTorch v2.4.0. The training audio is generated by mixing (on the fly during training):
<math id="S2.SS2.p2.3.m3.1" class="ltx_Math" alttext="0.64\times\text{audio}_{1}+0.41\times\text{audio}_{2}" display="inline"><semantics id="S2.SS2.p2.3.m3.1a"><mrow id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml"><mrow id="S2.SS2.p2.3.m3.1.1.2" xref="S2.SS2.p2.3.m3.1.1.2.cmml"><mn id="S2.SS2.p2.3.m3.1.1.2.2" xref="S2.SS2.p2.3.m3.1.1.2.2.cmml">0.64</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p2.3.m3.1.1.2.1" xref="S2.SS2.p2.3.m3.1.1.2.1.cmml">×</mo><msub id="S2.SS2.p2.3.m3.1.1.2.3" xref="S2.SS2.p2.3.m3.1.1.2.3.cmml"><mtext id="S2.SS2.p2.3.m3.1.1.2.3.2" xref="S2.SS2.p2.3.m3.1.1.2.3.2a.cmml">audio</mtext><mn id="S2.SS2.p2.3.m3.1.1.2.3.3" xref="S2.SS2.p2.3.m3.1.1.2.3.3.cmml">1</mn></msub></mrow><mo id="S2.SS2.p2.3.m3.1.1.1" xref="S2.SS2.p2.3.m3.1.1.1.cmml">+</mo><mrow id="S2.SS2.p2.3.m3.1.1.3" xref="S2.SS2.p2.3.m3.1.1.3.cmml"><mn id="S2.SS2.p2.3.m3.1.1.3.2" xref="S2.SS2.p2.3.m3.1.1.3.2.cmml">0.41</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p2.3.m3.1.1.3.1" xref="S2.SS2.p2.3.m3.1.1.3.1.cmml">×</mo><msub id="S2.SS2.p2.3.m3.1.1.3.3" xref="S2.SS2.p2.3.m3.1.1.3.3.cmml"><mtext id="S2.SS2.p2.3.m3.1.1.3.3.2" xref="S2.SS2.p2.3.m3.1.1.3.3.2a.cmml">audio</mtext><mn id="S2.SS2.p2.3.m3.1.1.3.3.3" xref="S2.SS2.p2.3.m3.1.1.3.3.3.cmml">2</mn></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.1b"><apply id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1"><plus id="S2.SS2.p2.3.m3.1.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1.1"></plus><apply id="S2.SS2.p2.3.m3.1.1.2.cmml" xref="S2.SS2.p2.3.m3.1.1.2"><times id="S2.SS2.p2.3.m3.1.1.2.1.cmml" xref="S2.SS2.p2.3.m3.1.1.2.1"></times><cn type="float" id="S2.SS2.p2.3.m3.1.1.2.2.cmml" xref="S2.SS2.p2.3.m3.1.1.2.2">0.64</cn><apply id="S2.SS2.p2.3.m3.1.1.2.3.cmml" xref="S2.SS2.p2.3.m3.1.1.2.3"><csymbol cd="ambiguous" id="S2.SS2.p2.3.m3.1.1.2.3.1.cmml" xref="S2.SS2.p2.3.m3.1.1.2.3">subscript</csymbol><ci id="S2.SS2.p2.3.m3.1.1.2.3.2a.cmml" xref="S2.SS2.p2.3.m3.1.1.2.3.2"><mtext id="S2.SS2.p2.3.m3.1.1.2.3.2.cmml" xref="S2.SS2.p2.3.m3.1.1.2.3.2">audio</mtext></ci><cn type="integer" id="S2.SS2.p2.3.m3.1.1.2.3.3.cmml" xref="S2.SS2.p2.3.m3.1.1.2.3.3">1</cn></apply></apply><apply id="S2.SS2.p2.3.m3.1.1.3.cmml" xref="S2.SS2.p2.3.m3.1.1.3"><times id="S2.SS2.p2.3.m3.1.1.3.1.cmml" xref="S2.SS2.p2.3.m3.1.1.3.1"></times><cn type="float" id="S2.SS2.p2.3.m3.1.1.3.2.cmml" xref="S2.SS2.p2.3.m3.1.1.3.2">0.41</cn><apply id="S2.SS2.p2.3.m3.1.1.3.3.cmml" xref="S2.SS2.p2.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS2.p2.3.m3.1.1.3.3.1.cmml" xref="S2.SS2.p2.3.m3.1.1.3.3">subscript</csymbol><ci id="S2.SS2.p2.3.m3.1.1.3.3.2a.cmml" xref="S2.SS2.p2.3.m3.1.1.3.3.2"><mtext id="S2.SS2.p2.3.m3.1.1.3.3.2.cmml" xref="S2.SS2.p2.3.m3.1.1.3.3.2">audio</mtext></ci><cn type="integer" id="S2.SS2.p2.3.m3.1.1.3.3.3.cmml" xref="S2.SS2.p2.3.m3.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.1c">0.64\times\text{audio}_{1}+0.41\times\text{audio}_{2}</annotation></semantics></math>, where audio<sub id="S2.SS2.p2.6.1" class="ltx_sub">1</sub> is a random 0.4s
up to 12s excerpt from a audiotrack of 12 action movies (in
house dataset contains full movies with music and effects) and
audio<sub id="S2.SS2.p2.6.2" class="ltx_sub">2</sub> is drawn from one of three buckets: (either Audioset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> sound, or MSP Podcast v1.7 train speech, or ambient urban/rural/transportation environmental background sound). The training audio is then passed through cyclic rotation (np.roll) and/or drop of random frequency-bands and re-normalising of lost energy to the remaining non-zero frequencies (metallic speech effect) and shout augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. This is the final signal given to both Teacher and student. It can
overflow the .wav [<math id="S2.SS2.p2.6.m6.1" class="ltx_Math" alttext="-" display="inline"><semantics id="S2.SS2.p2.6.m6.1a"><mo id="S2.SS2.p2.6.m6.1.1" xref="S2.SS2.p2.6.m6.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.6.m6.1b"><minus id="S2.SS2.p2.6.m6.1.1.cmml" xref="S2.SS2.p2.6.m6.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.6.m6.1c">-</annotation></semantics></math>1, 1] range due to shout.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2408.13920/assets/x1.png" id="S2.F2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="127" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text ltx_font_bold">Fig. 2</span>: </span>Arousal-Valence predictions for Crema-d test set, coloring according to Crema-d ground truth categories.</figcaption>
</figure>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2408.13920/assets/x2.png" id="S2.F3.g1" class="ltx_graphics ltx_img_square" width="225" height="240" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text ltx_font_bold">Fig. 3</span>: </span>CCC of Teacher, WavLM and Dawn (baselines), as well as the 5 students, on MSP Podcast v1.7 Test1.</figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>CCC Loss with Quadrant Correction</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">A/D/V datasets are annotated using 7-point Likert scale <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. The odd number of Likert points enhances annotator’s certainty in selecting positive / negative / neutral A/D/V. To leverage this certainty we introduce an auxiliary loss function: If Teacher’s A/D/V output is in different quadrant of the A/D/V 3D space from student’s A/D/V output, we extra-penalise the student via <math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="L1" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><mrow id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml"><mi id="S2.SS3.p1.1.m1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p1.1.m1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.cmml">​</mo><mn id="S2.SS3.p1.1.m1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><apply id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1"><times id="S2.SS3.p1.1.m1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1"></times><ci id="S2.SS3.p1.1.m1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.2">𝐿</ci><cn type="integer" id="S2.SS3.p1.1.m1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">L1</annotation></semantics></math> loss to output the value of Teacher. This auxiliary loss vanishes if teacher and student A/D/V quadrants agree. In this case only the CCC loss <span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>CCC loss is an Expected value summing the batch dimension.
</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> function applies.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2408.13920/assets/x3.png" id="S2.F4.g1" class="ltx_graphics ltx_img_landscape" width="461" height="120" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.2.1.1" class="ltx_text ltx_font_bold">Fig. 4</span>: </span>Wav2Small discrepancy from Teacher for Japanese audio track of Harry Potter vol1. Not included for train.</figcaption>
</figure>
<figure id="S2.F5" class="ltx_figure"><img src="/html/2408.13920/assets/x4.png" id="S2.F5.g1" class="ltx_graphics ltx_img_square" width="225" height="240" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F5.2.1.1" class="ltx_text ltx_font_bold">Fig. 5</span>: </span>CCC of all tested models on IEMOCAP (audio of all 5 Sessions has been concatenated to a single dataset), not used for training.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> trained a VGG14 of 4.9M parameters on
MSP Podcast v1.7 annotations. This resulted in very low CCC especially for valence, known as the most difficult
dimension to recognize with non transformer models: Their VGG14 achieved only 0.248 valence CCC vs 0.638 achieved
by Dawn on MSP Podcast v1.7 test-1 split. Now Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.2 Data / Augmentation ‣ 2 A / D / V Distillation ‣ Wav2Small: Distilling Wav2Vec2 to 72K parameters for Low-Resource Speech emotion recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that our 5 students reach a valence CCC <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="&gt;=" display="inline"><semantics id="S3.p1.1.m1.1a"><mo id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">&gt;=</mo><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><geq id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">&gt;=</annotation></semantics></math>0.37.
From the 5 students, MobileNets were pretrained hence
they score higher than (non pretrained) Wav2Small. Still,
Wav2Small reaches significant CCC, e.g. arousal CCC= 0.66
in MSP Podcast test-1, and = 0.56 in IEMOCAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>,
used as an out of distribution dataset only for evaluation.
MobileNetV3-S shows lower CCCs of MobileNetV4-S and
also has longer execution: 11ms compared to 5ms as show in
Table <a href="#S1.T1" title="Table 1 ‣ 1 INTRODUCTION ‣ Wav2Small: Distilling Wav2Vec2 to 72K parameters for Low-Resource Speech emotion recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> whose values are measured from .onnx running on
Intel(R) Xeon(R) Gold 6226R CPU @ 2.90GHz. Fig. <a href="#S2.F4" title="Figure 4 ‣ 2.3 CCC Loss with Quadrant Correction ‣ 2 A / D / V Distillation ‣ Wav2Small: Distilling Wav2Vec2 to 72K parameters for Low-Resource Speech emotion recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows
that Wav2Small follows the Teacher arousal and valence faithfully on a language not seen in training.
All 5 students have a LogMel layer (incorporated in .onnx via convolution). An extra conv is added to all
MobileNets to upscale the 1-channel output of LogMel to 3-channel RGB, needed to feed the pretrained weights.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Frequency bands of LogMel</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We discovered that num FFT bands = 64 leads to the best CCC
for MobileNetV4-M/S, MobileNetV3-S and Wav2Small. For
MobileNetV4-L we use num FFT bands = 144, because bum
FFT = 64 provides only 26 mel bins, which causes a highly
asymmetric aspect ratio (if we look at LogMel as an image)
yielding empty-input for convolutions. The larger num FFT
reduces the time resolution and therefore MobileNetV4-L
achieves only CCC = 0.59 arousal in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.2 Data / Augmentation ‣ 2 A / D / V Distillation ‣ Wav2Small: Distilling Wav2Vec2 to 72K parameters for Low-Resource Speech emotion recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a></p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Vectorisation of Time as Channels</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The power of Wav2Small comes from a large time resolution (tokens) by having a shallow architecture with 13 chanels. The
vectorisation of neighboring tokens into the channel dimension creates 169 ”channels” for the Bayesian pool. The joint
processing of the vectorised tokens by the classifier (self.adv(x) in Listing <a href="#S1.F1" title="Figure 1 ‣ 1.1 Wav2Small ‣ 1 INTRODUCTION ‣ Wav2Small: Distilling Wav2Vec2 to 72K parameters for Low-Resource Speech emotion recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), boosts Wav2Small to achieve arousal CCC = 0.66 in MSP Podcast, = 0.56 in IEMOCAP: Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.2 Data / Augmentation ‣ 2 A / D / V Distillation ‣ Wav2Small: Distilling Wav2Vec2 to 72K parameters for Low-Resource Speech emotion recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>/<a href="#S2.F5" title="Figure 5 ‣ 2.3 CCC Loss with Quadrant Correction ‣ 2 A / D / V Distillation ‣ Wav2Small: Distilling Wav2Vec2 to 72K parameters for Low-Resource Speech emotion recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> without pretraining. The valence dimension is the most difficult to predict: MobileNetV4-S obtains 0.42 and Wav2Small 0.37 yet in
out-of-domain movie as in Fig. <a href="#S2.F4" title="Figure 4 ‣ 2.3 CCC Loss with Quadrant Correction ‣ 2 A / D / V Distillation ‣ Wav2Small: Distilling Wav2Vec2 to 72K parameters for Low-Resource Speech emotion recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> Wav2Small does track the output of the Teacher faithfully.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Thinking of the lower valence achieved by all students compared to the Teacher, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> has shown that Wav2vec2 family of models as they have ASR pretraining, has learned linguistic cues to solve SER. Our 5 students do not posess linguistic
cues thus aiming towards a truly unsupervised A/D/V model that is fair for many languages.
A model equiped with additional textual features, as the
WavLM that won the 2024 SER classification challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> is a future avenue, although under the risk of introducing language/accent unfairness from focusing at words instead of
sound’s tonality.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.2 Data / Augmentation ‣ 2 A / D / V Distillation ‣ Wav2Small: Distilling Wav2Vec2 to 72K parameters for Low-Resource Speech emotion recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows that A/D/V values do separate also the categorical emotions to some extent. One also notices that
MobileNetV4-S and Wav2Small obtain an even larger dispersion along the valence axis. Therefore, an auxiliary
loss/teacher<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>https://huggingface.co/3loi/SER-Odyssey-Baseline-WavLM-Categorical</span></span></span>
should be added to distillation for predicting
also categorical emotions. Perhaps via CCC rather than
cross-entropy.
Observer variability and annotator agreement have been
investigated across various annotation schemes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> with findings revealing higher consensus when annotating A/D/V dimensions compared to annotating one out of six emotion categories. Thus we espy A/D/V to become the major avenue for SER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> in the future.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>CONCLUSION</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We propose a 72K parameter architecture named Wav2Small that reaches significant A/D/V score on both MSP Podcast
and IEMOCAP datasets, while using only 9 MB RAM compared to 36 MB RAM used by MobileNetV4-S, while having equal MACs. Mobilenet-V4 and MobileNet-V3 reduce the time-sample resolution to a single: 960-dimensional token / 1s audio, whereas Wav2Small provides 250: 169-dimensional-tokens / 1s audio, as its output (before Bayesian pool).
Hence Wav2Small is a potential replacement of the expensive input audio extractor of Transformer architectures, such as
wav2vec2.0 and WavLM. We also proposed a Teacher for distillation that obtains a new SOTA on MSP Podcast achieving
valence CCC = 0.676.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt, F. Eyben, and B.W. Schuller,

</span>
<span class="ltx_bibblock">“Dawn of the transformer era in speech emotion recognition: Closing the valence gap,”

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Pattern Anal. Mach. Intell., 45 (9), pp. 10745-10759</span>, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
H. Sun, Z. Lian, B. Liu, Y. Li, L. Sun, C. Cai, J. Tao, M. Wang, and Y. Cheng,

</span>
<span class="ltx_bibblock">“Emotionnas: Two-stream neural architecture search for speech emotion recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH</span>, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
L. Goncalves, A.N. Salman, A.R. Naini, L.M. Velazquez, T. Thebaud, L. Paola Garcia, N. Dehak, B. Sisman, and C. Busso,

</span>
<span class="ltx_bibblock">“Odyssey 2024-speech emotion recognition challenge: Dataset, baseline framework, and results,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Odyssey 10 (9,290), 4-54</span>, 2024.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
D. Qin, C. Leichner, M. Delakis, M. Fornoni, S. Luo, F. Yang, W. Wang, C. Banbury, C. Ye, B. Akin, V. Aggarwal, T. Zhu, D. Moro, and A. Howard et.al.,

</span>
<span class="ltx_bibblock">“Mobilenetv4 - universal models for the mobile ecosystem,”

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">preprint arXiv:2404.10518</span>, 2024.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
F. Schmid, K. Koutini, and G. Widmer,

</span>
<span class="ltx_bibblock">“Efficient large-scale audio tagging via transformer-to-cnn knowledge distillation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
W.N. Hsu, A. Sriram, A. Baevski, T. Likhomanenko, Q. Xu, V. Pratap, J. Kahn, A. Lee, R. Collobert, and G. Synnaeve,

</span>
<span class="ltx_bibblock">“Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training,”

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint 2104.01027</span>, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
B. T. Atmaja and A. Sasou,

</span>
<span class="ltx_bibblock">“Evaluating self-supervised speech representations for speech emotion recognition,”

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 10, pp. 124396–124407, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. H. Jo and K.C. Kwak,

</span>
<span class="ltx_bibblock">“Speech emotion recognition based on two-stream deep learning model using korean audio information,”

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Applied Sciences</span>, vol. 13, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Deng, Z. Zhang, E. Marchi, and B. Schuller,

</span>
<span class="ltx_bibblock">“Sparse autoencoder-based feature transfer learning for speech emotion recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proc. of Humaine assoc. conf. on affect. comput. and intelligent interaction</span>, 2013.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
H. Zhang, M. Cisse, Y.N. Dauphin, and D. Lopez-Paz,

</span>
<span class="ltx_bibblock">“mixup: Beyond empirical risk minimization.,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proc. ICLR</span>, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
O. Schrüfer, M. Milling, F. Burkhardt, F. Eyben, and B. Schuller,

</span>
<span class="ltx_bibblock">“Are you sure? analysing uncertainty quantification approaches for real-world speech emotion recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH</span>, 2024.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
H.C. Chou, L. Goncalves, S.G. Leem, A.N. Salman, C.C. Lee, and C. Busso,

</span>
<span class="ltx_bibblock">“Minority views matter: Evaluating speech emotion classifiers with human subjective annotations by an all-inclusive aggregation rule,”

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Affective Computing</span>, 2024.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
L. Beyer, X. Zhai, A. Royer, L. Markeeva, R. Anil, and A. Kolesnikov,

</span>
<span class="ltx_bibblock">“Knowledge distillation: A good teacher is patient and consistent.,”

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10925–10934</span>, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J. F. Gemmeke et al.,

</span>
<span class="ltx_bibblock">“Audio set: An ontology and human-labeled dataset for audio events,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2017.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
T. Raitio, A. Suni, J. Pohjalainen, M. Airaksinen, M. Vainio, and P. Alku,

</span>
<span class="ltx_bibblock">“Analysis and synthesis of shouted speech,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH</span>, 2013.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
R. Lotfian and C. Busso,

</span>
<span class="ltx_bibblock">“Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings,”

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Affective Comput.</span>, vol. 10, no. 4, pp. 471–483, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M.A. Nicolaou, B. Schuller, and S. Zafeiriou,

</span>
<span class="ltx_bibblock">“Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2016.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
C. Busso, M. Bulut, C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. Narayanan,

</span>
<span class="ltx_bibblock">“Iemocap: Interactive emotional dyadic motion capture database,”

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Language resources and evaluation, vol. 42, no. 4, pp. 335–359</span>, 2008.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
G. Ioannides, M. Owen, A. Fletcher, V. Rozgic, and C. Wang,

</span>
<span class="ltx_bibblock">“Towards paralinguistic-only speech representations for end-to-end speech emotion recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH</span>, 2023, pp. 1853–1857.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
H. Härm and T. Alumäe,

</span>
<span class="ltx_bibblock">“Taltech systems for the odyssey 2024 emotion recognition challenge,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Speaker Odyssey</span>, 2024.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
I.D. Wood, J.P. McCrae, V. Andryushechkin, and P. Buitelaar,

</span>
<span class="ltx_bibblock">“A comparison of emotion annotation approaches for text,”

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Information, 2018, 9(5), 117</span>, https://doi.org/10.3390/info9050117.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
S.D. Morgan,

</span>
<span class="ltx_bibblock">“Categorical and dimensional ratings of emotional speech: Behavioral findings from the morgan emotional speech set,”

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">J. Speech Lang. Hear. Research 2019</span>, doi: 10.1044/2019JSLHR-S-19-0144.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.13919" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.13920" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.13920">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.13920" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.13921" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 15:56:55 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
