<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.00124] Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish</title><meta property="og:description" content="Classifying Sorani Kurdish subdialects poses a challenge due to the need for publicly available datasets or reliable resources like social media or websites for data collection. We conducted field visits to various cit…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.00124">

<!--Generated on Sun May  5 17:52:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\setcode</span>
<p id="p1.2" class="ltx_p">utf8


</p>
</div>
<h1 class="ltx_title ltx_title_document">Where Are You From? Let Me Guess!
<br class="ltx_break">Subdialect Recognition of Speeches in Sorani Kurdish</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

<span id="id1.1.id1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="id1.1.id1.1.1" class="ltx_tr">
<span id="id1.1.id1.1.1.1" class="ltx_td ltx_align_center">Sana Isam and Hossein Hassani</span></span>
<span id="id1.1.id1.2.2" class="ltx_tr">
<span id="id1.1.id1.2.2.1" class="ltx_td ltx_align_center">University of Kurdistan Hewlêr</span></span>
<span id="id1.1.id1.3.3" class="ltx_tr">
<span id="id1.1.id1.3.3.1" class="ltx_td ltx_align_center">Kurdistan Region - Iraq</span></span>
<span id="id1.1.id1.4.4" class="ltx_tr">
<span id="id1.1.id1.4.4.1" class="ltx_td ltx_align_center"><span id="id1.1.id1.4.4.1.1" class="ltx_text ltx_font_typewriter">{sana.isam, hosseinh}@ukh.edu.krd</span></span></span>
</span>
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Classifying Sorani Kurdish subdialects poses a challenge due to the need for publicly available datasets or reliable resources like social media or websites for data collection. We conducted field visits to various cities and villages to address this issue, connecting with native speakers from different age groups, genders, academic backgrounds, and professions. We recorded their voices while engaging in conversations covering diverse topics such as lifestyle, background history, hobbies, interests, vacations, and life lessons. The target area of the research was the Kurdistan Region of Iraq. As a result, we accumulated 29 hours, 16 minutes, and 40 seconds of audio recordings from 107 interviews, constituting an unbalanced dataset encompassing six subdialects. Subsequently, we adapted three deep learning models: ANN, CNN, and RNN-LSTM. We explored various configurations, including different track durations, dataset splitting, and imbalanced dataset handling techniques such as oversampling and undersampling. Two hundred and twenty-five(225) experiments were conducted, and the outcomes were evaluated. The results indicated that the RNN-LSTM outperforms the other methods by achieving an accuracy of 96%. CNN achieved an accuracy of 93%, and ANN 75%. All three models demonstrated improved performance when applied to balanced datasets, primarily when we followed the oversampling approach. Future studies can explore additional future research directions to include other Kurdish dialects.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Understanding linguistic variety and how it affects communication depends heavily on dialects and subdialects. Kurdish provides considerable hurdles in natural language processing and language classifications due to its macro morphological structure and wide variety, as will be stated in detail. There are several dialects and subdialects of the Kurdish language and even within a subdialect, there might be differences across cities and towns.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The second main Kurdish dialect is Central Kurdish, well-known as the Sorani dialect, which is spoken largely in the North-Eastern part of Iraq and the western part of Iran <span id="S1.p2.1.1" class="ltx_text ltx_font_bold">?</span>). It is spoken in Sulaimani province, apart from Mergasour district; it is spoken in all areas of Erbil Province, Kirkuk and Halabja provinces in the Kurdistan Region of Iraq (KRI) and some other areas in Iraq, such as Basra and Jasan in Diyala province. It can also be spoken in several Kurdish cities in Iran, such as Mahabad, Bokan, Piranshahr, Sardsht,
Shno (Ushnawiye), Naqade, Takab Jwanro, Rawansar, Salasi, Babajani and so forth <span id="S1.p2.1.2" class="ltx_text ltx_font_bold">?</span>). In this respect, it has
appeared as the official language of the Kurdistan Regional
Government, parliament and other foundations <span id="S1.p2.1.3" class="ltx_text ltx_font_bold">?</span>).
We aim to develop a dataset and create a model that can accurately detect and classify the Kurdish language’s subdialects. Creating an audio dataset is critical for training and assessing machine learning models for subdialect categorisation. It enables us to correctly differentiate and categorise diverse subdialects by capturing their distinct phonetic and auditory properties.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The collection of a comprehensive voice dataset, consisting of 29 hours, 16 minutes and 40 seconds of recordings, encompassing six Kurdish Sorani subdialects (Garmiani, Hewleri, Karkuki, Pishdari, Sulaimani, Khoshnawi), represents a significant milestone for both us and other developers and computational linguists. This dataset will serve as a valuable resource for conducting research and delving deeper into the study of subdialects.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>The Kurdish Language</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">More than 30 million people worldwide communicate with one another using the Indo-European language known as Kurdish. People are distributed in regions of different countries, primarily Iraq, Iran, Syria and Turkey, as well as Armenia, Azerbaijan and Georgia. Kurdish is a language with a variety of dialects, on which many studies have been done on both the Kurdish language and its various dialects. Many books and articles were written concerning this topic. This has been the subject of discussion among orientalists with certain viewpoints <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">Hanani and Naser, 2020</a>]</cite>.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">There is no consensus among researchers or linguists regarding the division of Kurdish dialects and subdialects. Most of them are divided into four dialects but with different names for them <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Hassani et al., 2016</a>]</cite>. Table <a href="#S1.T1" title="Table 1 ‣ 1.1 The Kurdish Language ‣ 1 Introduction ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows various views on the subdialects of Sorani Kurdish.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<table id="S1.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">
<span id="S1.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.1.1.1" class="ltx_p" style="width:85.4pt;">Source</span>
</span>
</th>
<th id="S1.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S1.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.2.1.1" class="ltx_p" style="width:256.1pt;">Subdialect Names</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.1.2.1" class="ltx_tr">
<td id="S1.T1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S1.T1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.2.1.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="S1.T1.1.2.1.1.1.1.1" class="ltx_text ltx_font_bold">?</span>)</span>
</span>
</td>
<td id="S1.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.2.1.2.1.1" class="ltx_p" style="width:256.1pt;">Mukri, Sorani, Ardalani, Sulaimani, Garmiani</span>
</span>
</td>
</tr>
<tr id="S1.T1.1.3.2" class="ltx_tr">
<td id="S1.T1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S1.T1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.3.2.1.1.1" class="ltx_p" style="width:85.4pt;">Tafiq Wahby</span>
</span>
</td>
<td id="S1.T1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.3.2.2.1.1" class="ltx_p" style="width:256.1pt;">Mahabadi, Hewleri, Karkuki, Mukri, Sorani, Ardalani, Sulaimani, Garmiani</span>
</span>
</td>
</tr>
<tr id="S1.T1.1.4.3" class="ltx_tr">
<td id="S1.T1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S1.T1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.4.3.1.1.1" class="ltx_p" style="width:85.4pt;">Dr. Jamal Nabaz</span>
</span>
</td>
<td id="S1.T1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.4.3.2.1.1" class="ltx_p" style="width:256.1pt;">Karkuki, Mahabadi, Hewleri, Karkuki, Sulaimani, Mukri, Sorani, Ardalani, Sulaimani, Garmian</span>
</span>
</td>
</tr>
<tr id="S1.T1.1.5.4" class="ltx_tr">
<td id="S1.T1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S1.T1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.5.4.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="S1.T1.1.5.4.1.1.1.1" class="ltx_text ltx_font_bold">?</span>)</span>
</span>
</td>
<td id="S1.T1.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S1.T1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.5.4.2.1.1" class="ltx_p" style="width:256.1pt;">Mukri, Ardalani, Garmiani, Khoshnaw, Pishdari, Warmawa, Kirmanshahi, Hewleri</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Classification of Sorani subdialects</figcaption>
</figure>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">Geographically, central Kurmanji is located in Iraq and Iran. A line separates central Kurmanji from the north Kurmanji up to Sirwan River and the high road between Khanaqen (in Iraq), Qasri Shirin, Kermanshah and Malayer (in Iran) and from the east of Hamrin Hills in the west (Iraq) up to the line extending in Sahand Mountain, Masirabad, Bijar and Asadabad in the East (Iran) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Khorshid, 1983</a>]</cite>.
In addition, for a clearer understanding of the geographical distribution of the Sorani dialect and its subdialects, we adapted the map provided by <span id="S1.SS1.p3.1.1" class="ltx_text ltx_font_bold">?</span>). We have added three subdialects, namely Pishdari, Garmiani and Khoshnawi, and placed them in their respective approximate locations, illustrated in Figure <a href="#S1.F1" title="Figure 1 ‣ 1.1 The Kurdish Language ‣ 1 Introduction ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
In Sorani, Arabic has influenced the subdialects of Iraq and Persian has influenced the subdialects of Iran. Some words are imported from Arabic into the dialects and subdialects of Kurdish located in Iraq <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Khorshid, 1983</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2404.00124/assets/map.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="589" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Geographical distribution of Sorani dialect and its subdialects, adopted from <span id="S1.F1.1.1" class="ltx_text ltx_font_bold">?</span>).</figcaption>
</figure>
<div id="S1.SS1.p4" class="ltx_para">
<p id="S1.SS1.p4.1" class="ltx_p">Our investigation focused on the following Sorani subdialects:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Sulaimani</span>- It is a subdialect spoken by the people of Sulaimaniah, Sulaimaniah, as a centre of the present district, has only existed for 239 years. Sulaiman Baban founded it in 1784 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Cockrell-Abdullah, 2018</a>]</cite>, located in northern Iraq and southern Kurdistan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx31" title="" class="ltx_ref">Soane, 1912</a>]</cite>.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Karkuki</span>- Along with other dialects like Hewleri, Sulaimanih and Snaye that are also present there, Karkuk also has its own subdialect <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">Hussein, 2011</a>]</cite>.
Kakrkuki is identified as a subdialect of Central Kurmanji <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">Hussein, 2011</a>]</cite>. Due to its rich oil reserves, the region holds significant economic value and the Kurdish population of Karkuk has faced forced displacement on multiple occasions. As an accent, it’s close to Garmiani. Karkuk is one of the large cities in the Kurdistan region of Iraq, between the two rivers of Serwan and Zei Bchuk (Lesser Zab) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">Hussein, 2011</a>]</cite>. In Karkuk, seven branches of the subdialect exist Rozhbaiany, Kakeye, Shwany Kishk, Ajemye, Zengene, Sia Mensoury and Shekhany.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Hewleri</span>- Hewleri is another subdialect of Sorani dialect, named after the Hewler city. The subdialect located in Hewler district in Iraqi Kurdistan (except Zebari province) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Khorshid, 1983</a>]</cite>.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p"><span id="S1.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Khoshnawi</span>- The Khoshnawi subdialect is characterised by its significant scope and is primarily spoken by the inhabitants of Shaqlawa, Balisan and the surrounding villages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx27" title="" class="ltx_ref">Rahmani, 2009</a>]</cite>. Khoshnawi subdialect geographically starts from Malakan in the north and extends southward until Gomespan. In the east, it begins from Serwchawe and stretches westward until Mela Nevyean. The subdialect encompasses Safeen Mountain and Shaqlawe. It is divided between the provinces of Hewler and Sulaimani. While it is a clan, as previously indicated, it identifies as a separate Sorani subdialect in specific sources.</p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S1.I1.i5.p1" class="ltx_para">
<p id="S1.I1.i5.p1.1" class="ltx_p"><span id="S1.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Garmiani</span>- This dialect is located in the western Sulaimaniah region and is primarily found in villages such as Kalar, Kifri, QaraTappeh and Tuz <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">Khorshid, 1983</a>]</cite>.</p>
</div>
</li>
<li id="S1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S1.I1.i6.p1" class="ltx_para">
<p id="S1.I1.i6.p1.1" class="ltx_p"><span id="S1.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">Pishdari</span>- Another distinctive Sorani Kurdish subdialect known as Pishdari is frequently used as a synonym for Qaladzaye.
The subdialect is located in the northern region of Sulaimani, approximately 175 km away. Its center is in Qaladza, surrounded by various mountain chains, including Asos, Kurees, Doopeze, Bilfet, Mamend, Qendil, Zerine Kew, Pirane Resh and Kewe Resh. To the north, it borders Iran and Soran. To the west, it borders Rania, to the south, it borders Bingird, and to the east, it borders Iran. Due to the population’s forced migrations, their accents have undergone changes over time.</p>
</div>
</li>
</ol>
</div>
<div id="S1.SS1.p5" class="ltx_para">
<p id="S1.SS1.p5.1" class="ltx_p">There may be little or insufficient documentation, books or e-books that go into depth on the subdialects of Sorani Kurdish and how they’re employed. Instead of digging into the nuances of dialects and subdialects, the materials that are now accessible mostly describe the physical locations of these cities.</p>
</div>
<div id="S1.SS1.p6" class="ltx_para">
<p id="S1.SS1.p6.1" class="ltx_p">The rest of the paper is organized as follows. Section <a href="#S2" title="2 Related work ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> reviews the literature and the related work. Section <a href="#S3" title="3 Method ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the method that the research follows. We provide the results and discuss the outcome in Section <a href="#S4" title="4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Finally, Section <a href="#S5" title="5 Conclusion ‣ 4.10 Evaluation and Discussion ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> concludes the paper and provides some ideas about future work.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Numerous studies have been done on speech recognition, particularly in the areas of dialect and subdialect recognition. In speech, there are typically two distinct kinds of models, which are traditional models and deep learning models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">Ganapathiraju et al., 2004</a>]</cite>.
The research on Kurdish speech processing is quite limited <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Amani et al., 2021</a>]</cite>. <span id="S2.p1.1.1" class="ltx_text ltx_font_bold">?</span>) usded SVM model used for Kurdish dialect recognition in texts.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Regarding Kurdish recognition systems, the most recently developed Kurdish (Sorani) speech recognition system used Kaldi ASR. It used a trigram statistical language model and several acoustic models on diverse experiments, such as Tri1 used MFCC, delta, and delta-delta features for triphone modeling using the HMM-GMM algorithm. Tri2: HMM GMM-based tri-phone modeling with LDA and MLLT-transformed MFCC features Tri3: Triphone modeling based on HMM-GMM with MFCC, delta, delta-delta and SAT features, SGMM; Subspace Gaussian Mixture Model and with applying LDA to the MFCC features, Mono: Mono-phone modeling based on HMM with GMM and MFCC features They experienced this on the Jira dataset (the first Kurdish speech corpus-diphones based), designed by AsoSoft research and business group on natural language processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx35" title="" class="ltx_ref">Veisi et al., 2022</a>]</cite>, Jira corpus is a collection of speech in the office using a pre-defined noise-free microphone and crowdsourcing in the Telegram social network using a smartphone microphone, then manually eliminating noise. 100 sentences for testing and 700 sentences for training over 11 topics were included, whereby 576 speakers made more than 42,000 tracks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx25" title="" class="ltx_ref">Ortu et al., 2015</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Based on SVM modeling of the N-gram of phones <span id="S2.p3.1.1" class="ltx_text ltx_font_bold">?</span>) explored a new class of methods. The SVM techniques offer comprehensible phone strings that represent a dialect. This methodology may be used to enhance existing dialect cue inventories from linguistics and fill part of the gap between automated approaches and linguistic analysis.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Traditional Approaches of Speech Recognition</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Traditional models that have been used are: Support Vector Machine (SVM), Naïve Bayes, Sequential Minimal Optimisation(SMO), C4.5 Decision Tree Classifier (J48), Zero Rule(ZeroR), Repeated Incremental Pruning to Produce Error Reduction (JRip) and bottleneck <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">Alshutayri et al., 2016</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">?</span>) combined two spectral methods, SVM and N-gram, classify phone strings that represent a dialect. They use features such as the frequency centroid and standard deviation. Also, <span id="S2.SS1.p2.1.2" class="ltx_text ltx_font_bold">?</span>) proposed another adapted I-vector, as concept that originally designed for speaker recognition, for Arabic dialect recognition.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Regarding the process, input speech is first turned into a sequence or lattice of tokens <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx24" title="" class="ltx_ref">Murhaf, 2013</a>]</cite>, and then the N-gram is done. The outcome is utilised to predict the class labels for SVM sequence kernels. The approach was previously used and achieved success in <span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_bold">?</span>) and <span id="S2.SS1.p3.1.2" class="ltx_text ltx_font_bold">?</span>), using their cases as dialects or languages. Some difficulties that they faced in the research can be described as the small amount of training data for all three dialects of English, Mandarin and Arabic, which makes understanding N-gram analysis challenging to evaluate whether the top characteristics are discriminative between the sets of speaker-dependent characteristics or discriminative between the specific dialects. Although an issue that complicated the analysis of their system to some extent was when the data for a specific dialect could have some dialect-related channel artifacts.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p"><span id="S2.SS1.p4.1.1" class="ltx_text ltx_font_bold">?</span>) experimented with Naïve Bayes, SMO, J48, ZeroR and JRip classifiers and observed that by utilising SMO, they properly identified 6803 utterances and incorrectly classified 816 utterances, which achieved the best accuracy among other classifiers. After being verified and categorised by three human Arabic linguistic specialists, it was discovered that most of the misclassified utterances might have been better classified by converting them to Buckwalter to normal readable Arabic scripts, as reading the Buckwalter text is tough even for the experts on the Buckwalter transliteration system. Their method achieved an accuracy of around 50% with a training set percentage split of 60:40 and was better than splitting the training set 80%-90% with an accuracy equal to 42.85%. Besides the limitations related to writing style and the lack of official writing standards of dialects, they faced difficulties while collecting their datasets. Because they have both phonetic and acoustic information, traditional bottleneck features became attractive as a replacement for speech tasks such as Automatic Speech Recognition (ASR), Speaker Identification (SID) and Language Identification (LID) after Mel-Frequency Cepstral Coefficients (MFCC). Nonetheless, there are two possible negative effects.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p"><span id="S2.SS1.p5.1.1" class="ltx_text ltx_font_bold">?</span>) proposed a new method for extracting BNF without relying on a transcribed corpus. This approach uses an unsupervised extraction diagram trained with estimated phonetic labels. The method was evaluated on Chinese dialect and Pan-Arabic datasets and consistently outperformed the baseline MFCC-based system. The proposed BNF achieved a relative improvement of +48% in Equal Error Rate (EER) and +52% in overall performance compared to the baseline. Even with limited training data, the proposed feature showed a relative improvement of up to 24% without the need for a secondary transcribed corpus.</p>
</div>
<div id="S2.SS1.p6" class="ltx_para">
<p id="S2.SS1.p6.1" class="ltx_p"><span id="S2.SS1.p6.1.1" class="ltx_text ltx_font_bold">?</span>) proved that for classification jobs using discrete features, the MNB classifier is effective. As it identified the precision of a speaker with an accuracy of 67.9 percent for sentences with an average length of 7 words and above 90% when taking 16 words into consideration, the process was using 3 datasets: Corpus-6 and Corpus-26 and a custom extracted from Twitter with 16,385 utterances in 49h36m.</p>
</div>
<div id="S2.SS1.p7" class="ltx_para">
<p id="S2.SS1.p7.1" class="ltx_p"><span id="S2.SS1.p7.1.1" class="ltx_text ltx_font_bold">?</span>) presented a practical method for Persian speech recognition using HMMs. They employed syllables as units for their HMM-based approach and incorporated features such as MFCC and PARCOR. The training was conducted on the FARSDAT dataset, which consists of two speech corpora: ”Large FARSDAT” and ”Small FARSDAT.” The latter is a smaller Persian corpus recorded at low noise levels and includes phoneme-level segmentation and labelling. The speakers represent ten different Persian dialects, as a result, the HMM achieved a Word Error Rate (WER) of 18.3%, and the system performance was improved by approximately 16% through post-processing techniques. Despite various restrictions and challenges, it is widely recognised that the Persian language has relatively fewer computational studies compared to other languages. However, Persian stands out as a language with a rich vocabulary, allowing for the creation of numerous words through the addition of prefixes and suffixes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx30" title="" class="ltx_ref">Shafieian, 2022</a>]</cite>.</p>
</div>
<div id="S2.SS1.p8" class="ltx_para">
<p id="S2.SS1.p8.1" class="ltx_p">Then some open-source programmes were developed and later used for speech recognition purposes. The Kaldi toolkit, a free open-source programme designed to a large extent by Daniel Povey, includes a basic library of Kaldi’s C++ code that includes modeling of acoustic systems utilising subspace Gaussian mixture models (SGMM) and normal Gaussian mixture models, as well as all frequently used linear and affine transformations. <span id="S2.SS1.p8.1.1" class="ltx_text ltx_font_bold">?</span>) uses the Kaldi toolkit for the first public large-scale speaker verification corpus in Persian. The DeepMine corpus contains more than 1850 annotators and 540,000 tracks, totalling more than 480 hours of speech. In addition, it showed that the DeepMine repository is more difficult to use than RedDots Speech Recognition Challenge 2015 (RSR2015) and RedDots, according to text-dependent findings. The model was trained on the DeepMine database with 5.9 hours as a test set and 28.5 hours as a large test set, their WERs were 4.44 and 4.09, respectively.</p>
</div>
<div id="S2.SS1.p9" class="ltx_para">
<p id="S2.SS1.p9.1" class="ltx_p">Another open-source system was used for speaker-independent Urdu speech recognition based on the HMM approach that was proposed for developing in <span id="S2.SS1.p9.1.1" class="ltx_text ltx_font_bold">?</span>), the pen source framework called Sphinx4. They reported that their research is achieving satisfactory results in medium and large vocabulary sizes and used a small-sized vocabulary, specifically 52 isolated most spoken Urdu words. They received poor reliability from the system after utilising English acoustic models, as Sphinx4 libraries were used for some Latin languages like Italian or French and English. For all that, the WER was 60%, as some Roman letters do not exist in English. The researchers solved this by recording ten samples from ten different narrators, then combining each file from 52-word files into a single-word file. This took a considerable amount of time, while more different phonemes in words decrease accuracy, while the same types of phonemes in words could confuse the system’s recognition. Increasing the number of words will increase accuracy. The mean WER was 10.66%.</p>
</div>
<div id="S2.SS1.p10" class="ltx_para">
<p id="S2.SS1.p10.1" class="ltx_p">Two components of a system for Arabic dialect detection were proposed in <span id="S2.SS1.p10.1.1" class="ltx_text ltx_font_bold">?</span>) and are based on phonetic features and acoustic features. The first component is based on a phonotactic representation of the speech, and the second component oversees speech signal analysis to extract acoustic features. The choice is made through score-level fusion between the phonetic and acoustic systems following all testing and model phases. In this model, the PER with GMM-UBM model and the identity vector (i-vector) classifier were utilised. It could detect Egyptian Arabic dialects, Levantine Arabic accent or dialect, Saud, Levantine Arabic dialect and Gulf accents and their sub-dialects, which in total contain 3840 tracks. The model has been trained on the SARA (Spoken Arabic Regional Archive) dataset. The master dataset SARA consists of Arabic dialects that are spontaneous and canonical and gender independent, meaning that the speech was gathered through following specific instructions like reading and from human-human and human-machine conversations, as well as from the real world. When compared to other state-of-the-art systems, the acoustic feature might improve Arabic dialects more.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Deep Learning Approaches of Speech Recognition</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p"><span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_bold">?</span>) used the VarDial 2017 shared tasks to train and test their ADI systems, and they had high performance (68.7%) on the x-vectors technique in their study. Fusing the model with i-vectors marginally enhanced its performance. In addition to MSA, the technique was used to distinguish between the five main dialects of Arabic-Gulf, Iraqi, Levantine, Egyptian, Meghrebi, Yemenite and Maltese—as well as some of its subdialects. Nevertheless, the model has several drawbacks. For example, DNN is not helpful for ASR due to its high computational requirements, which were overcome by employing multiple GPU cores to use DNN for speech recognition.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">?</span>) proposed LAS (Listen, Attend and Spell), a neural network model that outputs a word sequence from an audio signal without the need for individual acoustic models, pronunciation models, language models, HMMs, etc. Sequence-to-sequence (Seq2Seq) learning model framework with attention is the foundation of the LAS model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">Chorowski et al., 2015</a>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">sutskever2014sequence</span>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">chan2016listen</span>]</cite>. Because traditional automated speech recognition systems require a distinctive pronunciation and language model for every dialect from the multi-dialect acoustic model, <span id="S2.SS2.p2.1.2" class="ltx_text ltx_font_bold">?</span>) designed a universal multi-dialect model for them in which mistakes are transmitted to the Language Model (LM) and Pronunciation Model (PM) if the Acoustic Model (AM) predicts a false collection of sub-word units from the incorrect dialect. The approach has several desirable features, including an improvement in low-resource languages and simplicity.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">?</span>) discovered that using DNN for feature extraction is more effective in comparison with a shallow network and that using spectrogram features improves outcomes in comparison to MFCC features. For the first time, they combined Deep Bidirectional Long Short Term (DBLSTM) and Deep Belief Network (DBN) with the output layer Connectionist Temporal Classification (CTC) to create an AM, they increased system accuracy by using the bidirectional network rather than unidirectional model. By using Kaldi-DNN and HMM shows that using DBLSTM improved Persian phoneme recognition accuracy, with DBLSTM neural network the LSTM, DLSTM and BLSTM were used too.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p"><span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold">?</span>) suggested a phoneme-based RNN-LSTM language model instead of the n-gram model in PPRLM since RNN and n-gram models achieve less accuracy than LSTM networks. The reason that they didn’t compare it to other studies, was as it is the only study to use the LSTM language model in PPRLM for dialect recognition since 2020. They trained models with the ends of sentences and showed it works better than training with the entire sentence, then found out that classifying dialects can be done only by looking at the sentence endings. In this new investigation, it was suggested only for the audio of 1 second and 0.5 seconds and achieves (83.8- 84.2%). Therefore, for the long sentences (3 seconds), the whole sentence was used, and the accuracy was 84.4%. This supports the previous studies that, with increasing the test duration, the accuracy would increase too. The model could successfully recognise the four region dialects: Ankara, Trabzon, Alanya and Kibris. The whole dataset contains 2.7 hours of noise-free audio. The audio was recorded from older people with low levels of education, not on purpose but due to the characteristics of the people selected. They could only find those. In the end, they suggested they could improve their model by using BLSTM.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p"><span id="S2.SS2.p5.1.1" class="ltx_text ltx_font_bold">?</span>) investigated and compared three different state-of-the-art models for the Urdu LVCSR system: 3-gram LM, RNNLM and Text normalised acoustic model + RNNLM, then TDNN-BLSTM was chosen to develop the system, and recording the decoded output lattice was done using RNNLM. WER was 13.5 when they developed their Urdu corpora, which contains 300 hours of noise-free recordings from 1671 speakers with a vocabulary of 199,000 words. The models were 3-gram LM, RNNLM and Text Normalised Acoustic Model + RNNLM, and the WER was 18.64, 16.94 and 13.50, respectively. The investigation was constrained by the fact that some Urdu words might be written in two alternative ways, if the decoded version differs from the reference text, ASR is penalised as one replacement, as well as some words that ASR occasionally inserts spaces into, can be true with or without spaces, yet they are incorrect for WER computation. After text normalisation of the training and test sets, these penalties can be eliminated by retraining transcriptions.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p">The theory of CNN and its practical application techniques are now evolving in tandem with significant growth in the number of CNN layers, which raises the computational complexity of the systems that use them, the network architecture, for instance, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx33" title="" class="ltx_ref">Valueva et al., 2020</a>]</cite>. Based on the shared-weight design of the convolution kernels or filters that slide along input features and produce translation-equivalent responses known as feature maps, CNNs are also known as Space-Invariant Artificial Neural Networks (SIANN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx38" title="" class="ltx_ref">Zhang et al., 1990</a>]</cite>.</p>
</div>
<div id="S2.SS2.p7" class="ltx_para">
<p id="S2.SS2.p7.1" class="ltx_p">To summarize, traditional models such as SVM, Naïve Bayes and HMMs have been used for dialect recognition and speech processing. N-gram analysis and feature adaptations like I-vector and bottleneck have shown promising results. Challenges include limited training data and artifacts. Open-source programmes like Kaldi toolkit aid in acoustic system modeling. Ongoing efforts aim to improve accuracy and performance in these areas. While deep learning approaches have gained prominence in recent years. Deep learning models such as CNNs, RNNs and transformer models have shown promising results in these tasks. These models can automatically learn hierarchical features and capture complex patterns in speech data. However, deep learning approaches require large amounts of labelled data and computational resources for training. Nonetheless, they offer the potential for further advancements in dialect recognition and speech processing.The usage of these techniques in music genre classification has shown a promising results <span id="S2.SS2.p7.1.1" class="ltx_text ltx_font_bold">?</span>; <span id="S2.SS2.p7.1.2" class="ltx_text ltx_font_bold">?</span>) that could be replicated in speech classification as too. We intend to base our experimental approaches on those findings.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The following section describes the method that the research follows.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Collection</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The theme of our speech corpus is a normal daily conversation about the personal background of speakers, which is obtained using an interview guided by lead questions in a way that the speech includes proper names, numerals, dates and times, the speakers’ past lives, their education, and such. The interview guide includes various sections: The participant’s background, routine duties, previous experiences, hobbies and interests, long answers to let the speakers talk about their weekends, vacations, life lessons and personal stories.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">To increase the number of participants, we choose a wide variety of questions for different groups of them. We assume that the interests, perspectives and experiences of participants are vary depending on their ages, professions, occupations, and educational backgrounds. For instance, elderly participants who have long experiences in life may have insightful stories to tell about their memories, experiences or life lessons. Younger people, on the other hand, could be more likely to talk about their goals, favorite books, movies or online hobbies. We also consider the individuals’ particular roles and jobs. For example, farmers or shepherds receive questions specifically geared toward their experiences, allowing them to share their perceptions of their daily activities, professional difficulties or relationships with the natural world. Similarly, a PhD. holder receives questions that aim to get answers that are more about their specific expertise, areas of interest in research or professional backgrounds.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">On the same vein, various roles and lifestyles of the participants, such as those of housewives and college students, can affect the talks. Whether it is their academic endeavors, extracurricular activities or their responsibilities and experiences inside the home, some questions target each of the mentioned sectors. We prefer that every participant, regardless of age, career, background or occupation, finds relevance and engagement in the conversation by incorporating a wide range of questions. This strategy not only makes the Conversational scenario inclusive but also gives a better and more comprehensive view of the participants’ lives, interests and viewpoints.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Additionally, some questions cover subjects such as friendships, vacations, and place of birth, allowing all participants to contribute their viewpoints and experiences. Furthermore, to collect various speech constructs, the questions include a range of sentence structures, including the past, present, and continuous tenses. The questions also include positive and negative statements and various phrase intonations, including rising, falling and questioning tones. That allows participants to express themselves utilizing a variety of sentence structures and linguistic elements.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">We plan the dialogue structure in a way that encourages rapport and desire for participants to engage in a more natural conversation. It should start with straightforward questions like number counting and basic personal information before moving on to subjects including interests, hobbies, daily routines and lifestyles. Finally, the conversation plan uses open-ended questions to collect detailed narratives and life stories, allowing participants to offer insightful commentary. This strategy makes recording diverse and rich voice data easier while ensuring a smooth flow of conversation.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Speech Data Editing and Segmentation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The speech recordings are subject to an editing procedure that removes long pauses, excessive background noise and any intervening voices, focusing exclusively on the intended speaker’s voice. We use different time frames to evaluate the performance, accuracy, and error rates in our approaches.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Data preprocessing</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The datasets are stored in the <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">wav</span> format. This format is renowned for its uncompressed nature and superior sound quality in contrast to the MPEG audio Layer-3 (mp3) file formats.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Feature Extraction</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">We use the Mel Frequency Cepstral Coefficients (MFCC) for feature extraction. Figure <a href="#S3.F2" title="Figure 2 ‣ 3.4 Feature Extraction ‣ 3 Method ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> outlines the process of extracting the MFCC.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><svg id="S3.F2.pic1" class="ltx_picture ltx_centering" height="236.77" overflow="visible" version="1.1" width="487.39"><g transform="translate(0,236.77) matrix(1 0 0 -1 0 0) translate(69.12,0) translate(0,206.97)"><g stroke="#000000" fill="#000000"><g stroke-width="0.4pt"><g stroke="#000000" fill="#FFB3B3"><path d="M 53.52 29.53 L -53.52 29.53 C -56.58 29.53 -59.06 27.05 -59.06 23.99 L -59.06 -23.99 C -59.06 -27.05 -56.58 -29.53 -53.52 -29.53 L 53.52 -29.53 C 56.58 -29.53 59.06 -27.05 59.06 -23.99 L 59.06 23.99 C 59.06 27.05 56.58 29.53 53.52 29.53 Z M -59.06 -29.53"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -25.1 -3.38)" fill="#000000" stroke="#000000"><foreignObject width="50.2" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F2.pic1.1.1.1.1.1.1.1" class="ltx_text" style="color:#000000;">Framing</span></foreignObject></g><g stroke="#000000" fill="#B3B3FF"><path d="M 113.5 -29.53 h 127.33 v 59.06 h -127.33 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 118.11 -4.84)" fill="#000000" stroke="#000000"><foreignObject width="118.11" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F2.pic1.2.2.2.2.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:85.4pt;">
<span id="S3.F2.pic1.2.2.2.2.2.1.1.1" class="ltx_p"></span>
<span id="S3.F2.pic1.2.2.2.2.2.1.1.2" class="ltx_p"><span id="S3.F2.pic1.2.2.2.2.2.1.1.2.1" class="ltx_text" style="color:#000000;">Fast Fourier</span></span>
<span id="S3.F2.pic1.2.2.2.2.2.1.1.3" class="ltx_p"><span id="S3.F2.pic1.2.2.2.2.2.1.1.3.1" class="ltx_text" style="color:#000000;">Transform</span></span>
</span></foreignObject></g><g stroke="#000000" fill="#B3FFB3"><path d="M 290.66 -29.53 h 127.33 v 59.06 h -127.33 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 295.28 -3.54)" fill="#000000" stroke="#000000"><foreignObject width="118.11" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F2.pic1.3.3.3.3.3.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:85.4pt;">
<span id="S3.F2.pic1.3.3.3.3.3.1.1.1" class="ltx_p"></span>
<span id="S3.F2.pic1.3.3.3.3.3.1.1.2" class="ltx_p"><span id="S3.F2.pic1.3.3.3.3.3.1.1.2.1" class="ltx_text" style="color:#000000;">Mel-frequency</span></span>
<span id="S3.F2.pic1.3.3.3.3.3.1.1.3" class="ltx_p"><span id="S3.F2.pic1.3.3.3.3.3.1.1.3.1" class="ltx_text" style="color:#000000;">Filtering</span></span>
</span></foreignObject></g><g stroke="#000000" fill="#FFD9B3"><path d="M 295.28 -206.69 h 118.11 v 59.06 h -118.11 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 319.33 -180.55)" fill="#000000" stroke="#000000"><foreignObject width="69.99" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F2.pic1.4.4.4.4.4.1.1" class="ltx_text" style="color:#000000;">Log Energy</span></foreignObject></g><g stroke="#000000" fill="#ECB3C6"><path d="M 113.5 -206.69 h 127.33 v 59.06 h -127.33 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 118.11 -182.01)" fill="#000000" stroke="#000000"><foreignObject width="118.11" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F2.pic1.5.5.5.5.5.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:85.4pt;">
<span id="S3.F2.pic1.5.5.5.5.5.1.1.1" class="ltx_p"></span>
<span id="S3.F2.pic1.5.5.5.5.5.1.1.2" class="ltx_p"><span id="S3.F2.pic1.5.5.5.5.5.1.1.2.1" class="ltx_text" style="color:#000000;">Discrete Cosine</span></span>
<span id="S3.F2.pic1.5.5.5.5.5.1.1.3" class="ltx_p"><span id="S3.F2.pic1.5.5.5.5.5.1.1.3.1" class="ltx_text" style="color:#000000;">Transform</span></span>
</span></foreignObject></g><g stroke="#000000" fill="#FFFFB3"><path d="M -68.84 -206.69 h 137.68 v 59.06 h -137.68 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -64.23 -180.62)" fill="#000000" stroke="#000000"><foreignObject width="128.45" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F2.pic1.6.6.6.6.6.1.1" class="ltx_text" style="color:#000000;">Selecting Coefficients</span></foreignObject></g></g><g stroke-width="0.8pt"><path d="M 59.33 0 L 109.62 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 109.62 0)"><path d="M 3.6 0 L -2.16 2.88 L 0 0 L -2.16 -2.88" style="stroke:none"></path></g></g><g stroke-width="0.8pt"><path d="M 241.11 0 L 286.79 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 286.79 0)"><path d="M 3.6 0 L -2.16 2.88 L 0 0 L -2.16 -2.88" style="stroke:none"></path></g></g><g stroke-width="0.8pt"><path d="M 354.33 -29.8 L 354.33 -143.76" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 354.33 -143.76)"><path d="M 3.6 0 L -2.16 2.88 L 0 0 L -2.16 -2.88" style="stroke:none"></path></g></g><g stroke-width="0.8pt"><path d="M 295 -177.17 L 244.71 -177.17" style="fill:none"></path><g transform="matrix(-1.0 0.0 0.0 -1.0 244.71 -177.17)"><path d="M 3.6 0 L -2.16 2.88 L 0 0 L -2.16 -2.88" style="stroke:none"></path></g></g><g stroke-width="0.8pt"><path d="M 113.22 -177.17 L 72.71 -177.17" style="fill:none"></path><g transform="matrix(-1.0 0.0 0.0 -1.0 72.71 -177.17)"><path d="M 3.6 0 L -2.16 2.88 L 0 0 L -2.16 -2.88" style="stroke:none"></path></g></g></g><g stroke-width="0.8pt" fill="#000000" stroke="#000000" color="#000000"><path d="M 0 0 M 0 0 L 0 0 L 0 0 L 0 0 Z M 0 0" style="fill:none"></path></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Feature extraction workflow</figcaption>
</figure>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Approaches</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">We adapt two neural network approaches for our study: Artificial Neural Network (ANN) and Convolutional Neural Networks (CNN). The following sections describe the adaption of those approaches.</p>
</div>
<section id="S3.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.1 </span>Artificial Neural Network</h4>

<div id="S3.SS5.SSS1.p1" class="ltx_para">
<p id="S3.SS5.SSS1.p1.1" class="ltx_p">Figure <a href="#S3.F3" title="Figure 3 ‣ 3.5.1 Artificial Neural Network ‣ 3.5 Approaches ‣ 3 Method ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> the adapted ANN with an input layer, three hidden layers, and the output layer. In the implementation, we utilize Keras and <span id="S3.SS5.SSS1.p1.1.1" class="ltx_text ltx_font_italic">scikit-learn</span> libraries to train ANN.</p>
</div>
<div id="S3.SS5.SSS1.p2" class="ltx_para">
<p id="S3.SS5.SSS1.p2.1" class="ltx_p">The model consists of layers with varying numbers of nodes. The first layer is constructed with 512 nodes, while the second and third layers have 256 and 64 nodes, respectively. The non-linear activation function ReLU is used in all three hidden layers of the model. The first layer is the input and the last is the output. The number of nodes in the output layer is six, i.e., the number of subdialects.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><svg id="S3.F3.pic1" class="ltx_picture ltx_centering" height="394.81" overflow="visible" version="1.1" width="615.28"><g transform="translate(0,394.81) matrix(1 0 0 -1 0 0) translate(71.42,0) translate(0,197.4)"><g stroke-width="0.8pt" fill="#000080" stroke="#000080" color="#000080"><path d="M -70.87 -196.85 M -70.87 -191.32 L -70.87 191.32 C -70.87 194.37 -68.39 196.85 -65.33 196.85 L 537.77 196.85 C 540.83 196.85 543.31 194.37 543.31 191.32 L 543.31 -191.32 C 543.31 -194.37 540.83 -196.85 537.77 -196.85 L -65.33 -196.85 C -68.39 -196.85 -70.87 -194.37 -70.87 -191.32 Z M 543.31 196.85" style="fill:none"></path></g><g stroke="#000000" fill="#000000" stroke-width="0.4pt"><g stroke="#000000" fill="#808080"><path d="M 23.85 0 C 23.85 13.17 13.17 23.85 0 23.85 C -13.17 23.85 -23.85 13.17 -23.85 0 C -23.85 -13.17 -13.17 -23.85 0 -23.85 C 13.17 -23.85 23.85 -13.17 23.85 0 Z M 0 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -16.72 -3.38)" fill="#000000" stroke="#000000"><foreignObject width="33.44" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.10.10.10.10.1.1" class="ltx_text" style="color:#000080;">Input</span></foreignObject></g><g stroke="#000000" fill="#D2D2D2"><path d="M 142.19 -98.43 C 142.19 -85.13 131.41 -74.35 118.11 -74.35 C 104.81 -74.35 94.03 -85.13 94.03 -98.43 C 94.03 -111.72 104.81 -122.5 118.11 -122.5 C 131.41 -122.5 142.19 -111.72 142.19 -98.43 Z M 118.11 -98.43"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 100.43 -103.15)" fill="#000000" stroke="#000000"><foreignObject width="35.36" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{ReLU}" display="inline"><semantics id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">𝐑𝐞𝐋𝐔</mi><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">𝐑𝐞𝐋𝐔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">\mathbf{ReLU}</annotation></semantics></math></foreignObject></g><g stroke="#000000" fill="#D2D2D2"><path d="M 142.19 0 C 142.19 13.3 131.41 24.08 118.11 24.08 C 104.81 24.08 94.03 13.3 94.03 0 C 94.03 -13.3 104.81 -24.08 118.11 -24.08 C 131.41 -24.08 142.19 -13.3 142.19 0 Z M 118.11 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 100.43 -4.73)" fill="#000000" stroke="#000000"><foreignObject width="35.36" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{ReLU}" display="inline"><semantics id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">𝐑𝐞𝐋𝐔</mi><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">𝐑𝐞𝐋𝐔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.1.1.m1.1c">\mathbf{ReLU}</annotation></semantics></math></foreignObject></g><g stroke="#000000" fill="#D2D2D2"><path d="M 142.19 98.43 C 142.19 111.72 131.41 122.5 118.11 122.5 C 104.81 122.5 94.03 111.72 94.03 98.43 C 94.03 85.13 104.81 74.35 118.11 74.35 C 131.41 74.35 142.19 85.13 142.19 98.43 Z M 118.11 98.43"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 100.43 93.7)" fill="#000000" stroke="#000000"><foreignObject width="35.36" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.F3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{ReLU}" display="inline"><semantics id="S3.F3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S3.F3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.F3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">𝐑𝐞𝐋𝐔</mi><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S3.F3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.F3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">𝐑𝐞𝐋𝐔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.1.1.m1.1c">\mathbf{ReLU}</annotation></semantics></math></foreignObject></g><g stroke="#000000" fill="#D9D9D9"><path d="M 260.3 -98.43 C 260.3 -85.13 249.52 -74.35 236.22 -74.35 C 222.92 -74.35 212.14 -85.13 212.14 -98.43 C 212.14 -111.72 222.92 -122.5 236.22 -122.5 C 249.52 -122.5 260.3 -111.72 260.3 -98.43 Z M 236.22 -98.43"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 218.54 -103.15)" fill="#000000" stroke="#000000"><foreignObject width="35.36" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.F3.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{ReLU}" display="inline"><semantics id="S3.F3.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S3.F3.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.F3.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">𝐑𝐞𝐋𝐔</mi><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S3.F3.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.F3.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">𝐑𝐞𝐋𝐔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.1.1.m1.1c">\mathbf{ReLU}</annotation></semantics></math></foreignObject></g><g stroke="#000000" fill="#D9D9D9"><path d="M 260.3 0 C 260.3 13.3 249.52 24.08 236.22 24.08 C 222.92 24.08 212.14 13.3 212.14 0 C 212.14 -13.3 222.92 -24.08 236.22 -24.08 C 249.52 -24.08 260.3 -13.3 260.3 0 Z M 236.22 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 218.54 -4.73)" fill="#000000" stroke="#000000"><foreignObject width="35.36" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.F3.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{ReLU}" display="inline"><semantics id="S3.F3.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S3.F3.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.F3.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">𝐑𝐞𝐋𝐔</mi><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S3.F3.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.F3.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">𝐑𝐞𝐋𝐔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.1.1.m1.1c">\mathbf{ReLU}</annotation></semantics></math></foreignObject></g><g stroke="#000000" fill="#D9D9D9"><path d="M 260.3 98.43 C 260.3 111.72 249.52 122.5 236.22 122.5 C 222.92 122.5 212.14 111.72 212.14 98.43 C 212.14 85.13 222.92 74.35 236.22 74.35 C 249.52 74.35 260.3 85.13 260.3 98.43 Z M 236.22 98.43"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 218.54 93.7)" fill="#000000" stroke="#000000"><foreignObject width="35.36" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.F3.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{ReLU}" display="inline"><semantics id="S3.F3.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S3.F3.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.F3.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">𝐑𝐞𝐋𝐔</mi><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S3.F3.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.F3.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">𝐑𝐞𝐋𝐔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.1.1.m1.1c">\mathbf{ReLU}</annotation></semantics></math></foreignObject></g><g stroke="#000000" fill="#DFDFDF"><path d="M 378.41 -98.43 C 378.41 -85.13 367.63 -74.35 354.33 -74.35 C 341.03 -74.35 330.25 -85.13 330.25 -98.43 C 330.25 -111.72 341.03 -122.5 354.33 -122.5 C 367.63 -122.5 378.41 -111.72 378.41 -98.43 Z M 354.33 -98.43"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 336.65 -103.15)" fill="#000000" stroke="#000000"><foreignObject width="35.36" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.F3.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{ReLU}" display="inline"><semantics id="S3.F3.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S3.F3.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.F3.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">𝐑𝐞𝐋𝐔</mi><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S3.F3.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.F3.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">𝐑𝐞𝐋𝐔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.1.1.m1.1c">\mathbf{ReLU}</annotation></semantics></math></foreignObject></g><g stroke="#000000" fill="#DFDFDF"><path d="M 378.41 0 C 378.41 13.3 367.63 24.08 354.33 24.08 C 341.03 24.08 330.25 13.3 330.25 0 C 330.25 -13.3 341.03 -24.08 354.33 -24.08 C 367.63 -24.08 378.41 -13.3 378.41 0 Z M 354.33 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 336.65 -4.73)" fill="#000000" stroke="#000000"><foreignObject width="35.36" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.F3.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{ReLU}" display="inline"><semantics id="S3.F3.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S3.F3.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.F3.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">𝐑𝐞𝐋𝐔</mi><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S3.F3.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.F3.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">𝐑𝐞𝐋𝐔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.1.1.m1.1c">\mathbf{ReLU}</annotation></semantics></math></foreignObject></g><g stroke="#000000" fill="#DFDFDF"><path d="M 378.41 98.43 C 378.41 111.72 367.63 122.5 354.33 122.5 C 341.03 122.5 330.25 111.72 330.25 98.43 C 330.25 85.13 341.03 74.35 354.33 74.35 C 367.63 74.35 378.41 85.13 378.41 98.43 Z M 354.33 98.43"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 336.65 93.7)" fill="#000000" stroke="#000000"><foreignObject width="35.36" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S3.F3.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{ReLU}" display="inline"><semantics id="S3.F3.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mi id="S3.F3.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.F3.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">𝐑𝐞𝐋𝐔</mi><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S3.F3.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.F3.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">𝐑𝐞𝐋𝐔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.1.1.m1.1c">\mathbf{ReLU}</annotation></semantics></math></foreignObject></g><g stroke="#000000" fill="#E6E6E6"><path d="M 496.06 -98.43 C 496.06 -85.38 485.49 -74.8 472.44 -74.8 C 459.39 -74.8 448.82 -85.38 448.82 -98.43 C 448.82 -111.47 459.39 -122.05 472.44 -122.05 C 485.49 -122.05 496.06 -111.47 496.06 -98.43 Z M 472.44 -98.43"></path></g><g stroke="#000000" fill="#E6E6E6"><path d="M 496.06 0 C 496.06 13.05 485.49 23.62 472.44 23.62 C 459.39 23.62 448.82 13.05 448.82 0 C 448.82 -13.05 459.39 -23.62 472.44 -23.62 C 485.49 -23.62 496.06 -13.05 496.06 0 Z M 472.44 0"></path></g><g stroke="#000000" fill="#E6E6E6"><path d="M 496.06 98.43 C 496.06 111.47 485.49 122.05 472.44 122.05 C 459.39 122.05 448.82 111.47 448.82 98.43 C 448.82 85.38 459.39 74.8 472.44 74.8 C 485.49 74.8 496.06 85.38 496.06 98.43 Z M 472.44 98.43"></path></g><path d="M 18.53 -15.44 L 97.27 -81.06" style="fill:none"></path><g transform="matrix(0.76822 -0.64018 0.64018 0.76822 97.27 -81.06)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 24.12 0 L 90.99 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 90.99 0)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 18.53 15.44 L 97.27 81.06" style="fill:none"></path><g transform="matrix(0.76822 0.64018 -0.64018 0.76822 97.27 81.06)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 142.47 -98.43 L 209.1 -98.43" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 209.1 -98.43)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 136.82 -82.83 L 215.38 -17.36" style="fill:none"></path><g transform="matrix(0.76822 0.64018 -0.64018 0.76822 215.38 -17.36)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 130.64 -77.54 L 222.27 75.17" style="fill:none"></path><g transform="matrix(0.5145 0.85748 -0.85748 0.5145 222.27 75.17)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 136.82 -15.59 L 215.38 -81.06" style="fill:none"></path><g transform="matrix(0.76822 -0.64018 0.64018 0.76822 215.38 -81.06)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 142.47 0 L 209.1 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 209.1 0)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 136.82 15.59 L 215.38 81.06" style="fill:none"></path><g transform="matrix(0.76822 0.64018 -0.64018 0.76822 215.38 81.06)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 130.64 77.54 L 222.27 -75.17" style="fill:none"></path><g transform="matrix(0.5145 -0.85748 0.85748 0.5145 222.27 -75.17)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 136.82 82.83 L 215.38 17.36" style="fill:none"></path><g transform="matrix(0.76822 -0.64018 0.64018 0.76822 215.38 17.36)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 142.47 98.43 L 209.1 98.43" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 209.1 98.43)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 260.58 -98.43 L 327.21 -98.43" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 327.21 -98.43)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 254.93 -82.83 L 333.49 -17.36" style="fill:none"></path><g transform="matrix(0.76822 0.64018 -0.64018 0.76822 333.49 -17.36)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 248.75 -77.54 L 340.38 75.17" style="fill:none"></path><g transform="matrix(0.5145 0.85748 -0.85748 0.5145 340.38 75.17)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 254.93 -15.59 L 333.49 -81.06" style="fill:none"></path><g transform="matrix(0.76822 -0.64018 0.64018 0.76822 333.49 -81.06)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 260.58 0 L 327.21 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 327.21 0)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 254.93 15.59 L 333.49 81.06" style="fill:none"></path><g transform="matrix(0.76822 0.64018 -0.64018 0.76822 333.49 81.06)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 248.75 77.54 L 340.38 -75.17" style="fill:none"></path><g transform="matrix(0.5145 -0.85748 0.85748 0.5145 340.38 -75.17)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 254.93 82.83 L 333.49 17.36" style="fill:none"></path><g transform="matrix(0.76822 -0.64018 0.64018 0.76822 333.49 17.36)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 260.58 98.43 L 327.21 98.43" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 327.21 98.43)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 378.69 -98.43 L 445.77 -98.43" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 445.77 -98.43)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 373.04 -82.83 L 451.96 -17.07" style="fill:none"></path><g transform="matrix(0.76822 0.64018 -0.64018 0.76822 451.96 -17.07)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 366.86 -77.54 L 458.72 75.56" style="fill:none"></path><g transform="matrix(0.5145 0.85748 -0.85748 0.5145 458.72 75.56)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 373.04 -15.59 L 451.96 -81.35" style="fill:none"></path><g transform="matrix(0.76822 -0.64018 0.64018 0.76822 451.96 -81.35)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 378.69 0 L 445.77 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 445.77 0)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 373.04 15.59 L 451.96 81.35" style="fill:none"></path><g transform="matrix(0.76822 0.64018 -0.64018 0.76822 451.96 81.35)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 366.86 77.54 L 458.72 -75.56" style="fill:none"></path><g transform="matrix(0.5145 -0.85748 0.85748 0.5145 458.72 -75.56)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 373.04 82.83 L 451.96 17.07" style="fill:none"></path><g transform="matrix(0.76822 -0.64018 0.64018 0.76822 451.96 17.07)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><path d="M 378.69 98.43 L 445.77 98.43" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 445.77 98.43)"><path d="M 2.77 0 L -1.66 2.21 L 0 0 L -1.66 -2.21" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -35.86 37.11)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="72.11" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.11.11.11.11.1.1" class="ltx_text">Input Layer</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 73.79 132.58)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="89.02" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.12.12.12.12.1.1" class="ltx_text">Hidden Layer1</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 191.9 132.58)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="89.02" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.13.13.13.13.1.1" class="ltx_text">Hidden Layer2</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 310.01 132.58)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="89.02" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.14.14.14.14.1.1" class="ltx_text">Hidden Layer3</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 431.01 132.58)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="83.25" height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.15.15.15.15.1.1" class="ltx_text">Output Layer</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 116.19 -134.3)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="3.84" height="1.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.16.16.16.16.1.1" class="ltx_text">.</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 234.3 -134.3)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="3.84" height="1.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.17.17.17.17.1.1" class="ltx_text">.</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 352.41 -134.3)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="3.84" height="1.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.18.18.18.18.1.1" class="ltx_text">.</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 470.52 -134.3)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="3.84" height="1.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.19.19.19.19.1.1" class="ltx_text">.</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 116.19 -153.99)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="3.84" height="1.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.20.20.20.20.1.1" class="ltx_text">.</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 234.3 -153.99)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="3.84" height="1.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.21.21.21.21.1.1" class="ltx_text">.</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 352.41 -153.99)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="3.84" height="1.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.22.22.22.22.1.1" class="ltx_text">.</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 470.52 -153.99)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="3.84" height="1.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.23.23.23.23.1.1" class="ltx_text">.</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 107.73 -181.13)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.24.24.24.24.1.1" class="ltx_text">512</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 225.84 -181.13)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.25.25.25.25.1.1" class="ltx_text">256</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 347.41 -181.13)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.26.26.26.26.1.1" class="ltx_text">64</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 468.98 -181.13)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.27.27.27.27.1.1" class="ltx_text">6</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 45.22 77.25)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="27.67" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.28.28.28.28.1.1" class="ltx_text">w(1)</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 163.33 106.77)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="27.67" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.29.29.29.29.1.1" class="ltx_text">w(2)</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 281.44 106.77)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="27.67" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.30.30.30.30.1.1" class="ltx_text">w(3)</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 399.55 106.77)" fill="#000080" stroke="#000080" color="#000080"><foreignObject width="27.67" height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F3.pic1.31.31.31.31.1.1" class="ltx_text">w(4)</span></foreignObject></g></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>ANN architecture for dialect classification with the input acoustic features of speech signal and subdialects as targets with ReLU activation function in hidden layers</figcaption>
</figure>
</section>
<section id="S3.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.2 </span>Convolutional Neural Networks</h4>

<div id="S3.SS5.SSS2.p1" class="ltx_para">
<p id="S3.SS5.SSS2.p1.1" class="ltx_p">Figure <a href="#S3.F4" title="Figure 4 ‣ 3.5.2 Convolutional Neural Networks ‣ 3.5 Approaches ‣ 3 Method ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the CNN model that comprises three subsequent convolutional layers, each followed by ReLU activation and max pooling. The initial convolutional layer implements downsampling of the feature maps by applying max pooling with a pool size of 3x3 and a stride of 2x2. The third layer of convolution in the model employs the max pooling technique with a pool size of 2x2 and stride. The utilisation of batch normalisation is implemented in the initial and secondary convolutional layers to normalise the activations of the layers, thereby enhancing the efficacy of the training process. Following the convolutional layers, the resultant output is transformed into a 1D (dimensional) vector using the <span id="S3.SS5.SSS2.p1.1.1" class="ltx_text ltx_font_italic">Flatten()</span> layer. This step involves preparing the data for the fully connected layers that follow. The architecture comprises a densely connected layer of 64 neurons and ReLU activation. The dropout regularisation technique has been employed with a rate of 0.3 to address the overfitting issue. The ultimate layer of output consists of six neurons that utilise Softmax activation to generate the anticipated probabilities for the various categories.</p>
</div>
<div id="S3.SS5.SSS2.p2" class="ltx_para">
<p id="S3.SS5.SSS2.p2.1" class="ltx_p">To calculate the total number of neurons, we sum up the neuron. We have 288 neurons in the first and second layers and 128 neurons in the third layer. The flatten Layer does not add any additional neurons. It reshapes the output of the last convolutional layer to a 1-D vector. the First Dense Layer, which was fully connected, outputs 64 neurons and 6 neurons would be the output of the output layer based on the number of classes.
Therefore, the overall number of neurons in the model is 288 + 288 + 128 + 64 + 6 = 774.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2404.00124/assets/aa_HD.jpg" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="628" height="291" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The adapted CNN</figcaption>
</figure>
<div id="S3.SS5.SSS2.p3" class="ltx_para">
<p id="S3.SS5.SSS2.p3.1" class="ltx_p">In the next step, we calculate the candidate cell state (<math id="S3.SS5.SSS2.p3.1.m1.1" class="ltx_Math" alttext="\widetilde{C_{t}}" display="inline"><semantics id="S3.SS5.SSS2.p3.1.m1.1a"><mover accent="true" id="S3.SS5.SSS2.p3.1.m1.1.1" xref="S3.SS5.SSS2.p3.1.m1.1.1.cmml"><msub id="S3.SS5.SSS2.p3.1.m1.1.1.2" xref="S3.SS5.SSS2.p3.1.m1.1.1.2.cmml"><mi id="S3.SS5.SSS2.p3.1.m1.1.1.2.2" xref="S3.SS5.SSS2.p3.1.m1.1.1.2.2.cmml">C</mi><mi id="S3.SS5.SSS2.p3.1.m1.1.1.2.3" xref="S3.SS5.SSS2.p3.1.m1.1.1.2.3.cmml">t</mi></msub><mo id="S3.SS5.SSS2.p3.1.m1.1.1.1" xref="S3.SS5.SSS2.p3.1.m1.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS2.p3.1.m1.1b"><apply id="S3.SS5.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS5.SSS2.p3.1.m1.1.1"><ci id="S3.SS5.SSS2.p3.1.m1.1.1.1.cmml" xref="S3.SS5.SSS2.p3.1.m1.1.1.1">~</ci><apply id="S3.SS5.SSS2.p3.1.m1.1.1.2.cmml" xref="S3.SS5.SSS2.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS5.SSS2.p3.1.m1.1.1.2.1.cmml" xref="S3.SS5.SSS2.p3.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS5.SSS2.p3.1.m1.1.1.2.2.cmml" xref="S3.SS5.SSS2.p3.1.m1.1.1.2.2">𝐶</ci><ci id="S3.SS5.SSS2.p3.1.m1.1.1.2.3.cmml" xref="S3.SS5.SSS2.p3.1.m1.1.1.2.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS2.p3.1.m1.1c">\widetilde{C_{t}}</annotation></semantics></math>) using the hyperbolic tangent function (<span id="S3.SS5.SSS2.p3.1.1" class="ltx_text ltx_font_italic">tanh</span>) (See Formula <a href="#S3.E1" title="In 3.5.2 Convolutional Neural Networks ‣ 3.5 Approaches ‣ 3 Method ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S3.SS5.SSS2.p4" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\widetilde{C_{t}}=\textit{tanh}(W_{C}\cdot[h_{t-1},x_{t}]+b_{C})" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mover accent="true" id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><msub id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml"><mi id="S3.E1.m1.1.1.3.2.2" xref="S3.E1.m1.1.1.3.2.2.cmml">C</mi><mi id="S3.E1.m1.1.1.3.2.3" xref="S3.E1.m1.1.1.3.2.3.cmml">t</mi></msub><mo id="S3.E1.m1.1.1.3.1" xref="S3.E1.m1.1.1.3.1.cmml">~</mo></mover><mo id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3a.cmml">tanh</mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.2.4" xref="S3.E1.m1.1.1.1.1.1.1.2.4.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2.4.2" xref="S3.E1.m1.1.1.1.1.1.1.2.4.2.cmml">W</mi><mi id="S3.E1.m1.1.1.1.1.1.1.2.4.3" xref="S3.E1.m1.1.1.1.1.1.1.2.4.3.cmml">C</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.2.3.cmml">⋅</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.2.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.2.2.3.cmml">[</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">h</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">t</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.E1.m1.1.1.1.1.1.1.2.2.2.4" xref="S3.E1.m1.1.1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.E1.m1.1.1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2.2.2.cmml">x</mi><mi id="S3.E1.m1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2.2.3.cmml">t</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.2.2.2.5" xref="S3.E1.m1.1.1.1.1.1.1.2.2.3.cmml">]</mo></mrow></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">+</mo><msub id="S3.E1.m1.1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.1.4.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.4.2" xref="S3.E1.m1.1.1.1.1.1.1.4.2.cmml">b</mi><mi id="S3.E1.m1.1.1.1.1.1.1.4.3" xref="S3.E1.m1.1.1.1.1.1.1.4.3.cmml">C</mi></msub></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"></eq><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><ci id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3.1">~</ci><apply id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.3.2.2">𝐶</ci><ci id="S3.E1.m1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.3.2.3">𝑡</ci></apply></apply><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><times id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.3a.cmml" xref="S3.E1.m1.1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3">tanh</mtext></ci><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><plus id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3"></plus><apply id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2"><ci id="S3.E1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3">⋅</ci><apply id="S3.E1.m1.1.1.1.1.1.1.2.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.2.4.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.4">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.4.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.4.2">𝑊</ci><ci id="S3.E1.m1.1.1.1.1.1.1.2.4.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.4.3">𝐶</ci></apply><interval closure="closed" id="S3.E1.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2"><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2">ℎ</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3"><minus id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1"></minus><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2">𝑡</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2.2.2">𝑥</ci><ci id="S3.E1.m1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2.2.2.3">𝑡</ci></apply></interval></apply><apply id="S3.E1.m1.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.4.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.4.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.4.2">𝑏</ci><ci id="S3.E1.m1.1.1.1.1.1.1.4.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.4.3">𝐶</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\widetilde{C_{t}}=\textit{tanh}(W_{C}\cdot[h_{t-1},x_{t}]+b_{C})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS5.SSS2.p5" class="ltx_para">
<p id="S3.SS5.SSS2.p5.1" class="ltx_p">The result <math id="S3.SS5.SSS2.p5.1.m1.1" class="ltx_Math" alttext="\widetilde{C_{t}}" display="inline"><semantics id="S3.SS5.SSS2.p5.1.m1.1a"><mover accent="true" id="S3.SS5.SSS2.p5.1.m1.1.1" xref="S3.SS5.SSS2.p5.1.m1.1.1.cmml"><msub id="S3.SS5.SSS2.p5.1.m1.1.1.2" xref="S3.SS5.SSS2.p5.1.m1.1.1.2.cmml"><mi id="S3.SS5.SSS2.p5.1.m1.1.1.2.2" xref="S3.SS5.SSS2.p5.1.m1.1.1.2.2.cmml">C</mi><mi id="S3.SS5.SSS2.p5.1.m1.1.1.2.3" xref="S3.SS5.SSS2.p5.1.m1.1.1.2.3.cmml">t</mi></msub><mo id="S3.SS5.SSS2.p5.1.m1.1.1.1" xref="S3.SS5.SSS2.p5.1.m1.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS2.p5.1.m1.1b"><apply id="S3.SS5.SSS2.p5.1.m1.1.1.cmml" xref="S3.SS5.SSS2.p5.1.m1.1.1"><ci id="S3.SS5.SSS2.p5.1.m1.1.1.1.cmml" xref="S3.SS5.SSS2.p5.1.m1.1.1.1">~</ci><apply id="S3.SS5.SSS2.p5.1.m1.1.1.2.cmml" xref="S3.SS5.SSS2.p5.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS5.SSS2.p5.1.m1.1.1.2.1.cmml" xref="S3.SS5.SSS2.p5.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS5.SSS2.p5.1.m1.1.1.2.2.cmml" xref="S3.SS5.SSS2.p5.1.m1.1.1.2.2">𝐶</ci><ci id="S3.SS5.SSS2.p5.1.m1.1.1.2.3.cmml" xref="S3.SS5.SSS2.p5.1.m1.1.1.2.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS2.p5.1.m1.1c">\widetilde{C_{t}}</annotation></semantics></math> represents the candidate values could be added to the cell state in the LSTM unit.</p>
</div>
</section>
<section id="S3.SS5.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.3 </span>Recurrent Neural Networks-Long Short-Term Memory</h4>

<div id="S3.SS5.SSS3.p1" class="ltx_para">
<p id="S3.SS5.SSS3.p1.1" class="ltx_p">For Recurrent Neural Networks-Long Short-Term Memory(RNN-LSTM), we consider five different divisions of training, validation and testing: 50:25:25, 60:20:20, 70:15:15, 80:10:10, and 90:5:5. We build a model when two consecutive LSTM layers are used. The model consists of two stacked LSTM layers, where the first layer processes the input sequence and the second layer processes the output sequence of the first layer. We define the first LSTM layer with 64 units. It takes the input_shape as its input and has <span id="S3.SS5.SSS3.p1.1.1" class="ltx_text ltx_font_italic">return_sequences=True</span>, which means it will return the output sequence rather than just the final output. This feature is typically used when stacking multiple LSTM layers. Furthermore, the second LSTM layer is also defined with 64 units. It does not have <span id="S3.SS5.SSS3.p1.1.2" class="ltx_text ltx_font_italic">return_sequences=True</span>, which means it will only return the final output instead of the entire sequence. Following the LSTM layers, a dense layer is added to the model using the Dense class from Keras. It consists of 64 units and uses the ReLU activation function.To prevent overfitting, a dropout layer is included after the dense layer. Finally, the Dense class with a Softmax activation function adds an output layer to the model. It consists of 6 units, corresponding to the number of classes in the classification task.</p>
</div>
<div id="S3.SS5.SSS3.p2" class="ltx_para">
<p id="S3.SS5.SSS3.p2.1" class="ltx_p">In addition, callbacks are defined to enhance the training process. In our implementation, the Earlystopping callback is used to monitor the validation loss and stop training early if there is no improvement in 10 epochs. The Earlystopping callback is passed, as we mentioned earlier.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments, Results, and Discussion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The following sections report on the data collection, describes the experiments, presents the results, and discusses the outcomes.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Data Collection</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We designed a guideline according to what we mentioned in the methodology. The guideline included 83 questions in five sections: counting (two questions), biography (20), daily routine (25), hobbies and interests (26), and long answers (10).</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span> Speaker Identification</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">In recruiting speakers we chose individuals proficient in the Kurdish-Sorani subdialects being targeted, free of any speech impediments and comfortable with having their speech recorded. The identification of speakers from specific subdialects presented difficulties, causing us to ask for the help of students in Hewlêr (Erbil) who were native speakers of the specific Kurdish-Sorani subdialects and stayed in university accommodations. Following that, we visited the districts, towns, and cities where they reside, such as Balisan, Garmian, Sulaimani, and Kirkuk. In areas that we could not reach for various reasons, we recoreded the interviews through online platforms, such as WhatsApp and Telegram.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Ethical Considerations</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">Before starting the recording sessions, an individual proficient in the language in question was engaged to aid in selecting and assessing the speaker’s accent. We also asked the participants to provide us permission by signing a formal document to authorize us to disseminate the records publicly. For the online participants, the act of signing the contract was facilitated by acquaintances or relatives of the parties involved, who granted permission to use the voice recordings.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Recording Locations</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The recording sessions were conducted in a range of stable environments, such as libraries, reception areas of accommodations, the living rooms of participants and classrooms within academic institutions. We deliberately recorded speech in typical, natural settings in the above-mentioned places. Throughout the recording process, ambient sounds and background noise typically present in such settings were recorded, thus providing a natural acoustic background. The objective was to capture audio in settings that simulated typical, everyday circumstances, ensuring a balance between ambient noise and clarity of speech, avoiding excessively loud or completely noise-free environments.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Recording Configuration </h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">During the recording sessions, the participants were asked to speak into a microphone connected to a laptop. The microphone was placed near the participants while we were at a distance. This setup allowed the recording of the participants’ speech data to occur simultaneously. Regarding the hardware, the recording microphone met the following specifications: 192K/24b sample rate, low impedance output (680Q), 100Hz–18000Hz frequency range and a maximum input sound pressure level of 125 dB, connected to laptop HP Pavilion x360 Convertible 14-dh2xxx.
In addition, the recording and editing process involved taking advantage of Audacity, a software application deliberately selected for its capacity to manage and edit audio recordings effectively. To improve the quality and realism of the auditory experience, stereo channels were utilised instead of mono <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">Giubilato et al., 2016</a>]</cite>. The audio recordings were obtained at a sampling frequency of 44100 Hz and sample rate of 22050 Hz and 11,025 Hz of bandwidth utilising a 32-bit depth and were saved in the.aup3 (Audacity 3 Project File) format using the Audacity software. Following the editing procedure, the data was encoded using Pulse Code Modulation (PCM) with a bit depth of 16. The resultant files were then stored in the wav (Waveform Audio File) format. The voices of the participants who submitted their recordings online were transmitted via platforms such as WhatsApp or Telegram and were subsequently converted into the .wav format.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>The Environment Configuration</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Python has been chosen as the programming language for model development because its numerous efficient libraries facilitate a more straightforward and faster process.
The libraries used for this thesis are as follows:
Regarding the environment, The utilisation of Google Collaboratory is preferred for this research due to the high computational requirements for training deep learning models, which are often challenging to access. The utilisation of a Jupiter notebook platform enables the facilitation of deep learning model training on Graphics Processing Units (GPUs) via cloud computing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">Garbade, 2021</a>]</cite>.
Additionally, Colab provides cost-effective subscription options such as Colab Pro and Colab Pro+, which offer enhanced features such as a higher-performing GPU, increased RAM and extended runtime. Despite the continued limitations on GPU time, it remains significantly higher than that of the free plan, as noted by <span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_bold">?</span>). As depicted in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.4 The Environment Configuration ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the NVIDIA Tesla P100 GPU is typically the provided option.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2404.00124/assets/colab.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="471" height="275" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The type of GPU provided by Google Collaboratory.</figcaption>
</figure>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">Regarding the platforms, we used Pycharm and Jupyter and the libraries were:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">TensorFlow</span>: TensorFlow is a powerful open-source library for machine learning and deep learning. It provides a comprehensive set of tools and functionalities for building and training neural network models.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">TensorFlow.keras.callbacks.EarlyStopping</span>: This callback provided by TensorFlow Keras allows for early stopping during model training based on specified criteria.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Json</span>: Used for reading and loading data from a JSON file.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><span id="S4.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">Numpy</span>: Used for array manipulation and processing.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p"><span id="S4.I1.i5.p1.1.1" class="ltx_text ltx_font_italic">TensorFlow.Keras</span>: Used for building and training the neural network model, it was an independent library, but, starting from TensorFlow 2.0, Keras became integrated into TensorFlow as the official high-level API.</p>
</div>
</li>
<li id="S4.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i6.p1" class="ltx_para">
<p id="S4.I1.i6.p1.1" class="ltx_p"><span id="S4.I1.i6.p1.1.1" class="ltx_text ltx_font_italic">Time</span>: Used for measuring the training time in seconds.</p>
</div>
</li>
<li id="S4.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i7.p1" class="ltx_para">
<p id="S4.I1.i7.p1.1" class="ltx_p"><span id="S4.I1.i7.p1.1.1" class="ltx_text ltx_font_italic">Sklearn.model_selection.train_test_split</span>: The data is split into training, validation and testing sets.</p>
</div>
</li>
</ul>
<p id="S4.SS4.p2.2" class="ltx_p">Regarding the hardware, the specifications mattered as some of the experiments were run on the laptop. The specifications were as follows:</p>
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">CPU: Intel(R) Core(TM) i7.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">RAM: 8.00 GB.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p">Operating System: 64-bit.</p>
</div>
</li>
<li id="S4.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i4.p1" class="ltx_para">
<p id="S4.I2.i4.p1.1" class="ltx_p">GPU: Intel(R) Iris (R) Plus Graphics.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Dataset Preparation</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">The total record time is 29 hours, 16 minutes and 40 seconds. Figure <a href="#S4.F6" title="Figure 6 ‣ 4.5 Dataset Preparation ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and Table <a href="#S4.T2" title="Table 2 ‣ 4.5 Dataset Preparation ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrate the details of the dataset, which we named it Sorani Nas (in English, Sorani Recognizer).</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Duration of recordings for different subdialects in Sorani Nas</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.1.1.1" class="ltx_p" style="width:85.4pt;">Subdialects</span>
</span>
</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T2.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.2.1.1" class="ltx_p" style="width:170.7pt;">Duration</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T2.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.2.1.1.1.1" class="ltx_p" style="width:85.4pt;">Garmiani</span>
</span>
</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.2.1.2.1.1" class="ltx_p" style="width:170.7pt;">2 hours, 58 minutes, 34 seconds</span>
</span>
</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T2.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.3.2.1.1.1" class="ltx_p" style="width:85.4pt;">Sulaimani</span>
</span>
</td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.3.2.2.1.1" class="ltx_p" style="width:170.7pt;">4 hours, 29 minutes, 27 seconds</span>
</span>
</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T2.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.4.3.1.1.1" class="ltx_p" style="width:85.4pt;">Khoshnawi</span>
</span>
</td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.4.3.2.1.1" class="ltx_p" style="width:170.7pt;">4 hours, 50 minutes, 22 seconds</span>
</span>
</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<td id="S4.T2.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T2.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.5.4.1.1.1" class="ltx_p" style="width:85.4pt;">Karkuki</span>
</span>
</td>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.5.4.2.1.1" class="ltx_p" style="width:170.7pt;">5 hours, 45 minutes</span>
</span>
</td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<td id="S4.T2.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T2.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.6.5.1.1.1" class="ltx_p" style="width:85.4pt;">Hewleri</span>
</span>
</td>
<td id="S4.T2.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T2.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.6.5.2.1.1" class="ltx_p" style="width:170.7pt;">5 hours, 13 minutes</span>
</span>
</td>
</tr>
<tr id="S4.T2.1.7.6" class="ltx_tr">
<td id="S4.T2.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T2.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.7.6.1.1.1" class="ltx_p" style="width:85.4pt;">Pishdari</span>
</span>
</td>
<td id="S4.T2.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T2.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.7.6.2.1.1" class="ltx_p" style="width:170.7pt;">6 hours, 49 minutes, 16 seconds</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F6" class="ltx_figure"><svg id="S4.F6.pic1" class="ltx_picture ltx_centering" height="331.71" overflow="visible" version="1.1" width="604.44"><g transform="translate(0,331.71) matrix(1 0 0 -1 0 0) translate(20.72,0) translate(0,47.21)" fill="#000000" stroke="#000000"><g stroke-width="1.5pt"><path d="M -19.69 -23.62 M -19.69 -23.62 L -19.69 283.46 L 582.68 283.46 L 582.68 -23.62 Z M 582.68 283.46" style="fill:none"></path></g><g stroke-width="0.4pt"><g stroke="#808080" fill="#808080" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#808080"><path d="M 0 0 L 582.68 0" style="fill:none"></path></g><g stroke="#808080" fill="#808080" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#808080"><path d="M 0 47.24 L 582.68 47.24" style="fill:none"></path></g><g stroke="#808080" fill="#808080" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#808080"><path d="M 0 94.49 L 582.68 94.49" style="fill:none"></path></g><g stroke="#808080" fill="#808080" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#808080"><path d="M 0 141.73 L 582.68 141.73" style="fill:none"></path></g><g stroke="#808080" fill="#808080" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#808080"><path d="M 0 188.98 L 582.68 188.98" style="fill:none"></path></g><g stroke="#808080" fill="#808080" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#808080"><path d="M 0 236.22 L 582.68 236.22" style="fill:none"></path></g><g stroke="#808080" fill="#808080" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" color="#808080"><path d="M 0 283.46 L 582.68 283.46" style="fill:none"></path></g><g fill="#4D4DFF"><path d="M 0 0 M 0 0 L 0 259.84 L 78.74 259.84 L 78.74 0 Z M 78.74 259.84"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 17.85 125.12)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="43.05" height="11.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S4.F6.pic1.1.1.1.1.1.1" class="ltx_text ltx_font_sansserif">23.42%</span></foreignObject></g><g fill="#FF4D4D"><path d="M 98.43 0 M 98.43 0 L 98.43 195.38 L 177.17 195.38 L 177.17 0 Z M 177.17 195.38"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 116.27 92.89)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="43.05" height="11.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S4.F6.pic1.2.2.2.2.1.1" class="ltx_text ltx_font_sansserif">17.61%</span></foreignObject></g><g fill="#4DFF4D"><path d="M 196.85 0 M 196.85 0 L 196.85 189.83 L 275.59 189.83 L 275.59 0 Z M 275.59 189.83"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 214.7 90.11)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="43.05" height="11.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S4.F6.pic1.3.3.3.3.1.1" class="ltx_text ltx_font_sansserif">17.11%</span></foreignObject></g><g fill="#FFA64D"><path d="M 295.28 0 M 295.28 0 L 295.28 164.98 L 374.02 164.98 L 374.02 0 Z M 374.02 164.98"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 313.12 77.69)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="43.05" height="11.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S4.F6.pic1.4.4.4.4.1.1" class="ltx_text ltx_font_sansserif">14.87%</span></foreignObject></g><g fill="#D24D79"><path d="M 393.7 0 M 393.7 0 L 393.7 161.76 L 472.44 161.76 L 472.44 0 Z M 472.44 161.76"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 411.55 76.08)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="43.05" height="11.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S4.F6.pic1.5.5.5.5.1.1" class="ltx_text ltx_font_sansserif">14.58%</span></foreignObject></g><g fill="#4DA6A6"><path d="M 492.13 0 M 492.13 0 L 492.13 112.83 L 570.87 112.83 L 570.87 0 Z M 570.87 112.83"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 509.97 51.61)" fill="#FFFFFF" stroke="#FFFFFF"><foreignObject width="43.05" height="11.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF"><span id="S4.F6.pic1.6.6.6.6.1.1" class="ltx_text ltx_font_sansserif">10.17%</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 14.23 -42.6)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 4.805)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 9.61)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Pishdari</text></g></g></g></g><g transform="matrix(1.0 0.0 0.0 1.0 114.91 -42.6)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 4.805)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 9.61)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Hewleri</text></g></g></g></g><g transform="matrix(1.0 0.0 0.0 1.0 211.6 -42.6)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 4.805)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 9.61)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Karkuki</text></g></g></g></g><g transform="matrix(1.0 0.0 0.0 1.0 301.36 -42.6)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 4.805)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 9.61)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Khoshnawi</text></g></g></g></g><g transform="matrix(1.0 0.0 0.0 1.0 403.09 -42.6)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 4.805)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 9.61)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Sulaimani</text></g></g></g></g><g transform="matrix(1.0 0.0 0.0 1.0 502.99 -42.52)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 4.73)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 9.46)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Garmiani</text></g></g></g></g></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Distribution and percentage of each subdialect in Sorani Nas</figcaption>
</figure>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">Figures <a href="#S4.F7" title="Figure 7 ‣ 4.5 Dataset Preparation ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and <a href="#S4.F8" title="Figure 8 ‣ 4.5 Dataset Preparation ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> illustrate the gender of the speakers, the method of the interview, the education level of participants, and their age range, respectively.</p>
</div>
<figure id="S4.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F7.1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S4.F7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:0pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(213.5pt,0.0pt) scale(65.0430985154701,65.0430985154701) ;"><svg id="S4.F7.1.1.pic1" class="ltx_picture" height="1" overflow="visible" version="1.1" width="1"><g transform="translate(0,1) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignObject width="0" height="0" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F7.1.1.pic1.1.1.1.1" class="ltx_ERROR undefined">\pie</span></foreignObject></g></svg>
</span></div>
<figcaption class="ltx_caption ltx_centering">(a) Gender Distribution</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F7.2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S4.F7.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:0pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(213.5pt,0.0pt) scale(65.0430985154701,65.0430985154701) ;"><svg id="S4.F7.2.1.pic1" class="ltx_picture" height="1" overflow="visible" version="1.1" width="1"><g transform="translate(0,1) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignObject width="0" height="0" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F7.2.1.pic1.1.1.1.1" class="ltx_ERROR undefined">\pie</span></foreignObject></g></svg>
</span></div>
<figcaption class="ltx_caption ltx_centering">(b) Gathering Methods: Online vs. Face-to-Face</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The distribution of participants’ genders and gathering method of the recording audios in Sorani Nas dataset</figcaption>
</figure>
<figure id="S4.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F8.1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S4.F8.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:0pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(213.5pt,0.0pt) scale(65.0430985154701,65.0430985154701) ;"><svg id="S4.F8.1.1.pic1" class="ltx_picture" height="1" overflow="visible" version="1.1" width="1"><g transform="translate(0,1) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignObject width="0" height="0" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F8.1.1.pic1.1.1.1.1" class="ltx_ERROR undefined">\pie</span><!-- %**** Ku-SubDialRecognition.tex Line 525 **** --></foreignObject></g></svg>
</span></div>
<figcaption class="ltx_caption ltx_centering">(a) Educational Level Distribution</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F8.2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S4.F8.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:0pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(213.5pt,0.0pt) scale(65.0430985154701,65.0430985154701) ;"><svg id="S4.F8.2.1.pic1" class="ltx_picture" height="1" overflow="visible" version="1.1" width="1"><g transform="translate(0,1) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignObject width="0" height="0" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F8.2.1.pic1.1.1.1.1" class="ltx_ERROR undefined">\pie</span></foreignObject></g></svg>
</span></div>
<figcaption class="ltx_caption ltx_centering">(b) Age Distribution</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The distribution of ages and educational level of the participants in Sorani Nas dataset</figcaption>
</figure>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‣ 4.5 Dataset Preparation ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> provides an overview of the Sorani Nas dataset, briefly covering the important information discussed in previous sections.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Specifications of Sorani Nas speech dataset</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T3.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.1.1.1.1" class="ltx_p" style="width:113.8pt;">Title</span>
</span>
</th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T3.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.1.1.2.1.1" class="ltx_p" style="width:113.8pt;">Value</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<td id="S4.T3.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T3.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.2.1.1.1.1" class="ltx_p" style="width:113.8pt;">The dataset name</span>
</span>
</td>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.2.1.2.1.1" class="ltx_p" style="width:113.8pt;">Sorani Nas(SN)</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<td id="S4.T3.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T3.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.3.2.1.1.1" class="ltx_p" style="width:113.8pt;">Recording hardware</span>
</span>
</td>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.3.2.2.1.1" class="ltx_p" style="width:113.8pt;">microphone</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<td id="S4.T3.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T3.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.4.3.1.1.1" class="ltx_p" style="width:113.8pt;">Recording software</span>
</span>
</td>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.4.3.2.1.1" class="ltx_p" style="width:113.8pt;">Audacity</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<td id="S4.T3.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T3.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.5.4.1.1.1" class="ltx_p" style="width:113.8pt;">Duration</span>
</span>
</td>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.5.4.2.1.1" class="ltx_p" style="width:113.8pt;">29h 16m 40 sec</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.6.5" class="ltx_tr">
<td id="S4.T3.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T3.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.6.5.1.1.1" class="ltx_p" style="width:113.8pt;">Number of speakers</span>
</span>
</td>
<td id="S4.T3.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.6.5.2.1.1" class="ltx_p" style="width:113.8pt;">107</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.7.6" class="ltx_tr">
<td id="S4.T3.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T3.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.7.6.1.1.1" class="ltx_p" style="width:113.8pt;">Average Duration of Speakers</span>
</span>
</td>
<td id="S4.T3.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.7.6.2.1.1" class="ltx_p" style="width:113.8pt;">16.4m</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.8.7" class="ltx_tr">
<td id="S4.T3.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T3.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.8.7.1.1.1" class="ltx_p" style="width:113.8pt;">Sample rate</span>
</span>
</td>
<td id="S4.T3.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.8.7.2.1.1" class="ltx_p" style="width:113.8pt;">44100 Hz</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.9.8" class="ltx_tr">
<td id="S4.T3.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T3.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.9.8.1.1.1" class="ltx_p" style="width:113.8pt;">Frequency</span>
</span>
</td>
<td id="S4.T3.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T3.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.9.8.2.1.1" class="ltx_p" style="width:113.8pt;">22050 Hz</span>
</span>
</td>
</tr>
<tr id="S4.T3.1.10.9" class="ltx_tr">
<td id="S4.T3.1.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T3.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.10.9.1.1.1" class="ltx_p" style="width:113.8pt;">Format</span>
</span>
</td>
<td id="S4.T3.1.10.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T3.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T3.1.10.9.2.1.1" class="ltx_p" style="width:113.8pt;">wav</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.1" class="ltx_p">Figure <a href="#S4.F6" title="Figure 6 ‣ 4.5 Dataset Preparation ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> provides a summary of the subdialect percentages in our data set Sorani Nas dataset, and Figures <a href="#S4.F9" title="Figure 9 ‣ 4.5 Dataset Preparation ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, <a href="#S4.F10" title="Figure 10 ‣ 4.5 Dataset Preparation ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, <a href="#S4.F11" title="Figure 11 ‣ 4.5 Dataset Preparation ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, <a href="#S4.F12" title="Figure 12 ‣ 4.5 Dataset Preparation ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>, <a href="#S4.F13" title="Figure 13 ‣ 4.5 Dataset Preparation ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>, and <a href="#S4.F14" title="Figure 14 ‣ 4.5 Dataset Preparation ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> show prescribed samples of the recordings, which are about greetings and their morning routines, for the studied subdialects.</p>
</div>
<figure id="S4.F9" class="ltx_figure">
<p id="S4.F9.1" class="ltx_p ltx_align_center"><span id="S4.F9.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/2404.00124/assets/garmiani_text.png" id="S4.F9.1.1.g1" class="ltx_graphics ltx_img_landscape" width="569" height="236" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>A sample of transcribed Garmiani speech from Sorani Nas</figcaption>
</figure>
<figure id="S4.F10" class="ltx_figure">
<p id="S4.F10.1" class="ltx_p ltx_align_center"><span id="S4.F10.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/2404.00124/assets/hewleri_text.png" id="S4.F10.1.1.g1" class="ltx_graphics ltx_img_landscape" width="569" height="236" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>A sample of transcribed Hewleri speech from Sorani Nas</figcaption>
</figure>
<figure id="S4.F11" class="ltx_figure">
<p id="S4.F11.1" class="ltx_p ltx_align_center"><span id="S4.F11.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/2404.00124/assets/khoshnawi_text.png" id="S4.F11.1.1.g1" class="ltx_graphics ltx_img_landscape" width="569" height="236" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>A sample of transcribed Khoshnawi speech from Sorani Nas</figcaption>
</figure>
<figure id="S4.F12" class="ltx_figure">
<p id="S4.F12.1" class="ltx_p ltx_align_center"><span id="S4.F12.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/2404.00124/assets/pishdari_text.png" id="S4.F12.1.1.g1" class="ltx_graphics ltx_img_landscape" width="569" height="275" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>A sample of transcribed Pishdari speech from Sorani Nas</figcaption>
</figure>
<figure id="S4.F13" class="ltx_figure">
<p id="S4.F13.1" class="ltx_p ltx_align_center"><span id="S4.F13.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/2404.00124/assets/Sulaimani_text.png" id="S4.F13.1.1.g1" class="ltx_graphics ltx_img_landscape" width="569" height="236" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>A sample of transcribed Sulaimani speech from Sorani Nas</figcaption>
</figure>
<figure id="S4.F14" class="ltx_figure">
<p id="S4.F14.1" class="ltx_p ltx_align_center"><span id="S4.F14.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/2404.00124/assets/karkuki_text.png" id="S4.F14.1.1.g1" class="ltx_graphics ltx_img_landscape" width="569" height="236" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>A sample of transcribed Karkuki speech from Sorani Nas</figcaption>
</figure>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Balancing The Dataset</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">The generated audio samples ranged in length from six minutes to forty-five minutes. To generate a dataset with wider diversity, we segmented the recordings into discrete time intervals, namely 1-second, 3-second, 5-second, 10-second, and 30-second segments.</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.1" class="ltx_p">We attempted to balance Sorani Nas with two techniques: undersampling and oversampling. The oversampling approach, as presented in Figure <a href="#S4.F15" title="Figure 15 ‣ 4.6 Balancing The Dataset ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>, depicts the distribution of classes within the imbalanced Sorani Nas dataset, particularly emphasising the subdialect classes. The dataset displays a notable imbalance among the subdialect classes, with dissimilar quantities of samples for each class.
The Random Oversampling technique was implemented on the dataset’s 3-second duration samples to minimise this issue. Consequently, the class distribution was altered, increasing the number of samples for each subdialect class to 8172, which, in this case, is the Pishdari subdialect, representing the maximum number of samples across all classes. The process of equalising sample counts was undertaken to mitigate the class imbalance and ensure a more equitable portrayal of the subdialects present in the dataset.</p>
</div>
<figure id="S4.F15" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F15.3" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S4.F15.3.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;">           </span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F15.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.00124/assets/3im.png" id="S4.F15.1.g1" class="ltx_graphics ltx_img_landscape" width="628" height="493" alt="Refer to caption">
<figcaption class="ltx_caption">(a) Imbalanced Sorani Nas</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F15.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.00124/assets/3ob.png" id="S4.F15.2.g1" class="ltx_graphics ltx_img_landscape" width="628" height="493" alt="Refer to caption">
<figcaption class="ltx_caption">(b) Balanced Sorani Nas</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Handling class imbalance in the Sorani Nas dataset using Random Oversampling</figcaption>
</figure>
<figure id="S4.F16" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F16.3" class="ltx_p ltx_figure_panel ltx_align_center"><span id="S4.F16.3.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;">           </span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F16.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.00124/assets/3im.png" id="S4.F16.1.g1" class="ltx_graphics ltx_img_landscape" width="628" height="493" alt="Refer to caption">
<figcaption class="ltx_caption">(a) Imbalanced Sorani Nas</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F16.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.00124/assets/3ub.png" id="S4.F16.2.g1" class="ltx_graphics ltx_img_landscape" width="628" height="493" alt="Refer to caption">
<figcaption class="ltx_caption">(b) Balanced Sorani Nas</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Handling Class Imbalance in the Sorani Nas dataset using Undersampling</figcaption>
</figure>
<div id="S4.SS6.p3" class="ltx_para">
<p id="S4.SS6.p3.1" class="ltx_p">Figure <a href="#S4.F16" title="Figure 16 ‣ 4.6 Balancing The Dataset ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> depicts the initial imbalanced distribution of subdialect classes in the Sorani Nas dataset. The dataset displayed heterogeneous sample sizes across the subdialect classes before applying undersampling. The observed difference was notably conspicuous, as the Garmiani subdialect had the smallest sample size, amounting to only 3566 samples. To tackle this problem, the dataset’s 3-second duration samples were subjected, as an example, to an undersampling technique. The undersampling process included randomly selecting a subset of samples from the majority class, reducing its quantity to align with the number of samples in the minority class. Consequently, the dataset was adjusted to ensure equal representation of each subdialect class by standardising the number of samples to 3566. This approach aimed to achieve a more equitable distribution of subdialects in the dataset.</p>
</div>
<figure id="S4.F17" class="ltx_figure"><img src="/html/2404.00124/assets/LSTM_Layers.png" id="S4.F17.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="432" height="185" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>RNN-LSTM full steps</figcaption>
</figure>
<div id="S4.SS6.p4" class="ltx_para">
<p id="S4.SS6.p4.1" class="ltx_p">Six distinct datasets were generated by segmenting the audio into durations of 1 second, 3 seconds, 5 seconds, 10 seconds and 30 seconds. To guarantee optimal clarity and quality, the audio files are saved in the <span id="S4.SS6.p4.1.1" class="ltx_text ltx_font_italic">wav</span> file format, utilising a bit rate of 1411 kbps.</p>
</div>
</section>
<section id="S4.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Experiments Using ANN</h3>

<div id="S4.SS7.p1" class="ltx_para">
<p id="S4.SS7.p1.1" class="ltx_p">We conducted a comprehensive set of experiments on an ANN model, exploring various combinations of dataset durations and training/testing set distributions. Specifically, we evaluated five different dataset track durations, including 1-second, 3-second, 5-second, 10-second and 30-second segments. We make a version of the dataset on each one. Additionally, we examined each one of them on different ratios of dataset splitting into training and testing sets, namely 90:10, 80:20, 70:30, 60:40 and 50:50. Furthermore, we investigated three different dataset types: an imbalanced dataset, a balanced dataset with an oversampling technique and a balanced dataset with an undersampling technique. We conducted 75 experiments on the ANN model, which are shown in Figure <a href="#S4.F18" title="Figure 18 ‣ 4.7 Experiments Using ANN ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a>, each with the predefined training model parameters described in Table <a href="#S4.T4" title="Table 4 ‣ 4.8 Experiments Using CNN ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S4.SS7.p2" class="ltx_para">
<p id="S4.SS7.p2.1" class="ltx_p">Among the experiments conducted on the imbalanced dataset, After conducting our experiments, we found that the highest accuracy was achieved when using the oversampled dataset with 5-second audio segments and an 80:10:10 dataset splitting ratio. This configuration resulted in an accuracy of 56%. Additionally, when using the undersampled dataset with 1-second segments and an 80:20 dataset splitting ratio, we achieved an accuracy of 45%. Similarly, when using 1-second segments with a 90:10 dataset splitting ratio on the imbalanced dataset, the accuracy was 45%.</p>
</div>
<div id="S4.SS7.p3" class="ltx_para">
<p id="S4.SS7.p3.1" class="ltx_p">On the other hand, the lowest accuracy was observed with the undersampled Sorani Nas dataset, using 1-second segments and a 90:10 training and testing set ratio, resulting in an</p>
</div>
<figure id="S4.F18" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F18.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.00124/assets/ann_im_acc.jpg" id="S4.F18.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(a) Imbalanced dataset with the different track durations and different dataset splitting</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F18.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.00124/assets/ann_un_acc.png" id="S4.F18.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(b) Undersampled dataset with the different track durations and different dataset splitting</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F18.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.00124/assets/ann_ov_acc.png" id="S4.F18.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(c) Oversampled dataset with the different track durations and different dataset splitting</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>ANN model accuracies</figcaption>
</figure>
<div id="S4.SS7.p4" class="ltx_para">
<p id="S4.SS7.p4.1" class="ltx_p">accuracy of 15%. The same low accuracy of 23% was obtained under similar circumstances but with the imbalanced dataset. Furthermore, an accuracy of 15% was obtained when using 3-second segments with 90:10 and 80:20 dataset splitting ratios on the oversampled Sorani Nas dataset.
Considering all the experiments conducted, the accuracies were generally lower for longer dataset durations, especially with 30-second segments. Similarly, accuracy tended to be lower when dataset splitting approaches 50:50 and 60:40. These findings suggest that shorter segment durations, combined with an overbalanced dataset and a higher proportion of training samples, yield better accuracy rates for the ANN model.</p>
</div>
</section>
<section id="S4.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.8 </span>Experiments Using CNN</h3>

<div id="S4.SS8.p1" class="ltx_para">
<p id="S4.SS8.p1.1" class="ltx_p">The CNN model underwent an experimentation process similar to that of the ANN model, including various durations of the segments (1 second, 3 seconds, 5 seconds, 10 seconds and 30 seconds). Additionally, each version of the audio segment duration dataset was tested using five different distributions for the training, validation and testing sets: 90:5:5, 80:10:10, 70:15:15, 60:20:20 and 50:25:25. All experiments with three different types of datasets were employed: imbalanced dataset, balanced with oversampling technique and balanced with undersampling technique.
<br class="ltx_break">We conducted 75 experiments on the CNN model, as represented in Figure <a href="#S4.F19" title="Figure 19 ‣ 4.8 Experiments Using CNN ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a>, and their accuracies are found in Figure <a href="#S4.F20" title="Figure 20 ‣ 4.8 Experiments Using CNN ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a>, each with the predefined training model parameters described in Table <a href="#S4.T4" title="Table 4 ‣ 4.8 Experiments Using CNN ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The oversampled dataset, consisting of 3 and 5-second segments, achieved the highest accuracy of 93%. This was observed in the dataset splitting ratios of 90:5:5. Close behind, an accuracy of 92% was attained using 3-second and 5-second segments in the dataset splitting’s of 80:10:10 and 70:15:15. For the undersampled dataset, the highest accuracy of 93% was achieved with 5-second sound durations in both the 80:10:10 and 90:5:5 dataset splitting ratios. On the other hand, the best accuracy for the imbalanced dataset was 89% in the 80:10:10 dataset splitting with 5-second segments.</p>
</div>
<div id="S4.SS8.p2" class="ltx_para">
<p id="S4.SS8.p2.1" class="ltx_p">Conversely, the lowest accuracy observed was 57% when utilising the imbalanced dataset with 30-second segments, particularly in the 70:15:15 distribution. For the unbalanced dataset, accuracy ranged from 65% to 75% with 30-second segments across various dataset-splitting versions. The worst case for oversampled datasets was 75% to 78% precision with 30-second track durations in almost all data set splitting ratios except 90:5:5.</p>
</div>
<div id="S4.SS8.p3" class="ltx_para">
<p id="S4.SS8.p3.1" class="ltx_p">Furthermore, our overall observations support the earlier notion that the imbalanced dataset tends to yield the lowest accuracy rates while balancing techniques such as undersampling and oversampling result in increased accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">Hernandez et al., 2013</a>]</cite>. In particular, the CNN model demonstrated superior performance to the ANN model.</p>
</div>
<div id="S4.SS8.p4" class="ltx_para">
<p id="S4.SS8.p4.1" class="ltx_p">In addition, Figure <a href="#S4.F21" title="Figure 21 ‣ 4.8 Experiments Using CNN ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21</span></a> shows how training and testing CNN model errors gradually decrease with increasing epochs in the CNN model. As the model is trained on the training data, it tries to minimise the error or loss function, resulting in lower errors over time. The decreasing trend of training and testing errors indicates that the model is learning and improving its performance. Finally, implementing Earlystopping technique causes the training process to halt when no improvement was observed for 10 consecutive epochs.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Model parameters</figcaption>
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T4.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.1.1.1.1" class="ltx_p" style="width:113.8pt;">Parameter</span>
</span>
</th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T4.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.1.1.2.1.1" class="ltx_p" style="width:113.8pt;">Value</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.2.1" class="ltx_tr">
<td id="S4.T4.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T4.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.2.1.1.1.1" class="ltx_p" style="width:113.8pt;">Learning_rate</span>
</span>
</td>
<td id="S4.T4.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T4.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.2.1.2.1.1" class="ltx_p" style="width:113.8pt;">0.0001</span>
</span>
</td>
</tr>
<tr id="S4.T4.1.3.2" class="ltx_tr">
<td id="S4.T4.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T4.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.3.2.1.1.1" class="ltx_p" style="width:113.8pt;">Batch_size</span>
</span>
</td>
<td id="S4.T4.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T4.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.3.2.2.1.1" class="ltx_p" style="width:113.8pt;">32</span>
</span>
</td>
</tr>
<tr id="S4.T4.1.4.3" class="ltx_tr">
<td id="S4.T4.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T4.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.4.3.1.1.1" class="ltx_p" style="width:113.8pt;">Epochs</span>
</span>
</td>
<td id="S4.T4.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T4.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.4.3.2.1.1" class="ltx_p" style="width:113.8pt;">200</span>
</span>
</td>
</tr>
<tr id="S4.T4.1.5.4" class="ltx_tr">
<td id="S4.T4.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T4.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.5.4.1.1.1" class="ltx_p" style="width:113.8pt;">Patience</span>
</span>
</td>
<td id="S4.T4.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T4.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T4.1.5.4.2.1.1" class="ltx_p" style="width:113.8pt;">10</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F19" class="ltx_figure">
<p id="S4.F19.1" class="ltx_p ltx_align_center"><span id="S4.F19.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/2404.00124/assets/CNN_3.png" id="S4.F19.1.1.g1" class="ltx_graphics ltx_img_landscape" width="550" height="393" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 19: </span>The experiments done on each model</figcaption>
</figure>
<figure id="S4.F20" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F20.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.00124/assets/cnn_im_acc.png" id="S4.F20.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(a) Imbalanced dataset with the different track durations and different dataset splitting</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F20.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.00124/assets/cnn_un_acc.png" id="S4.F20.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(b) Undersampled dataset with the different track durations and different dataset splitting</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F20.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.00124/assets/cnn_ov_acc.png" id="S4.F20.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(c) Oversampled dataset with the different track durations and different dataset splitting</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 20: </span>CNN model accuracies</figcaption>
</figure>
<figure id="S4.F21" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F21.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.00124/assets/cnn_epoch.png" id="S4.F21.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(a) Error/Loss analysis during CNN training</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F21.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.00124/assets/CNN_loss.png" id="S4.F21.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(b) Training and testing accuracy analysis during CNN training</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 21: </span>Error/Loss and training, testing analysis during CNN Training</figcaption>
</figure>
</section>
<section id="S4.SS9" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.9 </span>Experiments Using RNN-LSTM</h3>

<div id="S4.SS9.p1" class="ltx_para">
<p id="S4.SS9.p1.1" class="ltx_p">In our experiment with the RNN-LSTM model, we carried out the same 75 experiments as we did on the CNN model. All are illustrated in Figure <a href="#S4.F19" title="Figure 19 ‣ 4.8 Experiments Using CNN ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a>, and their accuracies are represented in Figure <a href="#S4.F20" title="Figure 20 ‣ 4.8 Experiments Using CNN ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a>. We achieved a remarkable accuracy of 96% by employing an oversampled dataset, an 80:10:10 dataset splitting ratio, and utilising 5-second track segments. Similarly, a high accuracy of 95% was obtained when using 3-second track segments, an 80:10:10 distribution, a 5-second segment duration, and a 90:10:10 dataset splitting ratio. For the undersampled dataset, the highest accuracy observed was 93% with 5-second sound durations in both the 80:10:10 and 90:5:5 dataset splitting ratios. On the other hand, the best accuracy for the imbalanced dataset was 92% with an 80:10:10 dataset splitting ratio and 5-second segments.</p>
</div>
<div id="S4.SS9.p2" class="ltx_para">
<p id="S4.SS9.p2.1" class="ltx_p">Conversely, the lowest accuracy observed was 51%, which occurred when utilising the imbalanced dataset with 30-second segments, particularly in the 80:10:10 dataset splitting ratio. For the unbalanced dataset, an accuracy of 55% was achieved with 30-second segments in the 60:20:20 dataset splitting ratio. Regarding the overbalanced datasets, the worst-case accuracy ranged from 75% to 78% with 30-second track durations in almost all dataset-splitting ratios except for the 90:5:5 ratio. Moreover, the gradual decrease of training and testing errors with increasing epochs in the RNN-LSTM model is illustrated in Figure <a href="#S4.F23" title="Figure 23 ‣ 4.9 Experiments Using RNN-LSTM ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">23</span></a>. The model minimises the error or loss function and increases the training/testing accuracies during training by learning from the training data, gradually reducing errors and enhancing its efficacy. The training process terminates when no improvement is observed for 10 consecutive epochs due to utilising early stopping technique.</p>
</div>
<figure id="S4.F22" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F22.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.00124/assets/rnn_im_acc.png" id="S4.F22.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(a) Imbalanced dataset with the different track durations and different dataset splitting</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F22.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.00124/assets/rnn_un_acc.png" id="S4.F22.2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(b)undersampled dataset with the different track durations and different dataset splitting</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F22.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.00124/assets/rnn_ov_acc.png" id="S4.F22.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(c) Oversampled dataset with the different track durations and different dataset splitting</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 22: </span>RNN-LSTM model accuracies</figcaption>
</figure>
<div id="S4.SS9.p3" class="ltx_para">
<p id="S4.SS9.p3.1" class="ltx_p">Ultimately, we observed that the RNN-LSTM model consistently outperformed the CNN model in accuracy.</p>
</div>
<figure id="S4.F23" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F23.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.00124/assets/rnn_epoch.png" id="S4.F23.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="157" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(a) Error/Loss analysis</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F23.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2404.00124/assets/RNN_loss.png" id="S4.F23.2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="157" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(b) Training and testing accuracy </figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 23: </span>Error/Loss and training, testing analysis during RNN-LSTM training</figcaption>
</figure>
</section>
<section id="S4.SS10" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.10 </span>Evaluation and Discussion</h3>

<div id="S4.SS10.p1" class="ltx_para">
<p id="S4.SS10.p1.1" class="ltx_p">We encountered several challenges during the classification and dataset development of Kurdish-Sorani subdialects. One major difficulty was the lack of a comprehensive automated classification system for subdialects within a specific dialect, which made it challenging to compare our results with existing studies. There has been limited research on dialects in general, and especially on the subdialects between Hewleri and Sulaimani. Furthermore, our interaction with individuals posed another challenge. Despite assuring them that the recordings would be used solely for research purposes and would not be shared on social media, some participants were hesitant to have their voices recorded. The winter season also added to the difficulties, as it was challenging to visit cities and villages, particularly those located in mountainous regions, due to poor road conditions and frequent cloud cover. These factors made it difficult to gather data effectively. Moreover, conducting visits and returning on the same day proved problematic as it was often necessary to make multiple trips to collect sufficient speech recordings. Despite these challenges, we developed the dataset and conducted a total of 225 experimental observations.</p>
</div>
<div id="S4.SS10.p2" class="ltx_para">
<p id="S4.SS10.p2.1" class="ltx_p">Based on the 225 experimental observations we conducted experiments with three different models, 75 on each model, as represented in Figure <a href="#S4.F19" title="Figure 19 ‣ 4.8 Experiments Using CNN ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a>. Imbalanced, undersampled and oversampled datasets. Each one of them with 5 different dataset splitting’s into training, validation and testing sets and three models on each one of them, namely ANN, CNN and RNN-LSTM on subdialect classification using Sorani Nas, 75 trails on each model, because we tried three different datasets, imbalanced, oversampled and undersampled datasets, As shown in Figure <a href="#S4.F24" title="Figure 24 ‣ 4.10 Evaluation and Discussion ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">24</span></a> it was observed that the highest accuracy rates were get when utilising an oversampled dataset. Each segment of the dataset had a duration of 5 seconds. The dataset was split into training, validation and testing sets using a ratio of 80:10:10 for RNN-LSTM and CNN and a ratio of 80:20 for ANN.</p>
</div>
<div id="S4.SS10.p3" class="ltx_para">
<p id="S4.SS10.p3.1" class="ltx_p">In addition, mostly the splitting’s that gives good accuracy are 90:5:5, 80:10:10 and 70:15:15 dataset splittings, with durations of 1 second, 3 seconds and 5 seconds, which repeated the experiment result in <span id="S4.SS10.p3.1.1" class="ltx_text ltx_font_bold">?</span>) which found out that 3 seconds perform much better than shorter ones, on the contrary, The experiments that give low accuracies are 30 seconds mostly,with the 50:25:25 and 60:20:20 dataset splitting as can be observed in Figure <a href="#S4.F18" title="Figure 18 ‣ 4.7 Experiments Using ANN ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a>, Figure <a href="#S4.F20" title="Figure 20 ‣ 4.8 Experiments Using CNN ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a> and Figure <a href="#S4.F22" title="Figure 22 ‣ 4.9 Experiments Using RNN-LSTM ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22</span></a>.</p>
</div>
<div id="S4.SS10.p4" class="ltx_para">
<p id="S4.SS10.p4.1" class="ltx_p">Furthermore, the accuracy of RNN-LSTM achieved the highest in all the situations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx32" title="" class="ltx_ref">Sunny et al., 2020</a>]</cite>, and RNN’s have the benefit of training faster and using less computational resources. That’s because there are fewer tensor operations to compute <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">Bansal et al., 2018</a>]</cite>. The training time in seconds of neural network models, such as RNN-LSTM, CNN and ANN, can vary depending on factors such as model complexity and architecture. Through experimentation, it has been observed that RNN-LSTM generally requires more training time compared to CNN, while CNN exhibits the minimum training time among the proposed models. This indicates that the computational requirements and the number of parameters in RNN-LSTM contribute to its longer training duration. Still, all the models, in general, don’t need so much time because of using MFCC feature extractor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx26" title="" class="ltx_ref">Ou and Ke, 2004</a>]</cite>. In contrast, the specialised structure of CNN allows for more efficient training. The respective training times for each model are presented in Figure <a href="#S4.F24" title="Figure 24 ‣ 4.10 Evaluation and Discussion ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">24</span></a>.</p>
</div>
<div id="S4.SS10.p5" class="ltx_para">
<p id="S4.SS10.p5.1" class="ltx_p">Moreover, employing the early stopping technique in all three models ensures that training will cease if there is no improvement in accuracy for consecutive 10 epochs. Consequently, the specific epoch number at which each model stops training will differ based on various factors that mentioned earlier.</p>
</div>
<figure id="S4.F24" class="ltx_figure"><img src="/html/2404.00124/assets/compare_models2.png" id="S4.F24.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="569" height="295" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 24: </span>Comparing ANN, CNN, and RNN-LSTM models</figcaption>
</figure>
<div id="S4.SS10.p6" class="ltx_para">
<p id="S4.SS10.p6.1" class="ltx_p">Table <a href="#S4.T5" title="Table 5 ‣ 4.10 Evaluation and Discussion ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> displays the performance metrics of a classification model for different classes. Precision measures the accuracy of positive predictions, recall evaluates the ability to identify positive instances correctly, F1-score is a measure of a model’s accuracy in binary classification tasks. It’s the harmonic mean of precision and recall, two other important metrics in classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">Chicco and Jurman, 2020</a>]</cite>, which provides a balanced assessment considering false positives, false negative, true positives and true negatives. Each row represents a class, with precision, recall and F1-Score values specified. These metrics help evaluate the model’s effectiveness in accurately classifying instances into different classes, with higher values indicating better performance.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Classification metrics for each subdialect in RNN-LSTM model</figcaption>
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T5.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.1.1.1.1.1" class="ltx_p" style="width:125.2pt;">Class</span>
</span>
</th>
<th id="S4.T5.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S4.T5.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.1.1.2.1.1" class="ltx_p" style="width:96.7pt;">F1-Score</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.1.2.1" class="ltx_tr">
<td id="S4.T5.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T5.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.2.1.1.1.1" class="ltx_p" style="width:125.2pt;">Garmiani</span>
</span>
</td>
<td id="S4.T5.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.2.1.2.1.1" class="ltx_p" style="width:96.7pt;">0.98</span>
</span>
</td>
</tr>
<tr id="S4.T5.1.3.2" class="ltx_tr">
<td id="S4.T5.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T5.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.3.2.1.1.1" class="ltx_p" style="width:125.2pt;">Hewleri</span>
</span>
</td>
<td id="S4.T5.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.3.2.2.1.1" class="ltx_p" style="width:96.7pt;">0.94</span>
</span>
</td>
</tr>
<tr id="S4.T5.1.4.3" class="ltx_tr">
<td id="S4.T5.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T5.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.4.3.1.1.1" class="ltx_p" style="width:125.2pt;">Karkuki</span>
</span>
</td>
<td id="S4.T5.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.4.3.2.1.1" class="ltx_p" style="width:96.7pt;">0.94</span>
</span>
</td>
</tr>
<tr id="S4.T5.1.5.4" class="ltx_tr">
<td id="S4.T5.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T5.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.5.4.1.1.1" class="ltx_p" style="width:125.2pt;">Khoshnawi</span>
</span>
</td>
<td id="S4.T5.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.5.4.2.1.1" class="ltx_p" style="width:96.7pt;">0.93</span>
</span>
</td>
</tr>
<tr id="S4.T5.1.6.5" class="ltx_tr">
<td id="S4.T5.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T5.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.6.5.1.1.1" class="ltx_p" style="width:125.2pt;">Pishdari</span>
</span>
</td>
<td id="S4.T5.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.6.5.2.1.1" class="ltx_p" style="width:96.7pt;">0.91</span>
</span>
</td>
</tr>
<tr id="S4.T5.1.7.6" class="ltx_tr">
<td id="S4.T5.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T5.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.7.6.1.1.1" class="ltx_p" style="width:125.2pt;">Sulaimani</span>
</span>
</td>
<td id="S4.T5.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T5.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.7.6.2.1.1" class="ltx_p" style="width:96.7pt;">0.92</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS10.p7" class="ltx_para">
<p id="S4.SS10.p7.1" class="ltx_p">Finally, we conducted validation on a subset of the dataset using two different approaches: human validation and machine prediction. Due to limited resources explaining the differences between Sorani subdialects, our validation process relied solely on human judgment. We played audio samples to native speakers of the Sorani subdialects and asked them to identify the corresponding subdialect. In some cases, they provided two subdialect options as they couldn’t determine an individual one. On the other hand, for machine prediction, we used our trained model to predict the subdialect of individual audio samples. Table <a href="#S4.SS10" title="4.10 Evaluation and Discussion ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.10</span></a> provides an example of the results obtained from both approaches for each subdialect.</p>
</div>
<div id="S4.SS10.p8" class="ltx_para">
<p id="S4.SS10.p8.1" class="ltx_p">While we examine the classification metrics in Table <a href="#S4.T5" title="Table 5 ‣ 4.10 Evaluation and Discussion ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the validation in Table <a href="#S4.SS10" title="4.10 Evaluation and Discussion ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.10</span></a>, and the confusion matrix which is provided in Figure <a href="#S4.F25" title="Figure 25 ‣ 4.10 Evaluation and Discussion ‣ 4 Experiments, Results, and Discussion ‣ Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in Sorani Kurdish" class="ltx_ref"><span class="ltx_text ltx_ref_tag">25</span></a> for the three models highlight the interrelation between various subdialects. These matrices corroborate with the geographical proximity of the subdialects and the confusion that the system or humans might have in their correct classification.</p>
</div>
<figure id="S4.SS10.tab1" class="ltx_table">
<div id="S4.SS10.tab1.1" class="ltx_block">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_block">Table 6: </span>Detecting single tracks from each subdialect result</figcaption>
<table id="S4.SS10.tab1.1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.SS10.tab1.1.1.1.1" class="ltx_tr">
<td id="S4.SS10.tab1.1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.1.1.1.1.1" class="ltx_p" style="width:113.8pt;">Transcribed (Kurdish)</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.1.1.2.1.1" class="ltx_p" style="width:94.7pt;">Transcribed (Latin)</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.1.1.3.1.1" class="ltx_p" style="width:52.6pt;">Actual subdialect</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.1.1.4.1.1" class="ltx_p" style="width:52.6pt;">Machine predicted</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.1.1.5.1.1" class="ltx_p" style="width:52.6pt;">Human predicted</span>
</span>
</td>
</tr>
<tr id="S4.SS10.tab1.1.1.2.2" class="ltx_tr">
<td id="S4.SS10.tab1.1.1.2.2.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_l ltx_border_t">
<span id="S4.SS10.tab1.1.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.2.2.1.1.1" class="ltx_p" style="width:113.8pt;"><span id="S4.SS10.tab1.1.1.2.2.1.1.1.1" class="ltx_ERROR undefined">\RL</span>‫وە ئەلەرم هەڵەسم‬‬‬</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.2.2.2.1.1" class="ltx_p" style="width:94.7pt;">We elerm hellesm</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.2.2.3.1.1" class="ltx_p" style="width:52.6pt;">Garmiani</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.2.2.4.1.1" class="ltx_p" style="width:52.6pt;">Garmiani</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.2.2.5.1.1" class="ltx_p" style="width:52.6pt;">Garmiani</span>
</span>
</td>
</tr>
<tr id="S4.SS10.tab1.1.1.3.3" class="ltx_tr">
<td id="S4.SS10.tab1.1.1.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.3.3.1.1.1" class="ltx_p" style="width:113.8pt;"><span id="S4.SS10.tab1.1.1.3.3.1.1.1.1" class="ltx_ERROR undefined">\RL</span>سبەینان هەڵەستم‬</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.3.3.2.1.1" class="ltx_p" style="width:94.7pt;">Sbeyinan hellestm</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.3.3.3.1.1" class="ltx_p" style="width:52.6pt;">Karkuki</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.3.3.4.1.1" class="ltx_p" style="width:52.6pt;">Pishdari</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.3.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.3.3.5.1.1" class="ltx_p" style="width:52.6pt;">Pishdari, Karkuki</span>
</span>
</td>
</tr>
<tr id="S4.SS10.tab1.1.1.4.4" class="ltx_tr">
<td id="S4.SS10.tab1.1.1.4.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.4.4.1.1.1" class="ltx_p" style="width:113.8pt;"><span id="S4.SS10.tab1.1.1.4.4.1.1.1.1" class="ltx_ERROR undefined">\RL</span>وەڵا بەخۆم هەردەستم‬</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.4.4.2.1.1" class="ltx_p" style="width:94.7pt;">Wella bexom herdestm</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.4.4.3.1.1" class="ltx_p" style="width:52.6pt;">Hewlri</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.4.4.4.1.1" class="ltx_p" style="width:52.6pt;">Pishdari</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.4.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.4.4.5.1.1" class="ltx_p" style="width:52.6pt;">Hewleri</span>
</span>
</td>
</tr>
<tr id="S4.SS10.tab1.1.1.5.5" class="ltx_tr">
<td id="S4.SS10.tab1.1.1.5.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.5.5.1.1.1" class="ltx_p" style="width:113.8pt;"><span id="S4.SS10.tab1.1.1.5.5.1.1.1.1" class="ltx_ERROR undefined">\RL</span>دوانزەی نیوەڕۆ هەڵەسم‬</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.5.5.2.1.1" class="ltx_p" style="width:94.7pt;">Dwanzeyi niywerro hellesm</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.5.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.5.5.3.1.1" class="ltx_p" style="width:52.6pt;">Sulaimani</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.5.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.5.5.4.1.1" class="ltx_p" style="width:52.6pt;">Sulaimani</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.5.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.5.5.5.1.1" class="ltx_p" style="width:52.6pt;">Sulaimani</span>
</span>
</td>
</tr>
<tr id="S4.SS10.tab1.1.1.6.6" class="ltx_tr">
<td id="S4.SS10.tab1.1.1.6.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.6.6.1.1.1" class="ltx_p" style="width:113.8pt;"><span id="S4.SS10.tab1.1.1.6.6.1.1.1.1" class="ltx_ERROR undefined">\RL</span>سبەینان بە مۆبایلی هەرەستم</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.6.6.2.1.1" class="ltx_p" style="width:94.7pt;">Sbeyinan be mobaylî herestm</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.6.6.3.1.1" class="ltx_p" style="width:52.6pt;">Khoshnawi</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.6.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.6.6.4.1.1" class="ltx_p" style="width:52.6pt;">Khoshnawi</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.6.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.6.6.5.1.1" class="ltx_p" style="width:52.6pt;">Hawleri, Khoshnawi</span>
</span>
</td>
</tr>
<tr id="S4.SS10.tab1.1.1.7.7" class="ltx_tr">
<td id="S4.SS10.tab1.1.1.7.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.7.7.1.1.1" class="ltx_p" style="width:113.8pt;"><span id="S4.SS10.tab1.1.1.7.7.1.1.1.1" class="ltx_ERROR undefined">\RL</span>دواییش هەڵدەستم‬</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.7.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.7.7.2.1.1" class="ltx_p" style="width:94.7pt;">Dwayîş helldestm</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.7.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.7.7.3.1.1" class="ltx_p" style="width:52.6pt;">Pishdari</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.7.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.7.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.7.7.4.1.1" class="ltx_p" style="width:52.6pt;">Pishdari</span>
</span>
</td>
<td id="S4.SS10.tab1.1.1.7.7.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.SS10.tab1.1.1.7.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.SS10.tab1.1.1.7.7.5.1.1" class="ltx_p" style="width:52.6pt;">Karkuki, Pishdari</span>
</span>
</td>
</tr>
</tbody>
</table>
<figure id="S4.F25" class="ltx_figure ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F25.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:218.5pt;"><img src="/html/2404.00124/assets/rnn_conf_mat.png" id="S4.F25.1.g1" class="ltx_graphics ltx_img_square" width="598" height="504" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(a) Confusion matrix of ANN model</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F25.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:218.5pt;"><img src="/html/2404.00124/assets/cnn_conf_mat.png" id="S4.F25.2.g1" class="ltx_graphics ltx_img_square" width="598" height="504" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(b) Confusion matrix of CNN model</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F25.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:455.2pt;"><img src="/html/2404.00124/assets/ann_conf_mat.png" id="S4.F25.3.g1" class="ltx_graphics ltx_img_square" width="299" height="252" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering">(c)Confusion matrix of RNN-LSTM model</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 25: </span>The confusion matrix of the three models</figcaption>
</figure>
<section id="S5" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This thesis focuses on creating a classification system for the Kurdish-Sorani subdialects located in the KRI, specifically for the subdialects of Hewleri, Garmiani, Karkuki, Khoshnawi, Pishdari and Sulaimani. The data collection presented several challenges due to the lack of available social media or references to gather from. However, this thesis successfully collected audio data, making it one of the first datasets to include audio recordings for most of these subdialects. The collected dataset, named Sorani Nas, includes approximately 30 hours of audio data, primarily comprising spontaneous speech from mentioned subdialects.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The thesis comprehensively explains the dataset preparation, including pre-processing steps and dataset balancing techniques. Additionally, it delves into the methodology used for the three models employed in the study: ANN, CNN and RNN-LSTM. This detailed methodology is crucial for understanding the approach taken in the study. Based on a thorough review of existing literature, a model was developed to categorise the Kurdish-Sorani subdialects specifically using the provided dataset. The ANN, CNN and RNN-LSTM models were trained on the same dataset, and different segment durations, dataset balancing techniques, and model parameters were modified to enhance accuracy. Experimental comparisons were conducted among the three models, and CNN and RNN-LSTM were the most recommended models by other researchers. Consequently, a comparison was made between these two models while also including ANN for evaluation purposes. The experimental results demonstrate that the ANN and CNN approaches are efficient, effective and reliable, achieving accuracies of 93% and 96%, respectively, across all classes. Furthermore, the proposed technique outperforms existing algorithms regarding both speed and accuracy.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Overall, this thesis significantly contributes to classifying Kurdish-Sorani subdialects in the KRI region. The developed models and dataset provide valuable resources for further research in this domain, and the findings highlight the efficacy and reliability of the proposed approach in accurately categorising these subdialects.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">In the future, we are interested in the further enrichment and extension of the scope of the subdialects to include other Sorani subdialects that are found in Iran. Expanding to encompass other dialects, like Kurmanji and Hawrami subdialects, would also be a valuable direction for future research. Also, transcription of the dataset could hep other types of research on Sorani subdialect studies.</p>
</div>
</section>
<section id="Sx1" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">Online Resources</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The dataset is partially publicly available for non-commercial use under the CC BY-NC-SA 4.0 license 5 at <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/KurdishBLARK/SoraniNas</span>.</p>
</div>
</section>
<section id="Sx2" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">We acknowledge and appreciate very much the the value of the contribution that the participants in the data collection process. We pay our utmost respect to every one of them. Their names become available along with Sorani Nas dataset. The dataset is partially available now and will be fully available upon the publication of the peer reviewed version of the paper.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography ltx_centering">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Alshutayri et al., 2016] </span>
<span class="ltx_bibblock">
Alshutayri, A., Atwell, E., Alosaimy, A., Dickins, J., Ingleby, M., and Watson,
J.

</span>
<span class="ltx_bibblock">(2016).

</span>
<span class="ltx_bibblock">Arabic language weka-based dialect classifier for Arabic automatic
speech recognition transcripts.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx1.1.1" class="ltx_text ltx_font_italic">Proceedings of the Third Workshop on NLP for Similar
Languages, Varieties and Dialects (VarDial3)</span>, pages 204–211.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Amani et al., 2021] </span>
<span class="ltx_bibblock">
Amani, A., Mohammadamini, M., and Veisi, H.

</span>
<span class="ltx_bibblock">(2021).

</span>
<span class="ltx_bibblock">Kurdish spoken dialect recognition using x-vector speaker embedding.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx2.1.1" class="ltx_text ltx_font_italic">International Conference on Speech and Computer</span>, pages
50–57. Cham, Switzerland: Springer.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ashraf et al., 2010] </span>
<span class="ltx_bibblock">
Ashraf, J., Iqbal, N., Khattak, N. S., and Zaidi, A. M.

</span>
<span class="ltx_bibblock">(2010).

</span>
<span class="ltx_bibblock">Speaker independent Urdu speech recognition using HMM.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx3.1.1" class="ltx_text ltx_font_italic">Natural Language Processing and Information Systems</span>, volume
6177, pages 140–148, Berlin, Heidelberg. Springer.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Bansal et al., 2018] </span>
<span class="ltx_bibblock">
Bansal, S., Kamper, H., Livescu, K., Lopez, A., and Goldwater, S.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">Low-resource speech-to-text translation.

</span>
<span class="ltx_bibblock"><span id="bib.bibx4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1803.09164</span>.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Chettiar and Kalaivani, 2021] </span>
<span class="ltx_bibblock">
Chettiar, G. and Kalaivani, S.

</span>
<span class="ltx_bibblock">(2021).

</span>
<span class="ltx_bibblock">Music genre classification techniques.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx5.1.1" class="ltx_text ltx_font_italic">International Journal of Engineering Research &amp; Technology</span>,
volume 10, pages 158–161.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Chicco and Jurman, 2020] </span>
<span class="ltx_bibblock">
Chicco, D. and Jurman, G.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">The advantages of the matthews correlation coefficient (mcc) over f1
score and accuracy in binary classification evaluation.

</span>
<span class="ltx_bibblock"><span id="bib.bibx6.1.1" class="ltx_text ltx_font_italic">BMC genomics</span>, 21(1):1–13.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Chorowski et al., 2015] </span>
<span class="ltx_bibblock">
Chorowski, J. K., Bahdanau, D., Serdyuk, D., Cho, K., and Bengio, Y.

</span>
<span class="ltx_bibblock">(2015).

</span>
<span class="ltx_bibblock">Attention-based models for speech recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bibx7.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 28.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Cockrell-Abdullah, 2018] </span>
<span class="ltx_bibblock">
Cockrell-Abdullah, A.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">There is no kurdish art.

</span>
<span class="ltx_bibblock"><span id="bib.bibx8.1.1" class="ltx_text ltx_font_italic">The Journal of Intersectionality</span>, 2(2):103–128.

</span>
<span class="ltx_bibblock">Available at: https://www.jstor.org/stable/10.13169/jinte.2.2.0103
[Accessed: 2023-01-12].

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Farooq et al., 2019] </span>
<span class="ltx_bibblock">
Farooq, M. U., Adeeba, F., Rauf, S., and Hussain, S.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">Improving large vocabulary Urdu speech recognition system using
deep neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx9.1.1" class="ltx_text ltx_font_italic">Interspeech</span>, pages 2978–2982.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ganapathiraju et al., 2004] </span>
<span class="ltx_bibblock">
Ganapathiraju, A., Hamaker, J. E., and Picone, J.

</span>
<span class="ltx_bibblock">(2004).

</span>
<span class="ltx_bibblock">Applications of support vector machines to speech recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx10.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Signal Processing</span>, volume 52, pages
2348–2355. IEEE.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Garbade, 2021] </span>
<span class="ltx_bibblock">
Garbade, M. J.

</span>
<span class="ltx_bibblock">(2021).

</span>
<span class="ltx_bibblock">What is google colab?

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://educationecosystem.com/blog/what-is-google-colab/</span>,
January 15.

</span>
<span class="ltx_bibblock">[Accessed: June 22, 2023].

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Giubilato et al., 2016] </span>
<span class="ltx_bibblock">
Giubilato, R., Pertile, M., and Debei, S.

</span>
<span class="ltx_bibblock">(2016).

</span>
<span class="ltx_bibblock">A comparison of monocular and stereo visual fastslam implementations.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx12.1.1" class="ltx_text ltx_font_italic">2016 IEEE Metrology for Aerospace (MetroAeroSpace)</span>, pages
227–232.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Gültekin and Artuner, 2020] </span>
<span class="ltx_bibblock">
Gültekin, I. and Artuner, H.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">Turkish dialect recognition using acoustic and phonotactic features
in deep learning architectures.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx13.1.1" class="ltx_text ltx_font_italic">Bilişim Teknolojileri Dergisi</span>, volume 13, pages
207–216. Gazi University.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Hama Khorshid, 2018] </span>
<span class="ltx_bibblock">
Hama Khorshid, F.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock"><span id="bib.bibx14.1.1" class="ltx_text ltx_font_italic">Zimanî kurdî û diyalêktekanî, twêjîneweyekî cugrafî</span>.

</span>
<span class="ltx_bibblock">Hewler: Rojhellat Printing Press.

</span>
<span class="ltx_bibblock">[in Kurdish].

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Hanani and Naser, 2020] </span>
<span class="ltx_bibblock">
Hanani, A. and Naser, R.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">Spoken Arabic dialect recognition using x-vectors.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx15.1.1" class="ltx_text ltx_font_italic">Natural Language Engineering</span>, volume 26, pages 691–700.
Cambridge University Press.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Hassani et al., 2016] </span>
<span class="ltx_bibblock">
Hassani, H., Medjedovic, D., et al.

</span>
<span class="ltx_bibblock">(2016).

</span>
<span class="ltx_bibblock">Automatic Kurdish dialects identification.

</span>
<span class="ltx_bibblock"><span id="bib.bibx16.1.1" class="ltx_text ltx_font_italic">Computer Science &amp; Information Technology</span>, 6:61–78.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Hassanpour, 1992] </span>
<span class="ltx_bibblock">
Hassanpour, A.

</span>
<span class="ltx_bibblock">(1992).

</span>
<span class="ltx_bibblock"><span id="bib.bibx17.1.1" class="ltx_text ltx_font_italic">Nationalism and language in Kurdistan, 1918-1985</span>.

</span>
<span class="ltx_bibblock">San Francisco: Mellen Research University Press.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Hernandez et al., 2013] </span>
<span class="ltx_bibblock">
Hernandez, J., Carrasco-Ochoa, J. A., and Martínez-Trinidad, J. F.

</span>
<span class="ltx_bibblock">(2013).

</span>
<span class="ltx_bibblock">An empirical study of oversampling and undersampling for instance
selection methods on imbalance datasets.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx18.1.1" class="ltx_text ltx_font_italic">Progress in Pattern Recognition, Image Analysis, Computer
Vision, and Applications: 18th Iberoamerican Congress, CIARP 2013, Havana,
Cuba, November 20-23, 2013, Proceedings, Part I 18</span>, pages 262–269.
Springer.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Hussein, 2011] </span>
<span class="ltx_bibblock">
Hussein, S. A.

</span>
<span class="ltx_bibblock">(2011).

</span>
<span class="ltx_bibblock"><span id="bib.bibx19.1.1" class="ltx_text ltx_font_italic">şêwezarekanî şarî kerkuk</span>.

</span>
<span class="ltx_bibblock">Hewler: Ministry of Culture and Youth Priniting Press.

</span>
<span class="ltx_bibblock">[in Kurdish].

</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Izady, 2015] </span>
<span class="ltx_bibblock">
Izady, M.

</span>
<span class="ltx_bibblock">(2015).

</span>
<span class="ltx_bibblock"><span id="bib.bibx20.1.1" class="ltx_text ltx_font_italic">Kurds: A concise handbook</span>.

</span>
<span class="ltx_bibblock">Taylor &amp; Francis.

</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Jabrael and Ahmed, 2019] </span>
<span class="ltx_bibblock">
Jabrael, S. J. and Ahmed, S. S.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">A critical analysis of space in language variation and change: some
examples of english and kurdish languages.

</span>
<span class="ltx_bibblock"><span id="bib.bibx21.1.1" class="ltx_text ltx_font_italic">Twezhar</span>.

</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Khorshid, 1983] </span>
<span class="ltx_bibblock">
Khorshid, F. H.

</span>
<span class="ltx_bibblock">(1983).

</span>
<span class="ltx_bibblock"><span id="bib.bibx22.1.1" class="ltx_text ltx_font_italic">Kurdish Language and the geographical distribution of its
dialects</span>.

</span>
<span class="ltx_bibblock">Ishbeelia Press.

</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Li et al., 2018] </span>
<span class="ltx_bibblock">
Li, B., Sainath, T. N., Sim, K. C., Bacchiani, M., Weinstein, E., Nguyen, P.,
Chen, Z., Wu, Y., and Rao, K.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">Multi-dialect speech recognition with a single sequence-to-sequence
model.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx23.1.1" class="ltx_text ltx_font_italic">2018 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP)</span>, pages 4749–4753. IEEE.

</span>
</li>
<li id="bib.bibx24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Murhaf, 2013] </span>
<span class="ltx_bibblock">
Murhaf, F.

</span>
<span class="ltx_bibblock">(2013).

</span>
<span class="ltx_bibblock">Erg tokenization and lexical categorization: A sequence labeling
approach.

</span>
<span class="ltx_bibblock">Master’s thesis.

</span>
</li>
<li id="bib.bibx25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ortu et al., 2015] </span>
<span class="ltx_bibblock">
Ortu, M., Destefanis, G., Adams, B., Murgia, A., Marchesi, M., and Tonelli, R.

</span>
<span class="ltx_bibblock">(2015).

</span>
<span class="ltx_bibblock">The Jira repository dataset: Understanding social aspects of
software development.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx25.1.1" class="ltx_text ltx_font_italic">Proceedings of the 11th International Conference on
Predictive Models and Data Analytics in Software Engineering</span>, pages 1–4.

</span>
</li>
<li id="bib.bibx26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ou and Ke, 2004] </span>
<span class="ltx_bibblock">
Ou, G. and Ke, D.

</span>
<span class="ltx_bibblock">(2004).

</span>
<span class="ltx_bibblock">Text-independent speaker verification based on relation of mfcc
components.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx26.1.1" class="ltx_text ltx_font_italic">2004 International Symposium on Chinese Spoken Language
Processing</span>, pages 57–60. IEEE.

</span>
</li>
<li id="bib.bibx27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Rahmani, 2009] </span>
<span class="ltx_bibblock">
Rahmani, W.

</span>
<span class="ltx_bibblock">(2009).

</span>
<span class="ltx_bibblock"><span id="bib.bibx27.1.1" class="ltx_text ltx_font_italic">Kurdistan û kurd le rwangeyi nexşewaniyewe</span>.

</span>
<span class="ltx_bibblock">Hewler: Rozhihalat Printing Press.

</span>
<span class="ltx_bibblock">[in Kurdish].

</span>
</li>
<li id="bib.bibx28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Richardson et al., 2009] </span>
<span class="ltx_bibblock">
Richardson, F. S., Campbell, W. M., and Torres-Carrasquillo, P. A.

</span>
<span class="ltx_bibblock">(2009).

</span>
<span class="ltx_bibblock">Discriminative n-gram selection for dialect recognition.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx28.1.1" class="ltx_text ltx_font_italic">Tenth Annual Conference of the International Speech
Communication Association</span>.

</span>
</li>
<li id="bib.bibx29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Salameh et al., 2018] </span>
<span class="ltx_bibblock">
Salameh, M., Bouamor, H., and Habash, N.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">Fine-grained Arabic dialect identification.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx29.1.1" class="ltx_text ltx_font_italic">Proceedings of the 27th International Conference on
Computational Linguistics</span>, pages 1332–1344.

</span>
</li>
<li id="bib.bibx30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Shafieian, 2022] </span>
<span class="ltx_bibblock">
Shafieian, M.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">Hidden Markov model and Persian speech recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bibx30.1.1" class="ltx_text ltx_font_italic">International Journal of Nonlinear Analysis and Applications</span>.

</span>
</li>
<li id="bib.bibx31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Soane, 1912] </span>
<span class="ltx_bibblock">
Soane, E. B.

</span>
<span class="ltx_bibblock">(1912).

</span>
<span class="ltx_bibblock">XXIV. notes on a Kurdish dialect, Sulaimania (southern
Turkish Kurdistan).

</span>
<span class="ltx_bibblock">In <span id="bib.bibx31.1.1" class="ltx_text ltx_font_italic">Journal of the Royal Asiatic Society</span>, volume 44, page
891–940. Cambridge University Press.

</span>
</li>
<li id="bib.bibx32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Sunny et al., 2020] </span>
<span class="ltx_bibblock">
Sunny, M. A. I., Maswood, M. M. S., and Alharbi, A. G.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">Deep learning-based stock price prediction using LSTM and
bi-directional LSTM model.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx32.1.1" class="ltx_text ltx_font_italic">2020 2nd Novel Intelligent and Leading Emerging Sciences
Conference (NILES)</span>, pages 87–92. IEEE.

</span>
</li>
<li id="bib.bibx33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Valueva et al., 2020] </span>
<span class="ltx_bibblock">
Valueva, M. V., Nagornov, N., Lyakhov, P. A., Valuev, G. V., and Chervyakov,
N. I.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">Application of the residue number system to reduce hardware costs of
the convolutional neural network implementation.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx33.1.1" class="ltx_text ltx_font_italic">Mathematics and Computers in Simulation</span>, volume 177, pages
232–243. Elsevier.

</span>
</li>
<li id="bib.bibx34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Veisi and Haji Mani, 2020] </span>
<span class="ltx_bibblock">
Veisi, H. and Haji Mani, A.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">Persian speech recognition using deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bibx34.1.1" class="ltx_text ltx_font_italic">International Journal of Speech Technology</span>, 23(4):893–905.

</span>
</li>
<li id="bib.bibx35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Veisi et al., 2022] </span>
<span class="ltx_bibblock">
Veisi, H., Hosseini, H., Mohammad Amini, M., Fathy, W., and Mahmudi, A.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">Jira: A central Kurdish speech recognition system, designing and
building speech corpus and pronunciation lexicon.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx35.1.1" class="ltx_text ltx_font_italic">Language Resources and Evaluation</span>, volume 56, pages
917–941. Springer.

</span>
</li>
<li id="bib.bibx36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Zeinali et al., 2019] </span>
<span class="ltx_bibblock">
Zeinali, H., Burget, L., and Černockỳ, J. H.

</span>
<span class="ltx_bibblock">(2019).

</span>
<span class="ltx_bibblock">A multi-purpose and large-scale speech corpus in Persian and
English for speaker and speech recognition: the DeepMine database.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx36.1.1" class="ltx_text ltx_font_italic">2019 IEEE Automatic Speech Recognition and Understanding
Workshop (ASRU)</span>, pages 397–402. IEEE.

</span>
</li>
<li id="bib.bibx37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Zhang and Hansen, 2017] </span>
<span class="ltx_bibblock">
Zhang, Q. and Hansen, J. H.

</span>
<span class="ltx_bibblock">(2017).

</span>
<span class="ltx_bibblock">Dialect recognition based on unsupervised bottleneck features.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx37.1.1" class="ltx_text ltx_font_italic">Interspeech</span>, pages 2576–2580.

</span>
</li>
<li id="bib.bibx38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Zhang et al., 1990] </span>
<span class="ltx_bibblock">
Zhang, W., Itoh, K., Tanida, J., and Ichioka, Y.

</span>
<span class="ltx_bibblock">(1990).

</span>
<span class="ltx_bibblock">Parallel distributed processing model with local space-invariant
interconnections and its optical architecture.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx38.1.1" class="ltx_text ltx_font_italic">Applied Optics</span>, volume 29, pages 4790–4797. Optica
Publishing Group.

</span>
</li>
<li id="bib.bibx39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Ziedan et al., 2016] </span>
<span class="ltx_bibblock">
Ziedan, R., Micheal, M., Alsammak, A., Mursi, M., and Elmaghraby, A.

</span>
<span class="ltx_bibblock">(2016).

</span>
<span class="ltx_bibblock">A unified approach for Arabic language dialect detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bibx39.1.1" class="ltx_text ltx_font_italic">Twenty Ninth International Conference on Computers
Applications in Industry and Engineering (CAINE)</span>, pages 165–170.

</span>
</li>
<li id="bib.bibx40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Zuhair and Hassani, 2021] </span>
<span class="ltx_bibblock">
Zuhair, A. and Hassani, H.

</span>
<span class="ltx_bibblock">(2021).

</span>
<span class="ltx_bibblock">Comparing the accuracy of deep neural networks (dnn) and
convolutional neural network (cnn) in music genre recognition (mgr):
experiments on kurdish music.

</span>
<span class="ltx_bibblock"><span id="bib.bibx40.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2111.11063</span>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_centering ltx_role_newpage"></div>
</div>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.00123" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.00124" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.00124">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.00124" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.00125" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 17:52:40 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
