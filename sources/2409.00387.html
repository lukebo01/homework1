<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.00387] Progressive Residual Extraction based Pre-training for Speech Representation Learning</title><meta property="og:description" content="Self-supervised learning (SSL) has garnered significant attention in speech processing, excelling in linguistic tasks such as speech recognition.
However, jointly improving the performance of pre-trained models on vari…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Progressive Residual Extraction based Pre-training for Speech Representation Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Progressive Residual Extraction based Pre-training for Speech Representation Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.00387">

<!--Generated on Sat Oct  5 19:42:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Self-supervised Learning,  Speech Representation Learning,  Speech Disentangle,  Pre-training
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Progressive Residual Extraction based Pre-training for Speech Representation Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tianrui Wang, Jin Li, Ziyang Ma, Rui Cao, Xie Chen, Longbiao Wang, Meng Ge
<br class="ltx_break">Xiaobao Wang, Yuguang Wang, Jianwu Dang, Nyima Tashi

</span><span class="ltx_author_notes">
This work was supported in part by the National Key R&amp;D Program of China (2022ZD0116101), the National Natural Science Foundation of China under Grant U23B2053 and Grant 62176182. <span id="id1.1.id1" class="ltx_text ltx_font_italic">(Corresponding authors: Longbiao Wang and Meng Ge)</span>.
Tianrui Wang, Jin Li, Rui Cao, and Xiaobao Wang are with the Tianjin Key Laboratory of Cognitive Computing and Application, College of Intelligence and Computing, Tianjin University, Tianjin 300350, China (e-mail: {wangtianrui, caorui_2022, lijin0120, wangxiaobao}@tju.edu.cn).
Longbiao Wang is with the Tianjin Key Laboratory of Cognitive Computing and Application, College of Intelligence and Computing, Tianjin University, Tianjin 300350, China, and also with the Huiyan Technology (Tianjin) Company, Ltd., Tianjin 300350, China (e-mail: longbiao_wang@tju.edu.cn).
Meng, Ge is with Saw Swee Hock School of Public Health, National University of Singapore, Singapore (e-mail: gemeng@nus.edu.sg).
Ziyang Ma and Xie Chen are with MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China (e-mail: {zym.22, chenxie95}@sjtu.edu.cn).
Yuguang Wang is with Huiyan Technology (Tianjin) Co., Ltd, Tianjin, China (e-mail: ygwang@huiyan-tech.com).
Jianwu Dang is with the Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Guangdong, China (e-mail: jdang@jaist.ac.jp).
Nyima Tashi is with the School of Information Science and Technology, Tibet University, Lhasa, China (e-mail: nmzx@utibet.edu.cn)
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Self-supervised learning (SSL) has garnered significant attention in speech processing, excelling in linguistic tasks such as speech recognition.
However, jointly improving the performance of pre-trained models on various downstream tasks, each requiring different speech information, poses significant challenges. To this purpose, we propose a progressive residual extraction based self-supervised learning method, named <span id="id2.id1.1" class="ltx_text ltx_font_smallcaps">ProgRE</span>. Specifically, we introduce two lightweight and specialized task modules into an encoder-style SSL backbone to enhance its ability to extract pitch variation and speaker information from speech. Furthermore, to prevent the interference of reinforced pitch variation and speaker information with irrelevant content information learning, we residually remove the information extracted by these two modules from the main branch. The main branch is then trained using HuBERT’s speech masking prediction to ensure the performance of the Transformer’s deep-layer features on content tasks. In this way, we can progressively extract pitch variation, speaker, and content representations from the input speech. Finally, we can combine multiple representations with diverse speech information using different layer weights to obtain task-specific representations for various downstream tasks. Experimental results indicate that our proposed method achieves joint performance improvements on various tasks, such as speaker identification, speech recognition, emotion recognition, speech enhancement, and voice conversion, compared to excellent SSL methods such as wav2vec2.0, HuBERT, and WavLM.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Self-supervised Learning, Speech Representation Learning, Speech Disentangle, Pre-training

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Speech self-supervised learning (SSL) aims to learn how to extract a universal representation of speech for various downstream tasks based on a massive amount of unlabeled data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. In this framework, a model is pre-trained on tasks using the speech itself to generate supervisory signals, rather than relying on external labels provided by humans <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. After pre-training, the model, regarded as a speech representation extractor, is fine-tuned using supervised speech data to achieve task-specific capabilities for specific downstream tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Existing well-known speech SSL methods can be categorized into two streams: generative and contrastive methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
Generative methods build an encoder to convert speech into representations and train the encoder by reconstructing the speech from these representations, including TERA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, SoundStream <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, and Encodec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Since generative methods are supervised on specific speech signals, they often excel in acoustic tasks but are not satisfied for content tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
Contrastive methods also build an encoder to convert speech into representations, but train the encoder by measuring the similarity between representations of different inputs or modules. Examples include wav2vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, and Data2vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. These contrastive methods are usually supervised on cluster-style macro information, so they perform well on content tasks but mediocrely on acoustic tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">With the development of multi-modal large language models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, the universality of SSL across various tasks has been highlighted <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
In pursuit of this objective, the SUPERB and SUPERB-SG benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> assemble fifteen downstream tasks to evaluate pre-trained models in areas, such as content, speaker, paralanguage, and acoustic processing.
Although researchers have proposed various impressive SSL strategies tailored to specific tasks such as speaker recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, emotion recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, and speech enhancement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, enhancing a model’s ability on one task often leads to a decline in its ability on other tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
This prompts a challenging research question: Can speech pre-training be equipped with the capability to simultaneously enhance performance across various tasks?
</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To answer this research question, initial studies have focused on combining multiple task-specific pre-trained models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> or using adapter-based multi-task pre-training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. These researches facilitate the concurrent extraction of speech representations tailored for diverse downstream tasks, and achieve preliminary success. However, these approaches, rooted in the Mixture of Experts (MOE) principle <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, lead to increased resource demands and do not fundamentally address the challenges inherent in achieving universal speech SSL representations. Some explorations have pointed out that the incompatibility between tasks makes it difficult for models to find a common direction of convergence across various tasks in multi-task learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.
The incompatibility between different tasks primarily stems from the varying contributions of different types of speech information to each task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. Correspondingly, existing SSL models also exhibit a trend of task modularization when extracting speech representations. The representations produced by different layers are suited to different downstream tasks. Representations from the shallow layers of Transformers focus on capturing acoustic information, with these layers closer to the input modeling richer acoustic details <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. In contrast, deep-layer representations, which contain more contextual and semantic information, perform better on tasks such as speech recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. We refer to these as task characteristics in our paper.
The layer-wise task characteristics can be explained by speech information disentanglement. Speech can theoretically be progressively disentangled into non-linguistic, para-linguistic, and linguistic information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
In practice, speech is typically decoupled into three components: speaker, content, and pitch variation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. These three types of information are theoretically independent of each other and can be freely combined for use in various downstream tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.
Moreover, studies have indicated that removing content information can enhance speaker recognition performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Conversely, other research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> suggests that removing content-independent speaker information can improve the performance of content-related tasks.
Therefore, leveraging the independence between different types of speech information and the task-specific characteristics of various Transformer layers in SSL models seems to be the key to addressing the varying demands for speech information across different downstream tasks.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Inspired by the above discussions, we propose a novel pre-training method called <span id="S1.p5.1.1" class="ltx_text ltx_font_smallcaps">ProgRE</span>, which progressively extracts the representations of pitch variation, speaker, and content from speech.
By doing so, <span id="S1.p5.1.2" class="ltx_text ltx_font_smallcaps">ProgRE</span> can ensure the extracted representations adapt to downstream tasks with various demands for speech information, achieving simultaneous compatibility effects.
Specifically, we first strengthen the extraction of pitch variation and speaker information in the two middle layers of the SSL model.
Since pitch-variation, speaker, and content information are theoretically independent of each other <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, we progressively remove the strengthened pitch-variation and speaker representations from the main branch.
This gradual purification of the main branch reduces the model’s burden and prevents the strengthened information from interfering with the learning of other irrelevant information, especially content information.
Specifically, based on HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, we insert two lightweight extractors to model pitch variation and speaker information of speech and progressively remove them from the main speech branch in a progressive residual manner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.
Finally, the residual main branch is trained by HuBERT’s self-supervised strategy to predict the masked units.
Experiments show that our <span id="S1.p5.1.3" class="ltx_text ltx_font_smallcaps">ProgRE</span> can jointly improve performance across various tasks.
Furthermore, visualizations demonstrate that specific layers contribute more significantly to their corresponding tasks. By strengthening the roles of different layers for different types of speech information, <span id="S1.p5.1.4" class="ltx_text ltx_font_smallcaps">ProgRE</span> with a weighted-sum mechanism can also be used to analyze the downstream task’s demands for various types of speech information.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our main contributions are summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We pointed out and experimentally verified that the task characteristics of different layers facilitated by the pre-training strategy, as well as mitigating the incompatibility between different tasks, are key to achieving a universal pre-training model.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We proposed a progressive residual extraction based pre-training method for speech representation learning. This approach enables the pre-trained model to balance the extraction of pitch variation, speaker information, and content information, leading to joint performance improvements across various downstream tasks.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We additionally introduced the extractors of pitch variation and speaker information, which can greatly improve the model’s ability to extract intonation and non-linguistic information and achieve state-of-the-art (SOTA) performance on various downstream tasks, especially speaker identification and voice conversion tasks.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">In addition to evaluating <span id="S1.I1.i4.p1.1.1" class="ltx_text ltx_font_smallcaps">ProgRE</span>’s performance on a 960-hour dataset, we also validated its effectiveness on large-scale pre-training tasks using an 84,500-hour English-Chinese bilingual dataset. Furthermore, we released code at <a target="_blank" href="https://github.com/wangtianrui/ProgRE" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/wangtianrui/ProgRE</a>.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">This paper is organized as follows. Section <a href="#S2" title="II Preliminaries ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> introduces fundamental concepts utilized in our approach. Section <a href="#S3" title="III Problem Formulation ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> outlines the process by which our method extracts representations for various downstream tasks. Section <a href="#S4" title="IV Progressive Residual Extraction Based Pre-training ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> provides a detailed description of our <span id="S1.p7.1.1" class="ltx_text ltx_font_smallcaps">ProgRE</span> method. Section <a href="#S5" title="V Experiments and Results ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> presents the experimental setup and analysis of results. Section <a href="#S6" title="VI Discussion ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> discusses the findings of the research. Finally, Section <a href="#S7" title="VII Conclusion ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> concludes the paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Preliminaries</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">HuBERT</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.5" class="ltx_p">HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, as shown in Fig. <a href="#S2.F1" title="Figure 1 ‣ II-A HuBERT ‣ II Preliminaries ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, is a typical SSL method which benefits from an offline clustering to generate pseudo labels <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="\bm{Z}" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">𝒁</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝒁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">\bm{Z}</annotation></semantics></math> for a BERT-like pre-training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. A convolutional module <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="f(\cdot)" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mrow id="S2.SS1.p1.2.m2.1.2" xref="S2.SS1.p1.2.m2.1.2.cmml"><mi id="S2.SS1.p1.2.m2.1.2.2" xref="S2.SS1.p1.2.m2.1.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.2.m2.1.2.1" xref="S2.SS1.p1.2.m2.1.2.1.cmml">​</mo><mrow id="S2.SS1.p1.2.m2.1.2.3.2" xref="S2.SS1.p1.2.m2.1.2.cmml"><mo stretchy="false" id="S2.SS1.p1.2.m2.1.2.3.2.1" xref="S2.SS1.p1.2.m2.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">⋅</mo><mo stretchy="false" id="S2.SS1.p1.2.m2.1.2.3.2.2" xref="S2.SS1.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.2.cmml" xref="S2.SS1.p1.2.m2.1.2"><times id="S2.SS1.p1.2.m2.1.2.1.cmml" xref="S2.SS1.p1.2.m2.1.2.1"></times><ci id="S2.SS1.p1.2.m2.1.2.2.cmml" xref="S2.SS1.p1.2.m2.1.2.2">𝑓</ci><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">f(\cdot)</annotation></semantics></math> converts signal into the frame-level feature <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="\bm{X}" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">𝑿</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">𝑿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">\bm{X}</annotation></semantics></math>. Then, the <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="\bm{X}" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mi id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">𝑿</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">𝑿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">\bm{X}</annotation></semantics></math> is encoded by the Transformer into the representation <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="\bm{O}" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><mi id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">𝑶</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">𝑶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">\bm{O}</annotation></semantics></math>. During pre-training, the frame-level features are masked randomly and successively, then fed to Transformer, the model is trained to predict the labels of the masked frames.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2409.00387/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="272" height="145" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Diagram of HuBERT, which takes raw waveform as input to perform a BERT-like self-supervised pre-training.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Residual Vector Quantization</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Residual vector quantization (RVQ) is commonly used in speech compression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, which performs progressively refined quantization of the representation <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="\bm{X}" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">𝑿</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">𝑿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">\bm{X}</annotation></semantics></math>, as shown in Fig <a href="#S2.F2" title="Figure 2 ‣ II-B Residual Vector Quantization ‣ II Preliminaries ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2409.00387/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="272" height="91" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Diagram of residual vector quantization. RVQ performs progressive residual quantization of <math id="S2.F2.2.m1.1" class="ltx_Math" alttext="\bm{X}" display="inline"><semantics id="S2.F2.2.m1.1b"><mi id="S2.F2.2.m1.1.1" xref="S2.F2.2.m1.1.1.cmml">𝑿</mi><annotation-xml encoding="MathML-Content" id="S2.F2.2.m1.1c"><ci id="S2.F2.2.m1.1.1.cmml" xref="S2.F2.2.m1.1.1">𝑿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.2.m1.1d">\bm{X}</annotation></semantics></math>.</figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.8" class="ltx_p">The representation <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="\bm{X}" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mi id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">𝑿</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">𝑿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">\bm{X}</annotation></semantics></math> is first encoded by the first quantization <math id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="Q_{1}" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><msub id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml"><mi id="S2.SS2.p2.2.m2.1.1.2" xref="S2.SS2.p2.2.m2.1.1.2.cmml">Q</mi><mn id="S2.SS2.p2.2.m2.1.1.3" xref="S2.SS2.p2.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><apply id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.2.m2.1.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p2.2.m2.1.1.2.cmml" xref="S2.SS2.p2.2.m2.1.1.2">𝑄</ci><cn type="integer" id="S2.SS2.p2.2.m2.1.1.3.cmml" xref="S2.SS2.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">Q_{1}</annotation></semantics></math>, resulting in the representation <math id="S2.SS2.p2.3.m3.1" class="ltx_Math" alttext="\bm{q}_{1}" display="inline"><semantics id="S2.SS2.p2.3.m3.1a"><msub id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml"><mi id="S2.SS2.p2.3.m3.1.1.2" xref="S2.SS2.p2.3.m3.1.1.2.cmml">𝒒</mi><mn id="S2.SS2.p2.3.m3.1.1.3" xref="S2.SS2.p2.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.1b"><apply id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.3.m3.1.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.p2.3.m3.1.1.2.cmml" xref="S2.SS2.p2.3.m3.1.1.2">𝒒</ci><cn type="integer" id="S2.SS2.p2.3.m3.1.1.3.cmml" xref="S2.SS2.p2.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.1c">\bm{q}_{1}</annotation></semantics></math>. The error between <math id="S2.SS2.p2.4.m4.1" class="ltx_Math" alttext="\bm{X}" display="inline"><semantics id="S2.SS2.p2.4.m4.1a"><mi id="S2.SS2.p2.4.m4.1.1" xref="S2.SS2.p2.4.m4.1.1.cmml">𝑿</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.4.m4.1b"><ci id="S2.SS2.p2.4.m4.1.1.cmml" xref="S2.SS2.p2.4.m4.1.1">𝑿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.4.m4.1c">\bm{X}</annotation></semantics></math> and <math id="S2.SS2.p2.5.m5.1" class="ltx_Math" alttext="\bm{q}_{1}" display="inline"><semantics id="S2.SS2.p2.5.m5.1a"><msub id="S2.SS2.p2.5.m5.1.1" xref="S2.SS2.p2.5.m5.1.1.cmml"><mi id="S2.SS2.p2.5.m5.1.1.2" xref="S2.SS2.p2.5.m5.1.1.2.cmml">𝒒</mi><mn id="S2.SS2.p2.5.m5.1.1.3" xref="S2.SS2.p2.5.m5.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.5.m5.1b"><apply id="S2.SS2.p2.5.m5.1.1.cmml" xref="S2.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.5.m5.1.1.1.cmml" xref="S2.SS2.p2.5.m5.1.1">subscript</csymbol><ci id="S2.SS2.p2.5.m5.1.1.2.cmml" xref="S2.SS2.p2.5.m5.1.1.2">𝒒</ci><cn type="integer" id="S2.SS2.p2.5.m5.1.1.3.cmml" xref="S2.SS2.p2.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.5.m5.1c">\bm{q}_{1}</annotation></semantics></math> is then encoded by a second quantization module <math id="S2.SS2.p2.6.m6.1" class="ltx_Math" alttext="Q_{2}" display="inline"><semantics id="S2.SS2.p2.6.m6.1a"><msub id="S2.SS2.p2.6.m6.1.1" xref="S2.SS2.p2.6.m6.1.1.cmml"><mi id="S2.SS2.p2.6.m6.1.1.2" xref="S2.SS2.p2.6.m6.1.1.2.cmml">Q</mi><mn id="S2.SS2.p2.6.m6.1.1.3" xref="S2.SS2.p2.6.m6.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.6.m6.1b"><apply id="S2.SS2.p2.6.m6.1.1.cmml" xref="S2.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.6.m6.1.1.1.cmml" xref="S2.SS2.p2.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.p2.6.m6.1.1.2.cmml" xref="S2.SS2.p2.6.m6.1.1.2">𝑄</ci><cn type="integer" id="S2.SS2.p2.6.m6.1.1.3.cmml" xref="S2.SS2.p2.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.6.m6.1c">Q_{2}</annotation></semantics></math>. In this way, the representation learned by <math id="S2.SS2.p2.7.m7.1" class="ltx_Math" alttext="Q_{2}" display="inline"><semantics id="S2.SS2.p2.7.m7.1a"><msub id="S2.SS2.p2.7.m7.1.1" xref="S2.SS2.p2.7.m7.1.1.cmml"><mi id="S2.SS2.p2.7.m7.1.1.2" xref="S2.SS2.p2.7.m7.1.1.2.cmml">Q</mi><mn id="S2.SS2.p2.7.m7.1.1.3" xref="S2.SS2.p2.7.m7.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.7.m7.1b"><apply id="S2.SS2.p2.7.m7.1.1.cmml" xref="S2.SS2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.7.m7.1.1.1.cmml" xref="S2.SS2.p2.7.m7.1.1">subscript</csymbol><ci id="S2.SS2.p2.7.m7.1.1.2.cmml" xref="S2.SS2.p2.7.m7.1.1.2">𝑄</ci><cn type="integer" id="S2.SS2.p2.7.m7.1.1.3.cmml" xref="S2.SS2.p2.7.m7.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.7.m7.1c">Q_{2}</annotation></semantics></math> contains minimal information that was already captured by <math id="S2.SS2.p2.8.m8.1" class="ltx_Math" alttext="Q_{1}" display="inline"><semantics id="S2.SS2.p2.8.m8.1a"><msub id="S2.SS2.p2.8.m8.1.1" xref="S2.SS2.p2.8.m8.1.1.cmml"><mi id="S2.SS2.p2.8.m8.1.1.2" xref="S2.SS2.p2.8.m8.1.1.2.cmml">Q</mi><mn id="S2.SS2.p2.8.m8.1.1.3" xref="S2.SS2.p2.8.m8.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.8.m8.1b"><apply id="S2.SS2.p2.8.m8.1.1.cmml" xref="S2.SS2.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.8.m8.1.1.1.cmml" xref="S2.SS2.p2.8.m8.1.1">subscript</csymbol><ci id="S2.SS2.p2.8.m8.1.1.2.cmml" xref="S2.SS2.p2.8.m8.1.1.2">𝑄</ci><cn type="integer" id="S2.SS2.p2.8.m8.1.1.3.cmml" xref="S2.SS2.p2.8.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.8.m8.1c">Q_{1}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Problem Formulation</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Unlike conventional self-supervised pre-training models for speech, which aim to extract a single representation that can be widely used in various downstream tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, following SUPERB-SG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, our <span id="S3.p1.1.1" class="ltx_text ltx_font_smallcaps">ProgRE</span> seeks to extract multiple representations of the input speech containing diverse speech information through a single SSL extractor. These representations can then be combined arbitrarily using a weighted-sum mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> with minimal weights to obtain task-specific representations for various downstream tasks, as shown in Fig <a href="#S3.F3" title="Figure 3 ‣ III Problem Formulation ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2409.00387/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="242" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Diagram of the weighted-sum mechanism-based speech representation extraction. Speech is encoded into representations by a multi-layer SSL model, and then the task-specific representation for various downstream tasks is assembled with task-specific layer weights.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.4" class="ltx_p">Given speech <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="\bm{x}" display="inline"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">𝒙</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝒙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">\bm{x}</annotation></semantics></math>, <span id="S3.p2.4.1" class="ltx_text ltx_font_smallcaps">ProgRE</span> uses a <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.p2.2.m2.1a"><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">n</annotation></semantics></math>-layer Transformer SSL model to extract layer-wise representations <math id="S3.p2.3.m3.4" class="ltx_Math" alttext="\mathbf{O}=\left\{\bm{O}_{1},\bm{O}_{1},\dots,\bm{O}_{n}\right\}" display="inline"><semantics id="S3.p2.3.m3.4a"><mrow id="S3.p2.3.m3.4.4" xref="S3.p2.3.m3.4.4.cmml"><mi id="S3.p2.3.m3.4.4.5" xref="S3.p2.3.m3.4.4.5.cmml">𝐎</mi><mo id="S3.p2.3.m3.4.4.4" xref="S3.p2.3.m3.4.4.4.cmml">=</mo><mrow id="S3.p2.3.m3.4.4.3.3" xref="S3.p2.3.m3.4.4.3.4.cmml"><mo id="S3.p2.3.m3.4.4.3.3.4" xref="S3.p2.3.m3.4.4.3.4.cmml">{</mo><msub id="S3.p2.3.m3.2.2.1.1.1" xref="S3.p2.3.m3.2.2.1.1.1.cmml"><mi id="S3.p2.3.m3.2.2.1.1.1.2" xref="S3.p2.3.m3.2.2.1.1.1.2.cmml">𝑶</mi><mn id="S3.p2.3.m3.2.2.1.1.1.3" xref="S3.p2.3.m3.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.p2.3.m3.4.4.3.3.5" xref="S3.p2.3.m3.4.4.3.4.cmml">,</mo><msub id="S3.p2.3.m3.3.3.2.2.2" xref="S3.p2.3.m3.3.3.2.2.2.cmml"><mi id="S3.p2.3.m3.3.3.2.2.2.2" xref="S3.p2.3.m3.3.3.2.2.2.2.cmml">𝑶</mi><mn id="S3.p2.3.m3.3.3.2.2.2.3" xref="S3.p2.3.m3.3.3.2.2.2.3.cmml">1</mn></msub><mo id="S3.p2.3.m3.4.4.3.3.6" xref="S3.p2.3.m3.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">…</mi><mo id="S3.p2.3.m3.4.4.3.3.7" xref="S3.p2.3.m3.4.4.3.4.cmml">,</mo><msub id="S3.p2.3.m3.4.4.3.3.3" xref="S3.p2.3.m3.4.4.3.3.3.cmml"><mi id="S3.p2.3.m3.4.4.3.3.3.2" xref="S3.p2.3.m3.4.4.3.3.3.2.cmml">𝑶</mi><mi id="S3.p2.3.m3.4.4.3.3.3.3" xref="S3.p2.3.m3.4.4.3.3.3.3.cmml">n</mi></msub><mo id="S3.p2.3.m3.4.4.3.3.8" xref="S3.p2.3.m3.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.4b"><apply id="S3.p2.3.m3.4.4.cmml" xref="S3.p2.3.m3.4.4"><eq id="S3.p2.3.m3.4.4.4.cmml" xref="S3.p2.3.m3.4.4.4"></eq><ci id="S3.p2.3.m3.4.4.5.cmml" xref="S3.p2.3.m3.4.4.5">𝐎</ci><set id="S3.p2.3.m3.4.4.3.4.cmml" xref="S3.p2.3.m3.4.4.3.3"><apply id="S3.p2.3.m3.2.2.1.1.1.cmml" xref="S3.p2.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m3.2.2.1.1.1.1.cmml" xref="S3.p2.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S3.p2.3.m3.2.2.1.1.1.2.cmml" xref="S3.p2.3.m3.2.2.1.1.1.2">𝑶</ci><cn type="integer" id="S3.p2.3.m3.2.2.1.1.1.3.cmml" xref="S3.p2.3.m3.2.2.1.1.1.3">1</cn></apply><apply id="S3.p2.3.m3.3.3.2.2.2.cmml" xref="S3.p2.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.p2.3.m3.3.3.2.2.2.1.cmml" xref="S3.p2.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S3.p2.3.m3.3.3.2.2.2.2.cmml" xref="S3.p2.3.m3.3.3.2.2.2.2">𝑶</ci><cn type="integer" id="S3.p2.3.m3.3.3.2.2.2.3.cmml" xref="S3.p2.3.m3.3.3.2.2.2.3">1</cn></apply><ci id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">…</ci><apply id="S3.p2.3.m3.4.4.3.3.3.cmml" xref="S3.p2.3.m3.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.p2.3.m3.4.4.3.3.3.1.cmml" xref="S3.p2.3.m3.4.4.3.3.3">subscript</csymbol><ci id="S3.p2.3.m3.4.4.3.3.3.2.cmml" xref="S3.p2.3.m3.4.4.3.3.3.2">𝑶</ci><ci id="S3.p2.3.m3.4.4.3.3.3.3.cmml" xref="S3.p2.3.m3.4.4.3.3.3.3">𝑛</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.4c">\mathbf{O}=\left\{\bm{O}_{1},\bm{O}_{1},\dots,\bm{O}_{n}\right\}</annotation></semantics></math>. Then, the task-specific representation <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="\bm{R}\in\mathbb{R}^{T\times D}" display="inline"><semantics id="S3.p2.4.m4.1a"><mrow id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml"><mi id="S3.p2.4.m4.1.1.2" xref="S3.p2.4.m4.1.1.2.cmml">𝑹</mi><mo id="S3.p2.4.m4.1.1.1" xref="S3.p2.4.m4.1.1.1.cmml">∈</mo><msup id="S3.p2.4.m4.1.1.3" xref="S3.p2.4.m4.1.1.3.cmml"><mi id="S3.p2.4.m4.1.1.3.2" xref="S3.p2.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S3.p2.4.m4.1.1.3.3" xref="S3.p2.4.m4.1.1.3.3.cmml"><mi id="S3.p2.4.m4.1.1.3.3.2" xref="S3.p2.4.m4.1.1.3.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.p2.4.m4.1.1.3.3.1" xref="S3.p2.4.m4.1.1.3.3.1.cmml">×</mo><mi id="S3.p2.4.m4.1.1.3.3.3" xref="S3.p2.4.m4.1.1.3.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><apply id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1"><in id="S3.p2.4.m4.1.1.1.cmml" xref="S3.p2.4.m4.1.1.1"></in><ci id="S3.p2.4.m4.1.1.2.cmml" xref="S3.p2.4.m4.1.1.2">𝑹</ci><apply id="S3.p2.4.m4.1.1.3.cmml" xref="S3.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.p2.4.m4.1.1.3.1.cmml" xref="S3.p2.4.m4.1.1.3">superscript</csymbol><ci id="S3.p2.4.m4.1.1.3.2.cmml" xref="S3.p2.4.m4.1.1.3.2">ℝ</ci><apply id="S3.p2.4.m4.1.1.3.3.cmml" xref="S3.p2.4.m4.1.1.3.3"><times id="S3.p2.4.m4.1.1.3.3.1.cmml" xref="S3.p2.4.m4.1.1.3.3.1"></times><ci id="S3.p2.4.m4.1.1.3.3.2.cmml" xref="S3.p2.4.m4.1.1.3.3.2">𝑇</ci><ci id="S3.p2.4.m4.1.1.3.3.3.cmml" xref="S3.p2.4.m4.1.1.3.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">\bm{R}\in\mathbb{R}^{T\times D}</annotation></semantics></math> is reorganized as follows:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="\bm{R}=\sum_{i=1}^{N}\omega_{i}\cdot\bm{O}_{i}," display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">𝑹</mi><mo rspace="0.111em" id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><munderover id="S3.E1.m1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.3.1.cmml"><mo movablelimits="false" id="S3.E1.m1.1.1.1.1.3.1.2.2" xref="S3.E1.m1.1.1.1.1.3.1.2.2.cmml">∑</mo><mrow id="S3.E1.m1.1.1.1.1.3.1.2.3" xref="S3.E1.m1.1.1.1.1.3.1.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.1.2.3.2" xref="S3.E1.m1.1.1.1.1.3.1.2.3.2.cmml">i</mi><mo id="S3.E1.m1.1.1.1.1.3.1.2.3.1" xref="S3.E1.m1.1.1.1.1.3.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.1.1.1.1.3.1.2.3.3" xref="S3.E1.m1.1.1.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.1.1.1.1.3.1.3" xref="S3.E1.m1.1.1.1.1.3.1.3.cmml">N</mi></munderover><mrow id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml"><msub id="S3.E1.m1.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.2.cmml">ω</mi><mi id="S3.E1.m1.1.1.1.1.3.2.2.3" xref="S3.E1.m1.1.1.1.1.3.2.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.1.1.1.3.2.1" xref="S3.E1.m1.1.1.1.1.3.2.1.cmml">⋅</mo><msub id="S3.E1.m1.1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.3.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2.3.2" xref="S3.E1.m1.1.1.1.1.3.2.3.2.cmml">𝑶</mi><mi id="S3.E1.m1.1.1.1.1.3.2.3.3" xref="S3.E1.m1.1.1.1.1.3.2.3.3.cmml">i</mi></msub></mrow></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"></eq><ci id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2">𝑹</ci><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><apply id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.1.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.3.1.2.cmml" xref="S3.E1.m1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1">subscript</csymbol><sum id="S3.E1.m1.1.1.1.1.3.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.1.2.2"></sum><apply id="S3.E1.m1.1.1.1.1.3.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.1.2.3"><eq id="S3.E1.m1.1.1.1.1.3.1.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1.2.3.1"></eq><ci id="S3.E1.m1.1.1.1.1.3.1.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.1.2.3.2">𝑖</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.3.1.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.1.1.1.1.3.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3.1.3">𝑁</ci></apply><apply id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2"><ci id="S3.E1.m1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.1">⋅</ci><apply id="S3.E1.m1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2.2">𝜔</ci><ci id="S3.E1.m1.1.1.1.1.3.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.2">𝑶</ci><ci id="S3.E1.m1.1.1.1.1.3.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\bm{R}=\sum_{i=1}^{N}\omega_{i}\cdot\bm{O}_{i},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.p2.5" class="ltx_p">where <math id="S3.p2.5.m1.1" class="ltx_Math" alttext="\omega_{i}" display="inline"><semantics id="S3.p2.5.m1.1a"><msub id="S3.p2.5.m1.1.1" xref="S3.p2.5.m1.1.1.cmml"><mi id="S3.p2.5.m1.1.1.2" xref="S3.p2.5.m1.1.1.2.cmml">ω</mi><mi id="S3.p2.5.m1.1.1.3" xref="S3.p2.5.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.5.m1.1b"><apply id="S3.p2.5.m1.1.1.cmml" xref="S3.p2.5.m1.1.1"><csymbol cd="ambiguous" id="S3.p2.5.m1.1.1.1.cmml" xref="S3.p2.5.m1.1.1">subscript</csymbol><ci id="S3.p2.5.m1.1.1.2.cmml" xref="S3.p2.5.m1.1.1.2">𝜔</ci><ci id="S3.p2.5.m1.1.1.3.cmml" xref="S3.p2.5.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m1.1c">\omega_{i}</annotation></semantics></math> is the task-specific layer-wise weight. These weights can be learned from task-specific fine-tuning.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2409.00387/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="515" height="250" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The diagram depicts our <span id="S3.F4.8.1" class="ltx_text ltx_font_smallcaps">ProgRE</span> model, which takes a waveform as input and progressively extracts three types of representations: pitch variation <math id="S3.F4.4.m1.1" class="ltx_Math" alttext="\bm{O}^{\text{ p}}" display="inline"><semantics id="S3.F4.4.m1.1b"><msup id="S3.F4.4.m1.1.1" xref="S3.F4.4.m1.1.1.cmml"><mi id="S3.F4.4.m1.1.1.2" xref="S3.F4.4.m1.1.1.2.cmml">𝑶</mi><mtext id="S3.F4.4.m1.1.1.3" xref="S3.F4.4.m1.1.1.3a.cmml"> p</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.F4.4.m1.1c"><apply id="S3.F4.4.m1.1.1.cmml" xref="S3.F4.4.m1.1.1"><csymbol cd="ambiguous" id="S3.F4.4.m1.1.1.1.cmml" xref="S3.F4.4.m1.1.1">superscript</csymbol><ci id="S3.F4.4.m1.1.1.2.cmml" xref="S3.F4.4.m1.1.1.2">𝑶</ci><ci id="S3.F4.4.m1.1.1.3a.cmml" xref="S3.F4.4.m1.1.1.3"><mtext mathsize="70%" id="S3.F4.4.m1.1.1.3.cmml" xref="S3.F4.4.m1.1.1.3"> p</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.4.m1.1d">\bm{O}^{\text{ p}}</annotation></semantics></math>, speaker <math id="S3.F4.5.m2.1" class="ltx_Math" alttext="\bm{O}^{\text{ s}}" display="inline"><semantics id="S3.F4.5.m2.1b"><msup id="S3.F4.5.m2.1.1" xref="S3.F4.5.m2.1.1.cmml"><mi id="S3.F4.5.m2.1.1.2" xref="S3.F4.5.m2.1.1.2.cmml">𝑶</mi><mtext id="S3.F4.5.m2.1.1.3" xref="S3.F4.5.m2.1.1.3a.cmml"> s</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.F4.5.m2.1c"><apply id="S3.F4.5.m2.1.1.cmml" xref="S3.F4.5.m2.1.1"><csymbol cd="ambiguous" id="S3.F4.5.m2.1.1.1.cmml" xref="S3.F4.5.m2.1.1">superscript</csymbol><ci id="S3.F4.5.m2.1.1.2.cmml" xref="S3.F4.5.m2.1.1.2">𝑶</ci><ci id="S3.F4.5.m2.1.1.3a.cmml" xref="S3.F4.5.m2.1.1.3"><mtext mathsize="70%" id="S3.F4.5.m2.1.1.3.cmml" xref="S3.F4.5.m2.1.1.3"> s</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.5.m2.1d">\bm{O}^{\text{ s}}</annotation></semantics></math>, and content <math id="S3.F4.6.m3.1" class="ltx_Math" alttext="\bm{O}^{\text{ c}}" display="inline"><semantics id="S3.F4.6.m3.1b"><msup id="S3.F4.6.m3.1.1" xref="S3.F4.6.m3.1.1.cmml"><mi id="S3.F4.6.m3.1.1.2" xref="S3.F4.6.m3.1.1.2.cmml">𝑶</mi><mtext id="S3.F4.6.m3.1.1.3" xref="S3.F4.6.m3.1.1.3a.cmml"> c</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.F4.6.m3.1c"><apply id="S3.F4.6.m3.1.1.cmml" xref="S3.F4.6.m3.1.1"><csymbol cd="ambiguous" id="S3.F4.6.m3.1.1.1.cmml" xref="S3.F4.6.m3.1.1">superscript</csymbol><ci id="S3.F4.6.m3.1.1.2.cmml" xref="S3.F4.6.m3.1.1.2">𝑶</ci><ci id="S3.F4.6.m3.1.1.3a.cmml" xref="S3.F4.6.m3.1.1.3"><mtext mathsize="70%" id="S3.F4.6.m3.1.1.3.cmml" xref="S3.F4.6.m3.1.1.3"> c</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.6.m3.1d">\bm{O}^{\text{ c}}</annotation></semantics></math> (indicated by black solid lines). The model is supervised by two offline systems trained on the unlabeled dataset (indicated by blue solid lines). For fine-tuning, a weighted-sum mechanism is employed (indicated by black dotted lines).</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Progressive Residual Extraction Based Pre-training</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">As mentioned in Section <a href="#S1" title="I Introduction ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, the HuBERT pre-training strategy results in deeper Transformer layers extracting representations dominated by content information, while shallower layers retain more acoustic details. We propose a progressive residual extraction based scheme and adapt it into HuBERT, enhancing its ability to capture pitch variation and speaker information without compromising its outstanding performance in extracting content information.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.9" class="ltx_p">As shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ III Problem Formulation ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>,
the input waveform <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="\bm{x}" display="inline"><semantics id="S4.p2.1.m1.1a"><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">𝒙</mi><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">𝒙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">\bm{x}</annotation></semantics></math> is converted into a frame-level representation <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="\bm{X}_{f}" display="inline"><semantics id="S4.p2.2.m2.1a"><msub id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mi id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">𝑿</mi><mi id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1">subscript</csymbol><ci id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">𝑿</ci><ci id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">\bm{X}_{f}</annotation></semantics></math> with a frame stride of 20ms by a convolutional module consisting of 7 layers of 1-D convolution. Next, the pitch information <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="\bm{O}^{\text{ p}}" display="inline"><semantics id="S4.p2.3.m3.1a"><msup id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><mi id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml">𝑶</mi><mtext id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3a.cmml"> p</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1">superscript</csymbol><ci id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2">𝑶</ci><ci id="S4.p2.3.m3.1.1.3a.cmml" xref="S4.p2.3.m3.1.1.3"><mtext mathsize="70%" id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3"> p</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">\bm{O}^{\text{ p}}</annotation></semantics></math> is extracted by a pitch extractor and removed from the main branch to obtain <math id="S4.p2.4.m4.1" class="ltx_Math" alttext="\bm{X}" display="inline"><semantics id="S4.p2.4.m4.1a"><mi id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml">𝑿</mi><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><ci id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1">𝑿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">\bm{X}</annotation></semantics></math>. The multi-layer Transformer encodes <math id="S4.p2.5.m5.1" class="ltx_Math" alttext="\bm{X}" display="inline"><semantics id="S4.p2.5.m5.1a"><mi id="S4.p2.5.m5.1.1" xref="S4.p2.5.m5.1.1.cmml">𝑿</mi><annotation-xml encoding="MathML-Content" id="S4.p2.5.m5.1b"><ci id="S4.p2.5.m5.1.1.cmml" xref="S4.p2.5.m5.1.1">𝑿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.5.m5.1c">\bm{X}</annotation></semantics></math> into the representation <math id="S4.p2.6.m6.1" class="ltx_Math" alttext="\bm{O}" display="inline"><semantics id="S4.p2.6.m6.1a"><mi id="S4.p2.6.m6.1.1" xref="S4.p2.6.m6.1.1.cmml">𝑶</mi><annotation-xml encoding="MathML-Content" id="S4.p2.6.m6.1b"><ci id="S4.p2.6.m6.1.1.cmml" xref="S4.p2.6.m6.1.1">𝑶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.6.m6.1c">\bm{O}</annotation></semantics></math>. In the middle <math id="S4.p2.7.m7.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.p2.7.m7.1a"><mi id="S4.p2.7.m7.1.1" xref="S4.p2.7.m7.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.p2.7.m7.1b"><ci id="S4.p2.7.m7.1.1.cmml" xref="S4.p2.7.m7.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.7.m7.1c">i</annotation></semantics></math>-th Transformer layer, we add a speaker extractor to extract the speaker information <math id="S4.p2.8.m8.1" class="ltx_Math" alttext="\bm{O}^{\text{ s}}" display="inline"><semantics id="S4.p2.8.m8.1a"><msup id="S4.p2.8.m8.1.1" xref="S4.p2.8.m8.1.1.cmml"><mi id="S4.p2.8.m8.1.1.2" xref="S4.p2.8.m8.1.1.2.cmml">𝑶</mi><mtext id="S4.p2.8.m8.1.1.3" xref="S4.p2.8.m8.1.1.3a.cmml"> s</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.p2.8.m8.1b"><apply id="S4.p2.8.m8.1.1.cmml" xref="S4.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S4.p2.8.m8.1.1.1.cmml" xref="S4.p2.8.m8.1.1">superscript</csymbol><ci id="S4.p2.8.m8.1.1.2.cmml" xref="S4.p2.8.m8.1.1.2">𝑶</ci><ci id="S4.p2.8.m8.1.1.3a.cmml" xref="S4.p2.8.m8.1.1.3"><mtext mathsize="70%" id="S4.p2.8.m8.1.1.3.cmml" xref="S4.p2.8.m8.1.1.3"> s</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.8.m8.1c">\bm{O}^{\text{ s}}</annotation></semantics></math> and remove it from the main branch. During pre-training, <math id="S4.p2.9.m9.1" class="ltx_Math" alttext="\bm{X}" display="inline"><semantics id="S4.p2.9.m9.1a"><mi id="S4.p2.9.m9.1.1" xref="S4.p2.9.m9.1.1.cmml">𝑿</mi><annotation-xml encoding="MathML-Content" id="S4.p2.9.m9.1b"><ci id="S4.p2.9.m9.1.1.cmml" xref="S4.p2.9.m9.1.1">𝑿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.9.m9.1c">\bm{X}</annotation></semantics></math> is randomly masked before being input into the Transformer, and pseudo-labels for the main branch and the speaker teacher network are obtained based on unsupervised or self-supervised strategies. During fine-tuning, a weighted-sum mechanism is employed to obtain various features for downstream tasks, with learnable weights. We will introduce our <span id="S4.p2.9.1" class="ltx_text ltx_font_smallcaps">ProgRE</span> method in detail in the following sub-sections.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Progressive Residual Extraction</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In order to achieve information removal in our <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">ProgRE</span>, we migrated Residual Vector Quantization (RVQ) mentioned in Section <a href="#S2.SS2" title="II-B Residual Vector Quantization ‣ II Preliminaries ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a> into continuous representation, performing residually refined extraction of the representation <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\bm{X}" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">𝑿</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">𝑿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\bm{X}</annotation></semantics></math>. We refer to this migrated method as progressive residual extraction.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.2" class="ltx_p">As shown in the <span id="S4.SS1.p2.2.1" class="ltx_text ltx_font_smallcaps">ProgRE</span> box in Fig. <a href="#S3.F4" title="Figure 4 ‣ III Problem Formulation ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we adapted our progressive residual extraction method into the HuBERT framework. We inserted the pitch extractor and speaker extractor as continuous-version <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="Q_{1}" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><msub id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">Q</mi><mn id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">𝑄</ci><cn type="integer" id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">Q_{1}</annotation></semantics></math> and <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="Q_{2}" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><msub id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mi id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml">Q</mi><mn id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2">𝑄</ci><cn type="integer" id="S4.SS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">Q_{2}</annotation></semantics></math> of Fig. <a href="#S2.F2" title="Figure 2 ‣ II-B Residual Vector Quantization ‣ II Preliminaries ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, respectively, and removed the extracted representations from the main branch. The information in the main branch is progressively refined and finally supervised by HuBERT’s self-supervision strategy to learn the extraction of cluster information focusing on content <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Since information removal in progressive residual extraction is only effective when all modules are trained jointly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, all modules of <span id="S4.SS1.p2.2.2" class="ltx_text ltx_font_smallcaps">ProgRE</span> are jointly pre-trained using HuBERT’s self-supervision strategy. Additionally, the speaker extractor is co-supervised by a teacher model specializing in capturing speaker information. Furthermore, the pitch extractor is constrained to extract only pitch variation information by inputting normalized pitch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. In this way, <span id="S4.SS1.p2.2.3" class="ltx_text ltx_font_smallcaps">ProgRE</span> can extract pitch variation representation, speaker representation, and content representation in a residual manner.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS1.5.1.1" class="ltx_text">IV-A</span>1 </span>Pitch Variation Modeling</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">Following the speech decoupling approach of voice conversion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, we extract the pitch <math id="S4.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\bm{F}_{0}" display="inline"><semantics id="S4.SS1.SSS1.p1.1.m1.1a"><msub id="S4.SS1.SSS1.p1.1.m1.1.1" xref="S4.SS1.SSS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.SSS1.p1.1.m1.1.1.2" xref="S4.SS1.SSS1.p1.1.m1.1.1.2.cmml">𝑭</mi><mn id="S4.SS1.SSS1.p1.1.m1.1.1.3" xref="S4.SS1.SSS1.p1.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p1.1.m1.1b"><apply id="S4.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.2">𝑭</ci><cn type="integer" id="S4.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS1.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p1.1.m1.1c">\bm{F}_{0}</annotation></semantics></math> from the waveform as the anchor for intonation variation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. The representation of the pitch extractor is expected to contain intonation variations while excluding content and speaker information, so we then perform log-normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> within each waveform’s pitch as follows:</p>
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.3" class="ltx_Math" alttext="\bm{P}=\frac{\log{\bm{F}_{0}}-\text{mean}\left(\log{\bm{F}_{0}}\right)}{\text{std}\left(\log{\bm{F}_{0}}\right)}." display="block"><semantics id="S4.E2.m1.3a"><mrow id="S4.E2.m1.3.3.1" xref="S4.E2.m1.3.3.1.1.cmml"><mrow id="S4.E2.m1.3.3.1.1" xref="S4.E2.m1.3.3.1.1.cmml"><mi id="S4.E2.m1.3.3.1.1.2" xref="S4.E2.m1.3.3.1.1.2.cmml">𝑷</mi><mo id="S4.E2.m1.3.3.1.1.1" xref="S4.E2.m1.3.3.1.1.1.cmml">=</mo><mfrac id="S4.E2.m1.2.2" xref="S4.E2.m1.2.2.cmml"><mrow id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.cmml"><mrow id="S4.E2.m1.1.1.1.3" xref="S4.E2.m1.1.1.1.3.cmml"><mi id="S4.E2.m1.1.1.1.3.1" xref="S4.E2.m1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S4.E2.m1.1.1.1.3a" xref="S4.E2.m1.1.1.1.3.cmml">⁡</mo><msub id="S4.E2.m1.1.1.1.3.2" xref="S4.E2.m1.1.1.1.3.2.cmml"><mi id="S4.E2.m1.1.1.1.3.2.2" xref="S4.E2.m1.1.1.1.3.2.2.cmml">𝑭</mi><mn id="S4.E2.m1.1.1.1.3.2.3" xref="S4.E2.m1.1.1.1.3.2.3.cmml">0</mn></msub></mrow><mo id="S4.E2.m1.1.1.1.2" xref="S4.E2.m1.1.1.1.2.cmml">−</mo><mrow id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml"><mtext id="S4.E2.m1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.3a.cmml">mean</mtext><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E2.m1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.cmml"><mo id="S4.E2.m1.1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E2.m1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.1.1.cmml">log</mi><mo lspace="0.167em" id="S4.E2.m1.1.1.1.1.1.1.1a" xref="S4.E2.m1.1.1.1.1.1.1.1.cmml">⁡</mo><msub id="S4.E2.m1.1.1.1.1.1.1.1.2" xref="S4.E2.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.1.2.2" xref="S4.E2.m1.1.1.1.1.1.1.1.2.2.cmml">𝑭</mi><mn id="S4.E2.m1.1.1.1.1.1.1.1.2.3" xref="S4.E2.m1.1.1.1.1.1.1.1.2.3.cmml">0</mn></msub></mrow><mo id="S4.E2.m1.1.1.1.1.1.1.3" xref="S4.E2.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mrow id="S4.E2.m1.2.2.2" xref="S4.E2.m1.2.2.2.cmml"><mtext id="S4.E2.m1.2.2.2.3" xref="S4.E2.m1.2.2.2.3a.cmml">std</mtext><mo lspace="0em" rspace="0em" id="S4.E2.m1.2.2.2.2" xref="S4.E2.m1.2.2.2.2.cmml">​</mo><mrow id="S4.E2.m1.2.2.2.1.1" xref="S4.E2.m1.2.2.2.1.1.1.cmml"><mo id="S4.E2.m1.2.2.2.1.1.2" xref="S4.E2.m1.2.2.2.1.1.1.cmml">(</mo><mrow id="S4.E2.m1.2.2.2.1.1.1" xref="S4.E2.m1.2.2.2.1.1.1.cmml"><mi id="S4.E2.m1.2.2.2.1.1.1.1" xref="S4.E2.m1.2.2.2.1.1.1.1.cmml">log</mi><mo lspace="0.167em" id="S4.E2.m1.2.2.2.1.1.1a" xref="S4.E2.m1.2.2.2.1.1.1.cmml">⁡</mo><msub id="S4.E2.m1.2.2.2.1.1.1.2" xref="S4.E2.m1.2.2.2.1.1.1.2.cmml"><mi id="S4.E2.m1.2.2.2.1.1.1.2.2" xref="S4.E2.m1.2.2.2.1.1.1.2.2.cmml">𝑭</mi><mn id="S4.E2.m1.2.2.2.1.1.1.2.3" xref="S4.E2.m1.2.2.2.1.1.1.2.3.cmml">0</mn></msub></mrow><mo id="S4.E2.m1.2.2.2.1.1.3" xref="S4.E2.m1.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mfrac></mrow><mo lspace="0em" id="S4.E2.m1.3.3.1.2" xref="S4.E2.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.3b"><apply id="S4.E2.m1.3.3.1.1.cmml" xref="S4.E2.m1.3.3.1"><eq id="S4.E2.m1.3.3.1.1.1.cmml" xref="S4.E2.m1.3.3.1.1.1"></eq><ci id="S4.E2.m1.3.3.1.1.2.cmml" xref="S4.E2.m1.3.3.1.1.2">𝑷</ci><apply id="S4.E2.m1.2.2.cmml" xref="S4.E2.m1.2.2"><divide id="S4.E2.m1.2.2.3.cmml" xref="S4.E2.m1.2.2"></divide><apply id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"><minus id="S4.E2.m1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.2"></minus><apply id="S4.E2.m1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.3"><log id="S4.E2.m1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.3.1"></log><apply id="S4.E2.m1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.3.2.1.cmml" xref="S4.E2.m1.1.1.1.3.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.3.2.2.cmml" xref="S4.E2.m1.1.1.1.3.2.2">𝑭</ci><cn type="integer" id="S4.E2.m1.1.1.1.3.2.3.cmml" xref="S4.E2.m1.1.1.1.3.2.3">0</cn></apply></apply><apply id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1"><times id="S4.E2.m1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.2"></times><ci id="S4.E2.m1.1.1.1.1.3a.cmml" xref="S4.E2.m1.1.1.1.1.3"><mtext id="S4.E2.m1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.3">mean</mtext></ci><apply id="S4.E2.m1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1"><log id="S4.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.1"></log><apply id="S4.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.2.2">𝑭</ci><cn type="integer" id="S4.E2.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.1.2.3">0</cn></apply></apply></apply></apply><apply id="S4.E2.m1.2.2.2.cmml" xref="S4.E2.m1.2.2.2"><times id="S4.E2.m1.2.2.2.2.cmml" xref="S4.E2.m1.2.2.2.2"></times><ci id="S4.E2.m1.2.2.2.3a.cmml" xref="S4.E2.m1.2.2.2.3"><mtext id="S4.E2.m1.2.2.2.3.cmml" xref="S4.E2.m1.2.2.2.3">std</mtext></ci><apply id="S4.E2.m1.2.2.2.1.1.1.cmml" xref="S4.E2.m1.2.2.2.1.1"><log id="S4.E2.m1.2.2.2.1.1.1.1.cmml" xref="S4.E2.m1.2.2.2.1.1.1.1"></log><apply id="S4.E2.m1.2.2.2.1.1.1.2.cmml" xref="S4.E2.m1.2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.2.2.2.1.1.1.2.1.cmml" xref="S4.E2.m1.2.2.2.1.1.1.2">subscript</csymbol><ci id="S4.E2.m1.2.2.2.1.1.1.2.2.cmml" xref="S4.E2.m1.2.2.2.1.1.1.2.2">𝑭</ci><cn type="integer" id="S4.E2.m1.2.2.2.1.1.1.2.3.cmml" xref="S4.E2.m1.2.2.2.1.1.1.2.3">0</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.3c">\bm{P}=\frac{\log{\bm{F}_{0}}-\text{mean}\left(\log{\bm{F}_{0}}\right)}{\text{std}\left(\log{\bm{F}_{0}}\right)}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.1" class="ltx_p">We then use the normalized pitch as input to ensure that the representation extracted by the pitch extractor module only contains pitch variation information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, thus ensuring the effectiveness of progressive residual extraction for other pitch-variation-irrelative tasks, such as speaker and content information extraction.
Specifically, we employ a lightweight convolutional recurrent module to process the normalized pitch, as illustrated in Fig. <a href="#S3.F4" title="Figure 4 ‣ III Problem Formulation ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Each convolution block (Conv) consists of a 1D convolution, batch normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, and ReLU activation function <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. A single-layer GRU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, followed by an output fully connected (FC) layer, is utilized to extract the representation of pitch variation <math id="S4.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="\bm{O}^{\text{ p}}" display="inline"><semantics id="S4.SS1.SSS1.p2.1.m1.1a"><msup id="S4.SS1.SSS1.p2.1.m1.1.1" xref="S4.SS1.SSS1.p2.1.m1.1.1.cmml"><mi id="S4.SS1.SSS1.p2.1.m1.1.1.2" xref="S4.SS1.SSS1.p2.1.m1.1.1.2.cmml">𝑶</mi><mtext id="S4.SS1.SSS1.p2.1.m1.1.1.3" xref="S4.SS1.SSS1.p2.1.m1.1.1.3a.cmml"> p</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p2.1.m1.1b"><apply id="S4.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1">superscript</csymbol><ci id="S4.SS1.SSS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1.2">𝑶</ci><ci id="S4.SS1.SSS1.p2.1.m1.1.1.3a.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1.3"><mtext mathsize="70%" id="S4.SS1.SSS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.SSS1.p2.1.m1.1.1.3"> p</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p2.1.m1.1c">\bm{O}^{\text{ p}}</annotation></semantics></math>.</p>
</div>
<div id="S4.SS1.SSS1.p3" class="ltx_para">
<p id="S4.SS1.SSS1.p3.2" class="ltx_p">Since the pitch is extracted from the waveform, we removed the pitch variation information from the main branch after the convolution block, as:</p>
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.2" class="ltx_Math" alttext="\bm{X}=\text{layernorm}\left(\text{Convolution}\left(\bm{x}\right)-\bm{O}^{\text{ p}}\right)," display="block"><semantics id="S4.E3.m1.2a"><mrow id="S4.E3.m1.2.2.1" xref="S4.E3.m1.2.2.1.1.cmml"><mrow id="S4.E3.m1.2.2.1.1" xref="S4.E3.m1.2.2.1.1.cmml"><mi id="S4.E3.m1.2.2.1.1.3" xref="S4.E3.m1.2.2.1.1.3.cmml">𝑿</mi><mo id="S4.E3.m1.2.2.1.1.2" xref="S4.E3.m1.2.2.1.1.2.cmml">=</mo><mrow id="S4.E3.m1.2.2.1.1.1" xref="S4.E3.m1.2.2.1.1.1.cmml"><mtext id="S4.E3.m1.2.2.1.1.1.3" xref="S4.E3.m1.2.2.1.1.1.3a.cmml">layernorm</mtext><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.2.1.1.1.2" xref="S4.E3.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="S4.E3.m1.2.2.1.1.1.1.1" xref="S4.E3.m1.2.2.1.1.1.1.1.1.cmml"><mo id="S4.E3.m1.2.2.1.1.1.1.1.2" xref="S4.E3.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m1.2.2.1.1.1.1.1.1" xref="S4.E3.m1.2.2.1.1.1.1.1.1.cmml"><mrow id="S4.E3.m1.2.2.1.1.1.1.1.1.2" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2.cmml"><mtext id="S4.E3.m1.2.2.1.1.1.1.1.1.2.2" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2.2a.cmml">Convolution</mtext><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.2.1.1.1.1.1.1.2.1" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2.1.cmml">​</mo><mrow id="S4.E3.m1.2.2.1.1.1.1.1.1.2.3.2" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2.cmml"><mo id="S4.E3.m1.2.2.1.1.1.1.1.1.2.3.2.1" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2.cmml">(</mo><mi id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml">𝒙</mi><mo id="S4.E3.m1.2.2.1.1.1.1.1.1.2.3.2.2" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E3.m1.2.2.1.1.1.1.1.1.1" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.cmml">−</mo><msup id="S4.E3.m1.2.2.1.1.1.1.1.1.3" xref="S4.E3.m1.2.2.1.1.1.1.1.1.3.cmml"><mi id="S4.E3.m1.2.2.1.1.1.1.1.1.3.2" xref="S4.E3.m1.2.2.1.1.1.1.1.1.3.2.cmml">𝑶</mi><mtext id="S4.E3.m1.2.2.1.1.1.1.1.1.3.3" xref="S4.E3.m1.2.2.1.1.1.1.1.1.3.3a.cmml"> p</mtext></msup></mrow><mo id="S4.E3.m1.2.2.1.1.1.1.1.3" xref="S4.E3.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E3.m1.2.2.1.2" xref="S4.E3.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.2b"><apply id="S4.E3.m1.2.2.1.1.cmml" xref="S4.E3.m1.2.2.1"><eq id="S4.E3.m1.2.2.1.1.2.cmml" xref="S4.E3.m1.2.2.1.1.2"></eq><ci id="S4.E3.m1.2.2.1.1.3.cmml" xref="S4.E3.m1.2.2.1.1.3">𝑿</ci><apply id="S4.E3.m1.2.2.1.1.1.cmml" xref="S4.E3.m1.2.2.1.1.1"><times id="S4.E3.m1.2.2.1.1.1.2.cmml" xref="S4.E3.m1.2.2.1.1.1.2"></times><ci id="S4.E3.m1.2.2.1.1.1.3a.cmml" xref="S4.E3.m1.2.2.1.1.1.3"><mtext id="S4.E3.m1.2.2.1.1.1.3.cmml" xref="S4.E3.m1.2.2.1.1.1.3">layernorm</mtext></ci><apply id="S4.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1"><minus id="S4.E3.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1"></minus><apply id="S4.E3.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2"><times id="S4.E3.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2.1"></times><ci id="S4.E3.m1.2.2.1.1.1.1.1.1.2.2a.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2.2"><mtext id="S4.E3.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2.2">Convolution</mtext></ci><ci id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1">𝒙</ci></apply><apply id="S4.E3.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.3">superscript</csymbol><ci id="S4.E3.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.3.2">𝑶</ci><ci id="S4.E3.m1.2.2.1.1.1.1.1.1.3.3a.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.3.3"><mtext mathsize="70%" id="S4.E3.m1.2.2.1.1.1.1.1.1.3.3.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.3.3"> p</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.2c">\bm{X}=\text{layernorm}\left(\text{Convolution}\left(\bm{x}\right)-\bm{O}^{\text{ p}}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.SSS1.p3.1" class="ltx_p">where <math id="S4.SS1.SSS1.p3.1.m1.1" class="ltx_Math" alttext="\bm{x}" display="inline"><semantics id="S4.SS1.SSS1.p3.1.m1.1a"><mi id="S4.SS1.SSS1.p3.1.m1.1.1" xref="S4.SS1.SSS1.p3.1.m1.1.1.cmml">𝒙</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.1.m1.1b"><ci id="S4.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p3.1.m1.1.1">𝒙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.1.m1.1c">\bm{x}</annotation></semantics></math> is the input signal, layer normalization is performed after removal to accelerate the convergence of the model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS2.5.1.1" class="ltx_text">IV-A</span>2 </span>Speaker Information Modeling</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">Unlike conventional utterance-level speaker representation extraction, the speaker extractor in <span id="S4.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_smallcaps">ProgRE</span> is a frame-level extraction module. The frame-level module leverages the mask prediction pre-training strategy of SSL, enabling the encoder to learn to predict the information randomly masked in the input sequence, thereby improving the model’s bidirectional and global speaker information extraction ability.</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.2" class="ltx_p">The inserted speaker extractor comprises an FC layer, frame-level attentive statistics (FAS), and layer normalization following an output FC layer. FAS is a frame-level Attentive Statistic Pooling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, which calculates mean and variance on each frame. We insert the speaker extractor after the <math id="S4.SS1.SSS2.p2.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS1.SSS2.p2.1.m1.1a"><mi id="S4.SS1.SSS2.p2.1.m1.1.1" xref="S4.SS1.SSS2.p2.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.1.m1.1b"><ci id="S4.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p2.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.1.m1.1c">i</annotation></semantics></math>-th Transformer block to extract the speaker representation <math id="S4.SS1.SSS2.p2.2.m2.4" class="ltx_Math" alttext="\bm{O}^{\text{ s}}\!=\!\left[\bm{o}^{\text{ s}}_{1},\bm{o}^{\text{ s}}_{2},\cdots,\bm{o}^{\text{ s}}_{T}\right]\in\mathbb{R}^{T\times D}" display="inline"><semantics id="S4.SS1.SSS2.p2.2.m2.4a"><mrow id="S4.SS1.SSS2.p2.2.m2.4.4" xref="S4.SS1.SSS2.p2.2.m2.4.4.cmml"><msup id="S4.SS1.SSS2.p2.2.m2.4.4.5" xref="S4.SS1.SSS2.p2.2.m2.4.4.5.cmml"><mi id="S4.SS1.SSS2.p2.2.m2.4.4.5.2" xref="S4.SS1.SSS2.p2.2.m2.4.4.5.2.cmml">𝑶</mi><mtext id="S4.SS1.SSS2.p2.2.m2.4.4.5.3" xref="S4.SS1.SSS2.p2.2.m2.4.4.5.3a.cmml"> s</mtext></msup><mo lspace="0.108em" rspace="0.108em" id="S4.SS1.SSS2.p2.2.m2.4.4.6" xref="S4.SS1.SSS2.p2.2.m2.4.4.6.cmml">=</mo><mrow id="S4.SS1.SSS2.p2.2.m2.4.4.3.3" xref="S4.SS1.SSS2.p2.2.m2.4.4.3.4.cmml"><mo id="S4.SS1.SSS2.p2.2.m2.4.4.3.3.4" xref="S4.SS1.SSS2.p2.2.m2.4.4.3.4.cmml">[</mo><msubsup id="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1" xref="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1.cmml"><mi id="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1.2.2" xref="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1.2.2.cmml">𝒐</mi><mn id="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1.3" xref="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1.3.cmml">1</mn><mtext id="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1.2.3" xref="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1.2.3a.cmml"> s</mtext></msubsup><mo id="S4.SS1.SSS2.p2.2.m2.4.4.3.3.5" xref="S4.SS1.SSS2.p2.2.m2.4.4.3.4.cmml">,</mo><msubsup id="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2" xref="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2.cmml"><mi id="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2.2.2" xref="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2.2.2.cmml">𝒐</mi><mn id="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2.3" xref="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2.3.cmml">2</mn><mtext id="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2.2.3" xref="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2.2.3a.cmml"> s</mtext></msubsup><mo id="S4.SS1.SSS2.p2.2.m2.4.4.3.3.6" xref="S4.SS1.SSS2.p2.2.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.SSS2.p2.2.m2.1.1" xref="S4.SS1.SSS2.p2.2.m2.1.1.cmml">⋯</mi><mo id="S4.SS1.SSS2.p2.2.m2.4.4.3.3.7" xref="S4.SS1.SSS2.p2.2.m2.4.4.3.4.cmml">,</mo><msubsup id="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3" xref="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3.cmml"><mi id="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3.2.2" xref="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3.2.2.cmml">𝒐</mi><mi id="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3.3" xref="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3.3.cmml">T</mi><mtext id="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3.2.3" xref="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3.2.3a.cmml"> s</mtext></msubsup><mo id="S4.SS1.SSS2.p2.2.m2.4.4.3.3.8" xref="S4.SS1.SSS2.p2.2.m2.4.4.3.4.cmml">]</mo></mrow><mo id="S4.SS1.SSS2.p2.2.m2.4.4.7" xref="S4.SS1.SSS2.p2.2.m2.4.4.7.cmml">∈</mo><msup id="S4.SS1.SSS2.p2.2.m2.4.4.8" xref="S4.SS1.SSS2.p2.2.m2.4.4.8.cmml"><mi id="S4.SS1.SSS2.p2.2.m2.4.4.8.2" xref="S4.SS1.SSS2.p2.2.m2.4.4.8.2.cmml">ℝ</mi><mrow id="S4.SS1.SSS2.p2.2.m2.4.4.8.3" xref="S4.SS1.SSS2.p2.2.m2.4.4.8.3.cmml"><mi id="S4.SS1.SSS2.p2.2.m2.4.4.8.3.2" xref="S4.SS1.SSS2.p2.2.m2.4.4.8.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS2.p2.2.m2.4.4.8.3.1" xref="S4.SS1.SSS2.p2.2.m2.4.4.8.3.1.cmml">×</mo><mi id="S4.SS1.SSS2.p2.2.m2.4.4.8.3.3" xref="S4.SS1.SSS2.p2.2.m2.4.4.8.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p2.2.m2.4b"><apply id="S4.SS1.SSS2.p2.2.m2.4.4.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4"><and id="S4.SS1.SSS2.p2.2.m2.4.4a.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4"></and><apply id="S4.SS1.SSS2.p2.2.m2.4.4b.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4"><eq id="S4.SS1.SSS2.p2.2.m2.4.4.6.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.6"></eq><apply id="S4.SS1.SSS2.p2.2.m2.4.4.5.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.5"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p2.2.m2.4.4.5.1.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.5">superscript</csymbol><ci id="S4.SS1.SSS2.p2.2.m2.4.4.5.2.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.5.2">𝑶</ci><ci id="S4.SS1.SSS2.p2.2.m2.4.4.5.3a.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.5.3"><mtext mathsize="70%" id="S4.SS1.SSS2.p2.2.m2.4.4.5.3.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.5.3"> s</mtext></ci></apply><list id="S4.SS1.SSS2.p2.2.m2.4.4.3.4.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.3.3"><apply id="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1.cmml" xref="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1.1.cmml" xref="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1">subscript</csymbol><apply id="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1.2.cmml" xref="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1.2.1.cmml" xref="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1">superscript</csymbol><ci id="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1.2.2.cmml" xref="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1.2.2">𝒐</ci><ci id="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1.2.3a.cmml" xref="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1.2.3"><mtext mathsize="70%" id="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1.2.3.cmml" xref="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1.2.3"> s</mtext></ci></apply><cn type="integer" id="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1.3.cmml" xref="S4.SS1.SSS2.p2.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2.cmml" xref="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2.1.cmml" xref="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2">subscript</csymbol><apply id="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2.2.cmml" xref="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2.2.1.cmml" xref="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2">superscript</csymbol><ci id="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2.2.2.cmml" xref="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2.2.2">𝒐</ci><ci id="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2.2.3a.cmml" xref="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2.2.3"><mtext mathsize="70%" id="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2.2.3.cmml" xref="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2.2.3"> s</mtext></ci></apply><cn type="integer" id="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2.3.cmml" xref="S4.SS1.SSS2.p2.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p2.2.m2.1.1">⋯</ci><apply id="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3.1.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3">subscript</csymbol><apply id="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3.2.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3.2.1.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3">superscript</csymbol><ci id="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3.2.2.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3.2.2">𝒐</ci><ci id="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3.2.3a.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3.2.3"><mtext mathsize="70%" id="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3.2.3.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3.2.3"> s</mtext></ci></apply><ci id="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3.3.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.3.3.3.3">𝑇</ci></apply></list></apply><apply id="S4.SS1.SSS2.p2.2.m2.4.4c.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4"><in id="S4.SS1.SSS2.p2.2.m2.4.4.7.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.7"></in><share href="#S4.SS1.SSS2.p2.2.m2.4.4.3.cmml" id="S4.SS1.SSS2.p2.2.m2.4.4d.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4"></share><apply id="S4.SS1.SSS2.p2.2.m2.4.4.8.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.8"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p2.2.m2.4.4.8.1.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.8">superscript</csymbol><ci id="S4.SS1.SSS2.p2.2.m2.4.4.8.2.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.8.2">ℝ</ci><apply id="S4.SS1.SSS2.p2.2.m2.4.4.8.3.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.8.3"><times id="S4.SS1.SSS2.p2.2.m2.4.4.8.3.1.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.8.3.1"></times><ci id="S4.SS1.SSS2.p2.2.m2.4.4.8.3.2.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.8.3.2">𝑇</ci><ci id="S4.SS1.SSS2.p2.2.m2.4.4.8.3.3.cmml" xref="S4.SS1.SSS2.p2.2.m2.4.4.8.3.3">𝐷</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p2.2.m2.4c">\bm{O}^{\text{ s}}\!=\!\left[\bm{o}^{\text{ s}}_{1},\bm{o}^{\text{ s}}_{2},\cdots,\bm{o}^{\text{ s}}_{T}\right]\in\mathbb{R}^{T\times D}</annotation></semantics></math>, as shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ III Problem Formulation ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S4.SS1.SSS2.p3" class="ltx_para">
<p id="S4.SS1.SSS2.p3.1" class="ltx_p">In addition to being trained with the main branch using HuBERT’s self-supervised strategy, we add a constraint to guide the speaker extractor in a teacher-student learning manner, focusing solely on speaker information. Similar to the K-means-based speech units of HuBERT, we obtain an utterance-level target <math id="S4.SS1.SSS2.p3.1.m1.1" class="ltx_Math" alttext="\bm{s}\in\mathbb{R}^{K}" display="inline"><semantics id="S4.SS1.SSS2.p3.1.m1.1a"><mrow id="S4.SS1.SSS2.p3.1.m1.1.1" xref="S4.SS1.SSS2.p3.1.m1.1.1.cmml"><mi id="S4.SS1.SSS2.p3.1.m1.1.1.2" xref="S4.SS1.SSS2.p3.1.m1.1.1.2.cmml">𝒔</mi><mo id="S4.SS1.SSS2.p3.1.m1.1.1.1" xref="S4.SS1.SSS2.p3.1.m1.1.1.1.cmml">∈</mo><msup id="S4.SS1.SSS2.p3.1.m1.1.1.3" xref="S4.SS1.SSS2.p3.1.m1.1.1.3.cmml"><mi id="S4.SS1.SSS2.p3.1.m1.1.1.3.2" xref="S4.SS1.SSS2.p3.1.m1.1.1.3.2.cmml">ℝ</mi><mi id="S4.SS1.SSS2.p3.1.m1.1.1.3.3" xref="S4.SS1.SSS2.p3.1.m1.1.1.3.3.cmml">K</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p3.1.m1.1b"><apply id="S4.SS1.SSS2.p3.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p3.1.m1.1.1"><in id="S4.SS1.SSS2.p3.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p3.1.m1.1.1.1"></in><ci id="S4.SS1.SSS2.p3.1.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p3.1.m1.1.1.2">𝒔</ci><apply id="S4.SS1.SSS2.p3.1.m1.1.1.3.cmml" xref="S4.SS1.SSS2.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p3.1.m1.1.1.3.1.cmml" xref="S4.SS1.SSS2.p3.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS1.SSS2.p3.1.m1.1.1.3.2.cmml" xref="S4.SS1.SSS2.p3.1.m1.1.1.3.2">ℝ</ci><ci id="S4.SS1.SSS2.p3.1.m1.1.1.3.3.cmml" xref="S4.SS1.SSS2.p3.1.m1.1.1.3.3">𝐾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p3.1.m1.1c">\bm{s}\in\mathbb{R}^{K}</annotation></semantics></math> for the speaker extractor based on an offline self-supervised pre-trained model, EMA-DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, which is an ECAPA-TDNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> pre-trained without any labels using knowledge distillation. Then, masked regression is employed to train the speaker extractor as follows:</p>
<table id="S4.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E4.m1.2" class="ltx_Math" alttext="\mathcal{L}_{\text{ s}}=-\frac{1}{T_{m}}\sum_{t\in T_{m}}\log\sigma\left(\text{sim}(\bm{A}^{s}\bm{o}^{\text{ s}}_{t},\bm{s})\right)," display="block"><semantics id="S4.E4.m1.2a"><mrow id="S4.E4.m1.2.2.1" xref="S4.E4.m1.2.2.1.1.cmml"><mrow id="S4.E4.m1.2.2.1.1" xref="S4.E4.m1.2.2.1.1.cmml"><msub id="S4.E4.m1.2.2.1.1.3" xref="S4.E4.m1.2.2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E4.m1.2.2.1.1.3.2" xref="S4.E4.m1.2.2.1.1.3.2.cmml">ℒ</mi><mtext id="S4.E4.m1.2.2.1.1.3.3" xref="S4.E4.m1.2.2.1.1.3.3a.cmml"> s</mtext></msub><mo id="S4.E4.m1.2.2.1.1.2" xref="S4.E4.m1.2.2.1.1.2.cmml">=</mo><mrow id="S4.E4.m1.2.2.1.1.1" xref="S4.E4.m1.2.2.1.1.1.cmml"><mo id="S4.E4.m1.2.2.1.1.1a" xref="S4.E4.m1.2.2.1.1.1.cmml">−</mo><mrow id="S4.E4.m1.2.2.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.cmml"><mfrac id="S4.E4.m1.2.2.1.1.1.1.3" xref="S4.E4.m1.2.2.1.1.1.1.3.cmml"><mn id="S4.E4.m1.2.2.1.1.1.1.3.2" xref="S4.E4.m1.2.2.1.1.1.1.3.2.cmml">1</mn><msub id="S4.E4.m1.2.2.1.1.1.1.3.3" xref="S4.E4.m1.2.2.1.1.1.1.3.3.cmml"><mi id="S4.E4.m1.2.2.1.1.1.1.3.3.2" xref="S4.E4.m1.2.2.1.1.1.1.3.3.2.cmml">T</mi><mi id="S4.E4.m1.2.2.1.1.1.1.3.3.3" xref="S4.E4.m1.2.2.1.1.1.1.3.3.3.cmml">m</mi></msub></mfrac><mo lspace="0em" rspace="0em" id="S4.E4.m1.2.2.1.1.1.1.2" xref="S4.E4.m1.2.2.1.1.1.1.2.cmml">​</mo><mrow id="S4.E4.m1.2.2.1.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.1.cmml"><munder id="S4.E4.m1.2.2.1.1.1.1.1.2" xref="S4.E4.m1.2.2.1.1.1.1.1.2.cmml"><mo movablelimits="false" id="S4.E4.m1.2.2.1.1.1.1.1.2.2" xref="S4.E4.m1.2.2.1.1.1.1.1.2.2.cmml">∑</mo><mrow id="S4.E4.m1.2.2.1.1.1.1.1.2.3" xref="S4.E4.m1.2.2.1.1.1.1.1.2.3.cmml"><mi id="S4.E4.m1.2.2.1.1.1.1.1.2.3.2" xref="S4.E4.m1.2.2.1.1.1.1.1.2.3.2.cmml">t</mi><mo id="S4.E4.m1.2.2.1.1.1.1.1.2.3.1" xref="S4.E4.m1.2.2.1.1.1.1.1.2.3.1.cmml">∈</mo><msub id="S4.E4.m1.2.2.1.1.1.1.1.2.3.3" xref="S4.E4.m1.2.2.1.1.1.1.1.2.3.3.cmml"><mi id="S4.E4.m1.2.2.1.1.1.1.1.2.3.3.2" xref="S4.E4.m1.2.2.1.1.1.1.1.2.3.3.2.cmml">T</mi><mi id="S4.E4.m1.2.2.1.1.1.1.1.2.3.3.3" xref="S4.E4.m1.2.2.1.1.1.1.1.2.3.3.3.cmml">m</mi></msub></mrow></munder><mrow id="S4.E4.m1.2.2.1.1.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.1.1.cmml"><mrow id="S4.E4.m1.2.2.1.1.1.1.1.1.3" xref="S4.E4.m1.2.2.1.1.1.1.1.1.3.cmml"><mi id="S4.E4.m1.2.2.1.1.1.1.1.1.3.1" xref="S4.E4.m1.2.2.1.1.1.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S4.E4.m1.2.2.1.1.1.1.1.1.3a" xref="S4.E4.m1.2.2.1.1.1.1.1.1.3.cmml">⁡</mo><mi id="S4.E4.m1.2.2.1.1.1.1.1.1.3.2" xref="S4.E4.m1.2.2.1.1.1.1.1.1.3.2.cmml">σ</mi></mrow><mo lspace="0em" rspace="0em" id="S4.E4.m1.2.2.1.1.1.1.1.1.2" xref="S4.E4.m1.2.2.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mtext id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3a.cmml">sim</mtext><mo lspace="0em" rspace="0em" id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><mrow id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msup id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">𝑨</mi><mi id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">s</mi></msup><mo lspace="0em" rspace="0em" id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><msubsup id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">𝒐</mi><mi id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">t</mi><mtext id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3a.cmml"> s</mtext></msubsup></mrow><mo id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml">𝒔</mi><mo stretchy="false" id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.4" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo id="S4.E4.m1.2.2.1.2" xref="S4.E4.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.2b"><apply id="S4.E4.m1.2.2.1.1.cmml" xref="S4.E4.m1.2.2.1"><eq id="S4.E4.m1.2.2.1.1.2.cmml" xref="S4.E4.m1.2.2.1.1.2"></eq><apply id="S4.E4.m1.2.2.1.1.3.cmml" xref="S4.E4.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.2.2.1.1.3.1.cmml" xref="S4.E4.m1.2.2.1.1.3">subscript</csymbol><ci id="S4.E4.m1.2.2.1.1.3.2.cmml" xref="S4.E4.m1.2.2.1.1.3.2">ℒ</ci><ci id="S4.E4.m1.2.2.1.1.3.3a.cmml" xref="S4.E4.m1.2.2.1.1.3.3"><mtext mathsize="70%" id="S4.E4.m1.2.2.1.1.3.3.cmml" xref="S4.E4.m1.2.2.1.1.3.3"> s</mtext></ci></apply><apply id="S4.E4.m1.2.2.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1"><minus id="S4.E4.m1.2.2.1.1.1.2.cmml" xref="S4.E4.m1.2.2.1.1.1"></minus><apply id="S4.E4.m1.2.2.1.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1"><times id="S4.E4.m1.2.2.1.1.1.1.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.2"></times><apply id="S4.E4.m1.2.2.1.1.1.1.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.3"><divide id="S4.E4.m1.2.2.1.1.1.1.3.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.3"></divide><cn type="integer" id="S4.E4.m1.2.2.1.1.1.1.3.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.3.2">1</cn><apply id="S4.E4.m1.2.2.1.1.1.1.3.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E4.m1.2.2.1.1.1.1.3.3.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.3.3">subscript</csymbol><ci id="S4.E4.m1.2.2.1.1.1.1.3.3.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.3.3.2">𝑇</ci><ci id="S4.E4.m1.2.2.1.1.1.1.3.3.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.3.3.3">𝑚</ci></apply></apply><apply id="S4.E4.m1.2.2.1.1.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1"><apply id="S4.E4.m1.2.2.1.1.1.1.1.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E4.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.2">subscript</csymbol><sum id="S4.E4.m1.2.2.1.1.1.1.1.2.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.2.2"></sum><apply id="S4.E4.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.2.3"><in id="S4.E4.m1.2.2.1.1.1.1.1.2.3.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.2.3.1"></in><ci id="S4.E4.m1.2.2.1.1.1.1.1.2.3.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.2.3.2">𝑡</ci><apply id="S4.E4.m1.2.2.1.1.1.1.1.2.3.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.2.3.3"><csymbol cd="ambiguous" id="S4.E4.m1.2.2.1.1.1.1.1.2.3.3.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.2.3.3">subscript</csymbol><ci id="S4.E4.m1.2.2.1.1.1.1.1.2.3.3.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.2.3.3.2">𝑇</ci><ci id="S4.E4.m1.2.2.1.1.1.1.1.2.3.3.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.2.3.3.3">𝑚</ci></apply></apply></apply><apply id="S4.E4.m1.2.2.1.1.1.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1"><times id="S4.E4.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.2"></times><apply id="S4.E4.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.3"><log id="S4.E4.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.3.1"></log><ci id="S4.E4.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.3.2">𝜎</ci></apply><apply id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1"><times id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.2"></times><ci id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3"><mtext id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.3">sim</mtext></ci><interval closure="open" id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1"><apply id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1"><times id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.1"></times><apply id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑨</ci><ci id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑠</ci></apply><apply id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2">𝒐</ci><ci id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3a.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3"><mtext mathsize="70%" id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3"> s</mtext></ci></apply><ci id="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E4.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply><ci id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1">𝒔</ci></interval></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.2c">\mathcal{L}_{\text{ s}}=-\frac{1}{T_{m}}\sum_{t\in T_{m}}\log\sigma\left(\text{sim}(\bm{A}^{s}\bm{o}^{\text{ s}}_{t},\bm{s})\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.SSS2.p3.6" class="ltx_p">where <math id="S4.SS1.SSS2.p3.2.m1.1" class="ltx_Math" alttext="\bm{A}^{s}\in\mathbb{R}^{D\times K}" display="inline"><semantics id="S4.SS1.SSS2.p3.2.m1.1a"><mrow id="S4.SS1.SSS2.p3.2.m1.1.1" xref="S4.SS1.SSS2.p3.2.m1.1.1.cmml"><msup id="S4.SS1.SSS2.p3.2.m1.1.1.2" xref="S4.SS1.SSS2.p3.2.m1.1.1.2.cmml"><mi id="S4.SS1.SSS2.p3.2.m1.1.1.2.2" xref="S4.SS1.SSS2.p3.2.m1.1.1.2.2.cmml">𝑨</mi><mi id="S4.SS1.SSS2.p3.2.m1.1.1.2.3" xref="S4.SS1.SSS2.p3.2.m1.1.1.2.3.cmml">s</mi></msup><mo id="S4.SS1.SSS2.p3.2.m1.1.1.1" xref="S4.SS1.SSS2.p3.2.m1.1.1.1.cmml">∈</mo><msup id="S4.SS1.SSS2.p3.2.m1.1.1.3" xref="S4.SS1.SSS2.p3.2.m1.1.1.3.cmml"><mi id="S4.SS1.SSS2.p3.2.m1.1.1.3.2" xref="S4.SS1.SSS2.p3.2.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS1.SSS2.p3.2.m1.1.1.3.3" xref="S4.SS1.SSS2.p3.2.m1.1.1.3.3.cmml"><mi id="S4.SS1.SSS2.p3.2.m1.1.1.3.3.2" xref="S4.SS1.SSS2.p3.2.m1.1.1.3.3.2.cmml">D</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.SSS2.p3.2.m1.1.1.3.3.1" xref="S4.SS1.SSS2.p3.2.m1.1.1.3.3.1.cmml">×</mo><mi id="S4.SS1.SSS2.p3.2.m1.1.1.3.3.3" xref="S4.SS1.SSS2.p3.2.m1.1.1.3.3.3.cmml">K</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p3.2.m1.1b"><apply id="S4.SS1.SSS2.p3.2.m1.1.1.cmml" xref="S4.SS1.SSS2.p3.2.m1.1.1"><in id="S4.SS1.SSS2.p3.2.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p3.2.m1.1.1.1"></in><apply id="S4.SS1.SSS2.p3.2.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p3.2.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p3.2.m1.1.1.2.1.cmml" xref="S4.SS1.SSS2.p3.2.m1.1.1.2">superscript</csymbol><ci id="S4.SS1.SSS2.p3.2.m1.1.1.2.2.cmml" xref="S4.SS1.SSS2.p3.2.m1.1.1.2.2">𝑨</ci><ci id="S4.SS1.SSS2.p3.2.m1.1.1.2.3.cmml" xref="S4.SS1.SSS2.p3.2.m1.1.1.2.3">𝑠</ci></apply><apply id="S4.SS1.SSS2.p3.2.m1.1.1.3.cmml" xref="S4.SS1.SSS2.p3.2.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p3.2.m1.1.1.3.1.cmml" xref="S4.SS1.SSS2.p3.2.m1.1.1.3">superscript</csymbol><ci id="S4.SS1.SSS2.p3.2.m1.1.1.3.2.cmml" xref="S4.SS1.SSS2.p3.2.m1.1.1.3.2">ℝ</ci><apply id="S4.SS1.SSS2.p3.2.m1.1.1.3.3.cmml" xref="S4.SS1.SSS2.p3.2.m1.1.1.3.3"><times id="S4.SS1.SSS2.p3.2.m1.1.1.3.3.1.cmml" xref="S4.SS1.SSS2.p3.2.m1.1.1.3.3.1"></times><ci id="S4.SS1.SSS2.p3.2.m1.1.1.3.3.2.cmml" xref="S4.SS1.SSS2.p3.2.m1.1.1.3.3.2">𝐷</ci><ci id="S4.SS1.SSS2.p3.2.m1.1.1.3.3.3.cmml" xref="S4.SS1.SSS2.p3.2.m1.1.1.3.3.3">𝐾</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p3.2.m1.1c">\bm{A}^{s}\in\mathbb{R}^{D\times K}</annotation></semantics></math> is a projection matrix, <math id="S4.SS1.SSS2.p3.3.m2.2" class="ltx_Math" alttext="\text{sim}\left(\cdot,\cdot\right)" display="inline"><semantics id="S4.SS1.SSS2.p3.3.m2.2a"><mrow id="S4.SS1.SSS2.p3.3.m2.2.3" xref="S4.SS1.SSS2.p3.3.m2.2.3.cmml"><mtext id="S4.SS1.SSS2.p3.3.m2.2.3.2" xref="S4.SS1.SSS2.p3.3.m2.2.3.2a.cmml">sim</mtext><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p3.3.m2.2.3.1" xref="S4.SS1.SSS2.p3.3.m2.2.3.1.cmml">​</mo><mrow id="S4.SS1.SSS2.p3.3.m2.2.3.3.2" xref="S4.SS1.SSS2.p3.3.m2.2.3.3.1.cmml"><mo id="S4.SS1.SSS2.p3.3.m2.2.3.3.2.1" xref="S4.SS1.SSS2.p3.3.m2.2.3.3.1.cmml">(</mo><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p3.3.m2.1.1" xref="S4.SS1.SSS2.p3.3.m2.1.1.cmml">⋅</mo><mo rspace="0em" id="S4.SS1.SSS2.p3.3.m2.2.3.3.2.2" xref="S4.SS1.SSS2.p3.3.m2.2.3.3.1.cmml">,</mo><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p3.3.m2.2.2" xref="S4.SS1.SSS2.p3.3.m2.2.2.cmml">⋅</mo><mo id="S4.SS1.SSS2.p3.3.m2.2.3.3.2.3" xref="S4.SS1.SSS2.p3.3.m2.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p3.3.m2.2b"><apply id="S4.SS1.SSS2.p3.3.m2.2.3.cmml" xref="S4.SS1.SSS2.p3.3.m2.2.3"><times id="S4.SS1.SSS2.p3.3.m2.2.3.1.cmml" xref="S4.SS1.SSS2.p3.3.m2.2.3.1"></times><ci id="S4.SS1.SSS2.p3.3.m2.2.3.2a.cmml" xref="S4.SS1.SSS2.p3.3.m2.2.3.2"><mtext id="S4.SS1.SSS2.p3.3.m2.2.3.2.cmml" xref="S4.SS1.SSS2.p3.3.m2.2.3.2">sim</mtext></ci><interval closure="open" id="S4.SS1.SSS2.p3.3.m2.2.3.3.1.cmml" xref="S4.SS1.SSS2.p3.3.m2.2.3.3.2"><ci id="S4.SS1.SSS2.p3.3.m2.1.1.cmml" xref="S4.SS1.SSS2.p3.3.m2.1.1">⋅</ci><ci id="S4.SS1.SSS2.p3.3.m2.2.2.cmml" xref="S4.SS1.SSS2.p3.3.m2.2.2">⋅</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p3.3.m2.2c">\text{sim}\left(\cdot,\cdot\right)</annotation></semantics></math> represents the cosine similarity, and <math id="S4.SS1.SSS2.p3.4.m3.1" class="ltx_Math" alttext="\sigma(\cdot)" display="inline"><semantics id="S4.SS1.SSS2.p3.4.m3.1a"><mrow id="S4.SS1.SSS2.p3.4.m3.1.2" xref="S4.SS1.SSS2.p3.4.m3.1.2.cmml"><mi id="S4.SS1.SSS2.p3.4.m3.1.2.2" xref="S4.SS1.SSS2.p3.4.m3.1.2.2.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p3.4.m3.1.2.1" xref="S4.SS1.SSS2.p3.4.m3.1.2.1.cmml">​</mo><mrow id="S4.SS1.SSS2.p3.4.m3.1.2.3.2" xref="S4.SS1.SSS2.p3.4.m3.1.2.cmml"><mo stretchy="false" id="S4.SS1.SSS2.p3.4.m3.1.2.3.2.1" xref="S4.SS1.SSS2.p3.4.m3.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S4.SS1.SSS2.p3.4.m3.1.1" xref="S4.SS1.SSS2.p3.4.m3.1.1.cmml">⋅</mo><mo stretchy="false" id="S4.SS1.SSS2.p3.4.m3.1.2.3.2.2" xref="S4.SS1.SSS2.p3.4.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p3.4.m3.1b"><apply id="S4.SS1.SSS2.p3.4.m3.1.2.cmml" xref="S4.SS1.SSS2.p3.4.m3.1.2"><times id="S4.SS1.SSS2.p3.4.m3.1.2.1.cmml" xref="S4.SS1.SSS2.p3.4.m3.1.2.1"></times><ci id="S4.SS1.SSS2.p3.4.m3.1.2.2.cmml" xref="S4.SS1.SSS2.p3.4.m3.1.2.2">𝜎</ci><ci id="S4.SS1.SSS2.p3.4.m3.1.1.cmml" xref="S4.SS1.SSS2.p3.4.m3.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p3.4.m3.1c">\sigma(\cdot)</annotation></semantics></math> denotes the sigmoid <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>. The speaker loss <math id="S4.SS1.SSS2.p3.5.m4.1" class="ltx_Math" alttext="\mathcal{L}_{\text{ s}}" display="inline"><semantics id="S4.SS1.SSS2.p3.5.m4.1a"><msub id="S4.SS1.SSS2.p3.5.m4.1.1" xref="S4.SS1.SSS2.p3.5.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS2.p3.5.m4.1.1.2" xref="S4.SS1.SSS2.p3.5.m4.1.1.2.cmml">ℒ</mi><mtext id="S4.SS1.SSS2.p3.5.m4.1.1.3" xref="S4.SS1.SSS2.p3.5.m4.1.1.3a.cmml"> s</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p3.5.m4.1b"><apply id="S4.SS1.SSS2.p3.5.m4.1.1.cmml" xref="S4.SS1.SSS2.p3.5.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p3.5.m4.1.1.1.cmml" xref="S4.SS1.SSS2.p3.5.m4.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.p3.5.m4.1.1.2.cmml" xref="S4.SS1.SSS2.p3.5.m4.1.1.2">ℒ</ci><ci id="S4.SS1.SSS2.p3.5.m4.1.1.3a.cmml" xref="S4.SS1.SSS2.p3.5.m4.1.1.3"><mtext mathsize="70%" id="S4.SS1.SSS2.p3.5.m4.1.1.3.cmml" xref="S4.SS1.SSS2.p3.5.m4.1.1.3"> s</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p3.5.m4.1c">\mathcal{L}_{\text{ s}}</annotation></semantics></math> is calculated only on the masked frames <math id="S4.SS1.SSS2.p3.6.m5.1" class="ltx_Math" alttext="T_{m}" display="inline"><semantics id="S4.SS1.SSS2.p3.6.m5.1a"><msub id="S4.SS1.SSS2.p3.6.m5.1.1" xref="S4.SS1.SSS2.p3.6.m5.1.1.cmml"><mi id="S4.SS1.SSS2.p3.6.m5.1.1.2" xref="S4.SS1.SSS2.p3.6.m5.1.1.2.cmml">T</mi><mi id="S4.SS1.SSS2.p3.6.m5.1.1.3" xref="S4.SS1.SSS2.p3.6.m5.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p3.6.m5.1b"><apply id="S4.SS1.SSS2.p3.6.m5.1.1.cmml" xref="S4.SS1.SSS2.p3.6.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p3.6.m5.1.1.1.cmml" xref="S4.SS1.SSS2.p3.6.m5.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.p3.6.m5.1.1.2.cmml" xref="S4.SS1.SSS2.p3.6.m5.1.1.2">𝑇</ci><ci id="S4.SS1.SSS2.p3.6.m5.1.1.3.cmml" xref="S4.SS1.SSS2.p3.6.m5.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p3.6.m5.1c">T_{m}</annotation></semantics></math>.</p>
</div>
<div id="S4.SS1.SSS2.p4" class="ltx_para">
<p id="S4.SS1.SSS2.p4.3" class="ltx_p">The extracted speaker representation <math id="S4.SS1.SSS2.p4.1.m1.1" class="ltx_Math" alttext="\bm{O}^{\text{ s}}" display="inline"><semantics id="S4.SS1.SSS2.p4.1.m1.1a"><msup id="S4.SS1.SSS2.p4.1.m1.1.1" xref="S4.SS1.SSS2.p4.1.m1.1.1.cmml"><mi id="S4.SS1.SSS2.p4.1.m1.1.1.2" xref="S4.SS1.SSS2.p4.1.m1.1.1.2.cmml">𝑶</mi><mtext id="S4.SS1.SSS2.p4.1.m1.1.1.3" xref="S4.SS1.SSS2.p4.1.m1.1.1.3a.cmml"> s</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.1.m1.1b"><apply id="S4.SS1.SSS2.p4.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p4.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p4.1.m1.1.1">superscript</csymbol><ci id="S4.SS1.SSS2.p4.1.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p4.1.m1.1.1.2">𝑶</ci><ci id="S4.SS1.SSS2.p4.1.m1.1.1.3a.cmml" xref="S4.SS1.SSS2.p4.1.m1.1.1.3"><mtext mathsize="70%" id="S4.SS1.SSS2.p4.1.m1.1.1.3.cmml" xref="S4.SS1.SSS2.p4.1.m1.1.1.3"> s</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.1.m1.1c">\bm{O}^{\text{ s}}</annotation></semantics></math> is then removed from the output of the <math id="S4.SS1.SSS2.p4.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS1.SSS2.p4.2.m2.1a"><mi id="S4.SS1.SSS2.p4.2.m2.1.1" xref="S4.SS1.SSS2.p4.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.2.m2.1b"><ci id="S4.SS1.SSS2.p4.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p4.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.2.m2.1c">i</annotation></semantics></math>-th Transformer block <math id="S4.SS1.SSS2.p4.3.m3.1" class="ltx_Math" alttext="\bm{O}_{i}" display="inline"><semantics id="S4.SS1.SSS2.p4.3.m3.1a"><msub id="S4.SS1.SSS2.p4.3.m3.1.1" xref="S4.SS1.SSS2.p4.3.m3.1.1.cmml"><mi id="S4.SS1.SSS2.p4.3.m3.1.1.2" xref="S4.SS1.SSS2.p4.3.m3.1.1.2.cmml">𝑶</mi><mi id="S4.SS1.SSS2.p4.3.m3.1.1.3" xref="S4.SS1.SSS2.p4.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.3.m3.1b"><apply id="S4.SS1.SSS2.p4.3.m3.1.1.cmml" xref="S4.SS1.SSS2.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p4.3.m3.1.1.1.cmml" xref="S4.SS1.SSS2.p4.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.p4.3.m3.1.1.2.cmml" xref="S4.SS1.SSS2.p4.3.m3.1.1.2">𝑶</ci><ci id="S4.SS1.SSS2.p4.3.m3.1.1.3.cmml" xref="S4.SS1.SSS2.p4.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.3.m3.1c">\bm{O}_{i}</annotation></semantics></math>, as</p>
<table id="S4.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E5.m1.1" class="ltx_Math" alttext="\bm{I}_{i+1}=\text{layernorm}\left(\bm{O}_{i}-\bm{O}^{\text{ s}}\right)," display="block"><semantics id="S4.E5.m1.1a"><mrow id="S4.E5.m1.1.1.1" xref="S4.E5.m1.1.1.1.1.cmml"><mrow id="S4.E5.m1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.cmml"><msub id="S4.E5.m1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.3.cmml"><mi id="S4.E5.m1.1.1.1.1.3.2" xref="S4.E5.m1.1.1.1.1.3.2.cmml">𝑰</mi><mrow id="S4.E5.m1.1.1.1.1.3.3" xref="S4.E5.m1.1.1.1.1.3.3.cmml"><mi id="S4.E5.m1.1.1.1.1.3.3.2" xref="S4.E5.m1.1.1.1.1.3.3.2.cmml">i</mi><mo id="S4.E5.m1.1.1.1.1.3.3.1" xref="S4.E5.m1.1.1.1.1.3.3.1.cmml">+</mo><mn id="S4.E5.m1.1.1.1.1.3.3.3" xref="S4.E5.m1.1.1.1.1.3.3.3.cmml">1</mn></mrow></msub><mo id="S4.E5.m1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.2.cmml">=</mo><mrow id="S4.E5.m1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.cmml"><mtext id="S4.E5.m1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.1.3a.cmml">layernorm</mtext><mo lspace="0em" rspace="0em" id="S4.E5.m1.1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E5.m1.1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E5.m1.1.1.1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E5.m1.1.1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E5.m1.1.1.1.1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E5.m1.1.1.1.1.1.1.1.1.2.2" xref="S4.E5.m1.1.1.1.1.1.1.1.1.2.2.cmml">𝑶</mi><mi id="S4.E5.m1.1.1.1.1.1.1.1.1.2.3" xref="S4.E5.m1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S4.E5.m1.1.1.1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msup id="S4.E5.m1.1.1.1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E5.m1.1.1.1.1.1.1.1.1.3.2" xref="S4.E5.m1.1.1.1.1.1.1.1.1.3.2.cmml">𝑶</mi><mtext id="S4.E5.m1.1.1.1.1.1.1.1.1.3.3" xref="S4.E5.m1.1.1.1.1.1.1.1.1.3.3a.cmml"> s</mtext></msup></mrow><mo id="S4.E5.m1.1.1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E5.m1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E5.m1.1b"><apply id="S4.E5.m1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1"><eq id="S4.E5.m1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.2"></eq><apply id="S4.E5.m1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.1.3.1.cmml" xref="S4.E5.m1.1.1.1.1.3">subscript</csymbol><ci id="S4.E5.m1.1.1.1.1.3.2.cmml" xref="S4.E5.m1.1.1.1.1.3.2">𝑰</ci><apply id="S4.E5.m1.1.1.1.1.3.3.cmml" xref="S4.E5.m1.1.1.1.1.3.3"><plus id="S4.E5.m1.1.1.1.1.3.3.1.cmml" xref="S4.E5.m1.1.1.1.1.3.3.1"></plus><ci id="S4.E5.m1.1.1.1.1.3.3.2.cmml" xref="S4.E5.m1.1.1.1.1.3.3.2">𝑖</ci><cn type="integer" id="S4.E5.m1.1.1.1.1.3.3.3.cmml" xref="S4.E5.m1.1.1.1.1.3.3.3">1</cn></apply></apply><apply id="S4.E5.m1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1"><times id="S4.E5.m1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.1.2"></times><ci id="S4.E5.m1.1.1.1.1.1.3a.cmml" xref="S4.E5.m1.1.1.1.1.1.3"><mtext id="S4.E5.m1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.1.3">layernorm</mtext></ci><apply id="S4.E5.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1"><minus id="S4.E5.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1"></minus><apply id="S4.E5.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E5.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.2.2">𝑶</ci><ci id="S4.E5.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.E5.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S4.E5.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.3.2">𝑶</ci><ci id="S4.E5.m1.1.1.1.1.1.1.1.1.3.3a.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.3.3"><mtext mathsize="70%" id="S4.E5.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.3.3"> s</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.1c">\bm{I}_{i+1}=\text{layernorm}\left(\bm{O}_{i}-\bm{O}^{\text{ s}}\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.SSS2.p4.4" class="ltx_p">where <math id="S4.SS1.SSS2.p4.4.m1.1" class="ltx_Math" alttext="\bm{I}_{i+1}" display="inline"><semantics id="S4.SS1.SSS2.p4.4.m1.1a"><msub id="S4.SS1.SSS2.p4.4.m1.1.1" xref="S4.SS1.SSS2.p4.4.m1.1.1.cmml"><mi id="S4.SS1.SSS2.p4.4.m1.1.1.2" xref="S4.SS1.SSS2.p4.4.m1.1.1.2.cmml">𝑰</mi><mrow id="S4.SS1.SSS2.p4.4.m1.1.1.3" xref="S4.SS1.SSS2.p4.4.m1.1.1.3.cmml"><mi id="S4.SS1.SSS2.p4.4.m1.1.1.3.2" xref="S4.SS1.SSS2.p4.4.m1.1.1.3.2.cmml">i</mi><mo id="S4.SS1.SSS2.p4.4.m1.1.1.3.1" xref="S4.SS1.SSS2.p4.4.m1.1.1.3.1.cmml">+</mo><mn id="S4.SS1.SSS2.p4.4.m1.1.1.3.3" xref="S4.SS1.SSS2.p4.4.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p4.4.m1.1b"><apply id="S4.SS1.SSS2.p4.4.m1.1.1.cmml" xref="S4.SS1.SSS2.p4.4.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p4.4.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p4.4.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.p4.4.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p4.4.m1.1.1.2">𝑰</ci><apply id="S4.SS1.SSS2.p4.4.m1.1.1.3.cmml" xref="S4.SS1.SSS2.p4.4.m1.1.1.3"><plus id="S4.SS1.SSS2.p4.4.m1.1.1.3.1.cmml" xref="S4.SS1.SSS2.p4.4.m1.1.1.3.1"></plus><ci id="S4.SS1.SSS2.p4.4.m1.1.1.3.2.cmml" xref="S4.SS1.SSS2.p4.4.m1.1.1.3.2">𝑖</ci><cn type="integer" id="S4.SS1.SSS2.p4.4.m1.1.1.3.3.cmml" xref="S4.SS1.SSS2.p4.4.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p4.4.m1.1c">\bm{I}_{i+1}</annotation></semantics></math> denotes the input of the next Transformer block.</p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS3.5.1.1" class="ltx_text">IV-A</span>3 </span>Content Information Modeling</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">The training of the main branch in our model follows the HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Specifically, we employ the BERT-like masked pseudo-label prediction task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> based on K-means clustering. This objective encourages the deep layers of the encoder to learn content representations while allowing the shallow-layer representations closer to the input to retain more acoustic details <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Preserving these task characteristics of different layers is crucial for ensuring the effectiveness of the pitch and speaker extractors.</p>
</div>
<div id="S4.SS1.SSS3.p2" class="ltx_para">
<p id="S4.SS1.SSS3.p2.3" class="ltx_p">Before pre-training, we perform offline clustering of MFCC or hidden-layer representations from the previously pre-trained model to generate pseudo-labels <math id="S4.SS1.SSS3.p2.1.m1.4" class="ltx_Math" alttext="\bm{Z}\!=\!\left[z_{1},z_{2},\cdots,z_{T}\right]" display="inline"><semantics id="S4.SS1.SSS3.p2.1.m1.4a"><mrow id="S4.SS1.SSS3.p2.1.m1.4.4" xref="S4.SS1.SSS3.p2.1.m1.4.4.cmml"><mi id="S4.SS1.SSS3.p2.1.m1.4.4.5" xref="S4.SS1.SSS3.p2.1.m1.4.4.5.cmml">𝒁</mi><mo lspace="0.108em" rspace="0.108em" id="S4.SS1.SSS3.p2.1.m1.4.4.4" xref="S4.SS1.SSS3.p2.1.m1.4.4.4.cmml">=</mo><mrow id="S4.SS1.SSS3.p2.1.m1.4.4.3.3" xref="S4.SS1.SSS3.p2.1.m1.4.4.3.4.cmml"><mo id="S4.SS1.SSS3.p2.1.m1.4.4.3.3.4" xref="S4.SS1.SSS3.p2.1.m1.4.4.3.4.cmml">[</mo><msub id="S4.SS1.SSS3.p2.1.m1.2.2.1.1.1" xref="S4.SS1.SSS3.p2.1.m1.2.2.1.1.1.cmml"><mi id="S4.SS1.SSS3.p2.1.m1.2.2.1.1.1.2" xref="S4.SS1.SSS3.p2.1.m1.2.2.1.1.1.2.cmml">z</mi><mn id="S4.SS1.SSS3.p2.1.m1.2.2.1.1.1.3" xref="S4.SS1.SSS3.p2.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S4.SS1.SSS3.p2.1.m1.4.4.3.3.5" xref="S4.SS1.SSS3.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.SS1.SSS3.p2.1.m1.3.3.2.2.2" xref="S4.SS1.SSS3.p2.1.m1.3.3.2.2.2.cmml"><mi id="S4.SS1.SSS3.p2.1.m1.3.3.2.2.2.2" xref="S4.SS1.SSS3.p2.1.m1.3.3.2.2.2.2.cmml">z</mi><mn id="S4.SS1.SSS3.p2.1.m1.3.3.2.2.2.3" xref="S4.SS1.SSS3.p2.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS1.SSS3.p2.1.m1.4.4.3.3.6" xref="S4.SS1.SSS3.p2.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.SSS3.p2.1.m1.1.1" xref="S4.SS1.SSS3.p2.1.m1.1.1.cmml">⋯</mi><mo id="S4.SS1.SSS3.p2.1.m1.4.4.3.3.7" xref="S4.SS1.SSS3.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S4.SS1.SSS3.p2.1.m1.4.4.3.3.3" xref="S4.SS1.SSS3.p2.1.m1.4.4.3.3.3.cmml"><mi id="S4.SS1.SSS3.p2.1.m1.4.4.3.3.3.2" xref="S4.SS1.SSS3.p2.1.m1.4.4.3.3.3.2.cmml">z</mi><mi id="S4.SS1.SSS3.p2.1.m1.4.4.3.3.3.3" xref="S4.SS1.SSS3.p2.1.m1.4.4.3.3.3.3.cmml">T</mi></msub><mo id="S4.SS1.SSS3.p2.1.m1.4.4.3.3.8" xref="S4.SS1.SSS3.p2.1.m1.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.1.m1.4b"><apply id="S4.SS1.SSS3.p2.1.m1.4.4.cmml" xref="S4.SS1.SSS3.p2.1.m1.4.4"><eq id="S4.SS1.SSS3.p2.1.m1.4.4.4.cmml" xref="S4.SS1.SSS3.p2.1.m1.4.4.4"></eq><ci id="S4.SS1.SSS3.p2.1.m1.4.4.5.cmml" xref="S4.SS1.SSS3.p2.1.m1.4.4.5">𝒁</ci><list id="S4.SS1.SSS3.p2.1.m1.4.4.3.4.cmml" xref="S4.SS1.SSS3.p2.1.m1.4.4.3.3"><apply id="S4.SS1.SSS3.p2.1.m1.2.2.1.1.1.cmml" xref="S4.SS1.SSS3.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p2.1.m1.2.2.1.1.1.1.cmml" xref="S4.SS1.SSS3.p2.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S4.SS1.SSS3.p2.1.m1.2.2.1.1.1.2.cmml" xref="S4.SS1.SSS3.p2.1.m1.2.2.1.1.1.2">𝑧</ci><cn type="integer" id="S4.SS1.SSS3.p2.1.m1.2.2.1.1.1.3.cmml" xref="S4.SS1.SSS3.p2.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS1.SSS3.p2.1.m1.3.3.2.2.2.cmml" xref="S4.SS1.SSS3.p2.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p2.1.m1.3.3.2.2.2.1.cmml" xref="S4.SS1.SSS3.p2.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S4.SS1.SSS3.p2.1.m1.3.3.2.2.2.2.cmml" xref="S4.SS1.SSS3.p2.1.m1.3.3.2.2.2.2">𝑧</ci><cn type="integer" id="S4.SS1.SSS3.p2.1.m1.3.3.2.2.2.3.cmml" xref="S4.SS1.SSS3.p2.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS1.SSS3.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS3.p2.1.m1.1.1">⋯</ci><apply id="S4.SS1.SSS3.p2.1.m1.4.4.3.3.3.cmml" xref="S4.SS1.SSS3.p2.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p2.1.m1.4.4.3.3.3.1.cmml" xref="S4.SS1.SSS3.p2.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S4.SS1.SSS3.p2.1.m1.4.4.3.3.3.2.cmml" xref="S4.SS1.SSS3.p2.1.m1.4.4.3.3.3.2">𝑧</ci><ci id="S4.SS1.SSS3.p2.1.m1.4.4.3.3.3.3.cmml" xref="S4.SS1.SSS3.p2.1.m1.4.4.3.3.3.3">𝑇</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.1.m1.4c">\bm{Z}\!=\!\left[z_{1},z_{2},\cdots,z_{T}\right]</annotation></semantics></math>, where each <math id="S4.SS1.SSS3.p2.2.m2.1" class="ltx_Math" alttext="z\in\left[U\right]" display="inline"><semantics id="S4.SS1.SSS3.p2.2.m2.1a"><mrow id="S4.SS1.SSS3.p2.2.m2.1.2" xref="S4.SS1.SSS3.p2.2.m2.1.2.cmml"><mi id="S4.SS1.SSS3.p2.2.m2.1.2.2" xref="S4.SS1.SSS3.p2.2.m2.1.2.2.cmml">z</mi><mo id="S4.SS1.SSS3.p2.2.m2.1.2.1" xref="S4.SS1.SSS3.p2.2.m2.1.2.1.cmml">∈</mo><mrow id="S4.SS1.SSS3.p2.2.m2.1.2.3.2" xref="S4.SS1.SSS3.p2.2.m2.1.2.3.1.cmml"><mo id="S4.SS1.SSS3.p2.2.m2.1.2.3.2.1" xref="S4.SS1.SSS3.p2.2.m2.1.2.3.1.1.cmml">[</mo><mi id="S4.SS1.SSS3.p2.2.m2.1.1" xref="S4.SS1.SSS3.p2.2.m2.1.1.cmml">U</mi><mo id="S4.SS1.SSS3.p2.2.m2.1.2.3.2.2" xref="S4.SS1.SSS3.p2.2.m2.1.2.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.2.m2.1b"><apply id="S4.SS1.SSS3.p2.2.m2.1.2.cmml" xref="S4.SS1.SSS3.p2.2.m2.1.2"><in id="S4.SS1.SSS3.p2.2.m2.1.2.1.cmml" xref="S4.SS1.SSS3.p2.2.m2.1.2.1"></in><ci id="S4.SS1.SSS3.p2.2.m2.1.2.2.cmml" xref="S4.SS1.SSS3.p2.2.m2.1.2.2">𝑧</ci><apply id="S4.SS1.SSS3.p2.2.m2.1.2.3.1.cmml" xref="S4.SS1.SSS3.p2.2.m2.1.2.3.2"><csymbol cd="latexml" id="S4.SS1.SSS3.p2.2.m2.1.2.3.1.1.cmml" xref="S4.SS1.SSS3.p2.2.m2.1.2.3.2.1">delimited-[]</csymbol><ci id="S4.SS1.SSS3.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS3.p2.2.m2.1.1">𝑈</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.2.m2.1c">z\in\left[U\right]</annotation></semantics></math> is a <math id="S4.SS1.SSS3.p2.3.m3.1" class="ltx_Math" alttext="U" display="inline"><semantics id="S4.SS1.SSS3.p2.3.m3.1a"><mi id="S4.SS1.SSS3.p2.3.m3.1.1" xref="S4.SS1.SSS3.p2.3.m3.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.3.m3.1b"><ci id="S4.SS1.SSS3.p2.3.m3.1.1.cmml" xref="S4.SS1.SSS3.p2.3.m3.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.3.m3.1c">U</annotation></semantics></math>-class variable. As illustrated in Fig. <a href="#S3.F4" title="Figure 4 ‣ III Problem Formulation ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, during pre-training, the frame-level output of the convolution module is randomly and successively masked, and then fed into the Transformer encoder. After extracting and removing pitch variation and speaker information, the main branch is trained to predict the pseudo-labels of the masked frames, as:</p>
<table id="S4.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E6.m1.5" class="ltx_Math" alttext="\mathcal{L}_{\text{ c}}=\frac{1}{T_{m}}\sum_{t\in T_{m}}\log{\frac{\exp\left(\text{sim}\left(\bm{A}^{c}\bm{o}^{\text{ c}}_{t},\bm{e}_{u}\right)/\tau\right)}{\sum_{u^{\prime}}^{U}\exp\left(\text{sim}\left(\bm{A}^{c}\bm{o}^{\text{ c}}_{t},\bm{e}_{u^{\prime}}\right)/\tau\right)}}," display="block"><semantics id="S4.E6.m1.5a"><mrow id="S4.E6.m1.5.5.1" xref="S4.E6.m1.5.5.1.1.cmml"><mrow id="S4.E6.m1.5.5.1.1" xref="S4.E6.m1.5.5.1.1.cmml"><msub id="S4.E6.m1.5.5.1.1.2" xref="S4.E6.m1.5.5.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E6.m1.5.5.1.1.2.2" xref="S4.E6.m1.5.5.1.1.2.2.cmml">ℒ</mi><mtext id="S4.E6.m1.5.5.1.1.2.3" xref="S4.E6.m1.5.5.1.1.2.3a.cmml"> c</mtext></msub><mo id="S4.E6.m1.5.5.1.1.1" xref="S4.E6.m1.5.5.1.1.1.cmml">=</mo><mrow id="S4.E6.m1.5.5.1.1.3" xref="S4.E6.m1.5.5.1.1.3.cmml"><mfrac id="S4.E6.m1.5.5.1.1.3.2" xref="S4.E6.m1.5.5.1.1.3.2.cmml"><mn id="S4.E6.m1.5.5.1.1.3.2.2" xref="S4.E6.m1.5.5.1.1.3.2.2.cmml">1</mn><msub id="S4.E6.m1.5.5.1.1.3.2.3" xref="S4.E6.m1.5.5.1.1.3.2.3.cmml"><mi id="S4.E6.m1.5.5.1.1.3.2.3.2" xref="S4.E6.m1.5.5.1.1.3.2.3.2.cmml">T</mi><mi id="S4.E6.m1.5.5.1.1.3.2.3.3" xref="S4.E6.m1.5.5.1.1.3.2.3.3.cmml">m</mi></msub></mfrac><mo lspace="0em" rspace="0em" id="S4.E6.m1.5.5.1.1.3.1" xref="S4.E6.m1.5.5.1.1.3.1.cmml">​</mo><mrow id="S4.E6.m1.5.5.1.1.3.3" xref="S4.E6.m1.5.5.1.1.3.3.cmml"><munder id="S4.E6.m1.5.5.1.1.3.3.1" xref="S4.E6.m1.5.5.1.1.3.3.1.cmml"><mo movablelimits="false" id="S4.E6.m1.5.5.1.1.3.3.1.2" xref="S4.E6.m1.5.5.1.1.3.3.1.2.cmml">∑</mo><mrow id="S4.E6.m1.5.5.1.1.3.3.1.3" xref="S4.E6.m1.5.5.1.1.3.3.1.3.cmml"><mi id="S4.E6.m1.5.5.1.1.3.3.1.3.2" xref="S4.E6.m1.5.5.1.1.3.3.1.3.2.cmml">t</mi><mo id="S4.E6.m1.5.5.1.1.3.3.1.3.1" xref="S4.E6.m1.5.5.1.1.3.3.1.3.1.cmml">∈</mo><msub id="S4.E6.m1.5.5.1.1.3.3.1.3.3" xref="S4.E6.m1.5.5.1.1.3.3.1.3.3.cmml"><mi id="S4.E6.m1.5.5.1.1.3.3.1.3.3.2" xref="S4.E6.m1.5.5.1.1.3.3.1.3.3.2.cmml">T</mi><mi id="S4.E6.m1.5.5.1.1.3.3.1.3.3.3" xref="S4.E6.m1.5.5.1.1.3.3.1.3.3.3.cmml">m</mi></msub></mrow></munder><mrow id="S4.E6.m1.5.5.1.1.3.3.2" xref="S4.E6.m1.5.5.1.1.3.3.2.cmml"><mi id="S4.E6.m1.5.5.1.1.3.3.2.1" xref="S4.E6.m1.5.5.1.1.3.3.2.1.cmml">log</mi><mo lspace="0.167em" id="S4.E6.m1.5.5.1.1.3.3.2a" xref="S4.E6.m1.5.5.1.1.3.3.2.cmml">⁡</mo><mfrac id="S4.E6.m1.4.4" xref="S4.E6.m1.4.4.cmml"><mrow id="S4.E6.m1.2.2.2.2" xref="S4.E6.m1.2.2.2.3.cmml"><mi id="S4.E6.m1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.cmml">exp</mi><mo id="S4.E6.m1.2.2.2.2a" xref="S4.E6.m1.2.2.2.3.cmml">⁡</mo><mrow id="S4.E6.m1.2.2.2.2.1" xref="S4.E6.m1.2.2.2.3.cmml"><mo id="S4.E6.m1.2.2.2.2.1.2" xref="S4.E6.m1.2.2.2.3.cmml">(</mo><mrow id="S4.E6.m1.2.2.2.2.1.1" xref="S4.E6.m1.2.2.2.2.1.1.cmml"><mrow id="S4.E6.m1.2.2.2.2.1.1.2" xref="S4.E6.m1.2.2.2.2.1.1.2.cmml"><mtext id="S4.E6.m1.2.2.2.2.1.1.2.4" xref="S4.E6.m1.2.2.2.2.1.1.2.4a.cmml">sim</mtext><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.2.2.1.1.2.3" xref="S4.E6.m1.2.2.2.2.1.1.2.3.cmml">​</mo><mrow id="S4.E6.m1.2.2.2.2.1.1.2.2.2" xref="S4.E6.m1.2.2.2.2.1.1.2.2.3.cmml"><mo id="S4.E6.m1.2.2.2.2.1.1.2.2.2.3" xref="S4.E6.m1.2.2.2.2.1.1.2.2.3.cmml">(</mo><mrow id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.cmml"><msup id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.2" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.2.cmml"><mi id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.2.2" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.2.2.cmml">𝑨</mi><mi id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.2.3" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.2.3.cmml">c</mi></msup><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.1" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.1.cmml">​</mo><msubsup id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3.cmml"><mi id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3.2.2" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3.2.2.cmml">𝒐</mi><mi id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3.3" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3.3.cmml">t</mi><mtext id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3.2.3" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3.2.3a.cmml"> c</mtext></msubsup></mrow><mo id="S4.E6.m1.2.2.2.2.1.1.2.2.2.4" xref="S4.E6.m1.2.2.2.2.1.1.2.2.3.cmml">,</mo><msub id="S4.E6.m1.2.2.2.2.1.1.2.2.2.2" xref="S4.E6.m1.2.2.2.2.1.1.2.2.2.2.cmml"><mi id="S4.E6.m1.2.2.2.2.1.1.2.2.2.2.2" xref="S4.E6.m1.2.2.2.2.1.1.2.2.2.2.2.cmml">𝒆</mi><mi id="S4.E6.m1.2.2.2.2.1.1.2.2.2.2.3" xref="S4.E6.m1.2.2.2.2.1.1.2.2.2.2.3.cmml">u</mi></msub><mo id="S4.E6.m1.2.2.2.2.1.1.2.2.2.5" xref="S4.E6.m1.2.2.2.2.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S4.E6.m1.2.2.2.2.1.1.3" xref="S4.E6.m1.2.2.2.2.1.1.3.cmml">/</mo><mi id="S4.E6.m1.2.2.2.2.1.1.4" xref="S4.E6.m1.2.2.2.2.1.1.4.cmml">τ</mi></mrow><mo id="S4.E6.m1.2.2.2.2.1.3" xref="S4.E6.m1.2.2.2.3.cmml">)</mo></mrow></mrow><mrow id="S4.E6.m1.4.4.4" xref="S4.E6.m1.4.4.4.cmml"><msubsup id="S4.E6.m1.4.4.4.3" xref="S4.E6.m1.4.4.4.3.cmml"><mo id="S4.E6.m1.4.4.4.3.2.2" xref="S4.E6.m1.4.4.4.3.2.2.cmml">∑</mo><msup id="S4.E6.m1.4.4.4.3.2.3" xref="S4.E6.m1.4.4.4.3.2.3.cmml"><mi id="S4.E6.m1.4.4.4.3.2.3.2" xref="S4.E6.m1.4.4.4.3.2.3.2.cmml">u</mi><mo id="S4.E6.m1.4.4.4.3.2.3.3" xref="S4.E6.m1.4.4.4.3.2.3.3.cmml">′</mo></msup><mi id="S4.E6.m1.4.4.4.3.3" xref="S4.E6.m1.4.4.4.3.3.cmml">U</mi></msubsup><mrow id="S4.E6.m1.4.4.4.2.1" xref="S4.E6.m1.4.4.4.2.2.cmml"><mi id="S4.E6.m1.3.3.3.1" xref="S4.E6.m1.3.3.3.1.cmml">exp</mi><mo id="S4.E6.m1.4.4.4.2.1a" xref="S4.E6.m1.4.4.4.2.2.cmml">⁡</mo><mrow id="S4.E6.m1.4.4.4.2.1.1" xref="S4.E6.m1.4.4.4.2.2.cmml"><mo id="S4.E6.m1.4.4.4.2.1.1.2" xref="S4.E6.m1.4.4.4.2.2.cmml">(</mo><mrow id="S4.E6.m1.4.4.4.2.1.1.1" xref="S4.E6.m1.4.4.4.2.1.1.1.cmml"><mrow id="S4.E6.m1.4.4.4.2.1.1.1.2" xref="S4.E6.m1.4.4.4.2.1.1.1.2.cmml"><mtext id="S4.E6.m1.4.4.4.2.1.1.1.2.4" xref="S4.E6.m1.4.4.4.2.1.1.1.2.4a.cmml">sim</mtext><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.4.2.1.1.1.2.3" xref="S4.E6.m1.4.4.4.2.1.1.1.2.3.cmml">​</mo><mrow id="S4.E6.m1.4.4.4.2.1.1.1.2.2.2" xref="S4.E6.m1.4.4.4.2.1.1.1.2.2.3.cmml"><mo id="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.3" xref="S4.E6.m1.4.4.4.2.1.1.1.2.2.3.cmml">(</mo><mrow id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.cmml"><msup id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.2" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.2.2" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.2.2.cmml">𝑨</mi><mi id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.2.3" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.2.3.cmml">c</mi></msup><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.1" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.1.cmml">​</mo><msubsup id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3.2.2" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3.2.2.cmml">𝒐</mi><mi id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3.3" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.cmml">t</mi><mtext id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3.2.3" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3.2.3a.cmml"> c</mtext></msubsup></mrow><mo id="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.4" xref="S4.E6.m1.4.4.4.2.1.1.1.2.2.3.cmml">,</mo><msub id="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2" xref="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.cmml"><mi id="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.2" xref="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.2.cmml">𝒆</mi><msup id="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.3" xref="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.3.cmml"><mi id="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.3.2" xref="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.3.2.cmml">u</mi><mo id="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.3.3" xref="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.3.3.cmml">′</mo></msup></msub><mo id="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.5" xref="S4.E6.m1.4.4.4.2.1.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S4.E6.m1.4.4.4.2.1.1.1.3" xref="S4.E6.m1.4.4.4.2.1.1.1.3.cmml">/</mo><mi id="S4.E6.m1.4.4.4.2.1.1.1.4" xref="S4.E6.m1.4.4.4.2.1.1.1.4.cmml">τ</mi></mrow><mo id="S4.E6.m1.4.4.4.2.1.1.3" xref="S4.E6.m1.4.4.4.2.2.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow></mrow></mrow></mrow><mo id="S4.E6.m1.5.5.1.2" xref="S4.E6.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E6.m1.5b"><apply id="S4.E6.m1.5.5.1.1.cmml" xref="S4.E6.m1.5.5.1"><eq id="S4.E6.m1.5.5.1.1.1.cmml" xref="S4.E6.m1.5.5.1.1.1"></eq><apply id="S4.E6.m1.5.5.1.1.2.cmml" xref="S4.E6.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S4.E6.m1.5.5.1.1.2.1.cmml" xref="S4.E6.m1.5.5.1.1.2">subscript</csymbol><ci id="S4.E6.m1.5.5.1.1.2.2.cmml" xref="S4.E6.m1.5.5.1.1.2.2">ℒ</ci><ci id="S4.E6.m1.5.5.1.1.2.3a.cmml" xref="S4.E6.m1.5.5.1.1.2.3"><mtext mathsize="70%" id="S4.E6.m1.5.5.1.1.2.3.cmml" xref="S4.E6.m1.5.5.1.1.2.3"> c</mtext></ci></apply><apply id="S4.E6.m1.5.5.1.1.3.cmml" xref="S4.E6.m1.5.5.1.1.3"><times id="S4.E6.m1.5.5.1.1.3.1.cmml" xref="S4.E6.m1.5.5.1.1.3.1"></times><apply id="S4.E6.m1.5.5.1.1.3.2.cmml" xref="S4.E6.m1.5.5.1.1.3.2"><divide id="S4.E6.m1.5.5.1.1.3.2.1.cmml" xref="S4.E6.m1.5.5.1.1.3.2"></divide><cn type="integer" id="S4.E6.m1.5.5.1.1.3.2.2.cmml" xref="S4.E6.m1.5.5.1.1.3.2.2">1</cn><apply id="S4.E6.m1.5.5.1.1.3.2.3.cmml" xref="S4.E6.m1.5.5.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.E6.m1.5.5.1.1.3.2.3.1.cmml" xref="S4.E6.m1.5.5.1.1.3.2.3">subscript</csymbol><ci id="S4.E6.m1.5.5.1.1.3.2.3.2.cmml" xref="S4.E6.m1.5.5.1.1.3.2.3.2">𝑇</ci><ci id="S4.E6.m1.5.5.1.1.3.2.3.3.cmml" xref="S4.E6.m1.5.5.1.1.3.2.3.3">𝑚</ci></apply></apply><apply id="S4.E6.m1.5.5.1.1.3.3.cmml" xref="S4.E6.m1.5.5.1.1.3.3"><apply id="S4.E6.m1.5.5.1.1.3.3.1.cmml" xref="S4.E6.m1.5.5.1.1.3.3.1"><csymbol cd="ambiguous" id="S4.E6.m1.5.5.1.1.3.3.1.1.cmml" xref="S4.E6.m1.5.5.1.1.3.3.1">subscript</csymbol><sum id="S4.E6.m1.5.5.1.1.3.3.1.2.cmml" xref="S4.E6.m1.5.5.1.1.3.3.1.2"></sum><apply id="S4.E6.m1.5.5.1.1.3.3.1.3.cmml" xref="S4.E6.m1.5.5.1.1.3.3.1.3"><in id="S4.E6.m1.5.5.1.1.3.3.1.3.1.cmml" xref="S4.E6.m1.5.5.1.1.3.3.1.3.1"></in><ci id="S4.E6.m1.5.5.1.1.3.3.1.3.2.cmml" xref="S4.E6.m1.5.5.1.1.3.3.1.3.2">𝑡</ci><apply id="S4.E6.m1.5.5.1.1.3.3.1.3.3.cmml" xref="S4.E6.m1.5.5.1.1.3.3.1.3.3"><csymbol cd="ambiguous" id="S4.E6.m1.5.5.1.1.3.3.1.3.3.1.cmml" xref="S4.E6.m1.5.5.1.1.3.3.1.3.3">subscript</csymbol><ci id="S4.E6.m1.5.5.1.1.3.3.1.3.3.2.cmml" xref="S4.E6.m1.5.5.1.1.3.3.1.3.3.2">𝑇</ci><ci id="S4.E6.m1.5.5.1.1.3.3.1.3.3.3.cmml" xref="S4.E6.m1.5.5.1.1.3.3.1.3.3.3">𝑚</ci></apply></apply></apply><apply id="S4.E6.m1.5.5.1.1.3.3.2.cmml" xref="S4.E6.m1.5.5.1.1.3.3.2"><log id="S4.E6.m1.5.5.1.1.3.3.2.1.cmml" xref="S4.E6.m1.5.5.1.1.3.3.2.1"></log><apply id="S4.E6.m1.4.4.cmml" xref="S4.E6.m1.4.4"><divide id="S4.E6.m1.4.4.5.cmml" xref="S4.E6.m1.4.4"></divide><apply id="S4.E6.m1.2.2.2.3.cmml" xref="S4.E6.m1.2.2.2.2"><exp id="S4.E6.m1.1.1.1.1.cmml" xref="S4.E6.m1.1.1.1.1"></exp><apply id="S4.E6.m1.2.2.2.2.1.1.cmml" xref="S4.E6.m1.2.2.2.2.1.1"><divide id="S4.E6.m1.2.2.2.2.1.1.3.cmml" xref="S4.E6.m1.2.2.2.2.1.1.3"></divide><apply id="S4.E6.m1.2.2.2.2.1.1.2.cmml" xref="S4.E6.m1.2.2.2.2.1.1.2"><times id="S4.E6.m1.2.2.2.2.1.1.2.3.cmml" xref="S4.E6.m1.2.2.2.2.1.1.2.3"></times><ci id="S4.E6.m1.2.2.2.2.1.1.2.4a.cmml" xref="S4.E6.m1.2.2.2.2.1.1.2.4"><mtext id="S4.E6.m1.2.2.2.2.1.1.2.4.cmml" xref="S4.E6.m1.2.2.2.2.1.1.2.4">sim</mtext></ci><interval closure="open" id="S4.E6.m1.2.2.2.2.1.1.2.2.3.cmml" xref="S4.E6.m1.2.2.2.2.1.1.2.2.2"><apply id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.cmml" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1"><times id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.1"></times><apply id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.2.1.cmml" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.2">superscript</csymbol><ci id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.2.2.cmml" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.2.2">𝑨</ci><ci id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.2.3.cmml" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.2.3">𝑐</ci></apply><apply id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3.cmml" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3.1.cmml" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3">subscript</csymbol><apply id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3.2.cmml" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3">superscript</csymbol><ci id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3.2.2">𝒐</ci><ci id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3.2.3a.cmml" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3.2.3"><mtext mathsize="70%" id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3.2.3.cmml" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3.2.3"> c</mtext></ci></apply><ci id="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3.3.cmml" xref="S4.E6.m1.2.2.2.2.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply><apply id="S4.E6.m1.2.2.2.2.1.1.2.2.2.2.cmml" xref="S4.E6.m1.2.2.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.2.2.1.1.2.2.2.2.1.cmml" xref="S4.E6.m1.2.2.2.2.1.1.2.2.2.2">subscript</csymbol><ci id="S4.E6.m1.2.2.2.2.1.1.2.2.2.2.2.cmml" xref="S4.E6.m1.2.2.2.2.1.1.2.2.2.2.2">𝒆</ci><ci id="S4.E6.m1.2.2.2.2.1.1.2.2.2.2.3.cmml" xref="S4.E6.m1.2.2.2.2.1.1.2.2.2.2.3">𝑢</ci></apply></interval></apply><ci id="S4.E6.m1.2.2.2.2.1.1.4.cmml" xref="S4.E6.m1.2.2.2.2.1.1.4">𝜏</ci></apply></apply><apply id="S4.E6.m1.4.4.4.cmml" xref="S4.E6.m1.4.4.4"><apply id="S4.E6.m1.4.4.4.3.cmml" xref="S4.E6.m1.4.4.4.3"><csymbol cd="ambiguous" id="S4.E6.m1.4.4.4.3.1.cmml" xref="S4.E6.m1.4.4.4.3">superscript</csymbol><apply id="S4.E6.m1.4.4.4.3.2.cmml" xref="S4.E6.m1.4.4.4.3"><csymbol cd="ambiguous" id="S4.E6.m1.4.4.4.3.2.1.cmml" xref="S4.E6.m1.4.4.4.3">subscript</csymbol><sum id="S4.E6.m1.4.4.4.3.2.2.cmml" xref="S4.E6.m1.4.4.4.3.2.2"></sum><apply id="S4.E6.m1.4.4.4.3.2.3.cmml" xref="S4.E6.m1.4.4.4.3.2.3"><csymbol cd="ambiguous" id="S4.E6.m1.4.4.4.3.2.3.1.cmml" xref="S4.E6.m1.4.4.4.3.2.3">superscript</csymbol><ci id="S4.E6.m1.4.4.4.3.2.3.2.cmml" xref="S4.E6.m1.4.4.4.3.2.3.2">𝑢</ci><ci id="S4.E6.m1.4.4.4.3.2.3.3.cmml" xref="S4.E6.m1.4.4.4.3.2.3.3">′</ci></apply></apply><ci id="S4.E6.m1.4.4.4.3.3.cmml" xref="S4.E6.m1.4.4.4.3.3">𝑈</ci></apply><apply id="S4.E6.m1.4.4.4.2.2.cmml" xref="S4.E6.m1.4.4.4.2.1"><exp id="S4.E6.m1.3.3.3.1.cmml" xref="S4.E6.m1.3.3.3.1"></exp><apply id="S4.E6.m1.4.4.4.2.1.1.1.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1"><divide id="S4.E6.m1.4.4.4.2.1.1.1.3.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.3"></divide><apply id="S4.E6.m1.4.4.4.2.1.1.1.2.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.2"><times id="S4.E6.m1.4.4.4.2.1.1.1.2.3.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.2.3"></times><ci id="S4.E6.m1.4.4.4.2.1.1.1.2.4a.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.2.4"><mtext id="S4.E6.m1.4.4.4.2.1.1.1.2.4.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.2.4">sim</mtext></ci><interval closure="open" id="S4.E6.m1.4.4.4.2.1.1.1.2.2.3.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.2.2.2"><apply id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1"><times id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.1"></times><apply id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.2.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.2.2">𝑨</ci><ci id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.2.3">𝑐</ci></apply><apply id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3">subscript</csymbol><apply id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3.2.2">𝒐</ci><ci id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3.2.3a.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3.2.3"><mtext mathsize="70%" id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3.2.3.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3.2.3"> c</mtext></ci></apply><ci id="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply><apply id="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.1.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2">subscript</csymbol><ci id="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.2.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.2">𝒆</ci><apply id="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.3.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.3"><csymbol cd="ambiguous" id="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.3.1.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.3">superscript</csymbol><ci id="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.3.2.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.3.2">𝑢</ci><ci id="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.3.3.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.2.2.2.2.3.3">′</ci></apply></apply></interval></apply><ci id="S4.E6.m1.4.4.4.2.1.1.1.4.cmml" xref="S4.E6.m1.4.4.4.2.1.1.1.4">𝜏</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m1.5c">\mathcal{L}_{\text{ c}}=\frac{1}{T_{m}}\sum_{t\in T_{m}}\log{\frac{\exp\left(\text{sim}\left(\bm{A}^{c}\bm{o}^{\text{ c}}_{t},\bm{e}_{u}\right)/\tau\right)}{\sum_{u^{\prime}}^{U}\exp\left(\text{sim}\left(\bm{A}^{c}\bm{o}^{\text{ c}}_{t},\bm{e}_{u^{\prime}}\right)/\tau\right)}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.SSS3.p2.9" class="ltx_p">where <math id="S4.SS1.SSS3.p2.4.m1.1" class="ltx_Math" alttext="\bm{A}^{c}" display="inline"><semantics id="S4.SS1.SSS3.p2.4.m1.1a"><msup id="S4.SS1.SSS3.p2.4.m1.1.1" xref="S4.SS1.SSS3.p2.4.m1.1.1.cmml"><mi id="S4.SS1.SSS3.p2.4.m1.1.1.2" xref="S4.SS1.SSS3.p2.4.m1.1.1.2.cmml">𝑨</mi><mi id="S4.SS1.SSS3.p2.4.m1.1.1.3" xref="S4.SS1.SSS3.p2.4.m1.1.1.3.cmml">c</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.4.m1.1b"><apply id="S4.SS1.SSS3.p2.4.m1.1.1.cmml" xref="S4.SS1.SSS3.p2.4.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p2.4.m1.1.1.1.cmml" xref="S4.SS1.SSS3.p2.4.m1.1.1">superscript</csymbol><ci id="S4.SS1.SSS3.p2.4.m1.1.1.2.cmml" xref="S4.SS1.SSS3.p2.4.m1.1.1.2">𝑨</ci><ci id="S4.SS1.SSS3.p2.4.m1.1.1.3.cmml" xref="S4.SS1.SSS3.p2.4.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.4.m1.1c">\bm{A}^{c}</annotation></semantics></math> is a projection matrix, <math id="S4.SS1.SSS3.p2.5.m2.4" class="ltx_Math" alttext="\bm{O}^{\text{ c}}\!=\!\left[\bm{o}^{\text{ c}}_{1},\bm{o}^{\text{ c}}_{2},\cdots,\bm{o}^{\text{ c}}_{T}\right]" display="inline"><semantics id="S4.SS1.SSS3.p2.5.m2.4a"><mrow id="S4.SS1.SSS3.p2.5.m2.4.4" xref="S4.SS1.SSS3.p2.5.m2.4.4.cmml"><msup id="S4.SS1.SSS3.p2.5.m2.4.4.5" xref="S4.SS1.SSS3.p2.5.m2.4.4.5.cmml"><mi id="S4.SS1.SSS3.p2.5.m2.4.4.5.2" xref="S4.SS1.SSS3.p2.5.m2.4.4.5.2.cmml">𝑶</mi><mtext id="S4.SS1.SSS3.p2.5.m2.4.4.5.3" xref="S4.SS1.SSS3.p2.5.m2.4.4.5.3a.cmml"> c</mtext></msup><mo lspace="0.108em" rspace="0.108em" id="S4.SS1.SSS3.p2.5.m2.4.4.4" xref="S4.SS1.SSS3.p2.5.m2.4.4.4.cmml">=</mo><mrow id="S4.SS1.SSS3.p2.5.m2.4.4.3.3" xref="S4.SS1.SSS3.p2.5.m2.4.4.3.4.cmml"><mo id="S4.SS1.SSS3.p2.5.m2.4.4.3.3.4" xref="S4.SS1.SSS3.p2.5.m2.4.4.3.4.cmml">[</mo><msubsup id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.cmml"><mi id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.2.2" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.2.2.cmml">𝒐</mi><mn id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.3" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.3.cmml">1</mn><mtext id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.2.3" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.2.3a.cmml"> c</mtext></msubsup><mo id="S4.SS1.SSS3.p2.5.m2.4.4.3.3.5" xref="S4.SS1.SSS3.p2.5.m2.4.4.3.4.cmml">,</mo><msubsup id="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2" xref="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2.cmml"><mi id="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2.2.2" xref="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2.2.2.cmml">𝒐</mi><mn id="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2.3" xref="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2.3.cmml">2</mn><mtext id="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2.2.3" xref="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2.2.3a.cmml"> c</mtext></msubsup><mo id="S4.SS1.SSS3.p2.5.m2.4.4.3.3.6" xref="S4.SS1.SSS3.p2.5.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S4.SS1.SSS3.p2.5.m2.1.1" xref="S4.SS1.SSS3.p2.5.m2.1.1.cmml">⋯</mi><mo id="S4.SS1.SSS3.p2.5.m2.4.4.3.3.7" xref="S4.SS1.SSS3.p2.5.m2.4.4.3.4.cmml">,</mo><msubsup id="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3" xref="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3.cmml"><mi id="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3.2.2" xref="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3.2.2.cmml">𝒐</mi><mi id="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3.3" xref="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3.3.cmml">T</mi><mtext id="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3.2.3" xref="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3.2.3a.cmml"> c</mtext></msubsup><mo id="S4.SS1.SSS3.p2.5.m2.4.4.3.3.8" xref="S4.SS1.SSS3.p2.5.m2.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.5.m2.4b"><apply id="S4.SS1.SSS3.p2.5.m2.4.4.cmml" xref="S4.SS1.SSS3.p2.5.m2.4.4"><eq id="S4.SS1.SSS3.p2.5.m2.4.4.4.cmml" xref="S4.SS1.SSS3.p2.5.m2.4.4.4"></eq><apply id="S4.SS1.SSS3.p2.5.m2.4.4.5.cmml" xref="S4.SS1.SSS3.p2.5.m2.4.4.5"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p2.5.m2.4.4.5.1.cmml" xref="S4.SS1.SSS3.p2.5.m2.4.4.5">superscript</csymbol><ci id="S4.SS1.SSS3.p2.5.m2.4.4.5.2.cmml" xref="S4.SS1.SSS3.p2.5.m2.4.4.5.2">𝑶</ci><ci id="S4.SS1.SSS3.p2.5.m2.4.4.5.3a.cmml" xref="S4.SS1.SSS3.p2.5.m2.4.4.5.3"><mtext mathsize="70%" id="S4.SS1.SSS3.p2.5.m2.4.4.5.3.cmml" xref="S4.SS1.SSS3.p2.5.m2.4.4.5.3"> c</mtext></ci></apply><list id="S4.SS1.SSS3.p2.5.m2.4.4.3.4.cmml" xref="S4.SS1.SSS3.p2.5.m2.4.4.3.3"><apply id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.1.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1">subscript</csymbol><apply id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.2.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.2.1.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1">superscript</csymbol><ci id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.2.2.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.2.2">𝒐</ci><ci id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.2.3a.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.2.3"><mtext mathsize="70%" id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.2.3.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.2.3"> c</mtext></ci></apply><cn type="integer" id="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.3.cmml" xref="S4.SS1.SSS3.p2.5.m2.2.2.1.1.1.3">1</cn></apply><apply id="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2.cmml" xref="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2.1.cmml" xref="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2">subscript</csymbol><apply id="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2.2.cmml" xref="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2.2.1.cmml" xref="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2">superscript</csymbol><ci id="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2.2.2.cmml" xref="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2.2.2">𝒐</ci><ci id="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2.2.3a.cmml" xref="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2.2.3"><mtext mathsize="70%" id="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2.2.3.cmml" xref="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2.2.3"> c</mtext></ci></apply><cn type="integer" id="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2.3.cmml" xref="S4.SS1.SSS3.p2.5.m2.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS1.SSS3.p2.5.m2.1.1.cmml" xref="S4.SS1.SSS3.p2.5.m2.1.1">⋯</ci><apply id="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3.cmml" xref="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3.1.cmml" xref="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3">subscript</csymbol><apply id="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3.2.cmml" xref="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3.2.1.cmml" xref="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3">superscript</csymbol><ci id="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3.2.2.cmml" xref="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3.2.2">𝒐</ci><ci id="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3.2.3a.cmml" xref="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3.2.3"><mtext mathsize="70%" id="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3.2.3.cmml" xref="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3.2.3"> c</mtext></ci></apply><ci id="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3.3.cmml" xref="S4.SS1.SSS3.p2.5.m2.4.4.3.3.3.3">𝑇</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.5.m2.4c">\bm{O}^{\text{ c}}\!=\!\left[\bm{o}^{\text{ c}}_{1},\bm{o}^{\text{ c}}_{2},\cdots,\bm{o}^{\text{ c}}_{T}\right]</annotation></semantics></math> is the output of last-layer Transformer, <math id="S4.SS1.SSS3.p2.6.m3.1" class="ltx_Math" alttext="\bm{e}_{u}" display="inline"><semantics id="S4.SS1.SSS3.p2.6.m3.1a"><msub id="S4.SS1.SSS3.p2.6.m3.1.1" xref="S4.SS1.SSS3.p2.6.m3.1.1.cmml"><mi id="S4.SS1.SSS3.p2.6.m3.1.1.2" xref="S4.SS1.SSS3.p2.6.m3.1.1.2.cmml">𝒆</mi><mi id="S4.SS1.SSS3.p2.6.m3.1.1.3" xref="S4.SS1.SSS3.p2.6.m3.1.1.3.cmml">u</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.6.m3.1b"><apply id="S4.SS1.SSS3.p2.6.m3.1.1.cmml" xref="S4.SS1.SSS3.p2.6.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p2.6.m3.1.1.1.cmml" xref="S4.SS1.SSS3.p2.6.m3.1.1">subscript</csymbol><ci id="S4.SS1.SSS3.p2.6.m3.1.1.2.cmml" xref="S4.SS1.SSS3.p2.6.m3.1.1.2">𝒆</ci><ci id="S4.SS1.SSS3.p2.6.m3.1.1.3.cmml" xref="S4.SS1.SSS3.p2.6.m3.1.1.3">𝑢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.6.m3.1c">\bm{e}_{u}</annotation></semantics></math> is the embedding for the K-means unit <math id="S4.SS1.SSS3.p2.7.m4.1" class="ltx_Math" alttext="u" display="inline"><semantics id="S4.SS1.SSS3.p2.7.m4.1a"><mi id="S4.SS1.SSS3.p2.7.m4.1.1" xref="S4.SS1.SSS3.p2.7.m4.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.7.m4.1b"><ci id="S4.SS1.SSS3.p2.7.m4.1.1.cmml" xref="S4.SS1.SSS3.p2.7.m4.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.7.m4.1c">u</annotation></semantics></math>, and <math id="S4.SS1.SSS3.p2.8.m5.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S4.SS1.SSS3.p2.8.m5.1a"><mi id="S4.SS1.SSS3.p2.8.m5.1.1" xref="S4.SS1.SSS3.p2.8.m5.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.8.m5.1b"><ci id="S4.SS1.SSS3.p2.8.m5.1.1.cmml" xref="S4.SS1.SSS3.p2.8.m5.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.8.m5.1c">\tau</annotation></semantics></math> scales the logit, set to 0.1. Similar to the speaker loss in speaker information modeling, the content loss <math id="S4.SS1.SSS3.p2.9.m6.1" class="ltx_Math" alttext="\mathcal{L}_{\text{ c}}" display="inline"><semantics id="S4.SS1.SSS3.p2.9.m6.1a"><msub id="S4.SS1.SSS3.p2.9.m6.1.1" xref="S4.SS1.SSS3.p2.9.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS3.p2.9.m6.1.1.2" xref="S4.SS1.SSS3.p2.9.m6.1.1.2.cmml">ℒ</mi><mtext id="S4.SS1.SSS3.p2.9.m6.1.1.3" xref="S4.SS1.SSS3.p2.9.m6.1.1.3a.cmml"> c</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p2.9.m6.1b"><apply id="S4.SS1.SSS3.p2.9.m6.1.1.cmml" xref="S4.SS1.SSS3.p2.9.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS3.p2.9.m6.1.1.1.cmml" xref="S4.SS1.SSS3.p2.9.m6.1.1">subscript</csymbol><ci id="S4.SS1.SSS3.p2.9.m6.1.1.2.cmml" xref="S4.SS1.SSS3.p2.9.m6.1.1.2">ℒ</ci><ci id="S4.SS1.SSS3.p2.9.m6.1.1.3a.cmml" xref="S4.SS1.SSS3.p2.9.m6.1.1.3"><mtext mathsize="70%" id="S4.SS1.SSS3.p2.9.m6.1.1.3.cmml" xref="S4.SS1.SSS3.p2.9.m6.1.1.3"> c</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p2.9.m6.1c">\mathcal{L}_{\text{ c}}</annotation></semantics></math> is only applied over the masked frames.</p>
</div>
</section>
<section id="S4.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS4.5.1.1" class="ltx_text">IV-A</span>4 </span>Loss Function of Pre-training</h4>

<div id="S4.SS1.SSS4.p1" class="ltx_para">
<p id="S4.SS1.SSS4.p1.8" class="ltx_p">As mentioned in section <a href="#S4.SS1" title="IV-A Progressive Residual Extraction ‣ IV Progressive Residual Extraction Based Pre-training ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>, our progressive residual extraction method works effectively only when all modules are trained jointly. Furthermore, the speaker extractor needs to be co-supervised by a pre-trained speaker-teacher model. Our <span id="S4.SS1.SSS4.p1.8.1" class="ltx_text ltx_font_smallcaps">ProgRE</span> model is pre-trained using the following multi-task loss function:</p>
<table id="S4.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E7.m1.1" class="ltx_Math" alttext="\mathcal{L}=\lambda_{\text{ f}}\cdot\mathcal{L}_{\text{ f}}+\lambda_{\text{ s}}\cdot\mathcal{L}_{\text{ s}}+\lambda_{\text{ c}}\cdot\mathcal{L}_{\text{ c}}," display="block"><semantics id="S4.E7.m1.1a"><mrow id="S4.E7.m1.1.1.1" xref="S4.E7.m1.1.1.1.1.cmml"><mrow id="S4.E7.m1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E7.m1.1.1.1.1.2" xref="S4.E7.m1.1.1.1.1.2.cmml">ℒ</mi><mo id="S4.E7.m1.1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.1.cmml">=</mo><mrow id="S4.E7.m1.1.1.1.1.3" xref="S4.E7.m1.1.1.1.1.3.cmml"><mrow id="S4.E7.m1.1.1.1.1.3.2" xref="S4.E7.m1.1.1.1.1.3.2.cmml"><msub id="S4.E7.m1.1.1.1.1.3.2.2" xref="S4.E7.m1.1.1.1.1.3.2.2.cmml"><mi id="S4.E7.m1.1.1.1.1.3.2.2.2" xref="S4.E7.m1.1.1.1.1.3.2.2.2.cmml">λ</mi><mtext id="S4.E7.m1.1.1.1.1.3.2.2.3" xref="S4.E7.m1.1.1.1.1.3.2.2.3a.cmml"> f</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="S4.E7.m1.1.1.1.1.3.2.1" xref="S4.E7.m1.1.1.1.1.3.2.1.cmml">⋅</mo><msub id="S4.E7.m1.1.1.1.1.3.2.3" xref="S4.E7.m1.1.1.1.1.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E7.m1.1.1.1.1.3.2.3.2" xref="S4.E7.m1.1.1.1.1.3.2.3.2.cmml">ℒ</mi><mtext id="S4.E7.m1.1.1.1.1.3.2.3.3" xref="S4.E7.m1.1.1.1.1.3.2.3.3a.cmml"> f</mtext></msub></mrow><mo id="S4.E7.m1.1.1.1.1.3.1" xref="S4.E7.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S4.E7.m1.1.1.1.1.3.3" xref="S4.E7.m1.1.1.1.1.3.3.cmml"><msub id="S4.E7.m1.1.1.1.1.3.3.2" xref="S4.E7.m1.1.1.1.1.3.3.2.cmml"><mi id="S4.E7.m1.1.1.1.1.3.3.2.2" xref="S4.E7.m1.1.1.1.1.3.3.2.2.cmml">λ</mi><mtext id="S4.E7.m1.1.1.1.1.3.3.2.3" xref="S4.E7.m1.1.1.1.1.3.3.2.3a.cmml"> s</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="S4.E7.m1.1.1.1.1.3.3.1" xref="S4.E7.m1.1.1.1.1.3.3.1.cmml">⋅</mo><msub id="S4.E7.m1.1.1.1.1.3.3.3" xref="S4.E7.m1.1.1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E7.m1.1.1.1.1.3.3.3.2" xref="S4.E7.m1.1.1.1.1.3.3.3.2.cmml">ℒ</mi><mtext id="S4.E7.m1.1.1.1.1.3.3.3.3" xref="S4.E7.m1.1.1.1.1.3.3.3.3a.cmml"> s</mtext></msub></mrow><mo id="S4.E7.m1.1.1.1.1.3.1a" xref="S4.E7.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S4.E7.m1.1.1.1.1.3.4" xref="S4.E7.m1.1.1.1.1.3.4.cmml"><msub id="S4.E7.m1.1.1.1.1.3.4.2" xref="S4.E7.m1.1.1.1.1.3.4.2.cmml"><mi id="S4.E7.m1.1.1.1.1.3.4.2.2" xref="S4.E7.m1.1.1.1.1.3.4.2.2.cmml">λ</mi><mtext id="S4.E7.m1.1.1.1.1.3.4.2.3" xref="S4.E7.m1.1.1.1.1.3.4.2.3a.cmml"> c</mtext></msub><mo lspace="0.222em" rspace="0.222em" id="S4.E7.m1.1.1.1.1.3.4.1" xref="S4.E7.m1.1.1.1.1.3.4.1.cmml">⋅</mo><msub id="S4.E7.m1.1.1.1.1.3.4.3" xref="S4.E7.m1.1.1.1.1.3.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E7.m1.1.1.1.1.3.4.3.2" xref="S4.E7.m1.1.1.1.1.3.4.3.2.cmml">ℒ</mi><mtext id="S4.E7.m1.1.1.1.1.3.4.3.3" xref="S4.E7.m1.1.1.1.1.3.4.3.3a.cmml"> c</mtext></msub></mrow></mrow></mrow><mo id="S4.E7.m1.1.1.1.2" xref="S4.E7.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E7.m1.1b"><apply id="S4.E7.m1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1"><eq id="S4.E7.m1.1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1.1.1"></eq><ci id="S4.E7.m1.1.1.1.1.2.cmml" xref="S4.E7.m1.1.1.1.1.2">ℒ</ci><apply id="S4.E7.m1.1.1.1.1.3.cmml" xref="S4.E7.m1.1.1.1.1.3"><plus id="S4.E7.m1.1.1.1.1.3.1.cmml" xref="S4.E7.m1.1.1.1.1.3.1"></plus><apply id="S4.E7.m1.1.1.1.1.3.2.cmml" xref="S4.E7.m1.1.1.1.1.3.2"><ci id="S4.E7.m1.1.1.1.1.3.2.1.cmml" xref="S4.E7.m1.1.1.1.1.3.2.1">⋅</ci><apply id="S4.E7.m1.1.1.1.1.3.2.2.cmml" xref="S4.E7.m1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.3.2.2.1.cmml" xref="S4.E7.m1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S4.E7.m1.1.1.1.1.3.2.2.2.cmml" xref="S4.E7.m1.1.1.1.1.3.2.2.2">𝜆</ci><ci id="S4.E7.m1.1.1.1.1.3.2.2.3a.cmml" xref="S4.E7.m1.1.1.1.1.3.2.2.3"><mtext mathsize="70%" id="S4.E7.m1.1.1.1.1.3.2.2.3.cmml" xref="S4.E7.m1.1.1.1.1.3.2.2.3"> f</mtext></ci></apply><apply id="S4.E7.m1.1.1.1.1.3.2.3.cmml" xref="S4.E7.m1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.3.2.3.1.cmml" xref="S4.E7.m1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S4.E7.m1.1.1.1.1.3.2.3.2.cmml" xref="S4.E7.m1.1.1.1.1.3.2.3.2">ℒ</ci><ci id="S4.E7.m1.1.1.1.1.3.2.3.3a.cmml" xref="S4.E7.m1.1.1.1.1.3.2.3.3"><mtext mathsize="70%" id="S4.E7.m1.1.1.1.1.3.2.3.3.cmml" xref="S4.E7.m1.1.1.1.1.3.2.3.3"> f</mtext></ci></apply></apply><apply id="S4.E7.m1.1.1.1.1.3.3.cmml" xref="S4.E7.m1.1.1.1.1.3.3"><ci id="S4.E7.m1.1.1.1.1.3.3.1.cmml" xref="S4.E7.m1.1.1.1.1.3.3.1">⋅</ci><apply id="S4.E7.m1.1.1.1.1.3.3.2.cmml" xref="S4.E7.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.3.3.2.1.cmml" xref="S4.E7.m1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S4.E7.m1.1.1.1.1.3.3.2.2.cmml" xref="S4.E7.m1.1.1.1.1.3.3.2.2">𝜆</ci><ci id="S4.E7.m1.1.1.1.1.3.3.2.3a.cmml" xref="S4.E7.m1.1.1.1.1.3.3.2.3"><mtext mathsize="70%" id="S4.E7.m1.1.1.1.1.3.3.2.3.cmml" xref="S4.E7.m1.1.1.1.1.3.3.2.3"> s</mtext></ci></apply><apply id="S4.E7.m1.1.1.1.1.3.3.3.cmml" xref="S4.E7.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.3.3.3.1.cmml" xref="S4.E7.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S4.E7.m1.1.1.1.1.3.3.3.2.cmml" xref="S4.E7.m1.1.1.1.1.3.3.3.2">ℒ</ci><ci id="S4.E7.m1.1.1.1.1.3.3.3.3a.cmml" xref="S4.E7.m1.1.1.1.1.3.3.3.3"><mtext mathsize="70%" id="S4.E7.m1.1.1.1.1.3.3.3.3.cmml" xref="S4.E7.m1.1.1.1.1.3.3.3.3"> s</mtext></ci></apply></apply><apply id="S4.E7.m1.1.1.1.1.3.4.cmml" xref="S4.E7.m1.1.1.1.1.3.4"><ci id="S4.E7.m1.1.1.1.1.3.4.1.cmml" xref="S4.E7.m1.1.1.1.1.3.4.1">⋅</ci><apply id="S4.E7.m1.1.1.1.1.3.4.2.cmml" xref="S4.E7.m1.1.1.1.1.3.4.2"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.3.4.2.1.cmml" xref="S4.E7.m1.1.1.1.1.3.4.2">subscript</csymbol><ci id="S4.E7.m1.1.1.1.1.3.4.2.2.cmml" xref="S4.E7.m1.1.1.1.1.3.4.2.2">𝜆</ci><ci id="S4.E7.m1.1.1.1.1.3.4.2.3a.cmml" xref="S4.E7.m1.1.1.1.1.3.4.2.3"><mtext mathsize="70%" id="S4.E7.m1.1.1.1.1.3.4.2.3.cmml" xref="S4.E7.m1.1.1.1.1.3.4.2.3"> c</mtext></ci></apply><apply id="S4.E7.m1.1.1.1.1.3.4.3.cmml" xref="S4.E7.m1.1.1.1.1.3.4.3"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.3.4.3.1.cmml" xref="S4.E7.m1.1.1.1.1.3.4.3">subscript</csymbol><ci id="S4.E7.m1.1.1.1.1.3.4.3.2.cmml" xref="S4.E7.m1.1.1.1.1.3.4.3.2">ℒ</ci><ci id="S4.E7.m1.1.1.1.1.3.4.3.3a.cmml" xref="S4.E7.m1.1.1.1.1.3.4.3.3"><mtext mathsize="70%" id="S4.E7.m1.1.1.1.1.3.4.3.3.cmml" xref="S4.E7.m1.1.1.1.1.3.4.3.3"> c</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E7.m1.1c">\mathcal{L}=\lambda_{\text{ f}}\cdot\mathcal{L}_{\text{ f}}+\lambda_{\text{ s}}\cdot\mathcal{L}_{\text{ s}}+\lambda_{\text{ c}}\cdot\mathcal{L}_{\text{ c}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.SSS4.p1.7" class="ltx_p">where <math id="S4.SS1.SSS4.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\text{ f}}" display="inline"><semantics id="S4.SS1.SSS4.p1.1.m1.1a"><msub id="S4.SS1.SSS4.p1.1.m1.1.1" xref="S4.SS1.SSS4.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS4.p1.1.m1.1.1.2" xref="S4.SS1.SSS4.p1.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S4.SS1.SSS4.p1.1.m1.1.1.3" xref="S4.SS1.SSS4.p1.1.m1.1.1.3a.cmml"> f</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS4.p1.1.m1.1b"><apply id="S4.SS1.SSS4.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS4.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS4.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS4.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS4.p1.1.m1.1.1.2">ℒ</ci><ci id="S4.SS1.SSS4.p1.1.m1.1.1.3a.cmml" xref="S4.SS1.SSS4.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S4.SS1.SSS4.p1.1.m1.1.1.3.cmml" xref="S4.SS1.SSS4.p1.1.m1.1.1.3"> f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS4.p1.1.m1.1c">\mathcal{L}_{\text{ f}}</annotation></semantics></math> denotes the mean square error of <math id="S4.SS1.SSS4.p1.2.m2.1" class="ltx_Math" alttext="\bm{X}_{f}" display="inline"><semantics id="S4.SS1.SSS4.p1.2.m2.1a"><msub id="S4.SS1.SSS4.p1.2.m2.1.1" xref="S4.SS1.SSS4.p1.2.m2.1.1.cmml"><mi id="S4.SS1.SSS4.p1.2.m2.1.1.2" xref="S4.SS1.SSS4.p1.2.m2.1.1.2.cmml">𝑿</mi><mi id="S4.SS1.SSS4.p1.2.m2.1.1.3" xref="S4.SS1.SSS4.p1.2.m2.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS4.p1.2.m2.1b"><apply id="S4.SS1.SSS4.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS4.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS4.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.SSS4.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS4.p1.2.m2.1.1.2">𝑿</ci><ci id="S4.SS1.SSS4.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS4.p1.2.m2.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS4.p1.2.m2.1c">\bm{X}_{f}</annotation></semantics></math>. <math id="S4.SS1.SSS4.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{L}_{\text{ s}}" display="inline"><semantics id="S4.SS1.SSS4.p1.3.m3.1a"><msub id="S4.SS1.SSS4.p1.3.m3.1.1" xref="S4.SS1.SSS4.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS4.p1.3.m3.1.1.2" xref="S4.SS1.SSS4.p1.3.m3.1.1.2.cmml">ℒ</mi><mtext id="S4.SS1.SSS4.p1.3.m3.1.1.3" xref="S4.SS1.SSS4.p1.3.m3.1.1.3a.cmml"> s</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS4.p1.3.m3.1b"><apply id="S4.SS1.SSS4.p1.3.m3.1.1.cmml" xref="S4.SS1.SSS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS4.p1.3.m3.1.1.1.cmml" xref="S4.SS1.SSS4.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.SSS4.p1.3.m3.1.1.2.cmml" xref="S4.SS1.SSS4.p1.3.m3.1.1.2">ℒ</ci><ci id="S4.SS1.SSS4.p1.3.m3.1.1.3a.cmml" xref="S4.SS1.SSS4.p1.3.m3.1.1.3"><mtext mathsize="70%" id="S4.SS1.SSS4.p1.3.m3.1.1.3.cmml" xref="S4.SS1.SSS4.p1.3.m3.1.1.3"> s</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS4.p1.3.m3.1c">\mathcal{L}_{\text{ s}}</annotation></semantics></math> and <math id="S4.SS1.SSS4.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{L}_{\text{ c}}" display="inline"><semantics id="S4.SS1.SSS4.p1.4.m4.1a"><msub id="S4.SS1.SSS4.p1.4.m4.1.1" xref="S4.SS1.SSS4.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.SSS4.p1.4.m4.1.1.2" xref="S4.SS1.SSS4.p1.4.m4.1.1.2.cmml">ℒ</mi><mtext id="S4.SS1.SSS4.p1.4.m4.1.1.3" xref="S4.SS1.SSS4.p1.4.m4.1.1.3a.cmml"> c</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS4.p1.4.m4.1b"><apply id="S4.SS1.SSS4.p1.4.m4.1.1.cmml" xref="S4.SS1.SSS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS4.p1.4.m4.1.1.1.cmml" xref="S4.SS1.SSS4.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.SSS4.p1.4.m4.1.1.2.cmml" xref="S4.SS1.SSS4.p1.4.m4.1.1.2">ℒ</ci><ci id="S4.SS1.SSS4.p1.4.m4.1.1.3a.cmml" xref="S4.SS1.SSS4.p1.4.m4.1.1.3"><mtext mathsize="70%" id="S4.SS1.SSS4.p1.4.m4.1.1.3.cmml" xref="S4.SS1.SSS4.p1.4.m4.1.1.3"> c</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS4.p1.4.m4.1c">\mathcal{L}_{\text{ c}}</annotation></semantics></math> represent the losses associated with speaker and content modeling, respectively, as described in equation (<a href="#S4.E4" title="In IV-A2 Speaker Information Modeling ‣ IV-A Progressive Residual Extraction ‣ IV Progressive Residual Extraction Based Pre-training ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) and (<a href="#S4.E6" title="In IV-A3 Content Information Modeling ‣ IV-A Progressive Residual Extraction ‣ IV Progressive Residual Extraction Based Pre-training ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). The hyper-parameters <math id="S4.SS1.SSS4.p1.5.m5.1" class="ltx_Math" alttext="\lambda_{\text{ f}}" display="inline"><semantics id="S4.SS1.SSS4.p1.5.m5.1a"><msub id="S4.SS1.SSS4.p1.5.m5.1.1" xref="S4.SS1.SSS4.p1.5.m5.1.1.cmml"><mi id="S4.SS1.SSS4.p1.5.m5.1.1.2" xref="S4.SS1.SSS4.p1.5.m5.1.1.2.cmml">λ</mi><mtext id="S4.SS1.SSS4.p1.5.m5.1.1.3" xref="S4.SS1.SSS4.p1.5.m5.1.1.3a.cmml"> f</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS4.p1.5.m5.1b"><apply id="S4.SS1.SSS4.p1.5.m5.1.1.cmml" xref="S4.SS1.SSS4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS4.p1.5.m5.1.1.1.cmml" xref="S4.SS1.SSS4.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS1.SSS4.p1.5.m5.1.1.2.cmml" xref="S4.SS1.SSS4.p1.5.m5.1.1.2">𝜆</ci><ci id="S4.SS1.SSS4.p1.5.m5.1.1.3a.cmml" xref="S4.SS1.SSS4.p1.5.m5.1.1.3"><mtext mathsize="70%" id="S4.SS1.SSS4.p1.5.m5.1.1.3.cmml" xref="S4.SS1.SSS4.p1.5.m5.1.1.3"> f</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS4.p1.5.m5.1c">\lambda_{\text{ f}}</annotation></semantics></math>, <math id="S4.SS1.SSS4.p1.6.m6.1" class="ltx_Math" alttext="\lambda_{\text{ s}}" display="inline"><semantics id="S4.SS1.SSS4.p1.6.m6.1a"><msub id="S4.SS1.SSS4.p1.6.m6.1.1" xref="S4.SS1.SSS4.p1.6.m6.1.1.cmml"><mi id="S4.SS1.SSS4.p1.6.m6.1.1.2" xref="S4.SS1.SSS4.p1.6.m6.1.1.2.cmml">λ</mi><mtext id="S4.SS1.SSS4.p1.6.m6.1.1.3" xref="S4.SS1.SSS4.p1.6.m6.1.1.3a.cmml"> s</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS4.p1.6.m6.1b"><apply id="S4.SS1.SSS4.p1.6.m6.1.1.cmml" xref="S4.SS1.SSS4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS4.p1.6.m6.1.1.1.cmml" xref="S4.SS1.SSS4.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS1.SSS4.p1.6.m6.1.1.2.cmml" xref="S4.SS1.SSS4.p1.6.m6.1.1.2">𝜆</ci><ci id="S4.SS1.SSS4.p1.6.m6.1.1.3a.cmml" xref="S4.SS1.SSS4.p1.6.m6.1.1.3"><mtext mathsize="70%" id="S4.SS1.SSS4.p1.6.m6.1.1.3.cmml" xref="S4.SS1.SSS4.p1.6.m6.1.1.3"> s</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS4.p1.6.m6.1c">\lambda_{\text{ s}}</annotation></semantics></math>, and <math id="S4.SS1.SSS4.p1.7.m7.1" class="ltx_Math" alttext="\lambda_{\text{ c}}" display="inline"><semantics id="S4.SS1.SSS4.p1.7.m7.1a"><msub id="S4.SS1.SSS4.p1.7.m7.1.1" xref="S4.SS1.SSS4.p1.7.m7.1.1.cmml"><mi id="S4.SS1.SSS4.p1.7.m7.1.1.2" xref="S4.SS1.SSS4.p1.7.m7.1.1.2.cmml">λ</mi><mtext id="S4.SS1.SSS4.p1.7.m7.1.1.3" xref="S4.SS1.SSS4.p1.7.m7.1.1.3a.cmml"> c</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS4.p1.7.m7.1b"><apply id="S4.SS1.SSS4.p1.7.m7.1.1.cmml" xref="S4.SS1.SSS4.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS4.p1.7.m7.1.1.1.cmml" xref="S4.SS1.SSS4.p1.7.m7.1.1">subscript</csymbol><ci id="S4.SS1.SSS4.p1.7.m7.1.1.2.cmml" xref="S4.SS1.SSS4.p1.7.m7.1.1.2">𝜆</ci><ci id="S4.SS1.SSS4.p1.7.m7.1.1.3a.cmml" xref="S4.SS1.SSS4.p1.7.m7.1.1.3"><mtext mathsize="70%" id="S4.SS1.SSS4.p1.7.m7.1.1.3.cmml" xref="S4.SS1.SSS4.p1.7.m7.1.1.3"> c</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS4.p1.7.m7.1c">\lambda_{\text{ c}}</annotation></semantics></math> for the three loss functions are set to 10.0, 1.0, and 1.0, respectively.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Fine-tuning</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">As introduced in Section <a href="#S3" title="III Problem Formulation ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, after pre-training, we utilize a weighted-sum mechanism for downstream fine-tuning, as depicted in Fig. <a href="#S3.F4" title="Figure 4 ‣ III Problem Formulation ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. All outputs of the hidden layers are weighted-sum with learnable weights as input to the downstream model. Due to the insertion of two specific task extractors, <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">ProgRE</span> utilizes the representations extracted by the pitch extractor, speaker extractor, and the outputs of Transformer layers (excluding the layers with inserted two extractors) for weighted-sum, as shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ III Problem Formulation ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. This approach enables us to use different weights to obtain representations suitable for various downstream tasks. Consequently, with a lightweight downstream model, we can achieve excellent performance on downstream tasks with a small amount of supervised data.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Experiments and Results</span>
</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Tasks and Datasets</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We pre-trained our model on LibriSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, WenetSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, and Multi-lingual Speech (MLS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. We conducted various fine-tuning experiments, including speech recognition (ASR), speaker identification (SID), speech enhancement (SE), emotion recognition (ER), and voice conversion (VC) to evaluate the model’s performance on content, speaker, intonation, and acoustic learning. These fine-tuning experiments utilized data from various datasets: LibriSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, VoxCeleb1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, Interactive Emotional Dyadic Motion Capture (IEMOCAP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, Voicebank-DEMAND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>, and the dataset of the Voice Conversion Challenge (VCC2020) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. All audio samples were sampled at 16 kHz.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">Pre-training</span>: We pre-trained two versions of <span id="S5.SS1.p2.1.2" class="ltx_text ltx_font_smallcaps">ProgRE</span> and HuBERT: Base and Large. For the Base model, we used 960 hours of LibriSpeech data for pre-training to ensure comparability with other open-source Base SSL models. For the Large model, we used a total of 84,500 hours of bilingual data in English and Chinese for pre-training. This bilingual dataset included 44,500 hours of MLS English data, 10,000 hours of Wenetspeech Chinese data, and 30,000 hours of Chinese speech data collected from the Internet. All pre-training data were used without labels.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold">Speech recognition fine-tuning</span>: The <span id="S5.SS1.p3.1.2" class="ltx_text ltx_font_italic">train-clean-100</span> and <span id="S5.SS1.p3.1.3" class="ltx_text ltx_font_italic">dev-clean</span> subsets of LibriSpeech were employed as the training and development datasets for ASR, respectively. The performance of the models was evaluated on the <span id="S5.SS1.p3.1.4" class="ltx_text ltx_font_italic">test-clean</span>, and <span id="S5.SS1.p3.1.5" class="ltx_text ltx_font_italic">test-other</span> subsets of LibriSpeech.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p"><span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold">Speaker identification fine-tuning</span>: We fine-tuned and evaluated the models on the VoxCeleb1 dataset for the SID task. VoxCeleb1 contains over 100,000 utterances from 1,251 celebrities, extracted from videos.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p"><span id="S5.SS1.p5.1.1" class="ltx_text ltx_font_bold">Speech enhancement fine-tuning</span>: We used the Voicebank-DEMAND dataset for SE. This dataset includes data from 28 speakers with various signal-to-noise ratio (SNR) levels: 15, 10, 5, and 0 dB. The test set consists of data from two additional speakers with 17.5, 12.5, 7.5, and 2.5 dB SNRs.</p>
</div>
<div id="S5.SS1.p6" class="ltx_para">
<p id="S5.SS1.p6.1" class="ltx_p"><span id="S5.SS1.p6.1.1" class="ltx_text ltx_font_bold">Speech emotion recognition fine-tuning</span>: We fine-tuned models on section 2 to 5 subsets of the IEMOCAP dataset for ER and evaluated their performance on the section 1 subset. The IEMOCAP dataset comprises approximately 12 hours of recordings encompassing various emotional expressions. Notably, IEMOCAP emphasizes the natural expression of emotions in conversations, where the emotional content of speech is closely tied to the spoken context.</p>
</div>
<div id="S5.SS1.p7" class="ltx_para">
<p id="S5.SS1.p7.1" class="ltx_p"><span id="S5.SS1.p7.1.1" class="ltx_text ltx_font_bold">Voice conversion fine-tuning</span>: Following the evaluation in SUPERB-SG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, we conducted the any-to-one voice conversion task of VCC2020, where TEF1 was chosen as the target speaker. The speaker model was directly trained on the target speaker training set.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Experimental Setup</span>
</h3>

<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS1.5.1.1" class="ltx_text">V-B</span>1 </span>Configuration of Models</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">To validate the effectiveness of our proposed <span id="S5.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_smallcaps">ProgRE</span>, we conducted comprehensive comparisons with some excellent self-supervised pre-training models as follows:</p>
</div>
<div id="S5.SS2.SSS1.p2" class="ltx_para">
<p id="S5.SS2.SSS1.p2.1" class="ltx_p"><span id="S5.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_bold">wav2vec 2.0</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>: wav2vec 2.0 is a self-supervised learning model that utilizes a quantization-based contrastive learning strategy to pretrain the encoder, distinguishing between positive samples (audio segments from the same utterance) and negative samples (segments from different utterances).</p>
</div>
<div id="S5.SS2.SSS1.p3" class="ltx_para">
<p id="S5.SS2.SSS1.p3.1" class="ltx_p"><span id="S5.SS2.SSS1.p3.1.1" class="ltx_text ltx_font_bold">WavLM</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>: WavLM simultaneously learns the BERT-like masked unit prediction and denoising during pre-training. It has shown SOTA performance on various downstream tasks.</p>
</div>
<div id="S5.SS2.SSS1.p4" class="ltx_para">
<p id="S5.SS2.SSS1.p4.1" class="ltx_p"><span id="S5.SS2.SSS1.p4.1.1" class="ltx_text ltx_font_bold">HuBERT</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>: HuBERT is a self-supervised speech representation learning approach that employs an offline clustering step to provide aligned target pseudo labels for a BERT-like prediction loss.</p>
</div>
<div id="S5.SS2.SSS1.p5" class="ltx_para">
<p id="S5.SS2.SSS1.p5.1" class="ltx_p"><span id="S5.SS2.SSS1.p5.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">ProgRE</span>: Our proposed progressive residual extraction based pre-training strategy is illustrated in Fig. <a href="#S3.F4" title="Figure 4 ‣ III Problem Formulation ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Two extractors are inserted into the encoder-style SSL backbone. The pitch extractor consists of three 256-channel convolutional layers with a kernel size of 5, a single-layer GRU with 256 cells, and a fully connected (FC) layer with hidden feature-dimension cells. The speaker extractor comprises a fully connected layer with 256 cells, an FAS layer with hidden feature-dimension cells, and another fully connected layer with hidden feature-dimension cells. The hidden feature dimension is 768 in the Base model and 1024 in the Large model.</p>
</div>
<div id="S5.SS2.SSS1.p6" class="ltx_para">
<p id="S5.SS2.SSS1.p6.2" class="ltx_p">We compared the Base and Large versions of the four models, keeping the parameter configurations of the main structures consistent. For the Base version model, the convolutional module consists of seven layers, each with 512 channels, with strides of <math id="S5.SS2.SSS1.p6.1.m1.7" class="ltx_Math" alttext="\left\{5,2,2,2,2,2,2\right\}" display="inline"><semantics id="S5.SS2.SSS1.p6.1.m1.7a"><mrow id="S5.SS2.SSS1.p6.1.m1.7.8.2" xref="S5.SS2.SSS1.p6.1.m1.7.8.1.cmml"><mo id="S5.SS2.SSS1.p6.1.m1.7.8.2.1" xref="S5.SS2.SSS1.p6.1.m1.7.8.1.cmml">{</mo><mn id="S5.SS2.SSS1.p6.1.m1.1.1" xref="S5.SS2.SSS1.p6.1.m1.1.1.cmml">5</mn><mo id="S5.SS2.SSS1.p6.1.m1.7.8.2.2" xref="S5.SS2.SSS1.p6.1.m1.7.8.1.cmml">,</mo><mn id="S5.SS2.SSS1.p6.1.m1.2.2" xref="S5.SS2.SSS1.p6.1.m1.2.2.cmml">2</mn><mo id="S5.SS2.SSS1.p6.1.m1.7.8.2.3" xref="S5.SS2.SSS1.p6.1.m1.7.8.1.cmml">,</mo><mn id="S5.SS2.SSS1.p6.1.m1.3.3" xref="S5.SS2.SSS1.p6.1.m1.3.3.cmml">2</mn><mo id="S5.SS2.SSS1.p6.1.m1.7.8.2.4" xref="S5.SS2.SSS1.p6.1.m1.7.8.1.cmml">,</mo><mn id="S5.SS2.SSS1.p6.1.m1.4.4" xref="S5.SS2.SSS1.p6.1.m1.4.4.cmml">2</mn><mo id="S5.SS2.SSS1.p6.1.m1.7.8.2.5" xref="S5.SS2.SSS1.p6.1.m1.7.8.1.cmml">,</mo><mn id="S5.SS2.SSS1.p6.1.m1.5.5" xref="S5.SS2.SSS1.p6.1.m1.5.5.cmml">2</mn><mo id="S5.SS2.SSS1.p6.1.m1.7.8.2.6" xref="S5.SS2.SSS1.p6.1.m1.7.8.1.cmml">,</mo><mn id="S5.SS2.SSS1.p6.1.m1.6.6" xref="S5.SS2.SSS1.p6.1.m1.6.6.cmml">2</mn><mo id="S5.SS2.SSS1.p6.1.m1.7.8.2.7" xref="S5.SS2.SSS1.p6.1.m1.7.8.1.cmml">,</mo><mn id="S5.SS2.SSS1.p6.1.m1.7.7" xref="S5.SS2.SSS1.p6.1.m1.7.7.cmml">2</mn><mo id="S5.SS2.SSS1.p6.1.m1.7.8.2.8" xref="S5.SS2.SSS1.p6.1.m1.7.8.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p6.1.m1.7b"><set id="S5.SS2.SSS1.p6.1.m1.7.8.1.cmml" xref="S5.SS2.SSS1.p6.1.m1.7.8.2"><cn type="integer" id="S5.SS2.SSS1.p6.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p6.1.m1.1.1">5</cn><cn type="integer" id="S5.SS2.SSS1.p6.1.m1.2.2.cmml" xref="S5.SS2.SSS1.p6.1.m1.2.2">2</cn><cn type="integer" id="S5.SS2.SSS1.p6.1.m1.3.3.cmml" xref="S5.SS2.SSS1.p6.1.m1.3.3">2</cn><cn type="integer" id="S5.SS2.SSS1.p6.1.m1.4.4.cmml" xref="S5.SS2.SSS1.p6.1.m1.4.4">2</cn><cn type="integer" id="S5.SS2.SSS1.p6.1.m1.5.5.cmml" xref="S5.SS2.SSS1.p6.1.m1.5.5">2</cn><cn type="integer" id="S5.SS2.SSS1.p6.1.m1.6.6.cmml" xref="S5.SS2.SSS1.p6.1.m1.6.6">2</cn><cn type="integer" id="S5.SS2.SSS1.p6.1.m1.7.7.cmml" xref="S5.SS2.SSS1.p6.1.m1.7.7">2</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p6.1.m1.7c">\left\{5,2,2,2,2,2,2\right\}</annotation></semantics></math> and kernels of <math id="S5.SS2.SSS1.p6.2.m2.7" class="ltx_Math" alttext="\left\{10,3,3,3,3,2,2\right\}" display="inline"><semantics id="S5.SS2.SSS1.p6.2.m2.7a"><mrow id="S5.SS2.SSS1.p6.2.m2.7.8.2" xref="S5.SS2.SSS1.p6.2.m2.7.8.1.cmml"><mo id="S5.SS2.SSS1.p6.2.m2.7.8.2.1" xref="S5.SS2.SSS1.p6.2.m2.7.8.1.cmml">{</mo><mn id="S5.SS2.SSS1.p6.2.m2.1.1" xref="S5.SS2.SSS1.p6.2.m2.1.1.cmml">10</mn><mo id="S5.SS2.SSS1.p6.2.m2.7.8.2.2" xref="S5.SS2.SSS1.p6.2.m2.7.8.1.cmml">,</mo><mn id="S5.SS2.SSS1.p6.2.m2.2.2" xref="S5.SS2.SSS1.p6.2.m2.2.2.cmml">3</mn><mo id="S5.SS2.SSS1.p6.2.m2.7.8.2.3" xref="S5.SS2.SSS1.p6.2.m2.7.8.1.cmml">,</mo><mn id="S5.SS2.SSS1.p6.2.m2.3.3" xref="S5.SS2.SSS1.p6.2.m2.3.3.cmml">3</mn><mo id="S5.SS2.SSS1.p6.2.m2.7.8.2.4" xref="S5.SS2.SSS1.p6.2.m2.7.8.1.cmml">,</mo><mn id="S5.SS2.SSS1.p6.2.m2.4.4" xref="S5.SS2.SSS1.p6.2.m2.4.4.cmml">3</mn><mo id="S5.SS2.SSS1.p6.2.m2.7.8.2.5" xref="S5.SS2.SSS1.p6.2.m2.7.8.1.cmml">,</mo><mn id="S5.SS2.SSS1.p6.2.m2.5.5" xref="S5.SS2.SSS1.p6.2.m2.5.5.cmml">3</mn><mo id="S5.SS2.SSS1.p6.2.m2.7.8.2.6" xref="S5.SS2.SSS1.p6.2.m2.7.8.1.cmml">,</mo><mn id="S5.SS2.SSS1.p6.2.m2.6.6" xref="S5.SS2.SSS1.p6.2.m2.6.6.cmml">2</mn><mo id="S5.SS2.SSS1.p6.2.m2.7.8.2.7" xref="S5.SS2.SSS1.p6.2.m2.7.8.1.cmml">,</mo><mn id="S5.SS2.SSS1.p6.2.m2.7.7" xref="S5.SS2.SSS1.p6.2.m2.7.7.cmml">2</mn><mo id="S5.SS2.SSS1.p6.2.m2.7.8.2.8" xref="S5.SS2.SSS1.p6.2.m2.7.8.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p6.2.m2.7b"><set id="S5.SS2.SSS1.p6.2.m2.7.8.1.cmml" xref="S5.SS2.SSS1.p6.2.m2.7.8.2"><cn type="integer" id="S5.SS2.SSS1.p6.2.m2.1.1.cmml" xref="S5.SS2.SSS1.p6.2.m2.1.1">10</cn><cn type="integer" id="S5.SS2.SSS1.p6.2.m2.2.2.cmml" xref="S5.SS2.SSS1.p6.2.m2.2.2">3</cn><cn type="integer" id="S5.SS2.SSS1.p6.2.m2.3.3.cmml" xref="S5.SS2.SSS1.p6.2.m2.3.3">3</cn><cn type="integer" id="S5.SS2.SSS1.p6.2.m2.4.4.cmml" xref="S5.SS2.SSS1.p6.2.m2.4.4">3</cn><cn type="integer" id="S5.SS2.SSS1.p6.2.m2.5.5.cmml" xref="S5.SS2.SSS1.p6.2.m2.5.5">3</cn><cn type="integer" id="S5.SS2.SSS1.p6.2.m2.6.6.cmml" xref="S5.SS2.SSS1.p6.2.m2.6.6">2</cn><cn type="integer" id="S5.SS2.SSS1.p6.2.m2.7.7.cmml" xref="S5.SS2.SSS1.p6.2.m2.7.7">2</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p6.2.m2.7c">\left\{10,3,3,3,3,2,2\right\}</annotation></semantics></math>. The Transformer contains 12 layers with 768 dimensions, 3072 inner dimensions, and 12 attention heads. In contrast, for the Large version model, the convolutional module maintains the same configuration as the Base version, but its Transformer contains 24 layers with 1024 dimensions, 4096 inner dimensions, and 16 attention heads.</p>
</div>
<div id="S5.SS2.SSS1.p7" class="ltx_para">
<p id="S5.SS2.SSS1.p7.2" class="ltx_p">Our pre-training codebase is built on MindSpore, resulting in a slight loss<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/wangtianrui/ProgRE/blob/master/supplementary_results/README.md#migration-errors" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/wangtianrui/ProgRE/blob/master/supplementary_results/README.md#migration-errors</a></span></span></span> in accuracy when migrating the model to various downstream fine-tuning tasks implemented in the PyTorch framework. To distinguish our baseline from HuBERT<math id="S5.SS2.SSS1.p7.1.m1.1" class="ltx_Math" alttext="{}_{\text{pt}}" display="inline"><semantics id="S5.SS2.SSS1.p7.1.m1.1a"><msub id="S5.SS2.SSS1.p7.1.m1.1.1" xref="S5.SS2.SSS1.p7.1.m1.1.1.cmml"><mi id="S5.SS2.SSS1.p7.1.m1.1.1a" xref="S5.SS2.SSS1.p7.1.m1.1.1.cmml"></mi><mtext id="S5.SS2.SSS1.p7.1.m1.1.1.1" xref="S5.SS2.SSS1.p7.1.m1.1.1.1a.cmml">pt</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p7.1.m1.1b"><apply id="S5.SS2.SSS1.p7.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p7.1.m1.1.1"><ci id="S5.SS2.SSS1.p7.1.m1.1.1.1a.cmml" xref="S5.SS2.SSS1.p7.1.m1.1.1.1"><mtext mathsize="70%" id="S5.SS2.SSS1.p7.1.m1.1.1.1.cmml" xref="S5.SS2.SSS1.p7.1.m1.1.1.1">pt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p7.1.m1.1c">{}_{\text{pt}}</annotation></semantics></math>, which is pre-trained in PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>, we refer to our baseline HuBERT, implemented under the MindSpore framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, as HuBERT<math id="S5.SS2.SSS1.p7.2.m2.1" class="ltx_Math" alttext="{}_{\text{ms}}" display="inline"><semantics id="S5.SS2.SSS1.p7.2.m2.1a"><msub id="S5.SS2.SSS1.p7.2.m2.1.1" xref="S5.SS2.SSS1.p7.2.m2.1.1.cmml"><mi id="S5.SS2.SSS1.p7.2.m2.1.1a" xref="S5.SS2.SSS1.p7.2.m2.1.1.cmml"></mi><mtext id="S5.SS2.SSS1.p7.2.m2.1.1.1" xref="S5.SS2.SSS1.p7.2.m2.1.1.1a.cmml">ms</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p7.2.m2.1b"><apply id="S5.SS2.SSS1.p7.2.m2.1.1.cmml" xref="S5.SS2.SSS1.p7.2.m2.1.1"><ci id="S5.SS2.SSS1.p7.2.m2.1.1.1a.cmml" xref="S5.SS2.SSS1.p7.2.m2.1.1.1"><mtext mathsize="70%" id="S5.SS2.SSS1.p7.2.m2.1.1.1.cmml" xref="S5.SS2.SSS1.p7.2.m2.1.1.1">ms</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p7.2.m2.1c">{}_{\text{ms}}</annotation></semantics></math> or baseline.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS2.5.1.1" class="ltx_text">V-B</span>2 </span>Pre-training Setup</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p">The pseudo labels, speaker teacher, and detailed settings for pre-training are introduced as follows:</p>
</div>
<div id="S5.SS2.SSS2.p2" class="ltx_para">
<p id="S5.SS2.SSS2.p2.1" class="ltx_p"><span id="S5.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Unsupervised unit discovery</span>: In our model’s pre-training process, we conduct two iterations, with the primary distinction being the origin of pseudo-labels for the main branch. During the first iteration, we extract 13-dimensional MFCCs along with their first-order and second-order differential features. Subsequently, we train a 100-class K-means model using the resulting 39-dimensional features from 10% (1% for Large) of the speech data. Finally, we assign the corresponding cluster center as the pseudo-label for each frame of speech. In the second iteration, we utilize the output of the middle layer of the model pre-trained in the first iteration as features (the 9th layer for the Base version and the 18th layer for the Large version). These features are then used to train a 500-class K-means model, and the corresponding cluster center is assigned as the pseudo-label for each frame of speech. For clustering, we utilize the MiniBatchKMeans implemented in the scikit-learn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> with a mini-batch strategy. We set the mini-batch size to be 10,000 frames. Additionally, we employ k-means++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> with 20 random starts for better initialization.</p>
</div>
<div id="S5.SS2.SSS2.p3" class="ltx_para">
<p id="S5.SS2.SSS2.p3.1" class="ltx_p"><span id="S5.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Self-supervised speaker teacher model</span>: We employed the open-source toolkit Wespeaker <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> to pre-train the EMA-DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> without labels as the speaker teacher model for our <span id="S5.SS2.SSS2.p3.1.2" class="ltx_text ltx_font_smallcaps">ProgRE</span>. Specifically, for the Base version of <span id="S5.SS2.SSS2.p3.1.3" class="ltx_text ltx_font_smallcaps">ProgRE</span>, we trained the EMA-DINO model with 512 intermediate dimensions on 960 hours of LibriSpeech. Similarly, for the Large version of <span id="S5.SS2.SSS2.p3.1.4" class="ltx_text ltx_font_smallcaps">ProgRE</span>, we trained the teacher model with 1024 intermediate dimensions on the 44,500 hour MLS English dataset. These teacher models will output a 192-dimensional utterance-level speaker embedding to serve as supervision for the speaker extractor of our <span id="S5.SS2.SSS2.p3.1.5" class="ltx_text ltx_font_smallcaps">ProgRE</span>.</p>
</div>
<div id="S5.SS2.SSS2.p4" class="ltx_para">
<p id="S5.SS2.SSS2.p4.1" class="ltx_p"><span id="S5.SS2.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Training detail</span>: For the Base version, with two iterations, <span id="S5.SS2.SSS2.p4.1.2" class="ltx_text ltx_font_smallcaps">ProgRE</span> Base was pre-trained for 400K steps per iteration on 32 Ascend910 GPUs, with a batch size of 60-second samples per GPU. For the Large version, <span id="S5.SS2.SSS2.p4.1.3" class="ltx_text ltx_font_smallcaps">ProgRE</span> Large was pre-trained for 350K steps in the first iteration and 1400K steps in the second iteration, using 96 Ascend910 GPUs with a batch size of 25-second samples per GPU. The Adam optimizer was used with a warm-up learning rate, ramping up from 0 to 5e-4 for the first 8% of steps and then decaying to 0.</p>
</div>
</section>
<section id="S5.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS3.5.1.1" class="ltx_text">V-B</span>3 </span>Fine-tuning Setup</h4>

<div id="S5.SS2.SSS3.p1" class="ltx_para">
<p id="S5.SS2.SSS3.p1.1" class="ltx_p">The downstream models and detailed settings for fine-tuning are introduced as follows:</p>
</div>
<div id="S5.SS2.SSS3.p2" class="ltx_para">
<p id="S5.SS2.SSS3.p2.1" class="ltx_p"><span id="S5.SS2.SSS3.p2.1.1" class="ltx_text ltx_font_bold">Downstream models</span>: For ASR, we used a 2-layer BiLSTM with 1024 cells, optimized by the character-unit CTC loss. For SID, we applied an utterance-level mean-pooling followed by a 1251-class FC layer, optimized by cross-entropy loss. For SE, we used a 3-layer BiLSTM with 256 cells followed by a sigmoid activation for mask-based filtering, trained via the L1 loss function. For ER, an utterance-level mean-pooling followed by convolutional attention with a kernel size of 5 was optimized by cross-entropy loss. For VC, we used the Taco2-AR model<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/s3prl/s3prl/blob/main/s3prl/downstream/a2o-vc-vcc2020/config.yaml" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/s3prl/s3prl/blob/main/s3prl/downstream/a2o-vc-vcc2020/config.yaml</a></span></span></span>. Taco2-AR extracts a speaker vector via an encoder consisting of 3 convolution layers and a 1024-cell BiLSTM and then generates the log-mel spectrograms using 2-layer 256-cell LSTM models in an auto-regressive manner.</p>
</div>
<div id="S5.SS2.SSS3.p3" class="ltx_para">
<p id="S5.SS2.SSS3.p3.1" class="ltx_p"><span id="S5.SS2.SSS3.p3.1.1" class="ltx_text ltx_font_bold">Fine-tuning detail</span>: To effectively evaluate the capabilities learned by the self-supervised pre-trained models, we froze the parameters of the pre-trained model during fine-tuning and only fine-tuned the weights of the downstream model and the weights of the weighted-sum mechanism. All downstream fine-tuning was performed using the Adam optimizer. Due to the varying data scales of different downstream tasks, the number of training steps, learning rate, and batch size used for fine-tuning each downstream task differed. The detailed configuration is shown in the TABLE <a href="#S5.T1" title="TABLE I ‣ V-B3 Fine-tuning Setup ‣ V-B Experimental Setup ‣ V Experiments and Results ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Fine-tuning configuration of downstream tasks.</figcaption>
<table id="S5.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T1.1.1" class="ltx_tr">
<td id="S5.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Task</td>
<td id="S5.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">update step</td>
<td id="S5.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">learning rate</td>
<td id="S5.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">batch size</td>
</tr>
<tr id="S5.T1.1.2" class="ltx_tr">
<td id="S5.T1.1.2.1" class="ltx_td ltx_align_center ltx_border_t">ASR</td>
<td id="S5.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">40k</td>
<td id="S5.T1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">1e-4</td>
<td id="S5.T1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">32</td>
</tr>
<tr id="S5.T1.1.3" class="ltx_tr">
<td id="S5.T1.1.3.1" class="ltx_td ltx_align_center">SID</td>
<td id="S5.T1.1.3.2" class="ltx_td ltx_align_center">100k</td>
<td id="S5.T1.1.3.3" class="ltx_td ltx_align_center">1e-3</td>
<td id="S5.T1.1.3.4" class="ltx_td ltx_align_center">64</td>
</tr>
<tr id="S5.T1.1.4" class="ltx_tr">
<td id="S5.T1.1.4.1" class="ltx_td ltx_align_center">SE</td>
<td id="S5.T1.1.4.2" class="ltx_td ltx_align_center">40k</td>
<td id="S5.T1.1.4.3" class="ltx_td ltx_align_center">1e-3</td>
<td id="S5.T1.1.4.4" class="ltx_td ltx_align_center">16</td>
</tr>
<tr id="S5.T1.1.5" class="ltx_tr">
<td id="S5.T1.1.5.1" class="ltx_td ltx_align_center">ER</td>
<td id="S5.T1.1.5.2" class="ltx_td ltx_align_center">50k</td>
<td id="S5.T1.1.5.3" class="ltx_td ltx_align_center">1e-4</td>
<td id="S5.T1.1.5.4" class="ltx_td ltx_align_center">16</td>
</tr>
<tr id="S5.T1.1.6" class="ltx_tr">
<td id="S5.T1.1.6.1" class="ltx_td ltx_align_center ltx_border_bb">VC</td>
<td id="S5.T1.1.6.2" class="ltx_td ltx_align_center ltx_border_bb">10k</td>
<td id="S5.T1.1.6.3" class="ltx_td ltx_align_center ltx_border_bb">1e-4</td>
<td id="S5.T1.1.6.4" class="ltx_td ltx_align_center ltx_border_bb">6</td>
</tr>
</table>
</figure>
</section>
<section id="S5.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS2.SSS4.5.1.1" class="ltx_text">V-B</span>4 </span>Metrics</h4>

<div id="S5.SS2.SSS4.p1" class="ltx_para">
<p id="S5.SS2.SSS4.p1.1" class="ltx_p">Word error rate (WER) was used to evaluate performance in the speech recognition task. Accuracy (Acc) was employed for speaker identification (SID) and speech emotion recognition (SER). Perceptual Evaluation of Speech Quality (PESQ) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> and scale-invariant signal-to-distortion ratio (SI-SDR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> were used to measure the quality of enhanced speech with a clean reference. Higher PESQ scores indicate better auditory quality of the enhanced speech, and higher SI-SDR values indicate greater similarity between the clean and enhanced signal distributions. For the voice conversion (VC) task, we used Mel-Cepstral Distortion (MCD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>, Pearson correlation coefficient of pitch (F0C) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>, WER, and speaker accept rate (SPK) for evaluation. WER was measured using a pre-trained ASR model<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_vox_960h_pl.pt" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_vox_960h_pl.pt</a></span></span></span>, and SPK was defined as the pass rate at which the speaker verification model<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/resemble-ai/Resemblyzer" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/resemble-ai/Resemblyzer</a></span></span></span> considered the converted speech to be consistent with the target speaker.</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.5.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.6.2" class="ltx_text ltx_font_italic">Ablation Study</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We first verify the effectiveness of each improvement in our model. We conduct ablation experiments involving the residual extraction, speaker extractor, and pitch extractor. In these experiments, we focus on the model’s performance on two tasks: speech recognition and speaker identification. These tasks evaluate the model’s ability to understand speech content and non-linguistic information, respectively <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>.</p>
</div>
<section id="S5.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS3.SSS1.5.1.1" class="ltx_text">V-C</span>1 </span>Importance of Residual Extraction</h4>

<div id="S5.SS3.SSS1.p1" class="ltx_para">
<p id="S5.SS3.SSS1.p1.2" class="ltx_p">Residual extraction is the core of our method. Based on the Base version model, we compared the performance of residual extraction with that of multi-task extraction, where multi-task extraction replaces the subtraction (denoted by <math id="S5.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\circleddash" display="inline"><semantics id="S5.SS3.SSS1.p1.1.m1.1a"><mo id="S5.SS3.SSS1.p1.1.m1.1.1" xref="S5.SS3.SSS1.p1.1.m1.1.1.cmml">⊝</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p1.1.m1.1b"><ci id="S5.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S5.SS3.SSS1.p1.1.m1.1.1">⊝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p1.1.m1.1c">\circleddash</annotation></semantics></math>) in residual extraction with addition (denoted by <math id="S5.SS3.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\oplus" display="inline"><semantics id="S5.SS3.SSS1.p1.2.m2.1a"><mo id="S5.SS3.SSS1.p1.2.m2.1.1" xref="S5.SS3.SSS1.p1.2.m2.1.1.cmml">⊕</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p1.2.m2.1b"><csymbol cd="latexml" id="S5.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S5.SS3.SSS1.p1.2.m2.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p1.2.m2.1c">\oplus</annotation></semantics></math>). Since the insertion layer of the speaker extractor also affects the performance of the model, in this ablation experiment, we inserted the speaker extractor at the position where it performs best in the Base version setting, which is after the 4th layer of the Transformer, results are shown in the TABLE <a href="#S5.T2" title="TABLE II ‣ V-C1 Importance of Residual Extraction ‣ V-C Ablation Study ‣ V Experiments and Results ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Comparison of residual extraction and multi-task extraction. <span id="S5.T2.12.1" class="ltx_text ltx_font_bold">BLOD</span> indicates the best result.
</figcaption>
<table id="S5.T2.10" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T2.2.2" class="ltx_tr">
<td id="S5.T2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;" rowspan="2"><span id="S5.T2.2.2.3.1" class="ltx_text">Index</span></td>
<td id="S5.T2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;" rowspan="2"><span id="S5.T2.2.2.4.1" class="ltx_text">Method</span></td>
<td id="S5.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;" colspan="2">ASR (WER) <math id="S5.T2.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T2.1.1.1.m1.1a"><mo stretchy="false" id="S5.T2.1.1.1.m1.1.1" xref="S5.T2.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;" colspan="2">SID (Acc) <math id="S5.T2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T2.2.2.2.m1.1a"><mo stretchy="false" id="S5.T2.2.2.2.m1.1.1" xref="S5.T2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.m1.1b"><ci id="S5.T2.2.2.2.m1.1.1.cmml" xref="S5.T2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T2.10.11" class="ltx_tr">
<td id="S5.T2.10.11.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">test-clean</td>
<td id="S5.T2.10.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">test-other</td>
<td id="S5.T2.10.11.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">dev</td>
<td id="S5.T2.10.11.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">test</td>
</tr>
<tr id="S5.T2.10.12" class="ltx_tr">
<td id="S5.T2.10.12.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">0</td>
<td id="S5.T2.10.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">baseline</td>
<td id="S5.T2.10.12.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">6.85</td>
<td id="S5.T2.10.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">16.77</td>
<td id="S5.T2.10.12.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">81.01</td>
<td id="S5.T2.10.12.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">79.94</td>
</tr>
<tr id="S5.T2.4.4" class="ltx_tr">
<td id="S5.T2.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">1</td>
<td id="S5.T2.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">
<math id="S5.T2.3.3.1.m1.1" class="ltx_Math" alttext="\oplus" display="inline"><semantics id="S5.T2.3.3.1.m1.1a"><mo id="S5.T2.3.3.1.m1.1.1" xref="S5.T2.3.3.1.m1.1.1.cmml">⊕</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.1.m1.1b"><csymbol cd="latexml" id="S5.T2.3.3.1.m1.1.1.cmml" xref="S5.T2.3.3.1.m1.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.1.m1.1c">\oplus</annotation></semantics></math> pitch <math id="S5.T2.4.4.2.m2.1" class="ltx_Math" alttext="\oplus" display="inline"><semantics id="S5.T2.4.4.2.m2.1a"><mo id="S5.T2.4.4.2.m2.1.1" xref="S5.T2.4.4.2.m2.1.1.cmml">⊕</mo><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.2.m2.1b"><csymbol cd="latexml" id="S5.T2.4.4.2.m2.1.1.cmml" xref="S5.T2.4.4.2.m2.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.2.m2.1c">\oplus</annotation></semantics></math> speaker</td>
<td id="S5.T2.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">8.06</td>
<td id="S5.T2.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">19.11</td>
<td id="S5.T2.4.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">88.75</td>
<td id="S5.T2.4.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">87.58</td>
</tr>
<tr id="S5.T2.6.6" class="ltx_tr">
<td id="S5.T2.6.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">2</td>
<td id="S5.T2.6.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">
<math id="S5.T2.5.5.1.m1.1" class="ltx_Math" alttext="\circleddash" display="inline"><semantics id="S5.T2.5.5.1.m1.1a"><mo id="S5.T2.5.5.1.m1.1.1" xref="S5.T2.5.5.1.m1.1.1.cmml">⊝</mo><annotation-xml encoding="MathML-Content" id="S5.T2.5.5.1.m1.1b"><ci id="S5.T2.5.5.1.m1.1.1.cmml" xref="S5.T2.5.5.1.m1.1.1">⊝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.5.1.m1.1c">\circleddash</annotation></semantics></math> pitch <math id="S5.T2.6.6.2.m2.1" class="ltx_Math" alttext="\oplus" display="inline"><semantics id="S5.T2.6.6.2.m2.1a"><mo id="S5.T2.6.6.2.m2.1.1" xref="S5.T2.6.6.2.m2.1.1.cmml">⊕</mo><annotation-xml encoding="MathML-Content" id="S5.T2.6.6.2.m2.1b"><csymbol cd="latexml" id="S5.T2.6.6.2.m2.1.1.cmml" xref="S5.T2.6.6.2.m2.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.6.2.m2.1c">\oplus</annotation></semantics></math> speaker</td>
<td id="S5.T2.6.6.4" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">7.87</td>
<td id="S5.T2.6.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">18.55</td>
<td id="S5.T2.6.6.6" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">89.69</td>
<td id="S5.T2.6.6.7" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">89.14</td>
</tr>
<tr id="S5.T2.8.8" class="ltx_tr">
<td id="S5.T2.8.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">3</td>
<td id="S5.T2.8.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">
<math id="S5.T2.7.7.1.m1.1" class="ltx_Math" alttext="\oplus" display="inline"><semantics id="S5.T2.7.7.1.m1.1a"><mo id="S5.T2.7.7.1.m1.1.1" xref="S5.T2.7.7.1.m1.1.1.cmml">⊕</mo><annotation-xml encoding="MathML-Content" id="S5.T2.7.7.1.m1.1b"><csymbol cd="latexml" id="S5.T2.7.7.1.m1.1.1.cmml" xref="S5.T2.7.7.1.m1.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.7.1.m1.1c">\oplus</annotation></semantics></math> pitch <math id="S5.T2.8.8.2.m2.1" class="ltx_Math" alttext="\circleddash" display="inline"><semantics id="S5.T2.8.8.2.m2.1a"><mo id="S5.T2.8.8.2.m2.1.1" xref="S5.T2.8.8.2.m2.1.1.cmml">⊝</mo><annotation-xml encoding="MathML-Content" id="S5.T2.8.8.2.m2.1b"><ci id="S5.T2.8.8.2.m2.1.1.cmml" xref="S5.T2.8.8.2.m2.1.1">⊝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.8.8.2.m2.1c">\circleddash</annotation></semantics></math> speaker</td>
<td id="S5.T2.8.8.4" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">6.71</td>
<td id="S5.T2.8.8.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">16.36</td>
<td id="S5.T2.8.8.6" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">88.77</td>
<td id="S5.T2.8.8.7" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">87.51</td>
</tr>
<tr id="S5.T2.10.10" class="ltx_tr">
<td id="S5.T2.10.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">4</td>
<td id="S5.T2.10.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">
<math id="S5.T2.9.9.1.m1.1" class="ltx_Math" alttext="\circleddash" display="inline"><semantics id="S5.T2.9.9.1.m1.1a"><mo id="S5.T2.9.9.1.m1.1.1" xref="S5.T2.9.9.1.m1.1.1.cmml">⊝</mo><annotation-xml encoding="MathML-Content" id="S5.T2.9.9.1.m1.1b"><ci id="S5.T2.9.9.1.m1.1.1.cmml" xref="S5.T2.9.9.1.m1.1.1">⊝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.9.9.1.m1.1c">\circleddash</annotation></semantics></math> pitch <math id="S5.T2.10.10.2.m2.1" class="ltx_Math" alttext="\circleddash" display="inline"><semantics id="S5.T2.10.10.2.m2.1a"><mo id="S5.T2.10.10.2.m2.1.1" xref="S5.T2.10.10.2.m2.1.1.cmml">⊝</mo><annotation-xml encoding="MathML-Content" id="S5.T2.10.10.2.m2.1b"><ci id="S5.T2.10.10.2.m2.1.1.cmml" xref="S5.T2.10.10.2.m2.1.1">⊝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.10.10.2.m2.1c">\circleddash</annotation></semantics></math> speaker</td>
<td id="S5.T2.10.10.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T2.10.10.4.1" class="ltx_text ltx_font_bold">6.52</span></td>
<td id="S5.T2.10.10.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T2.10.10.5.1" class="ltx_text ltx_font_bold">15.20</span></td>
<td id="S5.T2.10.10.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T2.10.10.6.1" class="ltx_text ltx_font_bold">90.95</span></td>
<td id="S5.T2.10.10.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T2.10.10.7.1" class="ltx_text ltx_font_bold">90.61</span></td>
</tr>
</table>
</figure>
<div id="S5.SS3.SSS1.p2" class="ltx_para">
<p id="S5.SS3.SSS1.p2.8" class="ltx_p">Although multi-task extraction (<math id="S5.SS3.SSS1.p2.1.m1.1" class="ltx_Math" alttext="\oplus" display="inline"><semantics id="S5.SS3.SSS1.p2.1.m1.1a"><mo id="S5.SS3.SSS1.p2.1.m1.1.1" xref="S5.SS3.SSS1.p2.1.m1.1.1.cmml">⊕</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p2.1.m1.1b"><csymbol cd="latexml" id="S5.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S5.SS3.SSS1.p2.1.m1.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p2.1.m1.1c">\oplus</annotation></semantics></math> pitch <math id="S5.SS3.SSS1.p2.2.m2.1" class="ltx_Math" alttext="\oplus" display="inline"><semantics id="S5.SS3.SSS1.p2.2.m2.1a"><mo id="S5.SS3.SSS1.p2.2.m2.1.1" xref="S5.SS3.SSS1.p2.2.m2.1.1.cmml">⊕</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p2.2.m2.1b"><csymbol cd="latexml" id="S5.SS3.SSS1.p2.2.m2.1.1.cmml" xref="S5.SS3.SSS1.p2.2.m2.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p2.2.m2.1c">\oplus</annotation></semantics></math> speaker) significantly enhances the pre-trained model’s ability to extract speaker information, it degrades the model’s performance on speech recognition. This occurs because multi-task extraction strengthens the model’s capacity to capture pitch variation and speaker information, but the enhanced content-irrelevant information interferes with the deeper Transformer’s capacity to extract content information.
When we remove the enhanced pitch variation information from the main branch (<math id="S5.SS3.SSS1.p2.3.m3.1" class="ltx_Math" alttext="\circleddash" display="inline"><semantics id="S5.SS3.SSS1.p2.3.m3.1a"><mo id="S5.SS3.SSS1.p2.3.m3.1.1" xref="S5.SS3.SSS1.p2.3.m3.1.1.cmml">⊝</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p2.3.m3.1b"><ci id="S5.SS3.SSS1.p2.3.m3.1.1.cmml" xref="S5.SS3.SSS1.p2.3.m3.1.1">⊝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p2.3.m3.1c">\circleddash</annotation></semantics></math> pitch <math id="S5.SS3.SSS1.p2.4.m4.1" class="ltx_Math" alttext="\oplus" display="inline"><semantics id="S5.SS3.SSS1.p2.4.m4.1a"><mo id="S5.SS3.SSS1.p2.4.m4.1.1" xref="S5.SS3.SSS1.p2.4.m4.1.1.cmml">⊕</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p2.4.m4.1b"><csymbol cd="latexml" id="S5.SS3.SSS1.p2.4.m4.1.1.cmml" xref="S5.SS3.SSS1.p2.4.m4.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p2.4.m4.1c">\oplus</annotation></semantics></math> speaker), the performance of speech recognition improves, and the improvement of speaker identification becomes more significant. This is because pitch variation information is less related to speaker and content information, and removing redundant information can enhance performance on irrelevant tasks jointly.
The 3-index method (<math id="S5.SS3.SSS1.p2.5.m5.1" class="ltx_Math" alttext="\oplus" display="inline"><semantics id="S5.SS3.SSS1.p2.5.m5.1a"><mo id="S5.SS3.SSS1.p2.5.m5.1.1" xref="S5.SS3.SSS1.p2.5.m5.1.1.cmml">⊕</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p2.5.m5.1b"><csymbol cd="latexml" id="S5.SS3.SSS1.p2.5.m5.1.1.cmml" xref="S5.SS3.SSS1.p2.5.m5.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p2.5.m5.1c">\oplus</annotation></semantics></math> pitch <math id="S5.SS3.SSS1.p2.6.m6.1" class="ltx_Math" alttext="\circleddash" display="inline"><semantics id="S5.SS3.SSS1.p2.6.m6.1a"><mo id="S5.SS3.SSS1.p2.6.m6.1.1" xref="S5.SS3.SSS1.p2.6.m6.1.1.cmml">⊝</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p2.6.m6.1b"><ci id="S5.SS3.SSS1.p2.6.m6.1.1.cmml" xref="S5.SS3.SSS1.p2.6.m6.1.1">⊝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p2.6.m6.1c">\circleddash</annotation></semantics></math> speaker) performs comparably to multi-task extraction (1-index) on speaker identification tasks, but its performance on speech recognition is improved compared to the 2-index method. This indicates that the speaker information extracted by the speaker extractor has a more significant interference with content extraction than pitch variation information.
The final results (<math id="S5.SS3.SSS1.p2.7.m7.1" class="ltx_Math" alttext="\circleddash" display="inline"><semantics id="S5.SS3.SSS1.p2.7.m7.1a"><mo id="S5.SS3.SSS1.p2.7.m7.1.1" xref="S5.SS3.SSS1.p2.7.m7.1.1.cmml">⊝</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p2.7.m7.1b"><ci id="S5.SS3.SSS1.p2.7.m7.1.1.cmml" xref="S5.SS3.SSS1.p2.7.m7.1.1">⊝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p2.7.m7.1c">\circleddash</annotation></semantics></math> pitch <math id="S5.SS3.SSS1.p2.8.m8.1" class="ltx_Math" alttext="\circleddash" display="inline"><semantics id="S5.SS3.SSS1.p2.8.m8.1a"><mo id="S5.SS3.SSS1.p2.8.m8.1.1" xref="S5.SS3.SSS1.p2.8.m8.1.1.cmml">⊝</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS1.p2.8.m8.1b"><ci id="S5.SS3.SSS1.p2.8.m8.1.1.cmml" xref="S5.SS3.SSS1.p2.8.m8.1.1">⊝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS1.p2.8.m8.1c">\circleddash</annotation></semantics></math> speaker) demonstrate that progressive residual extraction can jointly enhance the model’s performance on both speech recognition and speaker identification tasks. Progressively refining the content information in the main branch improves the model’s performance across various tasks.</p>
</div>
</section>
<section id="S5.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS3.SSS2.5.1.1" class="ltx_text">V-C</span>2 </span>Importance of Inserting the Speaker Extractor</h4>

<div id="S5.SS3.SSS2.p1" class="ltx_para">
<p id="S5.SS3.SSS2.p1.2" class="ltx_p">Unlike the pitch extractor, which directly takes its input from the waveform, the speaker extractor is inserted after the middle Transformer layer. Therefore, we conducted an ablation experiment on the insertion layer. In this ablation experiment, we used the Base version model, did not include the pitch extractor, and used residual extraction (subtraction, <math id="S5.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\circleddash" display="inline"><semantics id="S5.SS3.SSS2.p1.1.m1.1a"><mo id="S5.SS3.SSS2.p1.1.m1.1.1" xref="S5.SS3.SSS2.p1.1.m1.1.1.cmml">⊝</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS2.p1.1.m1.1b"><ci id="S5.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S5.SS3.SSS2.p1.1.m1.1.1">⊝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS2.p1.1.m1.1c">\circleddash</annotation></semantics></math>) as the insertion method. We inserted the speaker extractor before the Transformer (0th layer) or after the <math id="S5.SS3.SSS2.p1.2.m2.6" class="ltx_Math" alttext="\{2,4,6,8,10,12\}" display="inline"><semantics id="S5.SS3.SSS2.p1.2.m2.6a"><mrow id="S5.SS3.SSS2.p1.2.m2.6.7.2" xref="S5.SS3.SSS2.p1.2.m2.6.7.1.cmml"><mo stretchy="false" id="S5.SS3.SSS2.p1.2.m2.6.7.2.1" xref="S5.SS3.SSS2.p1.2.m2.6.7.1.cmml">{</mo><mn id="S5.SS3.SSS2.p1.2.m2.1.1" xref="S5.SS3.SSS2.p1.2.m2.1.1.cmml">2</mn><mo id="S5.SS3.SSS2.p1.2.m2.6.7.2.2" xref="S5.SS3.SSS2.p1.2.m2.6.7.1.cmml">,</mo><mn id="S5.SS3.SSS2.p1.2.m2.2.2" xref="S5.SS3.SSS2.p1.2.m2.2.2.cmml">4</mn><mo id="S5.SS3.SSS2.p1.2.m2.6.7.2.3" xref="S5.SS3.SSS2.p1.2.m2.6.7.1.cmml">,</mo><mn id="S5.SS3.SSS2.p1.2.m2.3.3" xref="S5.SS3.SSS2.p1.2.m2.3.3.cmml">6</mn><mo id="S5.SS3.SSS2.p1.2.m2.6.7.2.4" xref="S5.SS3.SSS2.p1.2.m2.6.7.1.cmml">,</mo><mn id="S5.SS3.SSS2.p1.2.m2.4.4" xref="S5.SS3.SSS2.p1.2.m2.4.4.cmml">8</mn><mo id="S5.SS3.SSS2.p1.2.m2.6.7.2.5" xref="S5.SS3.SSS2.p1.2.m2.6.7.1.cmml">,</mo><mn id="S5.SS3.SSS2.p1.2.m2.5.5" xref="S5.SS3.SSS2.p1.2.m2.5.5.cmml">10</mn><mo id="S5.SS3.SSS2.p1.2.m2.6.7.2.6" xref="S5.SS3.SSS2.p1.2.m2.6.7.1.cmml">,</mo><mn id="S5.SS3.SSS2.p1.2.m2.6.6" xref="S5.SS3.SSS2.p1.2.m2.6.6.cmml">12</mn><mo stretchy="false" id="S5.SS3.SSS2.p1.2.m2.6.7.2.7" xref="S5.SS3.SSS2.p1.2.m2.6.7.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS2.p1.2.m2.6b"><set id="S5.SS3.SSS2.p1.2.m2.6.7.1.cmml" xref="S5.SS3.SSS2.p1.2.m2.6.7.2"><cn type="integer" id="S5.SS3.SSS2.p1.2.m2.1.1.cmml" xref="S5.SS3.SSS2.p1.2.m2.1.1">2</cn><cn type="integer" id="S5.SS3.SSS2.p1.2.m2.2.2.cmml" xref="S5.SS3.SSS2.p1.2.m2.2.2">4</cn><cn type="integer" id="S5.SS3.SSS2.p1.2.m2.3.3.cmml" xref="S5.SS3.SSS2.p1.2.m2.3.3">6</cn><cn type="integer" id="S5.SS3.SSS2.p1.2.m2.4.4.cmml" xref="S5.SS3.SSS2.p1.2.m2.4.4">8</cn><cn type="integer" id="S5.SS3.SSS2.p1.2.m2.5.5.cmml" xref="S5.SS3.SSS2.p1.2.m2.5.5">10</cn><cn type="integer" id="S5.SS3.SSS2.p1.2.m2.6.6.cmml" xref="S5.SS3.SSS2.p1.2.m2.6.6">12</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS2.p1.2.m2.6c">\{2,4,6,8,10,12\}</annotation></semantics></math>-th layer. The results are shown in TABLE <a href="#S5.T3" title="TABLE III ‣ V-C2 Importance of Inserting the Speaker Extractor ‣ V-C Ablation Study ‣ V Experiments and Results ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Comparison of the insertion layer of speaker extractor. <span id="S5.T3.4.1" class="ltx_text ltx_font_bold">BLOD</span> indicates the best result.</figcaption>
<table id="S5.T3.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T3.2.2" class="ltx_tr">
<td id="S5.T3.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T3.2.2.3.1" class="ltx_text">Layer</span></td>
<td id="S5.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">ASR (WER) <math id="S5.T3.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.1.1.1.m1.1a"><mo stretchy="false" id="S5.T3.1.1.1.m1.1.1" xref="S5.T3.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T3.2.2.2" class="ltx_td ltx_align_center ltx_border_t" colspan="2">SID (Acc) <math id="S5.T3.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T3.2.2.2.m1.1a"><mo stretchy="false" id="S5.T3.2.2.2.m1.1.1" xref="S5.T3.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.m1.1b"><ci id="S5.T3.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T3.2.3" class="ltx_tr">
<td id="S5.T3.2.3.1" class="ltx_td ltx_align_center ltx_border_t">test-clean</td>
<td id="S5.T3.2.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">test-other</td>
<td id="S5.T3.2.3.3" class="ltx_td ltx_align_center ltx_border_t">dev</td>
<td id="S5.T3.2.3.4" class="ltx_td ltx_align_center ltx_border_t">test</td>
</tr>
<tr id="S5.T3.2.4" class="ltx_tr">
<td id="S5.T3.2.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T3.2.4.2" class="ltx_td ltx_align_center ltx_border_t">6.85</td>
<td id="S5.T3.2.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16.77</td>
<td id="S5.T3.2.4.4" class="ltx_td ltx_align_center ltx_border_t">81.01</td>
<td id="S5.T3.2.4.5" class="ltx_td ltx_align_center ltx_border_t">79.94</td>
</tr>
<tr id="S5.T3.2.5" class="ltx_tr">
<td id="S5.T3.2.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S5.T3.2.5.2" class="ltx_td ltx_align_center ltx_border_t">7.01</td>
<td id="S5.T3.2.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">17.02</td>
<td id="S5.T3.2.5.4" class="ltx_td ltx_align_center ltx_border_t">73.16</td>
<td id="S5.T3.2.5.5" class="ltx_td ltx_align_center ltx_border_t">72.17</td>
</tr>
<tr id="S5.T3.2.6" class="ltx_tr">
<td id="S5.T3.2.6.1" class="ltx_td ltx_align_center ltx_border_r">2</td>
<td id="S5.T3.2.6.2" class="ltx_td ltx_align_center">7.46</td>
<td id="S5.T3.2.6.3" class="ltx_td ltx_align_center ltx_border_r">17.61</td>
<td id="S5.T3.2.6.4" class="ltx_td ltx_align_center">77.59</td>
<td id="S5.T3.2.6.5" class="ltx_td ltx_align_center">75.37</td>
</tr>
<tr id="S5.T3.2.7" class="ltx_tr">
<td id="S5.T3.2.7.1" class="ltx_td ltx_align_center ltx_border_r">4</td>
<td id="S5.T3.2.7.2" class="ltx_td ltx_align_center"><span id="S5.T3.2.7.2.1" class="ltx_text ltx_font_bold">6.67</span></td>
<td id="S5.T3.2.7.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.2.7.3.1" class="ltx_text ltx_font_bold">16.04</span></td>
<td id="S5.T3.2.7.4" class="ltx_td ltx_align_center"><span id="S5.T3.2.7.4.1" class="ltx_text ltx_font_bold">88.89</span></td>
<td id="S5.T3.2.7.5" class="ltx_td ltx_align_center"><span id="S5.T3.2.7.5.1" class="ltx_text ltx_font_bold">87.64</span></td>
</tr>
<tr id="S5.T3.2.8" class="ltx_tr">
<td id="S5.T3.2.8.1" class="ltx_td ltx_align_center ltx_border_r">6</td>
<td id="S5.T3.2.8.2" class="ltx_td ltx_align_center">7.48</td>
<td id="S5.T3.2.8.3" class="ltx_td ltx_align_center ltx_border_r">18.30</td>
<td id="S5.T3.2.8.4" class="ltx_td ltx_align_center">86.88</td>
<td id="S5.T3.2.8.5" class="ltx_td ltx_align_center">85.37</td>
</tr>
<tr id="S5.T3.2.9" class="ltx_tr">
<td id="S5.T3.2.9.1" class="ltx_td ltx_align_center ltx_border_r">8</td>
<td id="S5.T3.2.9.2" class="ltx_td ltx_align_center">8.01</td>
<td id="S5.T3.2.9.3" class="ltx_td ltx_align_center ltx_border_r">19.41</td>
<td id="S5.T3.2.9.4" class="ltx_td ltx_align_center">74.36</td>
<td id="S5.T3.2.9.5" class="ltx_td ltx_align_center">72.55</td>
</tr>
<tr id="S5.T3.2.10" class="ltx_tr">
<td id="S5.T3.2.10.1" class="ltx_td ltx_align_center ltx_border_r">10</td>
<td id="S5.T3.2.10.2" class="ltx_td ltx_align_center">8.17</td>
<td id="S5.T3.2.10.3" class="ltx_td ltx_align_center ltx_border_r">21.27</td>
<td id="S5.T3.2.10.4" class="ltx_td ltx_align_center">73.26</td>
<td id="S5.T3.2.10.5" class="ltx_td ltx_align_center">69.54</td>
</tr>
<tr id="S5.T3.2.11" class="ltx_tr">
<td id="S5.T3.2.11.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">12</td>
<td id="S5.T3.2.11.2" class="ltx_td ltx_align_center ltx_border_b">7.74</td>
<td id="S5.T3.2.11.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">18.73</td>
<td id="S5.T3.2.11.4" class="ltx_td ltx_align_center ltx_border_b">84.61</td>
<td id="S5.T3.2.11.5" class="ltx_td ltx_align_center ltx_border_b">84.25</td>
</tr>
</table>
</figure>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Comparison of pre-training methods fine-tuned on different downstream tasks.</figcaption>
<table id="S5.T4.13" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T4.3.3" class="ltx_tr">
<td id="S5.T4.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;" colspan="2" rowspan="2"><span id="S5.T4.3.3.4.1" class="ltx_text">Method</span></td>
<td id="S5.T4.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;" rowspan="2"><span id="S5.T4.3.3.5.1" class="ltx_text"><span id="S5.T4.3.3.5.1.1" class="ltx_text"></span> <span id="S5.T4.3.3.5.1.2" class="ltx_text">
<span id="S5.T4.3.3.5.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T4.3.3.5.1.2.1.1" class="ltx_tr">
<span id="S5.T4.3.3.5.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">Param.</span></span>
<span id="S5.T4.3.3.5.1.2.1.2" class="ltx_tr">
<span id="S5.T4.3.3.5.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">(M)</span></span>
</span></span> <span id="S5.T4.3.3.5.1.3" class="ltx_text"></span></span></td>
<td id="S5.T4.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;" rowspan="2"><span id="S5.T4.3.3.6.1" class="ltx_text">Codebase</span></td>
<td id="S5.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;" colspan="2">ASR (WER) <math id="S5.T4.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T4.1.1.1.m1.1a"><mo stretchy="false" id="S5.T4.1.1.1.m1.1.1" xref="S5.T4.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T4.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;" colspan="2">SID (Acc) <math id="S5.T4.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T4.2.2.2.m1.1a"><mo stretchy="false" id="S5.T4.2.2.2.m1.1.1" xref="S5.T4.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.m1.1b"><ci id="S5.T4.2.2.2.m1.1.1.cmml" xref="S5.T4.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S5.T4.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;" colspan="2">SE</td>
<td id="S5.T4.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;" rowspan="2"><span id="S5.T4.3.3.3.1" class="ltx_text"><span id="S5.T4.3.3.3.1.2" class="ltx_text"></span> <span id="S5.T4.3.3.3.1.1" class="ltx_text">
<span id="S5.T4.3.3.3.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T4.3.3.3.1.1.1.2" class="ltx_tr">
<span id="S5.T4.3.3.3.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">ER</span></span>
<span id="S5.T4.3.3.3.1.1.1.1" class="ltx_tr">
<span id="S5.T4.3.3.3.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">(Acc) <math id="S5.T4.3.3.3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T4.3.3.3.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S5.T4.3.3.3.1.1.1.1.1.m1.1.1" xref="S5.T4.3.3.3.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.1.1.1.1.1.m1.1b"><ci id="S5.T4.3.3.3.1.1.1.1.1.m1.1.1.cmml" xref="S5.T4.3.3.3.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span></span>
</span></span> <span id="S5.T4.3.3.3.1.3" class="ltx_text"></span></span></td>
<td id="S5.T4.3.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;" colspan="4">VC</td>
</tr>
<tr id="S5.T4.9.9" class="ltx_tr">
<td id="S5.T4.9.9.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">test-clean</td>
<td id="S5.T4.9.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">test-other</td>
<td id="S5.T4.9.9.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">dev</td>
<td id="S5.T4.9.9.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">test</td>
<td id="S5.T4.4.4.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">PESQ <math id="S5.T4.4.4.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T4.4.4.1.m1.1a"><mo stretchy="false" id="S5.T4.4.4.1.m1.1.1" xref="S5.T4.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.1.m1.1b"><ci id="S5.T4.4.4.1.m1.1.1.cmml" xref="S5.T4.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S5.T4.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">SI-SDR <math id="S5.T4.5.5.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T4.5.5.2.m1.1a"><mo stretchy="false" id="S5.T4.5.5.2.m1.1.1" xref="S5.T4.5.5.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.5.5.2.m1.1b"><ci id="S5.T4.5.5.2.m1.1.1.cmml" xref="S5.T4.5.5.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.5.5.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S5.T4.6.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">MCD <math id="S5.T4.6.6.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T4.6.6.3.m1.1a"><mo stretchy="false" id="S5.T4.6.6.3.m1.1.1" xref="S5.T4.6.6.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.6.6.3.m1.1b"><ci id="S5.T4.6.6.3.m1.1.1.cmml" xref="S5.T4.6.6.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.6.6.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T4.7.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">F0C <math id="S5.T4.7.7.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T4.7.7.4.m1.1a"><mo stretchy="false" id="S5.T4.7.7.4.m1.1.1" xref="S5.T4.7.7.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.7.7.4.m1.1b"><ci id="S5.T4.7.7.4.m1.1.1.cmml" xref="S5.T4.7.7.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.7.7.4.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S5.T4.8.8.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">WER <math id="S5.T4.8.8.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T4.8.8.5.m1.1a"><mo stretchy="false" id="S5.T4.8.8.5.m1.1.1" xref="S5.T4.8.8.5.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.8.8.5.m1.1b"><ci id="S5.T4.8.8.5.m1.1.1.cmml" xref="S5.T4.8.8.5.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.8.8.5.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T4.9.9.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">SPK <math id="S5.T4.9.9.6.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T4.9.9.6.m1.1a"><mo stretchy="false" id="S5.T4.9.9.6.m1.1.1" xref="S5.T4.9.9.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T4.9.9.6.m1.1b"><ci id="S5.T4.9.9.6.m1.1.1.cmml" xref="S5.T4.9.9.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.9.9.6.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T4.13.14" class="ltx_tr">
<td id="S5.T4.13.14.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;" rowspan="5"><span id="S5.T4.13.14.1.1" class="ltx_text">
<span id="S5.T4.13.14.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:20.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:20.5pt;transform:translate(-6.82pt,-6.82pt) rotate(-90deg) ;">
<span id="S5.T4.13.14.1.1.1.1" class="ltx_p">Base</span>
</span></span></span></td>
<td id="S5.T4.13.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">wav2vec2.0</td>
<td id="S5.T4.13.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">94.70</td>
<td id="S5.T4.13.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">Fairseq</td>
<td id="S5.T4.13.14.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">6.75</td>
<td id="S5.T4.13.14.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">16.28</td>
<td id="S5.T4.13.14.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">79.08</td>
<td id="S5.T4.13.14.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">78.22</td>
<td id="S5.T4.13.14.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">2.94</td>
<td id="S5.T4.13.14.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">9.35</td>
<td id="S5.T4.13.14.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">62.21</td>
<td id="S5.T4.13.14.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">7.86</td>
<td id="S5.T4.13.14.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">0.35</td>
<td id="S5.T4.13.14.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">11.2</td>
<td id="S5.T4.13.14.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">92.0</td>
</tr>
<tr id="S5.T4.10.10" class="ltx_tr">
<td id="S5.T4.10.10.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">HuBERT<math id="S5.T4.10.10.1.m1.1" class="ltx_Math" alttext="{}_{\text{pt}}" display="inline"><semantics id="S5.T4.10.10.1.m1.1a"><msub id="S5.T4.10.10.1.m1.1.1" xref="S5.T4.10.10.1.m1.1.1.cmml"><mi id="S5.T4.10.10.1.m1.1.1a" xref="S5.T4.10.10.1.m1.1.1.cmml"></mi><mtext id="S5.T4.10.10.1.m1.1.1.1" xref="S5.T4.10.10.1.m1.1.1.1a.cmml">pt</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T4.10.10.1.m1.1b"><apply id="S5.T4.10.10.1.m1.1.1.cmml" xref="S5.T4.10.10.1.m1.1.1"><ci id="S5.T4.10.10.1.m1.1.1.1a.cmml" xref="S5.T4.10.10.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T4.10.10.1.m1.1.1.1.cmml" xref="S5.T4.10.10.1.m1.1.1.1">pt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.10.10.1.m1.1c">{}_{\text{pt}}</annotation></semantics></math>
</td>
<td id="S5.T4.10.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">94.70</td>
<td id="S5.T4.10.10.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">Fairseq</td>
<td id="S5.T4.10.10.4" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">6.72</td>
<td id="S5.T4.10.10.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">16.11</td>
<td id="S5.T4.10.10.6" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">81.42</td>
<td id="S5.T4.10.10.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">80.17</td>
<td id="S5.T4.10.10.8" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">2.99</td>
<td id="S5.T4.10.10.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">9.32</td>
<td id="S5.T4.10.10.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">62.44</td>
<td id="S5.T4.10.10.11" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">7.89</td>
<td id="S5.T4.10.10.12" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">0.34</td>
<td id="S5.T4.10.10.13" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">10.2</td>
<td id="S5.T4.10.10.14" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">94.0</td>
</tr>
<tr id="S5.T4.13.15" class="ltx_tr">
<td id="S5.T4.13.15.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">WavLM</td>
<td id="S5.T4.13.15.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">94.70</td>
<td id="S5.T4.13.15.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">Fairseq</td>
<td id="S5.T4.13.15.4" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.15.4.1" class="ltx_text ltx_font_bold">6.50</span></td>
<td id="S5.T4.13.15.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">15.27</td>
<td id="S5.T4.13.15.6" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">84.58</td>
<td id="S5.T4.13.15.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">83.59</td>
<td id="S5.T4.13.15.8" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">3.00</td>
<td id="S5.T4.13.15.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">9.08</td>
<td id="S5.T4.13.15.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">63.78</td>
<td id="S5.T4.13.15.11" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.15.11.1" class="ltx_text ltx_font_bold">7.74</span></td>
<td id="S5.T4.13.15.12" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">0.39</td>
<td id="S5.T4.13.15.13" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">9.9</td>
<td id="S5.T4.13.15.14" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">94.0</td>
</tr>
<tr id="S5.T4.11.11" class="ltx_tr">
<td id="S5.T4.11.11.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">HuBERT<math id="S5.T4.11.11.1.m1.1" class="ltx_Math" alttext="{}_{\text{ms}}" display="inline"><semantics id="S5.T4.11.11.1.m1.1a"><msub id="S5.T4.11.11.1.m1.1.1" xref="S5.T4.11.11.1.m1.1.1.cmml"><mi id="S5.T4.11.11.1.m1.1.1a" xref="S5.T4.11.11.1.m1.1.1.cmml"></mi><mtext id="S5.T4.11.11.1.m1.1.1.1" xref="S5.T4.11.11.1.m1.1.1.1a.cmml">ms</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T4.11.11.1.m1.1b"><apply id="S5.T4.11.11.1.m1.1.1.cmml" xref="S5.T4.11.11.1.m1.1.1"><ci id="S5.T4.11.11.1.m1.1.1.1a.cmml" xref="S5.T4.11.11.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T4.11.11.1.m1.1.1.1.cmml" xref="S5.T4.11.11.1.m1.1.1.1">ms</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.11.11.1.m1.1c">{}_{\text{ms}}</annotation></semantics></math>
</td>
<td id="S5.T4.11.11.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">94.70</td>
<td id="S5.T4.11.11.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">Mindspore</td>
<td id="S5.T4.11.11.4" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">6.85</td>
<td id="S5.T4.11.11.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">16.77</td>
<td id="S5.T4.11.11.6" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">81.01</td>
<td id="S5.T4.11.11.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">79.94</td>
<td id="S5.T4.11.11.8" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">2.96</td>
<td id="S5.T4.11.11.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">9.24</td>
<td id="S5.T4.11.11.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">62.05</td>
<td id="S5.T4.11.11.11" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">7.77</td>
<td id="S5.T4.11.11.12" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">0.35</td>
<td id="S5.T4.11.11.13" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">10.4</td>
<td id="S5.T4.11.11.14" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">94.0</td>
</tr>
<tr id="S5.T4.13.16" class="ltx_tr">
<td id="S5.T4.13.16.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.16.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">ProgRE</span></td>
<td id="S5.T4.13.16.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">97.04</td>
<td id="S5.T4.13.16.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">Mindspore</td>
<td id="S5.T4.13.16.4" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">6.52</td>
<td id="S5.T4.13.16.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.16.5.1" class="ltx_text ltx_font_bold">15.20</span></td>
<td id="S5.T4.13.16.6" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.16.6.1" class="ltx_text ltx_font_bold">90.95</span></td>
<td id="S5.T4.13.16.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.16.7.1" class="ltx_text ltx_font_bold">90.61</span></td>
<td id="S5.T4.13.16.8" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.16.8.1" class="ltx_text ltx_font_bold">3.04</span></td>
<td id="S5.T4.13.16.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.16.9.1" class="ltx_text ltx_font_bold">9.80</span></td>
<td id="S5.T4.13.16.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.16.10.1" class="ltx_text ltx_font_bold">63.96</span></td>
<td id="S5.T4.13.16.11" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">7.75</td>
<td id="S5.T4.13.16.12" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.16.12.1" class="ltx_text ltx_font_bold">0.40</span></td>
<td id="S5.T4.13.16.13" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.16.13.1" class="ltx_text ltx_font_bold">8.8</span></td>
<td id="S5.T4.13.16.14" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.16.14.1" class="ltx_text ltx_font_bold">97.0</span></td>
</tr>
<tr id="S5.T4.13.17" class="ltx_tr">
<td id="S5.T4.13.17.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;" rowspan="5"><span id="S5.T4.13.17.1.1" class="ltx_text">
<span id="S5.T4.13.17.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:24.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:24.6pt;transform:translate(-7.92pt,-6.94pt) rotate(-90deg) ;">
<span id="S5.T4.13.17.1.1.1.1" class="ltx_p">Large</span>
</span></span></span></td>
<td id="S5.T4.13.17.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">wav2vec2.0</td>
<td id="S5.T4.13.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">317.38</td>
<td id="S5.T4.13.17.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">Fairseq</td>
<td id="S5.T4.13.17.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">3.87</td>
<td id="S5.T4.13.17.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">8.80</td>
<td id="S5.T4.13.17.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">91.25</td>
<td id="S5.T4.13.17.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">90.54</td>
<td id="S5.T4.13.17.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">3.01</td>
<td id="S5.T4.13.17.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">9.28</td>
<td id="S5.T4.13.17.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">67.82</td>
<td id="S5.T4.13.17.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">8.20</td>
<td id="S5.T4.13.17.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">0.16</td>
<td id="S5.T4.13.17.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">16.7</td>
<td id="S5.T4.13.17.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">87.0</td>
</tr>
<tr id="S5.T4.12.12" class="ltx_tr">
<td id="S5.T4.12.12.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">HuBERT<math id="S5.T4.12.12.1.m1.1" class="ltx_Math" alttext="{}_{\text{pt}}" display="inline"><semantics id="S5.T4.12.12.1.m1.1a"><msub id="S5.T4.12.12.1.m1.1.1" xref="S5.T4.12.12.1.m1.1.1.cmml"><mi id="S5.T4.12.12.1.m1.1.1a" xref="S5.T4.12.12.1.m1.1.1.cmml"></mi><mtext id="S5.T4.12.12.1.m1.1.1.1" xref="S5.T4.12.12.1.m1.1.1.1a.cmml">pt</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T4.12.12.1.m1.1b"><apply id="S5.T4.12.12.1.m1.1.1.cmml" xref="S5.T4.12.12.1.m1.1.1"><ci id="S5.T4.12.12.1.m1.1.1.1a.cmml" xref="S5.T4.12.12.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T4.12.12.1.m1.1.1.1.cmml" xref="S5.T4.12.12.1.m1.1.1.1">pt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.12.12.1.m1.1c">{}_{\text{pt}}</annotation></semantics></math>
</td>
<td id="S5.T4.12.12.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">317.38</td>
<td id="S5.T4.12.12.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">Fairseq</td>
<td id="S5.T4.12.12.4" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">3.96</td>
<td id="S5.T4.12.12.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">8.82</td>
<td id="S5.T4.12.12.6" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">92.41</td>
<td id="S5.T4.12.12.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">92.29</td>
<td id="S5.T4.12.12.8" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">3.03</td>
<td id="S5.T4.12.12.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">9.41</td>
<td id="S5.T4.12.12.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">69.49</td>
<td id="S5.T4.12.12.11" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">7.77</td>
<td id="S5.T4.12.12.12" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">0.36</td>
<td id="S5.T4.12.12.13" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">11.3</td>
<td id="S5.T4.12.12.14" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">90.0</td>
</tr>
<tr id="S5.T4.13.18" class="ltx_tr">
<td id="S5.T4.13.18.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">WavLM</td>
<td id="S5.T4.13.18.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">317.38</td>
<td id="S5.T4.13.18.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">Fairseq</td>
<td id="S5.T4.13.18.4" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.18.4.1" class="ltx_text ltx_font_bold">3.79</span></td>
<td id="S5.T4.13.18.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.18.5.1" class="ltx_text ltx_font_bold">8.26</span></td>
<td id="S5.T4.13.18.6" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">96.47</td>
<td id="S5.T4.13.18.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">96.08</td>
<td id="S5.T4.13.18.8" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.18.8.1" class="ltx_text ltx_font_bold">3.11</span></td>
<td id="S5.T4.13.18.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">9.44</td>
<td id="S5.T4.13.18.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">70.69</td>
<td id="S5.T4.13.18.11" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">7.86</td>
<td id="S5.T4.13.18.12" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">0.35</td>
<td id="S5.T4.13.18.13" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">11.2</td>
<td id="S5.T4.13.18.14" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">92.0</td>
</tr>
<tr id="S5.T4.13.13" class="ltx_tr">
<td id="S5.T4.13.13.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">HuBERT<math id="S5.T4.13.13.1.m1.1" class="ltx_Math" alttext="{}_{\text{ms}}" display="inline"><semantics id="S5.T4.13.13.1.m1.1a"><msub id="S5.T4.13.13.1.m1.1.1" xref="S5.T4.13.13.1.m1.1.1.cmml"><mi id="S5.T4.13.13.1.m1.1.1a" xref="S5.T4.13.13.1.m1.1.1.cmml"></mi><mtext id="S5.T4.13.13.1.m1.1.1.1" xref="S5.T4.13.13.1.m1.1.1.1a.cmml">ms</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T4.13.13.1.m1.1b"><apply id="S5.T4.13.13.1.m1.1.1.cmml" xref="S5.T4.13.13.1.m1.1.1"><ci id="S5.T4.13.13.1.m1.1.1.1a.cmml" xref="S5.T4.13.13.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T4.13.13.1.m1.1.1.1.cmml" xref="S5.T4.13.13.1.m1.1.1.1">ms</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.13.13.1.m1.1c">{}_{\text{ms}}</annotation></semantics></math>
</td>
<td id="S5.T4.13.13.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">317.38</td>
<td id="S5.T4.13.13.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">Mindspore</td>
<td id="S5.T4.13.13.4" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">4.07</td>
<td id="S5.T4.13.13.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">9.41</td>
<td id="S5.T4.13.13.6" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">90.49</td>
<td id="S5.T4.13.13.7" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">90.38</td>
<td id="S5.T4.13.13.8" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">3.00</td>
<td id="S5.T4.13.13.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">9.26</td>
<td id="S5.T4.13.13.10" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">67.13</td>
<td id="S5.T4.13.13.11" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">7.84</td>
<td id="S5.T4.13.13.12" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">0.34</td>
<td id="S5.T4.13.13.13" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">11.5</td>
<td id="S5.T4.13.13.14" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">90.0</td>
</tr>
<tr id="S5.T4.13.19" class="ltx_tr">
<td id="S5.T4.13.19.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.19.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">ProgRE</span></td>
<td id="S5.T4.13.19.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">319.72</td>
<td id="S5.T4.13.19.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">Mindspore</td>
<td id="S5.T4.13.19.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.5pt;padding-right:4.5pt;">3.86</td>
<td id="S5.T4.13.19.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;">8.64</td>
<td id="S5.T4.13.19.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.19.6.1" class="ltx_text ltx_font_bold">97.67</span></td>
<td id="S5.T4.13.19.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.19.7.1" class="ltx_text ltx_font_bold">97.61</span></td>
<td id="S5.T4.13.19.8" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.5pt;padding-right:4.5pt;">3.09</td>
<td id="S5.T4.13.19.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.19.9.1" class="ltx_text ltx_font_bold">9.45</span></td>
<td id="S5.T4.13.19.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.19.10.1" class="ltx_text ltx_font_bold">70.73</span></td>
<td id="S5.T4.13.19.11" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.19.11.1" class="ltx_text ltx_font_bold">7.74</span></td>
<td id="S5.T4.13.19.12" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.19.12.1" class="ltx_text ltx_font_bold">0.39</span></td>
<td id="S5.T4.13.19.13" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.19.13.1" class="ltx_text ltx_font_bold">9.5</span></td>
<td id="S5.T4.13.19.14" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="S5.T4.13.19.14.1" class="ltx_text ltx_font_bold">94.0</span></td>
</tr>
</table>
</figure>
<div id="S5.SS3.SSS2.p2" class="ltx_para">
<p id="S5.SS3.SSS2.p2.2" class="ltx_p">The results indicate that the insertion of the speaker extractor at different layers significantly impacts the model’s performance. Compared to the baseline, inserting the speaker extractor before the Transformer (0th layer) or after the <math id="S5.SS3.SSS2.p2.1.m1.3" class="ltx_Math" alttext="\{2,8,10\}" display="inline"><semantics id="S5.SS3.SSS2.p2.1.m1.3a"><mrow id="S5.SS3.SSS2.p2.1.m1.3.4.2" xref="S5.SS3.SSS2.p2.1.m1.3.4.1.cmml"><mo stretchy="false" id="S5.SS3.SSS2.p2.1.m1.3.4.2.1" xref="S5.SS3.SSS2.p2.1.m1.3.4.1.cmml">{</mo><mn id="S5.SS3.SSS2.p2.1.m1.1.1" xref="S5.SS3.SSS2.p2.1.m1.1.1.cmml">2</mn><mo id="S5.SS3.SSS2.p2.1.m1.3.4.2.2" xref="S5.SS3.SSS2.p2.1.m1.3.4.1.cmml">,</mo><mn id="S5.SS3.SSS2.p2.1.m1.2.2" xref="S5.SS3.SSS2.p2.1.m1.2.2.cmml">8</mn><mo id="S5.SS3.SSS2.p2.1.m1.3.4.2.3" xref="S5.SS3.SSS2.p2.1.m1.3.4.1.cmml">,</mo><mn id="S5.SS3.SSS2.p2.1.m1.3.3" xref="S5.SS3.SSS2.p2.1.m1.3.3.cmml">10</mn><mo stretchy="false" id="S5.SS3.SSS2.p2.1.m1.3.4.2.4" xref="S5.SS3.SSS2.p2.1.m1.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS2.p2.1.m1.3b"><set id="S5.SS3.SSS2.p2.1.m1.3.4.1.cmml" xref="S5.SS3.SSS2.p2.1.m1.3.4.2"><cn type="integer" id="S5.SS3.SSS2.p2.1.m1.1.1.cmml" xref="S5.SS3.SSS2.p2.1.m1.1.1">2</cn><cn type="integer" id="S5.SS3.SSS2.p2.1.m1.2.2.cmml" xref="S5.SS3.SSS2.p2.1.m1.2.2">8</cn><cn type="integer" id="S5.SS3.SSS2.p2.1.m1.3.3.cmml" xref="S5.SS3.SSS2.p2.1.m1.3.3">10</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS2.p2.1.m1.3c">\{2,8,10\}</annotation></semantics></math>-th layers leads to a degradation in the model’s performance on speaker identification.
This outcome can be attributed to the task characteristics of different Transformer layers, as the main branch employs the self-supervised strategy of HuBERT.
Previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> analyzed the task characteristics of different layers and found that the fourth, fifth, sixth, and eleventh layers play significant roles in speaker tasks, with the fourth layer being the most influential.
Hence, strengthening and removing that does not align with the task characteristics of the main branch will instead degrade the model’s performance on that task.
Regarding the 0th layer, since our lightweight speaker extractor lacks temporal modeling, it cannot effectively extract speaker information from the output after local-processing convolution. Furthermore, inserting the speaker extractor after the <math id="S5.SS3.SSS2.p2.2.m2.6" class="ltx_Math" alttext="\{0,2,6,8,10,12\}" display="inline"><semantics id="S5.SS3.SSS2.p2.2.m2.6a"><mrow id="S5.SS3.SSS2.p2.2.m2.6.7.2" xref="S5.SS3.SSS2.p2.2.m2.6.7.1.cmml"><mo stretchy="false" id="S5.SS3.SSS2.p2.2.m2.6.7.2.1" xref="S5.SS3.SSS2.p2.2.m2.6.7.1.cmml">{</mo><mn id="S5.SS3.SSS2.p2.2.m2.1.1" xref="S5.SS3.SSS2.p2.2.m2.1.1.cmml">0</mn><mo id="S5.SS3.SSS2.p2.2.m2.6.7.2.2" xref="S5.SS3.SSS2.p2.2.m2.6.7.1.cmml">,</mo><mn id="S5.SS3.SSS2.p2.2.m2.2.2" xref="S5.SS3.SSS2.p2.2.m2.2.2.cmml">2</mn><mo id="S5.SS3.SSS2.p2.2.m2.6.7.2.3" xref="S5.SS3.SSS2.p2.2.m2.6.7.1.cmml">,</mo><mn id="S5.SS3.SSS2.p2.2.m2.3.3" xref="S5.SS3.SSS2.p2.2.m2.3.3.cmml">6</mn><mo id="S5.SS3.SSS2.p2.2.m2.6.7.2.4" xref="S5.SS3.SSS2.p2.2.m2.6.7.1.cmml">,</mo><mn id="S5.SS3.SSS2.p2.2.m2.4.4" xref="S5.SS3.SSS2.p2.2.m2.4.4.cmml">8</mn><mo id="S5.SS3.SSS2.p2.2.m2.6.7.2.5" xref="S5.SS3.SSS2.p2.2.m2.6.7.1.cmml">,</mo><mn id="S5.SS3.SSS2.p2.2.m2.5.5" xref="S5.SS3.SSS2.p2.2.m2.5.5.cmml">10</mn><mo id="S5.SS3.SSS2.p2.2.m2.6.7.2.6" xref="S5.SS3.SSS2.p2.2.m2.6.7.1.cmml">,</mo><mn id="S5.SS3.SSS2.p2.2.m2.6.6" xref="S5.SS3.SSS2.p2.2.m2.6.6.cmml">12</mn><mo stretchy="false" id="S5.SS3.SSS2.p2.2.m2.6.7.2.7" xref="S5.SS3.SSS2.p2.2.m2.6.7.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS2.p2.2.m2.6b"><set id="S5.SS3.SSS2.p2.2.m2.6.7.1.cmml" xref="S5.SS3.SSS2.p2.2.m2.6.7.2"><cn type="integer" id="S5.SS3.SSS2.p2.2.m2.1.1.cmml" xref="S5.SS3.SSS2.p2.2.m2.1.1">0</cn><cn type="integer" id="S5.SS3.SSS2.p2.2.m2.2.2.cmml" xref="S5.SS3.SSS2.p2.2.m2.2.2">2</cn><cn type="integer" id="S5.SS3.SSS2.p2.2.m2.3.3.cmml" xref="S5.SS3.SSS2.p2.2.m2.3.3">6</cn><cn type="integer" id="S5.SS3.SSS2.p2.2.m2.4.4.cmml" xref="S5.SS3.SSS2.p2.2.m2.4.4">8</cn><cn type="integer" id="S5.SS3.SSS2.p2.2.m2.5.5.cmml" xref="S5.SS3.SSS2.p2.2.m2.5.5">10</cn><cn type="integer" id="S5.SS3.SSS2.p2.2.m2.6.6.cmml" xref="S5.SS3.SSS2.p2.2.m2.6.6">12</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS2.p2.2.m2.6c">\{0,2,6,8,10,12\}</annotation></semantics></math>-th layers leads to a decline in the main branch’s training efficacy, as observed in the degraded performance on the speech recognition task. In summary, the effectiveness of inserting residual extraction relies on the task characteristics of different Transformer layers obtained from the main branch training strategy.</p>
</div>
</section>
<section id="S5.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS3.SSS3.5.1.1" class="ltx_text">V-C</span>3 </span>Importance of Inserting the Pitch Extractor</h4>

<div id="S5.SS3.SSS3.p1" class="ltx_para">
<p id="S5.SS3.SSS3.p1.1" class="ltx_p">We explored the role of the pitch extractor in the Base version of our model, and the results are presented in TABLE <a href="#S5.T5" title="TABLE V ‣ V-C3 Importance of Inserting the Pitch Extractor ‣ V-C Ablation Study ‣ V Experiments and Results ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. The findings indicate that strengthening and then removing pitch variation can improve the model’s performance in both speech recognition and speaker identification tasks. This improvement can be attributed to the fact that pitch variation primarily contains intonation information, with minimal speaker and content information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. Consequently, removing pitch variation information from the main branch facilitates the subsequent extraction of speaker and content information.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Evaluation of inserting the pitch extractor.</figcaption>
<table id="S5.T5.6" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T5.2.2" class="ltx_tr">
<td id="S5.T5.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" rowspan="2"><span id="S5.T5.2.2.3.1" class="ltx_text">Method</span></td>
<td id="S5.T5.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" rowspan="2"><span id="S5.T5.2.2.4.1" class="ltx_text"><span id="S5.T5.2.2.4.1.1" class="ltx_text"></span> <span id="S5.T5.2.2.4.1.2" class="ltx_text">
<span id="S5.T5.2.2.4.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T5.2.2.4.1.2.1.1" class="ltx_tr">
<span id="S5.T5.2.2.4.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">Param.</span></span>
<span id="S5.T5.2.2.4.1.2.1.2" class="ltx_tr">
<span id="S5.T5.2.2.4.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">(M)</span></span>
</span></span> <span id="S5.T5.2.2.4.1.3" class="ltx_text"></span></span></td>
<td id="S5.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="2">ASR (WER) <math id="S5.T5.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T5.1.1.1.m1.1a"><mo stretchy="false" id="S5.T5.1.1.1.m1.1.1" xref="S5.T5.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.m1.1b"><ci id="S5.T5.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T5.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="2">SID (Acc) <math id="S5.T5.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S5.T5.2.2.2.m1.1a"><mo stretchy="false" id="S5.T5.2.2.2.m1.1.1" xref="S5.T5.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.2.m1.1b"><ci id="S5.T5.2.2.2.m1.1.1.cmml" xref="S5.T5.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S5.T5.6.7" class="ltx_tr">
<td id="S5.T5.6.7.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">test-clean</td>
<td id="S5.T5.6.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">test-other</td>
<td id="S5.T5.6.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">dev</td>
<td id="S5.T5.6.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">test</td>
</tr>
<tr id="S5.T5.6.8" class="ltx_tr">
<td id="S5.T5.6.8.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">baseline</td>
<td id="S5.T5.6.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">94.70</td>
<td id="S5.T5.6.8.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">6.85</td>
<td id="S5.T5.6.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">16.77</td>
<td id="S5.T5.6.8.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">81.01</td>
<td id="S5.T5.6.8.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">79.94</td>
</tr>
<tr id="S5.T5.3.3" class="ltx_tr">
<td id="S5.T5.3.3.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">
<math id="S5.T5.3.3.1.m1.1" class="ltx_Math" alttext="\circleddash" display="inline"><semantics id="S5.T5.3.3.1.m1.1a"><mo id="S5.T5.3.3.1.m1.1.1" xref="S5.T5.3.3.1.m1.1.1.cmml">⊝</mo><annotation-xml encoding="MathML-Content" id="S5.T5.3.3.1.m1.1b"><ci id="S5.T5.3.3.1.m1.1.1.cmml" xref="S5.T5.3.3.1.m1.1.1">⊝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.3.1.m1.1c">\circleddash</annotation></semantics></math> pitch</td>
<td id="S5.T5.3.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">95.47</td>
<td id="S5.T5.3.3.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">6.74</td>
<td id="S5.T5.3.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">16.38</td>
<td id="S5.T5.3.3.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">81.66</td>
<td id="S5.T5.3.3.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">80.03</td>
</tr>
<tr id="S5.T5.4.4" class="ltx_tr">
<td id="S5.T5.4.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">
<math id="S5.T5.4.4.1.m1.1" class="ltx_Math" alttext="\circleddash" display="inline"><semantics id="S5.T5.4.4.1.m1.1a"><mo id="S5.T5.4.4.1.m1.1.1" xref="S5.T5.4.4.1.m1.1.1.cmml">⊝</mo><annotation-xml encoding="MathML-Content" id="S5.T5.4.4.1.m1.1b"><ci id="S5.T5.4.4.1.m1.1.1.cmml" xref="S5.T5.4.4.1.m1.1.1">⊝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.4.4.1.m1.1c">\circleddash</annotation></semantics></math> speaker</td>
<td id="S5.T5.4.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">96.27</td>
<td id="S5.T5.4.4.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">6.67</td>
<td id="S5.T5.4.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">16.04</td>
<td id="S5.T5.4.4.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">88.89</td>
<td id="S5.T5.4.4.6" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;">87.64</td>
</tr>
<tr id="S5.T5.6.6" class="ltx_tr">
<td id="S5.T5.6.6.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">
<math id="S5.T5.5.5.1.m1.1" class="ltx_Math" alttext="\circleddash" display="inline"><semantics id="S5.T5.5.5.1.m1.1a"><mo id="S5.T5.5.5.1.m1.1.1" xref="S5.T5.5.5.1.m1.1.1.cmml">⊝</mo><annotation-xml encoding="MathML-Content" id="S5.T5.5.5.1.m1.1b"><ci id="S5.T5.5.5.1.m1.1.1.cmml" xref="S5.T5.5.5.1.m1.1.1">⊝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.5.5.1.m1.1c">\circleddash</annotation></semantics></math> speaker <math id="S5.T5.6.6.2.m2.1" class="ltx_Math" alttext="\circleddash" display="inline"><semantics id="S5.T5.6.6.2.m2.1a"><mo id="S5.T5.6.6.2.m2.1.1" xref="S5.T5.6.6.2.m2.1.1.cmml">⊝</mo><annotation-xml encoding="MathML-Content" id="S5.T5.6.6.2.m2.1b"><ci id="S5.T5.6.6.2.m2.1.1.cmml" xref="S5.T5.6.6.2.m2.1.1">⊝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.6.6.2.m2.1c">\circleddash</annotation></semantics></math> pitch</td>
<td id="S5.T5.6.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">97.04</td>
<td id="S5.T5.6.6.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T5.6.6.4.1" class="ltx_text ltx_font_bold">6.52</span></td>
<td id="S5.T5.6.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T5.6.6.5.1" class="ltx_text ltx_font_bold">15.20</span></td>
<td id="S5.T5.6.6.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T5.6.6.6.1" class="ltx_text ltx_font_bold">90.95</span></td>
<td id="S5.T5.6.6.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T5.6.6.7.1" class="ltx_text ltx_font_bold">90.61</span></td>
</tr>
</table>
</figure>
</section>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.5.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.6.2" class="ltx_text ltx_font_italic">Comparing SSL Models on Various Downstream Tasks</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.4" class="ltx_p">In order to verify the performance of our proposed method on various downstream tasks, we conducted a comparison with existing open-source pre-trained models on speech recognition, speaker identification, speech enhancement, emotion recognition, and voice conversion tasks. The results are shown in TABLE <a href="#S5.T4" title="TABLE IV ‣ V-C2 Importance of Inserting the Speaker Extractor ‣ V-C Ablation Study ‣ V Experiments and Results ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. Comparing the results of HuBERT<math id="S5.SS4.p1.1.m1.1" class="ltx_Math" alttext="{}_{\text{ms}}" display="inline"><semantics id="S5.SS4.p1.1.m1.1a"><msub id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml"><mi id="S5.SS4.p1.1.m1.1.1a" xref="S5.SS4.p1.1.m1.1.1.cmml"></mi><mtext id="S5.SS4.p1.1.m1.1.1.1" xref="S5.SS4.p1.1.m1.1.1.1a.cmml">ms</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><apply id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1"><ci id="S5.SS4.p1.1.m1.1.1.1a.cmml" xref="S5.SS4.p1.1.m1.1.1.1"><mtext mathsize="70%" id="S5.SS4.p1.1.m1.1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1.1">ms</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">{}_{\text{ms}}</annotation></semantics></math> and HuBERT<math id="S5.SS4.p1.2.m2.1" class="ltx_Math" alttext="{}_{\text{pt}}" display="inline"><semantics id="S5.SS4.p1.2.m2.1a"><msub id="S5.SS4.p1.2.m2.1.1" xref="S5.SS4.p1.2.m2.1.1.cmml"><mi id="S5.SS4.p1.2.m2.1.1a" xref="S5.SS4.p1.2.m2.1.1.cmml"></mi><mtext id="S5.SS4.p1.2.m2.1.1.1" xref="S5.SS4.p1.2.m2.1.1.1a.cmml">pt</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.2.m2.1b"><apply id="S5.SS4.p1.2.m2.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1"><ci id="S5.SS4.p1.2.m2.1.1.1a.cmml" xref="S5.SS4.p1.2.m2.1.1.1"><mtext mathsize="70%" id="S5.SS4.p1.2.m2.1.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1.1">pt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.2.m2.1c">{}_{\text{pt}}</annotation></semantics></math>, it shows that the pre-trained HuBERT based on the MindSpore loses accuracy when migrating to PyTorch, resulting in slight performance degradation on each downstream task. Despite this disadvantage, our proposed method still demonstrates SOTA performance on most tasks. Note that HuBERT<math id="S5.SS4.p1.3.m3.1" class="ltx_Math" alttext="{}_{\text{ms}}" display="inline"><semantics id="S5.SS4.p1.3.m3.1a"><msub id="S5.SS4.p1.3.m3.1.1" xref="S5.SS4.p1.3.m3.1.1.cmml"><mi id="S5.SS4.p1.3.m3.1.1a" xref="S5.SS4.p1.3.m3.1.1.cmml"></mi><mtext id="S5.SS4.p1.3.m3.1.1.1" xref="S5.SS4.p1.3.m3.1.1.1a.cmml">ms</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.3.m3.1b"><apply id="S5.SS4.p1.3.m3.1.1.cmml" xref="S5.SS4.p1.3.m3.1.1"><ci id="S5.SS4.p1.3.m3.1.1.1a.cmml" xref="S5.SS4.p1.3.m3.1.1.1"><mtext mathsize="70%" id="S5.SS4.p1.3.m3.1.1.1.cmml" xref="S5.SS4.p1.3.m3.1.1.1">ms</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.3.m3.1c">{}_{\text{ms}}</annotation></semantics></math> is used as our baseline instead of HuBERT<math id="S5.SS4.p1.4.m4.1" class="ltx_Math" alttext="{}_{\text{pt}}" display="inline"><semantics id="S5.SS4.p1.4.m4.1a"><msub id="S5.SS4.p1.4.m4.1.1" xref="S5.SS4.p1.4.m4.1.1.cmml"><mi id="S5.SS4.p1.4.m4.1.1a" xref="S5.SS4.p1.4.m4.1.1.cmml"></mi><mtext id="S5.SS4.p1.4.m4.1.1.1" xref="S5.SS4.p1.4.m4.1.1.1a.cmml">pt</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.4.m4.1b"><apply id="S5.SS4.p1.4.m4.1.1.cmml" xref="S5.SS4.p1.4.m4.1.1"><ci id="S5.SS4.p1.4.m4.1.1.1a.cmml" xref="S5.SS4.p1.4.m4.1.1.1"><mtext mathsize="70%" id="S5.SS4.p1.4.m4.1.1.1.cmml" xref="S5.SS4.p1.4.m4.1.1.1">pt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.4.m4.1c">{}_{\text{pt}}</annotation></semantics></math>. In addition, for the Large version of <span id="S5.SS4.p1.4.1" class="ltx_text ltx_font_smallcaps">ProgRE</span>, we inserted the speaker extractor after the 6th layer Transformer.</p>
</div>
<section id="S5.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS4.SSS1.5.1.1" class="ltx_text">V-D</span>1 </span>Speech Recognition</h4>

<div id="S5.SS4.SSS1.p1" class="ltx_para">
<p id="S5.SS4.SSS1.p1.1" class="ltx_p">We compared the models’ ability to content understanding via fine-tuning models on speech recognition.</p>
</div>
<div id="S5.SS4.SSS1.p2" class="ltx_para">
<p id="S5.SS4.SSS1.p2.1" class="ltx_p">In the Base version models, compared to HuBERT<math id="S5.SS4.SSS1.p2.1.m1.1" class="ltx_Math" alttext="{}_{\text{ms}}" display="inline"><semantics id="S5.SS4.SSS1.p2.1.m1.1a"><msub id="S5.SS4.SSS1.p2.1.m1.1.1" xref="S5.SS4.SSS1.p2.1.m1.1.1.cmml"><mi id="S5.SS4.SSS1.p2.1.m1.1.1a" xref="S5.SS4.SSS1.p2.1.m1.1.1.cmml"></mi><mtext id="S5.SS4.SSS1.p2.1.m1.1.1.1" xref="S5.SS4.SSS1.p2.1.m1.1.1.1a.cmml">ms</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.1.m1.1b"><apply id="S5.SS4.SSS1.p2.1.m1.1.1.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1"><ci id="S5.SS4.SSS1.p2.1.m1.1.1.1a.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.1"><mtext mathsize="70%" id="S5.SS4.SSS1.p2.1.m1.1.1.1.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.1">ms</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.1.m1.1c">{}_{\text{ms}}</annotation></semantics></math>, our <span id="S5.SS4.SSS1.p2.1.1" class="ltx_text ltx_font_smallcaps">ProgRE</span> achieves a relative WER reduction of 8.04%, our <span id="S5.SS4.SSS1.p2.1.2" class="ltx_text ltx_font_smallcaps">ProgRE</span> even outperforms WavLM implemented by Fairseq on test-other, indicating that residual extraction of the pitch variation and speaker information can effectively facilitate the learning of irrelevant content information.</p>
</div>
<div id="S5.SS4.SSS1.p3" class="ltx_para">
<p id="S5.SS4.SSS1.p3.6" class="ltx_p">In the large version models, in addition to our proposed <span id="S5.SS4.SSS1.p3.6.1" class="ltx_text ltx_font_smallcaps">ProgRE</span>, we also pre-trained HuBERT<math id="S5.SS4.SSS1.p3.1.m1.1" class="ltx_Math" alttext="{}_{\text{ms}}" display="inline"><semantics id="S5.SS4.SSS1.p3.1.m1.1a"><msub id="S5.SS4.SSS1.p3.1.m1.1.1" xref="S5.SS4.SSS1.p3.1.m1.1.1.cmml"><mi id="S5.SS4.SSS1.p3.1.m1.1.1a" xref="S5.SS4.SSS1.p3.1.m1.1.1.cmml"></mi><mtext id="S5.SS4.SSS1.p3.1.m1.1.1.1" xref="S5.SS4.SSS1.p3.1.m1.1.1.1a.cmml">ms</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p3.1.m1.1b"><apply id="S5.SS4.SSS1.p3.1.m1.1.1.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1"><ci id="S5.SS4.SSS1.p3.1.m1.1.1.1a.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1.1"><mtext mathsize="70%" id="S5.SS4.SSS1.p3.1.m1.1.1.1.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1.1">ms</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p3.1.m1.1c">{}_{\text{ms}}</annotation></semantics></math> based on the MindSpore framework on our 84,500 hours of bilingual English-Chinese data for comparison.
The results indicate that the 84,500 hours of bilingual data did not bring a significant improvement in performance compared with LibriLight’s 60,000 hours.
The amount of English data used in pre-training is 15,500 hours less than that of HuBERT<math id="S5.SS4.SSS1.p3.2.m2.1" class="ltx_Math" alttext="{}_{\text{pt}}" display="inline"><semantics id="S5.SS4.SSS1.p3.2.m2.1a"><msub id="S5.SS4.SSS1.p3.2.m2.1.1" xref="S5.SS4.SSS1.p3.2.m2.1.1.cmml"><mi id="S5.SS4.SSS1.p3.2.m2.1.1a" xref="S5.SS4.SSS1.p3.2.m2.1.1.cmml"></mi><mtext id="S5.SS4.SSS1.p3.2.m2.1.1.1" xref="S5.SS4.SSS1.p3.2.m2.1.1.1a.cmml">pt</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p3.2.m2.1b"><apply id="S5.SS4.SSS1.p3.2.m2.1.1.cmml" xref="S5.SS4.SSS1.p3.2.m2.1.1"><ci id="S5.SS4.SSS1.p3.2.m2.1.1.1a.cmml" xref="S5.SS4.SSS1.p3.2.m2.1.1.1"><mtext mathsize="70%" id="S5.SS4.SSS1.p3.2.m2.1.1.1.cmml" xref="S5.SS4.SSS1.p3.2.m2.1.1.1">pt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p3.2.m2.1c">{}_{\text{pt}}</annotation></semantics></math>. Coupled with the limitations of the MindSpore framework, HuBERT<math id="S5.SS4.SSS1.p3.3.m3.1" class="ltx_Math" alttext="{}_{\text{ms}}" display="inline"><semantics id="S5.SS4.SSS1.p3.3.m3.1a"><msub id="S5.SS4.SSS1.p3.3.m3.1.1" xref="S5.SS4.SSS1.p3.3.m3.1.1.cmml"><mi id="S5.SS4.SSS1.p3.3.m3.1.1a" xref="S5.SS4.SSS1.p3.3.m3.1.1.cmml"></mi><mtext id="S5.SS4.SSS1.p3.3.m3.1.1.1" xref="S5.SS4.SSS1.p3.3.m3.1.1.1a.cmml">ms</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p3.3.m3.1b"><apply id="S5.SS4.SSS1.p3.3.m3.1.1.cmml" xref="S5.SS4.SSS1.p3.3.m3.1.1"><ci id="S5.SS4.SSS1.p3.3.m3.1.1.1a.cmml" xref="S5.SS4.SSS1.p3.3.m3.1.1.1"><mtext mathsize="70%" id="S5.SS4.SSS1.p3.3.m3.1.1.1.cmml" xref="S5.SS4.SSS1.p3.3.m3.1.1.1">ms</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p3.3.m3.1c">{}_{\text{ms}}</annotation></semantics></math>’s ASR ability in English is worse than that of HuBERT<math id="S5.SS4.SSS1.p3.4.m4.1" class="ltx_Math" alttext="{}_{\text{pt}}" display="inline"><semantics id="S5.SS4.SSS1.p3.4.m4.1a"><msub id="S5.SS4.SSS1.p3.4.m4.1.1" xref="S5.SS4.SSS1.p3.4.m4.1.1.cmml"><mi id="S5.SS4.SSS1.p3.4.m4.1.1a" xref="S5.SS4.SSS1.p3.4.m4.1.1.cmml"></mi><mtext id="S5.SS4.SSS1.p3.4.m4.1.1.1" xref="S5.SS4.SSS1.p3.4.m4.1.1.1a.cmml">pt</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p3.4.m4.1b"><apply id="S5.SS4.SSS1.p3.4.m4.1.1.cmml" xref="S5.SS4.SSS1.p3.4.m4.1.1"><ci id="S5.SS4.SSS1.p3.4.m4.1.1.1a.cmml" xref="S5.SS4.SSS1.p3.4.m4.1.1.1"><mtext mathsize="70%" id="S5.SS4.SSS1.p3.4.m4.1.1.1.cmml" xref="S5.SS4.SSS1.p3.4.m4.1.1.1">pt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p3.4.m4.1c">{}_{\text{pt}}</annotation></semantics></math>. Although additional experiments<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/wangtianrui/ProgRE/blob/master/supplementary_results/README.md#chinese-asr" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/wangtianrui/ProgRE/blob/master/supplementary_results/README.md#chinese-asr</a></span></span></span> showed that HuBERT<math id="S5.SS4.SSS1.p3.5.m5.1" class="ltx_Math" alttext="{}_{\text{ms}}" display="inline"><semantics id="S5.SS4.SSS1.p3.5.m5.1a"><msub id="S5.SS4.SSS1.p3.5.m5.1.1" xref="S5.SS4.SSS1.p3.5.m5.1.1.cmml"><mi id="S5.SS4.SSS1.p3.5.m5.1.1a" xref="S5.SS4.SSS1.p3.5.m5.1.1.cmml"></mi><mtext id="S5.SS4.SSS1.p3.5.m5.1.1.1" xref="S5.SS4.SSS1.p3.5.m5.1.1.1a.cmml">ms</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p3.5.m5.1b"><apply id="S5.SS4.SSS1.p3.5.m5.1.1.cmml" xref="S5.SS4.SSS1.p3.5.m5.1.1"><ci id="S5.SS4.SSS1.p3.5.m5.1.1.1a.cmml" xref="S5.SS4.SSS1.p3.5.m5.1.1.1"><mtext mathsize="70%" id="S5.SS4.SSS1.p3.5.m5.1.1.1.cmml" xref="S5.SS4.SSS1.p3.5.m5.1.1.1">ms</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p3.5.m5.1c">{}_{\text{ms}}</annotation></semantics></math> performs better than HuBERT<math id="S5.SS4.SSS1.p3.6.m6.1" class="ltx_Math" alttext="{}_{\text{pt}}" display="inline"><semantics id="S5.SS4.SSS1.p3.6.m6.1a"><msub id="S5.SS4.SSS1.p3.6.m6.1.1" xref="S5.SS4.SSS1.p3.6.m6.1.1.cmml"><mi id="S5.SS4.SSS1.p3.6.m6.1.1a" xref="S5.SS4.SSS1.p3.6.m6.1.1.cmml"></mi><mtext id="S5.SS4.SSS1.p3.6.m6.1.1.1" xref="S5.SS4.SSS1.p3.6.m6.1.1.1a.cmml">pt</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p3.6.m6.1b"><apply id="S5.SS4.SSS1.p3.6.m6.1.1.cmml" xref="S5.SS4.SSS1.p3.6.m6.1.1"><ci id="S5.SS4.SSS1.p3.6.m6.1.1.1a.cmml" xref="S5.SS4.SSS1.p3.6.m6.1.1.1"><mtext mathsize="70%" id="S5.SS4.SSS1.p3.6.m6.1.1.1.cmml" xref="S5.SS4.SSS1.p3.6.m6.1.1.1">pt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p3.6.m6.1c">{}_{\text{pt}}</annotation></semantics></math> in Chinese ASR, it is unfair to compare this to Fairseq models that have not seen Chinese data, so those results are not shown in this paper. Despite these challenges, the proposed model still shows excellent performance, second only to WavLM pre-trained on 94,000 hours of English-only pertaining data, which proves that the progressive residual extraction strategy is also applicable to large-scale pre-training tasks.</p>
</div>
</section>
<section id="S5.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS4.SSS2.5.1.1" class="ltx_text">V-D</span>2 </span>Speaker Identification</h4>

<div id="S5.SS4.SSS2.p1" class="ltx_para">
<p id="S5.SS4.SSS2.p1.1" class="ltx_p">We assessed the model’s capacity to extract speaker information through speaker identification tasks. The results demonstrate that our proposed method achieves SOTA performance in both the Base and Large version groups, showcasing substantial enhancements compared to HuBERT<math id="S5.SS4.SSS2.p1.1.m1.1" class="ltx_Math" alttext="{}_{\text{ms}}" display="inline"><semantics id="S5.SS4.SSS2.p1.1.m1.1a"><msub id="S5.SS4.SSS2.p1.1.m1.1.1" xref="S5.SS4.SSS2.p1.1.m1.1.1.cmml"><mi id="S5.SS4.SSS2.p1.1.m1.1.1a" xref="S5.SS4.SSS2.p1.1.m1.1.1.cmml"></mi><mtext id="S5.SS4.SSS2.p1.1.m1.1.1.1" xref="S5.SS4.SSS2.p1.1.m1.1.1.1a.cmml">ms</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p1.1.m1.1b"><apply id="S5.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S5.SS4.SSS2.p1.1.m1.1.1"><ci id="S5.SS4.SSS2.p1.1.m1.1.1.1a.cmml" xref="S5.SS4.SSS2.p1.1.m1.1.1.1"><mtext mathsize="70%" id="S5.SS4.SSS2.p1.1.m1.1.1.1.cmml" xref="S5.SS4.SSS2.p1.1.m1.1.1.1">ms</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p1.1.m1.1c">{}_{\text{ms}}</annotation></semantics></math>. This improvement in speaker information extraction can be attributed to the residual insertion of the speaker extractor at an optimal position, which ensures the effectiveness of main branch training while significantly boosting the insertion layer’s ability to extract speaker information.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2409.00387/assets/x5.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="560" height="246" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Layer-wise weight visualization in the weighted-sum mechanism of HuBERT and <span id="S5.F5.3.1" class="ltx_text ltx_font_smallcaps">ProgRE</span>. The first row weights come from HuBERT, and the second row comes from <span id="S5.F5.4.2" class="ltx_text ltx_font_smallcaps">ProgRE</span>. We show weights fine-tuned on ASR and SID tasks for both Base and Large version models (left column is Base version, right column is Large version).</figcaption>
</figure>
</section>
<section id="S5.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS4.SSS3.5.1.1" class="ltx_text">V-D</span>3 </span>Speech Enhancement</h4>

<div id="S5.SS4.SSS3.p1" class="ltx_para">
<p id="S5.SS4.SSS3.p1.1" class="ltx_p">Speech enhancement requires extracting clean and detailed acoustic information from noisy input, so the ability of the pre-trained model to extract various speech information is comprehensively evaluated. The results show that our <span id="S5.SS4.SSS3.p1.1.1" class="ltx_text ltx_font_smallcaps">ProgRE</span> achieves the best performance among the Base version models. This is because WavLM Base does not introduce denoising during pre-training, thus <span id="S5.SS4.SSS3.p1.1.2" class="ltx_text ltx_font_smallcaps">ProgRE</span>, with its superior content information extraction, speaker information extraction, and pitch information extraction abilities, achieves the best speech enhancement performance among the Base models. In the Large version models, the overall PESQ metric is improved compared to the Base, but the SI-SDR has decreased. We speculate that this is because the features extracted by the Large model are more abstract with refined semantic information, making the enhanced speech more intelligible to human ears but causing distortion in numerical acoustic details. WavLM Large introduced denoising during pre-training, and combined with its excellent performance in the speech recognition task, it achieves the highest PESQ score. The PESQ score of our proposed method is slightly lower than that of WavLM Large. However, by strengthening the extraction of pitch and speaker information closer to the speech signal, our method <span id="S5.SS4.SSS3.p1.1.3" class="ltx_text ltx_font_smallcaps">ProgRE</span> Large achieves a slightly higher SI-SDR score compared to WavLM Large.</p>
</div>
</section>
<section id="S5.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS4.SSS4.5.1.1" class="ltx_text">V-D</span>4 </span>Speech Emotion Recognition</h4>

<div id="S5.SS4.SSS4.p1" class="ltx_para">
<p id="S5.SS4.SSS4.p1.1" class="ltx_p">Since emotive expression and content are interrelated in the IEMOCAP dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, the performance of speech emotion recognition reflects both the model’s ability to content understanding and its ability to extract paralinguistic information. The results show that the proposed model achieves the best performance under both Base and Large configurations. This improvement can be attributed to the residual extraction of pitch variation information, which allows the model to capture more intonation and tonal details while also extracting refined and accurate content information.</p>
</div>
</section>
<section id="S5.SS4.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S5.SS4.SSS5.5.1.1" class="ltx_text">V-D</span>5 </span>Voice Conversion</h4>

<div id="S5.SS4.SSS5.p1" class="ltx_para">
<p id="S5.SS4.SSS5.p1.1" class="ltx_p">We conducted an any-to-one voice conversion experiment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. The model needs to remove the intonation and speaker information from the input speech and then generate the speech of the target speaker based on the remaining content information, thus serving as a test of the model’s capability to disentangle speech information. In addition to objective evaluation, we also present audio samples on the demo page<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://wangtianrui.github.io/progre_vc" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://wangtianrui.github.io/progre_vc</a></span></span></span>. From the results, we can see that <span id="S5.SS4.SSS5.p1.1.1" class="ltx_text ltx_font_smallcaps">ProgRE</span> achieves a significantly higher speaker verification pass rate (SPK) and F0 correlation (F0C) compared to other reference models, indicating that the content representations extracted by <span id="S5.SS4.SSS5.p1.1.2" class="ltx_text ltx_font_smallcaps">ProgRE</span> contain fewer speaker and intonation information (an in-depth analysis can be found in Section <a href="#S5.SS5" title="V-E Layer-wise Weight in Weighted-sum Mechanism ‣ V Experiments and Results ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-E</span></span></a>). This proves the effective information removal of residual extraction in <span id="S5.SS4.SSS5.p1.1.3" class="ltx_text ltx_font_smallcaps">ProgRE</span>. Moreover, the superior performance on the WER metric suggests that the <span id="S5.SS4.SSS5.p1.1.4" class="ltx_text ltx_font_smallcaps">ProgRE</span>-based VC model can retain more complete content information. The SOTA overall performance is due to <span id="S5.SS4.SSS5.p1.1.5" class="ltx_text ltx_font_smallcaps">ProgRE</span> extracting pitch variation and speaker information residually at optimal layers, making the intonation, speaker, and content information extracted by our model more independent, enhancing the disentanglement of speech information.</p>
</div>
<div id="S5.SS4.SSS5.p2" class="ltx_para">
<p id="S5.SS4.SSS5.p2.1" class="ltx_p">Unexpectedly, the performances of the Large models are generally slightly worse than those of the Base models. We speculate that this is because the downstream model is too small to fit the high-dimensional features extracted by the Large models, making it difficult for the model to converge. Nevertheless, our proposed method still shows the best performance among the Large models, indicating that the content information extracted by our proposed model is more refined and easier for the downstream model to learn.</p>
</div>
</section>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS5.5.1.1" class="ltx_text">V-E</span> </span><span id="S5.SS5.6.2" class="ltx_text ltx_font_italic">Layer-wise Weight in Weighted-sum Mechanism</span>
</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">In order to explore the task characteristics of features at different layers, we visualized the weights in the weighted-sum mechanism, as shown in Fig. <a href="#S5.F5" title="Figure 5 ‣ V-D2 Speaker Identification ‣ V-D Comparing SSL Models on Various Downstream Tasks ‣ V Experiments and Results ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Since the numerical distribution of our proposed method is extreme, we cropped the value by 0.45 when drawing the weights of <span id="S5.SS5.p1.1.1" class="ltx_text ltx_font_smallcaps">ProgRE</span>, as shown in green text. To more intuitively show the difference in values, we marked the top-2 weights.
As can be seen from Fig. <a href="#S5.F5" title="Figure 5 ‣ V-D2 Speaker Identification ‣ V-D Comparing SSL Models on Various Downstream Tasks ‣ V Experiments and Results ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, consistent with the findings in other papers, the HuBERT model exhibits different task characteristics at different layers.
For content understanding tasks such as speech identification, the weights of the features extracted by the deep Transformer layers are high. Conversely, for the extraction of non-linguistic information such as speaker recognition, the weights of the features extracted by the shallow layers play major roles.
Compared with HuBERT, the weights of our method on the speaker identification task are concentrated in the layer where the speaker extractor is inserted, and the weights of the other layers are close to zero. This demonstrates that residual extraction can effectively remove information from the main branch, allowing subsequent layers to focus on speaker-irrelevant tasks.
Additionally, the weights of the proposed method on the speech recognition task are similar to the weight distribution of the original HuBERT, further proving that adding residual extraction of corresponding task information at the appropriate layer does not change the task characteristics distribution of the main branch layer for other irrelevant tasks.
For speech enhancement, the overall weight distribution of <span id="S5.SS5.p1.1.2" class="ltx_text ltx_font_smallcaps">ProgRE</span> is similar to that of HuBERT, with weights gradually decreasing from shallow to deep layers. For emotion recognition, pitch variation representation and content-related layers play key roles. This is because pitch variation contains emotion-related intonation information. For voice conversion, the shallow-layer weights of HuBERT and <span id="S5.SS5.p1.1.3" class="ltx_text ltx_font_smallcaps">ProgRE</span> are slightly larger than deep-layer’s weights. However, the weights of the pitch representation and speaker feature layers in <span id="S5.SS5.p1.1.4" class="ltx_text ltx_font_smallcaps">ProgRE</span> are low. Combined with the results of Section <a href="#S5.SS4.SSS5" title="V-D5 Voice Conversion ‣ V-D Comparing SSL Models on Various Downstream Tasks ‣ V Experiments and Results ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-D</span>5</span></a>, our method can reduce the intonation and speaker information in the weighted-sum representation by decreasing the weights of the pitch and speaker extractor layers. This also implies that the intonation and speaker information contained in the representations of other layers is less than that of HuBERT.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Discussion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this study, we explored the effectiveness of residual extraction in speech self-supervised pre-training, highlighting that the task characteristics of different layers facilitated by the pre-training strategy are crucial for achieving joint improvement across various downstream tasks.
Previous research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> has shown that actively normalizing speaker information in speech recognition models can effectively enhance performance on ASR. Consistent with these findings, our experimental results demonstrate that actively removing content-irrelevant speech information from the main branch, such as speaker information or pitch variation, can improve the model’s ability to extract content information.
This validates that residual extraction is essential for a single SSL model to adapt to various downstream tasks with diverse demands for different types of speech information. Moreover, as illustrated in TABLE <a href="#S5.T3" title="TABLE III ‣ V-C2 Importance of Inserting the Speaker Extractor ‣ V-C Ablation Study ‣ V Experiments and Results ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> and Fig. <a href="#S5.F5" title="Figure 5 ‣ V-D2 Speaker Identification ‣ V-D Comparing SSL Models on Various Downstream Tasks ‣ V Experiments and Results ‣ Progressive Residual Extraction based Pre-training for Speech Representation Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the strengthening of pre-training model tasks should align with the layer-specific task characteristics facilitated by the main branch pre-training strategy.
Deviating from these task characteristics can detrimentally affect the main branch’s training efficacy, resulting in degraded performance.
To verify the practicality of our proposed method, we expanded the dataset to 84,500 hours of English-Chinese bilingual data. The experimental results demonstrate that the proposed method is adaptable to large-scale pre-training tasks, achieving joint performance improvements across various tasks. Notably, <span id="S6.p1.1.1" class="ltx_text ltx_font_smallcaps">ProgRE</span> exhibits state-of-the-art performance in speaker information extraction and speech information disentanglement capabilities.
This confirms our hypothesis: to make the pre-training model more universal, it is essential to enhance specific capabilities while minimizing the interference of these strengthened features with other irrelevant tasks. Our findings provide a significant reference for the development of universal pre-training models.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.3" class="ltx_p">This paper also points out some interesting issues that need further exploration. First, after additional experimental verification, we found that our model trained under the MindSpore framework incurs a numerical relative error of 0.25% when being migrated to the PyTorch framework, slightly affecting the model’s performance during fine-tuning. To address this, we have open-sourced our code to encourage other researchers to try it under the PyTorch framework. Second, for the Large version model, we used 44,500 hours of English data and 40,000 hours of low-quality Chinese data. From the comparative experiment of HuBERT<math id="S6.p2.1.m1.1" class="ltx_Math" alttext="{}_{\text{pt}}" display="inline"><semantics id="S6.p2.1.m1.1a"><msub id="S6.p2.1.m1.1.1" xref="S6.p2.1.m1.1.1.cmml"><mi id="S6.p2.1.m1.1.1a" xref="S6.p2.1.m1.1.1.cmml"></mi><mtext id="S6.p2.1.m1.1.1.1" xref="S6.p2.1.m1.1.1.1a.cmml">pt</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.p2.1.m1.1b"><apply id="S6.p2.1.m1.1.1.cmml" xref="S6.p2.1.m1.1.1"><ci id="S6.p2.1.m1.1.1.1a.cmml" xref="S6.p2.1.m1.1.1.1"><mtext mathsize="70%" id="S6.p2.1.m1.1.1.1.cmml" xref="S6.p2.1.m1.1.1.1">pt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.m1.1c">{}_{\text{pt}}</annotation></semantics></math> and HuBERT<math id="S6.p2.2.m2.1" class="ltx_Math" alttext="{}_{\text{ms}}" display="inline"><semantics id="S6.p2.2.m2.1a"><msub id="S6.p2.2.m2.1.1" xref="S6.p2.2.m2.1.1.cmml"><mi id="S6.p2.2.m2.1.1a" xref="S6.p2.2.m2.1.1.cmml"></mi><mtext id="S6.p2.2.m2.1.1.1" xref="S6.p2.2.m2.1.1.1a.cmml">ms</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.p2.2.m2.1b"><apply id="S6.p2.2.m2.1.1.cmml" xref="S6.p2.2.m2.1.1"><ci id="S6.p2.2.m2.1.1.1a.cmml" xref="S6.p2.2.m2.1.1.1"><mtext mathsize="70%" id="S6.p2.2.m2.1.1.1.cmml" xref="S6.p2.2.m2.1.1.1">ms</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.2.m2.1c">{}_{\text{ms}}</annotation></semantics></math>, we observed that although our total dataset is larger than that of HuBERT<math id="S6.p2.3.m3.1" class="ltx_Math" alttext="{}_{\text{pt}}" display="inline"><semantics id="S6.p2.3.m3.1a"><msub id="S6.p2.3.m3.1.1" xref="S6.p2.3.m3.1.1.cmml"><mi id="S6.p2.3.m3.1.1a" xref="S6.p2.3.m3.1.1.cmml"></mi><mtext id="S6.p2.3.m3.1.1.1" xref="S6.p2.3.m3.1.1.1a.cmml">pt</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.p2.3.m3.1b"><apply id="S6.p2.3.m3.1.1.cmml" xref="S6.p2.3.m3.1.1"><ci id="S6.p2.3.m3.1.1.1a.cmml" xref="S6.p2.3.m3.1.1.1"><mtext mathsize="70%" id="S6.p2.3.m3.1.1.1.cmml" xref="S6.p2.3.m3.1.1.1">pt</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.3.m3.1c">{}_{\text{pt}}</annotation></semantics></math>, which was trained with 60,000 hours of English-only data, the performance in downstream fine-tuning tasks for English is declined. We speculate that speech quality and language differences significantly impact the performance of pre-trained models. Third, for a fair comparison, the speaker-teacher model in our <span id="S6.p2.3.1" class="ltx_text ltx_font_smallcaps">ProgRE</span> is limited to a self-supervised model on the pre-training data. Exploring the possibility of directly using a powerful supervised model is an attractive direction. Finally, this paper focuses on the speech pre-training model, whether the concept of progressive residual extraction is applicable to audio pre-training in general is another interesting issue.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Improving performance on various downstream tasks jointly is a challenge that has garnered significant attention in speech self-supervised pre-training. In this paper, we highlight that different downstream tasks require different types of speech information. To make an SSL model more universal, it is crucial to mitigate the mutual interference of irrelevant speech information extraction during pre-training. Inspired by pitch-speaker-content decoupling in voice conversion and speaker information normalization in speech recognition, we propose a progressive residual extraction based pre-training method. By leveraging the task characteristics of different layers in HuBERT’s self-supervised strategy, we enhance specific layers’ abilities to extract pitch variation and speaker information. Subsequently, we remove this enhanced information from the main branch using a residual extraction approach. This removal reduces the subsequent learning burden on content extraction for the main branch, ultimately achieving joint improvements across various downstream tasks.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised autoregressive model for speech representation learning,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, 2019, pp. 146–150.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maaløe <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Self-supervised speech representation learning: A review,” <em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S.-W. Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Superb: Speech processing universal performance benchmark,” in <em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, 2021, pp. 3465–3469.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
X. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang, and J. Tang, “Self-supervised learning: Generative or contrastive,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE TKDE</em>, vol. 35, no. 1, pp. 857–876, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. T. Liu, S.-W. Li, and H.-Y. Lee, “Tera: Self-supervised learning of transformer encoder representation for speech,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE TASLP</em>, vol. 29, pp. 2351–2366, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi, “Soundstream: An end-to-end neural audio codec,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE TASLP</em>, vol. 30, pp. 495–507, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Défossez, J. Copet, G. Synnaeve, and Y. Adi, “High fidelity neural audio compression,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
T. Wang, L. Zhou, Z. Zhang, Y. Wu, S. Liu, Y. Gaur, Z. Chen, J. Li, and F. Wei, “Viola: Unified codec language models for speech recognition, synthesis, and translation,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.16107</em>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A framework for self-supervised learning of speech representations,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, vol. 33, 2020, pp. 12 449–12 460.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
W.-N. Hsu, B. Bolte, Y. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, “Hubert: Self-supervised speech representation learning by masked prediction of hidden units,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE TASLP</em>, vol. 29, pp. 3451–3460, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Wavlm: Large-scale self-supervised pre-training for full stack speech processing,” <em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic">IEEE JSTSP</em>, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. Baevski, A. Babu, W.-N. Hsu, and M. Auli, “Efficient self-supervised learning with contextualized target representations for vision, speech and language,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2023, pp. 1416–1429.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. Zhao and W.-Q. Zhang, “Improving automatic speech recognition performance for low-resource languages with self-supervised models,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE JSTSP</em>, vol. 16, no. 6, pp. 1227–1241, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Z. Huang, S. Watanabe, S.-W. Yang, P. García, and S. Khudanpur, “Investigating self-supervised learning for speech enhancement and separation,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">ICASSP</em>.   IEEE, 2022, pp. 6837–6841.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
D. Zhang, S. Li, X. Zhang, J. Zhan, P. Wang, Y. Zhou, and X. Qiu, “Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">The 2023 Conference on Empirical Methods in Natural Language Processing</em>, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maaløe <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Self-supervised speech representation learning: A review,” <em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic">IEEE JSTSP</em>, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
H.-S. Tsai, H.-J. Chang, W.-C. Huang, Z. Huang, K. Lakhotia, S.-w. Yang, S. Dong, A. T. Liu, C.-I. J. Lai, J. Shi <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Superb-sg: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,” in <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">AMACL</em>, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S. Chen, Y. Wu, C. Wang, Z. Chen, Z. Chen, S. Liu, J. Wu, Y. Qian, F. Wei, J. Li <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Unispeech-sat: Universal speech representation learning with speaker aware pre-training,” in <em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic">ICASSP</em>.   IEEE, 2022, pp. 6152–6156.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
B. Han, W. Huang, Z. Chen, and Y. Qian, “Improving dino-based self-supervised speaker verification with progressive cluster-aware training,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">ICASSPW</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A. Khare, S. Parthasarathy, and S. Sundaram, “Self-supervised learning with cross-modal transformers for emotion recognition,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE SLT</em>, 2021, pp. 381–388.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Y. Li, Y. Mohamied, P. Bell, and C. Lai, “Exploration of a self-supervised speech model: A study on emotional corpora,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">SLT</em>.   IEEE, 2023, pp. 868–875.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
T. Wang, X. Chen, Z. Chen, S. Yu, and W. Zhu, “An adapter based multi-label pre-training for speech separation and enhancement,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">ICASSP</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
B. Irvin, M. Stamenovic, M. Kegler, and L.-C. Yang, “Self-supervised learning for speech enhancement through synthesis,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">ICASSP</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J. Lin, M. Ge, W. Wang, H. Li, and M. Feng, “Selective hubert: Self-supervised pre-training for target speaker in clean and mixture speech,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Letters</em>, 2024.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Z. Ma, Z. Zheng, G. Yang, Y. Wang, C. Zhang, and X. Chen, “Pushing the limits of unsupervised unit discovery for ssl speech representation,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
X. Wang, Y. Cheng, Y. Yang, Y. Yu, F. Li, and S. Peng, “Multitask joint strategies of self-supervised representation learning on biomedical networks for drug discovery,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Nature Machine Intelligence</em>, vol. 5, no. 4, pp. 445–456, 2023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
S. Masoudnia and R. Ebrahimpour, “Mixture of experts: a literature survey,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence Review</em>, vol. 42, pp. 275–293, 2014.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
X. Zhang, D. Zhang, S. Li, Y. Zhou, and X. Qiu, “Speechtokenizer: Unified speech tokenizer for speech large language models,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2024.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A. Pasad, J.-C. Chou, and K. Livescu, “Layer-wise analysis of a self-supervised speech representation model,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.   IEEE, 2021, pp. 914–921.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J. Lim and K. Kim, “Wav2vec-vc: Voice conversion via hidden representations of wav2vec 2.0,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2024, pp. 10 326–10 330.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
H. Fujisaki, “Information, prosody, and modeling-with emphasis on tonal features of speech,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Speech Prosody 2004, International Conference</em>, 2004.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
D. Wang, L. Deng, Y. T. Yeung, X. Chen, X. Liu, and H. Meng, “Vqmivc: Vector quantization and mutual information-based unsupervised speech representation disentanglement for one-shot voice conversion,” in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
A. Triantafyllopoulos, B. W. Schuller, G. İymen, M. Sezgin, X. He, Z. Yang, P. Tzirakis, S. Liu, S. Mertes, E. André <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “An overview of affective speech synthesis and conversion in the deep learning era,” <em id="bib.bib33.2.2" class="ltx_emph ltx_font_italic">Proceedings of the IEEE</em>, 2023.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Q.-B. Hong, C.-H. Wu, and H.-M. Wang, “Decomposition and reorganization of phonetic information for speaker embedding learning,” <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM TASLP</em>, vol. 31, pp. 1745–1757, 2023.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
T. Liu, K. A. Lee, Q. Wang, and H. Li, “Disentangling voice and content with self-supervision for speaker recognition,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">NIPS</em>, vol. 36, 2024.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
K. Qian, Y. Zhang, H. Gao, J. Ni, C.-I. Lai, D. Cox, M. Hasegawa-Johnson, and S. Chang, “Contentvec: An improved self-supervised speech representation by disentangling speakers,” in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2022, pp. 18 003–18 017.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
D. Giuliani, M. Gerosa, and F. Brugnara, “Improved automatic speech recognition through speaker normalization,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Computer Speech &amp; Language</em>, vol. 20, no. 1, pp. 107–123, 2006.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">NAACL-HLT</em>.   Audio Engineering Society, 2019.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
S. Schneider, A. Baevski, R. Collobert, and M. Auli, “wav2vec: Unsupervised pre-training for speech recognition,” in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, 2019, pp. 3465–3469.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
A. Vasuki and P. Vanathi, “A review of vector quantization techniques,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">IEEE Potentials</em>, vol. 25, no. 4, pp. 39–47, 2006.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
M. Morise, H. Kawahara, and H. Katayose, “Fast and reliable f0 estimation method based on the period extraction of vocal fold vibration of singing voice and speech,” in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">International Conference: Audio for Games</em>.   Audio Engineering Society, 2009.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
L. Huang, J. Qin, Y. Zhou, F. Zhu, L. Liu, and L. Shao, “Normalization techniques in training dnns: Methodology, analysis and application,” <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">IEEE TPAMI</em>, vol. 45, no. 8, pp. 10 173–10 196, 2023.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
A. D. Rasamoelina, F. Adjailia, and P. Sinčák, “A review of activation function for artificial neural network,” in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">IEEE SAMI</em>, 2020, pp. 281–286.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
R. Dey and F. M. Salem, “Gate-variants of gated recurrent unit neural networks,” in <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">MWSCAS</em>.   IEEE, 2017, pp. 1597–1600.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
B. Desplanques, J. Thienpondt, and K. Demuynck, “Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification,” in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, 2020.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an asr corpus based on public domain audio books,” in <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">ICASSP</em>.   IEEE, 2015, pp. 5206–5210.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
B. Zhang, H. Lv, P. Guo, Q. Shao, C. Yang, L. Xie, X. Xu, H. Bu, X. Chen, C. Zeng <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition,” in <em id="bib.bib47.2.2" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2022, pp. 6182–6186.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
V. Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert, “Mls: A large-scale multilingual dataset for speech research,” <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.03411</em>, 2020.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
A. Nagrani, J. S. Chung, and A. Zisserman, “Voxceleb: a large-scale speaker identification dataset,” in <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, 2017.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap: Interactive emotional dyadic motion capture database,” <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Language resources and evaluation</em>, vol. 42, pp. 335–359, 2008.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
J. Thiemann, N. Ito, and E. Vincent, “The diverse environments multi-channel acoustic noise database (demand): A database of multichannel environmental noise recordings,” in <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Proceedings of Meetings on Acoustics</em>, vol. 19, no. 1.   AIP Publishing, 2013.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Y. Zhao, W.-C. Huang, X. Tian, J. Yamagishi, R. K. Das, T. Kinnunen, Z. Ling, and T. Toda, “Voice conversion challenge 2020 –- intra-lingual semi-parallel and cross-lingual voice conversion,” in <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge 2020</em>, 2020, pp. 80–98.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
W.-C. Huang, S.-W. Yang, T. Hayashi, H.-Y. Lee, S. Watanabe, and T. Toda, “S3prl-vc: Open-source voice conversion framework with self-supervised speech representations,” in <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Proc. ICASSP</em>, 2022.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli, “fairseq: A fast, extensible toolkit for sequence modeling,” in <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">CNACACL</em>, 2019, pp. 48–53.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
L. Huawei Technologies Co., “Huawei mindspore ai development framework,” in <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence Technology</em>.   Springer, 2022, pp. 137–162.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
O. Kramer and O. Kramer, “Scikit-learn,” <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Machine learning for evolution strategies</em>, pp. 45–53, 2016.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
D. Arthur, S. Vassilvitskii <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “k-means++: The advantages of careful seeding,” in <em id="bib.bib57.2.2" class="ltx_emph ltx_font_italic">Soda</em>, vol. 7, 2007, pp. 1027–1035.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
H. Wang, C. Liang, S. Wang, Z. Chen, B. Zhang, X. Xiang, Y. Deng, and Y. Qian, “Wespeaker: A research and production oriented speaker embedding learning toolkit,” in <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra, “Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs,” in <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">2001 IEEE international conference on acoustics, speech, and signal processing. Proceedings (Cat. No. 01CH37221)</em>, vol. 2.   IEEE, 2001, pp. 749–752.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
J. Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, “Sdr–half-baked or well done?” in <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2019, pp. 626–630.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
J. Kominek, T. Schultz, and A. W. Black, “Synthesizer voice quality of new languages calibrated with mean mel cepstral distortion.” in <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">SLTU</em>, 2008, pp. 63–68.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
I. Cohen, Y. Huang, J. Chen, J. Benesty, J. Benesty, J. Chen, Y. Huang, and I. Cohen, “Pearson correlation coefficient,” <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Noise reduction in speech processing</em>, pp. 1–4, 2009.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
L. C. Nygaard and C. Y. Tzeng, “Perceptual integration of linguistic and non-linguistic properties of speech,” <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">The handbook of speech perception</em>, pp. 398–427, 2021.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.00386" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.00387" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.00387">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.00387" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.00388" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 19:42:14 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
