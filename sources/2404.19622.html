<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.19622] Fake it to make it: Using synthetic data to remedy the data shortage in joint multimodal speech-and-gesture synthesis</title><meta property="og:description" content="Although humans engaged in face-to-face conversation simultaneously communicate both verbally and non-verbally, methods for joint and unified synthesis of speech audio and co-speech 3D gesture motion from text are a ne…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Fake it to make it: Using synthetic data to remedy the data shortage in joint multimodal speech-and-gesture synthesis">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Fake it to make it: Using synthetic data to remedy the data shortage in joint multimodal speech-and-gesture synthesis">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.19622">

<!--Generated on Sun May  5 22:42:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Fake it to make it: Using synthetic data to remedy the data shortage
<br class="ltx_break">in joint multimodal speech-and-gesture synthesis</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shivam Mehta
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">KTH Royal Institute of Technology, Sweden
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anna Deichler
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">KTH Royal Institute of Technology, Sweden
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jim O’Regan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">KTH Royal Institute of Technology, Sweden
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Birger Moëll
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">KTH Royal Institute of Technology, Sweden
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Jonas Beskow
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">KTH Royal Institute of Technology, Sweden
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gustav Eje Henter
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">KTH Royal Institute of Technology, Sweden
</span>
<span class="ltx_contact ltx_role_affiliation">Motorica AB, Sweden
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Simon Alexanderson
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">KTH Royal Institute of Technology, Sweden
</span>
<span class="ltx_contact ltx_role_affiliation">Motorica AB, Sweden
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Although humans engaged in face-to-face conversation simultaneously communicate both verbally and non-verbally, methods for joint and unified synthesis of speech audio and co-speech 3D gesture motion from text are a new and emerging field. These technologies hold great promise for more human-like, efficient, expressive, and robust synthetic communication, but are currently held back by the lack of suitably large datasets, as existing methods are trained on parallel data from all constituent modalities. Inspired by student-teacher methods, we propose a straightforward solution to the data shortage, by simply synthesising additional training material. Specifically, we use unimodal synthesis models trained on large datasets to create multimodal (but synthetic) parallel training data, and then pre-train a joint synthesis model on that material. In addition, we propose a new synthesis architecture that adds better and more controllable prosody modelling to the state-of-the-art method in the field. Our results confirm that pre-training on large amounts of synthetic data improves the quality of both the speech and the motion synthesised by the multimodal model, with the proposed architecture yielding further benefits when pre-trained on the synthetic data.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Human beings are embodied, and we use a wide gamut of the expressions afforded by our bodies to communicate. In concert with the lexical and non-lexical (prosodic) components of speech, humans also leverage gestures realised by face, head, arm, finger, and body motion – all driven by a shared, underlying communicative intent <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">60</span></a>]</cite> – to improve face-to-face communication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">69</span></a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Research into automatically recreating different kinds of human communicative behaviour, whether it be speech audio from text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">89</span></a>]</cite>, or gesture motion from speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">97</span></a>]</cite>, have a long history, as these are key enabling technologies for, e.g., virtual agents, game characters, and social robots <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>, <a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>, <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>, <a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">71</span></a>]</cite>.
The advent of deep learning has led to an explosion of research in the two fields <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">87</span></a>, <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>, <a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">69</span></a>]</cite>.
Gesture synthesis, in particular, has been shown to benefit from access to both lexical and acoustic representations of speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>, <a href="#bib.bib109" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">109</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>.
That said, joint and simultaneous synthesis of both speech and gesture communication (pioneered in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">82</span></a>]</cite>) remains severely under-explored.
This despite the fact that simultaneously generating both modalities together not only better emulates how humans produce communicative expressions, but also offers a stepping stone towards creating non-redundant gestures that can complement and even replace speech, like human gestures do <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>.
On top of this, recent research efforts towards integrating the synthesis of the two modalities have demonstrated improvements in coherent <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>, <a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite>, compact <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">99</span></a>, <a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite>, jointly and rapidly learnable <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite>, convincing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>, <a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite>, and cross-modally appropriate <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite> synthesis of speech and 3D gestures from text.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2404.19622/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="180" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">MAGI: Multimodal Audio and Gesture, Integrated</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The current state of the art in joint multimodal speech-and-gesture synthesis, Match-TTSG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite>, achieves strong performance via modern techniques such as conditional flow matching (OT-CFM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite> with U-Net Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">96</span></a>]</cite> encoders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite>.
However, there still remains a noticeable gap between synthesised model output and recordings of natural human speech and gesticulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite>.
This contrasts with recent breakthroughs in “generative AI”, which can synthesise text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>, images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">77</span></a>, <a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite>, and speech audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">88</span></a>, <a href="#bib.bib84" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">84</span></a>]</cite> that all are nigh indistinguishable from those created by humans.
The critical difference is that whereas those strong models for synthesising single modalities benefit from training on vast amounts of data (cf. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>), existing parallel datasets of speech audio, text transcriptions, and human motion are radically smaller.
This is especially true if we require good motion quality (which at present generally necessitates high-end 3D motion capture) and speech audio with a spontaneous character and quality suitable for speech synthesis.
The state-of-the-art joint synthesis system demonstrated in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite> was thus trained on 4.5 hours of parallel speech and gesture data from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>; larger parallel corpora exist <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>, <a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>]</cite>, but exhibit some quality issues (cf. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>) and do not exceed 100 hours, a far cry from the corpora used to train leading generative AI systems.
It stands to reason that multimodal synthesis systems could gain substantially from overcoming the limitations imposed by training only on presently available parallel corpora.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we propose two improvements to the state-of-the art multimodal speech-and-gesture synthesis:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We pre-train<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We use “<em id="footnote1.1" class="ltx_emph ltx_font_italic">pre-training</em>” to refer to any training (by others or by us) performed prior to training on our multimodal target dataset.</span></span></span> a joint speech-and-gesture synthesis model on a large parallel corpus of <em id="S1.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">synthetic</em> training data created using leading text, text-to-speech, and speech-to-gesture systems (Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Fake it to make it: Using synthetic data to remedy the data shortage in joint multimodal speech-and-gesture synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), before fine-tuning on our target data. This
offers a simple way for multimodal models to benefit from advances in unimodal synthesis systems.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We extend <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite> with a probabilistic duration model (similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite>) and individual models of pitch and energy (similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite>). This enables more lifelike and more controllable synthetic expression.</p>
</div>
</li>
</ol>
<p id="S1.p4.2" class="ltx_p">The resulting joint synthesis system is orders of magnitude smaller and faster than the models used for synthesising the pre-training data.
Our subjective evaluations show that the proposed pre-training on synthetic data improves the speech as well as the gestures created by a joint synthesis system, and that the architectural modifications further benefit a system pre-trained on large synthetic data and also enable output control.
For video examples and code, please see our demo page at <a target="_blank" href="https://shivammehta25.github.io/MAGI/" title="" class="ltx_ref ltx_href">shivammehta25.github.io/MAGI/</a>;</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we review synthesis of text, speech audio, and 3D gesture motion, along with existing work in multimodal speech-and-gesture synthesis.
For each task, we state how the methods relate to our contributions
and briefly discuss how synthetic data can improve synthesis models.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Text generation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The rise of large language models (LLMs) has brought revolutionary improvements to text generation.
Transformer-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">96</span></a>]</cite> LLMs using Generative Pre-trained Transformers (GPTs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">74</span></a>]</cite> like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib92" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">92</span></a>]</cite>
are capable of generating text virtually indistinguishable from that written by humans.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The critical methodological advances for LLMs are pre-training on vast amounts of diverse data, coupled with fine-tuning on a small amount of high-quality, in-domain material, e.g., via
Reinforcement Learning from Human Feedback (RLHF) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>.
This methodology of pre-training foundation models followed by fine-tuning on the best data has been validated to give excellent results across several modalities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib116" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">116</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>.
In this paper, we for the first time use that methodology in joint speech-and-gesture synthesis.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Fine-tuned LLMs allow generating of diverse text samples for many domains through <em id="S2.SS1.p3.1.1" class="ltx_emph ltx_font_italic">prompting</em> the model, i.e., providing a written text prompt at runtime describing the output to generate. Prompting has been useful for many tasks including creating synthetic dialogue datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> and selecting appropriate gestures based on verbal utterances <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>.
We use this ability to create an arbitrarily large material of conversational text sentences in the style of a given speaker/corpus as a basis for our synthetic-data creation.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Speech synthesis</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Recent advances in deep generative modelling have significantly improved text-to-speech (TTS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">87</span></a>]</cite>, reaching naturalness levels that rival recorded human speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">88</span></a>, <a href="#bib.bib84" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">84</span></a>]</cite>.
TTS models are often divided into two broad classes: autoregressive (AR) and non-autoregressive (NAR). AR models produce acoustic outputs sequentially,
using mechanisms such as neural cross-attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib83" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">83</span></a>, <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>, <a href="#bib.bib115" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">115</span></a>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> or neural transducers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>, <a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">62</span></a>, <a href="#bib.bib106" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">106</span></a>]</cite> to connect inputs symbols to the outputs.
Non-autoregressive models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">79</span></a>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>, <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>, <a href="#bib.bib118" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">118</span></a>, <a href="#bib.bib72" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">72</span></a>, <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>, <a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> instead generate the entire utterance in parallel.
NAR models are typically faster, especially on GPUs, but AR methods often yield slightly better synthesis.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Recently, there has been a trend <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>, <a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>, <a href="#bib.bib98" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">98</span></a>, <a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite> to quantise audio waveforms into discrete tokens <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>, and then adapt an LLM-like autoregressive approach (e.g., with GPTs) to learn to model these audio tokens on large datasets.
Synthesised token sequences can subsequently be converted back to audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">85</span></a>]</cite>.
Speaker and style adaptation can be achieved by seeding (prompting) the model with an audio snippet, something we leverage to create diverse stochastic synthetic training data for our work.
</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">LLM-like TTS can give exceptional results when trained on large datasets, but models risk confabulating (similar to well-known issues with LLMs) and getting trapped in feedback loops due to the autoregression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>.
Our paper therefore describes a pipeline for mitigating these problems when creating synthetic training data at scale.
</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">In NAR TTS, it has been found that conditioning the TTS on the output of a model of prosodic properties, e.g., per-phone pitch and energy, can benefit synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">118</span></a>, <a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">79</span></a>, <a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">70</span></a>]</cite>.
This also enables control over the speech, by replacing or manipulating the prosodic features prior to synthesis.
Speech-sound durations are especially important for convincing prosody, and
probabilistic modelling of durations can substantially improve
deep generative TTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>.
This appears especially useful for speech uttered spontaneously in conversation, as considered here, due to its highly diverse
prosodic structure <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>.
We therefore introduce a probabilistic duration model, coupled with explicit pitch and energy models, into the multimodal synthesis architecture.
Better duration modelling should help create speech rhythm and timings with adequate time for gesture-preparation phases, so that gesture strokes can
synchronise with the speech.
Improved control should not only affect the output speech but also the gestures we generate with it.
</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Gesture synthesis</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Like TTS, deep learning has led to a boom in 3D gesture synthesis from speech text and/or audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">69</span></a>]</cite>, adapting techniques like GANs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">100</span></a>, <a href="#bib.bib101" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">101</span></a>]</cite>, normalising flows <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>, VAEs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, VQ-VAEs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">107</span></a>, <a href="#bib.bib108" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">108</span></a>]</cite>,
combined adversarial learning and regression losses <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>]</cite>, and flow-VAEs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">90</span></a>]</cite>.
Following text-prompted diffusion models for human motion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">91</span></a>, <a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>, <a href="#bib.bib114" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">114</span></a>]</cite>
diffusion models have seen rapid adoption for generating 3D gesture motion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">112</span></a>, <a href="#bib.bib117" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">117</span></a>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>.
Flow matching <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite> improves synthesis speed by learning models that require fewer diffusion steps during sampling, and has recently been adapted to
motion generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> and TTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>, <a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>.
Similar to LLMs and large TTS models, separate efforts wholly or partly model gestures autoregressively as a sequence of discrete tokens <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">104</span></a>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>, <a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">The most recent large-scale comparison of gesture-generation models, the GENEA Challenge 2023 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>, found that the two strongest methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib105" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">105</span></a>]</cite> (which are extensions of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib103" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">103</span></a>]</cite>) were based on diffusion models.
Among these, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> made use of self-supervised text-and speech embeddings from data2vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>, subsequently aligned with gesture motion using CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">75</span></a>]</cite> training, to improve the coherence between gestures and the two speech-input modalities. In addition to modelling beat gestures, the approach recognises the need for additional input modalities to generate representational gestures, such as iconic and deictic pointing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>, for more nuanced and contextually relevant non-verbal communication.
Our data-synthesis pipeline leverages their approach to create synthetic training gestures that well match the synthetic speech text and audio input.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Joint synthesis of speech and gestures</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Speech synthesis and gesture generation have traditionally been treated as separate problems, performed on different data by distinct research communities.
TTS is mainly developed for read-aloud speech, whereas co-speech gesturing is more closely associated with conversational settings.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">Joint synthesis of speech and motion was first considered by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">82</span></a>]</cite>.
The first neural model was DurIAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">111</span></a>]</cite>, which simultaneously generated speech audio and 3D facial expressions, albeit for speech read aloud.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> trained separate deep-learning TTS and speech-to-gesture systems to synthesise speech and 3D motion for the same speaker and the same (spontaneous) speaking style.
This was followed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">99</span></a>]</cite>, which investigated adapting and extending AR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">83</span></a>]</cite> and NAR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite> neural TTS models to perform joint multimodal synthesis.
Their joint models reduced the number of parameters needed over <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>, but the best model (the one based on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">83</span></a>]</cite>) required complex multi-stage training to speak intelligibly and did not improve quality.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">Diff-TTSG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite> advanced joint speech-and-gesture synthesis by employing probabilistic modelling, specifically a strong denoising probabilistic model (DPMs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">86</span></a>]</cite> building on the TTS work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">72</span></a>]</cite>.
This model could be trained on speech-and-gesture data from scratch in one go and produced improved results over <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">99</span></a>]</cite>, but internally used separate pipelines for producing the two output modalities, leading to suboptimal coherence between them.
Match-TTSG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite> improved on this aspect by using a compact and unified decoder to jointly sample both output modalities.
It also used conditional flow matching <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite> rather than diffusion, for much faster output synthesis.
Experiments found that Match-TTSG improved on the previous best model in all respects, establishing it as the current state of the art.</p>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<p id="S2.SS4.p4.1" class="ltx_p">Most of the above models were trained only on small, parallel multimodal datasets from a single speaker.
(The one exception is <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">99</span></a>]</cite>, which required pre-training part of the network on a TTS corpus to produce intelligible output at all.)
The results in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite> show that, e.g., the synthetic speech falls short of human-level naturalness, and the quality we find from systems trained on very large datasets.
Accordingly, we propose to circumvent the data limitation by using strong unimodal synthesisers to create a large synthetic training corpus for our joint model.
</p>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Training on synthetic data</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">Training models on synthetic data is gaining interest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">93</span></a>]</cite>, e.g., for privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">66</span></a>]</cite> and in model compression through distillation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>, including generative models like TTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">94</span></a>]</cite>.
Synthesis (and synthetic data) is also appealing in cases where real data is scarce or difficult to obtain, as demonstrated in applications to human poses and motion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">95</span></a>, <a href="#bib.bib113" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">113</span></a>]</cite>.
It also allows for the creation of diverse and controlled datasets that can enable
more accurate and versatile models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>.
We here propose to generalise such approaches by chaining together multiple unimodal synthesisers, to enable training multimodal speech-and-gesture models.</p>
</div>
<div id="S2.SS5.p2" class="ltx_para">
<p id="S2.SS5.p2.1" class="ltx_p">There may be a risk that the individual unimodal synthesisers we use, being trained on non-overlapping data, could fail to capture mutual information that connects the modalities.
The final multimodal system trained on the synthetic corpus might then suffer from artefacts and fail to recreate proper inter-modal dependencies.
However, recent theoretical and practical results show that little <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite> or no <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">68</span></a>, <a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite> parallel data may suffice for learning joint distributions of multiple random variables (modalities).
Training on corpora generated by synthesisers built from non-overlapping material might thus not be as risky as it could seem.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We now describe our method for creating wholly synthetic multimodal datasets for training synthesis models, and then detail our modifications to the Match-TTSG architecture.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Creating synthetic training data</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our pipeline for creating synthetic training data had the following main steps:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Generating written sentences in the style of conversational speech transcriptions.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Synthesising diverse speech audio from the text.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Validating/filtering the synthetic speech audio using automatic speech recognition, and aligning the input text with the synthesised audio.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Synthesising gestures from the generated speech audio files and their corresponding time-aligned text.</p>
</div>
</li>
</ol>
<p id="S3.SS1.p1.2" class="ltx_p">We provide more detail in the following subsections.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Text generation</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">The first step was to create text sentences that can form the basis of synthesising multimodal data in a conversational style.
For this we used GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> and deliberate prompting.
Specifically, we prompted the model with a list of 50 transcriptions of sentences from the training split <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite> of the Trinity Speech-Gesture Dataset II (TSGD2) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>, each enclosed in triple quotes, followed by a request to produce 50 additional phrases in the same style (including hesitations and disfluencies seen in the transcriptions) but ignoring the content.
Further prompting then followed, to make the model generate additional output based around different emotions and scenarios, and obtain a more diverse material.
The emotional categories we provided were: disgust, sadness, fear, frustration, surprise, excitement, happiness, confusion, and denial.
Our prompts often gave similar instructions multiple times, as we found this led to more realistic output.
The main instruction prompt and a number of example continuations can be found in Appendix <a href="#A1" title="Appendix A GPT-4 prompts ‣ Fake it to make it: Using synthetic data to remedy the data shortage in joint multimodal speech-and-gesture synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">We utilised the above procedure to generate a total of 600 phrases (available through <a target="_blank" href="https://shivammehta25.github.io/MAGI/" title="" class="ltx_ref ltx_href">the webpage</a>), each approximately 250 characters in length.
We found that limiting the length of the prompt helps prevent issues with the subsequent speech synthesis, which tended to produce unintelligible or confabulated output for overly long utterances.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Speech generation</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">The next step was to synthesise speech audio from the 600 LLM-generated phrases.
For this, we considered multiple TTS systems capable of multi-speaker and spontaneous speech synthesis, including Bark<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/suno-ai/bark" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/suno-ai/bark</a></span></span></span>, XTTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>, and ElevenLabs<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://elevenlabs.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://elevenlabs.io/</a></span></span></span>. However, Bark exhibited frequent confabulations and unexpected changes in speaker identity within a single utterance, which seemed problematic for learning to maintain a consistent vocal identity.
Although ElevenLabs demonstrated high-quality output, its status as a non-open source and proprietary solution led us to exclude it. Ultimately, we selected XTTS for generating our synthetic speech dataset, due to it combining more consistent synthesis with a research-permissible license.
We limited each synthesised utterance to at most 400 XTTS speech tokens, since anything longer than that is virtually certain too long for our prompts, and thus must contain confabulation or gibberish speech.
For everything else, default XTTS synthesis hyperparameters were used.
In the end, each synthesised audio utterance was around 20–23 seconds long, taking about half that time to synthesise.
</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">In order to obtain more diverse data containing multiple speakers, each of the 600 phrases was synthesised 16 times, once in each of 16 different voices.
These voices were selected as a gender-balanced set (8 male and 8 female speakers) from the VCTK corpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">102</span></a>]</cite>, and elicited from XTTS by seeding the synthesis of each individual utterance with the audio of longest VCTK utterance spoken by the relevant speaker as an acoustic prompt.
These prompting utterances tended to be around 9 seconds long.
In total, we thus synthesised 16 <math id="S3.SS1.SSS2.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS1.SSS2.p2.1.m1.1a"><mo id="S3.SS1.SSS2.p2.1.m1.1.1" xref="S3.SS1.SSS2.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p2.1.m1.1b"><times id="S3.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p2.1.m1.1c">\times</annotation></semantics></math> 600 = 9600 audio utterances.</p>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.1" class="ltx_p">Interestingly, despite the spontaneous nature of the input phrases, we found that false starts and fillers explicitly present in the input were sometimes omitted in the XTTS output.
This could be partly due to the choice of temperature parameter at synthesis time (the default, 0.65), which favours more consistent and likely output, and partly due to the public English-language training datasets cover read rather than spontaneous speech.
Since XTTS furthermore was prompted using a snippet of read-aloud speech audio from VCTK, the output audio tended to sound more like reading than speaking spontaneously.
</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Data filtering and forced alignment</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">Following speech synthesis, a number of data-processing steps were performed to obtain a suitable dataset for training a strong gesture-generation system.
To begin with, all synthesised audio utterances longer than 25 seconds were immediately and permanently discarded, since these overwhelmingly tended to contain issues related to confabulation and the like.
The output from XTTS did not have exact fidelity to the text it was prompted with, so automatic speech recognition (ASR) was used to get more accurate input to the gesture-generation system.
ASR was performed using Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">76</span></a>]</cite>, using the <span id="S3.SS1.SSS3.p1.1.1" class="ltx_text ltx_font_typewriter">medium.en</span> model, which has in previous uses proven to be less prone to confabulation than the large variants, whilst providing sufficient accuracy.
Interestingly, Whisper tended to prefer British English spelling, possibly since VCTK was recorded in the UK.
The ASR derived transcripts
then replaced the original TTS input text for each utterance in all subsequent processing.</p>
</div>
<div id="S3.SS1.SSS3.p2" class="ltx_para">
<p id="S3.SS1.SSS3.p2.1" class="ltx_p">The gesture-generation system we chose for the final synthesis (<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>) requires word-level timestamps for the text transcriptions.
Although we considered several tools that attempt to obtain word timings from Whisper directly, none were sufficiently accurate for our needs.
Instead, we obtained the requisite timings using the Montreal Forced Aligner (MFA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite>. Text input to MFA was processed word-by-word to remove leading and trailing punctuation and to perform case folding to lower case.
Utterances that MFA failed to align were also excluded from consideration.</p>
</div>
<div id="S3.SS1.SSS3.p3" class="ltx_para">
<p id="S3.SS1.SSS3.p3.1" class="ltx_p">Following the filtering and alignment process, we were left with 8173 audio utterances for our final synthetic dataset, meaning that 1427 utterances (about 15%) were discarded during the filtering step.
The remaining data had a total duration of 37.6 hours, which also ended up being the size of the final synthetic training corpus.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2404.19622/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="423" height="166" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Schematic overview of the proposed MAGI architecture and its prosody predictor.</span></figcaption>
</figure>
</section>
<section id="S3.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.4 </span>Gesture generation</h4>

<div id="S3.SS1.SSS4.p1" class="ltx_para">
<p id="S3.SS1.SSS4.p1.1" class="ltx_p">We used a recent diffusion-based gesture-generation method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> that performed well in a large comparative evaluation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite> to generate synthetic gesture data.
That system leveraged data2vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite> embeddings to represent audio input, which help achieve a more speaker-independent representation.
On top of that, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite> introduced a Contrastive Speech and Motion Pre-training (CSMP) module, to learn joint embeddings of speech and gesture that can strengthen the semantic coupling between these modalities.
By utilising the output of the CSMP module as a conditioning signal within the diffusion-based gesture-synthesis model, the system can generate co-speech gestures that are human-like and semantically aware, thereby improving the quality and appropriateness of the generated gestures to the spoken content.
The CSMP module requires word-level timestamps, which is why forced-alignment was performed in Sec. <a href="#S3.SS1.SSS3" title="3.1.3 Data filtering and forced alignment ‣ 3.1 Creating synthetic training data ‣ 3 Method ‣ Fake it to make it: Using synthetic data to remedy the data shortage in joint multimodal speech-and-gesture synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.3</span></a>.</p>
</div>
<div id="S3.SS1.SSS4.p2" class="ltx_para">
<p id="S3.SS1.SSS4.p2.1" class="ltx_p">Since this paper is focused on multimodal synthesis from data where no interlocutor is present or recorded (i.e., not back-and-forth conversations), interlocutor-related inputs were removed from the architecture.
The input is thus an audio track with time-aligned text transcripts.
We used the pre-trained weights from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> for the CSMP module and re-trained the diffusion-based gesture model to comply with the change of input, using the same architecture and learning rate as in the paper. The training was done using two NVIDIA RTX3090 GPUs (194k updates, each with batch size 60) on the subset of the Talking With Hands (TWH) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite> provided in the GENEA 2023 Challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>. We used the trained system to generate text-and-audio-driven gestures for the 8173 previously transcribed synthetic speech utterances, and used Autodesk MotionBuilder after synthesis to retarget the output motion to the skeleton of the TSGD2 data and visualiser in Sec. <a href="#S4.SS1" title="4.1 Data and systems ‣ 4 Experiments ‣ Fake it to make it: Using synthetic data to remedy the data shortage in joint multimodal speech-and-gesture synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. While the synthesised motion encompasses the full body (without fingers), we only consider upper-body motion in this work. Compared to conventional conditioning approaches where audio is represented using mel-spectrograms, the speaker-independent data2vec embeddings in the CSMP module are expected to better handle the differences between natural and synthetic voices during synthesis, thus making it feasible to generate large amounts of gesture data based on synthetic speech without undue degradations due to domain mismatch.
This data was used to train the different multimodal synthesis systems considered in our experiments.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Proposed multimodal synthesis system</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The current state of the art in joint speech-and-gesture synthesis is Match-TTSG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite>, a non-autoregressive model which uses conditional flow matching (OT-CFM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite>
to learn Ordinary Differential Equations (ODEs) with more linear vector fields than continuous-time diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">86</span></a>]</cite> create.
Such simpler vector fields offer advantages for easier learning and faster synthesis.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We extend the Match-TTSG framework in three ways:</p>
<ol id="S3.I2" class="ltx_enumerate">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">Probabilistic instead of deterministic duration modelling, which can benefit deep generative NAR TTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite>.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">Additional prosody-prediction modules, which are widely used in NAR TTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">118</span></a>, <a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite>.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p">A speaker-identity input, as necessary for pre-training on the multispeaker data in the large synthetic training set.</p>
</div>
</li>
</ol>
<p id="S3.SS2.p2.2" class="ltx_p">We call the resulting system <em id="S3.SS2.p2.2.1" class="ltx_emph ltx_font_italic">MAGI</em> for <em id="S3.SS2.p2.2.2" class="ltx_emph ltx_font_italic">Multimodal Audio and Gesture, Integrated</em>; see Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.1.3 Data filtering and forced alignment ‣ 3.1 Creating synthetic training data ‣ 3 Method ‣ Fake it to make it: Using synthetic data to remedy the data shortage in joint multimodal speech-and-gesture synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for a diagram.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">For (1), we augment the original Match-TTSG architecture with a probabilistic duration predictor based on OT-CFM, as introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite>, to learn distributions over speech and gesture durations.
This is trained jointly with the rest of the system.
It replaces the deterministic duration predictor in Match-TTSG,
inherited from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">79</span></a>, <a href="#bib.bib118" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">118</span></a>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>, <a href="#bib.bib72" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">72</span></a>, <a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>, and uses the same network architecture.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">To learn better prosody correlations and enable control over the output, we drew inspiration from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">79</span></a>, <a href="#bib.bib118" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">118</span></a>]</cite> and incorporated two prosody-predictor modules into our system: one for pitch prediction and one for energy prediction, both using the same architecture and hyperparameters as the <em id="S3.SS2.p4.1.1" class="ltx_emph ltx_font_italic">variance adaptor</em> in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite>.
Such prosody predictors improve the synthesis as they enable the model to learn a less oversmoothed representation, thereby enhancing the variability of the generated output by conditioning the synthesis process on additional prosodic features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">80</span></a>]</cite>.
The pitch of the training data utterances was extracted using the PyWorld wrapper for the WORLD vocoder<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://pypi.org/project/pyworld/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pypi.org/project/pyworld/</a></span></span></span> with linear interpolation applied in unvoiced segments to achieve continuous pitch contours for the entire utterances.
We employed a bucketing approach similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite>, separately for pitch and energy, to turn predicted continuous values into embedding vectors to be summed with the text-encoder output vectors. However, in contrast to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite>, we performed token-level prediction instead of frame-level prediction for the two prosodic properties, since it has been stated<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/ming024/FastSpeech2?tab=readme-ov-file#implementation-issues" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ming024/FastSpeech2?tab=readme-ov-file#implementation-issues</a></span></span></span> that this improves the synthesis whilst reducing memory consumption.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">Like in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">72</span></a>]</cite>, Match-TTSG includes a projection layer that maps the text-encoder output vectors onto a predicted average output vector per token (sub-phone).
These averages are used for the so-called <em id="S3.SS2.p5.1.1" class="ltx_emph ltx_font_italic">prior loss</em> in the monotonic alignment search.
The process of sampling the output features (i.e., the flow-matching decoder) is also conditioned on these predicted average vectors.
However, the latter can introduce an information bottleneck, since averages do not include information about variance, correlations, or higher moments of the output distribution.
To improve information flow we instead condition the MAGI decoder directly on the last layer of the text-encoder, prior to the projection layer.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">Finally, we added a speaker embedding for multispeaker synthesis.
Specifically, we used a one-hot speaker vector to represent the 16 different speakers in the synthetic training data. This vector was concatenated to other inputs at multiple stages of the synthesis process, including the text encoder, prosody predictors and decoder.
The idea with this was to minimise information loss and ensure coherent output across different speaker identities.
Since the concatenated vectors only have 16 elements, the impact on model parameter count is small (an increase of a few thousand).</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section experimentally compares our proposed training method and architecture with the previous state-of-the-art method Match-TTSG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite>.
Since this is a synthesis work, the gold standard approach to evaluation – and thus the focus of our experimental validation – is subjective user studies.
The experiments closely follows those in
previous joint synthesis works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>, <a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite>, which in turn follows established practices in speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite> and gesture evaluation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Data and systems</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To test the effectiveness of our method we carried out 3 different subjective evaluations with systems trained on Trinity Speech-Gesture Dataset II (TSGD2) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>, a dataset containing 6 hours of multimodal data: recordings of time-aligned 44.1 kHz audio coupled with 120 fps marker-based 3D motion capture, in which a male native speaker of Hiberno-English discusses a variety of topics whilst gesturing freely. The same train-test split of the data was used as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite>, with around 4.5 hours of training data – much less than the 38 hours of synthetic multimodal data we created.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We trained Match-TTSG <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">(MAT)</span> containing 30.2M parameters and MAGI <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_bold">(MAGI</span>, 31.6M parameters) for 300k steps on only the TSGD2 data, and refer to these conditions <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_bold">MAT-T</span> and <span id="S4.SS1.p2.1.4" class="ltx_text ltx_font_bold">MAGI-T</span> respectively.
We also took the same two architectures (albeit with one-hot speaker vectors for Match-TTSG) and first pre-trained them for 200k updates on the synthetic multispeaker data, followed by fine-tuning for 100k updates on our target dataset, TSGD2. We refer to these as <span id="S4.SS1.p2.1.5" class="ltx_text ltx_font_bold">MAT-FT</span> and <span id="S4.SS1.p2.1.6" class="ltx_text ltx_font_bold">MAGI-FT</span>.
Output samples for held-out sentences were synthesised using 100 neural function evaluations (NFEs; equivalent to number of Euler-forward steps used by the ODE solver) for audio-and-motion synthesis, whilst 10 NFEs were used for the preceding stochastic duration modelling, since it is lower-dimensional and converged more rapidly.
Training and synthesis were performed on NVIDIA RTX 3090 GPUs with batch size 32.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">15 utterances from the held-out set were used to evaluate each modality individually.
We used pre-trained Universal HiFi-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> to generate vocoded but otherwise natural speech referred to as <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">NAT</span>.
We used the same vocoder to generate waveforms from the output mel spectrograms synthesised by the trained multimodal-synthesis systems, while Blender was used to render the motion representations into 3D avatar video, using exactly the same upper-body avatar and visualiser as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>, <a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>]</cite>. The motion data was represented as rotational representation using exponential maps <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> of 45-dim pose vectors and were downsampled to 86.13 fps using cubic interpolation to match the frame rate of the mel-spectrograms.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation setup</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To gain an objective insight into the intelligibility of the synthetic speed, we synthesised the test set sentences from TSGD2, which we then passed to Whisper ASR, to use the Word Error Rate (WER) results as an indicator of their intelligibility. For subjective evaluation, user studies are the gold standard when evaluating synthesis methods. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite>, we used comprehensive evaluation, conducting individual studies of each generated modality. We additionally evaluate the appropriateness of the modalities in terms of each other, to determine how well they fit together.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">In our studies, participants had an interface with five unique response choices, with the exact details varying slightly across different investigations. All participants were native English speakers recruited through <a target="_blank" href="https://www.prolific.com/" title="" class="ltx_ref ltx_href">the Prolific crowdsourcing platform</a>. Each test was designed to last around 20 minutes and participants were compensated 4 GBP (12 GBP/hr) for participation. For the purpose of statistical examination, we converted responses into numerical values. These values were then analysed for statistical significance at the 0.05 threshold using pairwise <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">t</annotation></semantics></math>-tests.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Speech-quality evaluation</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">To assess perceived naturalness of the synthesized speech, we employed the Mean Opinion Score (MOS) testing approach, drawing inspiration from the Blizzard Challenge for text-to-speech systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">73</span></a>]</cite>. Participants were asked, “How natural does the synthesized speech sound?”, rating their responses on a scale from 1 to 5, where 1 represented “Completely unnatural” and 5 indicated “Completely natural.” The intermediary values of 2 to 4 were provided without textual descriptions. Each participant evaluated 15 stimuli per system and 4 attention checks resulting in a total of 525 responses per condition by 35 participants. Fine-tuning with synthetic data led to performance enhancements for both MAGI and MAT, reducing the WER from 13.28% in MAGI-T to 9.29% in MAGI-FT, and from 12.26% in MAT-T to 8.35% in MAT-FT.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Motion-quality evaluation</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">We evaluate motion quality using video stimuli that only visualised motion, without any audio, in order to have an independent assessment of motion quality. This ensures that ratings are not affected by speech and follows the practice of recent evaluations of gesture quality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>, <a href="#bib.bib78" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">78</span></a>]</cite>. Similarly to the speech evaluation, participants were asked “How natural and humanlike the gesture motion appear?”, and gave responses on a scale of 1 (“Completely unnatural”) to 5 (“Completely natural”). The number of stimuli and attention checks were identical to the speech-only evaluation.</p>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Speech-and-motion appropriateness evaluation</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">We finally evaluated how appropriate the generated speech and motion were for each other, whilst controlling for the effect of their individual quality following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>, <a href="#bib.bib78" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">78</span></a>, <a href="#bib.bib110" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">110</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>, <a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>]</cite>.
For each speech segment and condition, we created two video stimuli: one with the original video and sound, and the other combining the original speech audio with motion from a different video clip, adjusting the motion speed to align with the audio duration. Both videos feature comparable motion quality and characteristics from the same condition, but only one video’s motion is synchronised with the audio track, without indicating which video is which.</p>
</div>
<div id="S4.SS2.SSS3.p2" class="ltx_para">
<p id="S4.SS2.SSS3.p2.2" class="ltx_p">The test inquired which character’s motion most accurately matched the speech in rhythm, intonation, and meaning. Participant ability to identify the correctly synchronised video indicates a strong rhythmic and/or semantic link between generated motion and speech.
Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite> we opted for five response choices instead of the typical three for better resolution. Options were “Left is much better”, “Left is slightly better”, “Both are equal”, “Right is slightly better”, “Right is much better”. For the purposes of analysis, numbers in the range of <math id="S4.SS2.SSS3.p2.1.m1.1" class="ltx_Math" alttext="-2" display="inline"><semantics id="S4.SS2.SSS3.p2.1.m1.1a"><mrow id="S4.SS2.SSS3.p2.1.m1.1.1" xref="S4.SS2.SSS3.p2.1.m1.1.1.cmml"><mo id="S4.SS2.SSS3.p2.1.m1.1.1a" xref="S4.SS2.SSS3.p2.1.m1.1.1.cmml">−</mo><mn id="S4.SS2.SSS3.p2.1.m1.1.1.2" xref="S4.SS2.SSS3.p2.1.m1.1.1.2.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.1.m1.1b"><apply id="S4.SS2.SSS3.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1"><minus id="S4.SS2.SSS3.p2.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1"></minus><cn type="integer" id="S4.SS2.SSS3.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.p2.1.m1.1.1.2">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.1.m1.1c">-2</annotation></semantics></math> to 2 were assigned to each response, as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite>, with <math id="S4.SS2.SSS3.p2.2.m2.1" class="ltx_Math" alttext="-2" display="inline"><semantics id="S4.SS2.SSS3.p2.2.m2.1a"><mrow id="S4.SS2.SSS3.p2.2.m2.1.1" xref="S4.SS2.SSS3.p2.2.m2.1.1.cmml"><mo id="S4.SS2.SSS3.p2.2.m2.1.1a" xref="S4.SS2.SSS3.p2.2.m2.1.1.cmml">−</mo><mn id="S4.SS2.SSS3.p2.2.m2.1.1.2" xref="S4.SS2.SSS3.p2.2.m2.1.1.2.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p2.2.m2.1b"><apply id="S4.SS2.SSS3.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS3.p2.2.m2.1.1"><minus id="S4.SS2.SSS3.p2.2.m2.1.1.1.cmml" xref="S4.SS2.SSS3.p2.2.m2.1.1"></minus><cn type="integer" id="S4.SS2.SSS3.p2.2.m2.1.1.2.cmml" xref="S4.SS2.SSS3.p2.2.m2.1.1.2">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p2.2.m2.1c">-2</annotation></semantics></math> representing the participant’s preference for the mismatched stimulus and 2 the matched stimulus.
Participants reviewed motions from 14 of the 15 segments, displayed as 7 screens of pairs of videos, plus two audio and two video attention checks, covering all conditions for these segments. 70 persons completed the test, yielding 490 responses per system.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and discussion</h2>

<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.17.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S5.T1.18.2" class="ltx_text" style="font-size:90%;">Result of three evaluations showing Mean Opinion Scores (MOS) with 95% confidence intervals.</span></figcaption>
<table id="S5.T1.15" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.15.16.1" class="ltx_tr">
<th id="S5.T1.15.16.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Condition</th>
<th id="S5.T1.15.16.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Speech</th>
<th id="S5.T1.15.16.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Gesture</th>
<th id="S5.T1.15.16.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">Speech &amp; gesture</th>
</tr>
<tr id="S5.T1.3.3" class="ltx_tr">
<th id="S5.T1.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">NAT</th>
<th id="S5.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">4.30<math id="S5.T1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.1.1.1.m1.1a"><mo id="S5.T1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">\pm</annotation></semantics></math>0.06</th>
<th id="S5.T1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">4.10<math id="S5.T1.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.2.2.2.m1.1a"><mo id="S5.T1.2.2.2.m1.1.1" xref="S5.T1.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.m1.1b"><csymbol cd="latexml" id="S5.T1.2.2.2.m1.1.1.cmml" xref="S5.T1.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.m1.1c">\pm</annotation></semantics></math>0.08</th>
<th id="S5.T1.3.3.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">1.10<math id="S5.T1.3.3.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.3.3.3.m1.1a"><mo id="S5.T1.3.3.3.m1.1.1" xref="S5.T1.3.3.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.m1.1b"><csymbol cd="latexml" id="S5.T1.3.3.3.m1.1.1.cmml" xref="S5.T1.3.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.m1.1c">\pm</annotation></semantics></math>0.10</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.6.6" class="ltx_tr">
<th id="S5.T1.6.6.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">MAT-T</th>
<td id="S5.T1.4.4.1" class="ltx_td ltx_align_center ltx_border_t">3.43<math id="S5.T1.4.4.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.4.4.1.m1.1a"><mo id="S5.T1.4.4.1.m1.1.1" xref="S5.T1.4.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.1.m1.1b"><csymbol cd="latexml" id="S5.T1.4.4.1.m1.1.1.cmml" xref="S5.T1.4.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.1.m1.1c">\pm</annotation></semantics></math>0.10</td>
<td id="S5.T1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">3.28<math id="S5.T1.5.5.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.5.5.2.m1.1a"><mo id="S5.T1.5.5.2.m1.1.1" xref="S5.T1.5.5.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.2.m1.1b"><csymbol cd="latexml" id="S5.T1.5.5.2.m1.1.1.cmml" xref="S5.T1.5.5.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.2.m1.1c">\pm</annotation></semantics></math>0.11</td>
<td id="S5.T1.6.6.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.52<math id="S5.T1.6.6.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.6.6.3.m1.1a"><mo id="S5.T1.6.6.3.m1.1.1" xref="S5.T1.6.6.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.6.6.3.m1.1b"><csymbol cd="latexml" id="S5.T1.6.6.3.m1.1.1.cmml" xref="S5.T1.6.6.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.6.3.m1.1c">\pm</annotation></semantics></math>0.10</td>
</tr>
<tr id="S5.T1.9.9" class="ltx_tr">
<th id="S5.T1.9.9.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">MAT-FT</th>
<td id="S5.T1.7.7.1" class="ltx_td ltx_align_center">3.56<math id="S5.T1.7.7.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.7.7.1.m1.1a"><mo id="S5.T1.7.7.1.m1.1.1" xref="S5.T1.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.1.m1.1b"><csymbol cd="latexml" id="S5.T1.7.7.1.m1.1.1.cmml" xref="S5.T1.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.1.m1.1c">\pm</annotation></semantics></math>0.10</td>
<td id="S5.T1.8.8.2" class="ltx_td ltx_align_center">3.39<math id="S5.T1.8.8.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.8.8.2.m1.1a"><mo id="S5.T1.8.8.2.m1.1.1" xref="S5.T1.8.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.8.8.2.m1.1b"><csymbol cd="latexml" id="S5.T1.8.8.2.m1.1.1.cmml" xref="S5.T1.8.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.8.2.m1.1c">\pm</annotation></semantics></math>0.09</td>
<td id="S5.T1.9.9.3" class="ltx_td ltx_nopad_r ltx_align_center">0.56<math id="S5.T1.9.9.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.9.9.3.m1.1a"><mo id="S5.T1.9.9.3.m1.1.1" xref="S5.T1.9.9.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.9.9.3.m1.1b"><csymbol cd="latexml" id="S5.T1.9.9.3.m1.1.1.cmml" xref="S5.T1.9.9.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.9.9.3.m1.1c">\pm</annotation></semantics></math>0.09</td>
</tr>
<tr id="S5.T1.12.12" class="ltx_tr">
<th id="S5.T1.12.12.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">MAGI-T</th>
<td id="S5.T1.10.10.1" class="ltx_td ltx_align_center ltx_border_t">3.44<math id="S5.T1.10.10.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.10.10.1.m1.1a"><mo id="S5.T1.10.10.1.m1.1.1" xref="S5.T1.10.10.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.10.10.1.m1.1b"><csymbol cd="latexml" id="S5.T1.10.10.1.m1.1.1.cmml" xref="S5.T1.10.10.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.10.10.1.m1.1c">\pm</annotation></semantics></math>0.09</td>
<td id="S5.T1.11.11.2" class="ltx_td ltx_align_center ltx_border_t">3.11<math id="S5.T1.11.11.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.11.11.2.m1.1a"><mo id="S5.T1.11.11.2.m1.1.1" xref="S5.T1.11.11.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.11.11.2.m1.1b"><csymbol cd="latexml" id="S5.T1.11.11.2.m1.1.1.cmml" xref="S5.T1.11.11.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.11.11.2.m1.1c">\pm</annotation></semantics></math>0.10</td>
<td id="S5.T1.12.12.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.51<math id="S5.T1.12.12.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.12.12.3.m1.1a"><mo id="S5.T1.12.12.3.m1.1.1" xref="S5.T1.12.12.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.12.12.3.m1.1b"><csymbol cd="latexml" id="S5.T1.12.12.3.m1.1.1.cmml" xref="S5.T1.12.12.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.12.12.3.m1.1c">\pm</annotation></semantics></math>0.09</td>
</tr>
<tr id="S5.T1.15.15" class="ltx_tr">
<th id="S5.T1.15.15.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">MAGI-FT</th>
<td id="S5.T1.13.13.1" class="ltx_td ltx_align_center ltx_border_bb">3.62<math id="S5.T1.13.13.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.13.13.1.m1.1a"><mo id="S5.T1.13.13.1.m1.1.1" xref="S5.T1.13.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.13.13.1.m1.1b"><csymbol cd="latexml" id="S5.T1.13.13.1.m1.1.1.cmml" xref="S5.T1.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.13.13.1.m1.1c">\pm</annotation></semantics></math>0.08</td>
<td id="S5.T1.14.14.2" class="ltx_td ltx_align_center ltx_border_bb">3.52<math id="S5.T1.14.14.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.14.14.2.m1.1a"><mo id="S5.T1.14.14.2.m1.1.1" xref="S5.T1.14.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.14.14.2.m1.1b"><csymbol cd="latexml" id="S5.T1.14.14.2.m1.1.1.cmml" xref="S5.T1.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.14.14.2.m1.1c">\pm</annotation></semantics></math>0.11</td>
<td id="S5.T1.15.15.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">0.60<math id="S5.T1.15.15.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T1.15.15.3.m1.1a"><mo id="S5.T1.15.15.3.m1.1.1" xref="S5.T1.15.15.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.15.15.3.m1.1b"><csymbol cd="latexml" id="S5.T1.15.15.3.m1.1.1.cmml" xref="S5.T1.15.15.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.15.15.3.m1.1c">\pm</annotation></semantics></math>0.09</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Our investigation revealed several key insights into the effect of pre-training and architectural modifications. Pre-training on synthetic data markedly enhanced the quality of synthesised speech, though adjustments to the architecture did not significantly alter its naturalness. Despite this, both MAGI-FT and MAT-FT yielded higher Mean Opinion Scores (MOS), albeit without statistical significance. Notably, MAGI facilitated greater control over pitch and energy – a feature absent from Match-TTSG. However, despite improvements, the synthesised speech did not achieve the level of naturalness present in the human-recorded speech from the held-out set, see Table <a href="#S5.T1" title="Table 1 ‣ 5 Results and discussion ‣ Fake it to make it: Using synthetic data to remedy the data shortage in joint multimodal speech-and-gesture synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">In terms of synthesised gestures, MAGI outperformed other conditions in human-likeness. However, they remained inferior to human-motion reference data. The influence of synthetic data pre-training and the proposed model’s architecture on gesture synthesis presented a more nuanced picture. Specifically, pre-training on synthetic data only significantly benefited the proposed model, and, intriguingly, the MAGI enhanced gestures in a larger dataset but had the opposite effect on a smaller dataset. This discrepancy might stem from the prosody predictors in our model being trained on per-phone rather than per-frame data, leading to a scarcity of training data for these predictors in smaller datasets. However, with adequate pre-training on expansive datasets, these models demonstrated better convergence. These findings align with prior speech evaluations, where the novel architecture’s advantages were more pronounced following pre-training on a larger dataset.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Further, no model matched the cross-modal appropriateness found in multimodal human recordings, echoing the challenges observed in unimodal gesture synthesis where recent evaluations did not approach the appropriateness of human data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">110</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>. Although MAGI pre-trained on synthetic data showcased superior performance, it did not significantly exceed the existing benchmarks in synthesis systems. This observation may be attributed to the inherent difficulty in discerning significant differences in appropriateness, as opposed to naturalness or human-likeness, and the comparison against a robust baseline without alterations that directly influence cross-modal synthesis aspects. Lastly, the cross-modal aspects might conceivably be less accurately represented in synthetic datasets created from unimodal synthesisers trained on non-cohesive data.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Pitch and energy control</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">As stated, the proposed multi-stage architecture with separate prosody predictors allows for modifying or substituting the pitch and energy contours before synthesis.
This enables direct control of prosodic properties of the speech, with the synthesis process having the option to adjust the gestures to match.
On our demo page <a target="_blank" href="https://shivammehta25.github.io/MAGI/" title="" class="ltx_ref ltx_href">shivammehta25.github.io/MAGI/</a> we provide example videos showing the effect that modifying (scaling) the pitch and energy contours returned by the predictors has on the synthesised output.
One can observe that reducing the pitch seems to promote creaky voice, which makes sense from a speech-production perspective and fits earlier findings from autoregressive TTS on spontaneous-speech data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and future work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We have described improvements to the joint and simultaneous multimodal synthesis of speech audio and 3D gesture motion from text.
Specifically, we propose training on data synthesised by a chain of strong unimodal synthesis systems to address the shortage of multimodal training data.
We also augment the state-of-the-art architecture for speech-and-gesture synthesis, Match-TTSG, with a stochastic duration model, TTS-inspired prosody predictors for controllability, and the ability to perform multispeaker synthesis.
The final model, called MAGI, is radically smaller than those that generated the synthetic data.
Experiments confirm that pre-training on synthetic data significantly improved unimodal speech and gesture quality.
The architectural improvements reaped benefits when pre-training on large amounts of synthetic data, with the added prosody control having a clear effect on the audio output.
</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Relevant future work includes investigating alternative options for mitigating the shortage of multimodal training data, such as pre-training on data lacking one or more of the modalities; incorporating RL-based approaches, particularly effective for generation of situated gestures as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>; or (following the CSMP methodology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>) leveraging various self-supervised representations trained on large amounts of data.
Possible architectural extensions include flow matching for pitch and energy, and similar control over motion properties such as gesture radius and symmetry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Acknowledgements</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation, by the Swedish Research Council (VR) projs. 2023-05441 and 2017-00626 (Språkbanken Tal), by Digital Futures, and by the Industrial Strategic Technology Development Program (grant no. 20023495) funded by MOTIE, Korea.


<span id="S7.p1.1.1" class="ltx_text" style="font-size:90%;"></span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Abdullin et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Yelaman Abdullin, Diego Molla-Aliod, Bahadorreza Ofoghi, John Yearwood, and Qingyang Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Synthetic dialogue dataset generation using llm agents.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib1.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2401.17461</em><span id="bib.bib1.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Achiam et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">GPT-4 technical report.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib2.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.08774</em><span id="bib.bib2.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Ahuja et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Chaitanya Ahuja, Dong Won Lee, Ryo Ishii, and Louis-Philippe Morency.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">No gestures left behind: Learning relationships between spoken language and freeform gestures.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib3.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. EMNLP</em><span id="bib.bib3.11.3" class="ltx_text" style="font-size:90%;">, pages 1884–1895, 2020.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.4.4.1" class="ltx_text" style="font-size:90%;">Alexanderson [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.6.1" class="ltx_text" style="font-size:90%;">
Simon Alexanderson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">The StyleGestures entry to the GENEA Challenge 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib4.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. GENEA Workshop</em><span id="bib.bib4.10.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Alexanderson et al. [2020a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Simon Alexanderson, Gustav Eje Henter, Taras Kucherenko, and Jonas Beskow.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Style-controllable speech-driven gesture synthesis using normalising flows.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib5.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Comput. Graph. Forum</em><span id="bib.bib5.10.2" class="ltx_text" style="font-size:90%;">, 39(2):487–496, 2020a.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Alexanderson et al. [2020b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Simon Alexanderson, Éva Székely, Gustav Eje Henter, Taras Kucherenko, and Jonas Beskow.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">Generating coherent spontaneous speech and gesture from text.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib6.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IVA</em><span id="bib.bib6.11.3" class="ltx_text" style="font-size:90%;">, pages 1–3, 2020b.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Alexanderson et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and Gustav Eje Henter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">Listen, denoise, action! Audio-driven motion synthesis with diffusion models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib7.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM Trans. Graph.</em><span id="bib.bib7.10.2" class="ltx_text" style="font-size:90%;">, 42(4):1–20, 2023.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Ao et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Tenglong Ao, Zeyi Zhang, and Libin Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">GestureDiffuCLIP: Gesture diffusion model with CLIP latents.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib8.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM Trans. Graph.</em><span id="bib.bib8.10.2" class="ltx_text" style="font-size:90%;">, 42(4):1–18, 2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Baevski et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">data2vec: A general framework for self-supervised learning in speech, vision and language.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib9.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Machine Learning</em><span id="bib.bib9.11.3" class="ltx_text" style="font-size:90%;">, pages 1298–1312, 2022.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Bai et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">Training a helpful and harmless assistant with reinforcement learning from human feedback.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib10.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2204.05862</em><span id="bib.bib10.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.4.4.1" class="ltx_text" style="font-size:90%;">Betker [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.6.1" class="ltx_text" style="font-size:90%;">
James Betker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">Better speech synthesis through scaling.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib11.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.07243</em><span id="bib.bib11.9.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Betker et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Improving image generation with better captions, 2023.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Borsos et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">Audiolm: a language modeling approach to audio generation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib13.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em><span id="bib.bib13.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text" style="font-size:90%;">Brown et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">Language models are few-shot learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib14.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib14.11.3" class="ltx_text" style="font-size:90%;">, pages 1877–1901, 2020.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">Cassell et al. [2000]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
Justine Cassell, Joseph Sullivan, Scott Prevost, and Elizabeth Churchill.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib15.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Embodied conversational agents</em><span id="bib.bib15.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.10.1" class="ltx_text" style="font-size:90%;">MIT press, 2000.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.4.4.1" class="ltx_text" style="font-size:90%;">Coqui.ai [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.6.1" class="ltx_text" style="font-size:90%;">
Coqui.ai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">xTTS - TTS 0.22.0 documentation, 2023.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Défossez et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">High fidelity neural audio compression.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib17.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2210.13438</em><span id="bib.bib17.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">Deichler et al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Anna Deichler, Shivam Mehta, Simon Alexanderson, and Jonas Beskow.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">Diffusion-based co-speech gesture generation using joint text and audio representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib18.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 25th International Conference on Multimodal Interaction</em><span id="bib.bib18.11.3" class="ltx_text" style="font-size:90%;">, pages 755–762, 2023a.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Deichler et al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Anna Deichler, Siyang Wang, Simon Alexanderson, and Jonas Beskow.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Learning to generate pointing gestures in situated embodied conversational agents.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib19.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Frontiers in Robotics and AI</em><span id="bib.bib19.10.2" class="ltx_text" style="font-size:90%;">, 10:1110534, 2023b.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.4.4.1" class="ltx_text" style="font-size:90%;">Ferstl and McDonnell [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.6.1" class="ltx_text" style="font-size:90%;">
Ylva. Ferstl and Rachel McDonnell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">Investigating the use of recurrent motion modelling for speech gesture generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib20.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IVA</em><span id="bib.bib20.10.3" class="ltx_text" style="font-size:90%;">, pages 93–98, 2018.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.4.4.1" class="ltx_text" style="font-size:90%;">Ferstl and McDonnell [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.6.1" class="ltx_text" style="font-size:90%;">
Ylva Ferstl and Rachel McDonnell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">Multi-task learning for continuous control of non-verbal behaviour in humanoid social robots.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib21.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction (HRI)</em><span id="bib.bib21.10.3" class="ltx_text" style="font-size:90%;">, pages 411–420. IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Ferstl et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Ylva Ferstl, Michael Neff, and Rachel McDonnell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Adversarial gesture generation with realistic gesture phasing.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib22.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Comput. Graph.</em><span id="bib.bib22.10.2" class="ltx_text" style="font-size:90%;">, 89:117–130, 2020.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Ferstl et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Ylva Ferstl, Michael Neff, and Rachel McDonnell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">ExpressGesture: Expressive gesture generation from speech through database matching.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib23.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Comput. Animat. Virt. W.</em><span id="bib.bib23.10.2" class="ltx_text" style="font-size:90%;">, page e2016, 2021.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Ghorbani et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Saeed Ghorbani, Ylva Ferstl, Daniel Holden, Nikolaus F. Troje, and Marc-André Carbonneau.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">ZeroEGGS: Zero-shot example-based gesture generation from speech.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib24.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Comput. Graph. Forum</em><span id="bib.bib24.10.2" class="ltx_text" style="font-size:90%;">, 42(1):206–216, 2023.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.4.4.1" class="ltx_text" style="font-size:90%;">Grassia [1998]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.6.1" class="ltx_text" style="font-size:90%;">
F. Sebastian Grassia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">Practical parameterization of rotations using the exponential map.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib25.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">J. Graph. Tool.</em><span id="bib.bib25.9.2" class="ltx_text" style="font-size:90%;">, 3(3):29–48, 1998.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Guo et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Yiwei Guo, Chenpeng Du, Ziyang Ma, Xie Chen, and Kai Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">VoiceFlow: Efficient text-to-speech with rectified flow matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib26.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. ICASSP</em><span id="bib.bib26.11.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Habibie et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Ikhsanul Habibie, Weipeng Xu, Dushyant Mehta, Lingjie Liu, Hans-Peter Seidel, Gerard Pons-Moll, Mohamed Elgharib, and Christian Theobalt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">Learning speech-driven 3D conversational gestures from video.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib27.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Intelligent Virtual Agents</em><span id="bib.bib27.11.3" class="ltx_text" style="font-size:90%;">, pages 101–108, 2021.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Henighan et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">Scaling laws for autoregressive generative modeling.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib28.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2010.14701</em><span id="bib.bib28.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Hensel et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Laura Birka Hensel, Nutchanon Yongsatianchot, Parisa Torshizi, Elena Minucci, and Stacy Marsella.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Large language models in textual analysis for gesture selection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib29.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 25th International Conference on Multimodal Interaction</em><span id="bib.bib29.11.3" class="ltx_text" style="font-size:90%;">, pages 378–387, 2023.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="font-size:90%;">Hinton et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">Distilling the knowledge in a neural network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib30.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NIPS Deep Learning and Representation Learning Workshop</em><span id="bib.bib30.11.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.4.4.1" class="ltx_text" style="font-size:90%;">Hostetter [2007]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.6.1" class="ltx_text" style="font-size:90%;">
Autumn B Hostetter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">When do gestures communicate? a meta-analysis.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib31.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Psychological Bulletin</em><span id="bib.bib31.9.2" class="ltx_text" style="font-size:90%;">, 133(2):297, 2007.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text" style="font-size:90%;">Hu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">
Vincent Tao Hu, Wenzhe Yin, Pingchuan Ma, Yunlu Chen, Basura Fernando, Yuki M Asano, Efstratios Gavves, Pascal Mettes, Björn Ommer, and Cees G. M. Snoek.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">Motion flow matching for human motion synthesis and editing.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib32.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2312.08895</em><span id="bib.bib32.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.4.4.1" class="ltx_text" style="font-size:90%;">ITU-T P.800 [1996]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.6.1" class="ltx_text" style="font-size:90%;">
ITU-T P.800.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">Methods for subjective determination of transmission quality.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">Standard, ITU, 1996.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">Jonell et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
Patrik Jonell, Taras Kucherenko, Gustav Eje Henter, and Jonas Beskow.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">Let’s face it: Probabilistic multi-modal interlocutor-aware generation of facial gestures in dyadic settings.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib34.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IVA</em><span id="bib.bib34.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.4.4.1" class="ltx_text" style="font-size:90%;">Kendon [1988]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.6.1" class="ltx_text" style="font-size:90%;">
Adam Kendon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">How gestures can become like words.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib35.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Cross-Cultural Perspectives in Nonverbal Communication</em><span id="bib.bib35.10.3" class="ltx_text" style="font-size:90%;">. C. J. Hogrefe, Inc., 1988.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.5.5.1" class="ltx_text" style="font-size:90%;">Kim et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">
Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies, Matthias Niessner, Patrick Pérez, Christian Richardt, Michael Zollhöfer, and Christian Theobalt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="font-size:90%;">Neural style-preserving visual dubbing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib36.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 32nd International Conference on Neural Information Processing Systems</em><span id="bib.bib36.11.3" class="ltx_text" style="font-size:90%;">, pages 2535–2545, 2019.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text" style="font-size:90%;">Kim et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">
Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh Yoon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">Glow-TTS: A generative flow for text-to-speech via monotonic alignment search.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib37.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. NeurIPS</em><span id="bib.bib37.11.3" class="ltx_text" style="font-size:90%;">, pages 8067–8077, 2020.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text" style="font-size:90%;">Kim et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">
Jaehyeon Kim, Jungil Kong, and Juhee Son.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">VITS: Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib38.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. ICML</em><span id="bib.bib38.11.3" class="ltx_text" style="font-size:90%;">, pages 5530–5540, 2021.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.5.5.1" class="ltx_text" style="font-size:90%;">Kim et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">
Jihoon Kim, Jiseob Kim, and Sungjoon Choi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">Flame: Free-form language-based motion synthesis &amp; editing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib39.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial Intelligence</em><span id="bib.bib39.11.3" class="ltx_text" style="font-size:90%;">, pages 8255–8263, 2023.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="font-size:90%;">Kong et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">
Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib40.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. NeurIPS</em><span id="bib.bib40.11.3" class="ltx_text" style="font-size:90%;">, pages 17022–17033, 2020.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Kong et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
Jungil Kong, Jihoon Park, Beomjeong Kim, Jeongmin Kim, Dohee Kong, and Sangjin Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">VITS2: Improving quality and efficiency of single-stage text-to-speech with adversarial learning and architecture design.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib41.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. Interspeech</em><span id="bib.bib41.11.3" class="ltx_text" style="font-size:90%;">, pages 4374–4378, 2023.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.4.4.1" class="ltx_text" style="font-size:90%;">Kopp and Wachsmuth [2004]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.6.1" class="ltx_text" style="font-size:90%;">
Stefan Kopp and Ipke Wachsmuth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">Synthesizing multimodal utterances for conversational agents.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib42.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Animation and Virtual Worlds</em><span id="bib.bib42.10.3" class="ltx_text" style="font-size:90%;">, pages 39–52. Wiley Online Library, 2004.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text" style="font-size:90%;">Kucherenko et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">
Taras Kucherenko, Patrik Jonell, Sanne van Waveren, Gustav Eje Henter, Simon Alexanderson, Iolanda Leite, and Hedvig Kjellström.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="font-size:90%;">Gesticulator: A framework for semantically-aware speech-driven gesture generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib43.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the ACM International Conference on Intelligent Virtual Agents</em><span id="bib.bib43.11.3" class="ltx_text" style="font-size:90%;">, pages 242–250, 2020.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text" style="font-size:90%;">Kucherenko et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">
Taras Kucherenko, Rajmund Nagy, Michael Neff, Hedvig Kjellström, and Gustav Eje Henter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text" style="font-size:90%;">Multimodal analysis of the predictability of hand-gesture properties.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib44.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Autonomous Agents and Multiagent Systems</em><span id="bib.bib44.11.3" class="ltx_text" style="font-size:90%;">, pages 770–779, 2022.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text" style="font-size:90%;">Kucherenko et al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="font-size:90%;">
Taras Kucherenko, Rajmund Nagy, Youngwoo Yoon, Jieyeon Woo, Teodor Nikolov, Mihail Tsakov, and Gustav Eje Henter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="font-size:90%;">The GENEA Challenge 2023: A large-scale evaluation of gesture generation models in monadic and dyadic settings.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib45.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Multimodal Interaction</em><span id="bib.bib45.11.3" class="ltx_text" style="font-size:90%;">, pages 792–801, 2023a.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.5.5.1" class="ltx_text" style="font-size:90%;">Kucherenko et al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="font-size:90%;">
Taras Kucherenko, Pieter Wolfert, Youngwoo Yoon, Carla Viegas, Teodor Nikolov, Mihail Tsakov, and Gustav Eje Henter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.8.1" class="ltx_text" style="font-size:90%;">Evaluating gesture-generation in a large-scale open challenge: The GENEA Challenge 2022.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib46.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.08737</em><span id="bib.bib46.10.2" class="ltx_text" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.5.5.1" class="ltx_text" style="font-size:90%;">Lajszczak et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text" style="font-size:90%;">
Mateusz Lajszczak, Guillermo Cambara Ruiz, Yang Li, Fatih Beyhan, Arent van Korlaar, Fan Yang, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.8.1" class="ltx_text" style="font-size:90%;">Base tts: Lessons from building a billion-parameter text-to-speech model on 100k hours of data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib47.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv</em><span id="bib.bib47.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text" style="font-size:90%;">Lameris et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text" style="font-size:90%;">
Harm Lameris, Shivam Mehta, Gustav Eje Henter, Joakim Gustafson, and Éva Székely.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text" style="font-size:90%;">Prosody-controllable spontaneous TTS with neural HMMs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib48.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. ICASSP</em><span id="bib.bib48.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.5.5.1" class="ltx_text" style="font-size:90%;">Le et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.7.1" class="ltx_text" style="font-size:90%;">
Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal Manohar, Yossi Adi, Jay Mahadeokar, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.8.1" class="ltx_text" style="font-size:90%;">Voicebox: Text-guided multilingual universal speech generation at scale.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib49.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.15687</em><span id="bib.bib49.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.5.5.1" class="ltx_text" style="font-size:90%;">Lee et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.7.1" class="ltx_text" style="font-size:90%;">
Gilwoo Lee, Zhiwei Deng, Shugao Ma, Takaaki Shiratori, Siddhartha S. Srinivasa, and Yaser Sheikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.8.1" class="ltx_text" style="font-size:90%;">Talking With Hands 16.2 M: A large-scale dataset of synchronized body-finger motion and audio for conversational motion analysis and synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib50.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib50.11.3" class="ltx_text" style="font-size:90%;">, pages 763–772, 2019.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.7.1" class="ltx_text" style="font-size:90%;">
Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.8.1" class="ltx_text" style="font-size:90%;">Neural speech synthesis with transformer network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib51.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI conference on artificial intelligence</em><span id="bib.bib51.11.3" class="ltx_text" style="font-size:90%;">, pages 6706–6713, 2019.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.5.5.1" class="ltx_text" style="font-size:90%;">Lipman et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.7.1" class="ltx_text" style="font-size:90%;">
Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.8.1" class="ltx_text" style="font-size:90%;">Flow matching for generative modeling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib52.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. ICLR</em><span id="bib.bib52.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2022a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.7.1" class="ltx_text" style="font-size:90%;">
Alexander H. Liu, Cheng-I Jeff Lai, Wei-Ning Hsu, Michael Auli, Alexei Baevski, and James Glass.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.8.1" class="ltx_text" style="font-size:90%;">Simple and effective unsupervised speech synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib53.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. Interspeech</em><span id="bib.bib53.11.3" class="ltx_text" style="font-size:90%;">, pages 843–847, 2022a.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2022b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.7.1" class="ltx_text" style="font-size:90%;">
Haiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen Peng, Zhengqing Li, You Zhou, Elif Bozkurt, and Bo Zheng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.8.1" class="ltx_text" style="font-size:90%;">BEAT: A large-scale semantic and emotional multi-modal dataset for conversational gestures synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib54.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision</em><span id="bib.bib54.11.3" class="ltx_text" style="font-size:90%;">, pages 612–630, 2022b.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.7.1" class="ltx_text" style="font-size:90%;">
Yu Liu, Gelareh Mohammadi, Yang Song, and Wafa Johal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.8.1" class="ltx_text" style="font-size:90%;">Speech-based gesture generation for robots and embodied agents: A scoping review.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib55.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Human-Agent Interaction</em><span id="bib.bib55.11.3" class="ltx_text" style="font-size:90%;">, pages 31–38, 2021.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.5.5.1" class="ltx_text" style="font-size:90%;">Lu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.7.1" class="ltx_text" style="font-size:90%;">
Shuhong Lu, Youngwoo Yoon, and Andrew Feng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.8.1" class="ltx_text" style="font-size:90%;">Co-speech gesture synthesis using discrete gesture token learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib56.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em><span id="bib.bib56.11.3" class="ltx_text" style="font-size:90%;">, pages 9808–9815. IEEE, 2023.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.5.5.1" class="ltx_text" style="font-size:90%;">Mariooryad et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.7.1" class="ltx_text" style="font-size:90%;">
Soroosh Mariooryad, Matt Shannon, Siyuan Ma, Tom Bagby, David Kao, Daisy Stanton, Eric Battenberg, and RJ Skerry-Ryan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.8.1" class="ltx_text" style="font-size:90%;">Learning the joint distribution of two sequences using little or no paired data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib57.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2212.03232</em><span id="bib.bib57.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.5.5.1" class="ltx_text" style="font-size:90%;">McAuliffe et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.7.1" class="ltx_text" style="font-size:90%;">
Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.8.1" class="ltx_text" style="font-size:90%;">Montreal Forced Aligner: Trainable text-speech alignment using Kaldi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib58.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. Interspeech 2017</em><span id="bib.bib58.11.3" class="ltx_text" style="font-size:90%;">, pages 498–502, 2017.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.4.4.1" class="ltx_text" style="font-size:90%;">McNeill [1992]</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.6.1" class="ltx_text" style="font-size:90%;">
David McNeill.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib59.7.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Hand and mind: What gestures reveal about thought</em><span id="bib.bib59.8.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.9.1" class="ltx_text" style="font-size:90%;">University of Chicago Press, 1992.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib60.4.4.1" class="ltx_text" style="font-size:90%;">McNeill [2008]</span></span>
<span class="ltx_bibblock"><span id="bib.bib60.6.1" class="ltx_text" style="font-size:90%;">
David McNeill.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib60.7.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Gesture and Thought</em><span id="bib.bib60.8.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.9.1" class="ltx_text" style="font-size:90%;">University of Chicago Press, 2008.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib61.5.5.1" class="ltx_text" style="font-size:90%;">Mehta et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib61.7.1" class="ltx_text" style="font-size:90%;">
Shivam Mehta, Éva Székely, Jonas Beskow, and Gustav Eje Henter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.8.1" class="ltx_text" style="font-size:90%;">Neural HMMs are all you need (for high-quality attention-free TTS).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib61.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. ICASSP</em><span id="bib.bib61.11.3" class="ltx_text" style="font-size:90%;">, pages 7457–7461, 2022.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib62.5.5.1" class="ltx_text" style="font-size:90%;">Mehta et al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib62.7.1" class="ltx_text" style="font-size:90%;">
Shivam Mehta, Ambika Kirkland, Harm Lameris, Jonas Beskow, Éva Székely, and Gustav Eje Henter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.8.1" class="ltx_text" style="font-size:90%;">OverFlow: Putting flows on top of neural transducers for better TTS.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib62.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. Interspeech</em><span id="bib.bib62.11.3" class="ltx_text" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib63.5.5.1" class="ltx_text" style="font-size:90%;">Mehta et al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib63.7.1" class="ltx_text" style="font-size:90%;">
Shivam Mehta, Siyang Wang, Simon Alexanderson, Jonas Beskow, Éva Székely, and Gustav Eje Henter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.8.1" class="ltx_text" style="font-size:90%;">Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib63.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. SSW</em><span id="bib.bib63.11.3" class="ltx_text" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib64.5.5.1" class="ltx_text" style="font-size:90%;">Mehta et al. [2024a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib64.7.1" class="ltx_text" style="font-size:90%;">
Shivam Mehta, Ruibo Tu, Simon Alexanderson, Jonas Beskow, Éva Székely, and Gustav Eje Henter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.8.1" class="ltx_text" style="font-size:90%;">Unified speech and gesture synthesis using flow matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib64.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. ICASSP</em><span id="bib.bib64.11.3" class="ltx_text" style="font-size:90%;">, 2024a.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib65.5.5.1" class="ltx_text" style="font-size:90%;">Mehta et al. [2024b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib65.7.1" class="ltx_text" style="font-size:90%;">
Shivam Mehta, Ruibo Tu, Jonas Beskow, Éva Székely, and Gustav Eje Henter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.8.1" class="ltx_text" style="font-size:90%;">Matcha-TTS: A fast TTS architecture with conditional flow matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib65.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. ICASSP</em><span id="bib.bib65.11.3" class="ltx_text" style="font-size:90%;">, 2024b.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib66.5.5.1" class="ltx_text" style="font-size:90%;">Miao et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib66.7.1" class="ltx_text" style="font-size:90%;">
Xiaoxiao Miao, Xin Wang, Erica Cooper, Junichi Yamagishi, Nicholas Evans, Massimiliano Todisco, Jean-François Bonastre, and Mickael Rouvier.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.8.1" class="ltx_text" style="font-size:90%;">SynVox2: Towards a privacy-friendly VoxCeleb2 dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib66.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. ICASSP</em><span id="bib.bib66.11.3" class="ltx_text" style="font-size:90%;">, pages 11421–11425, 2024.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib67.5.5.1" class="ltx_text" style="font-size:90%;">Ng et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib67.7.1" class="ltx_text" style="font-size:90%;">
Evonne Ng, Javier Romero, Timur Bagautdinov, Shaojie Bai, Trevor Darrell, Angjoo Kanazawa, and Alexander Richard.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.8.1" class="ltx_text" style="font-size:90%;">From audio to photoreal embodiment: Synthesizing humans in conversations.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib67.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2401.01885</em><span id="bib.bib67.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib68.5.5.1" class="ltx_text" style="font-size:90%;">Ni et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib68.7.1" class="ltx_text" style="font-size:90%;">
Junrui Ni, Liming Wang, Heting Gao, Kaizhi Qian, Yang Zhang, Shiyu Chang, and Mark Hasegawa-Johnson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.8.1" class="ltx_text" style="font-size:90%;">Unsupervised text-to-speech synthesis by unsupervised automatic speech recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib68.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. Interspeech</em><span id="bib.bib68.11.3" class="ltx_text" style="font-size:90%;">, pages 461–465, 2022.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib69.5.5.1" class="ltx_text" style="font-size:90%;">Nyatsanga et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib69.7.1" class="ltx_text" style="font-size:90%;">
Simbarashe Nyatsanga, Taras Kucherenko, Chaitanya Ahuja, Gustav Eje Henter, and Michael Neff.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.8.1" class="ltx_text" style="font-size:90%;">A comprehensive review of data-driven co-speech gesture generation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib69.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Comput. Graph. Forum</em><span id="bib.bib69.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib70.5.5.1" class="ltx_text" style="font-size:90%;">Ogun et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib70.7.1" class="ltx_text" style="font-size:90%;">
Sewade Ogun, Vincent Colotte, and Emmanuel Vincent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.8.1" class="ltx_text" style="font-size:90%;">Stochastic pitch prediction improves the diversity and naturalness of speech in Glow-TTS.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib70.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. Interspeech</em><span id="bib.bib70.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib71.5.5.1" class="ltx_text" style="font-size:90%;">Pelachaud et al. [1996]</span></span>
<span class="ltx_bibblock"><span id="bib.bib71.7.1" class="ltx_text" style="font-size:90%;">
Catherine Pelachaud, Norman I Badler, and Mark Steedman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.8.1" class="ltx_text" style="font-size:90%;">Modeling and animating conversational agents.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib71.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Adaptive hypertext and hypermedia</em><span id="bib.bib71.11.3" class="ltx_text" style="font-size:90%;">, pages 21–30. Springer, 1996.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib72.5.5.1" class="ltx_text" style="font-size:90%;">Popov et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib72.7.1" class="ltx_text" style="font-size:90%;">
Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.8.1" class="ltx_text" style="font-size:90%;">Grad-TTS: A diffusion probabilistic model for text-to-speech.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib72.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. ICML</em><span id="bib.bib72.11.3" class="ltx_text" style="font-size:90%;">, pages 8599–8608, 2021.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib73.5.5.1" class="ltx_text" style="font-size:90%;">Prahallad et al. [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib73.7.1" class="ltx_text" style="font-size:90%;">
Kishore Prahallad, Anandaswarup Vadapalli, Naresh Elluru, Gautam Mantena, Bhargav Pulugundla, Peri Bhaskararao, Hema A. Murthy, Simon King, Vasilis Karaiskos, and Alan W. Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.8.1" class="ltx_text" style="font-size:90%;">The Blizzard Challenge 2013–Indian language task.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib73.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the Blizzard Challenge Workshop</em><span id="bib.bib73.11.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib74.5.5.1" class="ltx_text" style="font-size:90%;">Radford et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib74.7.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.8.1" class="ltx_text" style="font-size:90%;">Improving language understanding by generative pre-training, 2018.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib75.5.5.1" class="ltx_text" style="font-size:90%;">Radford et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib75.7.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.8.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib75.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Machine Learning</em><span id="bib.bib75.11.3" class="ltx_text" style="font-size:90%;">, pages 8748–8763, 2021.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib76.5.5.1" class="ltx_text" style="font-size:90%;">Radford et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib76.7.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.8.1" class="ltx_text" style="font-size:90%;">Robust speech recognition via large-scale weak supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib76.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Machine Learning</em><span id="bib.bib76.11.3" class="ltx_text" style="font-size:90%;">, pages 28492–28518, 2023.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib77.5.5.1" class="ltx_text" style="font-size:90%;">Ramesh et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib77.7.1" class="ltx_text" style="font-size:90%;">
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.8.1" class="ltx_text" style="font-size:90%;">Hierarchical text-conditional image generation with CLIP latents.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib77.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2204.06125</em><span id="bib.bib77.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib78.5.5.1" class="ltx_text" style="font-size:90%;">Rebol et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib78.7.1" class="ltx_text" style="font-size:90%;">
Manuel Rebol, Christian Güti, and Krzysztof Pietroszek.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.8.1" class="ltx_text" style="font-size:90%;">Passing a non-verbal Turing test: Evaluating gesture animations generated from speech.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib78.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Virtual Reality and 3D User Interfaces</em><span id="bib.bib78.11.3" class="ltx_text" style="font-size:90%;">, pages 573–581, 2021.
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib79.5.5.1" class="ltx_text" style="font-size:90%;">Ren et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib79.7.1" class="ltx_text" style="font-size:90%;">
Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.8.1" class="ltx_text" style="font-size:90%;">FastSpeech 2: Fast and high-quality end-to-end text to speech.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib79.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. ICLR</em><span id="bib.bib79.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib80.5.5.1" class="ltx_text" style="font-size:90%;">Ren et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib80.7.1" class="ltx_text" style="font-size:90%;">
Yi Ren, Xu Tan, Tao Qin, Zhou Zhao, and Tie-Yan Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.8.1" class="ltx_text" style="font-size:90%;">Revisiting over-smoothness in text to speech.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib80.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. ACL</em><span id="bib.bib80.11.3" class="ltx_text" style="font-size:90%;">, pages 8197–8213, 2022.
</span>
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib81.5.5.1" class="ltx_text" style="font-size:90%;">Rombach et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib81.7.1" class="ltx_text" style="font-size:90%;">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.8.1" class="ltx_text" style="font-size:90%;">High-resolution image synthesis with latent diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib81.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. CVPR</em><span id="bib.bib81.11.3" class="ltx_text" style="font-size:90%;">, pages 10684–10695, 2022.
</span>
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib82.5.5.1" class="ltx_text" style="font-size:90%;">Salem et al. [2010]</span></span>
<span class="ltx_bibblock"><span id="bib.bib82.7.1" class="ltx_text" style="font-size:90%;">
Maha Salem, Stefan Kopp, Ipke Wachsmuth, and Frank Joublin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.8.1" class="ltx_text" style="font-size:90%;">Towards an integrated model of speech and gesture production for multi-modal robot behavior.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib82.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. RO-MAN</em><span id="bib.bib82.11.3" class="ltx_text" style="font-size:90%;">, pages 614–619, 2010.
</span>
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib83.5.5.1" class="ltx_text" style="font-size:90%;">Shen et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib83.7.1" class="ltx_text" style="font-size:90%;">
Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.8.1" class="ltx_text" style="font-size:90%;">Natural TTS synthesis by conditioning WaveNet on mel spectrogram predictions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib83.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. ICASSP</em><span id="bib.bib83.11.3" class="ltx_text" style="font-size:90%;">, pages 4779–4783, 2018.
</span>
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib84.5.5.1" class="ltx_text" style="font-size:90%;">Shen et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib84.7.1" class="ltx_text" style="font-size:90%;">
Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.8.1" class="ltx_text" style="font-size:90%;">NaturalSpeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib84.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2304.09116</em><span id="bib.bib84.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib85.4.4.1" class="ltx_text" style="font-size:90%;">Siuzdak [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib85.6.1" class="ltx_text" style="font-size:90%;">
Hubert Siuzdak.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.7.1" class="ltx_text" style="font-size:90%;">Vocos: Closing the gap between time-domain and fourier-based neural vocoders for high-quality audio synthesis.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib85.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.00814</em><span id="bib.bib85.9.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib86.5.5.1" class="ltx_text" style="font-size:90%;">Song et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib86.7.1" class="ltx_text" style="font-size:90%;">
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.8.1" class="ltx_text" style="font-size:90%;">Score-based generative modeling through stochastic differential equations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib86.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. ICLR</em><span id="bib.bib86.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib87.5.5.1" class="ltx_text" style="font-size:90%;">Tan et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib87.7.1" class="ltx_text" style="font-size:90%;">
Xu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.8.1" class="ltx_text" style="font-size:90%;">A survey on neural speech synthesis.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib87.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2106.15561</em><span id="bib.bib87.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib88.5.5.1" class="ltx_text" style="font-size:90%;">Tan et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib88.7.1" class="ltx_text" style="font-size:90%;">
Xu Tan, Jiawei Chen, Haohe Liu, Jian Cong, Chen Zhang, Yanqing Liu, Xi Wang, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib88.8.1" class="ltx_text" style="font-size:90%;">NaturalSpeech: End-to-end text to speech synthesis with human-level quality.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib88.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2205.04421</em><span id="bib.bib88.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib89.4.4.1" class="ltx_text" style="font-size:90%;">Taylor [2009]</span></span>
<span class="ltx_bibblock"><span id="bib.bib89.6.1" class="ltx_text" style="font-size:90%;">
Paul Taylor.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib89.7.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Text-to-speech synthesis</em><span id="bib.bib89.8.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib89.9.1" class="ltx_text" style="font-size:90%;">Cambridge University Press, 2009.
</span>
</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib90.5.5.1" class="ltx_text" style="font-size:90%;">Taylor et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib90.7.1" class="ltx_text" style="font-size:90%;">
Sarah Taylor, Jonathan Windle, David Greenwood, and Iain Matthews.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib90.8.1" class="ltx_text" style="font-size:90%;">Speech-driven conversational agents using conditional Flow-VAEs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib90.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib90.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the ACM European Conference on Visual Media Production</em><span id="bib.bib90.11.3" class="ltx_text" style="font-size:90%;">, pages 6:1–6:9, 2021.
</span>
</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib91.5.5.1" class="ltx_text" style="font-size:90%;">Tevet et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib91.7.1" class="ltx_text" style="font-size:90%;">
Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H. Bermano.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib91.8.1" class="ltx_text" style="font-size:90%;">Human motion diffusion model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib91.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib91.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Learning Representations</em><span id="bib.bib91.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib92.5.5.1" class="ltx_text" style="font-size:90%;">Touvron et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib92.7.1" class="ltx_text" style="font-size:90%;">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib92.8.1" class="ltx_text" style="font-size:90%;">Llama: Open and efficient foundation language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib92.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2302.13971</em><span id="bib.bib92.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib93.4.4.1" class="ltx_text" style="font-size:90%;">van Breugel and van der Schaar [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib93.6.1" class="ltx_text" style="font-size:90%;">
Boris van Breugel and Mihaela van der Schaar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib93.7.1" class="ltx_text" style="font-size:90%;">Beyond privacy: Navigating the opportunities and challenges of synthetic data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib93.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2304.03722</em><span id="bib.bib93.9.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib94.5.5.1" class="ltx_text" style="font-size:90%;">van den Oord et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib94.7.1" class="ltx_text" style="font-size:90%;">
Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib94.8.1" class="ltx_text" style="font-size:90%;">Parallel WaveNet: Fast high-fidelity speech synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib94.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib94.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Machine Learning</em><span id="bib.bib94.11.3" class="ltx_text" style="font-size:90%;">, pages 3918–3926, 2018.
</span>
</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib95.5.5.1" class="ltx_text" style="font-size:90%;">Varol et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib95.7.1" class="ltx_text" style="font-size:90%;">
Gül Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J Black, Ivan Laptev, and Cordelia Schmid.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib95.8.1" class="ltx_text" style="font-size:90%;">Learning from synthetic humans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib95.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib95.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib95.11.3" class="ltx_text" style="font-size:90%;">, pages 109–117, 2017.
</span>
</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib96.5.5.1" class="ltx_text" style="font-size:90%;">Vaswani et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib96.7.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib96.8.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib96.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib96.10.2" class="ltx_text" style="font-size:90%;">, 30, 2017.
</span>
</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib97.5.5.1" class="ltx_text" style="font-size:90%;">Wagner et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib97.7.1" class="ltx_text" style="font-size:90%;">
Petra Wagner, Zofia Malisz, and Stefan Kopp.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib97.8.1" class="ltx_text" style="font-size:90%;">Gesture and speech in interaction: An overview.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib97.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Speech Communication</em><span id="bib.bib97.10.2" class="ltx_text" style="font-size:90%;">, 57:209–232, 2014.
</span>
</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib98.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib98.7.1" class="ltx_text" style="font-size:90%;">
Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib98.8.1" class="ltx_text" style="font-size:90%;">Neural codec language models are zero-shot text to speech synthesizers.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib98.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2301.02111</em><span id="bib.bib98.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib99.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib99.7.1" class="ltx_text" style="font-size:90%;">
Siyang Wang, Simon Alexanderson, Joakim Gustafson, Jonas Beskow, Gustav Eje Henter, and Éva Székely.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib99.8.1" class="ltx_text" style="font-size:90%;">Integrated speech and gesture synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib99.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib99.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. ICMI</em><span id="bib.bib99.11.3" class="ltx_text" style="font-size:90%;">, pages 177–185, 2021.
</span>
</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib100.5.5.1" class="ltx_text" style="font-size:90%;">Wu et al. [2021a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib100.7.1" class="ltx_text" style="font-size:90%;">
Bowen Wu, Chaoran Liu, Carlos T. Ishi, and Hiroshi Ishiguro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib100.8.1" class="ltx_text" style="font-size:90%;">Modeling the conditional distribution of co-speech upper body gesture jointly using conditional-GAN and unrolled-GAN.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib100.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Electronics</em><span id="bib.bib100.10.2" class="ltx_text" style="font-size:90%;">, 10(3):228, 2021a.
</span>
</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib101.5.5.1" class="ltx_text" style="font-size:90%;">Wu et al. [2021b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib101.7.1" class="ltx_text" style="font-size:90%;">
Bowen Wu, Chaoran Liu, Carlos T. Ishi, and Hiroshi Ishiguro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib101.8.1" class="ltx_text" style="font-size:90%;">Probabilistic human-like gesture synthesis from speech using GRU-based WGAN.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib101.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib101.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Companion Publication of the International Conference on Multimodal Interaction</em><span id="bib.bib101.11.3" class="ltx_text" style="font-size:90%;">, pages 194–201, 2021b.
</span>
</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib102.5.5.1" class="ltx_text" style="font-size:90%;">Yamagishi et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib102.7.1" class="ltx_text" style="font-size:90%;">
Junichi Yamagishi, Christophe Veaux, and Kirsten MacDonald.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib102.8.1" class="ltx_text" style="font-size:90%;">CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit (version 0.92), 2019.
</span>
</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib103.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib103.7.1" class="ltx_text" style="font-size:90%;">
Sicheng Yang, Zhiyong Wu, Minglei Li, Zhensong Zhang, Lei Hao, Weihong Bao, Ming Cheng, and Long Xiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib103.8.1" class="ltx_text" style="font-size:90%;">Diffusestylegesture: stylized audio-driven co-speech gesture generation with diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib103.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib103.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Joint Conference on Artificial Intelligence</em><span id="bib.bib103.11.3" class="ltx_text" style="font-size:90%;">, pages 5860–5868, 2023a.
</span>
</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib104.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib104.7.1" class="ltx_text" style="font-size:90%;">
Sicheng Yang, Zhiyong Wu, Minglei Li, Zhensong Zhang, Lei Hao, Weihong Bao, and Haolin Zhuang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib104.8.1" class="ltx_text" style="font-size:90%;">QPGesture: Quantization-based and phase-guided motion matching for natural speech-driven gesture generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib104.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib104.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib104.11.3" class="ltx_text" style="font-size:90%;">, pages 2321–2330, 2023b.
</span>
</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib105.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2023c]</span></span>
<span class="ltx_bibblock"><span id="bib.bib105.7.1" class="ltx_text" style="font-size:90%;">
Sicheng Yang, Haiwei Xue, Zhensong Zhang, Minglei Li, Zhiyong Wu, Xiaofei Wu, Songcen Xu, and Zonghong Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib105.8.1" class="ltx_text" style="font-size:90%;">The diffusestylegesture+ entry to the genea challenge 2023.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib105.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib105.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Multimodal Interaction</em><span id="bib.bib105.11.3" class="ltx_text" style="font-size:90%;">, pages 779–785, 2023c.
</span>
</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib106.5.5.1" class="ltx_text" style="font-size:90%;">Yasuda et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib106.7.1" class="ltx_text" style="font-size:90%;">
Yusuke Yasuda, Xin Wang, and Junichi Yamagishi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib106.8.1" class="ltx_text" style="font-size:90%;">Effect of choice of probability distribution, randomness, and search methods for alignment modeling in sequence-to-sequence text-to-speech synthesis using hard alignment.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib106.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib106.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em><span id="bib.bib106.11.3" class="ltx_text" style="font-size:90%;">, pages 6724–6728. IEEE, 2020.
</span>
</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib107.5.5.1" class="ltx_text" style="font-size:90%;">Yazdian et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib107.7.1" class="ltx_text" style="font-size:90%;">
Payam Jome Yazdian, Mo Chen, and Angelica Lim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib107.8.1" class="ltx_text" style="font-size:90%;">Gesture2Vec: Clustering gestures using representation learning methods for co-speech gesture generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib107.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib107.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems</em><span id="bib.bib107.11.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib108.5.5.1" class="ltx_text" style="font-size:90%;">Yi et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib108.7.1" class="ltx_text" style="font-size:90%;">
Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong Wen, Timo Bolkart, Dacheng Tao, and Michael J Black.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib108.8.1" class="ltx_text" style="font-size:90%;">Generating holistic 3D human motion from speech.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib108.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib108.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib108.11.3" class="ltx_text" style="font-size:90%;">, pages 469–480, 2023.
</span>
</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib109.5.5.1" class="ltx_text" style="font-size:90%;">Yoon et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib109.7.1" class="ltx_text" style="font-size:90%;">
Youngwoo Yoon, Bok Cha, Joo-Haeng Lee, Minsu Jang, Jaeyeon Lee, Jaehong Kim, and Geehyuk Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib109.8.1" class="ltx_text" style="font-size:90%;">Speech gesture generation from the trimodal context of text, audio, and speaker identity.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib109.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM T. Graphic.</em><span id="bib.bib109.10.2" class="ltx_text" style="font-size:90%;">, 39(6):222:1–222:16, 2020.
</span>
</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib110.5.5.1" class="ltx_text" style="font-size:90%;">Yoon et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib110.7.1" class="ltx_text" style="font-size:90%;">
Youngwoo Yoon, Pieter Wolfert, Taras Kucherenko, Carla Viegas, Teodor Nikolov, Mihail Tsakov, and Gustav Eje Henter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib110.8.1" class="ltx_text" style="font-size:90%;">The GENEA Challenge 2022: A large evaluation of data-driven co-speech gesture generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib110.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib110.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Multimodal Interaction</em><span id="bib.bib110.11.3" class="ltx_text" style="font-size:90%;">, pages 736–747, 2022.
</span>
</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib111.5.5.1" class="ltx_text" style="font-size:90%;">Yu et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib111.7.1" class="ltx_text" style="font-size:90%;">
Chengzhu Yu, Heng Lu, Na Hu, Meng Yu, Chao Weng, Kun Xu, Peng Liu, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib111.8.1" class="ltx_text" style="font-size:90%;">DurIAN: Duration informed attention network for multimodal synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib111.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib111.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. Interspeech</em><span id="bib.bib111.11.3" class="ltx_text" style="font-size:90%;">, pages 2027–2031, 2020.
</span>
</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib112.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib112.7.1" class="ltx_text" style="font-size:90%;">
Fan Zhang, Naye Ji, Fuxing Gao, and Yongping Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib112.8.1" class="ltx_text" style="font-size:90%;">DiffMotion: Speech-driven gesture synthesis using denoising diffusion model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib112.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib112.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. MMM</em><span id="bib.bib112.11.3" class="ltx_text" style="font-size:90%;">, pages 231–242, 2023a.
</span>
</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib113.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib113.7.1" class="ltx_text" style="font-size:90%;">
He Zhang, Sebastian Starke, Taku Komura, and Jun Saito.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib113.8.1" class="ltx_text" style="font-size:90%;">Motion synthesis and editing in low-dimensional spaces.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib113.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Graphics Forum</em><span id="bib.bib113.10.2" class="ltx_text" style="font-size:90%;">, 39(8):509–521, 2020.
</span>
</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib114.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib114.7.1" class="ltx_text" style="font-size:90%;">
Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib114.8.1" class="ltx_text" style="font-size:90%;">MotionDiffuse: Text-driven human motion generation with diffusion model.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib114.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</em><span id="bib.bib114.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib115.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib115.7.1" class="ltx_text" style="font-size:90%;">
Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib115.8.1" class="ltx_text" style="font-size:90%;">Speak foreign languages with your own voice: Cross-lingual neural codec language modeling.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib115.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv e-prints</em><span id="bib.bib115.10.2" class="ltx_text" style="font-size:90%;">, pages arXiv–2303, 2023b.
</span>
</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib116.5.5.1" class="ltx_text" style="font-size:90%;">Zhou et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib116.7.1" class="ltx_text" style="font-size:90%;">
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib116.8.1" class="ltx_text" style="font-size:90%;">LIMA: Less is more for alignment.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib116.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.11206</em><span id="bib.bib116.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib117.5.5.1" class="ltx_text" style="font-size:90%;">Zhu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib117.7.1" class="ltx_text" style="font-size:90%;">
Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu, and Lequan Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib117.8.1" class="ltx_text" style="font-size:90%;">Taming diffusion models for audio-driven co-speech gesture generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib117.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib117.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib117.11.3" class="ltx_text" style="font-size:90%;">, pages 10544–10553, 2023.
</span>
</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib118.4.4.1" class="ltx_text" style="font-size:90%;">Łańcucki [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib118.6.1" class="ltx_text" style="font-size:90%;">
Adrian Łańcucki.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib118.7.1" class="ltx_text" style="font-size:90%;">Fastpitch: Parallel text-to-speech with pitch prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib118.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib118.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em><span id="bib.bib118.10.3" class="ltx_text" style="font-size:90%;">, pages 6588–6592, 2021.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p1" class="ltx_para ltx_noindent">
<span id="p1.1" class="ltx_ERROR undefined">\thetitle</span>
<br class="ltx_break ltx_centering">
<p id="p1.2" class="ltx_p ltx_align_center"><span id="p1.2.1" class="ltx_text" style="font-size:144%;">Supplementary Material 
<br class="ltx_break"></span></p>
</div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:144%;">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>GPT-4 prompts</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p"><span id="A1.p1.1.1" class="ltx_text" style="font-size:144%;">After supplying 50 example utterance transcriptions from TSGD2, the generation of synthetic phrases was initialised using the following text prompt:</span></p>
<blockquote id="A1.p1.2" class="ltx_quote">
<p id="A1.p1.2.1" class="ltx_p"><span id="A1.p1.2.1.1" class="ltx_text" style="font-size:144%;">Take these sentences into account and learn their conversational style ignore the content learn the hesitations and disfluencies (using three dots for long pauses …) and uh, um and uhm for conversational disfluencies. Generate more sentences like this in form of spontaneous conversational monologues. Remember disfluencies should be either repeating, …, uh, um or uhm. Make it sound natural as human would converse. Create 50 phrases which mimics how people speak including filled pauses. Make phrases that are highly emotional but realistic.</span></p>
</blockquote>
<p id="A1.p1.3" class="ltx_p"><span id="A1.p1.3.1" class="ltx_text" style="font-size:144%;">After that, the model was prompted to generate further phrases corresponding to a variety scenarios and emotions, to obtain more variety. Here are some examples:</span></p>
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p"><span id="A1.I1.i1.p1.1.1" class="ltx_text" style="font-size:144%;">Continue generation for a happy emotion imagine a different scenario of getting your research paper accepted after a difficult and long review process.</span></p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p"><span id="A1.I1.i2.p1.1.1" class="ltx_text" style="font-size:144%;">Continue generation about confusion emotion talk about getting lost on the way to a new city</span></p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p"><span id="A1.I1.i3.p1.1.1" class="ltx_text" style="font-size:144%;">Continue the generation for happy tone and talk about your favourite movie that you recently watched at the theatre and recommend it to people</span></p>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.1" class="ltx_p"><span id="A1.I1.i4.p1.1.1" class="ltx_text" style="font-size:144%;">Make these sentences a bit smaller just reduce 5–10 words max per sentence. Keep the conversational style and disfluences it is very important to keep those. keep uh, um and pauses, repeats, fixes and other disfluences.
continue happy generation talk about a perfect date at your favourite restaurant</span></p>
</div>
</li>
<li id="A1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i5.p1" class="ltx_para">
<p id="A1.I1.i5.p1.1" class="ltx_p"><span id="A1.I1.i5.p1.1.1" class="ltx_text" style="font-size:144%;">Generate more, but start the sentence in a different way. generate some sentences with negation like saying no to things, denying something etc. focus on new topics, you can forget the old topic. talk about people compelling you to learn to play different sports, musical instruments, dance forms and you denying to it.</span></p>
</div>
</li>
<li id="A1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i6.p1" class="ltx_para">
<p id="A1.I1.i6.p1.1" class="ltx_p"><span id="A1.I1.i6.p1.1.1" class="ltx_text" style="font-size:144%;">Continue generation, talk about how excited you are to get a new dream job</span></p>
</div>
</li>
</ul>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.19621" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.19622" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.19622">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.19622" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.19623" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 22:42:14 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
