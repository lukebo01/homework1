<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.16905] Samsung Research China-Beijing at SemEval-2024 Task 3: A multi-stage framework for Emotion-Cause Pair Extraction in Conversations</title><meta property="og:description" content="In human-computer interaction, it is crucial for agents to respond to human by understanding their emotions. Unraveling the causes of emotions is more challenging. A new task named Multimodal Emotion-Cause Pair Extract…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Samsung Research China-Beijing at SemEval-2024 Task 3: A multi-stage framework for Emotion-Cause Pair Extraction in Conversations">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Samsung Research China-Beijing at SemEval-2024 Task 3: A multi-stage framework for Emotion-Cause Pair Extraction in Conversations">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.16905">

<!--Generated on Sun May  5 20:25:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Samsung Research China-Beijing at SemEval-2024 Task 3: A multi-stage framework for Emotion-Cause Pair Extraction in Conversations</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shen Zhang<sup id="id1.1.1" class="ltx_sup"><math id="id1.1.1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="id1.1.1.m1.1a"><mo id="id1.1.1.m1.1.1" xref="id1.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="id1.1.1.m1.1b"><ci id="id1.1.1.m1.1.1.cmml" xref="id1.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.m1.1c">\star</annotation></semantics></math></sup>, Haojie Zhang<sup id="id2.2.2" class="ltx_sup"><math id="id2.2.2.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="id2.2.2.m1.1a"><mo id="id2.2.2.m1.1.1" xref="id2.2.2.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="id2.2.2.m1.1b"><ci id="id2.2.2.m1.1.1.cmml" xref="id2.2.2.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.2.m1.1c">\star</annotation></semantics></math>🖂</sup>, Jing Zhang, 
<br class="ltx_break"><span id="id3.3.id1" class="ltx_text ltx_font_bold">Xudong Zhang, Yimeng Zhuang, Jinting Wu</span>

<br class="ltx_break">Samsung R&amp;D Institute China-Beijing 
<br class="ltx_break">{shen02.zhang, tayee.chang, jing97.zhang, 
<br class="ltx_break">xudong.z1, ym.zhuang, jinting01.wu}@samsung.com
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">In human-computer interaction, it is crucial for agents to respond to human by understanding their emotions. Unraveling the causes of emotions is more challenging. A new task named Multimodal Emotion-Cause Pair Extraction in Conversations is responsible for recognizing emotion and identifying causal expressions. In this study, we propose a multi-stage framework to generate emotion and extract the emotion causal pairs given the target emotion. In the first stage, Llama-2-based InstructERC is utilized to extract the emotion category of each utterance in a conversation. After emotion recognition, a two-stream attention model is employed to extract the emotion causal pairs given the target emotion for subtask 2 while MuTEC is employed to extract causal span for subtask 1. Our approach achieved first place for both of the two subtasks in the competition.</p>
</div>
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><math id="footnote1.m1.1" class="ltx_Math" alttext="\star" display="inline"><semantics id="footnote1.m1.1b"><mo id="footnote1.m1.1.1" xref="footnote1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="footnote1.m1.1c"><ci id="footnote1.m1.1.1.cmml" xref="footnote1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m1.1d">\star</annotation></semantics></math>: equal contributions. 🖂: Corresponding Author. 
<br class="ltx_break">Shen Zhang is in charge of the basic subtask-emotion recognition in conversation (ERC) and Haojie Zhang is responsible for the pipeline framework and causal pair extraction and causal span extraction subtasks.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Comprehending emotions plays a vital role in developing artificial intelligence with human-like capabilities, as emotions are inherent to humans and exert a substantial impact on our thinking, choices, and social engagements <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib44" title="" class="ltx_ref">2023b</a>)</cite>. Dialogues, being a fundamental mode of human communication, abound with a variety of emotions <cite class="ltx_cite ltx_citemacro_cite">C. et al. (<a href="#bib.bib2" title="" class="ltx_ref">2008</a>); Poria et al. (<a href="#bib.bib33" title="" class="ltx_ref">2019</a>); Zahiri and Choi (<a href="#bib.bib49" title="" class="ltx_ref">2017</a>); Li et al. (<a href="#bib.bib27" title="" class="ltx_ref">2017</a>); Xia and Ding (<a href="#bib.bib48" title="" class="ltx_ref">2019</a>); Ding et al. (<a href="#bib.bib7" title="" class="ltx_ref">2020</a>); Wei et al. (<a href="#bib.bib47" title="" class="ltx_ref">2020</a>); Fan et al. (<a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite>. Going beyond simple emotion identification, unraveling the underlying catalysts of these emotions within conversations represents a more complex and less-explored challenge <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib44" title="" class="ltx_ref">2023b</a>)</cite>.
Hence,  <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib42" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib43" title="" class="ltx_ref">2024</a>)</cite> introduces a novel undertaking known as Recognizing Emotion Cause in Emotion-Cause-in-Friends (ECF). ECF contains 1,344 conversations and 13,509 utterances where 9,272 emotion-cause pairs are annotated, covering textual, visual, and acoustic modalities. All utterances are annotated by one of the seven emotion labels, which are neutral, surprise, fear, sadness, joy, disgust, and anger. Within ECF, a significant task is identified as Emotion-Cause Pair Extraction in Conversations (ECPEC). ECPEC is responsible for identifying causal expressions related to a specific utterance in conversations where the emotion is implicitly expressed. ECPEC provides two Multimodal Emotion Cause Analysis in Conversations (ECAC) subtasks:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Subtask 1: Textual Emotion-Cause Pair Extraction in Conversations. Given a conversation containing the speaker and the text of each utterance <math id="S1.I1.i1.p1.1.m1.3" class="ltx_Math" alttext="U=[U_{1},U_{2},...U_{n}]" display="inline"><semantics id="S1.I1.i1.p1.1.m1.3a"><mrow id="S1.I1.i1.p1.1.m1.3.3" xref="S1.I1.i1.p1.1.m1.3.3.cmml"><mi id="S1.I1.i1.p1.1.m1.3.3.5" xref="S1.I1.i1.p1.1.m1.3.3.5.cmml">U</mi><mo id="S1.I1.i1.p1.1.m1.3.3.4" xref="S1.I1.i1.p1.1.m1.3.3.4.cmml">=</mo><mrow id="S1.I1.i1.p1.1.m1.3.3.3.3" xref="S1.I1.i1.p1.1.m1.3.3.3.4.cmml"><mo stretchy="false" id="S1.I1.i1.p1.1.m1.3.3.3.3.4" xref="S1.I1.i1.p1.1.m1.3.3.3.4.cmml">[</mo><msub id="S1.I1.i1.p1.1.m1.1.1.1.1.1" xref="S1.I1.i1.p1.1.m1.1.1.1.1.1.cmml"><mi id="S1.I1.i1.p1.1.m1.1.1.1.1.1.2" xref="S1.I1.i1.p1.1.m1.1.1.1.1.1.2.cmml">U</mi><mn id="S1.I1.i1.p1.1.m1.1.1.1.1.1.3" xref="S1.I1.i1.p1.1.m1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S1.I1.i1.p1.1.m1.3.3.3.3.5" xref="S1.I1.i1.p1.1.m1.3.3.3.4.cmml">,</mo><msub id="S1.I1.i1.p1.1.m1.2.2.2.2.2" xref="S1.I1.i1.p1.1.m1.2.2.2.2.2.cmml"><mi id="S1.I1.i1.p1.1.m1.2.2.2.2.2.2" xref="S1.I1.i1.p1.1.m1.2.2.2.2.2.2.cmml">U</mi><mn id="S1.I1.i1.p1.1.m1.2.2.2.2.2.3" xref="S1.I1.i1.p1.1.m1.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S1.I1.i1.p1.1.m1.3.3.3.3.6" xref="S1.I1.i1.p1.1.m1.3.3.3.4.cmml">,</mo><mrow id="S1.I1.i1.p1.1.m1.3.3.3.3.3" xref="S1.I1.i1.p1.1.m1.3.3.3.3.3.cmml"><mi mathvariant="normal" id="S1.I1.i1.p1.1.m1.3.3.3.3.3.2" xref="S1.I1.i1.p1.1.m1.3.3.3.3.3.2.cmml">…</mi><mo lspace="0em" rspace="0em" id="S1.I1.i1.p1.1.m1.3.3.3.3.3.1" xref="S1.I1.i1.p1.1.m1.3.3.3.3.3.1.cmml">​</mo><msub id="S1.I1.i1.p1.1.m1.3.3.3.3.3.3" xref="S1.I1.i1.p1.1.m1.3.3.3.3.3.3.cmml"><mi id="S1.I1.i1.p1.1.m1.3.3.3.3.3.3.2" xref="S1.I1.i1.p1.1.m1.3.3.3.3.3.3.2.cmml">U</mi><mi id="S1.I1.i1.p1.1.m1.3.3.3.3.3.3.3" xref="S1.I1.i1.p1.1.m1.3.3.3.3.3.3.3.cmml">n</mi></msub></mrow><mo stretchy="false" id="S1.I1.i1.p1.1.m1.3.3.3.3.7" xref="S1.I1.i1.p1.1.m1.3.3.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i1.p1.1.m1.3b"><apply id="S1.I1.i1.p1.1.m1.3.3.cmml" xref="S1.I1.i1.p1.1.m1.3.3"><eq id="S1.I1.i1.p1.1.m1.3.3.4.cmml" xref="S1.I1.i1.p1.1.m1.3.3.4"></eq><ci id="S1.I1.i1.p1.1.m1.3.3.5.cmml" xref="S1.I1.i1.p1.1.m1.3.3.5">𝑈</ci><list id="S1.I1.i1.p1.1.m1.3.3.3.4.cmml" xref="S1.I1.i1.p1.1.m1.3.3.3.3"><apply id="S1.I1.i1.p1.1.m1.1.1.1.1.1.cmml" xref="S1.I1.i1.p1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S1.I1.i1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S1.I1.i1.p1.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S1.I1.i1.p1.1.m1.1.1.1.1.1.2.cmml" xref="S1.I1.i1.p1.1.m1.1.1.1.1.1.2">𝑈</ci><cn type="integer" id="S1.I1.i1.p1.1.m1.1.1.1.1.1.3.cmml" xref="S1.I1.i1.p1.1.m1.1.1.1.1.1.3">1</cn></apply><apply id="S1.I1.i1.p1.1.m1.2.2.2.2.2.cmml" xref="S1.I1.i1.p1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S1.I1.i1.p1.1.m1.2.2.2.2.2.1.cmml" xref="S1.I1.i1.p1.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S1.I1.i1.p1.1.m1.2.2.2.2.2.2.cmml" xref="S1.I1.i1.p1.1.m1.2.2.2.2.2.2">𝑈</ci><cn type="integer" id="S1.I1.i1.p1.1.m1.2.2.2.2.2.3.cmml" xref="S1.I1.i1.p1.1.m1.2.2.2.2.2.3">2</cn></apply><apply id="S1.I1.i1.p1.1.m1.3.3.3.3.3.cmml" xref="S1.I1.i1.p1.1.m1.3.3.3.3.3"><times id="S1.I1.i1.p1.1.m1.3.3.3.3.3.1.cmml" xref="S1.I1.i1.p1.1.m1.3.3.3.3.3.1"></times><ci id="S1.I1.i1.p1.1.m1.3.3.3.3.3.2.cmml" xref="S1.I1.i1.p1.1.m1.3.3.3.3.3.2">…</ci><apply id="S1.I1.i1.p1.1.m1.3.3.3.3.3.3.cmml" xref="S1.I1.i1.p1.1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S1.I1.i1.p1.1.m1.3.3.3.3.3.3.1.cmml" xref="S1.I1.i1.p1.1.m1.3.3.3.3.3.3">subscript</csymbol><ci id="S1.I1.i1.p1.1.m1.3.3.3.3.3.3.2.cmml" xref="S1.I1.i1.p1.1.m1.3.3.3.3.3.3.2">𝑈</ci><ci id="S1.I1.i1.p1.1.m1.3.3.3.3.3.3.3.cmml" xref="S1.I1.i1.p1.1.m1.3.3.3.3.3.3.3">𝑛</ci></apply></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i1.p1.1.m1.3c">U=[U_{1},U_{2},...U_{n}]</annotation></semantics></math>, the model is aim to predict emotion-cause pairs, which include emotion utterance’s emotion category and the textual cause span in a specific cause utterance (e.g. U3_joy, U2_"You made up!").</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Subtask 2: Multimodal Emotion Cause Analysis in Conversations. Given a conversation including the speaker, text and audio-visual clip for each utterance, the model is aim to predict emotion-cause pairs, which include emotion category and a cause utterance (e.g. U5_Disgust, U5).</p>
</div>
</li>
</ul>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To address the above problem,  <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a href="#bib.bib42" title="" class="ltx_ref">2023a</a>)</cite> proposed a two-step approach. First, they extract the emotional utterances and causal utterances by a multi-task learning framework and then pair and filter them.  <cite class="ltx_cite ltx_citemacro_citet">Zhao et al. (<a href="#bib.bib55" title="" class="ltx_ref">2023</a>)</cite> proposes an end-to-end method by leveraging multi-task learning in a pipeline manner. However, these methods still suffer from low evaluation performances.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Motivated by the phenomenon that the performance of the emotion recognition of utterances in a conversation harnessed by the traditional manner is generally poor, we design a new pipeline framework. Firstly we utilize the Llama-2-based InstructERC <cite class="ltx_cite ltx_citemacro_cite">Lei et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023a</a>)</cite> to extract the emotion category of each utterance in a conversation. Then we consider the emotion causal pair extraction as the causal emotion entailment subtask and employ a two-stream attention model to extract the emotion causal pairs given the target emotion. For the causal span extraction, we employ MuTEC <cite class="ltx_cite ltx_citemacro_cite">Bhat and Modi (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite> which is an end-to-end multi-task learning framework.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2404.16905/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="96" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The overview of proposed model framework.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Emotion Recognition in Conversation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Emotion recognition in conversation (ERC), which is a task to predict emotions of utterances during conversations, is crucial in both of the two ECAC subtasks. The existing methods can be divided into graph-based, RNN-based, Transformer-based, LLM-based, and knowledge-injecting methods.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Graph-based methods <cite class="ltx_cite ltx_citemacro_cite">Shen et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021b</a>); Li et al. (<a href="#bib.bib23" title="" class="ltx_ref">2024</a>); Zhang et al. (<a href="#bib.bib50" title="" class="ltx_ref">2019</a>); Taichi et al. (<a href="#bib.bib41" title="" class="ltx_ref">2020</a>); Ghosal et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite> aims to represent the correlations between emotions of utterances and speakers in the conversations. RNN-based methods <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a href="#bib.bib16" title="" class="ltx_ref">2023</a>); Lei et al. (<a href="#bib.bib22" title="" class="ltx_ref">2023c</a>); Majumder et al. (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>); Hazarika et al. (<a href="#bib.bib15" title="" class="ltx_ref">2018</a>); Poria et al. (<a href="#bib.bib32" title="" class="ltx_ref">2017</a>)</cite> using GRU and LSTM <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib45" title="" class="ltx_ref">2020</a>)</cite> to capture the dependency of interlocutors and emotions of utterances. To model the emotional states during long-range context, Transformer-based methods <cite class="ltx_cite ltx_citemacro_cite">Song et al. (<a href="#bib.bib40" title="" class="ltx_ref">2022</a>); Liu et al. (<a href="#bib.bib29" title="" class="ltx_ref">2023b</a>); Chudasama et al. (<a href="#bib.bib5" title="" class="ltx_ref">2022</a>); Shen et al. (<a href="#bib.bib37" title="" class="ltx_ref">2021a</a>); Hu et al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite> utilize encoder-decoder framework or encoder-only models, such as BERT <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib25" title="" class="ltx_ref">2020</a>)</cite> and RoBERTa <cite class="ltx_cite ltx_citemacro_cite">Kim and Vossen (<a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite>, to establish the correlation between long-range emotional states during conversations. Considering more than seven utterances in single conversation input, InstructERC <cite class="ltx_cite ltx_citemacro_cite">Lei et al. (<a href="#bib.bib21" title="" class="ltx_ref">2023b</a>)</cite> defines the ERC task as a generative task based on LLMs, which unifies emotion labels between three common ERC datasets and utilizes auxiliary tasks (speaker identification and emotion prediction) by using instruction template to capture speaker relationships and emotional states in future utterances. Knowledge-injecting methods <cite class="ltx_cite ltx_citemacro_cite">Freudenthaler et al. (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>); Ghosal et al. (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>); Zhong et al. (<a href="#bib.bib56" title="" class="ltx_ref">2019</a>); Zhu et al. (<a href="#bib.bib57" title="" class="ltx_ref">2021</a>); Lei et al. (<a href="#bib.bib21" title="" class="ltx_ref">2023b</a>)</cite> use external knowledge to analyze conversation scenarios.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Emotion Causes in Conversations</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Poria et al. (<a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite> introduces the task of recognizing emotion causes in conversations and introduce two novel sub-tasks: Causal Span Extraction (CSE) and Causal Emotion Entailment (CEE), designed to identify the emotion cause at the span-level and utterance-level, respectively.</p>
</div>
<section id="S2.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Causal Emotion Entailment</h5>

<div id="S2.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Poria et al. (<a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite> define CEE as a classification task for utterance pairs and establish robust Transformer-based baselines for it.
<cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a href="#bib.bib42" title="" class="ltx_ref">2023a</a>)</cite> introduces a multi-modality conversation dataset Emotion-Cause-in-Friends (ECF) and propose a two-step approach to extract the causal pairs. They first extract the emotion utterances and the potential causal utterances individually and then pair and filter them. <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite> introduce the social commonsense knowledge to propagate causal clues between utterances.  <cite class="ltx_cite ltx_citemacro_citet">Zhao et al. (<a href="#bib.bib55" title="" class="ltx_ref">2023</a>)</cite> propose the Knowledge-Bridged Causal Interaction Network (KBCIN), which integrates commonsense knowledge (CSK) as three bridges called semantics-level bridge, emotion-level bridge and action-level bridge.</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Causal Span Extraction</h5>

<div id="S2.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px2.p1.1" class="ltx_p">involves identifying the causal span (emotion cause) for a given non-neutral utterance.  <cite class="ltx_cite ltx_citemacro_citet">Poria et al. (<a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite> first introduces the subtask and employs the pre-trained Transformer-based model to formulate the Causal Span Extraction as the Machine Reading Comprehension (MRC).  <cite class="ltx_cite ltx_citemacro_citet">Bhat and Modi (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite> propose a multi-task learning framework to extract the causal pairs and causal span in an utterance in a joint end-to-end manner. Besides, they also propose a two-step approach consisting of Emotion Prediction (EP), followed by Causal Span (CSE).</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>System Overview</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>System Architecture</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The overview of the architecture of our proposed model is shown in Figure  <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Samsung Research China-Beijing at SemEval-2024 Task 3: A multi-stage framework for Emotion-Cause Pair Extraction in Conversations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The InstructERC aims to extract the emotion of utterances. TSAM model is a two-stream attention model utilized to extract the causal pairs given the predicted emotion utterance. The MuTEC is an end-to-end network designed to extract the causal span based on the causal pair extraction.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Emotion Recognition in Conversations</h3>

<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>InstructERC for Emotion Recognition</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">InstructERC <cite class="ltx_cite ltx_citemacro_cite">Lei et al. (<a href="#bib.bib21" title="" class="ltx_ref">2023b</a>)</cite> reformulate the ERC task from a discriminative framework to a generative framework and design a prompt template which comprises job description, historical utterance window, label set and emotional domain retrieval module. Besides emotion recognition task, InstructERC also utilizes speaker identification and emotion prediction tasks for ERC task. The performance of emotional domain retrieval module, which is based on Sentence BERT <cite class="ltx_cite ltx_citemacro_cite">Reimers and Gurevych (<a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>, rely on the abundance of corpus. Taking into account that no additional data can be used, we only retain job description, historical utterance window and label statement in the instruct template.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Hierarchical Emotion Label</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">The hierarchical classification structure is shown in Figure  <a href="#S3.F2" title="Figure 2 ‣ 3.2.2 Hierarchical Emotion Label ‣ 3.2 Emotion Recognition in Conversations ‣ 3 System Overview ‣ Samsung Research China-Beijing at SemEval-2024 Task 3: A multi-stage framework for Emotion-Cause Pair Extraction in Conversations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The emotion labels in dataset can be split into three categories: neutral, positive and negative, which positive set consists of surprise and joy while negative set includes fear, sadness, disgust and anger.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2404.16905/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="105" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The Hierarchical Structure of Emotion labels.</figcaption>
</figure>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Auxiliary Tasks and Instruct Design</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">Auxiliary tasks are proven as one of the efficient data augment methods <cite class="ltx_cite ltx_citemacro_cite">Lei et al. (<a href="#bib.bib21" title="" class="ltx_ref">2023b</a>)</cite>. Besides emotion recognition and speaker identification tasks, we add three auxiliary tasks in training data: sub-label recognition, positive recognition, and negative recognition tasks. The instruct template is depicted in Figure  <a href="#S3.F3" title="Figure 3 ‣ 3.2.3 Auxiliary Tasks and Instruct Design ‣ 3.2 Emotion Recognition in Conversations ‣ 3 System Overview ‣ Samsung Research China-Beijing at SemEval-2024 Task 3: A multi-stage framework for Emotion-Cause Pair Extraction in Conversations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2404.16905/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The Schematic of Instruct Template for ERC.</figcaption>
</figure>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">For emotion recognition and speaker identification task, we follow the format of instruct template in InstructERC, which consists of job description, historical content and label statement. For sub-label recognition (SR), positive recognition (PR) and negative recognition (NR) tasks, we utilize the corresponding label set which is mentioned in Section <a href="#S3.SS2.SSS2" title="3.2.2 Hierarchical Emotion Label ‣ 3.2 Emotion Recognition in Conversations ‣ 3 System Overview ‣ Samsung Research China-Beijing at SemEval-2024 Task 3: A multi-stage framework for Emotion-Cause Pair Extraction in Conversations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a> to replace the label statement separately. The number of Speakers in the dataset is 304. The number of utterances from other speakers except the protagonist is far lower than the number of protagonists. Therefore, we unified all speakers other than the protagonist into ’Others’.</p>
</div>
<div id="S3.SS2.SSS3.p3" class="ltx_para">
<p id="S3.SS2.SSS3.p3.1" class="ltx_p">Visual data also plays an essential role in ERC. For video clips, we utilize LLaVA to generate descriptions of background, speaker movement and personal state. Therefore, we add background description, movement description and personal state description in instruct template. The background exhibits the information of scene in the conversation. The movement description depicts the action of speakers during corresponding utterances. The personal state description provides the observation of speakers’ facial expressions. Considering the influence of the context, we have generated two sets of descriptions. The input of the first group only includes the clips corresponding to the utterances, while the second group adds the clips sequence corresponding to the historical utterances to the input of second group.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Emotion Cause Span Extraction</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Emotion cause span extraction aims to extract the start position and end position of the causal utterance in a conversation. Typically, we can utilize a pipeline framework which firstly predicts the emotion and then predicts the cause span. For the cause span predictor, we can use SpanBERT <cite class="ltx_cite ltx_citemacro_cite">Joshi et al. (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>, RoBERTa <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib30" title="" class="ltx_ref">2019</a>)</cite> as the feature extractor and employ two heads on the top of them to extract the start and end positions given the causal utterance. The two-step model offers an advantage in its modularity, allowing the application of distinct architectures for the emotion predictor and cause span predictor. However, it comes with two drawbacks: 1) Errors in the first step can propagate to the next, and 2) This approach assumes that emotion prediction and cause-span prediction are mutually exclusive tasks. In our system, we follow MuTEC <cite class="ltx_cite ltx_citemacro_citet">Bhat and Modi (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite> and use an end-to-end framework in a joint multi-task learning manner to extract the causal span in a conversation.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.3" class="ltx_p">During the training period, the input comprises the target utterance <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="U_{t}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">U</mi><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝑈</ci><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">U_{t}</annotation></semantics></math>, the candidate causes utterance <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="U_{i}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><msub id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">U</mi><mi id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">𝑈</ci><ci id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">U_{i}</annotation></semantics></math>, and the historical context. MuTEC employs a pre-trained model (PLM) to extract the context representations. For emotion recognition, which is an auxiliary task, it employs a classification head on the top of the PLM. The end position is predicted by the prediction head of the concatenated representations of the given start index and the sequence output from the PLM. In this stage, the golden start index is used as the start index. The training loss is a linear combination of the loss for cause-span prediction and emotion prediction: <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{L}_{Loss}=\mathcal{L}_{CSE}+\beta\mathcal{L}_{Emotion}" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mrow id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><msub id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.3.m3.1.1.2.2" xref="S3.SS3.p2.3.m3.1.1.2.2.cmml">ℒ</mi><mrow id="S3.SS3.p2.3.m3.1.1.2.3" xref="S3.SS3.p2.3.m3.1.1.2.3.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2.3.2" xref="S3.SS3.p2.3.m3.1.1.2.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1.2.3.1" xref="S3.SS3.p2.3.m3.1.1.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.3.m3.1.1.2.3.3" xref="S3.SS3.p2.3.m3.1.1.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1.2.3.1a" xref="S3.SS3.p2.3.m3.1.1.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.3.m3.1.1.2.3.4" xref="S3.SS3.p2.3.m3.1.1.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1.2.3.1b" xref="S3.SS3.p2.3.m3.1.1.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.3.m3.1.1.2.3.5" xref="S3.SS3.p2.3.m3.1.1.2.3.5.cmml">s</mi></mrow></msub><mo id="S3.SS3.p2.3.m3.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.cmml">=</mo><mrow id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml"><msub id="S3.SS3.p2.3.m3.1.1.3.2" xref="S3.SS3.p2.3.m3.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.3.m3.1.1.3.2.2" xref="S3.SS3.p2.3.m3.1.1.3.2.2.cmml">ℒ</mi><mrow id="S3.SS3.p2.3.m3.1.1.3.2.3" xref="S3.SS3.p2.3.m3.1.1.3.2.3.cmml"><mi id="S3.SS3.p2.3.m3.1.1.3.2.3.2" xref="S3.SS3.p2.3.m3.1.1.3.2.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1.3.2.3.1" xref="S3.SS3.p2.3.m3.1.1.3.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.3.m3.1.1.3.2.3.3" xref="S3.SS3.p2.3.m3.1.1.3.2.3.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1.3.2.3.1a" xref="S3.SS3.p2.3.m3.1.1.3.2.3.1.cmml">​</mo><mi id="S3.SS3.p2.3.m3.1.1.3.2.3.4" xref="S3.SS3.p2.3.m3.1.1.3.2.3.4.cmml">E</mi></mrow></msub><mo id="S3.SS3.p2.3.m3.1.1.3.1" xref="S3.SS3.p2.3.m3.1.1.3.1.cmml">+</mo><mrow id="S3.SS3.p2.3.m3.1.1.3.3" xref="S3.SS3.p2.3.m3.1.1.3.3.cmml"><mi id="S3.SS3.p2.3.m3.1.1.3.3.2" xref="S3.SS3.p2.3.m3.1.1.3.3.2.cmml">β</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1.3.3.1" xref="S3.SS3.p2.3.m3.1.1.3.3.1.cmml">​</mo><msub id="S3.SS3.p2.3.m3.1.1.3.3.3" xref="S3.SS3.p2.3.m3.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.3.m3.1.1.3.3.3.2" xref="S3.SS3.p2.3.m3.1.1.3.3.3.2.cmml">ℒ</mi><mrow id="S3.SS3.p2.3.m3.1.1.3.3.3.3" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.cmml"><mi id="S3.SS3.p2.3.m3.1.1.3.3.3.3.2" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1.3.3.3.3.1" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS3.p2.3.m3.1.1.3.3.3.3.3" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1.3.3.3.3.1a" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS3.p2.3.m3.1.1.3.3.3.3.4" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1.3.3.3.3.1b" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS3.p2.3.m3.1.1.3.3.3.3.5" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1.3.3.3.3.1c" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS3.p2.3.m3.1.1.3.3.3.3.6" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1.3.3.3.3.1d" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS3.p2.3.m3.1.1.3.3.3.3.7" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.7.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1.3.3.3.3.1e" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS3.p2.3.m3.1.1.3.3.3.3.8" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.8.cmml">n</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><eq id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1"></eq><apply id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.2.1.cmml" xref="S3.SS3.p2.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.2.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2.2">ℒ</ci><apply id="S3.SS3.p2.3.m3.1.1.2.3.cmml" xref="S3.SS3.p2.3.m3.1.1.2.3"><times id="S3.SS3.p2.3.m3.1.1.2.3.1.cmml" xref="S3.SS3.p2.3.m3.1.1.2.3.1"></times><ci id="S3.SS3.p2.3.m3.1.1.2.3.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2.3.2">𝐿</ci><ci id="S3.SS3.p2.3.m3.1.1.2.3.3.cmml" xref="S3.SS3.p2.3.m3.1.1.2.3.3">𝑜</ci><ci id="S3.SS3.p2.3.m3.1.1.2.3.4.cmml" xref="S3.SS3.p2.3.m3.1.1.2.3.4">𝑠</ci><ci id="S3.SS3.p2.3.m3.1.1.2.3.5.cmml" xref="S3.SS3.p2.3.m3.1.1.2.3.5">𝑠</ci></apply></apply><apply id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3"><plus id="S3.SS3.p2.3.m3.1.1.3.1.cmml" xref="S3.SS3.p2.3.m3.1.1.3.1"></plus><apply id="S3.SS3.p2.3.m3.1.1.3.2.cmml" xref="S3.SS3.p2.3.m3.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.3.2.1.cmml" xref="S3.SS3.p2.3.m3.1.1.3.2">subscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.3.2.2.cmml" xref="S3.SS3.p2.3.m3.1.1.3.2.2">ℒ</ci><apply id="S3.SS3.p2.3.m3.1.1.3.2.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3.2.3"><times id="S3.SS3.p2.3.m3.1.1.3.2.3.1.cmml" xref="S3.SS3.p2.3.m3.1.1.3.2.3.1"></times><ci id="S3.SS3.p2.3.m3.1.1.3.2.3.2.cmml" xref="S3.SS3.p2.3.m3.1.1.3.2.3.2">𝐶</ci><ci id="S3.SS3.p2.3.m3.1.1.3.2.3.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3.2.3.3">𝑆</ci><ci id="S3.SS3.p2.3.m3.1.1.3.2.3.4.cmml" xref="S3.SS3.p2.3.m3.1.1.3.2.3.4">𝐸</ci></apply></apply><apply id="S3.SS3.p2.3.m3.1.1.3.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3"><times id="S3.SS3.p2.3.m3.1.1.3.3.1.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.1"></times><ci id="S3.SS3.p2.3.m3.1.1.3.3.2.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.2">𝛽</ci><apply id="S3.SS3.p2.3.m3.1.1.3.3.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.3.3.3.1.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.3">subscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.3.3.3.2.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.3.2">ℒ</ci><apply id="S3.SS3.p2.3.m3.1.1.3.3.3.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3"><times id="S3.SS3.p2.3.m3.1.1.3.3.3.3.1.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.1"></times><ci id="S3.SS3.p2.3.m3.1.1.3.3.3.3.2.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.2">𝐸</ci><ci id="S3.SS3.p2.3.m3.1.1.3.3.3.3.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.3">𝑚</ci><ci id="S3.SS3.p2.3.m3.1.1.3.3.3.3.4.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.4">𝑜</ci><ci id="S3.SS3.p2.3.m3.1.1.3.3.3.3.5.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.5">𝑡</ci><ci id="S3.SS3.p2.3.m3.1.1.3.3.3.3.6.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.6">𝑖</ci><ci id="S3.SS3.p2.3.m3.1.1.3.3.3.3.7.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.7">𝑜</ci><ci id="S3.SS3.p2.3.m3.1.1.3.3.3.3.8.cmml" xref="S3.SS3.p2.3.m3.1.1.3.3.3.3.8">𝑛</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">\mathcal{L}_{Loss}=\mathcal{L}_{CSE}+\beta\mathcal{L}_{Emotion}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.3" class="ltx_p">During the inference period, as the start index is unknown, it uses top <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">k</annotation></semantics></math> start indices as the candidate start indices and gets <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">k</annotation></semantics></math> candidate end indices. Finally, it gets the final start-end indices by argmaxing the <math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="k\times k" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><mrow id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml"><mi id="S3.SS3.p3.3.m3.1.1.2" xref="S3.SS3.p3.3.m3.1.1.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p3.3.m3.1.1.1" xref="S3.SS3.p3.3.m3.1.1.1.cmml">×</mo><mi id="S3.SS3.p3.3.m3.1.1.3" xref="S3.SS3.p3.3.m3.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1"><times id="S3.SS3.p3.3.m3.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1.1"></times><ci id="S3.SS3.p3.3.m3.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2">𝑘</ci><ci id="S3.SS3.p3.3.m3.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">k\times k</annotation></semantics></math> start-end pairs.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2404.16905/assets/image/face_framework.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="188" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The framework of the face module.</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Emotion-Cause Pair Extraction</h3>

<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>TSAM Model</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">In our pipeline framework, for Subtask2, we first extract the emotion of the utterance and then extract the causal pairs given the emotional utterance in a conversation. The causal pairs extraction is typically modelled as the causal emotion entailment (CEE) task. In our system, we employ TSAM model from  <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a href="#bib.bib51" title="" class="ltx_ref">2022</a>)</cite> as the causal pair extractor. TSAM mainly comprises three modules: Speaker Attention Network (SAN), Emotion Attention Network (EAN), and Interaction Network (IN). The EAN and SAN integrate emotion and speaker information simultaneously, and the subsequent interaction module efficiently exchanges pertinent information between the EAN and SAN through a mutual BiAffine transformation <cite class="ltx_cite ltx_citemacro_cite">Dozat and Manning (<a href="#bib.bib8" title="" class="ltx_ref">2016</a>)</cite>.</p>
</div>
<section id="S3.SS4.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Contextual Utterance Representation</h5>

<div id="S3.SS4.SSS1.Px1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.Px1.p1.3" class="ltx_p">The pre-trained RoBERTa is employed as the utterance encoder, and we obtain contextual utterance representations by inputting the entire conversational history <math id="S3.SS4.SSS1.Px1.p1.1.m1.1" class="ltx_Math" alttext="{U}_{t}" display="inline"><semantics id="S3.SS4.SSS1.Px1.p1.1.m1.1a"><msub id="S3.SS4.SSS1.Px1.p1.1.m1.1.1" xref="S3.SS4.SSS1.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS4.SSS1.Px1.p1.1.m1.1.1.2" xref="S3.SS4.SSS1.Px1.p1.1.m1.1.1.2.cmml">U</mi><mi id="S3.SS4.SSS1.Px1.p1.1.m1.1.1.3" xref="S3.SS4.SSS1.Px1.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.Px1.p1.1.m1.1b"><apply id="S3.SS4.SSS1.Px1.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS1.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS4.SSS1.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS4.SSS1.Px1.p1.1.m1.1.1.2">𝑈</ci><ci id="S3.SS4.SSS1.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS4.SSS1.Px1.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.Px1.p1.1.m1.1c">{U}_{t}</annotation></semantics></math>, into the RoBERTa <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib30" title="" class="ltx_ref">2019</a>)</cite>,
separated by a special token [CLS], where <math id="S3.SS4.SSS1.Px1.p1.2.m2.5" class="ltx_Math" alttext="i=0,1,2,...,t" display="inline"><semantics id="S3.SS4.SSS1.Px1.p1.2.m2.5a"><mrow id="S3.SS4.SSS1.Px1.p1.2.m2.5.6" xref="S3.SS4.SSS1.Px1.p1.2.m2.5.6.cmml"><mi id="S3.SS4.SSS1.Px1.p1.2.m2.5.6.2" xref="S3.SS4.SSS1.Px1.p1.2.m2.5.6.2.cmml">i</mi><mo id="S3.SS4.SSS1.Px1.p1.2.m2.5.6.1" xref="S3.SS4.SSS1.Px1.p1.2.m2.5.6.1.cmml">=</mo><mrow id="S3.SS4.SSS1.Px1.p1.2.m2.5.6.3.2" xref="S3.SS4.SSS1.Px1.p1.2.m2.5.6.3.1.cmml"><mn id="S3.SS4.SSS1.Px1.p1.2.m2.1.1" xref="S3.SS4.SSS1.Px1.p1.2.m2.1.1.cmml">0</mn><mo id="S3.SS4.SSS1.Px1.p1.2.m2.5.6.3.2.1" xref="S3.SS4.SSS1.Px1.p1.2.m2.5.6.3.1.cmml">,</mo><mn id="S3.SS4.SSS1.Px1.p1.2.m2.2.2" xref="S3.SS4.SSS1.Px1.p1.2.m2.2.2.cmml">1</mn><mo id="S3.SS4.SSS1.Px1.p1.2.m2.5.6.3.2.2" xref="S3.SS4.SSS1.Px1.p1.2.m2.5.6.3.1.cmml">,</mo><mn id="S3.SS4.SSS1.Px1.p1.2.m2.3.3" xref="S3.SS4.SSS1.Px1.p1.2.m2.3.3.cmml">2</mn><mo id="S3.SS4.SSS1.Px1.p1.2.m2.5.6.3.2.3" xref="S3.SS4.SSS1.Px1.p1.2.m2.5.6.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS4.SSS1.Px1.p1.2.m2.4.4" xref="S3.SS4.SSS1.Px1.p1.2.m2.4.4.cmml">…</mi><mo id="S3.SS4.SSS1.Px1.p1.2.m2.5.6.3.2.4" xref="S3.SS4.SSS1.Px1.p1.2.m2.5.6.3.1.cmml">,</mo><mi id="S3.SS4.SSS1.Px1.p1.2.m2.5.5" xref="S3.SS4.SSS1.Px1.p1.2.m2.5.5.cmml">t</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.Px1.p1.2.m2.5b"><apply id="S3.SS4.SSS1.Px1.p1.2.m2.5.6.cmml" xref="S3.SS4.SSS1.Px1.p1.2.m2.5.6"><eq id="S3.SS4.SSS1.Px1.p1.2.m2.5.6.1.cmml" xref="S3.SS4.SSS1.Px1.p1.2.m2.5.6.1"></eq><ci id="S3.SS4.SSS1.Px1.p1.2.m2.5.6.2.cmml" xref="S3.SS4.SSS1.Px1.p1.2.m2.5.6.2">𝑖</ci><list id="S3.SS4.SSS1.Px1.p1.2.m2.5.6.3.1.cmml" xref="S3.SS4.SSS1.Px1.p1.2.m2.5.6.3.2"><cn type="integer" id="S3.SS4.SSS1.Px1.p1.2.m2.1.1.cmml" xref="S3.SS4.SSS1.Px1.p1.2.m2.1.1">0</cn><cn type="integer" id="S3.SS4.SSS1.Px1.p1.2.m2.2.2.cmml" xref="S3.SS4.SSS1.Px1.p1.2.m2.2.2">1</cn><cn type="integer" id="S3.SS4.SSS1.Px1.p1.2.m2.3.3.cmml" xref="S3.SS4.SSS1.Px1.p1.2.m2.3.3">2</cn><ci id="S3.SS4.SSS1.Px1.p1.2.m2.4.4.cmml" xref="S3.SS4.SSS1.Px1.p1.2.m2.4.4">…</ci><ci id="S3.SS4.SSS1.Px1.p1.2.m2.5.5.cmml" xref="S3.SS4.SSS1.Px1.p1.2.m2.5.5">𝑡</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.Px1.p1.2.m2.5c">i=0,1,2,...,t</annotation></semantics></math>. We use the representation of [CLS] as the contextual representation of the utterance, which can be denoted as <math id="S3.SS4.SSS1.Px1.p1.3.m3.1" class="ltx_Math" alttext="h_{u}^{i}\in H_{u}" display="inline"><semantics id="S3.SS4.SSS1.Px1.p1.3.m3.1a"><mrow id="S3.SS4.SSS1.Px1.p1.3.m3.1.1" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.cmml"><msubsup id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2.cmml"><mi id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2.2.2" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2.2.2.cmml">h</mi><mi id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2.2.3" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2.2.3.cmml">u</mi><mi id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2.3" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2.3.cmml">i</mi></msubsup><mo id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.1" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.1.cmml">∈</mo><msub id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.3" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.3.2" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.3.2.cmml">H</mi><mi id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.3.3" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.3.3.cmml">u</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.Px1.p1.3.m3.1b"><apply id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.cmml" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1"><in id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.1"></in><apply id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2">superscript</csymbol><apply id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2.2.cmml" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2.2.1.cmml" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2.2.2.cmml" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2.2.2">ℎ</ci><ci id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2.2.3.cmml" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2.2.3">𝑢</ci></apply><ci id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2.3.cmml" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.2.3">𝑖</ci></apply><apply id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.3.cmml" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.3.2">𝐻</ci><ci id="S3.SS4.SSS1.Px1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS4.SSS1.Px1.p1.3.m3.1.1.3.3">𝑢</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.Px1.p1.3.m3.1c">h_{u}^{i}\in H_{u}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS4.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Emotion Attention Network</h5>

<div id="S3.SS4.SSS1.Px2.p1" class="ltx_para">
<p id="S3.SS4.SSS1.Px2.p1.3" class="ltx_p">To represent emotions, the EAN utilizes an emotion embedding network as the extractor of emotion representations, <math id="S3.SS4.SSS1.Px2.p1.1.m1.1" class="ltx_Math" alttext="X_{e}^{k}=Embedding(e_{k})" display="inline"><semantics id="S3.SS4.SSS1.Px2.p1.1.m1.1a"><mrow id="S3.SS4.SSS1.Px2.p1.1.m1.1.1" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.cmml"><msubsup id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3.2.2" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3.2.2.cmml">X</mi><mi id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3.2.3" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3.2.3.cmml">e</mi><mi id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3.3" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3.3.cmml">k</mi></msubsup><mo id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.2" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.2.cmml">=</mo><mrow id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.cmml"><mi id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.3" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2.cmml">​</mo><mi id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.4" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2a" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2.cmml">​</mo><mi id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.5" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.5.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2b" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2.cmml">​</mo><mi id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.6" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2c" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2.cmml">​</mo><mi id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.7" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.7.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2d" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2.cmml">​</mo><mi id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.8" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.8.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2e" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2.cmml">​</mo><mi id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.9" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.9.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2f" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2.cmml">​</mo><mi id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.10" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.10.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2g" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2.cmml">​</mo><mi id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.11" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.11.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2h" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2.cmml">​</mo><mrow id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1.2" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1.1" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1.1.2" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1.1.2.cmml">e</mi><mi id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1.1.3" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1.1.3.cmml">k</mi></msub><mo stretchy="false" id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1.3" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.Px2.p1.1.m1.1b"><apply id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1"><eq id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.2"></eq><apply id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3">superscript</csymbol><apply id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3.2.1.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3.2.2.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3.2.2">𝑋</ci><ci id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3.2.3.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3.2.3">𝑒</ci></apply><ci id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.3.3">𝑘</ci></apply><apply id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1"><times id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.2"></times><ci id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.3.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.3">𝐸</ci><ci id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.4.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.4">𝑚</ci><ci id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.5.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.5">𝑏</ci><ci id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.6.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.6">𝑒</ci><ci id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.7.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.7">𝑑</ci><ci id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.8.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.8">𝑑</ci><ci id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.9.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.9">𝑖</ci><ci id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.10.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.10">𝑛</ci><ci id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.11.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.11">𝑔</ci><apply id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1.1.2">𝑒</ci><ci id="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS4.SSS1.Px2.p1.1.m1.1.1.1.1.1.1.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.Px2.p1.1.m1.1c">X_{e}^{k}=Embedding(e_{k})</annotation></semantics></math>, where <math id="S3.SS4.SSS1.Px2.p1.2.m2.1" class="ltx_Math" alttext="e_{k}" display="inline"><semantics id="S3.SS4.SSS1.Px2.p1.2.m2.1a"><msub id="S3.SS4.SSS1.Px2.p1.2.m2.1.1" xref="S3.SS4.SSS1.Px2.p1.2.m2.1.1.cmml"><mi id="S3.SS4.SSS1.Px2.p1.2.m2.1.1.2" xref="S3.SS4.SSS1.Px2.p1.2.m2.1.1.2.cmml">e</mi><mi id="S3.SS4.SSS1.Px2.p1.2.m2.1.1.3" xref="S3.SS4.SSS1.Px2.p1.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.Px2.p1.2.m2.1b"><apply id="S3.SS4.SSS1.Px2.p1.2.m2.1.1.cmml" xref="S3.SS4.SSS1.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.Px2.p1.2.m2.1.1.1.cmml" xref="S3.SS4.SSS1.Px2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.Px2.p1.2.m2.1.1.2.cmml" xref="S3.SS4.SSS1.Px2.p1.2.m2.1.1.2">𝑒</ci><ci id="S3.SS4.SSS1.Px2.p1.2.m2.1.1.3.cmml" xref="S3.SS4.SSS1.Px2.p1.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.Px2.p1.2.m2.1c">e_{k}</annotation></semantics></math> represents <math id="S3.SS4.SSS1.Px2.p1.3.m3.1" class="ltx_Math" alttext="k\text{-}th" display="inline"><semantics id="S3.SS4.SSS1.Px2.p1.3.m3.1a"><mrow id="S3.SS4.SSS1.Px2.p1.3.m3.1.1" xref="S3.SS4.SSS1.Px2.p1.3.m3.1.1.cmml"><mi id="S3.SS4.SSS1.Px2.p1.3.m3.1.1.2" xref="S3.SS4.SSS1.Px2.p1.3.m3.1.1.2.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS1.Px2.p1.3.m3.1.1.1" xref="S3.SS4.SSS1.Px2.p1.3.m3.1.1.1.cmml">​</mo><mtext id="S3.SS4.SSS1.Px2.p1.3.m3.1.1.3" xref="S3.SS4.SSS1.Px2.p1.3.m3.1.1.3a.cmml">-</mtext><mo lspace="0em" rspace="0em" id="S3.SS4.SSS1.Px2.p1.3.m3.1.1.1a" xref="S3.SS4.SSS1.Px2.p1.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS4.SSS1.Px2.p1.3.m3.1.1.4" xref="S3.SS4.SSS1.Px2.p1.3.m3.1.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS4.SSS1.Px2.p1.3.m3.1.1.1b" xref="S3.SS4.SSS1.Px2.p1.3.m3.1.1.1.cmml">​</mo><mi id="S3.SS4.SSS1.Px2.p1.3.m3.1.1.5" xref="S3.SS4.SSS1.Px2.p1.3.m3.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.Px2.p1.3.m3.1b"><apply id="S3.SS4.SSS1.Px2.p1.3.m3.1.1.cmml" xref="S3.SS4.SSS1.Px2.p1.3.m3.1.1"><times id="S3.SS4.SSS1.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS4.SSS1.Px2.p1.3.m3.1.1.1"></times><ci id="S3.SS4.SSS1.Px2.p1.3.m3.1.1.2.cmml" xref="S3.SS4.SSS1.Px2.p1.3.m3.1.1.2">𝑘</ci><ci id="S3.SS4.SSS1.Px2.p1.3.m3.1.1.3a.cmml" xref="S3.SS4.SSS1.Px2.p1.3.m3.1.1.3"><mtext id="S3.SS4.SSS1.Px2.p1.3.m3.1.1.3.cmml" xref="S3.SS4.SSS1.Px2.p1.3.m3.1.1.3">-</mtext></ci><ci id="S3.SS4.SSS1.Px2.p1.3.m3.1.1.4.cmml" xref="S3.SS4.SSS1.Px2.p1.3.m3.1.1.4">𝑡</ci><ci id="S3.SS4.SSS1.Px2.p1.3.m3.1.1.5.cmml" xref="S3.SS4.SSS1.Px2.p1.3.m3.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.Px2.p1.3.m3.1c">k\text{-}th</annotation></semantics></math> emotion label. The embedding network can be considered as the lookup-table operation.
The emotion embedding matrix is initialized using a random initializer and is fine-tuned throughout the training process. Employing a multi-head attention mechanism  <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>, the EAN treats utterance representations as query vectors and emotion representations as key and value vectors. The calculation process of the EAN mirrors that of a typical multi-head self-attention module (MHSA).</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="H_{e}=MHSA(Q,K,V)" display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.4" xref="S3.E1.m1.3.4.cmml"><msub id="S3.E1.m1.3.4.2" xref="S3.E1.m1.3.4.2.cmml"><mi id="S3.E1.m1.3.4.2.2" xref="S3.E1.m1.3.4.2.2.cmml">H</mi><mi id="S3.E1.m1.3.4.2.3" xref="S3.E1.m1.3.4.2.3.cmml">e</mi></msub><mo id="S3.E1.m1.3.4.1" xref="S3.E1.m1.3.4.1.cmml">=</mo><mrow id="S3.E1.m1.3.4.3" xref="S3.E1.m1.3.4.3.cmml"><mi id="S3.E1.m1.3.4.3.2" xref="S3.E1.m1.3.4.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.4.3.1" xref="S3.E1.m1.3.4.3.1.cmml">​</mo><mi id="S3.E1.m1.3.4.3.3" xref="S3.E1.m1.3.4.3.3.cmml">H</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.4.3.1a" xref="S3.E1.m1.3.4.3.1.cmml">​</mo><mi id="S3.E1.m1.3.4.3.4" xref="S3.E1.m1.3.4.3.4.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.4.3.1b" xref="S3.E1.m1.3.4.3.1.cmml">​</mo><mi id="S3.E1.m1.3.4.3.5" xref="S3.E1.m1.3.4.3.5.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.4.3.1c" xref="S3.E1.m1.3.4.3.1.cmml">​</mo><mrow id="S3.E1.m1.3.4.3.6.2" xref="S3.E1.m1.3.4.3.6.1.cmml"><mo stretchy="false" id="S3.E1.m1.3.4.3.6.2.1" xref="S3.E1.m1.3.4.3.6.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">Q</mi><mo id="S3.E1.m1.3.4.3.6.2.2" xref="S3.E1.m1.3.4.3.6.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">K</mi><mo id="S3.E1.m1.3.4.3.6.2.3" xref="S3.E1.m1.3.4.3.6.1.cmml">,</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">V</mi><mo stretchy="false" id="S3.E1.m1.3.4.3.6.2.4" xref="S3.E1.m1.3.4.3.6.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.4.cmml" xref="S3.E1.m1.3.4"><eq id="S3.E1.m1.3.4.1.cmml" xref="S3.E1.m1.3.4.1"></eq><apply id="S3.E1.m1.3.4.2.cmml" xref="S3.E1.m1.3.4.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.4.2.1.cmml" xref="S3.E1.m1.3.4.2">subscript</csymbol><ci id="S3.E1.m1.3.4.2.2.cmml" xref="S3.E1.m1.3.4.2.2">𝐻</ci><ci id="S3.E1.m1.3.4.2.3.cmml" xref="S3.E1.m1.3.4.2.3">𝑒</ci></apply><apply id="S3.E1.m1.3.4.3.cmml" xref="S3.E1.m1.3.4.3"><times id="S3.E1.m1.3.4.3.1.cmml" xref="S3.E1.m1.3.4.3.1"></times><ci id="S3.E1.m1.3.4.3.2.cmml" xref="S3.E1.m1.3.4.3.2">𝑀</ci><ci id="S3.E1.m1.3.4.3.3.cmml" xref="S3.E1.m1.3.4.3.3">𝐻</ci><ci id="S3.E1.m1.3.4.3.4.cmml" xref="S3.E1.m1.3.4.3.4">𝑆</ci><ci id="S3.E1.m1.3.4.3.5.cmml" xref="S3.E1.m1.3.4.3.5">𝐴</ci><vector id="S3.E1.m1.3.4.3.6.1.cmml" xref="S3.E1.m1.3.4.3.6.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑄</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝐾</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝑉</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">H_{e}=MHSA(Q,K,V)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.SSS1.Px2.p1.4" class="ltx_p">where <math id="S3.SS4.SSS1.Px2.p1.4.m1.2" class="ltx_Math" alttext="Q=H_{u},K=V=H_{e}" display="inline"><semantics id="S3.SS4.SSS1.Px2.p1.4.m1.2a"><mrow id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.3.cmml"><mrow id="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1" xref="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.cmml"><mi id="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.2" xref="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.2.cmml">Q</mi><mo id="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.1" xref="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.1.cmml">=</mo><msub id="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.3" xref="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.3.cmml"><mi id="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.3.2" xref="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.3.2.cmml">H</mi><mi id="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.3.3" xref="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.3.3.cmml">u</mi></msub></mrow><mo id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.3" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.3a.cmml">,</mo><mrow id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.cmml"><mi id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.2" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.2.cmml">K</mi><mo id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.3" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.3.cmml">=</mo><mi id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.4" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.4.cmml">V</mi><mo id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.5" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.5.cmml">=</mo><msub id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.6" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.6.cmml"><mi id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.6.2" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.6.2.cmml">H</mi><mi id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.6.3" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.6.3.cmml">e</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.Px2.p1.4.m1.2b"><apply id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.3.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2"><csymbol cd="ambiguous" id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.3a.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1"><eq id="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.1.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.1"></eq><ci id="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.2.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.2">𝑄</ci><apply id="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.3.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.3.1.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.3.2.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.3.2">𝐻</ci><ci id="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.3.3.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.1.1.1.1.3.3">𝑢</ci></apply></apply><apply id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2"><and id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2a.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2"></and><apply id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2b.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2"><eq id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.3.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.3"></eq><ci id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.2.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.2">𝐾</ci><ci id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.4.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.4">𝑉</ci></apply><apply id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2c.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2"><eq id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.5.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.5"></eq><share href="#S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.4.cmml" id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2d.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2"></share><apply id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.6.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.6"><csymbol cd="ambiguous" id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.6.1.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.6">subscript</csymbol><ci id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.6.2.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.6.2">𝐻</ci><ci id="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.6.3.cmml" xref="S3.SS4.SSS1.Px2.p1.4.m1.2.2.2.2.6.3">𝑒</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.Px2.p1.4.m1.2c">Q=H_{u},K=V=H_{e}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS4.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Speaker Attention Network</h5>

<div id="S3.SS4.SSS1.Px3.p1" class="ltx_para">
<p id="S3.SS4.SSS1.Px3.p1.1" class="ltx_p">The SAN facilitates interactions between utterances to incorporate speaker information by applying attention over the speaker relation graph.
There are two types of relation edges: (1) Intra-relation type, which signifies how the utterance influences other utterances, including itself, expressed by the same speaker; (2) Inter-relation type, indicating how the utterance influences those expressed by other speakers. The speaker representation given a relationship can be formulated by the graphical attention mechanism <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib51" title="" class="ltx_ref">2022</a>)</cite>.</p>
<table id="S3.E2" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E2X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle h_{s}^{i}" display="inline"><semantics id="S3.E2X.2.1.1.m1.1a"><msubsup id="S3.E2X.2.1.1.m1.1.1" xref="S3.E2X.2.1.1.m1.1.1.cmml"><mi id="S3.E2X.2.1.1.m1.1.1.2.2" xref="S3.E2X.2.1.1.m1.1.1.2.2.cmml">h</mi><mi id="S3.E2X.2.1.1.m1.1.1.2.3" xref="S3.E2X.2.1.1.m1.1.1.2.3.cmml">s</mi><mi id="S3.E2X.2.1.1.m1.1.1.3" xref="S3.E2X.2.1.1.m1.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.E2X.2.1.1.m1.1b"><apply id="S3.E2X.2.1.1.m1.1.1.cmml" xref="S3.E2X.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.1.1">superscript</csymbol><apply id="S3.E2X.2.1.1.m1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.1.1.2.1.cmml" xref="S3.E2X.2.1.1.m1.1.1">subscript</csymbol><ci id="S3.E2X.2.1.1.m1.1.1.2.2.cmml" xref="S3.E2X.2.1.1.m1.1.1.2.2">ℎ</ci><ci id="S3.E2X.2.1.1.m1.1.1.2.3.cmml" xref="S3.E2X.2.1.1.m1.1.1.2.3">𝑠</ci></apply><ci id="S3.E2X.2.1.1.m1.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2X.2.1.1.m1.1c">\displaystyle h_{s}^{i}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2X.3.2.2.m1.1" class="ltx_Math" alttext="\displaystyle=\sum_{i\in\mathcal{R}}\sum_{j\in\mathcal{N}_{i}^{r}}\alpha_{ijr}W_{r}h_{j}^{u}" display="inline"><semantics id="S3.E2X.3.2.2.m1.1a"><mrow id="S3.E2X.3.2.2.m1.1.1" xref="S3.E2X.3.2.2.m1.1.1.cmml"><mi id="S3.E2X.3.2.2.m1.1.1.2" xref="S3.E2X.3.2.2.m1.1.1.2.cmml"></mi><mo id="S3.E2X.3.2.2.m1.1.1.1" xref="S3.E2X.3.2.2.m1.1.1.1.cmml">=</mo><mrow id="S3.E2X.3.2.2.m1.1.1.3" xref="S3.E2X.3.2.2.m1.1.1.3.cmml"><mstyle displaystyle="true" id="S3.E2X.3.2.2.m1.1.1.3.1" xref="S3.E2X.3.2.2.m1.1.1.3.1.cmml"><munder id="S3.E2X.3.2.2.m1.1.1.3.1a" xref="S3.E2X.3.2.2.m1.1.1.3.1.cmml"><mo movablelimits="false" id="S3.E2X.3.2.2.m1.1.1.3.1.2" xref="S3.E2X.3.2.2.m1.1.1.3.1.2.cmml">∑</mo><mrow id="S3.E2X.3.2.2.m1.1.1.3.1.3" xref="S3.E2X.3.2.2.m1.1.1.3.1.3.cmml"><mi id="S3.E2X.3.2.2.m1.1.1.3.1.3.2" xref="S3.E2X.3.2.2.m1.1.1.3.1.3.2.cmml">i</mi><mo id="S3.E2X.3.2.2.m1.1.1.3.1.3.1" xref="S3.E2X.3.2.2.m1.1.1.3.1.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2X.3.2.2.m1.1.1.3.1.3.3" xref="S3.E2X.3.2.2.m1.1.1.3.1.3.3.cmml">ℛ</mi></mrow></munder></mstyle><mrow id="S3.E2X.3.2.2.m1.1.1.3.2" xref="S3.E2X.3.2.2.m1.1.1.3.2.cmml"><mstyle displaystyle="true" id="S3.E2X.3.2.2.m1.1.1.3.2.1" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.cmml"><munder id="S3.E2X.3.2.2.m1.1.1.3.2.1a" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.cmml"><mo movablelimits="false" id="S3.E2X.3.2.2.m1.1.1.3.2.1.2" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.2.cmml">∑</mo><mrow id="S3.E2X.3.2.2.m1.1.1.3.2.1.3" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.3.cmml"><mi id="S3.E2X.3.2.2.m1.1.1.3.2.1.3.2" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.3.2.cmml">j</mi><mo id="S3.E2X.3.2.2.m1.1.1.3.2.1.3.1" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.3.1.cmml">∈</mo><msubsup id="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3.2.2" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3.2.2.cmml">𝒩</mi><mi id="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3.2.3" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3.2.3.cmml">i</mi><mi id="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3.3" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3.3.cmml">r</mi></msubsup></mrow></munder></mstyle><mrow id="S3.E2X.3.2.2.m1.1.1.3.2.2" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.cmml"><msub id="S3.E2X.3.2.2.m1.1.1.3.2.2.2" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.2.cmml"><mi id="S3.E2X.3.2.2.m1.1.1.3.2.2.2.2" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.2.2.cmml">α</mi><mrow id="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.cmml"><mi id="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.2" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.1" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.1.cmml">​</mo><mi id="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.3" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.3.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.1a" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.1.cmml">​</mo><mi id="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.4" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.4.cmml">r</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E2X.3.2.2.m1.1.1.3.2.2.1" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.1.cmml">​</mo><msub id="S3.E2X.3.2.2.m1.1.1.3.2.2.3" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.3.cmml"><mi id="S3.E2X.3.2.2.m1.1.1.3.2.2.3.2" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.3.2.cmml">W</mi><mi id="S3.E2X.3.2.2.m1.1.1.3.2.2.3.3" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.3.3.cmml">r</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2X.3.2.2.m1.1.1.3.2.2.1a" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.1.cmml">​</mo><msubsup id="S3.E2X.3.2.2.m1.1.1.3.2.2.4" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.4.cmml"><mi id="S3.E2X.3.2.2.m1.1.1.3.2.2.4.2.2" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.4.2.2.cmml">h</mi><mi id="S3.E2X.3.2.2.m1.1.1.3.2.2.4.2.3" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.4.2.3.cmml">j</mi><mi id="S3.E2X.3.2.2.m1.1.1.3.2.2.4.3" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.4.3.cmml">u</mi></msubsup></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2X.3.2.2.m1.1b"><apply id="S3.E2X.3.2.2.m1.1.1.cmml" xref="S3.E2X.3.2.2.m1.1.1"><eq id="S3.E2X.3.2.2.m1.1.1.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.1"></eq><csymbol cd="latexml" id="S3.E2X.3.2.2.m1.1.1.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.2">absent</csymbol><apply id="S3.E2X.3.2.2.m1.1.1.3.cmml" xref="S3.E2X.3.2.2.m1.1.1.3"><apply id="S3.E2X.3.2.2.m1.1.1.3.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E2X.3.2.2.m1.1.1.3.1.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.1">subscript</csymbol><sum id="S3.E2X.3.2.2.m1.1.1.3.1.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.1.2"></sum><apply id="S3.E2X.3.2.2.m1.1.1.3.1.3.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.1.3"><in id="S3.E2X.3.2.2.m1.1.1.3.1.3.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.1.3.1"></in><ci id="S3.E2X.3.2.2.m1.1.1.3.1.3.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.1.3.2">𝑖</ci><ci id="S3.E2X.3.2.2.m1.1.1.3.1.3.3.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.1.3.3">ℛ</ci></apply></apply><apply id="S3.E2X.3.2.2.m1.1.1.3.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2"><apply id="S3.E2X.3.2.2.m1.1.1.3.2.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.1"><csymbol cd="ambiguous" id="S3.E2X.3.2.2.m1.1.1.3.2.1.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.1">subscript</csymbol><sum id="S3.E2X.3.2.2.m1.1.1.3.2.1.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.2"></sum><apply id="S3.E2X.3.2.2.m1.1.1.3.2.1.3.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.3"><in id="S3.E2X.3.2.2.m1.1.1.3.2.1.3.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.3.1"></in><ci id="S3.E2X.3.2.2.m1.1.1.3.2.1.3.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.3.2">𝑗</ci><apply id="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3"><csymbol cd="ambiguous" id="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3">superscript</csymbol><apply id="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3"><csymbol cd="ambiguous" id="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3.2.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3">subscript</csymbol><ci id="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3.2.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3.2.2">𝒩</ci><ci id="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3.2.3.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3.2.3">𝑖</ci></apply><ci id="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3.3.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.3.3.3">𝑟</ci></apply></apply></apply><apply id="S3.E2X.3.2.2.m1.1.1.3.2.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2"><times id="S3.E2X.3.2.2.m1.1.1.3.2.2.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.1"></times><apply id="S3.E2X.3.2.2.m1.1.1.3.2.2.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S3.E2X.3.2.2.m1.1.1.3.2.2.2.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.2">subscript</csymbol><ci id="S3.E2X.3.2.2.m1.1.1.3.2.2.2.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.2.2">𝛼</ci><apply id="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3"><times id="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.1"></times><ci id="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.2">𝑖</ci><ci id="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.3.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.3">𝑗</ci><ci id="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.4.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.2.3.4">𝑟</ci></apply></apply><apply id="S3.E2X.3.2.2.m1.1.1.3.2.2.3.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S3.E2X.3.2.2.m1.1.1.3.2.2.3.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.3">subscript</csymbol><ci id="S3.E2X.3.2.2.m1.1.1.3.2.2.3.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.3.2">𝑊</ci><ci id="S3.E2X.3.2.2.m1.1.1.3.2.2.3.3.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.3.3">𝑟</ci></apply><apply id="S3.E2X.3.2.2.m1.1.1.3.2.2.4.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.4"><csymbol cd="ambiguous" id="S3.E2X.3.2.2.m1.1.1.3.2.2.4.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.4">superscript</csymbol><apply id="S3.E2X.3.2.2.m1.1.1.3.2.2.4.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.4"><csymbol cd="ambiguous" id="S3.E2X.3.2.2.m1.1.1.3.2.2.4.2.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.4">subscript</csymbol><ci id="S3.E2X.3.2.2.m1.1.1.3.2.2.4.2.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.4.2.2">ℎ</ci><ci id="S3.E2X.3.2.2.m1.1.1.3.2.2.4.2.3.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.4.2.3">𝑗</ci></apply><ci id="S3.E2X.3.2.2.m1.1.1.3.2.2.4.3.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.4.3">𝑢</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2X.3.2.2.m1.1c">\displaystyle=\sum_{i\in\mathcal{R}}\sum_{j\in\mathcal{N}_{i}^{r}}\alpha_{ijr}W_{r}h_{j}^{u}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(2)</span></td>
</tr>
<tr id="S3.E2Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2Xa.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle\alpha_{ijr}" display="inline"><semantics id="S3.E2Xa.2.1.1.m1.1a"><msub id="S3.E2Xa.2.1.1.m1.1.1" xref="S3.E2Xa.2.1.1.m1.1.1.cmml"><mi id="S3.E2Xa.2.1.1.m1.1.1.2" xref="S3.E2Xa.2.1.1.m1.1.1.2.cmml">α</mi><mrow id="S3.E2Xa.2.1.1.m1.1.1.3" xref="S3.E2Xa.2.1.1.m1.1.1.3.cmml"><mi id="S3.E2Xa.2.1.1.m1.1.1.3.2" xref="S3.E2Xa.2.1.1.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2Xa.2.1.1.m1.1.1.3.1" xref="S3.E2Xa.2.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.E2Xa.2.1.1.m1.1.1.3.3" xref="S3.E2Xa.2.1.1.m1.1.1.3.3.cmml">j</mi><mo lspace="0em" rspace="0em" id="S3.E2Xa.2.1.1.m1.1.1.3.1a" xref="S3.E2Xa.2.1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.E2Xa.2.1.1.m1.1.1.3.4" xref="S3.E2Xa.2.1.1.m1.1.1.3.4.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.E2Xa.2.1.1.m1.1b"><apply id="S3.E2Xa.2.1.1.m1.1.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.E2Xa.2.1.1.m1.1.1.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1">subscript</csymbol><ci id="S3.E2Xa.2.1.1.m1.1.1.2.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.2">𝛼</ci><apply id="S3.E2Xa.2.1.1.m1.1.1.3.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.3"><times id="S3.E2Xa.2.1.1.m1.1.1.3.1.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.3.1"></times><ci id="S3.E2Xa.2.1.1.m1.1.1.3.2.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.3.2">𝑖</ci><ci id="S3.E2Xa.2.1.1.m1.1.1.3.3.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.3.3">𝑗</ci><ci id="S3.E2Xa.2.1.1.m1.1.1.3.4.cmml" xref="S3.E2Xa.2.1.1.m1.1.1.3.4">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2Xa.2.1.1.m1.1c">\displaystyle\alpha_{ijr}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2Xa.3.2.2.m1.1" class="ltx_math_unparsed" alttext="\displaystyle=softmax(ReLu(\alpha_{r}^{T}W_{r}[h_{i}^{u}||h_{j}^{u}]))" display="inline"><semantics id="S3.E2Xa.3.2.2.m1.1a"><mrow id="S3.E2Xa.3.2.2.m1.1b"><mo id="S3.E2Xa.3.2.2.m1.1.1">=</mo><mi id="S3.E2Xa.3.2.2.m1.1.2">s</mi><mi id="S3.E2Xa.3.2.2.m1.1.3">o</mi><mi id="S3.E2Xa.3.2.2.m1.1.4">f</mi><mi id="S3.E2Xa.3.2.2.m1.1.5">t</mi><mi id="S3.E2Xa.3.2.2.m1.1.6">m</mi><mi id="S3.E2Xa.3.2.2.m1.1.7">a</mi><mi id="S3.E2Xa.3.2.2.m1.1.8">x</mi><mrow id="S3.E2Xa.3.2.2.m1.1.9"><mo stretchy="false" id="S3.E2Xa.3.2.2.m1.1.9.1">(</mo><mi id="S3.E2Xa.3.2.2.m1.1.9.2">R</mi><mi id="S3.E2Xa.3.2.2.m1.1.9.3">e</mi><mi id="S3.E2Xa.3.2.2.m1.1.9.4">L</mi><mi id="S3.E2Xa.3.2.2.m1.1.9.5">u</mi><mrow id="S3.E2Xa.3.2.2.m1.1.9.6"><mo stretchy="false" id="S3.E2Xa.3.2.2.m1.1.9.6.1">(</mo><msubsup id="S3.E2Xa.3.2.2.m1.1.9.6.2"><mi id="S3.E2Xa.3.2.2.m1.1.9.6.2.2.2">α</mi><mi id="S3.E2Xa.3.2.2.m1.1.9.6.2.2.3">r</mi><mi id="S3.E2Xa.3.2.2.m1.1.9.6.2.3">T</mi></msubsup><msub id="S3.E2Xa.3.2.2.m1.1.9.6.3"><mi id="S3.E2Xa.3.2.2.m1.1.9.6.3.2">W</mi><mi id="S3.E2Xa.3.2.2.m1.1.9.6.3.3">r</mi></msub><mrow id="S3.E2Xa.3.2.2.m1.1.9.6.4"><mo stretchy="false" id="S3.E2Xa.3.2.2.m1.1.9.6.4.1">[</mo><msubsup id="S3.E2Xa.3.2.2.m1.1.9.6.4.2"><mi id="S3.E2Xa.3.2.2.m1.1.9.6.4.2.2.2">h</mi><mi id="S3.E2Xa.3.2.2.m1.1.9.6.4.2.2.3">i</mi><mi id="S3.E2Xa.3.2.2.m1.1.9.6.4.2.3">u</mi></msubsup><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E2Xa.3.2.2.m1.1.9.6.4.3">|</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E2Xa.3.2.2.m1.1.9.6.4.4">|</mo><msubsup id="S3.E2Xa.3.2.2.m1.1.9.6.4.5"><mi id="S3.E2Xa.3.2.2.m1.1.9.6.4.5.2.2">h</mi><mi id="S3.E2Xa.3.2.2.m1.1.9.6.4.5.2.3">j</mi><mi id="S3.E2Xa.3.2.2.m1.1.9.6.4.5.3">u</mi></msubsup><mo stretchy="false" id="S3.E2Xa.3.2.2.m1.1.9.6.4.6">]</mo></mrow><mo stretchy="false" id="S3.E2Xa.3.2.2.m1.1.9.6.5">)</mo></mrow><mo stretchy="false" id="S3.E2Xa.3.2.2.m1.1.9.7">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.E2Xa.3.2.2.m1.1c">\displaystyle=softmax(ReLu(\alpha_{r}^{T}W_{r}[h_{i}^{u}||h_{j}^{u}]))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="S3.SS4.SSS1.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Interaction Network</h5>

<div id="S3.SS4.SSS1.Px4.p1" class="ltx_para">
<p id="S3.SS4.SSS1.Px4.p1.1" class="ltx_p">To efficiently exchange pertinent information between the EAN and SAN, a mutual Bi-Affine transformation is applied as a bridge <cite class="ltx_cite ltx_citemacro_cite">Dozat and Manning (<a href="#bib.bib8" title="" class="ltx_ref">2016</a>)</cite>. In our Interaction Network, we integrate a masking mechanism to accommodate the existence of empty utterance speakers in some instances, which differs from the original approach. We denote this approach as the Masking Interaction Network (MIN).</p>
<table id="S3.E3" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E3X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3X.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle\dot{H}_{e}" display="inline"><semantics id="S3.E3X.2.1.1.m1.1a"><msub id="S3.E3X.2.1.1.m1.1.1" xref="S3.E3X.2.1.1.m1.1.1.cmml"><mover accent="true" id="S3.E3X.2.1.1.m1.1.1.2" xref="S3.E3X.2.1.1.m1.1.1.2.cmml"><mi id="S3.E3X.2.1.1.m1.1.1.2.2" xref="S3.E3X.2.1.1.m1.1.1.2.2.cmml">H</mi><mo id="S3.E3X.2.1.1.m1.1.1.2.1" xref="S3.E3X.2.1.1.m1.1.1.2.1.cmml">˙</mo></mover><mi id="S3.E3X.2.1.1.m1.1.1.3" xref="S3.E3X.2.1.1.m1.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E3X.2.1.1.m1.1b"><apply id="S3.E3X.2.1.1.m1.1.1.cmml" xref="S3.E3X.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.E3X.2.1.1.m1.1.1.1.cmml" xref="S3.E3X.2.1.1.m1.1.1">subscript</csymbol><apply id="S3.E3X.2.1.1.m1.1.1.2.cmml" xref="S3.E3X.2.1.1.m1.1.1.2"><ci id="S3.E3X.2.1.1.m1.1.1.2.1.cmml" xref="S3.E3X.2.1.1.m1.1.1.2.1">˙</ci><ci id="S3.E3X.2.1.1.m1.1.1.2.2.cmml" xref="S3.E3X.2.1.1.m1.1.1.2.2">𝐻</ci></apply><ci id="S3.E3X.2.1.1.m1.1.1.3.cmml" xref="S3.E3X.2.1.1.m1.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3X.2.1.1.m1.1c">\displaystyle\dot{H}_{e}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3X.3.2.2.m1.1" class="ltx_Math" alttext="\displaystyle=softmax(Mask(H_{e}W_{1}H_{s}^{T}))H_{s}" display="inline"><semantics id="S3.E3X.3.2.2.m1.1a"><mrow id="S3.E3X.3.2.2.m1.1.1" xref="S3.E3X.3.2.2.m1.1.1.cmml"><mi id="S3.E3X.3.2.2.m1.1.1.3" xref="S3.E3X.3.2.2.m1.1.1.3.cmml"></mi><mo id="S3.E3X.3.2.2.m1.1.1.2" xref="S3.E3X.3.2.2.m1.1.1.2.cmml">=</mo><mrow id="S3.E3X.3.2.2.m1.1.1.1" xref="S3.E3X.3.2.2.m1.1.1.1.cmml"><mi id="S3.E3X.3.2.2.m1.1.1.1.3" xref="S3.E3X.3.2.2.m1.1.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E3X.3.2.2.m1.1.1.1.2" xref="S3.E3X.3.2.2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3X.3.2.2.m1.1.1.1.4" xref="S3.E3X.3.2.2.m1.1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3X.3.2.2.m1.1.1.1.2a" xref="S3.E3X.3.2.2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3X.3.2.2.m1.1.1.1.5" xref="S3.E3X.3.2.2.m1.1.1.1.5.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E3X.3.2.2.m1.1.1.1.2b" xref="S3.E3X.3.2.2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3X.3.2.2.m1.1.1.1.6" xref="S3.E3X.3.2.2.m1.1.1.1.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3X.3.2.2.m1.1.1.1.2c" xref="S3.E3X.3.2.2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3X.3.2.2.m1.1.1.1.7" xref="S3.E3X.3.2.2.m1.1.1.1.7.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E3X.3.2.2.m1.1.1.1.2d" xref="S3.E3X.3.2.2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3X.3.2.2.m1.1.1.1.8" xref="S3.E3X.3.2.2.m1.1.1.1.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3X.3.2.2.m1.1.1.1.2e" xref="S3.E3X.3.2.2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3X.3.2.2.m1.1.1.1.9" xref="S3.E3X.3.2.2.m1.1.1.1.9.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E3X.3.2.2.m1.1.1.1.2f" xref="S3.E3X.3.2.2.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E3X.3.2.2.m1.1.1.1.1.1" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3X.3.2.2.m1.1.1.1.1.1.2" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3X.3.2.2.m1.1.1.1.1.1.1" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.cmml"><mi id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.3" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.2" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.4" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.2a" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.5" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.2b" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.6" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.6.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.2c" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">H</mi><mi id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">e</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><msub id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">W</mi><mn id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.1a" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><msubsup id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.cmml"><mi id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.2" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.2.cmml">H</mi><mi id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.3" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.3.cmml">s</mi><mi id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.3" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.3.cmml">T</mi></msubsup></mrow><mo stretchy="false" id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E3X.3.2.2.m1.1.1.1.1.1.3" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E3X.3.2.2.m1.1.1.1.2g" xref="S3.E3X.3.2.2.m1.1.1.1.2.cmml">​</mo><msub id="S3.E3X.3.2.2.m1.1.1.1.10" xref="S3.E3X.3.2.2.m1.1.1.1.10.cmml"><mi id="S3.E3X.3.2.2.m1.1.1.1.10.2" xref="S3.E3X.3.2.2.m1.1.1.1.10.2.cmml">H</mi><mi id="S3.E3X.3.2.2.m1.1.1.1.10.3" xref="S3.E3X.3.2.2.m1.1.1.1.10.3.cmml">s</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3X.3.2.2.m1.1b"><apply id="S3.E3X.3.2.2.m1.1.1.cmml" xref="S3.E3X.3.2.2.m1.1.1"><eq id="S3.E3X.3.2.2.m1.1.1.2.cmml" xref="S3.E3X.3.2.2.m1.1.1.2"></eq><csymbol cd="latexml" id="S3.E3X.3.2.2.m1.1.1.3.cmml" xref="S3.E3X.3.2.2.m1.1.1.3">absent</csymbol><apply id="S3.E3X.3.2.2.m1.1.1.1.cmml" xref="S3.E3X.3.2.2.m1.1.1.1"><times id="S3.E3X.3.2.2.m1.1.1.1.2.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.2"></times><ci id="S3.E3X.3.2.2.m1.1.1.1.3.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.3">𝑠</ci><ci id="S3.E3X.3.2.2.m1.1.1.1.4.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.4">𝑜</ci><ci id="S3.E3X.3.2.2.m1.1.1.1.5.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.5">𝑓</ci><ci id="S3.E3X.3.2.2.m1.1.1.1.6.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.6">𝑡</ci><ci id="S3.E3X.3.2.2.m1.1.1.1.7.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.7">𝑚</ci><ci id="S3.E3X.3.2.2.m1.1.1.1.8.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.8">𝑎</ci><ci id="S3.E3X.3.2.2.m1.1.1.1.9.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.9">𝑥</ci><apply id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1"><times id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.2"></times><ci id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.3">𝑀</ci><ci id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.4.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.4">𝑎</ci><ci id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.5.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.5">𝑠</ci><ci id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.6.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.6">𝑘</ci><apply id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1"><times id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.1"></times><apply id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.2">𝐻</ci><ci id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.3">𝑒</ci></apply><apply id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.2">𝑊</ci><cn type="integer" id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.3">1</cn></apply><apply id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4">superscript</csymbol><apply id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.1.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.2.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.2">𝐻</ci><ci id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.3.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.3">𝑠</ci></apply><ci id="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.3">𝑇</ci></apply></apply></apply><apply id="S3.E3X.3.2.2.m1.1.1.1.10.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.10"><csymbol cd="ambiguous" id="S3.E3X.3.2.2.m1.1.1.1.10.1.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.10">subscript</csymbol><ci id="S3.E3X.3.2.2.m1.1.1.1.10.2.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.10.2">𝐻</ci><ci id="S3.E3X.3.2.2.m1.1.1.1.10.3.cmml" xref="S3.E3X.3.2.2.m1.1.1.1.10.3">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3X.3.2.2.m1.1c">\displaystyle=softmax(Mask(H_{e}W_{1}H_{s}^{T}))H_{s}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(3)</span></td>
</tr>
<tr id="S3.E3Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3Xa.2.1.1.m1.1" class="ltx_Math" alttext="\displaystyle\dot{H}_{s}" display="inline"><semantics id="S3.E3Xa.2.1.1.m1.1a"><msub id="S3.E3Xa.2.1.1.m1.1.1" xref="S3.E3Xa.2.1.1.m1.1.1.cmml"><mover accent="true" id="S3.E3Xa.2.1.1.m1.1.1.2" xref="S3.E3Xa.2.1.1.m1.1.1.2.cmml"><mi id="S3.E3Xa.2.1.1.m1.1.1.2.2" xref="S3.E3Xa.2.1.1.m1.1.1.2.2.cmml">H</mi><mo id="S3.E3Xa.2.1.1.m1.1.1.2.1" xref="S3.E3Xa.2.1.1.m1.1.1.2.1.cmml">˙</mo></mover><mi id="S3.E3Xa.2.1.1.m1.1.1.3" xref="S3.E3Xa.2.1.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E3Xa.2.1.1.m1.1b"><apply id="S3.E3Xa.2.1.1.m1.1.1.cmml" xref="S3.E3Xa.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.E3Xa.2.1.1.m1.1.1.1.cmml" xref="S3.E3Xa.2.1.1.m1.1.1">subscript</csymbol><apply id="S3.E3Xa.2.1.1.m1.1.1.2.cmml" xref="S3.E3Xa.2.1.1.m1.1.1.2"><ci id="S3.E3Xa.2.1.1.m1.1.1.2.1.cmml" xref="S3.E3Xa.2.1.1.m1.1.1.2.1">˙</ci><ci id="S3.E3Xa.2.1.1.m1.1.1.2.2.cmml" xref="S3.E3Xa.2.1.1.m1.1.1.2.2">𝐻</ci></apply><ci id="S3.E3Xa.2.1.1.m1.1.1.3.cmml" xref="S3.E3Xa.2.1.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3Xa.2.1.1.m1.1c">\displaystyle\dot{H}_{s}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E3Xa.3.2.2.m1.1" class="ltx_Math" alttext="\displaystyle=softmax(Mask(H_{s}W_{2}H_{e}^{T}))H_{e}" display="inline"><semantics id="S3.E3Xa.3.2.2.m1.1a"><mrow id="S3.E3Xa.3.2.2.m1.1.1" xref="S3.E3Xa.3.2.2.m1.1.1.cmml"><mi id="S3.E3Xa.3.2.2.m1.1.1.3" xref="S3.E3Xa.3.2.2.m1.1.1.3.cmml"></mi><mo id="S3.E3Xa.3.2.2.m1.1.1.2" xref="S3.E3Xa.3.2.2.m1.1.1.2.cmml">=</mo><mrow id="S3.E3Xa.3.2.2.m1.1.1.1" xref="S3.E3Xa.3.2.2.m1.1.1.1.cmml"><mi id="S3.E3Xa.3.2.2.m1.1.1.1.3" xref="S3.E3Xa.3.2.2.m1.1.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E3Xa.3.2.2.m1.1.1.1.2" xref="S3.E3Xa.3.2.2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3Xa.3.2.2.m1.1.1.1.4" xref="S3.E3Xa.3.2.2.m1.1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3Xa.3.2.2.m1.1.1.1.2a" xref="S3.E3Xa.3.2.2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3Xa.3.2.2.m1.1.1.1.5" xref="S3.E3Xa.3.2.2.m1.1.1.1.5.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E3Xa.3.2.2.m1.1.1.1.2b" xref="S3.E3Xa.3.2.2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3Xa.3.2.2.m1.1.1.1.6" xref="S3.E3Xa.3.2.2.m1.1.1.1.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E3Xa.3.2.2.m1.1.1.1.2c" xref="S3.E3Xa.3.2.2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3Xa.3.2.2.m1.1.1.1.7" xref="S3.E3Xa.3.2.2.m1.1.1.1.7.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E3Xa.3.2.2.m1.1.1.1.2d" xref="S3.E3Xa.3.2.2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3Xa.3.2.2.m1.1.1.1.8" xref="S3.E3Xa.3.2.2.m1.1.1.1.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3Xa.3.2.2.m1.1.1.1.2e" xref="S3.E3Xa.3.2.2.m1.1.1.1.2.cmml">​</mo><mi id="S3.E3Xa.3.2.2.m1.1.1.1.9" xref="S3.E3Xa.3.2.2.m1.1.1.1.9.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E3Xa.3.2.2.m1.1.1.1.2f" xref="S3.E3Xa.3.2.2.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E3Xa.3.2.2.m1.1.1.1.1.1" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.2" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.cmml"><mi id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.3" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.2" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.4" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.2a" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.5" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.2b" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.6" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.6.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.2c" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">H</mi><mi id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">s</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><msub id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">W</mi><mn id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.1a" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><msubsup id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.cmml"><mi id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.2" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.2.cmml">H</mi><mi id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.3" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.3.cmml">e</mi><mi id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.3" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.3.cmml">T</mi></msubsup></mrow><mo stretchy="false" id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.3" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E3Xa.3.2.2.m1.1.1.1.2g" xref="S3.E3Xa.3.2.2.m1.1.1.1.2.cmml">​</mo><msub id="S3.E3Xa.3.2.2.m1.1.1.1.10" xref="S3.E3Xa.3.2.2.m1.1.1.1.10.cmml"><mi id="S3.E3Xa.3.2.2.m1.1.1.1.10.2" xref="S3.E3Xa.3.2.2.m1.1.1.1.10.2.cmml">H</mi><mi id="S3.E3Xa.3.2.2.m1.1.1.1.10.3" xref="S3.E3Xa.3.2.2.m1.1.1.1.10.3.cmml">e</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3Xa.3.2.2.m1.1b"><apply id="S3.E3Xa.3.2.2.m1.1.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.1"><eq id="S3.E3Xa.3.2.2.m1.1.1.2.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.2"></eq><csymbol cd="latexml" id="S3.E3Xa.3.2.2.m1.1.1.3.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.3">absent</csymbol><apply id="S3.E3Xa.3.2.2.m1.1.1.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1"><times id="S3.E3Xa.3.2.2.m1.1.1.1.2.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.2"></times><ci id="S3.E3Xa.3.2.2.m1.1.1.1.3.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.3">𝑠</ci><ci id="S3.E3Xa.3.2.2.m1.1.1.1.4.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.4">𝑜</ci><ci id="S3.E3Xa.3.2.2.m1.1.1.1.5.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.5">𝑓</ci><ci id="S3.E3Xa.3.2.2.m1.1.1.1.6.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.6">𝑡</ci><ci id="S3.E3Xa.3.2.2.m1.1.1.1.7.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.7">𝑚</ci><ci id="S3.E3Xa.3.2.2.m1.1.1.1.8.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.8">𝑎</ci><ci id="S3.E3Xa.3.2.2.m1.1.1.1.9.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.9">𝑥</ci><apply id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1"><times id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.2"></times><ci id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.3">𝑀</ci><ci id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.4.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.4">𝑎</ci><ci id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.5.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.5">𝑠</ci><ci id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.6.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.6">𝑘</ci><apply id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1"><times id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.1"></times><apply id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.2">𝐻</ci><ci id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.2.3">𝑠</ci></apply><apply id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.2">𝑊</ci><cn type="integer" id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.3.3">2</cn></apply><apply id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4">superscript</csymbol><apply id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.2.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.2">𝐻</ci><ci id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.3.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.2.3">𝑒</ci></apply><ci id="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.3.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.1.1.1.1.1.1.4.3">𝑇</ci></apply></apply></apply><apply id="S3.E3Xa.3.2.2.m1.1.1.1.10.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.10"><csymbol cd="ambiguous" id="S3.E3Xa.3.2.2.m1.1.1.1.10.1.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.10">subscript</csymbol><ci id="S3.E3Xa.3.2.2.m1.1.1.1.10.2.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.10.2">𝐻</ci><ci id="S3.E3Xa.3.2.2.m1.1.1.1.10.3.cmml" xref="S3.E3Xa.3.2.2.m1.1.1.1.10.3">𝑒</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3Xa.3.2.2.m1.1c">\displaystyle=softmax(Mask(H_{s}W_{2}H_{e}^{T}))H_{e}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="S3.SS4.SSS1.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Cause Predictor</h5>

<div id="S3.SS4.SSS1.Px5.p1" class="ltx_para">
<p id="S3.SS4.SSS1.Px5.p1.6" class="ltx_p">The ultimate utterance representation for <math id="S3.SS4.SSS1.Px5.p1.1.m1.1" class="ltx_Math" alttext="U_{i}" display="inline"><semantics id="S3.SS4.SSS1.Px5.p1.1.m1.1a"><msub id="S3.SS4.SSS1.Px5.p1.1.m1.1.1" xref="S3.SS4.SSS1.Px5.p1.1.m1.1.1.cmml"><mi id="S3.SS4.SSS1.Px5.p1.1.m1.1.1.2" xref="S3.SS4.SSS1.Px5.p1.1.m1.1.1.2.cmml">U</mi><mi id="S3.SS4.SSS1.Px5.p1.1.m1.1.1.3" xref="S3.SS4.SSS1.Px5.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.Px5.p1.1.m1.1b"><apply id="S3.SS4.SSS1.Px5.p1.1.m1.1.1.cmml" xref="S3.SS4.SSS1.Px5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.Px5.p1.1.m1.1.1.1.cmml" xref="S3.SS4.SSS1.Px5.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.Px5.p1.1.m1.1.1.2.cmml" xref="S3.SS4.SSS1.Px5.p1.1.m1.1.1.2">𝑈</ci><ci id="S3.SS4.SSS1.Px5.p1.1.m1.1.1.3.cmml" xref="S3.SS4.SSS1.Px5.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.Px5.p1.1.m1.1c">U_{i}</annotation></semantics></math> is acquired by concatenating the output <math id="S3.SS4.SSS1.Px5.p1.2.m2.1" class="ltx_Math" alttext="\dot{H}_{e}" display="inline"><semantics id="S3.SS4.SSS1.Px5.p1.2.m2.1a"><msub id="S3.SS4.SSS1.Px5.p1.2.m2.1.1" xref="S3.SS4.SSS1.Px5.p1.2.m2.1.1.cmml"><mover accent="true" id="S3.SS4.SSS1.Px5.p1.2.m2.1.1.2" xref="S3.SS4.SSS1.Px5.p1.2.m2.1.1.2.cmml"><mi id="S3.SS4.SSS1.Px5.p1.2.m2.1.1.2.2" xref="S3.SS4.SSS1.Px5.p1.2.m2.1.1.2.2.cmml">H</mi><mo id="S3.SS4.SSS1.Px5.p1.2.m2.1.1.2.1" xref="S3.SS4.SSS1.Px5.p1.2.m2.1.1.2.1.cmml">˙</mo></mover><mi id="S3.SS4.SSS1.Px5.p1.2.m2.1.1.3" xref="S3.SS4.SSS1.Px5.p1.2.m2.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.Px5.p1.2.m2.1b"><apply id="S3.SS4.SSS1.Px5.p1.2.m2.1.1.cmml" xref="S3.SS4.SSS1.Px5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.Px5.p1.2.m2.1.1.1.cmml" xref="S3.SS4.SSS1.Px5.p1.2.m2.1.1">subscript</csymbol><apply id="S3.SS4.SSS1.Px5.p1.2.m2.1.1.2.cmml" xref="S3.SS4.SSS1.Px5.p1.2.m2.1.1.2"><ci id="S3.SS4.SSS1.Px5.p1.2.m2.1.1.2.1.cmml" xref="S3.SS4.SSS1.Px5.p1.2.m2.1.1.2.1">˙</ci><ci id="S3.SS4.SSS1.Px5.p1.2.m2.1.1.2.2.cmml" xref="S3.SS4.SSS1.Px5.p1.2.m2.1.1.2.2">𝐻</ci></apply><ci id="S3.SS4.SSS1.Px5.p1.2.m2.1.1.3.cmml" xref="S3.SS4.SSS1.Px5.p1.2.m2.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.Px5.p1.2.m2.1c">\dot{H}_{e}</annotation></semantics></math> and <math id="S3.SS4.SSS1.Px5.p1.3.m3.1" class="ltx_Math" alttext="\dot{H}_{s}" display="inline"><semantics id="S3.SS4.SSS1.Px5.p1.3.m3.1a"><msub id="S3.SS4.SSS1.Px5.p1.3.m3.1.1" xref="S3.SS4.SSS1.Px5.p1.3.m3.1.1.cmml"><mover accent="true" id="S3.SS4.SSS1.Px5.p1.3.m3.1.1.2" xref="S3.SS4.SSS1.Px5.p1.3.m3.1.1.2.cmml"><mi id="S3.SS4.SSS1.Px5.p1.3.m3.1.1.2.2" xref="S3.SS4.SSS1.Px5.p1.3.m3.1.1.2.2.cmml">H</mi><mo id="S3.SS4.SSS1.Px5.p1.3.m3.1.1.2.1" xref="S3.SS4.SSS1.Px5.p1.3.m3.1.1.2.1.cmml">˙</mo></mover><mi id="S3.SS4.SSS1.Px5.p1.3.m3.1.1.3" xref="S3.SS4.SSS1.Px5.p1.3.m3.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.Px5.p1.3.m3.1b"><apply id="S3.SS4.SSS1.Px5.p1.3.m3.1.1.cmml" xref="S3.SS4.SSS1.Px5.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.Px5.p1.3.m3.1.1.1.cmml" xref="S3.SS4.SSS1.Px5.p1.3.m3.1.1">subscript</csymbol><apply id="S3.SS4.SSS1.Px5.p1.3.m3.1.1.2.cmml" xref="S3.SS4.SSS1.Px5.p1.3.m3.1.1.2"><ci id="S3.SS4.SSS1.Px5.p1.3.m3.1.1.2.1.cmml" xref="S3.SS4.SSS1.Px5.p1.3.m3.1.1.2.1">˙</ci><ci id="S3.SS4.SSS1.Px5.p1.3.m3.1.1.2.2.cmml" xref="S3.SS4.SSS1.Px5.p1.3.m3.1.1.2.2">𝐻</ci></apply><ci id="S3.SS4.SSS1.Px5.p1.3.m3.1.1.3.cmml" xref="S3.SS4.SSS1.Px5.p1.3.m3.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.Px5.p1.3.m3.1c">\dot{H}_{s}</annotation></semantics></math> from the <math id="S3.SS4.SSS1.Px5.p1.4.m4.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS4.SSS1.Px5.p1.4.m4.1a"><mi id="S3.SS4.SSS1.Px5.p1.4.m4.1.1" xref="S3.SS4.SSS1.Px5.p1.4.m4.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.Px5.p1.4.m4.1b"><ci id="S3.SS4.SSS1.Px5.p1.4.m4.1.1.cmml" xref="S3.SS4.SSS1.Px5.p1.4.m4.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.Px5.p1.4.m4.1c">L</annotation></semantics></math>-layer TSAM. Subsequently, the concatenated vector undergoes classification using a fully-connected network. Given the target utterance <math id="S3.SS4.SSS1.Px5.p1.5.m5.1" class="ltx_Math" alttext="U_{i}" display="inline"><semantics id="S3.SS4.SSS1.Px5.p1.5.m5.1a"><msub id="S3.SS4.SSS1.Px5.p1.5.m5.1.1" xref="S3.SS4.SSS1.Px5.p1.5.m5.1.1.cmml"><mi id="S3.SS4.SSS1.Px5.p1.5.m5.1.1.2" xref="S3.SS4.SSS1.Px5.p1.5.m5.1.1.2.cmml">U</mi><mi id="S3.SS4.SSS1.Px5.p1.5.m5.1.1.3" xref="S3.SS4.SSS1.Px5.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.Px5.p1.5.m5.1b"><apply id="S3.SS4.SSS1.Px5.p1.5.m5.1.1.cmml" xref="S3.SS4.SSS1.Px5.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.Px5.p1.5.m5.1.1.1.cmml" xref="S3.SS4.SSS1.Px5.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.Px5.p1.5.m5.1.1.2.cmml" xref="S3.SS4.SSS1.Px5.p1.5.m5.1.1.2">𝑈</ci><ci id="S3.SS4.SSS1.Px5.p1.5.m5.1.1.3.cmml" xref="S3.SS4.SSS1.Px5.p1.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.Px5.p1.5.m5.1c">U_{i}</annotation></semantics></math>, the causal probability of the <math id="S3.SS4.SSS1.Px5.p1.6.m6.1" class="ltx_Math" alttext="U_{j}" display="inline"><semantics id="S3.SS4.SSS1.Px5.p1.6.m6.1a"><msub id="S3.SS4.SSS1.Px5.p1.6.m6.1.1" xref="S3.SS4.SSS1.Px5.p1.6.m6.1.1.cmml"><mi id="S3.SS4.SSS1.Px5.p1.6.m6.1.1.2" xref="S3.SS4.SSS1.Px5.p1.6.m6.1.1.2.cmml">U</mi><mi id="S3.SS4.SSS1.Px5.p1.6.m6.1.1.3" xref="S3.SS4.SSS1.Px5.p1.6.m6.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS1.Px5.p1.6.m6.1b"><apply id="S3.SS4.SSS1.Px5.p1.6.m6.1.1.cmml" xref="S3.SS4.SSS1.Px5.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS4.SSS1.Px5.p1.6.m6.1.1.1.cmml" xref="S3.SS4.SSS1.Px5.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS4.SSS1.Px5.p1.6.m6.1.1.2.cmml" xref="S3.SS4.SSS1.Px5.p1.6.m6.1.1.2">𝑈</ci><ci id="S3.SS4.SSS1.Px5.p1.6.m6.1.1.3.cmml" xref="S3.SS4.SSS1.Px5.p1.6.m6.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS1.Px5.p1.6.m6.1c">U_{j}</annotation></semantics></math> can be formulated as follows:</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.2" class="ltx_math_unparsed" alttext="p_{i,j}=sigmoid(fc(H_{s}^{j}||H_{e}^{j}))" display="block"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2b"><msub id="S3.E4.m1.2.3"><mi id="S3.E4.m1.2.3.2">p</mi><mrow id="S3.E4.m1.2.2.2.4"><mi id="S3.E4.m1.1.1.1.1">i</mi><mo id="S3.E4.m1.2.2.2.4.1">,</mo><mi id="S3.E4.m1.2.2.2.2">j</mi></mrow></msub><mo id="S3.E4.m1.2.4">=</mo><mi id="S3.E4.m1.2.5">s</mi><mi id="S3.E4.m1.2.6">i</mi><mi id="S3.E4.m1.2.7">g</mi><mi id="S3.E4.m1.2.8">m</mi><mi id="S3.E4.m1.2.9">o</mi><mi id="S3.E4.m1.2.10">i</mi><mi id="S3.E4.m1.2.11">d</mi><mrow id="S3.E4.m1.2.12"><mo stretchy="false" id="S3.E4.m1.2.12.1">(</mo><mi id="S3.E4.m1.2.12.2">f</mi><mi id="S3.E4.m1.2.12.3">c</mi><mrow id="S3.E4.m1.2.12.4"><mo stretchy="false" id="S3.E4.m1.2.12.4.1">(</mo><msubsup id="S3.E4.m1.2.12.4.2"><mi id="S3.E4.m1.2.12.4.2.2.2">H</mi><mi id="S3.E4.m1.2.12.4.2.2.3">s</mi><mi id="S3.E4.m1.2.12.4.2.3">j</mi></msubsup><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E4.m1.2.12.4.3">|</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S3.E4.m1.2.12.4.4">|</mo><msubsup id="S3.E4.m1.2.12.4.5"><mi id="S3.E4.m1.2.12.4.5.2.2">H</mi><mi id="S3.E4.m1.2.12.4.5.2.3">e</mi><mi id="S3.E4.m1.2.12.4.5.3">j</mi></msubsup><mo stretchy="false" id="S3.E4.m1.2.12.4.6">)</mo></mrow><mo stretchy="false" id="S3.E4.m1.2.12.5">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.E4.m1.2c">p_{i,j}=sigmoid(fc(H_{s}^{j}||H_{e}^{j}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS4.SSS1.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Multi-task Learning Auxiliary Task (MTLA)</h5>

<div id="S3.SS4.SSS1.Px6.p1" class="ltx_para">
<p id="S3.SS4.SSS1.Px6.p1.1" class="ltx_p">One drawback of the pipeline framework is that the extraction of utterance emotion and causal information are treated as separate tasks, potentially limiting the exploration of implicit relationships between them. Therefore, we incorporate emotion prediction as an auxiliary task within a multi-task learning framework. For emotion prediction, we utilize a classification head atop the Transformer-based model and apply the Dice loss <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite> as the multi-category classification loss.</p>
</div>
</section>
</section>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Infusion of Video and Audio Information </h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">The video data potentially carries rich knowledge for emotion analysis and existing research <cite class="ltx_cite ltx_citemacro_cite">Caridakis et al. (<a href="#bib.bib3" title="" class="ltx_ref">2007</a>)</cite> has underscored the significance of multi-modal information in augmenting the semantic prediction capabilities of models. Our study leverages the visual and auditory cues present in conversational contexts with the aim of bolstering the efficacy of our language models in emotion analysis tasks.</p>
</div>
<section id="S3.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.1 </span>Embedding and Concating Strategy</h4>

<div id="S3.SS5.SSS1.p1" class="ltx_para">
<p id="S3.SS5.SSS1.p1.1" class="ltx_p">We set up specific embedding and fusion strategies for different language models. For BERT, we use the concatenation of textual and multi-modal features in the hidden layer. For Large Language Models (LLMs), our approach is characterized by the utilization of visual captions as supportive prompts, thereby furnishing the LLMs with an enriched informational context.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T1.1.1.1.1" class="ltx_text ltx_font_bold">Models</span></td>
<td id="S3.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.2.1" class="ltx_text ltx_font_bold">LLM</span></td>
<td id="S3.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.3.1" class="ltx_text ltx_font_bold">w-avg F1</span></td>
<td id="S3.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.4.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
</tr>
<tr id="S3.T1.1.2" class="ltx_tr">
<td id="S3.T1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Origin InstructERC</td>
<td id="S3.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">Llama-2-7B-chat</td>
<td id="S3.T1.1.2.3" class="ltx_td ltx_align_center ltx_border_t">53.83</td>
<td id="S3.T1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">50.87</td>
</tr>
<tr id="S3.T1.1.3" class="ltx_tr">
<td id="S3.T1.1.3.1" class="ltx_td ltx_align_left">Origin InstructERC</td>
<td id="S3.T1.1.3.2" class="ltx_td ltx_align_center">Llama-2-13B-chat</td>
<td id="S3.T1.1.3.3" class="ltx_td ltx_align_center">55.50</td>
<td id="S3.T1.1.3.4" class="ltx_td ltx_align_center">48.93</td>
</tr>
<tr id="S3.T1.1.4" class="ltx_tr">
<td id="S3.T1.1.4.1" class="ltx_td ltx_align_left ltx_border_t">Ours-ERC-7B</td>
<td id="S3.T1.1.4.2" class="ltx_td ltx_align_center ltx_border_t">Llama-2-7B-chat</td>
<td id="S3.T1.1.4.3" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.1.4.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T1.1.5" class="ltx_tr">
<td id="S3.T1.1.5.1" class="ltx_td ltx_align_left">+ 3 auxiliary tasks</td>
<td id="S3.T1.1.5.2" class="ltx_td"></td>
<td id="S3.T1.1.5.3" class="ltx_td ltx_align_center">56.88</td>
<td id="S3.T1.1.5.4" class="ltx_td ltx_align_center">61.38</td>
</tr>
<tr id="S3.T1.1.6" class="ltx_tr">
<td id="S3.T1.1.6.1" class="ltx_td ltx_align_left">+ 3 auxiliary tasks &amp; historical clips desc</td>
<td id="S3.T1.1.6.2" class="ltx_td"></td>
<td id="S3.T1.1.6.3" class="ltx_td ltx_align_center">57.74</td>
<td id="S3.T1.1.6.4" class="ltx_td ltx_align_center">57.02</td>
</tr>
<tr id="S3.T1.1.7" class="ltx_tr">
<td id="S3.T1.1.7.1" class="ltx_td ltx_align_left">+ 3 auxiliary tasks &amp; utterance clips desc</td>
<td id="S3.T1.1.7.2" class="ltx_td"></td>
<td id="S3.T1.1.7.3" class="ltx_td ltx_align_center">58.42</td>
<td id="S3.T1.1.7.4" class="ltx_td ltx_align_center">57.92</td>
</tr>
<tr id="S3.T1.1.8" class="ltx_tr">
<td id="S3.T1.1.8.1" class="ltx_td ltx_align_left ltx_border_t">Ours-ERC-13B</td>
<td id="S3.T1.1.8.2" class="ltx_td ltx_align_center ltx_border_t">Llama-2-13B-chat</td>
<td id="S3.T1.1.8.3" class="ltx_td ltx_border_t"></td>
<td id="S3.T1.1.8.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T1.1.9" class="ltx_tr">
<td id="S3.T1.1.9.1" class="ltx_td ltx_align_left">+ 3 auxiliary tasks</td>
<td id="S3.T1.1.9.2" class="ltx_td"></td>
<td id="S3.T1.1.9.3" class="ltx_td ltx_align_center">57.85</td>
<td id="S3.T1.1.9.4" class="ltx_td ltx_align_center"><span id="S3.T1.1.9.4.1" class="ltx_text ltx_font_bold">61.45</span></td>
</tr>
<tr id="S3.T1.1.10" class="ltx_tr">
<td id="S3.T1.1.10.1" class="ltx_td ltx_align_left">+ 3 auxiliary tasks &amp; historical clips desc</td>
<td id="S3.T1.1.10.2" class="ltx_td"></td>
<td id="S3.T1.1.10.3" class="ltx_td ltx_align_center"><span id="S3.T1.1.10.3.1" class="ltx_text ltx_font_bold">58.64</span></td>
<td id="S3.T1.1.10.4" class="ltx_td ltx_align_center">60.83</td>
</tr>
<tr id="S3.T1.1.11" class="ltx_tr">
<td id="S3.T1.1.11.1" class="ltx_td ltx_align_left ltx_border_b">+ 3 auxiliary tasks &amp; utterance clips desc</td>
<td id="S3.T1.1.11.2" class="ltx_td ltx_border_b"></td>
<td id="S3.T1.1.11.3" class="ltx_td ltx_align_center ltx_border_b">58.50</td>
<td id="S3.T1.1.11.4" class="ltx_td ltx_align_center ltx_border_b">61.04</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results of ERC task on test set without neutral utterances.</figcaption>
</figure>
</section>
<section id="S3.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.2 </span>Extract Audio Feature Set</h4>

<div id="S3.SS5.SSS2.p1" class="ltx_para">
<p id="S3.SS5.SSS2.p1.1" class="ltx_p">Audio data contains valuable information for emotion analysis, including tone, pitch, speed, and intensity of speech, as well as non-linguistic sounds and pauses, which together convey rich emotional cues. We use openSMILE  <cite class="ltx_cite ltx_citemacro_cite">Eyben et al. (<a href="#bib.bib10" title="" class="ltx_ref">2010</a>)</cite> to extract two comprehensive feature sets: GeMAPS  <cite class="ltx_cite ltx_citemacro_cite">Eyben et al. (<a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite> and ComParE  <cite class="ltx_cite ltx_citemacro_cite">Schuller and Batliner (<a href="#bib.bib36" title="" class="ltx_ref">2013</a>)</cite>. GeMAPS is proposed for its effectiveness in capturing emotion-relevant vocal characteristics and ComParE encompasses a wide range of descriptors.</p>
</div>
</section>
<section id="S3.SS5.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.3 </span>Video Image to Text</h4>

<div id="S3.SS5.SSS3.p1" class="ltx_para">
<p id="S3.SS5.SSS3.p1.1" class="ltx_p">Integrating multi-modal features directly into the hidden layers of Large Language Models (LLMs) presents a significant challenge, primarily due to the prohibitive requirements for data and computational resources, such as GPUs. Although some finetuning strategies like prompt tuning could achieve it by addiing features to the input layer, we convert video to text with captioning where we can leverage our well-trained ERC model.</p>
</div>
<div id="S3.SS5.SSS3.p2" class="ltx_para">
<p id="S3.SS5.SSS3.p2.1" class="ltx_p">The performance of image captioning has been further enhanced with the outstanding NLU ability of LLMs. Large VLMs like LLaVA  <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib28" title="" class="ltx_ref">2023a</a>)</cite> provide GPT-4 level multi-modal capability by visual instruction tuning. Furthermore, the Audio-Visual Language Model, Video-Llama  <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib52" title="" class="ltx_ref">2023a</a>)</cite>, integrates both visual and audio encoders, enabling the comprehensive fusion of entire video content into LLMs. Without further training the VLMs as lack data, a well-designed prompt instructs the model to generate an emotion-related description. Our prompt asks the model to generate information from the front-ground event and place to character movements, the main character, facial expression, and finally emotion. The use of Chain-of-Thought <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib46" title="" class="ltx_ref">2022</a>)</cite> prompting further guides the model through a step-by-step process to derive the final emotion label. The output generated at each step is then incorporated into the ERC model, enriching it with a more detailed informational context.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tr id="S3.T2.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T2.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S3.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.1.1.2.1" class="ltx_text ltx_font_bold">Pre-trained Model</span></td>
<td id="S3.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.1.1.3.1" class="ltx_text ltx_font_bold">Test Pos.F1<sup id="S3.T2.1.1.3.1.1" class="ltx_sup"><span id="S3.T2.1.1.3.1.1.1" class="ltx_text ltx_font_medium">*</span></sup></span></td>
<td id="S3.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.1.1.4.1" class="ltx_text ltx_font_bold">Eval Pos.F1<sup id="S3.T2.1.1.4.1.1" class="ltx_sup"><span id="S3.T2.1.1.4.1.1.1" class="ltx_text ltx_font_medium">**</span></sup></span></td>
</tr>
<tr id="S3.T2.1.2" class="ltx_tr">
<td id="S3.T2.1.2.1" class="ltx_td ltx_align_left ltx_border_t">Origin TSAM</td>
<td id="S3.T2.1.2.2" class="ltx_td ltx_align_center ltx_border_t">RoBERTa-base</td>
<td id="S3.T2.1.2.3" class="ltx_td ltx_align_center ltx_border_t">74.3</td>
<td id="S3.T2.1.2.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T2.1.3" class="ltx_tr">
<td id="S3.T2.1.3.1" class="ltx_td ltx_align_left ltx_border_t">Ours-CEE</td>
<td id="S3.T2.1.3.2" class="ltx_td ltx_align_center ltx_border_t">base</td>
<td id="S3.T2.1.3.3" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.1.3.4" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T2.1.4" class="ltx_tr">
<td id="S3.T2.1.4.1" class="ltx_td ltx_align_left">+MIN</td>
<td id="S3.T2.1.4.2" class="ltx_td ltx_align_center">RoBERTa-base</td>
<td id="S3.T2.1.4.3" class="ltx_td ltx_align_center">75.5</td>
<td id="S3.T2.1.4.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T2.1.5" class="ltx_tr">
<td id="S3.T2.1.5.1" class="ltx_td ltx_align_left">+MIN &amp; MTLA</td>
<td id="S3.T2.1.5.2" class="ltx_td ltx_align_center">RoBERTa-base</td>
<td id="S3.T2.1.5.3" class="ltx_td ltx_align_center">75.9</td>
<td id="S3.T2.1.5.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T2.1.6" class="ltx_tr">
<td id="S3.T2.1.6.1" class="ltx_td ltx_align_left">+MIN &amp; MTLA</td>
<td id="S3.T2.1.6.2" class="ltx_td ltx_align_center">RoBERTa-large</td>
<td id="S3.T2.1.6.3" class="ltx_td ltx_align_center">76.9</td>
<td id="S3.T2.1.6.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T2.1.7" class="ltx_tr">
<td id="S3.T2.1.7.1" class="ltx_td ltx_align_left">+MIN &amp; MTLA &amp; Ensemble</td>
<td id="S3.T2.1.7.2" class="ltx_td ltx_align_center">RoBERTa-large</td>
<td id="S3.T2.1.7.3" class="ltx_td ltx_align_center"><span id="S3.T2.1.7.3.1" class="ltx_text ltx_font_bold">78.0</span></td>
<td id="S3.T2.1.7.4" class="ltx_td ltx_align_center"><span id="S3.T2.1.7.4.1" class="ltx_text ltx_font_bold">38.7</span></td>
</tr>
<tr id="S3.T2.1.8" class="ltx_tr">
<td id="S3.T2.1.8.1" class="ltx_td ltx_align_left ltx_border_t">Ours-CSE</td>
<td id="S3.T2.1.8.2" class="ltx_td ltx_align_center ltx_border_t">BERT-base</td>
<td id="S3.T2.1.8.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T2.1.8.4" class="ltx_td ltx_align_center ltx_border_t">31.62 (w-avg.)</td>
</tr>
<tr id="S3.T2.1.9" class="ltx_tr">
<td id="S3.T2.1.9.1" class="ltx_td ltx_align_left ltx_border_bb">Ours-CSE</td>
<td id="S3.T2.1.9.2" class="ltx_td ltx_align_center ltx_border_bb">RoBERTa-large</td>
<td id="S3.T2.1.9.3" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S3.T2.1.9.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.1.9.4.1" class="ltx_text ltx_font_bold">32.23 (w-avg.)</span></td>
</tr>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S3.I1" class="ltx_itemize ltx_centering ltx_figure_panel">
<li id="S3.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">*</span> 
<div id="S3.I1.ix1.p1" class="ltx_para">
<p id="S3.I1.ix1.p1.1" class="ltx_p">The results are based on ground truth emotion labels.</p>
</div>
</li>
<li id="S3.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">**</span> 
<div id="S3.I1.ix2.p1" class="ltx_para">
<p id="S3.I1.ix2.p1.1" class="ltx_p">The results are based on emotion labels given by ERC.</p>
</div>
</li>
</ul>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results of our models for the causal emotion entailment subtask.</figcaption>
</figure>
</section>
<section id="S3.SS5.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.4 </span>Video image to Face Embedding</h4>

<div id="S3.SS5.SSS4.p1" class="ltx_para">
<p id="S3.SS5.SSS4.p1.1" class="ltx_p">The faces in the video images contain rich emotion-related information, so pre-trained models are used to extract the face embeddings and correspond the identity of the face to the speaker in the text. The framework of the face module is shown in Figure  <a href="#S3.F4" title="Figure 4 ‣ 3.3 Emotion Cause Span Extraction ‣ 3 System Overview ‣ Samsung Research China-Beijing at SemEval-2024 Task 3: A multi-stage framework for Emotion-Cause Pair Extraction in Conversations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S3.SS5.SSS4.p2" class="ltx_para">
<p id="S3.SS5.SSS4.p2.1" class="ltx_p">Firstly, the Multi-Task Convolutional Neural Network (MTCNN)  <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib54" title="" class="ltx_ref">2016</a>)</cite> is used to detect the bounding boxes and key points of the faces. Next, the face images are affine transformed to a forward and intermediate state, and the faces are cropped and resized. The cropped images are then used for two subtasks: face matching and Face Emotion Recognition (FER). During face matching, two images of each protagonist are selected to build a matching database. With the help of MobileFaceNets  <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite>, the embeddings of the face images are extracted, and the identity of each face image is obtained by calculating its similarity with the embeddings of faces in the matching database. During FER, the emotion-related embedding of the face image corresponding to the speaker is extracted by VGG19  <cite class="ltx_cite ltx_citemacro_cite">Simonyan and Zisserman (<a href="#bib.bib39" title="" class="ltx_ref">2015</a>)</cite> for subsequent multimodal analysis. When the speaker is a supporting character that is not included in the matching database, the features of the face image with the largest area are selected. When no face is detected or the speaker cannot be matched, the output features are filled with 0.</p>
</div>
</section>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Model Ensemble</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">Ensembling models has been proven to be effective in boosting system performance across various tasks <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib53" title="" class="ltx_ref">2023b</a>)</cite>. For the extraction of causal pairs, we utilize various models for ensemble learning. We utilize a majority voting mechanism to determine the final prediction, aiming for optimal performance on the test dataset.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Training Data</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The split of dataset is same as SHARK <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib44" title="" class="ltx_ref">2023b</a>)</cite>. The ECF dataset is divided into training, validation and test sets, which incclude 9966, 1087, 2566 utterances.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Training Details</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">For ERC task, we use InstructERC with Llama-2-7B-chat and LLamMA2-13B-chat, which retain default parameters. We finetune ERC model by peft on single A100 with batch size 8. The length of historical window is 12.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">For both the causal emotion entailment subtask and the causal span extraction subtask, we adopt the default hyperparameter settings from the respective original papers. We found that conducting a hyper-parameter grid search did not yield any additional performance improvements.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discuss</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Emotion Recognition</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We use weight average F1 score and accuracy to evaluate the performance of the model. It should be noted that according to the rules of the competition, we removed the neutral utterances when computing F1 score and accuracy. The result of ERC on test set is shown in Table <a href="#S3.T1" title="Table 1 ‣ 3.5.1 Embedding and Concating Strategy ‣ 3.5 Infusion of Video and Audio Information ‣ 3 System Overview ‣ Samsung Research China-Beijing at SemEval-2024 Task 3: A multi-stage framework for Emotion-Cause Pair Extraction in Conversations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. All models is trained on four auxiliary tasks mentioned by in Section <a href="#S3.SS2.SSS3" title="3.2.3 Auxiliary Tasks and Instruct Design ‣ 3.2 Emotion Recognition in Conversations ‣ 3 System Overview ‣ Samsung Research China-Beijing at SemEval-2024 Task 3: A multi-stage framework for Emotion-Cause Pair Extraction in Conversations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.3</span></a>. The best weight average F1 score is 58.64, which is achieved by Llama-2-13B with historical clips descriptions. The descriptions which contains information with the emotions of speakers improve 0.79 (from 57.85 to 58.64). As for accuracy, the Llama-2-13B without video clips descriptions achieves the highest score of 61.45. Compared with InstructERC’s training data strategy, we have added additional auxiliary tasks and improve 12.52 on accuracy.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.8" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T3.8.9" class="ltx_tr">
<td id="S5.T3.8.9.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S5.T3.8.9.1.1" class="ltx_text ltx_font_bold">Modality</span></td>
<td id="S5.T3.8.9.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.8.9.2.1" class="ltx_text ltx_font_bold">Feature Set</span></td>
<td id="S5.T3.8.9.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.8.9.3.1" class="ltx_text ltx_font_bold">Feature Selection</span></td>
<td id="S5.T3.8.9.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.8.9.4.1" class="ltx_text ltx_font_bold">Feature Dimension</span></td>
<td id="S5.T3.8.9.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.8.9.5.1" class="ltx_text ltx_font_bold">Test Pos.F1</span></td>
</tr>
<tr id="S5.T3.1.1" class="ltx_tr">
<td id="S5.T3.1.1.2" class="ltx_td ltx_align_left ltx_border_t" rowspan="5"><span id="S5.T3.1.1.2.1" class="ltx_text ltx_font_bold">Audio</span></td>
<td id="S5.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_t">GeMAPS</td>
<td id="S5.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_t">
<div id="S5.T3.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.8pt;height:6.6pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p id="S5.T3.1.1.1.1.1" class="ltx_p"><math id="S5.T3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.1.1.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.1.m1.1b"><times id="S5.T3.1.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.1.m1.1c">\times</annotation></semantics></math></p>
</span></div>
</td>
<td id="S5.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_t">62</td>
<td id="S5.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_t">39.0</td>
</tr>
<tr id="S5.T3.2.2" class="ltx_tr">
<td id="S5.T3.2.2.2" class="ltx_td ltx_align_center">ComParE</td>
<td id="S5.T3.2.2.1" class="ltx_td ltx_align_center">
<div id="S5.T3.2.2.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.8pt;height:6.6pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p id="S5.T3.2.2.1.1.1" class="ltx_p"><math id="S5.T3.2.2.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.2.2.1.1.1.m1.1a"><mo id="S5.T3.2.2.1.1.1.m1.1.1" xref="S5.T3.2.2.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.1.1.1.m1.1b"><times id="S5.T3.2.2.1.1.1.m1.1.1.cmml" xref="S5.T3.2.2.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.1.1.1.m1.1c">\times</annotation></semantics></math></p>
</span></div>
</td>
<td id="S5.T3.2.2.3" class="ltx_td ltx_align_center">top 1000</td>
<td id="S5.T3.2.2.4" class="ltx_td ltx_align_center">62.4</td>
</tr>
<tr id="S5.T3.3.3" class="ltx_tr">
<td id="S5.T3.3.3.2" class="ltx_td ltx_align_center">ComParE</td>
<td id="S5.T3.3.3.1" class="ltx_td ltx_align_center"><span id="S5.T3.3.3.1.1" class="ltx_text" style="position:relative; bottom:2.6pt;">
<span id="S5.T3.3.3.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:0.0pt;height:0pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(0.7,0.7) ;">
<span id="S5.T3.3.3.1.1.1.1" class="ltx_p"><math id="S5.T3.3.3.1.1.1.1.m1.1" class="ltx_Math" alttext="\sqrt{}" display="inline"><semantics id="S5.T3.3.3.1.1.1.1.m1.1a"><msqrt id="S5.T3.3.3.1.1.1.1.m1.1.1" xref="S5.T3.3.3.1.1.1.1.m1.1.1.cmml"><mi id="S5.T3.3.3.1.1.1.1.m1.1.1.2" xref="S5.T3.3.3.1.1.1.1.m1.1.1.2.cmml"></mi></msqrt><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.1.1.1.1.m1.1b"><apply id="S5.T3.3.3.1.1.1.1.m1.1.1.cmml" xref="S5.T3.3.3.1.1.1.1.m1.1.1"><root id="S5.T3.3.3.1.1.1.1.m1.1.1a.cmml" xref="S5.T3.3.3.1.1.1.1.m1.1.1"></root><csymbol cd="latexml" id="S5.T3.3.3.1.1.1.1.m1.1.1.2.cmml" xref="S5.T3.3.3.1.1.1.1.m1.1.1.2">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.1.1.1.1.m1.1c">\sqrt{}</annotation></semantics></math></span>
</span></span></span></td>
<td id="S5.T3.3.3.3" class="ltx_td ltx_align_center">352</td>
<td id="S5.T3.3.3.4" class="ltx_td ltx_align_center">67.6</td>
</tr>
<tr id="S5.T3.4.4" class="ltx_tr">
<td id="S5.T3.4.4.2" class="ltx_td ltx_align_center">ComParE</td>
<td id="S5.T3.4.4.1" class="ltx_td ltx_align_center"><span id="S5.T3.4.4.1.1" class="ltx_text" style="position:relative; bottom:2.6pt;">
<span id="S5.T3.4.4.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:0.0pt;height:0pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(0.7,0.7) ;">
<span id="S5.T3.4.4.1.1.1.1" class="ltx_p"><math id="S5.T3.4.4.1.1.1.1.m1.1" class="ltx_Math" alttext="\sqrt{}" display="inline"><semantics id="S5.T3.4.4.1.1.1.1.m1.1a"><msqrt id="S5.T3.4.4.1.1.1.1.m1.1.1" xref="S5.T3.4.4.1.1.1.1.m1.1.1.cmml"><mi id="S5.T3.4.4.1.1.1.1.m1.1.1.2" xref="S5.T3.4.4.1.1.1.1.m1.1.1.2.cmml"></mi></msqrt><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.1.1.1.1.m1.1b"><apply id="S5.T3.4.4.1.1.1.1.m1.1.1.cmml" xref="S5.T3.4.4.1.1.1.1.m1.1.1"><root id="S5.T3.4.4.1.1.1.1.m1.1.1a.cmml" xref="S5.T3.4.4.1.1.1.1.m1.1.1"></root><csymbol cd="latexml" id="S5.T3.4.4.1.1.1.1.m1.1.1.2.cmml" xref="S5.T3.4.4.1.1.1.1.m1.1.1.2">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.1.1.1.1.m1.1c">\sqrt{}</annotation></semantics></math></span>
</span></span></span></td>
<td id="S5.T3.4.4.3" class="ltx_td ltx_align_center">296</td>
<td id="S5.T3.4.4.4" class="ltx_td ltx_align_center">70.5</td>
</tr>
<tr id="S5.T3.5.5" class="ltx_tr">
<td id="S5.T3.5.5.2" class="ltx_td ltx_align_center">ComParE</td>
<td id="S5.T3.5.5.1" class="ltx_td ltx_align_center"><span id="S5.T3.5.5.1.1" class="ltx_text" style="position:relative; bottom:2.6pt;">
<span id="S5.T3.5.5.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:0.0pt;height:0pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(0.7,0.7) ;">
<span id="S5.T3.5.5.1.1.1.1" class="ltx_p"><math id="S5.T3.5.5.1.1.1.1.m1.1" class="ltx_Math" alttext="\sqrt{}" display="inline"><semantics id="S5.T3.5.5.1.1.1.1.m1.1a"><msqrt id="S5.T3.5.5.1.1.1.1.m1.1.1" xref="S5.T3.5.5.1.1.1.1.m1.1.1.cmml"><mi id="S5.T3.5.5.1.1.1.1.m1.1.1.2" xref="S5.T3.5.5.1.1.1.1.m1.1.1.2.cmml"></mi></msqrt><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.1.1.1.1.m1.1b"><apply id="S5.T3.5.5.1.1.1.1.m1.1.1.cmml" xref="S5.T3.5.5.1.1.1.1.m1.1.1"><root id="S5.T3.5.5.1.1.1.1.m1.1.1a.cmml" xref="S5.T3.5.5.1.1.1.1.m1.1.1"></root><csymbol cd="latexml" id="S5.T3.5.5.1.1.1.1.m1.1.1.2.cmml" xref="S5.T3.5.5.1.1.1.1.m1.1.1.2">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.1.1.1.1.m1.1c">\sqrt{}</annotation></semantics></math></span>
</span></span></span></td>
<td id="S5.T3.5.5.3" class="ltx_td ltx_align_center">128</td>
<td id="S5.T3.5.5.4" class="ltx_td ltx_align_center"><span id="S5.T3.5.5.4.1" class="ltx_text ltx_font_bold">73.9</span></td>
</tr>
<tr id="S5.T3.6.6" class="ltx_tr">
<td id="S5.T3.6.6.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="3"><span id="S5.T3.6.6.2.1" class="ltx_text ltx_font_bold">Vision</span></td>
<td id="S5.T3.6.6.3" class="ltx_td ltx_align_center ltx_border_t">Max Img</td>
<td id="S5.T3.6.6.1" class="ltx_td ltx_align_center ltx_border_t">
<div id="S5.T3.6.6.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.8pt;height:6.6pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p id="S5.T3.6.6.1.1.1" class="ltx_p"><math id="S5.T3.6.6.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.6.6.1.1.1.m1.1a"><mo id="S5.T3.6.6.1.1.1.m1.1.1" xref="S5.T3.6.6.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.1.1.1.m1.1b"><times id="S5.T3.6.6.1.1.1.m1.1.1.cmml" xref="S5.T3.6.6.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.1.1.1.m1.1c">\times</annotation></semantics></math></p>
</span></div>
</td>
<td id="S5.T3.6.6.4" class="ltx_td ltx_align_center ltx_border_t">128</td>
<td id="S5.T3.6.6.5" class="ltx_td ltx_align_center ltx_border_t">70.7</td>
</tr>
<tr id="S5.T3.7.7" class="ltx_tr">
<td id="S5.T3.7.7.2" class="ltx_td ltx_align_center">Speaker Img</td>
<td id="S5.T3.7.7.1" class="ltx_td ltx_align_center">
<div id="S5.T3.7.7.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.8pt;height:6.6pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p id="S5.T3.7.7.1.1.1" class="ltx_p"><math id="S5.T3.7.7.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.7.7.1.1.1.m1.1a"><mo id="S5.T3.7.7.1.1.1.m1.1.1" xref="S5.T3.7.7.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T3.7.7.1.1.1.m1.1b"><times id="S5.T3.7.7.1.1.1.m1.1.1.cmml" xref="S5.T3.7.7.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.7.7.1.1.1.m1.1c">\times</annotation></semantics></math></p>
</span></div>
</td>
<td id="S5.T3.7.7.3" class="ltx_td ltx_align_center">128</td>
<td id="S5.T3.7.7.4" class="ltx_td ltx_align_center">74.3</td>
</tr>
<tr id="S5.T3.8.8" class="ltx_tr">
<td id="S5.T3.8.8.2" class="ltx_td ltx_align_center ltx_border_bb">Emotional Speaker Img</td>
<td id="S5.T3.8.8.1" class="ltx_td ltx_align_center ltx_border_bb">
<div id="S5.T3.8.8.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.8pt;height:6.6pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<p id="S5.T3.8.8.1.1.1" class="ltx_p"><math id="S5.T3.8.8.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T3.8.8.1.1.1.m1.1a"><mo id="S5.T3.8.8.1.1.1.m1.1.1" xref="S5.T3.8.8.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T3.8.8.1.1.1.m1.1b"><times id="S5.T3.8.8.1.1.1.m1.1.1.cmml" xref="S5.T3.8.8.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.8.8.1.1.1.m1.1c">\times</annotation></semantics></math></p>
</span></div>
</td>
<td id="S5.T3.8.8.3" class="ltx_td ltx_align_center ltx_border_bb">512</td>
<td id="S5.T3.8.8.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.8.8.4.1" class="ltx_text ltx_font_bold">74.8</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results of multi-modality experiments for the causal emotion entailment subtask.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Emotion Cause Span Extraction</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We utilize an end-to-end framework for cause span extraction and achieve a final performance of 32.23 in weighted average proportional F1 score on the official evaluation dataset as is shown in the Table  <a href="#S3.T2" title="Table 2 ‣ 3.5.3 Video Image to Text ‣ 3.5 Infusion of Video and Audio Information ‣ 3 System Overview ‣ Samsung Research China-Beijing at SemEval-2024 Task 3: A multi-stage framework for Emotion-Cause Pair Extraction in Conversations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Our result significantly surpasses the result of 26.40 above <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="\sim+6.0" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mrow id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml"></mi><mo id="S5.SS2.p1.1.m1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.cmml">∼</mo><mrow id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml"><mo id="S5.SS2.p1.1.m1.1.1.3a" xref="S5.SS2.p1.1.m1.1.1.3.cmml">+</mo><mn id="S5.SS2.p1.1.m1.1.1.3.2" xref="S5.SS2.p1.1.m1.1.1.3.2.cmml">6.0</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">absent</csymbol><apply id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3"><plus id="S5.SS2.p1.1.m1.1.1.3.1.cmml" xref="S5.SS2.p1.1.m1.1.1.3"></plus><cn type="float" id="S5.SS2.p1.1.m1.1.1.3.2.cmml" xref="S5.SS2.p1.1.m1.1.1.3.2">6.0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">\sim+6.0</annotation></semantics></math> achieved by the second-place participant. Furthermore, our results achieved the highest scores across all other official evaluation metrics, validating the effectiveness of our approach for subtask 1.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Causal Emotion Entailment</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.3" class="ltx_p">In our initial experiments focusing solely on text modality, we utilize the TSAM model as our baseline for the causal pair extraction subtask. As is shown in Table  <a href="#S3.T2" title="Table 2 ‣ 3.5.3 Video Image to Text ‣ 3.5 Infusion of Video and Audio Information ‣ 3 System Overview ‣ Samsung Research China-Beijing at SemEval-2024 Task 3: A multi-stage framework for Emotion-Cause Pair Extraction in Conversations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, After incorporating the MIN, our positive F1 score improves by <math id="S5.SS3.p1.1.m1.1" class="ltx_Math" alttext="+1.2" display="inline"><semantics id="S5.SS3.p1.1.m1.1a"><mrow id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mo id="S5.SS3.p1.1.m1.1.1a" xref="S5.SS3.p1.1.m1.1.1.cmml">+</mo><mn id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml">1.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><plus id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"></plus><cn type="float" id="S5.SS3.p1.1.m1.1.1.2.cmml" xref="S5.SS3.p1.1.m1.1.1.2">1.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">+1.2</annotation></semantics></math>. Furthermore, with the introduction of emotional multi-task learning as an auxiliary task, our result sees an additional improvement of <math id="S5.SS3.p1.2.m2.1" class="ltx_Math" alttext="+0.4" display="inline"><semantics id="S5.SS3.p1.2.m2.1a"><mrow id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml"><mo id="S5.SS3.p1.2.m2.1.1a" xref="S5.SS3.p1.2.m2.1.1.cmml">+</mo><mn id="S5.SS3.p1.2.m2.1.1.2" xref="S5.SS3.p1.2.m2.1.1.2.cmml">0.4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><apply id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"><plus id="S5.SS3.p1.2.m2.1.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"></plus><cn type="float" id="S5.SS3.p1.2.m2.1.1.2.cmml" xref="S5.SS3.p1.2.m2.1.1.2">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">+0.4</annotation></semantics></math>. Furthermore, we achieve an additional improvement of approximately <math id="S5.SS3.p1.3.m3.1" class="ltx_Math" alttext="\sim+1.1" display="inline"><semantics id="S5.SS3.p1.3.m3.1a"><mrow id="S5.SS3.p1.3.m3.1.1" xref="S5.SS3.p1.3.m3.1.1.cmml"><mi id="S5.SS3.p1.3.m3.1.1.2" xref="S5.SS3.p1.3.m3.1.1.2.cmml"></mi><mo id="S5.SS3.p1.3.m3.1.1.1" xref="S5.SS3.p1.3.m3.1.1.1.cmml">∼</mo><mrow id="S5.SS3.p1.3.m3.1.1.3" xref="S5.SS3.p1.3.m3.1.1.3.cmml"><mo id="S5.SS3.p1.3.m3.1.1.3a" xref="S5.SS3.p1.3.m3.1.1.3.cmml">+</mo><mn id="S5.SS3.p1.3.m3.1.1.3.2" xref="S5.SS3.p1.3.m3.1.1.3.2.cmml">1.1</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.3.m3.1b"><apply id="S5.SS3.p1.3.m3.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1"><csymbol cd="latexml" id="S5.SS3.p1.3.m3.1.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S5.SS3.p1.3.m3.1.1.2.cmml" xref="S5.SS3.p1.3.m3.1.1.2">absent</csymbol><apply id="S5.SS3.p1.3.m3.1.1.3.cmml" xref="S5.SS3.p1.3.m3.1.1.3"><plus id="S5.SS3.p1.3.m3.1.1.3.1.cmml" xref="S5.SS3.p1.3.m3.1.1.3"></plus><cn type="float" id="S5.SS3.p1.3.m3.1.1.3.2.cmml" xref="S5.SS3.p1.3.m3.1.1.3.2">1.1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.3.m3.1c">\sim+1.1</annotation></semantics></math> in the official final evaluation dataset through model ensembling.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">We also conduct experiments involving other modalities, including audio and vision, as is show in Table  <a href="#S5.T3" title="Table 3 ‣ 5.1 Emotion Recognition ‣ 5 Results and Discuss ‣ Samsung Research China-Beijing at SemEval-2024 Task 3: A multi-stage framework for Emotion-Cause Pair Extraction in Conversations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. For both audio and vision features, we concatenate them with the pure textual features. Regarding audio, we experiment with two public feature sets: GeMAPS and ComParE. The GeMAPS feature has a dimension of 62, while the ComParE feature has a dimension of 6373. For the ComParE features, we employ an L1-based logistic regression classifier for feature selection, and we find that the best performance is achieved with a feature selection dimension of 128, resulting in a performance of 73.9. For the vision modality, we achieve a performance of 74.8, which is comparable to the result of the audio modality. However, upon introducing either audio or visual modalities, we observe a decreasing trend compared to the pure textual modality. This observation inspires us to develop a more reasonable approach to incorporate multi-modality in conversation analysis.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we propose a joint pipeline framework for Subtask1 and Subtask2. Firstly, we utilize the Llama-2-based Instruct ERC model to extract the emotional content of utterances in a conversation. Next, we employ a two-stream attention model to identify causal pairs based on the predicted emotional states of the utterances. Lastly, we adopt an end-to-end framework using a multi-task learning approach to extract causal spans within a conversation. Our approach achieved first place in the competition, and the effectiveness of our approach is further confirmed by the ablation study. In future work, we plan to explore the integration of audio and visual modalities to enhance the performance of the task.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhat and Modi (2023)</span>
<span class="ltx_bibblock">
Ashwani Bhat and Ashutosh Modi. 2023.

</span>
<span class="ltx_bibblock">Multi-task learning framework for extracting emotion cause span and entailment in conversations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Transfer Learning for Natural Language Processing Workshop</em>, pages 33–51.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">C. et al. (2008)</span>
<span class="ltx_bibblock">
Busso C., Bulut M., and Lee et al. CC. 2008.

</span>
<span class="ltx_bibblock">IEMOCAP: Interactive emotional dyadic motion capture database.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Language resources and evaluation</em>, 42:335–359.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caridakis et al. (2007)</span>
<span class="ltx_bibblock">
George Caridakis, Ginevra Castellano, Loic Kessous, Amaryllis Raouzaiou, Lori Malatesta, Stelios Asteriadis, and Kostas Karpouzis. 2007.

</span>
<span class="ltx_bibblock">Multimodal emotion recognition from expressive faces, body gestures and speech.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence and Innovations 2007: from Theory to Applications</em>, pages 375–388, Boston, MA. Springer US.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2018)</span>
<span class="ltx_bibblock">
Sheng Chen, Yang Liu, Xiang Gao, and Zhen Han. 2018.

</span>
<span class="ltx_bibblock">Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Biometric Recognition: 13th Chinese Conference, CCBR 2018, Urumqi, China, August 11-12, 2018, Proceedings 13</em>, pages 428–438. Springer.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chudasama et al. (2022)</span>
<span class="ltx_bibblock">
Vishal Chudasama, Purbayan Kar, Ashish Gudmalwar, Nirmesh Shah, Pankaj Wasnik, and Naoyuki Onoe. 2022.

</span>
<span class="ltx_bibblock">M2fnet: Multi-modal fusion network for emotion recognition in conversation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</em>, pages 4652–4661.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. (2020)</span>
<span class="ltx_bibblock">
Zixiang Ding, Rui Xia, and Jianfei Yu. 2020.

</span>
<span class="ltx_bibblock">ECPE-2D: Emotion-cause pair extraction based on joint two-dimensional representation, interaction and prediction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">In Association for Computational Linguistics (ACL)</em>, page 3161–3170.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dozat and Manning (2016)</span>
<span class="ltx_bibblock">
Timothy Dozat and Christopher D Manning. 2016.

</span>
<span class="ltx_bibblock">Deep biaffine attention for neural dependency parsing.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1611.01734</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eyben et al. (2016)</span>
<span class="ltx_bibblock">
Florian Eyben, Klaus R. Scherer, Björn W. Schuller, Johan Sundberg, Elisabeth André, Carlos Busso, Laurence Y. Devillers, Julien Epps, Petri Laukka, Shrikanth S. Narayanan, and Khiet P. Truong. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TAFFC.2015.2457417" title="" class="ltx_ref ltx_href">The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em>, 7(2):190–202.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eyben et al. (2010)</span>
<span class="ltx_bibblock">
Florian Eyben, Martin Wöllmer, and Björn Schuller. 2010.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/1873951.1874246" title="" class="ltx_ref ltx_href">Opensmile: the munich versatile and fast open-source audio feature extractor</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 18th ACM International Conference on Multimedia</em>, MM ’10, page 1459–1462, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2020)</span>
<span class="ltx_bibblock">
Chuang Fan, Chaofa Yuan, Jiachen Du, Lin Gui, Min Yang, and Ruifeng Xu. 2020.

</span>
<span class="ltx_bibblock">Transition-based directed graph construction for emotion-cause pair extraction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">In Association for Computational Linguistics (ACL)</em>, page 3707–3717.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freudenthaler et al. (2022)</span>
<span class="ltx_bibblock">
Bernhard Freudenthaler, Jorge Martinez-Gil, Anna Fensel, Kai Höfig, Stefan Huber, and Dirk Jacob. 2022.

</span>
<span class="ltx_bibblock">KI-Net: Ai-based optimization in industrial manufacturing—a project overview.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">International Conference on Computer Aided Systems Theory</em>, pages 554–561. Springer.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghosal et al. (2020)</span>
<span class="ltx_bibblock">
Deepanway Ghosal, Navonil Majumder, Alexander Gelbukh, Rada Mihalcea, and Soujanya Poria. 2020.

</span>
<span class="ltx_bibblock">Cosmic: Commonsense knowledge for emotion identification in conversations.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.02795</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghosal et al. (2019)</span>
<span class="ltx_bibblock">
Deepanway Ghosal, Navonil Majumder, Soujanya Poria, Niyati Chhaya, and Alexander Gelbukh. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/1908.11540" title="" class="ltx_ref ltx_href">DialogueGCN: A graph convolutional neural network for emotion recognition in conversation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.11540</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hazarika et al. (2018)</span>
<span class="ltx_bibblock">
Devamanyu Hazarika, Soujanya Poria, Rada Mihalcea, Erik Cambria, and Roger Zimmermann. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D18-1280" title="" class="ltx_ref ltx_href">ICON: Interactive conversational memory network for multimodal emotion detection</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 2594–2604.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2023)</span>
<span class="ltx_bibblock">
Dou Hu, Yinan Bao, Lingwei Wei, Wei Zhou, and Songlin Hu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2306.01505" title="" class="ltx_ref ltx_href">Supervised adversarial contrastive learning for emotion recognition in conversations</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.01505</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2022)</span>
<span class="ltx_bibblock">
Guimin Hu, Ting-En Lin, Yi Zhao, Guangming Lu, Yuchuan Wu, and Yongbin Li. 2022.

</span>
<span class="ltx_bibblock">UniMSE: Towards unified multimodal sentiment analysis and emotion recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pages 7837–7851.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al. (2020)</span>
<span class="ltx_bibblock">
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. 2020.

</span>
<span class="ltx_bibblock">Spanbert: Improving pre-training by representing and predicting spans.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Transactions of the association for computational linguistics</em>, 8:64–77.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim and Vossen (2021)</span>
<span class="ltx_bibblock">
Taewoon Kim and Piek Vossen. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2108.12009" title="" class="ltx_ref ltx_href">EmoBERTa: Speaker-aware emotion recognition in conversation with roberta</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al. (2023a)</span>
<span class="ltx_bibblock">
Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, and Sirui Wang. 2023a.

</span>
<span class="ltx_bibblock">Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.11911</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al. (2023b)</span>
<span class="ltx_bibblock">
Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, and Sirui Wang. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.48550/arXiv.2309.11911" title="" class="ltx_ref ltx_href">InstructERC: Reforming emotion recognition in conversation with a retrieval multi-task llms framework</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>, arXiv:2309.11911.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al. (2023c)</span>
<span class="ltx_bibblock">
Shanglin Lei, Xiaoping Wang, Guanting Dong, Jiang Li, and Yingjian Liu. 2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICTAI59109.2023.00133" title="" class="ltx_ref ltx_href">Watch the speakers: A hybrid continuous attribution network for emotion recognition in conversation with emotion disentanglement</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI)</em>, pages 881–888.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
Jiang Li, Xiaoping Wang, Guoqing Lv, and Zhigang Zeng. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TMM.2023.3260635" title="" class="ltx_ref ltx_href">GraphCFC: A directed graph based cross-modal feature complementation approach for multimodal conversational emotion recognition</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Multimedia</em>, 26:77–89.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022)</span>
<span class="ltx_bibblock">
Jiangnan Li, Fandong Meng, Zheng Lin, Rui Liu, Peng Fu, Yanan Cao, Weiping Wang, and Jie Zhou. 2022.

</span>
<span class="ltx_bibblock">Neutral utterances are also causes: Enhancing conversational causal emotion entailment with social commonsense knowledge.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.00759</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020)</span>
<span class="ltx_bibblock">
Jingye Li, Meishan Zhang, Donghong Ji, and Yijiang Liu. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2003.01478" title="" class="ltx_ref ltx_href">Multi-task learning with auxiliary speaker identification for conversational emotion recognition</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.01478</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2019)</span>
<span class="ltx_bibblock">
Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu, and Jiwei Li. 2019.

</span>
<span class="ltx_bibblock">Dice loss for data-imbalanced nlp tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.02855</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2017)</span>
<span class="ltx_bibblock">
Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.48550/arXiv.1710.03957" title="" class="ltx_ref ltx_href">DailyDialog a manually labelled multi-turn dialogue dataset</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>, arXiv:1710.03957.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023a.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Xiao Liu, Jian Zhang, Heng Zhang, Fuzhao Xue, and Yang You. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2305.00262" title="" class="ltx_ref ltx_href">Hierarchical dialogue understanding with special tokens and turn-level attention</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.00262</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Majumder et al. (2019)</span>
<span class="ltx_bibblock">
N. Majumder, S. Poria, D. Hazarika, R. Mihalcea, A. Gelbukh, and E. Cambria. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1609/aaai.v33i01.33016818" title="" class="ltx_ref ltx_href">DialogueRNN: An attentive rnn for emotion detection in conversations</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, 33(01):6818–6825.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poria et al. (2017)</span>
<span class="ltx_bibblock">
Soujanya Poria, Erik Cambria, Devamanyu Hazarika, Navonil Majumder, Amir Zadeh, and Louis-Philippe Morency. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P17-1081" title="" class="ltx_ref ltx_href">Context-dependent sentiment analysis in user-generated videos</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 873–883.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poria et al. (2019)</span>
<span class="ltx_bibblock">
Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.48550/arXiv.1810.02508" title="" class="ltx_ref ltx_href">MELD: A multimodal multi-party dataset for emotion recognition in conversations</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>, arXiv:1810.02508.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poria et al. (2021)</span>
<span class="ltx_bibblock">
Soujanya Poria, Navonil Majumder, Devamanyu Hazarika, Deepanway Ghosal, Rishabh Bhardwaj, Samson Yu Bai Jian, Pengfei Hong, Romila Ghosh, Abhinaba Roy, Niyati Chhaya, et al. 2021.

</span>
<span class="ltx_bibblock">Recognizing emotion cause in conversations.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Cognitive Computation</em>, 13:1317–1332.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock">Sentence-bert: Sentence embeddings using siamese bert-networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.10084</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuller and Batliner (2013)</span>
<span class="ltx_bibblock">
Bjorn Schuller and Anton Batliner. 2013.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing</em>, 1st edition.

</span>
<span class="ltx_bibblock">Wiley Publishing.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2021a)</span>
<span class="ltx_bibblock">
Weizhou Shen, Junqing Chen, Xiaojun Quan, and Zhixian Xie. 2021a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1609/aaai.v35i15.17625" title="" class="ltx_ref ltx_href">DialogXL: All-in-one xlnet for multi-party conversation emotion recognition</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, 35(15):13789–13797.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2021b)</span>
<span class="ltx_bibblock">
Weizhou Shen, Siyue Wu, Yunyi Yang, and Xiaojun Quan. 2021b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2105.12907" title="" class="ltx_ref ltx_href">Directed acyclic graph network for conversational emotion recognition</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.12907</em>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and Zisserman (2015)</span>
<span class="ltx_bibblock">
Karen Simonyan and Andrew Zisserman. 2015.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2022)</span>
<span class="ltx_bibblock">
Xiaohui Song, Longtao Huang, Hui Xue, and Songlin Hu. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2210.08713" title="" class="ltx_ref ltx_href">Supervised prototypical contrastive learning for emotion recognition in conversation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.08713</em>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taichi et al. (2020)</span>
<span class="ltx_bibblock">
Ishiwatari Taichi, Yasuda Yuki, Miyazaki Taro, and Goto Jun. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.597" title="" class="ltx_ref ltx_href">Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 7360–7370. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023a)</span>
<span class="ltx_bibblock">
Fanfan Wang, Zixiang Ding, Rui Xia, Zhaoyu Li, and Jianfei Yu. 2023a.

</span>
<span class="ltx_bibblock">Multimodal emotion-cause pair extraction in conversations.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em>, 14(3):1832–1844.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024)</span>
<span class="ltx_bibblock">
Fanfan Wang, Heqing Ma, Rui Xia, Jianfei Yu, and Erik Cambria. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2024.semeval2024-1.273" title="" class="ltx_ref ltx_href">Semeval-2024 task 3: Multimodal emotion cause analysis in conversations</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)</em>, pages 2022–2033, Mexico City, Mexico. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023b)</span>
<span class="ltx_bibblock">
Fanfan Wang, Jianfei Yu, and Rui Xia. 2023b.

</span>
<span class="ltx_bibblock">Generative emotion cause triplet extraction in conversations with commonsense knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">In Findings of the Association for Computational Linguistics: EMNLP 2023</em>, page 3952–3963.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Yan Wang, Jiayu Zhang, Jun Ma, Shaojun Wang, and Jing Xiao. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.sigdial-1.23" title="" class="ltx_ref ltx_href">Contextualized emotion recognition in conversation as sequence tagging</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue</em>, pages 186–195.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:246411621" title="" class="ltx_ref ltx_href">Chain of thought prompting elicits reasoning in large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2201.11903.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2020)</span>
<span class="ltx_bibblock">
Penghui Wei, Jiahao Zhao, and Wenji Mao. 2020.

</span>
<span class="ltx_bibblock">Effective inter-clause modeling for end-to-end emotion-cause pair extraction.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">In Association for Computational Linguistics (ACL)</em>, page 3171–3181.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia and Ding (2019)</span>
<span class="ltx_bibblock">
Rui Xia and Zixiang Ding. 2019.

</span>
<span class="ltx_bibblock">Emotion-cause pair extraction: A new task to emotion analysis in texts.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 1003–1012.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zahiri and Choi (2017)</span>
<span class="ltx_bibblock">
Sayyed M. Zahiri and Jinho D. Choi. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.48550/arXiv.1708.04299" title="" class="ltx_ref ltx_href">Emotion detection on tv show transcripts with sequence-based convolutional neural networks</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>, arXiv:1708.04299.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2019)</span>
<span class="ltx_bibblock">
Dong Zhang, Liangqing Wu, Changlong Sun, and et.al. 2019.

</span>
<span class="ltx_bibblock">Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">IJCAI</em>, pages 5415–5421.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Duzhen Zhang, Zhen Yang, Fandong Meng, Xiuyi Chen, and Jie Zhou. 2022.

</span>
<span class="ltx_bibblock">Tsam: A two-stream attention model for causal emotion entailment.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.00819</em>.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023a)</span>
<span class="ltx_bibblock">
Hang Zhang, Xin Li, and Lidong Bing. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2306.02858" title="" class="ltx_ref ltx_href">Video-LLaMA: An instruction-tuned audio-visual language model for video understanding</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.02858</em>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023b)</span>
<span class="ltx_bibblock">
Haojie Zhang, Xiao Li, Renhua Gu, Xiaoyan Qu, Xiangfeng Meng, Shuo Hu, and Song Liu. 2023b.

</span>
<span class="ltx_bibblock">Samsung research china-beijing at semeval-2023 task 2: An al-r model for multilingual complex named entity recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)</em>, pages 114–120.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2016)</span>
<span class="ltx_bibblock">
Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. 2016.

</span>
<span class="ltx_bibblock">Joint face detection and alignment using multitask cascaded convolutional networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">IEEE signal processing letters</em>, 23(10):1499–1503.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023)</span>
<span class="ltx_bibblock">
Weixiang Zhao, Yanyan Zhao, Zhuojun Li, and Bing Qin. 2023.

</span>
<span class="ltx_bibblock">Knowledge-bridged causal interaction network for causal emotion entailment.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, 37(11):14020–14028.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al. (2019)</span>
<span class="ltx_bibblock">
Peixiang Zhong, Di Wang, and Chunyan Miao. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1016" title="" class="ltx_ref ltx_href">Knowledge-enriched transformer for emotion detection in textual conversations</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 165–176.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2021)</span>
<span class="ltx_bibblock">
Lixing Zhu, Gabriele Pergola, Lin Gui, Deyu Zhou, and Yulan He. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.acl-long.125" title="" class="ltx_ref ltx_href">Topic-driven and knowledge-aware transformer for dialogue emotion detection</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 1571–1582.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.16904" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.16905" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.16905">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.16905" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.16906" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 20:25:32 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
