<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.00898] Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs</title><meta property="og:description" content="In this paper, we analyse the error patterns of the raw waveform acoustic models in TIMIT's
phone recognition task. Our analysis goes beyond the conventional phone error rate (PER) metric. We categorise the phones into‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.00898">

<!--Generated on Fri Jul  5 17:56:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=1,2]ErfanLoweimi
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=3]AndreaCarmantini
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=3]PeterBell
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>[affiliation=3]SteveRenals
<span id="p1.3.4" class="ltx_ERROR undefined">\name</span>[affiliation=2]ZoranCvetkovic</p>
</div>
<h1 class="ltx_title ltx_title_document">Phonetic Error Analysis of Raw Waveform Acoustic Models
<br class="ltx_break">with Parametric and Non-Parametric CNNs</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.1" class="ltx_p">In this paper, we analyse the error patterns of the raw waveform acoustic models in TIMIT's
phone recognition task. Our analysis goes beyond the conventional phone error rate (PER) metric. We categorise the phones into three groups: {affricate, diphthong, fricative, nasal, plosive, semi-vowel, vowel, silence}, {consonant, vowel<sup id="id4.1.1" class="ltx_sup">+</sup>, silence}, and {voiced, unvoiced, silence} and, compute the PER for each broad phonetic class in each category.
We also construct a confusion matrix for each category using the substitution errors and compare the confusion patterns with those of the Filterbank and Wav2vec 2.0 systems.
Our raw waveform acoustic models consists of parametric (Sinc2Net) or non-parametric CNNs and Bidirectional LSTMs, achieving down to 13.7%/15.2% PERs on TIMIT Dev/Test sets, outperforming reported PERs for raw waveform models in the literature.
We also investigate the impact of transfer learning from WSJ on the phonetic error patterns and confusion matrices. It reduces the PER to 11.8%/13.7% on the Dev/Test sets.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Raw waveform modelling, phone recognition, phonetic error analysis, broad phonetic class, confusion matrix
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The conventional metric for evaluating phone recognition systems is phone error rate (PER), which measures the Levenshtein distance involving substitution, deletion, and insertion errors. However, PER lacks insight into the contribution of various broad phonetic classes (BPCs).
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, a detailed phonetic error analysis was carried out, computing the percentage of PER associated with each BPC. To this end, three phonetic categories were defined: {affricate, diphthong, fricative, nasal, plosive, semi-vowel, vowel, silence}, {consonant, vowel<sup id="S1.p1.1.1" class="ltx_sup">+</sup>, silence}, and {voiced, unvoiced, silence}. Then, the substitution, deletion, insertion and subsequently the PER for each BPC within these categorisations were computed. A confusion matrix for each category was also calculated and the confusion patterns were analysed and visualised for various types of models.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this paper, we perform phonetic error analysis using BPCs on the raw waveform acoustic models, in contrast to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> where the acoustic models are Mel Filterbank-based. Raw waveform models perform minimal processing, leaving the speech parametrisation to be learned jointly with the acoustic model, tailored for the given task. As such there is no task-blind and lossy feature engineering process which may inadvertently lead to task-relevant information loss. Further, compared with the MFCC or Filterbank features, raw waveform models have access to information encoded in the Fourier transform's phase spectrum, demonstrated to be useful in a wide range of applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, including speech reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, enhancement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, source-filter separation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, etc.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The key contributions of this paper are summarised below:</p>
</div>
<div id="S1.p4" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Development of raw waveform acoustic models with a cascade of parametric (Sinc2Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>) or non-parametric CNNs and recurrent layers, which achieve the highest performance on TIMIT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, compared to other raw waveform models.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Calculation of the PER for all broad phonetic classes within each phonetic categorisation for the raw waveform models.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Computation of a confusion matrix for each phonetic categorisation for the raw waveform models.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">Exploration of the impact of transfer learning from WSJ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> on the phonetic errors and confusion matrices.</p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i5.p1" class="ltx_para">
<p id="S1.I1.i5.p1.1" class="ltx_p">Comparative analysis of the PER per BPC and the confusion patterns of the raw waveform models with the state-of-the-art Wav2vec 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and Filterbank based systems.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Having reviewed the related work in Section¬†2, covering the raw waveform acoustic modeling and applications of the BPCs in speech processing, Section¬†3 describes the architecture of our raw waveform acoustic models. Section¬†4 details the three phonetic categorisations. Section¬†5 presents the experimental results as well as discussion and Section¬†6 concludes the paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Raw waveform Acoustic Modelling</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Palaz et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> investigated the usefulness of raw waveform models on the TIMIT phone recognition task and showed CNNs have superior performance over fully-connected networks. Tuske et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> compared raw waveform with traditional features in an LVCSR task. Sainath et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> deployed raw waveform modelling for joint acoustic modelling and beamforming in a multi-channel scenarios.
Ghahremani et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> used a TDNN architecture for raw waveform modeling and investigated the usefulness of i-vector for speaker adaptation.¬†Zhu et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and Von Platen et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> built multi-scale raw waveform models, to construct representations with high spectral and temporal resolutions.
Advantages of modelling speech in the waveform domain have been shown earlier also in the context of SVM and GMM-HMM approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The above cited works rely on conventional CNNs, which employ non-parametric FIR filters, while another line of research employs parameterised CNNs characterised by few parameters. SincNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, the first of this type of CNNs, have been applied for phone recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, speech recognition (both hybrid <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and end-to-end (E2E) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>) and speaker recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. The kernel of the SincNet's filters in the time domain, is a sinc function, leading to a filterbank with rectangular filters in the frequency domain. Each filter is characterised by two trainable parameters: centre frequency and bandwidth.
Loweimi et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> generalised this idea to modulated kernel-based CNNs and developed Sinc2Net, GammaNet and GaussNet where the filters in the frequency domain take triangular, Gammatone and Gaussian shapes, respectively.
Other examples of parametric CNNs include ParzNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and Complex Gabor CNN (CGCNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
Yue et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> applied parametric CNNs in Dysarthric speech recognition.
Fainberg et al <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> studied the speaker adaptation via retraining the Sinc layer parameters and showed this functionally resembles the VTLN. </p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Applications of BPCs</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The notion of broad phonetic classes, used in this work for phonetic error analysis, has had a wide range of applications.
In speaker verification and identification tasks, BPCs ‚Äìparticularly vowels and nasals‚Äì proved more informative than other broad phonetic classes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Lu et al¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> showed that using BPCs' posteriorgrams can improve speech quality and intelligibility in the speech enhancement task. BPCs have also been applied in language identification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and in speech coding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> by allocating different number of bits to speech frames belonging to different BPCs. They were also proven useful for speech emotion recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, particularly the vowel class. In addition, the BPCs were applied as a loss function in phone recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, and in automatic speech recognition (ASR) for decision tree-based state clustering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, guiding the decoding process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> and multilingual speech recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Architecture</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Fig.¬†<a href="#S3.F1" title="Figure 1 ‚Ä£ 3 Architecture ‚Ä£ Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> depicts the architecture we employed for raw waveform acoustic modeling, consisting of a cascade of parametric or non-parametric convolutional, Bidirectional Long Short-Term Memory (BLSTM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, and fully-connected (FC) layers. This design leverages complementary modeling capabilities of individual layers: CNN for feature extraction, BLSTM for context and sequential modelling and FC layer(s) for further abstraction extraction and improvement of linear separability of the classes, right before the softmax layer which essentially is a linear classifier. The output layer comprises two heads: one for context-dependent (CD) state-clustered triphones and another for context-independent (CI) monophones. The CD head plays the key role and the CI is utilised for regularisation purposes.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">We experimented with both parametric and non-parametric convolutional layers. For the former, we adopted Sinc2Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> whose kernel in the time domain is the Sinc-squared function</p>
</div>
<div id="S3.p3" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.7" class="ltx_Math" alttext="h^{(i)}(t)\,=\,\underbrace{\text{sinc}^{2}(B^{(i)}t)}_{\text{kernel}}\,\underbrace{\cos(2\pi f^{(i)}_{c}t)}_{\text{carrier}}" display="block"><semantics id="S3.E1.m1.7a"><mrow id="S3.E1.m1.7.8" xref="S3.E1.m1.7.8.cmml"><mrow id="S3.E1.m1.7.8.2" xref="S3.E1.m1.7.8.2.cmml"><msup id="S3.E1.m1.7.8.2.2" xref="S3.E1.m1.7.8.2.2.cmml"><mi id="S3.E1.m1.7.8.2.2.2" xref="S3.E1.m1.7.8.2.2.2.cmml">h</mi><mrow id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.7.8.2.2.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.3.1" xref="S3.E1.m1.7.8.2.2.cmml">(</mo><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S3.E1.m1.1.1.1.3.2" xref="S3.E1.m1.7.8.2.2.cmml">)</mo></mrow></msup><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.8.2.1" xref="S3.E1.m1.7.8.2.1.cmml">‚Äã</mo><mrow id="S3.E1.m1.7.8.2.3.2" xref="S3.E1.m1.7.8.2.cmml"><mo stretchy="false" id="S3.E1.m1.7.8.2.3.2.1" xref="S3.E1.m1.7.8.2.cmml">(</mo><mi id="S3.E1.m1.7.7" xref="S3.E1.m1.7.7.cmml">t</mi><mo rspace="0.170em" stretchy="false" id="S3.E1.m1.7.8.2.3.2.2" xref="S3.E1.m1.7.8.2.cmml">)</mo></mrow></mrow><mo rspace="0.448em" id="S3.E1.m1.7.8.1" xref="S3.E1.m1.7.8.1.cmml">=</mo><mrow id="S3.E1.m1.7.8.3" xref="S3.E1.m1.7.8.3.cmml"><munder id="S3.E1.m1.7.8.3.2" xref="S3.E1.m1.7.8.3.2.cmml"><munder accentunder="true" id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><mrow id="S3.E1.m1.3.3.2" xref="S3.E1.m1.3.3.2.cmml"><msup id="S3.E1.m1.3.3.2.4" xref="S3.E1.m1.3.3.2.4.cmml"><mtext id="S3.E1.m1.3.3.2.4.2" xref="S3.E1.m1.3.3.2.4.2a.cmml">sinc</mtext><mn id="S3.E1.m1.3.3.2.4.3" xref="S3.E1.m1.3.3.2.4.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.2.3" xref="S3.E1.m1.3.3.2.3.cmml">‚Äã</mo><mrow id="S3.E1.m1.3.3.2.2.1" xref="S3.E1.m1.3.3.2.2.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.2.2.1.2" xref="S3.E1.m1.3.3.2.2.1.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.2.2.1.1" xref="S3.E1.m1.3.3.2.2.1.1.cmml"><msup id="S3.E1.m1.3.3.2.2.1.1.2" xref="S3.E1.m1.3.3.2.2.1.1.2.cmml"><mi id="S3.E1.m1.3.3.2.2.1.1.2.2" xref="S3.E1.m1.3.3.2.2.1.1.2.2.cmml">B</mi><mrow id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.3.3.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.3.1" xref="S3.E1.m1.3.3.2.2.1.1.2.cmml">(</mo><mi id="S3.E1.m1.2.2.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.3.2" xref="S3.E1.m1.3.3.2.2.1.1.2.cmml">)</mo></mrow></msup><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.2.2.1.1.1" xref="S3.E1.m1.3.3.2.2.1.1.1.cmml">‚Äã</mo><mi id="S3.E1.m1.3.3.2.2.1.1.3" xref="S3.E1.m1.3.3.2.2.1.1.3.cmml">t</mi></mrow><mo stretchy="false" id="S3.E1.m1.3.3.2.2.1.3" xref="S3.E1.m1.3.3.2.2.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.3.cmml">‚èü</mo></munder><mtext id="S3.E1.m1.7.8.3.2.2" xref="S3.E1.m1.7.8.3.2.2a.cmml">kernel</mtext></munder><mo lspace="0.167em" rspace="0em" id="S3.E1.m1.7.8.3.1" xref="S3.E1.m1.7.8.3.1.cmml">‚Äã</mo><munder id="S3.E1.m1.7.8.3.3" xref="S3.E1.m1.7.8.3.3.cmml"><munder accentunder="true" id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6.cmml"><mrow id="S3.E1.m1.6.6.3.3" xref="S3.E1.m1.6.6.3.4.cmml"><mi id="S3.E1.m1.5.5.2.2" xref="S3.E1.m1.5.5.2.2.cmml">cos</mi><mo id="S3.E1.m1.6.6.3.3a" xref="S3.E1.m1.6.6.3.4.cmml">‚Å°</mo><mrow id="S3.E1.m1.6.6.3.3.1" xref="S3.E1.m1.6.6.3.4.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.3.3.1.2" xref="S3.E1.m1.6.6.3.4.cmml">(</mo><mrow id="S3.E1.m1.6.6.3.3.1.1" xref="S3.E1.m1.6.6.3.3.1.1.cmml"><mn id="S3.E1.m1.6.6.3.3.1.1.2" xref="S3.E1.m1.6.6.3.3.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.3.3.1.1.1" xref="S3.E1.m1.6.6.3.3.1.1.1.cmml">‚Äã</mo><mi id="S3.E1.m1.6.6.3.3.1.1.3" xref="S3.E1.m1.6.6.3.3.1.1.3.cmml">œÄ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.3.3.1.1.1a" xref="S3.E1.m1.6.6.3.3.1.1.1.cmml">‚Äã</mo><msubsup id="S3.E1.m1.6.6.3.3.1.1.4" xref="S3.E1.m1.6.6.3.3.1.1.4.cmml"><mi id="S3.E1.m1.6.6.3.3.1.1.4.2.2" xref="S3.E1.m1.6.6.3.3.1.1.4.2.2.cmml">f</mi><mi id="S3.E1.m1.6.6.3.3.1.1.4.3" xref="S3.E1.m1.6.6.3.3.1.1.4.3.cmml">c</mi><mrow id="S3.E1.m1.4.4.1.1.1.3" xref="S3.E1.m1.6.6.3.3.1.1.4.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.3.1" xref="S3.E1.m1.6.6.3.3.1.1.4.cmml">(</mo><mi id="S3.E1.m1.4.4.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.3.2" xref="S3.E1.m1.6.6.3.3.1.1.4.cmml">)</mo></mrow></msubsup><mo lspace="0em" rspace="0em" id="S3.E1.m1.6.6.3.3.1.1.1b" xref="S3.E1.m1.6.6.3.3.1.1.1.cmml">‚Äã</mo><mi id="S3.E1.m1.6.6.3.3.1.1.5" xref="S3.E1.m1.6.6.3.3.1.1.5.cmml">t</mi></mrow><mo stretchy="false" id="S3.E1.m1.6.6.3.3.1.3" xref="S3.E1.m1.6.6.3.4.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.6.6.4" xref="S3.E1.m1.6.6.4.cmml">‚èü</mo></munder><mtext id="S3.E1.m1.7.8.3.3.2" xref="S3.E1.m1.7.8.3.3.2a.cmml">carrier</mtext></munder></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.7b"><apply id="S3.E1.m1.7.8.cmml" xref="S3.E1.m1.7.8"><eq id="S3.E1.m1.7.8.1.cmml" xref="S3.E1.m1.7.8.1"></eq><apply id="S3.E1.m1.7.8.2.cmml" xref="S3.E1.m1.7.8.2"><times id="S3.E1.m1.7.8.2.1.cmml" xref="S3.E1.m1.7.8.2.1"></times><apply id="S3.E1.m1.7.8.2.2.cmml" xref="S3.E1.m1.7.8.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.7.8.2.2.1.cmml" xref="S3.E1.m1.7.8.2.2">superscript</csymbol><ci id="S3.E1.m1.7.8.2.2.2.cmml" xref="S3.E1.m1.7.8.2.2.2">‚Ñé</ci><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">ùëñ</ci></apply><ci id="S3.E1.m1.7.7.cmml" xref="S3.E1.m1.7.7">ùë°</ci></apply><apply id="S3.E1.m1.7.8.3.cmml" xref="S3.E1.m1.7.8.3"><times id="S3.E1.m1.7.8.3.1.cmml" xref="S3.E1.m1.7.8.3.1"></times><apply id="S3.E1.m1.7.8.3.2.cmml" xref="S3.E1.m1.7.8.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.7.8.3.2.1.cmml" xref="S3.E1.m1.7.8.3.2">subscript</csymbol><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><ci id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.3">‚èü</ci><apply id="S3.E1.m1.3.3.2.cmml" xref="S3.E1.m1.3.3.2"><times id="S3.E1.m1.3.3.2.3.cmml" xref="S3.E1.m1.3.3.2.3"></times><apply id="S3.E1.m1.3.3.2.4.cmml" xref="S3.E1.m1.3.3.2.4"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.2.4.1.cmml" xref="S3.E1.m1.3.3.2.4">superscript</csymbol><ci id="S3.E1.m1.3.3.2.4.2a.cmml" xref="S3.E1.m1.3.3.2.4.2"><mtext id="S3.E1.m1.3.3.2.4.2.cmml" xref="S3.E1.m1.3.3.2.4.2">sinc</mtext></ci><cn type="integer" id="S3.E1.m1.3.3.2.4.3.cmml" xref="S3.E1.m1.3.3.2.4.3">2</cn></apply><apply id="S3.E1.m1.3.3.2.2.1.1.cmml" xref="S3.E1.m1.3.3.2.2.1"><times id="S3.E1.m1.3.3.2.2.1.1.1.cmml" xref="S3.E1.m1.3.3.2.2.1.1.1"></times><apply id="S3.E1.m1.3.3.2.2.1.1.2.cmml" xref="S3.E1.m1.3.3.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.2.2.1.1.2.1.cmml" xref="S3.E1.m1.3.3.2.2.1.1.2">superscript</csymbol><ci id="S3.E1.m1.3.3.2.2.1.1.2.2.cmml" xref="S3.E1.m1.3.3.2.2.1.1.2.2">ùêµ</ci><ci id="S3.E1.m1.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1">ùëñ</ci></apply><ci id="S3.E1.m1.3.3.2.2.1.1.3.cmml" xref="S3.E1.m1.3.3.2.2.1.1.3">ùë°</ci></apply></apply></apply><ci id="S3.E1.m1.7.8.3.2.2a.cmml" xref="S3.E1.m1.7.8.3.2.2"><mtext mathsize="70%" id="S3.E1.m1.7.8.3.2.2.cmml" xref="S3.E1.m1.7.8.3.2.2">kernel</mtext></ci></apply><apply id="S3.E1.m1.7.8.3.3.cmml" xref="S3.E1.m1.7.8.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.7.8.3.3.1.cmml" xref="S3.E1.m1.7.8.3.3">subscript</csymbol><apply id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6"><ci id="S3.E1.m1.6.6.4.cmml" xref="S3.E1.m1.6.6.4">‚èü</ci><apply id="S3.E1.m1.6.6.3.4.cmml" xref="S3.E1.m1.6.6.3.3"><cos id="S3.E1.m1.5.5.2.2.cmml" xref="S3.E1.m1.5.5.2.2"></cos><apply id="S3.E1.m1.6.6.3.3.1.1.cmml" xref="S3.E1.m1.6.6.3.3.1.1"><times id="S3.E1.m1.6.6.3.3.1.1.1.cmml" xref="S3.E1.m1.6.6.3.3.1.1.1"></times><cn type="integer" id="S3.E1.m1.6.6.3.3.1.1.2.cmml" xref="S3.E1.m1.6.6.3.3.1.1.2">2</cn><ci id="S3.E1.m1.6.6.3.3.1.1.3.cmml" xref="S3.E1.m1.6.6.3.3.1.1.3">ùúã</ci><apply id="S3.E1.m1.6.6.3.3.1.1.4.cmml" xref="S3.E1.m1.6.6.3.3.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.3.3.1.1.4.1.cmml" xref="S3.E1.m1.6.6.3.3.1.1.4">subscript</csymbol><apply id="S3.E1.m1.6.6.3.3.1.1.4.2.cmml" xref="S3.E1.m1.6.6.3.3.1.1.4"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.3.3.1.1.4.2.1.cmml" xref="S3.E1.m1.6.6.3.3.1.1.4">superscript</csymbol><ci id="S3.E1.m1.6.6.3.3.1.1.4.2.2.cmml" xref="S3.E1.m1.6.6.3.3.1.1.4.2.2">ùëì</ci><ci id="S3.E1.m1.4.4.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1">ùëñ</ci></apply><ci id="S3.E1.m1.6.6.3.3.1.1.4.3.cmml" xref="S3.E1.m1.6.6.3.3.1.1.4.3">ùëê</ci></apply><ci id="S3.E1.m1.6.6.3.3.1.1.5.cmml" xref="S3.E1.m1.6.6.3.3.1.1.5">ùë°</ci></apply></apply></apply><ci id="S3.E1.m1.7.8.3.3.2a.cmml" xref="S3.E1.m1.7.8.3.3.2"><mtext mathsize="70%" id="S3.E1.m1.7.8.3.3.2.cmml" xref="S3.E1.m1.7.8.3.3.2">carrier</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.7c">h^{(i)}(t)\,=\,\underbrace{\text{sinc}^{2}(B^{(i)}t)}_{\text{kernel}}\,\underbrace{\cos(2\pi f^{(i)}_{c}t)}_{\text{carrier}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.4" class="ltx_p">where <math id="S3.p4.1.m1.2" class="ltx_Math" alttext="h^{(i)}(t)" display="inline"><semantics id="S3.p4.1.m1.2a"><mrow id="S3.p4.1.m1.2.3" xref="S3.p4.1.m1.2.3.cmml"><msup id="S3.p4.1.m1.2.3.2" xref="S3.p4.1.m1.2.3.2.cmml"><mi id="S3.p4.1.m1.2.3.2.2" xref="S3.p4.1.m1.2.3.2.2.cmml">h</mi><mrow id="S3.p4.1.m1.1.1.1.3" xref="S3.p4.1.m1.2.3.2.cmml"><mo stretchy="false" id="S3.p4.1.m1.1.1.1.3.1" xref="S3.p4.1.m1.2.3.2.cmml">(</mo><mi id="S3.p4.1.m1.1.1.1.1" xref="S3.p4.1.m1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S3.p4.1.m1.1.1.1.3.2" xref="S3.p4.1.m1.2.3.2.cmml">)</mo></mrow></msup><mo lspace="0em" rspace="0em" id="S3.p4.1.m1.2.3.1" xref="S3.p4.1.m1.2.3.1.cmml">‚Äã</mo><mrow id="S3.p4.1.m1.2.3.3.2" xref="S3.p4.1.m1.2.3.cmml"><mo stretchy="false" id="S3.p4.1.m1.2.3.3.2.1" xref="S3.p4.1.m1.2.3.cmml">(</mo><mi id="S3.p4.1.m1.2.2" xref="S3.p4.1.m1.2.2.cmml">t</mi><mo stretchy="false" id="S3.p4.1.m1.2.3.3.2.2" xref="S3.p4.1.m1.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.2b"><apply id="S3.p4.1.m1.2.3.cmml" xref="S3.p4.1.m1.2.3"><times id="S3.p4.1.m1.2.3.1.cmml" xref="S3.p4.1.m1.2.3.1"></times><apply id="S3.p4.1.m1.2.3.2.cmml" xref="S3.p4.1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.p4.1.m1.2.3.2.1.cmml" xref="S3.p4.1.m1.2.3.2">superscript</csymbol><ci id="S3.p4.1.m1.2.3.2.2.cmml" xref="S3.p4.1.m1.2.3.2.2">‚Ñé</ci><ci id="S3.p4.1.m1.1.1.1.1.cmml" xref="S3.p4.1.m1.1.1.1.1">ùëñ</ci></apply><ci id="S3.p4.1.m1.2.2.cmml" xref="S3.p4.1.m1.2.2">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.2c">h^{(i)}(t)</annotation></semantics></math>, <math id="S3.p4.2.m2.1" class="ltx_Math" alttext="B^{(i)}" display="inline"><semantics id="S3.p4.2.m2.1a"><msup id="S3.p4.2.m2.1.2" xref="S3.p4.2.m2.1.2.cmml"><mi id="S3.p4.2.m2.1.2.2" xref="S3.p4.2.m2.1.2.2.cmml">B</mi><mrow id="S3.p4.2.m2.1.1.1.3" xref="S3.p4.2.m2.1.2.cmml"><mo stretchy="false" id="S3.p4.2.m2.1.1.1.3.1" xref="S3.p4.2.m2.1.2.cmml">(</mo><mi id="S3.p4.2.m2.1.1.1.1" xref="S3.p4.2.m2.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S3.p4.2.m2.1.1.1.3.2" xref="S3.p4.2.m2.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><apply id="S3.p4.2.m2.1.2.cmml" xref="S3.p4.2.m2.1.2"><csymbol cd="ambiguous" id="S3.p4.2.m2.1.2.1.cmml" xref="S3.p4.2.m2.1.2">superscript</csymbol><ci id="S3.p4.2.m2.1.2.2.cmml" xref="S3.p4.2.m2.1.2.2">ùêµ</ci><ci id="S3.p4.2.m2.1.1.1.1.cmml" xref="S3.p4.2.m2.1.1.1.1">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">B^{(i)}</annotation></semantics></math> and <math id="S3.p4.3.m3.1" class="ltx_Math" alttext="f^{(i)}_{c}" display="inline"><semantics id="S3.p4.3.m3.1a"><msubsup id="S3.p4.3.m3.1.2" xref="S3.p4.3.m3.1.2.cmml"><mi id="S3.p4.3.m3.1.2.2.2" xref="S3.p4.3.m3.1.2.2.2.cmml">f</mi><mi id="S3.p4.3.m3.1.2.3" xref="S3.p4.3.m3.1.2.3.cmml">c</mi><mrow id="S3.p4.3.m3.1.1.1.3" xref="S3.p4.3.m3.1.2.cmml"><mo stretchy="false" id="S3.p4.3.m3.1.1.1.3.1" xref="S3.p4.3.m3.1.2.cmml">(</mo><mi id="S3.p4.3.m3.1.1.1.1" xref="S3.p4.3.m3.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S3.p4.3.m3.1.1.1.3.2" xref="S3.p4.3.m3.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.p4.3.m3.1b"><apply id="S3.p4.3.m3.1.2.cmml" xref="S3.p4.3.m3.1.2"><csymbol cd="ambiguous" id="S3.p4.3.m3.1.2.1.cmml" xref="S3.p4.3.m3.1.2">subscript</csymbol><apply id="S3.p4.3.m3.1.2.2.cmml" xref="S3.p4.3.m3.1.2"><csymbol cd="ambiguous" id="S3.p4.3.m3.1.2.2.1.cmml" xref="S3.p4.3.m3.1.2">superscript</csymbol><ci id="S3.p4.3.m3.1.2.2.2.cmml" xref="S3.p4.3.m3.1.2.2.2">ùëì</ci><ci id="S3.p4.3.m3.1.1.1.1.cmml" xref="S3.p4.3.m3.1.1.1.1">ùëñ</ci></apply><ci id="S3.p4.3.m3.1.2.3.cmml" xref="S3.p4.3.m3.1.2.3">ùëê</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.3.m3.1c">f^{(i)}_{c}</annotation></semantics></math> denote the impulse response, bandwidth and centre frequency of the <math id="S3.p4.4.m4.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S3.p4.4.m4.1a"><msup id="S3.p4.4.m4.1.1" xref="S3.p4.4.m4.1.1.cmml"><mi id="S3.p4.4.m4.1.1.2" xref="S3.p4.4.m4.1.1.2.cmml">i</mi><mrow id="S3.p4.4.m4.1.1.3" xref="S3.p4.4.m4.1.1.3.cmml"><mi id="S3.p4.4.m4.1.1.3.2" xref="S3.p4.4.m4.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.p4.4.m4.1.1.3.1" xref="S3.p4.4.m4.1.1.3.1.cmml">‚Äã</mo><mi id="S3.p4.4.m4.1.1.3.3" xref="S3.p4.4.m4.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p4.4.m4.1b"><apply id="S3.p4.4.m4.1.1.cmml" xref="S3.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p4.4.m4.1.1.1.cmml" xref="S3.p4.4.m4.1.1">superscript</csymbol><ci id="S3.p4.4.m4.1.1.2.cmml" xref="S3.p4.4.m4.1.1.2">ùëñ</ci><apply id="S3.p4.4.m4.1.1.3.cmml" xref="S3.p4.4.m4.1.1.3"><times id="S3.p4.4.m4.1.1.3.1.cmml" xref="S3.p4.4.m4.1.1.3.1"></times><ci id="S3.p4.4.m4.1.1.3.2.cmml" xref="S3.p4.4.m4.1.1.3.2">ùë°</ci><ci id="S3.p4.4.m4.1.1.3.3.cmml" xref="S3.p4.4.m4.1.1.3.3">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.4.m4.1c">i^{th}</annotation></semantics></math> filter, respectively.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">In the frequency domain, Sinc2Net acts as a filterbank with triangular filters centred around corresponding carrier frequencies, and is thus closely comparable with the triangular filters used in the Mel Filterbank¬†(FBank) features. The triangular filters ¬†are biologically more plausible than the rectangular filters in SincNet as they implicitly model the spectral masking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">Here, both CNN and FC sub-networks contain one layer. The FC layer includes 1024 nodes and the convolutional one consists of 128 kernels of length 129 with a max pooling of size 4. Dropout <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> and ReLU activation function are used in both convolutional and FC layers. The BLSTM layers contain 550 nodes in each direction along with dropout. Batch normalisation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> was used in both BLSTM and FC layers.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2406.00898/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Our raw waveform acoustic models consist of a cascade of (parametric or non-parametric) convolutional, BLSTM and fully-connected (FC) layers. The output layer composed of context-dependent (CD) and context-independent (CI) heads.</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Phonetic Categorisations</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We have used three phonetic categorisations, similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, specified in Table¬†<a href="#S5.T1" title="Table 1 ‚Ä£ 5 Experimental Results and Discussion ‚Ä£ Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Note that silence in all categorisations remains identical and encompasses non-speech segments at the beginning/end of utterances, epenthetic silence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, short pauses, and closures before the Plosives. Additionally, the Vowel<sup id="S4.p1.1.1" class="ltx_sup">+</sup> in the second category, represents the union of vowels and diphthongs, grouped together due to their similarity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.2" class="ltx_p">The sum of the PERs of all broad phonetic classes (<math id="S4.p2.1.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S4.p2.1.m1.1a"><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">ùëê</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">c</annotation></semantics></math>) within each category (<math id="S4.p2.2.m2.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S4.p2.2.m2.1a"><mi id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><ci id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">ùê∂</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">C</annotation></semantics></math>) equals the overall PER:</p>
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.1" class="ltx_Math" alttext="\text{PER}\ =\ \sum_{c\in C}\text{PER}_{c}\ \stackrel{{\scriptstyle\text{e.g.}}}{{=}}\ \text{PER}_{\text{voi}}+\text{PER}_{\text{sil}}+\text{PER}_{\text{unv}}" display="block"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><mtext id="S4.E2.m1.1.1.2" xref="S4.E2.m1.1.1.2a.cmml">PER</mtext><mo lspace="0.778em" rspace="0.611em" id="S4.E2.m1.1.1.3" xref="S4.E2.m1.1.1.3.cmml">=</mo><mrow id="S4.E2.m1.1.1.4" xref="S4.E2.m1.1.1.4.cmml"><munder id="S4.E2.m1.1.1.4.1" xref="S4.E2.m1.1.1.4.1.cmml"><mo movablelimits="false" id="S4.E2.m1.1.1.4.1.2" xref="S4.E2.m1.1.1.4.1.2.cmml">‚àë</mo><mrow id="S4.E2.m1.1.1.4.1.3" xref="S4.E2.m1.1.1.4.1.3.cmml"><mi id="S4.E2.m1.1.1.4.1.3.2" xref="S4.E2.m1.1.1.4.1.3.2.cmml">c</mi><mo id="S4.E2.m1.1.1.4.1.3.1" xref="S4.E2.m1.1.1.4.1.3.1.cmml">‚àà</mo><mi id="S4.E2.m1.1.1.4.1.3.3" xref="S4.E2.m1.1.1.4.1.3.3.cmml">C</mi></mrow></munder><msub id="S4.E2.m1.1.1.4.2" xref="S4.E2.m1.1.1.4.2.cmml"><mtext id="S4.E2.m1.1.1.4.2.2" xref="S4.E2.m1.1.1.4.2.2a.cmml">PER</mtext><mi id="S4.E2.m1.1.1.4.2.3" xref="S4.E2.m1.1.1.4.2.3.cmml">c</mi></msub></mrow><mover id="S4.E2.m1.1.1.5" xref="S4.E2.m1.1.1.5.cmml"><mo id="S4.E2.m1.1.1.5.2" xref="S4.E2.m1.1.1.5.2.cmml">=</mo><mtext id="S4.E2.m1.1.1.5.3" xref="S4.E2.m1.1.1.5.3a.cmml">e.g.</mtext></mover><mrow id="S4.E2.m1.1.1.6" xref="S4.E2.m1.1.1.6.cmml"><msub id="S4.E2.m1.1.1.6.2" xref="S4.E2.m1.1.1.6.2.cmml"><mtext id="S4.E2.m1.1.1.6.2.2" xref="S4.E2.m1.1.1.6.2.2a.cmml">PER</mtext><mtext id="S4.E2.m1.1.1.6.2.3" xref="S4.E2.m1.1.1.6.2.3a.cmml">voi</mtext></msub><mo id="S4.E2.m1.1.1.6.1" xref="S4.E2.m1.1.1.6.1.cmml">+</mo><msub id="S4.E2.m1.1.1.6.3" xref="S4.E2.m1.1.1.6.3.cmml"><mtext id="S4.E2.m1.1.1.6.3.2" xref="S4.E2.m1.1.1.6.3.2a.cmml">PER</mtext><mtext id="S4.E2.m1.1.1.6.3.3" xref="S4.E2.m1.1.1.6.3.3a.cmml">sil</mtext></msub><mo id="S4.E2.m1.1.1.6.1a" xref="S4.E2.m1.1.1.6.1.cmml">+</mo><msub id="S4.E2.m1.1.1.6.4" xref="S4.E2.m1.1.1.6.4.cmml"><mtext id="S4.E2.m1.1.1.6.4.2" xref="S4.E2.m1.1.1.6.4.2a.cmml">PER</mtext><mtext id="S4.E2.m1.1.1.6.4.3" xref="S4.E2.m1.1.1.6.4.3a.cmml">unv</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><and id="S4.E2.m1.1.1a.cmml" xref="S4.E2.m1.1.1"></and><apply id="S4.E2.m1.1.1b.cmml" xref="S4.E2.m1.1.1"><eq id="S4.E2.m1.1.1.3.cmml" xref="S4.E2.m1.1.1.3"></eq><ci id="S4.E2.m1.1.1.2a.cmml" xref="S4.E2.m1.1.1.2"><mtext id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1.2">PER</mtext></ci><apply id="S4.E2.m1.1.1.4.cmml" xref="S4.E2.m1.1.1.4"><apply id="S4.E2.m1.1.1.4.1.cmml" xref="S4.E2.m1.1.1.4.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.4.1.1.cmml" xref="S4.E2.m1.1.1.4.1">subscript</csymbol><sum id="S4.E2.m1.1.1.4.1.2.cmml" xref="S4.E2.m1.1.1.4.1.2"></sum><apply id="S4.E2.m1.1.1.4.1.3.cmml" xref="S4.E2.m1.1.1.4.1.3"><in id="S4.E2.m1.1.1.4.1.3.1.cmml" xref="S4.E2.m1.1.1.4.1.3.1"></in><ci id="S4.E2.m1.1.1.4.1.3.2.cmml" xref="S4.E2.m1.1.1.4.1.3.2">ùëê</ci><ci id="S4.E2.m1.1.1.4.1.3.3.cmml" xref="S4.E2.m1.1.1.4.1.3.3">ùê∂</ci></apply></apply><apply id="S4.E2.m1.1.1.4.2.cmml" xref="S4.E2.m1.1.1.4.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.4.2.1.cmml" xref="S4.E2.m1.1.1.4.2">subscript</csymbol><ci id="S4.E2.m1.1.1.4.2.2a.cmml" xref="S4.E2.m1.1.1.4.2.2"><mtext id="S4.E2.m1.1.1.4.2.2.cmml" xref="S4.E2.m1.1.1.4.2.2">PER</mtext></ci><ci id="S4.E2.m1.1.1.4.2.3.cmml" xref="S4.E2.m1.1.1.4.2.3">ùëê</ci></apply></apply></apply><apply id="S4.E2.m1.1.1c.cmml" xref="S4.E2.m1.1.1"><apply id="S4.E2.m1.1.1.5.cmml" xref="S4.E2.m1.1.1.5"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.5.1.cmml" xref="S4.E2.m1.1.1.5">superscript</csymbol><eq id="S4.E2.m1.1.1.5.2.cmml" xref="S4.E2.m1.1.1.5.2"></eq><ci id="S4.E2.m1.1.1.5.3a.cmml" xref="S4.E2.m1.1.1.5.3"><mtext mathsize="70%" id="S4.E2.m1.1.1.5.3.cmml" xref="S4.E2.m1.1.1.5.3">e.g.</mtext></ci></apply><share href="#S4.E2.m1.1.1.4.cmml" id="S4.E2.m1.1.1d.cmml" xref="S4.E2.m1.1.1"></share><apply id="S4.E2.m1.1.1.6.cmml" xref="S4.E2.m1.1.1.6"><plus id="S4.E2.m1.1.1.6.1.cmml" xref="S4.E2.m1.1.1.6.1"></plus><apply id="S4.E2.m1.1.1.6.2.cmml" xref="S4.E2.m1.1.1.6.2"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.6.2.1.cmml" xref="S4.E2.m1.1.1.6.2">subscript</csymbol><ci id="S4.E2.m1.1.1.6.2.2a.cmml" xref="S4.E2.m1.1.1.6.2.2"><mtext id="S4.E2.m1.1.1.6.2.2.cmml" xref="S4.E2.m1.1.1.6.2.2">PER</mtext></ci><ci id="S4.E2.m1.1.1.6.2.3a.cmml" xref="S4.E2.m1.1.1.6.2.3"><mtext mathsize="70%" id="S4.E2.m1.1.1.6.2.3.cmml" xref="S4.E2.m1.1.1.6.2.3">voi</mtext></ci></apply><apply id="S4.E2.m1.1.1.6.3.cmml" xref="S4.E2.m1.1.1.6.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.6.3.1.cmml" xref="S4.E2.m1.1.1.6.3">subscript</csymbol><ci id="S4.E2.m1.1.1.6.3.2a.cmml" xref="S4.E2.m1.1.1.6.3.2"><mtext id="S4.E2.m1.1.1.6.3.2.cmml" xref="S4.E2.m1.1.1.6.3.2">PER</mtext></ci><ci id="S4.E2.m1.1.1.6.3.3a.cmml" xref="S4.E2.m1.1.1.6.3.3"><mtext mathsize="70%" id="S4.E2.m1.1.1.6.3.3.cmml" xref="S4.E2.m1.1.1.6.3.3">sil</mtext></ci></apply><apply id="S4.E2.m1.1.1.6.4.cmml" xref="S4.E2.m1.1.1.6.4"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.6.4.1.cmml" xref="S4.E2.m1.1.1.6.4">subscript</csymbol><ci id="S4.E2.m1.1.1.6.4.2a.cmml" xref="S4.E2.m1.1.1.6.4.2"><mtext id="S4.E2.m1.1.1.6.4.2.cmml" xref="S4.E2.m1.1.1.6.4.2">PER</mtext></ci><ci id="S4.E2.m1.1.1.6.4.3a.cmml" xref="S4.E2.m1.1.1.6.4.3"><mtext mathsize="70%" id="S4.E2.m1.1.1.6.4.3.cmml" xref="S4.E2.m1.1.1.6.4.3">unv</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">\text{PER}\ =\ \sum_{c\in C}\text{PER}_{c}\ \stackrel{{\scriptstyle\text{e.g.}}}{{=}}\ \text{PER}_{\text{voi}}+\text{PER}_{\text{sil}}+\text{PER}_{\text{unv}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S4.p2.3" class="ltx_p">For example, the overall PER equals the sum of PERs of the Voiced, Silence and Unvoiced BPCs.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Results and Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Models were trained using the PyTorch-Kaldi toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> with the cross entropy loss and batch size of 8. The CD and CI output heads consist of 1936 and 48 nodes, respectively. The FBank features are 83-D: 80 filters plus three pitch-related features. For the transfer learning from WSJ, systems were initially trained on WSJ, and then only the weights between the penultimate and output layers were trained from scratch on TIMIT.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Mapping of phones to BPCs in three categorisations.</figcaption>
<table id="S5.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.2.3.1" class="ltx_tr">
<th id="S5.T1.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">classes</th>
<th id="S5.T1.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">phones</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.2.4.1" class="ltx_tr">
<th id="S5.T1.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">Affricates (aff)</th>
<td id="S5.T1.2.4.1.2" class="ltx_td ltx_align_center ltx_border_tt">ch jh</td>
</tr>
<tr id="S5.T1.2.5.2" class="ltx_tr">
<th id="S5.T1.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Diphthongs (dip)</th>
<td id="S5.T1.2.5.2.2" class="ltx_td ltx_align_center">aw ay ey ow oy</td>
</tr>
<tr id="S5.T1.2.6.3" class="ltx_tr">
<th id="S5.T1.2.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Fricatives (fri)</th>
<td id="S5.T1.2.6.3.2" class="ltx_td ltx_align_center">dh f s sh th v z</td>
</tr>
<tr id="S5.T1.2.7.4" class="ltx_tr">
<th id="S5.T1.2.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Nasal (nas)</th>
<td id="S5.T1.2.7.4.2" class="ltx_td ltx_align_center">m n ng</td>
</tr>
<tr id="S5.T1.2.8.5" class="ltx_tr">
<th id="S5.T1.2.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Plosive (plo)</th>
<td id="S5.T1.2.8.5.2" class="ltx_td ltx_align_center">b d dx g k p t</td>
</tr>
<tr id="S5.T1.2.9.6" class="ltx_tr">
<th id="S5.T1.2.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Semi-vowel (sem)</th>
<td id="S5.T1.2.9.6.2" class="ltx_td ltx_align_center">hh l r w y</td>
</tr>
<tr id="S5.T1.2.10.7" class="ltx_tr">
<th id="S5.T1.2.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Silence (sil)</th>
<td id="S5.T1.2.10.7.2" class="ltx_td ltx_align_center">sil</td>
</tr>
<tr id="S5.T1.2.11.8" class="ltx_tr">
<th id="S5.T1.2.11.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Vowel (vow)</th>
<td id="S5.T1.2.11.8.2" class="ltx_td ltx_align_center">aa ae ah eh er ih iy uh uw</td>
</tr>
<tr id="S5.T1.2.12.9" class="ltx_tr">
<th id="S5.T1.2.12.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Consonant (con)</th>
<td id="S5.T1.2.12.9.2" class="ltx_td ltx_align_center ltx_border_t">
<table id="S5.T1.2.12.9.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T1.2.12.9.2.1.1" class="ltx_tr">
<td id="S5.T1.2.12.9.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">b ch d dh dx f g hh jh k l m n ng p r s sh</td>
</tr>
<tr id="S5.T1.2.12.9.2.1.2" class="ltx_tr">
<td id="S5.T1.2.12.9.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">t th v w y z</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T1.2.13.10" class="ltx_tr">
<th id="S5.T1.2.13.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Silence (sil)</th>
<td id="S5.T1.2.13.10.2" class="ltx_td ltx_align_center">sil</td>
</tr>
<tr id="S5.T1.2.2" class="ltx_tr">
<th id="S5.T1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Vowel<sup id="S5.T1.2.2.2.1" class="ltx_sup">+</sup> (vow<sup id="S5.T1.2.2.2.2" class="ltx_sup">+</sup>)</th>
<td id="S5.T1.2.2.3" class="ltx_td ltx_align_center">aw ay ey ow oy aa ae ah eh er ih iy uh uw</td>
</tr>
<tr id="S5.T1.2.14.11" class="ltx_tr">
<th id="S5.T1.2.14.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Voiced (voi)</th>
<td id="S5.T1.2.14.11.2" class="ltx_td ltx_align_center ltx_border_t">
<table id="S5.T1.2.14.11.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T1.2.14.11.2.1.1" class="ltx_tr">
<td id="S5.T1.2.14.11.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">aa ae ah aw ay b d dh dx eh er ey g hh</td>
</tr>
<tr id="S5.T1.2.14.11.2.1.2" class="ltx_tr">
<td id="S5.T1.2.14.11.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">ih iy jh l m n ng ow oy r uh uw v w y z</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T1.2.15.12" class="ltx_tr">
<th id="S5.T1.2.15.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Silence (sil)</th>
<td id="S5.T1.2.15.12.2" class="ltx_td ltx_align_center">sil</td>
</tr>
<tr id="S5.T1.2.16.13" class="ltx_tr">
<th id="S5.T1.2.16.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Unvoiced (unv)</th>
<td id="S5.T1.2.16.13.2" class="ltx_td ltx_align_center ltx_border_bb">ch f k p s sh t th</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>PERs of various phone recognition systems on TIMIT.</figcaption>
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Feature</th>
<th id="S5.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Architecture</th>
<th id="S5.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Dev</th>
<th id="S5.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Test</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.2.1" class="ltx_tr">
<th id="S5.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">FBank-83 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</th>
<th id="S5.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt">Best System in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</th>
<td id="S5.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">12.8</td>
<td id="S5.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">14.1</td>
</tr>
<tr id="S5.T2.1.3.2" class="ltx_tr">
<th id="S5.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">FBank-83-WSJ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</th>
<th id="S5.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Best System in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</th>
<td id="S5.T2.1.3.2.3" class="ltx_td ltx_align_center">11.5</td>
<td id="S5.T2.1.3.2.4" class="ltx_td ltx_align_center">13.1</td>
</tr>
<tr id="S5.T2.1.4.3" class="ltx_tr">
<th id="S5.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Raw-Wav <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</th>
<th id="S5.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">CNN</th>
<td id="S5.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t">21.9</td>
</tr>
<tr id="S5.T2.1.5.4" class="ltx_tr">
<th id="S5.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Raw-Wav (E2E) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>
</th>
<th id="S5.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">CNN</th>
<td id="S5.T2.1.5.4.3" class="ltx_td ltx_align_center">18.9</td>
<td id="S5.T2.1.5.4.4" class="ltx_td ltx_align_center">21.1</td>
</tr>
<tr id="S5.T2.1.6.5" class="ltx_tr">
<th id="S5.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Raw-Wav (E2E) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>
</th>
<th id="S5.T2.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">SincNet</th>
<td id="S5.T2.1.6.5.3" class="ltx_td ltx_align_center">17.3</td>
<td id="S5.T2.1.6.5.4" class="ltx_td ltx_align_center">19.3</td>
</tr>
<tr id="S5.T2.1.7.6" class="ltx_tr">
<th id="S5.T2.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Raw-Wav <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</th>
<th id="S5.T2.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">CNN</th>
<td id="S5.T2.1.7.6.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.7.6.4" class="ltx_td ltx_align_center">18.1</td>
</tr>
<tr id="S5.T2.1.8.7" class="ltx_tr">
<th id="S5.T2.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Raw-Wav <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>
</th>
<th id="S5.T2.1.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">SincNet</th>
<td id="S5.T2.1.8.7.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.8.7.4" class="ltx_td ltx_align_center">17.2</td>
</tr>
<tr id="S5.T2.1.9.8" class="ltx_tr">
<th id="S5.T2.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Raw-Wav <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</th>
<th id="S5.T2.1.9.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">GammaNet</th>
<td id="S5.T2.1.9.8.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.9.8.4" class="ltx_td ltx_align_center">17.2</td>
</tr>
<tr id="S5.T2.1.10.9" class="ltx_tr">
<th id="S5.T2.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Raw-Wav <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</th>
<th id="S5.T2.1.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">CGCNN</th>
<td id="S5.T2.1.10.9.3" class="ltx_td ltx_align_center">15.2</td>
<td id="S5.T2.1.10.9.4" class="ltx_td ltx_align_center">17.1</td>
</tr>
<tr id="S5.T2.1.11.10" class="ltx_tr">
<th id="S5.T2.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Raw-Wav <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</th>
<th id="S5.T2.1.11.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">GaussNet</th>
<td id="S5.T2.1.11.10.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.11.10.4" class="ltx_td ltx_align_center">17.0</td>
</tr>
<tr id="S5.T2.1.12.11" class="ltx_tr">
<th id="S5.T2.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Raw-Wav <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</th>
<th id="S5.T2.1.12.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Sinc2Net</th>
<td id="S5.T2.1.12.11.3" class="ltx_td ltx_align_center">-</td>
<td id="S5.T2.1.12.11.4" class="ltx_td ltx_align_center">16.9</td>
</tr>
<tr id="S5.T2.1.13.12" class="ltx_tr">
<th id="S5.T2.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Raw-Wav <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</th>
<th id="S5.T2.1.13.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ParzNet</th>
<td id="S5.T2.1.13.12.3" class="ltx_td ltx_align_center">15.0</td>
<td id="S5.T2.1.13.12.4" class="ltx_td ltx_align_center">16.5</td>
</tr>
<tr id="S5.T2.1.14.13" class="ltx_tr">
<th id="S5.T2.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Raw-Wav <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</th>
<th id="S5.T2.1.14.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">CNN</th>
<td id="S5.T2.1.14.13.3" class="ltx_td ltx_align_center">14.9</td>
<td id="S5.T2.1.14.13.4" class="ltx_td ltx_align_center">16.5</td>
</tr>
<tr id="S5.T2.1.15.14" class="ltx_tr">
<th id="S5.T2.1.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Raw-Wav</th>
<th id="S5.T2.1.15.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Sinc2Net</th>
<td id="S5.T2.1.15.14.3" class="ltx_td ltx_align_center ltx_border_t">13.7</td>
<td id="S5.T2.1.15.14.4" class="ltx_td ltx_align_center ltx_border_t">15.5</td>
</tr>
<tr id="S5.T2.1.16.15" class="ltx_tr">
<th id="S5.T2.1.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Raw-Wav</th>
<th id="S5.T2.1.16.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">CNN</th>
<td id="S5.T2.1.16.15.3" class="ltx_td ltx_align_center">14.1</td>
<td id="S5.T2.1.16.15.4" class="ltx_td ltx_align_center">15.2</td>
</tr>
<tr id="S5.T2.1.17.16" class="ltx_tr">
<th id="S5.T2.1.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Raw-Wav-WSJ</th>
<th id="S5.T2.1.17.16.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">CNN</th>
<td id="S5.T2.1.17.16.3" class="ltx_td ltx_align_center ltx_border_bb">11.8</td>
<td id="S5.T2.1.17.16.4" class="ltx_td ltx_align_center ltx_border_bb">13.7</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Table¬†<a href="#S5.T2" title="Table 2 ‚Ä£ 5 Experimental Results and Discussion ‚Ä£ Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the PER on TIMIT's Dev and Test sets, comparing the performance of various raw waveform systems reported in the literature. As seen, the performance of the proposed raw waveform models, with both parametric and non-parametric CNNs, are close to each other and outperform other models with a notable margin. This performance gain is primarily due to the effective combination of the CNNs and BLSTMs.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Figs.¬†<a href="#S5.F2" title="Figure 2 ‚Ä£ 5 Experimental Results and Discussion ‚Ä£ Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S5.F3" title="Figure 3 ‚Ä£ 5 Experimental Results and Discussion ‚Ä£ Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> depict the breakdown of PER over the Filterbank and raw waveform models. Despite differences, the overall trends in phonetic error distribution remain consistent across these systems. For example, the largest errors always belong to the vowel class, due to being highly sensitive to the speaker attributes such as ID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and emotion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.
The consistent trends observed across various front-end and back-end configurations imply that the fundamental challenge of class confusions transcends the specific choices of these components.</p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2406.00898/assets/x2.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="343" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Recognition errors for FBank and raw waveform models. (a) PER, (b) Sub, (C) Del, (e) Ins.</figcaption>
</figure>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2406.00898/assets/x3.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="172" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>PER for FBank and raw waveform models. (a) Consonant/Vowel<sup id="S5.F3.4.1" class="ltx_sup">+</sup>/Silence, (b) Voiced/Unvoiced/Silence.</figcaption>
</figure>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2406.00898/assets/x4.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="441" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Confusion matrices of three phonetic categorisations for Sinc2Net on TIMIT's Dev set. The <span id="S5.F4.3.1" class="ltx_text ltx_font_bold">bold</span> and <span id="S5.F4.4.2" class="ltx_text ltx_framed ltx_framed_underline">underlined</span> numbers denote the first and second mostly confused classes.</figcaption>
</figure>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.3" class="ltx_p">Figs.¬†<a href="#S5.F4" title="Figure 4 ‚Ä£ 5 Experimental Results and Discussion ‚Ä£ Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S5.F5" title="Figure 5 ‚Ä£ 5 Experimental Results and Discussion ‚Ä£ Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> present the confusion matrices on the three phonetic categorisations for the raw waveform models, without and with transfer learning from WSJ, respectively. The confusions are computed using the substitution errors: the <math id="S5.p4.1.m1.2" class="ltx_Math" alttext="[i,j]" display="inline"><semantics id="S5.p4.1.m1.2a"><mrow id="S5.p4.1.m1.2.3.2" xref="S5.p4.1.m1.2.3.1.cmml"><mo stretchy="false" id="S5.p4.1.m1.2.3.2.1" xref="S5.p4.1.m1.2.3.1.cmml">[</mo><mi id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml">i</mi><mo id="S5.p4.1.m1.2.3.2.2" xref="S5.p4.1.m1.2.3.1.cmml">,</mo><mi id="S5.p4.1.m1.2.2" xref="S5.p4.1.m1.2.2.cmml">j</mi><mo stretchy="false" id="S5.p4.1.m1.2.3.2.3" xref="S5.p4.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.2b"><interval closure="closed" id="S5.p4.1.m1.2.3.1.cmml" xref="S5.p4.1.m1.2.3.2"><ci id="S5.p4.1.m1.1.1.cmml" xref="S5.p4.1.m1.1.1">ùëñ</ci><ci id="S5.p4.1.m1.2.2.cmml" xref="S5.p4.1.m1.2.2">ùëó</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.2c">[i,j]</annotation></semantics></math> entry of each confusion matrix reflects the number of times the phones belonging to the BPC of the <math id="S5.p4.2.m2.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S5.p4.2.m2.1a"><msup id="S5.p4.2.m2.1.1" xref="S5.p4.2.m2.1.1.cmml"><mi id="S5.p4.2.m2.1.1.2" xref="S5.p4.2.m2.1.1.2.cmml">i</mi><mrow id="S5.p4.2.m2.1.1.3" xref="S5.p4.2.m2.1.1.3.cmml"><mi id="S5.p4.2.m2.1.1.3.2" xref="S5.p4.2.m2.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.p4.2.m2.1.1.3.1" xref="S5.p4.2.m2.1.1.3.1.cmml">‚Äã</mo><mi id="S5.p4.2.m2.1.1.3.3" xref="S5.p4.2.m2.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.p4.2.m2.1b"><apply id="S5.p4.2.m2.1.1.cmml" xref="S5.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S5.p4.2.m2.1.1.1.cmml" xref="S5.p4.2.m2.1.1">superscript</csymbol><ci id="S5.p4.2.m2.1.1.2.cmml" xref="S5.p4.2.m2.1.1.2">ùëñ</ci><apply id="S5.p4.2.m2.1.1.3.cmml" xref="S5.p4.2.m2.1.1.3"><times id="S5.p4.2.m2.1.1.3.1.cmml" xref="S5.p4.2.m2.1.1.3.1"></times><ci id="S5.p4.2.m2.1.1.3.2.cmml" xref="S5.p4.2.m2.1.1.3.2">ùë°</ci><ci id="S5.p4.2.m2.1.1.3.3.cmml" xref="S5.p4.2.m2.1.1.3.3">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.2.m2.1c">i^{th}</annotation></semantics></math> row have been confused with the phones belonging to BPC of <math id="S5.p4.3.m3.1" class="ltx_Math" alttext="j^{th}" display="inline"><semantics id="S5.p4.3.m3.1a"><msup id="S5.p4.3.m3.1.1" xref="S5.p4.3.m3.1.1.cmml"><mi id="S5.p4.3.m3.1.1.2" xref="S5.p4.3.m3.1.1.2.cmml">j</mi><mrow id="S5.p4.3.m3.1.1.3" xref="S5.p4.3.m3.1.1.3.cmml"><mi id="S5.p4.3.m3.1.1.3.2" xref="S5.p4.3.m3.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.p4.3.m3.1.1.3.1" xref="S5.p4.3.m3.1.1.3.1.cmml">‚Äã</mo><mi id="S5.p4.3.m3.1.1.3.3" xref="S5.p4.3.m3.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.p4.3.m3.1b"><apply id="S5.p4.3.m3.1.1.cmml" xref="S5.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S5.p4.3.m3.1.1.1.cmml" xref="S5.p4.3.m3.1.1">superscript</csymbol><ci id="S5.p4.3.m3.1.1.2.cmml" xref="S5.p4.3.m3.1.1.2">ùëó</ci><apply id="S5.p4.3.m3.1.1.3.cmml" xref="S5.p4.3.m3.1.1.3"><times id="S5.p4.3.m3.1.1.3.1.cmml" xref="S5.p4.3.m3.1.1.3.1"></times><ci id="S5.p4.3.m3.1.1.3.2.cmml" xref="S5.p4.3.m3.1.1.3.2">ùë°</ci><ci id="S5.p4.3.m3.1.1.3.3.cmml" xref="S5.p4.3.m3.1.1.3.3">‚Ñé</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.3.m3.1c">j^{th}</annotation></semantics></math> column.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.2" class="ltx_p">To facilitate comparison with the confusion matrices of the various systems in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, such as Wav2vec 2.0 and FBank systems, we have reported the 1<math id="S5.p5.1.m1.1" class="ltx_Math" alttext="{{}^{s}t}" display="inline"><semantics id="S5.p5.1.m1.1a"><mmultiscripts id="S5.p5.1.m1.1.1" xref="S5.p5.1.m1.1.1.cmml"><mi id="S5.p5.1.m1.1.1.2" xref="S5.p5.1.m1.1.1.2.cmml">t</mi><mprescripts id="S5.p5.1.m1.1.1a" xref="S5.p5.1.m1.1.1.cmml"></mprescripts><mrow id="S5.p5.1.m1.1.1b" xref="S5.p5.1.m1.1.1.cmml"></mrow><mi id="S5.p5.1.m1.1.1.3" xref="S5.p5.1.m1.1.1.3.cmml">s</mi></mmultiscripts><annotation-xml encoding="MathML-Content" id="S5.p5.1.m1.1b"><apply id="S5.p5.1.m1.1.1.cmml" xref="S5.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p5.1.m1.1.1.1.cmml" xref="S5.p5.1.m1.1.1">superscript</csymbol><ci id="S5.p5.1.m1.1.1.2.cmml" xref="S5.p5.1.m1.1.1.2">ùë°</ci><ci id="S5.p5.1.m1.1.1.3.cmml" xref="S5.p5.1.m1.1.1.3">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.1.m1.1c">{{}^{s}t}</annotation></semantics></math> and 2<sup id="S5.p5.2.1" class="ltx_sup"><span id="S5.p5.2.1.1" class="ltx_text ltx_font_italic">nd</span></sup> most confused classes for each system in Table¬†<a href="#S5.T3" title="Table 3 ‚Ä£ 5 Experimental Results and Discussion ‚Ä£ Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Notably, there is a marked similarity in the confusion patterns among broad phonetic categories (BPCs) across different systems. For example, the Plosives and Fricatives or Vowels, Diphthongs and Semi-vowels are consistently highly confusable over all systems which is attributed to class confusability, as discussed. Another example is Affricates which are consistently confused with Fricatives and Plosives, or Nasals which are mostly confused with Plosives and Silence.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">Note that the diagonal items in the confusion matrices indicate the number of within-class confusions. As seen, the diagonal element for the Silence class is always zero because it is a single-class category (Table¬†<a href="#S5.T1" title="Table 1 ‚Ä£ 5 Experimental Results and Discussion ‚Ä£ Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). As such, there are no other classes in this category to cause within-class confusion.</p>
</div>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.1" class="ltx_p">Note that the diagonal items in each confusion matrix indicate within-class confusions. In all confusion matrices, the diagonal element for the Silence class is zero because it is a single-class category (Table¬†<a href="#S5.T1" title="Table 1 ‚Ä£ 5 Experimental Results and Discussion ‚Ä£ Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), meaning there are no other classes within this category to cause within-class confusion.</p>
</div>
<div id="S5.p8" class="ltx_para">
<p id="S5.p8.1" class="ltx_p">Transfer learning from WSJ, has a significant effect on the performance of the raw waveform models, resulting in PERs of 11.8% and 13.7% on the Dev and Test sets, respectively. However, the error distribution across BPCs (Figs.¬†<a href="#S5.F2" title="Figure 2 ‚Ä£ 5 Experimental Results and Discussion ‚Ä£ Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S5.F3" title="Figure 3 ‚Ä£ 5 Experimental Results and Discussion ‚Ä£ Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) and confusion patterns (Table¬†<a href="#S5.T3" title="Table 3 ‚Ä£ 5 Experimental Results and Discussion ‚Ä£ Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) remain largely similar.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The first and second most confused classes for various BPCs and systems on TIMIT's Dev set. For example, (vow, dip/sem) means the first most confused class is Vowel; Diphthogs and Semi-vowels are tied for the second most confused class.</figcaption>
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">System</th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Affricate</th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Diphthong</th>
<th id="S5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Fricative</th>
<th id="S5.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Nasal</th>
<th id="S5.T3.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Plosive</th>
<th id="S5.T3.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Semi-Vowel</th>
<th id="S5.T3.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Silence</th>
<th id="S5.T3.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Vowel</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<th id="S5.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">FBank <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</th>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">aff, fri</td>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">vow, dip/sem</td>
<td id="S5.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">fri, plo</td>
<td id="S5.T3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">nas, plo</td>
<td id="S5.T3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">plo, fri</td>
<td id="S5.T3.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">vow, sem</td>
<td id="S5.T3.1.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">fri/nas, plo</td>
<td id="S5.T3.1.2.1.9" class="ltx_td ltx_align_center ltx_border_tt">vow, dip/sem</td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<th id="S5.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">FBank-WSJ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</th>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">fri, aff/plo</td>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">vow, dip</td>
<td id="S5.T3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">fri, plo</td>
<td id="S5.T3.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r">nas, sil</td>
<td id="S5.T3.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r">plo, fri</td>
<td id="S5.T3.1.3.2.7" class="ltx_td ltx_align_center ltx_border_r">vow, sem</td>
<td id="S5.T3.1.3.2.8" class="ltx_td ltx_align_center ltx_border_r">nas, plo</td>
<td id="S5.T3.1.3.2.9" class="ltx_td ltx_align_center">vow, dip</td>
</tr>
<tr id="S5.T3.1.4.3" class="ltx_tr">
<th id="S5.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Wav2vec 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</th>
<td id="S5.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">plo, sem</td>
<td id="S5.T3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">vow, sem/sil</td>
<td id="S5.T3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">fri, plo/sil</td>
<td id="S5.T3.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r">nas, sil</td>
<td id="S5.T3.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r">plo, fri/aff</td>
<td id="S5.T3.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r">vow, sem</td>
<td id="S5.T3.1.4.3.8" class="ltx_td ltx_align_center ltx_border_r">plo, fri/sem</td>
<td id="S5.T3.1.4.3.9" class="ltx_td ltx_align_center">vow, dip</td>
</tr>
<tr id="S5.T3.1.5.4" class="ltx_tr">
<th id="S5.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">RawWav</th>
<td id="S5.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">aff, fri/plo</td>
<td id="S5.T3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">vow, dip</td>
<td id="S5.T3.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">fri, plo</td>
<td id="S5.T3.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">nas, plo</td>
<td id="S5.T3.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">plo, fri</td>
<td id="S5.T3.1.5.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">vow, sem</td>
<td id="S5.T3.1.5.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">fri, nas</td>
<td id="S5.T3.1.5.4.9" class="ltx_td ltx_align_center ltx_border_t">vow, dip</td>
</tr>
<tr id="S5.T3.1.6.5" class="ltx_tr">
<th id="S5.T3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">RawWav-WSJ</th>
<td id="S5.T3.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">aff, plo</td>
<td id="S5.T3.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">vow, dip</td>
<td id="S5.T3.1.6.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">fri, plo</td>
<td id="S5.T3.1.6.5.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">nas, plo</td>
<td id="S5.T3.1.6.5.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">plo, fri</td>
<td id="S5.T3.1.6.5.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">vow, dip</td>
<td id="S5.T3.1.6.5.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">fri, nas/plo</td>
<td id="S5.T3.1.6.5.9" class="ltx_td ltx_align_center ltx_border_bb">vow, dip</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p9" class="ltx_para">
<p id="S5.p9.2" class="ltx_p">Fig.¬†<a href="#S5.F6" title="Figure 6 ‚Ä£ 5 Experimental Results and Discussion ‚Ä£ Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates the performance gain (relative PER reduction) after transfer learning from WSJ across BPCs for both FBank and raw waveform models. The average PER gains shown in Fig.¬†<a href="#S5.F6" title="Figure 6 ‚Ä£ 5 Experimental Results and Discussion ‚Ä£ Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>¬†(a) is <math id="S5.p9.1.m1.1" class="ltx_Math" alttext="8.4" display="inline"><semantics id="S5.p9.1.m1.1a"><mn id="S5.p9.1.m1.1.1" xref="S5.p9.1.m1.1.1.cmml">8.4</mn><annotation-xml encoding="MathML-Content" id="S5.p9.1.m1.1b"><cn type="float" id="S5.p9.1.m1.1.1.cmml" xref="S5.p9.1.m1.1.1">8.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p9.1.m1.1c">8.4</annotation></semantics></math>% for FBank and <math id="S5.p9.2.m2.1" class="ltx_Math" alttext="17.6" display="inline"><semantics id="S5.p9.2.m2.1a"><mn id="S5.p9.2.m2.1.1" xref="S5.p9.2.m2.1.1.cmml">17.6</mn><annotation-xml encoding="MathML-Content" id="S5.p9.2.m2.1b"><cn type="float" id="S5.p9.2.m2.1.1.cmml" xref="S5.p9.2.m2.1.1">17.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p9.2.m2.1c">17.6</annotation></semantics></math>% for raw waveform models. This difference can be attributed to the raw waveform model's access to richer information, albeit requiring more data and larger model to fully leverage its potential.</p>
</div>
<div id="S5.p10" class="ltx_para">
<p id="S5.p10.3" class="ltx_p">There are two additional important observations in Fig.¬†<a href="#S5.F6" title="Figure 6 ‚Ä£ 5 Experimental Results and Discussion ‚Ä£ Phonetic Error Analysis of Raw Waveform Acoustic Models with Parametric and Non-Parametric CNNs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Firstly, for both FBank and raw waveform models, the performance gain after transfer learning for the Vowel<sup id="S5.p10.3.1" class="ltx_sup">+</sup> class, namely union of Vowels and Diphthongs, is minimal or even negative. These classes are particularly sensitive to speaker attributes (e.g., speaker ID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and emotion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>). The transfer learning from WSJ does not adequately address speaker variability and speaker invariant representation learning because the WSJ training set (si284) comprises only <math id="S5.p10.2.m2.1" class="ltx_Math" alttext="282" display="inline"><semantics id="S5.p10.2.m2.1a"><mn id="S5.p10.2.m2.1.1" xref="S5.p10.2.m2.1.1.cmml">282</mn><annotation-xml encoding="MathML-Content" id="S5.p10.2.m2.1b"><cn type="integer" id="S5.p10.2.m2.1.1.cmml" xref="S5.p10.2.m2.1.1">282</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p10.2.m2.1c">282</annotation></semantics></math> speakers while TIMIT has a richer speaker space with <math id="S5.p10.3.m3.1" class="ltx_Math" alttext="630" display="inline"><semantics id="S5.p10.3.m3.1a"><mn id="S5.p10.3.m3.1.1" xref="S5.p10.3.m3.1.1.cmml">630</mn><annotation-xml encoding="MathML-Content" id="S5.p10.3.m3.1b"><cn type="integer" id="S5.p10.3.m3.1.1.cmml" xref="S5.p10.3.m3.1.1">630</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p10.3.m3.1c">630</annotation></semantics></math> speakers.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Confusion matrices of three phonetic categorisations for RawWav-CNN-WSJ on TIMIT's Dev set. The <span id="S5.F5.3.1" class="ltx_text ltx_font_bold">bold</span> and <span id="S5.F5.4.2" class="ltx_text ltx_framed ltx_framed_underline">underlined</span> denote the first and second mostly confused classes.</figcaption>
</figure>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2406.00898/assets/x6.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="402" height="269" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Relative gain after transfer learning from WSJ.</figcaption>
</figure>
<div id="S5.p11" class="ltx_para">
<p id="S5.p11.1" class="ltx_p">Secondly, a significant performance gap is observed for the Nasal and Silence classes between FBank and raw waveform models. While the former experiences a relative performance degradation of -1% for Nasals and -10% for Silence, the latter achieves an improvement of 22% and 19%, respectively. This observation, particularly for the Silence class, is remarkable as even advanced models like Wav2vec 2.0 struggle to enhance performance over the silence class (please refer to Fig.¬†24 in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>).
Such a performance gap can be partially attributed to the additional information in the raw waveform, namely the phase spectrum, helping in better handling of these classes and, consequently, more effective leveraging of transfer learning.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we conducted an extensive evaluation of raw waveform acoustic models on TIMIT phone recognition task, moving beyond the commonly used PER metric.
Our analysis involved decomposing the overall substitution, deletion, insertion, and PER, calculating each metric for each broad phonetic class (BPC) within three phonetic categorisations: {affricate, diphthong, fricative, nasal, plosive, semi-vowel, silence, vowel}, {consonant, vowel<sup id="S6.p1.1.1" class="ltx_sup">+</sup>, silence}, and {voiced, unvoiced, silence}.
We developed a raw waveform model with the highest performance on TIMIT, compared with raw waveform models reported in the literature and computed the PER for each BPC in each category. Furthermore, we examined the impact of transfer learning from WSJ on the raw waveform model's performance across various BPCs. We also constructed a confusion matrix for each phonetic categorisation, both for the raw waveform models without and with transfer learning, and compared the phonetic confusion patterns with those of the Wav2vec 2.0 and Filterbank systems.
Future research directions encompass exploring alternative modeling techniques and examining how different languages influence errors within the BPCs.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
E.¬†Loweimi, A.¬†Carmantini, P.¬†Bell, S.¬†Renals, and Z.¬†Cvetkovic, ``Phonetic error analysis beyond phone error rate,'' </span><em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em><span id="bib.bib1.3.3" class="ltx_text" style="font-size:90%;">, vol.¬†31, pp. 3346‚Äì3361, 2023.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
E.¬†Loweimi, ``Robust phase-based speech signal processing; from source-filter separation to model-based robust asr,'' Ph.D. dissertation, University of Sheffield, 2018. [Online]. Available: </span><a target="_blank" href="http://etheses.whiterose.ac.uk/19409/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://etheses.whiterose.ac.uk/19409/</a><span id="bib.bib2.2.2" class="ltx_text" style="font-size:90%;">
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
E.¬†Loweimi, S.¬†Ahadi, and H.¬†Sheikhzadeh, ``Phase-only speech reconstruction using very short frames.'' in </span><em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">INTERSPEECH</em><span id="bib.bib3.3.3" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Z.¬†Yue, E.¬†Loweimi, and Z.¬†Cvetkovic, ``Dysarthric Speech Recognition, Detection and Classification using Raw Phase and Magnitude Spectra,'' in </span><em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">INTERSPEECH</em><span id="bib.bib4.3.3" class="ltx_text" style="font-size:90%;">, 2023, pp. 1533‚Äì1537.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
E.¬†Loweimi, Z.¬†Cvetkovic, P.¬†Bell, and S.¬†Renals, ``Speech acoustic modelling from raw phase spectrum,'' in </span><em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP</em><span id="bib.bib5.3.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
E.¬†Loweimi, J.¬†Barker, and T.¬†Hain, ``Statistical normalisation of phase-based feature representation for robust speech recognition,'' in </span><em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP</em><span id="bib.bib6.3.3" class="ltx_text" style="font-size:90%;">, 2017, pp. 5310‚Äì5314.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
H.¬†Choi, J.¬†Kim, J.¬†Huh, A.¬†Kim, J.¬†Ha, and K.¬†Lee, ``Phase-aware speech enhancement with deep complex u-net,'' in </span><em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICLR</em><span id="bib.bib7.3.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
E.¬†Loweimi, S.¬†M. Ahadi, and S.¬†Loveymi, ``On the importance of phase and magnitude spectra in speech enhancement,'' in </span><em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Iranian Conference on Electrical Engineering</em><span id="bib.bib8.3.3" class="ltx_text" style="font-size:90%;">, 2011, pp. 1‚Äì6.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
E.¬†Loweimi, J.¬†Barker, O.¬†Saz¬†Torralba, and T.¬†Hain, ``Robust source-filter separation of speech signal in the phase domain,'' in </span><em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">INTERSPEECH</em><span id="bib.bib9.3.3" class="ltx_text" style="font-size:90%;">, 2017, pp. 414‚Äì418.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
E.¬†Loweimi, P.¬†Bell, and S.¬†Renals, ``On learning interpretable CNNs with parametric modulated kernel-based filters,'' in </span><em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">INTERSPEECH</em><span id="bib.bib10.3.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
J.¬†S. Garofolo, L.¬†F. Lamel, W.¬†M. Fisher, J.¬†G. Fiscus, D.¬†S. Pallett, and N.¬†L. Dahlgren, ``DARPA TIMIT acoustic phonetic continuous speech corpus,'' 1993.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
D.¬†B. Paul and J.¬†M. Baker, ``The design for the Wall Street Journal-based CSR corpus,'' in </span><em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP</em><span id="bib.bib12.3.3" class="ltx_text" style="font-size:90%;">, 1992, pp. 899‚Äì902.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
A.¬†Baevski, Y.¬†Zhou, A.¬†Mohamed, and M.¬†Auli, ``wav2vec 2.0: A framework for self-supervised learning of speech representations,'' in </span><em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NIPS</em><span id="bib.bib13.3.3" class="ltx_text" style="font-size:90%;">, vol.¬†33, 2020, pp. 12‚Äâ449‚Äì12‚Äâ460.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
D.¬†Palaz, M.¬†Magimai-Doss, and R.¬†Collobert, ``End-to-end acoustic modeling using convolutional neural networks for hmm-based automatic speech recognition,'' </span><em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Speech Communication</em><span id="bib.bib14.3.3" class="ltx_text" style="font-size:90%;">, vol. 108, pp. 15‚Äì32, 2019.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Z.¬†T√ºske, R.¬†Schl√ºter, and H.¬†Ney, ``Acoustic modeling of speech waveform based on multi-resolution, neural network signal processing,'' in </span><em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP</em><span id="bib.bib15.3.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
T.¬†N. Sainath, R.¬†J. Weiss, K.¬†W. Wilson, A.¬†Narayanan, and M.¬†Bacchiani, ``Factored spatial and spectral multichannel raw waveform CLDNNs,'' in </span><em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP</em><span id="bib.bib16.3.3" class="ltx_text" style="font-size:90%;">, 2016, pp. 5075‚Äì5079.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
P.¬†Ghahremani, V.¬†Manohar, D.¬†Povey, and S.¬†Khudanpur, ``Acoustic modelling from the signal domain using CNNs,'' in </span><em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">INTERSPEECH</em><span id="bib.bib17.3.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Z.¬†Zhu, J.¬†H. Engel, and A.¬†Hannun, ``Learning multiscale features directly from waveforms,'' in </span><em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">INTERSPEECH</em><span id="bib.bib18.3.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
P.¬†von Platen, C.¬†Zhang, and P.¬†C. Woodland, ``Multi-span acoustic modelling using raw waveform signals,'' in </span><em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">INTERSPEECH</em><span id="bib.bib19.3.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
J.¬†Yousafzai, P.¬†Sollich, Z.¬†Cvetkovic, and B.¬†Yu, ``Combined features and kernel design for noise robust phoneme classification using support vector machines,'' </span><em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE/ACM Trans. Audio, Speech and Lang. Proc.</em><span id="bib.bib20.3.3" class="ltx_text" style="font-size:90%;">, vol.¬†19, pp. 1396‚Äì1407, 2011.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
M.¬†Ager, Z.¬†Cvetkoviƒá, and P.¬†Sollich, ``Combined waveform-cepstral representation for robust speech recognition,'' in </span><em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ISIT</em><span id="bib.bib21.3.3" class="ltx_text" style="font-size:90%;">, 2011, pp. 864‚Äì868.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
M.¬†Ravanelli and Y.¬†Bengio, ``Speaker and speech recognition from raw waveform with SincNet,'' in </span><em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP</em><span id="bib.bib22.3.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
T.¬†Parcollet, M.¬†Morchid, and G.¬†Linar√®s, ``E2E-SINCNET: Toward fully end-to-end speech recognition,'' in </span><em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP</em><span id="bib.bib23.3.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 7714‚Äì7718.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
E.¬†Loweimi, P.¬†Bell, and S.¬†Renals, ``On the Robustness and Training Dynamics of Raw Waveform Models,'' in </span><em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">INTERSPEECH</em><span id="bib.bib24.3.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 1001‚Äì1005.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
D.¬†Oglic, Z.¬†Cvetkovic, and P.¬†Sollich, ``Learning waveform-based acoustic models using deep variational convolutional neural networks,'' </span><em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE/ACM Trans. Audio, Speech and Lang. Proc.</em><span id="bib.bib25.3.3" class="ltx_text" style="font-size:90%;">, vol.¬†29, pp. 2850‚Äì2863, 2021.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
P.¬†No√©, T.¬†Parcollet, and M.¬†Morchid, ``CGCNN: Complex gabor convolutional neural network on raw speech,'' in </span><em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP</em><span id="bib.bib26.3.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 7724‚Äì7728.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Z.¬†Yue, E.¬†Loweimi, H.¬†Christensen, J.¬†Barker, and Z.¬†Cvetkovic, ``Dysarthric Speech Recognition From Raw Waveform with Parametric CNNs,'' in </span><em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">INTERSPEECH</em><span id="bib.bib27.3.3" class="ltx_text" style="font-size:90%;">, 2022, pp. 31‚Äì35.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
J.¬†Fainberg, O.¬†Klejch, E.¬†Loweimi, P.¬†Bell, and S.¬†Renals, ``Acoustic model adaptation from raw waveforms with SincNet,'' in </span><em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ASRU</em><span id="bib.bib28.3.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
J.¬†Eatock and J.¬†Mason, ``A quantitative assessment of the relative speaker discriminating properties of phonemes,'' in </span><em id="bib.bib29.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP</em><span id="bib.bib29.3.3" class="ltx_text" style="font-size:90%;">, 1994, pp. 133‚Äì136.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
M.¬†Antal and G.¬†Toderean, ``Speaker recognition and broad phonetic groups,'' in </span><em id="bib.bib30.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">SPPRA</em><span id="bib.bib30.3.3" class="ltx_text" style="font-size:90%;">, ser. SPPRA'06.¬†¬†¬†ACTA Press, 2006, p. 155‚Äì159.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Y.-J. Lu, C.-F. Liao, X.¬†Lu, J.¬†weih Hung, and Y.¬†Tsao, ``Incorporating Broad Phonetic Information for Speech Enhancement,'' in </span><em id="bib.bib31.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">INTERSPEECH</em><span id="bib.bib31.3.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 2417‚Äì2421.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
T.¬†Kempton and R.¬†K. Moore, ``Language identification: insights from the classification of hand annotated phone transcripts,'' in </span><em id="bib.bib32.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Odyssey</em><span id="bib.bib32.3.3" class="ltx_text" style="font-size:90%;">.¬†¬†¬†ISCA, 2008.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
L.¬†Zhang, T.¬†Wang, and V.¬†Cuperman, ``A CELP variable rate speech codec with low average rate,'' in </span><em id="bib.bib33.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP</em><span id="bib.bib33.3.3" class="ltx_text" style="font-size:90%;">, vol.¬†2, 1997, pp. 735‚Äì738.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
J.¬†Yuan, X.¬†Cai, R.¬†Zheng, L.¬†Huang, and K.¬†Church, ``The role of phonetic units in speech emotion recognition,'' </span><em id="bib.bib34.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib34.3.3" class="ltx_text" style="font-size:90%;">, vol. abs/2108.01132, 2021.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Y.-T. Lee, X.-B. Chen, H.-S. Lee, J.-S.¬†R. Jang, and H.-M. Wang, ``Multi-task learning for acoustic modeling using articulatory attributes,'' in </span><em id="bib.bib35.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">APSIPA</em><span id="bib.bib35.3.3" class="ltx_text" style="font-size:90%;">, 2019, pp. 855‚Äì861.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
S.¬†Young </span><em id="bib.bib36.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">et¬†al.</em><span id="bib.bib36.3.3" class="ltx_text" style="font-size:90%;">, </span><em id="bib.bib36.4.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">The HTK Book Version 3.4</em><span id="bib.bib36.5.5" class="ltx_text" style="font-size:90%;">.¬†¬†¬†Cambridge University Press, 2006.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
S.¬†Ziegler, B.¬†Ludusan, and G.¬†Gravier, ``Using broad phonetic classes to guide search in automatic speech recognition,'' in </span><em id="bib.bib37.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">INTERSPEECH</em><span id="bib.bib37.3.3" class="ltx_text" style="font-size:90%;">, 2012, pp. 1023‚Äì1026.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
A.¬†≈Ωgank, B.¬†Horvat, and Z.¬†Kaƒçiƒç, ``Data-driven generation of phonetic broad classes, based on phoneme confusion matrix similarity,'' </span><em id="bib.bib38.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Speech Communication</em><span id="bib.bib38.3.3" class="ltx_text" style="font-size:90%;">, vol.¬†47, no.¬†3, pp. 379‚Äì393, 2005.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
A.¬†Graves, S.¬†Fern√°ndez, and J.¬†Schmidhuber, ``Bidirectional lstm networks for improved phoneme classification and recognition,'' in </span><em id="bib.bib39.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICANN</em><span id="bib.bib39.3.3" class="ltx_text" style="font-size:90%;">, 2005.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
B.¬†Moore, </span><em id="bib.bib40.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">An Introduction to the Psychology of Hearing</em><span id="bib.bib40.3.3" class="ltx_text" style="font-size:90%;">, 5th¬†ed.¬†¬†¬†London: Elsevier Academic Press, 2004.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
N.¬†Srivastava </span><em id="bib.bib41.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">et¬†al.</em><span id="bib.bib41.3.3" class="ltx_text" style="font-size:90%;">, ``Dropout: A simple way to prevent neural networks from overfitting,'' </span><em id="bib.bib41.4.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Machine Learning Research</em><span id="bib.bib41.5.5" class="ltx_text" style="font-size:90%;">, vol.¬†15, pp. 1929‚Äì1958, 2014.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
S.¬†Ioffe and C.¬†Szegedy, ``Batch normalization: Accelerating deep network training by reducing internal covariate shift,'' in </span><em id="bib.bib42.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib42.3.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
M.¬†Ravanelli, T.¬†Parcollet, and Y.¬†Bengio, ``The PyTorch-Kaldi speech recognition toolkit,'' in </span><em id="bib.bib43.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP</em><span id="bib.bib43.3.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
D.¬†Povey </span><em id="bib.bib44.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">et¬†al.</em><span id="bib.bib44.3.3" class="ltx_text" style="font-size:90%;">, ``The Kaldi speech recognition toolkit,'' in </span><em id="bib.bib44.4.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">ASRU</em><span id="bib.bib44.5.5" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
E.¬†Loweimi, Z.¬†Yue, P.¬†Bell, S.¬†Renals, and Z.¬†Cvetkovic, ``Multi-stream acoustic modelling using raw real and imaginary parts of the fourier transform,'' </span><em id="bib.bib45.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em><span id="bib.bib45.3.3" class="ltx_text" style="font-size:90%;">, vol.¬†31, pp. 876‚Äì890, 2023.
</span>
</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.00897" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.00898" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.00898">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.00898" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.00899" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 17:56:16 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
