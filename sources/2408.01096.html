<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.01096] Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding</title><meta property="og:description" content="We introduce a project that revives a piece of 15th-century Korean court music, Chihwapyeong and Chwipunghyeong, composed upon the poem Songs of the Dragon Flying to Heaven. One of the earliest examples of Jeongganbo, …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.01096">

<!--Generated on Thu Sep  5 12:42:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">We introduce a project that revives a piece of 15th-century Korean court music, <span id="id1.id1.1" class="ltx_text ltx_font_italic">Chihwapyeong</span> and <span id="id1.id1.2" class="ltx_text ltx_font_italic">Chwipunghyeong</span>, composed upon the poem <span id="id1.id1.3" class="ltx_text ltx_font_italic">Songs of the Dragon Flying to Heaven</span>. One of the earliest examples of <span id="id1.id1.4" class="ltx_text ltx_font_italic">Jeongganbo</span>, a Korean musical notation system, the remaining version only consists of a rudimentary melody. Our research team, commissioned by the National Gugak (Korean Traditional Music) Center, aimed to transform this old melody into a performable arrangement for a six-part ensemble.
Using <span id="id1.id1.5" class="ltx_text ltx_font_italic">Jeongganbo</span> data acquired through bespoke optical music recognition, we trained a BERT-like masked language model and an encoder-decoder transformer model.
We also propose an encoding scheme that strictly follows the structure of <span id="id1.id1.6" class="ltx_text ltx_font_italic">Jeongganbo</span> and denotes note durations as positions.
The resulting machine-transformed version of <span id="id1.id1.7" class="ltx_text ltx_font_italic">Chihwapyeong</span> and <span id="id1.id1.8" class="ltx_text ltx_font_italic">Chwipunghyeong</span> were evaluated by experts and performed by the Court Music Orchestra of National Gugak Center. Our work demonstrates that generative models can successfully be applied to traditional music with limited training data if combined with careful design.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text ltx_font_italic">Six dragons fly on the east land; every endeavour is a heavenly blessing</span>. This is the first line of lyrics in <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">Yongbieocheonga</span>, the first text written in the Korean alphabet (Hangul, 한글). Sejong the Great, one of the most respected figures in Korean history, invented and introduced Hangul in 1446. In addition to this remarkable achievement, he ordered scholar-officials to write <span id="S1.p1.1.3" class="ltx_text ltx_font_italic">Yongbieocheonga</span>, and composed music to accompany the lyrics.
Three other pieces composed at the time are <span id="S1.p1.1.4" class="ltx_text ltx_font_italic">Yeo-Min-Lak</span>, <span id="S1.p1.1.5" class="ltx_text ltx_font_bold ltx_font_italic">Chi-Hwa-Pyeong</span> and <span id="S1.p1.1.6" class="ltx_text ltx_font_bold ltx_font_italic">Chwi-Pung-Hyeong</span>. These compositions are still preserved in the <span id="S1.p1.1.7" class="ltx_text ltx_font_italic">Veritable Records of Sejong</span>, which is the oldest surviving musical score in Korea<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
More detailed information is available here<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2408.01096/assets/framework-final.png" id="S1.F1.g1" class="ltx_graphics ltx_img_square" width="598" height="677" alt="The image shows the entire framework of a research process described in a paper. The process involves converting modern Korean court music scores (jeongganbo) into a machine-readable dataset using Optical Music Recognition (OMR) and token encoding. The 15th century’s old melody is infilled by a BERT-like Masked Language Model (MLM) and further processed by a Transformer language model to produce melodies for six traditional Korean musical instruments. The instruments depicted are gayageum, daegeum, ajaeng, haegeum, piri and geomungo.">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text ltx_font_bold">Figure 1</span>: </span>Overview of the proposed research framework</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Among these three pieces, only <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">Yeominlak</span> is handed down to the present day, while the other two are no longer performed.
The National Gugak Center<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span id="footnote1.1" class="ltx_text ltx_font_italic">Gugak</span> (국악) is the Korean term for traditional music</span></span></span>, which is the primary organization dedicated to the preservation and development of traditional music, commissioned the task of reconstructing these two pieces in a performable format using artificial intelligence systems.
Given a simple melody of 512 <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">gaks</span> (measures) of <span id="S1.p2.1.3" class="ltx_text ltx_font_italic">Chihwapyeong</span> or 132 gaks of <span id="S1.p2.1.4" class="ltx_text ltx_font_italic">Chwipunghyeong</span>, the system must generate scores for six different instruments.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Our solution encompasses a wide range of tasks in the field of music information retrieval—constructing a specialized dataset, optical music recognition, designing a domain-specific encoding scheme, training models with limited data, and generating music of concert-level quality. In this paper, we present in detail the different frameworks used in the project: two types of transformer-based models; a symbolic dataset of Korean court music acquired through optical music recognition; and a novel “Jeonggan-like” encoding method that notates monophonic melody by combining notes’ position and pitch, along with a beat counter that informs the transformer the temporal position.
The effectiveness of the proposed techniques was validated through quantitative metrics and subjective evaluation by experts from the National Gugak Center. Finally, we introduce a web demo that allows users to examine and generate traditional Korean court music interactively.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">This project has significance not only for cultural preservation but also for wider considerations in machine learning and music generation.
One of the many benefits to be had from the inter-cultural study of music is the different perspectives expressed in ‘the music itself’ as well as any notational and/or theoretical traditions that go alongside it.
As presented in previous research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, the encoding of music makes significant differences in machine learning tasks.
In thinking through different ways of digitally encoding music, we stand to learn a great deal from the various syntaxes that have been used in diverse traditional contexts.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Recent advances in neural network-based music generation have resulted in much artistic output. Since 2020, the <span id="S2.p1.1.1" class="ltx_text ltx_font_italic">AI Music Generation Challenge<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S2.p1.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> has been held annually, focusing on generating songs in the style of Irish and Swedish folk music. This event has allowed for exploration of new methods for generation and evaluation of traditional music through the means of deep learning models.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">Beethoven X project</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> utilized neural networks to learn Beethoven’s compositional style and complete his unfinished 10th Symphony. The resulting work has been performed by an orchestra—a project outline similar to that of ours.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Attempts at automatic generation have been made for traditional music from beyond the West, including Persia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and China <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. The limited progress in such areas is often due to the distinctive traditional musical systems that demand deep understanding and unique methodologies. Such idiosyncrasies put much interest and meaning in the computational research of traditional music, since it can present new methods and perspectives to the field as a whole, while also helping preserve diverse musical heritages.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2408.01096/assets/jeonggan_example.png" id="S2.F2.g1" class="ltx_graphics ltx_img_landscape" width="658" height="133" alt="The image contains two corresponding musical scores. The lower part shows An example of Jeongganbo in the original notion, and the upper part displays a broadly equivalent conversion to Western classical notion. These two scores are aligned with light blue dotted lines between them.">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text ltx_font_bold">Figure 2</span>: </span>An example of Jeongganbo in the original notion (below) and a broadly equivalent conversion to Western classical notion (above).
Dashed lines are part of neither notation and added simply to clarify the temporal alignment between the two systems.</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Jeongganbo Dataset</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Jeongganbo Notation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">As depicted in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, Korean court music is performed on a variety of instruments, including plucked string instruments (<span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">Gayageum</span> and <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">Geomungo</span>), bowed string instruments (<span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_italic">Haegeum</span> and <span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_italic">Ajaeng</span>), and wind instruments (<span id="S3.SS1.p1.1.5" class="ltx_text ltx_font_italic">Daegeum</span> and <span id="S3.SS1.p1.1.6" class="ltx_text ltx_font_italic">Piri</span>), among others. These instruments are played together in a heterophonic texture, with each instrument employing its distinctive playing techniques and ornamentations.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2408.01096/assets/position.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="186" alt="This image shows five examples of how to interpret the rhythm based on the position of characters within jeonggan. Each position is assigned a number from 0 to 15. If there is only one character in the center of the square, it is interpreted as a dotted quarter note. The other four examples show how the rhythm is divided based on the row and column positions within the jeonggan.">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text ltx_font_bold">Figure 3</span>: </span>Jeonggan-like encoding position labels</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Much of Korean court music is written in <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Jeongganbo</span>, a traditional musical notation system. <span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_italic">Jeongganbo</span> is recognized as the first system in East Asia capable of simultaneously representing both pitch and duration of notes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. This versatility has been instrumental in passing down court music throughout history <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_italic">Jeongganbo</span> uses grid-divided boxes (<span id="S3.SS1.p3.1.2" class="ltx_text ltx_font_italic">Jeonggans</span>) as the basic unit of time.
The <span id="S3.SS1.p3.1.3" class="ltx_text ltx_font_italic">number</span> of characters (notes)
and their <span id="S3.SS1.p3.1.4" class="ltx_text ltx_font_italic">position</span> within each <span id="S3.SS1.p3.1.5" class="ltx_text ltx_font_italic">jeonggan</span> varies to denote rhythm.
Figure <a href="#S2.F2" title="Figure 2 ‣ 2 Related Works ‣ Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>
provides an example passage, and figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Jeongganbo Notation ‣ 3 Jeongganbo Dataset ‣ Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> provides a schematic overview of possible positions.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.3" class="ltx_p">Here, we provide a broad introduction to this rhythmic notation system in quasi-Western musical theoretic language.
Each <span id="S3.SS1.p4.3.1" class="ltx_text ltx_font_italic">jeonggan</span> is broadly equivalent to a beat.
If a <span id="S3.SS1.p4.3.2" class="ltx_text ltx_font_italic">jeonggan</span> features only one character, this note event starts at the beginning of the beat and lasts the beat’s full duration.
The first box (‘0’) in figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Jeongganbo Notation ‣ 3 Jeongganbo Dataset ‣ Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> is in this form as is the second <span id="S3.SS1.p4.3.3" class="ltx_text ltx_font_italic">jeonggan</span> of figure <a href="#S2.F2" title="Figure 2 ‣ 2 Related Works ‣ Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> where the ‘compound beats’ correspond to the duration <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="\quarternote\Pu" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mrow id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">𝅘𝅥</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p4.1.m1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.cmml">​</mo><merror class="ltx_ERROR undefined undefined" id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3b.cmml"><mtext id="S3.SS1.p4.1.m1.1.1.3a" xref="S3.SS1.p4.1.m1.1.1.3b.cmml">\Pu</mtext></merror></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><times id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1"></times><ci id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2">𝅘𝅥</ci><ci id="S3.SS1.p4.1.m1.1.1.3b.cmml" xref="S3.SS1.p4.1.m1.1.1.3"><merror class="ltx_ERROR undefined undefined" id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3"><mtext id="S3.SS1.p4.1.m1.1.1.3a.cmml" xref="S3.SS1.p4.1.m1.1.1.3">\Pu</mtext></merror></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">\quarternote\Pu</annotation></semantics></math>
(in this case for the note B<math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="\flat" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mi mathvariant="normal" id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">♭</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">♭</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">\flat</annotation></semantics></math>4).
At the next metrical level we have the ‘column’ division of the ‘rows’.
This number of ‘rows’ relates broadly to the top level division of the beat.
The use of three vertically stacked characters refers to 3 equal divisions of this beat (here, 3 x <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="\eighthnote" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mi mathvariant="normal" id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml">𝅘𝅥𝅮</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><ci id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">𝅘𝅥𝅮</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">\eighthnote</annotation></semantics></math>s).</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.3" class="ltx_p">For example, in figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Jeongganbo Notation ‣ 3 Jeongganbo Dataset ‣ Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the numbers 4–9 feature a 3-part division of the <math id="S3.SS1.p5.1.m1.1" class="ltx_Math" alttext="\quarternote\Pu" display="inline"><semantics id="S3.SS1.p5.1.m1.1a"><mrow id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml"><mi mathvariant="normal" id="S3.SS1.p5.1.m1.1.1.2" xref="S3.SS1.p5.1.m1.1.1.2.cmml">𝅘𝅥</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p5.1.m1.1.1.1" xref="S3.SS1.p5.1.m1.1.1.1.cmml">​</mo><merror class="ltx_ERROR undefined undefined" id="S3.SS1.p5.1.m1.1.1.3" xref="S3.SS1.p5.1.m1.1.1.3b.cmml"><mtext id="S3.SS1.p5.1.m1.1.1.3a" xref="S3.SS1.p5.1.m1.1.1.3b.cmml">\Pu</mtext></merror></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><apply id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1"><times id="S3.SS1.p5.1.m1.1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1.1"></times><ci id="S3.SS1.p5.1.m1.1.1.2.cmml" xref="S3.SS1.p5.1.m1.1.1.2">𝅘𝅥</ci><ci id="S3.SS1.p5.1.m1.1.1.3b.cmml" xref="S3.SS1.p5.1.m1.1.1.3"><merror class="ltx_ERROR undefined undefined" id="S3.SS1.p5.1.m1.1.1.3.cmml" xref="S3.SS1.p5.1.m1.1.1.3"><mtext id="S3.SS1.p5.1.m1.1.1.3a.cmml" xref="S3.SS1.p5.1.m1.1.1.3">\Pu</mtext></merror></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">\quarternote\Pu</annotation></semantics></math> beat into 3 x <math id="S3.SS1.p5.2.m2.1" class="ltx_Math" alttext="\eighthnote" display="inline"><semantics id="S3.SS1.p5.2.m2.1a"><mi mathvariant="normal" id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml">𝅘𝅥𝅮</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><ci id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">𝅘𝅥𝅮</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">\eighthnote</annotation></semantics></math>s (positions 4, 6, 8), and a 2x division of those <math id="S3.SS1.p5.3.m3.1" class="ltx_Math" alttext="\eighthnote" display="inline"><semantics id="S3.SS1.p5.3.m3.1a"><mi mathvariant="normal" id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml">𝅘𝅥𝅮</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><ci id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1">𝅘𝅥𝅮</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">\eighthnote</annotation></semantics></math>s (e.g., 4–5). If the following <span id="S3.SS1.p5.3.1" class="ltx_text ltx_font_italic">jeonggan</span> is empty, the previously played note is sustained.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">Playing techniques and ornamentations called <span id="S3.SS1.p6.1.1" class="ltx_text ltx_font_italic">sigimsae</span> are sometimes notated for each instrument. When sigimsae are placed to the right of notes, they serve as ornamentations or embellishments for the corresponding note; when written on their own, they indicate timed instructions to play a specific note or musical phrase. For convenience, the example score is notated horizontally, but in practice, the score page is read from top to bottom and right to left. A line in <span id="S3.SS1.p6.1.2" class="ltx_text ltx_font_italic">Jeongganbo</span> can consist of anything from four to twenty beats, with each line representing a phrase unit.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2408.01096/assets/EncodingSchemes.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="254" alt="
This table compares three methods for encoding jeongganbo: ’JG-like,’ ’REMI-like,’ and ’ABC-like.’">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text ltx_font_bold">Figure 4</span>: </span>Comparison between encoding schemes</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Machine Readable Dataset</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We have constructed a dataset of 85 pieces by applying optical musical recognition (OMR) to all compositions available within the manuscripts published by the National Gugak Center. The manuscripts cover the entire repertoire of remaining Korean court music<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The term “court music” used in this paper originally refers to <span id="footnote2.1" class="ltx_text ltx_font_italic">Jeong-ak</span>. Jeong-ak includes not only court music but also salon music and military music.
However, for readability, we use “court music” here.</span></span></span>.
OMR was necessary since the scores are only provided as PDF images and the semantic data is unavailable.
We implemented and trained an encoder-decoder transformer with CNN
by synthesizing various <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">Jeonggan</span> images in a rule-based approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
In total, the dataset comprises 28 010 jeonggans across 85 pieces. When counting each instrument part independently, the combined total amounts to 141 820 jeonggans.
Out of 90 pieces notated in jeongganbo for ensembles of at least two different instruments in the published manuscripts, we excluded 5 pieces that have discrepancies in the total number of jeonggans across instruments.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Jeonggan-like Encoding</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In the field of symbolic music generation for Western monophonic and polyphonic music, encoding schemes such as ABC notation, which denotes pitch and duration separately, are effective and prevalent<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
However, when it comes to Korean court music, whose heterophonic structure is a defining characteristic, it is crucial that the intricate alignment of different melodies be well-represented in encoding. The genre also exhibits prolonged notes and considerable variations in note lengths, which proves to be a challenge for learning algorithms, especially when data is limited.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">These distinct musical qualities call for a specialized encoding scheme; for this, we propose <span id="S4.p2.1.1" class="ltx_text ltx_font_italic">Jeonggan (JG)-like encoding</span>, which closely follows the positional notation of <span id="S4.p2.1.2" class="ltx_text ltx_font_italic">Jeongganbo</span>. This symbolic music encoding method is modeled to inherently reflect the composition and notation style of traditional Korean court music.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">The detailed rules of encoding are as follows. The boundary of a <span id="S4.p3.1.1" class="ltx_text ltx_font_italic">Jeonggan</span> is designated as a bar (<code id="S4.p3.1.2" class="ltx_verbatim ltx_font_typewriter">|</code>) token.
Change of measure (called <span id="S4.p3.1.3" class="ltx_text ltx_font_italic">Gak</span>) is indicated by a line break (<code id="S4.p3.1.4" class="ltx_verbatim ltx_font_typewriter">\n</code>).
As illustrated in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Jeongganbo Notation ‣ 3 Jeongganbo Dataset ‣ Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the position of each note is denoted by a number between 0 and 15, after which the pitch symbol follows.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Ornamentations (<span id="S4.p4.1.1" class="ltx_text ltx_font_italic">sigimsae</span>) can either have a duration or not. <span id="S4.p4.1.2" class="ltx_text ltx_font_italic">Sigimsae</span> with duration, such as the ‘ㄱ’ symbol in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.1 Jeongganbo Notation ‣ 3 Jeongganbo Dataset ‣ Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, are handled in the same way as pitch symbols. <span id="S4.p4.1.3" class="ltx_text ltx_font_italic">Sigimsae</span> without duration such as ‘ ^ ’, which appear at the side of the pitch character, are placed after the corresponding pitch symbol.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">There are several advantages that we can expect to gain from using <span id="S4.p5.1.1" class="ltx_text ltx_font_italic">JG-like</span> encoding. First, with position-based encoding, the duration-related vocabulary is limited to just 16 entries. In contrast, duration-based encoding schemes require learning each duration token as a separate entry, resulting in a significantly larger vocabulary.
Additionally, rather than determining the length of a note with a single calculation, JG-like encoding allows for the flexible adjustment of note lengths during inference via combination of <span id="S4.p5.1.2" class="ltx_text ltx_font_italic">jeonggan</span> boundary and position tokens. This enables generation of music that is more adaptable to the time step and takes into account the sequence of the input source, which can be expected to result in more dynamic and context-aware music generation.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Other Possible Encodings</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">REMI (revamped MIDI-derived events) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> first proposed the usage of beat-position feature rather than time-shifting to encode temporal position. We also experiment with REMI-like encoding which adopts three token types: beat position, new beat (instead of new measure), and pitch tokens.
We intentionally design REMI-like and JG-like encoding to share the same structure and result in the same number of tokens for a given melody. They differ in that JG encoding provides intra-JG position, while REMI encoding provides the beat position of the note.
According to the position labels shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Jeongganbo Notation ‣ 3 Jeongganbo Dataset ‣ Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, any of [0, 1, 4, 10, 12] can correspond to beat position 0. However, in JG-like encoding, each occurrence of position tokens limits the possibilities of subsequent ones. For instance, a position token of 0 implies that no more notes will occur in the same <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">jeonggan</span>, and if the first note is 1, one or more additional notes should follow with values of 2-3 or 6-9. In contrast, in REMI-like encoding, any offset value can follow a beat position of 0. To examine the impact of this position-based logic on the generation process, we use REMI-like encoding as our first baseline for comparison.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">As a second baseline, we implement an ABC-like encoding scheme that does not have a separate bar token and encodes each note as a combination of pitch and duration values. Note that we do not omit duration tokens that are equal to unit length as ABC encoding typically does.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Orchestral Part Generation</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Transformer Sequence-to-sequence Model</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We implement an encoder-decoder transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> model to generate melodies for different instruments based on a given instrument’s melody, leveraging its ability to learn long-term dependencies. Unlike RNN-based models, the transformer calculates relationships between all elements in a sequence via the self-attention mechanism, enhancing its capability in symbolic music generation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. The model consists of an encoder that processes the input sequence and a decoder for generating the output sequence. Our objective is to generate melodies that synchronize with the input melody across musically equivalent phrases; self- and cross-attention within the model enable understanding of musical context at measure and bar levels, capturing the repeating structure of melodies and accents prominent in traditional Korean court music.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2408.01096/assets/architecture-final.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="573" alt="This image shows the model architecture used for orchestral part generation. The model encodes 1 to 5 input melodies as Note features, which are composed of tokens and instruments, and beat counters, which include previous positions, jeonggan positions, and gak positions. This encoded information is then processed by the transformer encoder and decoder to generate new melodies for the target part.">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.2.1.1" class="ltx_text ltx_font_bold">Figure 5</span>: </span>Orchestral part generation</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Beat Counter</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Instead of sinusoidal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> or learned <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> positional embedding commonly utilized in transformer-based models, we implemented a ‘beat counter’ embedding that provides information about temporal position.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">For a model to learn to ‘parse’ semantic position only from the tokens’ sequential position is challenging, if not impossible, with limited training data and a small number of transformer layers. Therefore, we explicitly encode the musical position of each symbol as a combination of measure index, beat index, and sub-beat index (in-<span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_italic">jeonggan</span> position) as shown in Figure <a href="#S5.F5" title="Figure 5 ‣ 5.1 Transformer Sequence-to-sequence Model ‣ 5 Orchestral Part Generation ‣ Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. This information is summed into note embedding, just like positional encoding of transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">As previous research of PopMAG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> demonstrated, metrical position embeddings can replace the positional encoding of transformers in symbolic music. A minor difference between PopMAG and our approach is that the model predicts only the appearance of new measures or new beats without the index of them, and that the new beat can be used for elongating the duration of previous note.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">The same idea of embedding the beat counting has been previously applied to RNN-based Irish melody generation in ABC format <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, while its advantage was not properly evaluated. A similar idea, using metrical position instead of or along with absolute token position, has also been applied to transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
However, our results presented in Section <a href="#S6.SS3" title="6.3 Results and Discussion ‣ 6 Experiment and Results ‣ Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3</span></a> demonstrate that this beat counter embedding is essential for making the model properly understand the musical contents.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experiment and Results</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Training</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">We split the <span id="S6.SS1.p1.1.1" class="ltx_text ltx_font_italic">Jeongganbo</span> dataset into three subsets: 75 pieces for training, 5 for validation, and 5 for testing. Each piece contains melodies for up to 6 instruments. The sequence-to-sequence model takes 4 measures of melody, each from a randomly selected number of instruments, as input to the encoder; and given a target instrument condition, it generates the corresponding 4 measures of melody for the target instrument. Note that the number of beats in a single measure is at least 4 or to a maximum of 20 in our dataset.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">The transformer encoder and decoder both consist of 6 layers, 4 attention heads, and a hidden dimension size of 128 with dropout of 0.2. We train for 35 000 updates across 300 epochs using negative log-likelihood loss. We also employ mixed precision training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> to enhance performance and efficiency.
We utilize the Adam optimizer with an initial learning rate of 0.001 and apply a cosine learning rate scheduler with 1000 warmup steps. Using a batch size of 16, training can be conducted on a single Nvidia RTX A6000 GPU.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Evaluation Metrics</h3>

<section id="S6.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1 </span>Length Match Rate</h4>

<div id="S6.SS2.SSS1.p1" class="ltx_para">
<p id="S6.SS2.SSS1.p1.1" class="ltx_p">As an evaluation metric, we check whether the input and output melodies share the same number of measures, a consistency necessitated by our task. Since the length of a measure can change in the middle of a piece, this metric serves as an indicator of the model’s ability to capture the musical context of the input melody and accordingly generate a musically complete melody.
We measure <span id="S6.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_italic">length match rate</span> as the percentage of generated melodies whose number of <span id="S6.SS2.SSS1.p1.1.2" class="ltx_text ltx_font_italic">jeonggans</span>, after decoding the output tokens, matches that of the input melody.</p>
</div>
</section>
<section id="S6.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2 </span>F<sub id="S6.SS2.SSS2.1.1" class="ltx_sub">1</sub>-Score</h4>

<div id="S6.SS2.SSS2.p1" class="ltx_para">
<p id="S6.SS2.SSS2.p1.1" class="ltx_p">Regarding the generation task as one with a fixed answer, we can measure the accuracy of the generated melody by directly comparing it with the ground-truth target melody. Thus, as a general accuracy metric, we calculate the F<sub id="S6.SS2.SSS2.p1.1.1" class="ltx_sub">1</sub>-score of predicted notes, where only the notes with the exact same onset position and pitch are counted as correct. To make a fair comparison between encoding methods, note onset positions in JG-like encoding were converted to those in the REMI-like format. Ornamentations without duration were not counted.</p>
</div>
</section>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Results and Discussion</h3>

<figure id="S6.T1" class="ltx_table">
<table id="S6.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T1.2.1.1" class="ltx_tr">
<th id="S6.T1.2.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding:0.25pt 4.0pt;"></th>
<th id="S6.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding:0.25pt 4.0pt;" colspan="2">
<span id="S6.T1.2.1.1.2.1" class="ltx_text ltx_font_italic">Piri</span> to <span id="S6.T1.2.1.1.2.2" class="ltx_text ltx_font_italic">Geom.</span>
</th>
<th id="S6.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding:0.25pt 4.0pt;" colspan="2">Every to <span id="S6.T1.2.1.1.3.1" class="ltx_text ltx_font_italic">Daeg.</span>
</th>
</tr>
<tr id="S6.T1.2.2.2" class="ltx_tr">
<th id="S6.T1.2.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row" style="padding:0.25pt 4.0pt;"></th>
<th id="S6.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:0.25pt 4.0pt;">len-mat</th>
<th id="S6.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding:0.25pt 4.0pt;">F1</th>
<th id="S6.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:0.25pt 4.0pt;">len-mat</th>
<th id="S6.T1.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:0.25pt 4.0pt;">F1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T1.2.3.1" class="ltx_tr">
<th id="S6.T1.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.25pt 4.0pt;">JG-like</th>
<td id="S6.T1.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.25pt 4.0pt;">0.942</td>
<td id="S6.T1.2.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.25pt 4.0pt;">0.679</td>
<td id="S6.T1.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.25pt 4.0pt;">1.0</td>
<td id="S6.T1.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.25pt 4.0pt;">0.614</td>
</tr>
<tr id="S6.T1.2.4.2" class="ltx_tr">
<th id="S6.T1.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.25pt 4.0pt;">REMI-like</th>
<td id="S6.T1.2.4.2.2" class="ltx_td ltx_align_center" style="padding:0.25pt 4.0pt;">0.923</td>
<td id="S6.T1.2.4.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.25pt 4.0pt;">0.567</td>
<td id="S6.T1.2.4.2.4" class="ltx_td ltx_align_center" style="padding:0.25pt 4.0pt;">1.0</td>
<td id="S6.T1.2.4.2.5" class="ltx_td ltx_align_center" style="padding:0.25pt 4.0pt;">0.532</td>
</tr>
<tr id="S6.T1.2.5.3" class="ltx_tr">
<th id="S6.T1.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.25pt 4.0pt;">ABC-like</th>
<td id="S6.T1.2.5.3.2" class="ltx_td ltx_align_center" style="padding:0.25pt 4.0pt;">1.0</td>
<td id="S6.T1.2.5.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.25pt 4.0pt;">0.704</td>
<td id="S6.T1.2.5.3.4" class="ltx_td ltx_align_center" style="padding:0.25pt 4.0pt;">0.903</td>
<td id="S6.T1.2.5.3.5" class="ltx_td ltx_align_center" style="padding:0.25pt 4.0pt;">0.542</td>
</tr>
<tr id="S6.T1.2.6.4" class="ltx_tr">
<th id="S6.T1.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding:0.25pt 4.0pt;">JG w/o Counter</th>
<td id="S6.T1.2.6.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.25pt 4.0pt;">0.135</td>
<td id="S6.T1.2.6.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.25pt 4.0pt;">0.043</td>
<td id="S6.T1.2.6.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.25pt 4.0pt;">0.269</td>
<td id="S6.T1.2.6.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.25pt 4.0pt;">0.052</td>
</tr>
<tr id="S6.T1.2.7.5" class="ltx_tr">
<th id="S6.T1.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding:0.25pt 4.0pt;">REMI w/o Counter</th>
<td id="S6.T1.2.7.5.2" class="ltx_td ltx_align_center" style="padding:0.25pt 4.0pt;">0.269</td>
<td id="S6.T1.2.7.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.25pt 4.0pt;">0.081</td>
<td id="S6.T1.2.7.5.4" class="ltx_td ltx_align_center" style="padding:0.25pt 4.0pt;">0.192</td>
<td id="S6.T1.2.7.5.5" class="ltx_td ltx_align_center" style="padding:0.25pt 4.0pt;">0.039</td>
</tr>
<tr id="S6.T1.2.8.6" class="ltx_tr">
<th id="S6.T1.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding:0.25pt 4.0pt;">ABC w/o Counter</th>
<td id="S6.T1.2.8.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.25pt 4.0pt;">0.403</td>
<td id="S6.T1.2.8.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding:0.25pt 4.0pt;">0.090</td>
<td id="S6.T1.2.8.6.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.25pt 4.0pt;">0.115</td>
<td id="S6.T1.2.8.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:0.25pt 4.0pt;">0.016</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T1.3.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Quantitative evaluation results</figcaption>
</figure>
<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">In our sequential generation process, melody for the instrument geomungo, characterized by its low pitch range and simple melodies, is the first to be generated from the initial piri melody. The daegeum, typically featuring the most complex and nuanced melodies among the six instruments, is the last in line.
Table <a href="#S6.T1" title="Table 1 ‣ 6.3 Results and Discussion ‣ 6 Experiment and Results ‣ Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> displays the results of objective evaluation, specifically focusing on geomungo and daegeum.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">For generation of geomungo melodies, ABC-like encoding yields the best results. This appears to be due to the simple and regular melodic structure of the geomungo which fits in well with ABC-like encoding.
On the other hand, in the task of generating daegeum melodies, JG-like encoding achieves higher F<sub id="S6.SS3.p2.1.1" class="ltx_sub">1</sub>-scores. This indicates that JG-like encoding outperforms other methods in generating complex and varied melodies. We also discover that as rhythmic complexity increases, the measure length match rate of ABC-like encoding decreases.</p>
</div>
<div id="S6.SS3.p3" class="ltx_para">
<p id="S6.SS3.p3.1" class="ltx_p">To examine the effectiveness of the beat counter technique, we compare our model that incorporates beat counter with a baseline model that instead employs absolute position embedding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, a technique commonly used in symbolic music generation.</p>
</div>
<div id="S6.SS3.p4" class="ltx_para">
<p id="S6.SS3.p4.1" class="ltx_p">The results in the lower part of Table <a href="#S6.T1" title="Table 1 ‣ 6.3 Results and Discussion ‣ 6 Experiment and Results ‣ Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> show that the models without beat counter fail to generate melodies with appropriate lengths. The problem is less severe in ABC-like encoding, as processing accumulating duration tokens can be easier than counting <span id="S6.SS3.p4.1.1" class="ltx_text ltx_font_italic">jeonggan</span> boundaries. This demonstrates the efficacy of the beat counter technique in JG-like encoding, and its ability to replace traditional positional encoding.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>15th Century Melody Transformation</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">To generate an entire ensemble score using our method, we require an initial input melody with a specified instrument. However, the remaining 15th-century score of <span id="S7.p1.1.1" class="ltx_text ltx_font_italic">Chihwapyeong</span> and <span id="S7.p1.1.2" class="ltx_text ltx_font_italic">Chwipunghyeong</span> only provide a single melody without any mention of instruments. It also features rhythmic groupings of eight beats, which is rare in court music that is played today. We therefore need to transform the old melody for a specific instrument used in court music; to maintain the outline of the original melody while achieving plausible transformation, we train a masked language model on our <span id="S7.p1.1.3" class="ltx_text ltx_font_italic">Jeongganbo</span> dataset before infilling the 15th-century melody.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>BERT-like Masked Language Model</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">Bidirectional Encoder Representations from Transformers (BERT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> is a self-supervised language representation learning model that uses a bidirectional transformer instead of a causal transformer decoder. It is trained with a masked language model (MLM) objective, where tokens in the input sentence are randomly masked and the model predicts the original vocabulary ID of said masked tokens. Because of its advantage in exploiting bidirectional context, BERT-like models have also been adapted for music audio generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and symbolic music generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> along with representation-learning purpose adaptation on symbolic music <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.</p>
</div>
<section id="S7.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.1 </span>Piano-roll-like Encoding</h4>

<div id="S7.SS1.SSS1.p1" class="ltx_para">
<p id="S7.SS1.SSS1.p1.1" class="ltx_p">One of the main limitations of using a BERT-like model for generative tasks is that the sequence of given (unmasked) tokens and masked tokens has to be pre-defined. This means that one has to decide the number and position of new tokens to be inserted for a given original sequence. To avoid this, we use piano-roll-like encoding for the MLM, a technique widely employed in works on music generation with limited rhythmic patterns such as in Bach Chorales <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.
Here, each <span id="S7.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_italic">jeonggan</span> is represented as six frames, with each frame including features for symbol (pitch or <span id="S7.SS1.SSS1.p1.1.2" class="ltx_text ltx_font_italic">sigimsae</span> with duration) and for ornamentation. We also apply the aforementioned beat counter in piano-roll encoding.</p>
</div>
</section>
<section id="S7.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.2 </span>Training with Masking</h4>

<div id="S7.SS1.SSS2.p1" class="ltx_para">
<p id="S7.SS1.SSS2.p1.1" class="ltx_p">Following examples in MusicBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, we train the model with masked language model objective with various masking methods: i) masking 5% of frames, ii) replacing 5% of frames, iii) masking 20% of note onsets, iv) replacing 10% of note onsets, v) erasing 10% of note onsets, vi) masking the entire 6 frames of 15% of <span id="S7.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_italic">jeonggans</span>, and vii) masking 50% of ornamentations.</p>
</div>
<div id="S7.SS1.SSS2.p2" class="ltx_para">
<p id="S7.SS1.SSS2.p2.1" class="ltx_p">Though the model can be trained to handle an arbitrary number of input instruments, we only train the model with a single instrument as with our orchestration transformer, since the main intended usage of the model is to create variations of a single melody. We train a 12-layer model with the same dataset and hyperparameter settings as with the orchestration model.</p>
</div>
</section>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Inference Procedure</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">For converting and performing monophonic melodies, we opt for a 30x <math id="S7.SS2.p1.1.m1.1" class="ltx_Math" alttext="\eighthnote" display="inline"><semantics id="S7.SS2.p1.1.m1.1a"><mi mathvariant="normal" id="S7.SS2.p1.1.m1.1.1" xref="S7.SS2.p1.1.m1.1.1.cmml">𝅘𝅥𝅮</mi><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.1.m1.1b"><ci id="S7.SS2.p1.1.m1.1.1.cmml" xref="S7.SS2.p1.1.m1.1.1">𝅘𝅥𝅮</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.1.m1.1c">\eighthnote</annotation></semantics></math> span which equals to 10 <span id="S7.SS2.p1.1.1" class="ltx_text ltx_font_italic">jeonggans</span>.
This also corresponds to the rhythmic pattern of the 4—7th movement of <span id="S7.SS2.p1.1.2" class="ltx_text ltx_font_italic">Yeominlak</span>. The original <span id="S7.SS2.p1.1.3" class="ltx_text ltx_font_italic">Chihwapyeong</span> and <span id="S7.SS2.p1.1.4" class="ltx_text ltx_font_italic">Chwipunghyeong</span> melody, which can be interpreted in an 8/8 time signature, were modified by strategically inserting empty <span id="S7.SS2.p1.1.5" class="ltx_text ltx_font_italic">jeonggans</span> to the 5th and 7th positions, to imitate <span id="S7.SS2.p1.1.6" class="ltx_text ltx_font_italic">Yeominlak</span>’s rhythmic pattern.
Utilizing the masked language model, the modified melodies were seamlessly transformed into a piri melody. Piri, a double-reed instrument known for its loud volume, was chosen as the main instrument for conveying the original melody due to its prominent role in contemporary court music.</p>
</div>
<div id="S7.SS2.p2" class="ltx_para">
<p id="S7.SS2.p2.1" class="ltx_p">As the models were all trained on 4-measure chunks, we generate the full sequence of 512 or 132 measures using a moving window, providing two measures of previously generated output as teacher-forcing inputs and generating one more measure for each four-measure input. These were applied in a similar manner to both melody transformation and orchestral part generation.
Once the melody is transformed into a piri melody, we feed it to the orchestral transformer to generate parts for five other instruments. We sequentially generate for each instrument with the previously generated part as input. The final generation order is as follows: piri, geomungo, gayageum, ajaeng, haegeum, and daegeum.</p>
</div>
<div id="S7.SS2.p3" class="ltx_para">
<p id="S7.SS2.p3.1" class="ltx_p">Following the initial generation of melodies for all six instruments, we perform a refinement step. Here, each instrument’s melody is regenerated with the melodies of the other five as input. This additional process helps to reinforce the melodies that initially had to be generated without the context of the other instruments.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Expert Reviews</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">The Court Music Orchestra of the National Gugak Center performed the generated <span id="S7.SS3.p1.1.1" class="ltx_text ltx_font_italic">Chihwapyeong</span> and <span id="S7.SS3.p1.1.2" class="ltx_text ltx_font_italic">Chwipunghyeong</span> on the birth anniversary of King Sejong at Gyeongbokgung Palace on May 14th, 2024. They performed it again at the National Gugak Center on June 2nd, 2024 with an introduction to technical background by the authors. Due to time constraints, only partial excerpts from the entire score were performed.</p>
</div>
<div id="S7.SS3.p2" class="ltx_para">
<p id="S7.SS3.p2.1" class="ltx_p">The musicians gave positive opinions such as <span id="S7.SS3.p2.1.1" class="ltx_text ltx_font_italic">“genre-specific rhythm and melodic flow were well-represented”</span> and <span id="S7.SS3.p2.1.2" class="ltx_text ltx_font_italic">“the generated pieces presented ornamentation techniques and melodic progressions specialized for each instrument.”</span>
Still, there were a few instances where notes that did not fit the scale appeared, and when notes outside the appropriate range were present, the performers had to alter or omit them or change their octave to perform the piece.
However, the generated results were acknowledged to closely resemble the target style of Yeominlak. Thus, the Court Music Orchestra decided to play the pieces in a similar ensemble size to Yeominlak without further modification.</p>
</div>
<div id="S7.SS3.p3" class="ltx_para">
<p id="S7.SS3.p3.1" class="ltx_p">We additionally evaluate the generated scores, focusing on the effects of the refinement step. The evaluation criteria were carefully selected to assess aspects that require a deep understanding of the genre. These criteria include 1) the appropriateness of the scale and range for each instrument (<span id="S7.SS3.p3.1.1" class="ltx_text ltx_font_italic">scale</span>), 2) the proper use of unique characteristics and ornamentations specific to each instrument (<span id="S7.SS3.p3.1.2" class="ltx_text ltx_font_italic">sigimsae</span>), 3) the suitability of the rhythmic structure of strong and weak beats (<span id="S7.SS3.p3.1.3" class="ltx_text ltx_font_italic">rhythm</span>), and 4) the harmony and coherence among the instruments when performed together as an ensemble (<span id="S7.SS3.p3.1.4" class="ltx_text ltx_font_italic">harmony</span>).</p>
</div>
<div id="S7.SS3.p4" class="ltx_para">
<p id="S7.SS3.p4.1" class="ltx_p">Seven employees from the National Gugak Center who majored in Korean traditional music instruments or theory participated in a subjective survey.
We name the pre-refinement generation results piece A, and the final output after the refinement step piece B. The evaluators were not informed of this distinction.
The participants assessed the pieces for the four criteria on a 5-point scale (1-5) and provided qualitative feedback on the two compositions. The results are summarized in Table <a href="#S7.T2" title="Table 2 ‣ 7.3 Expert Reviews ‣ 7 15th Century Melody Transformation ‣ Six Dragons Fly Again: Reviving 15th-Century Korean Court Music with Transformers and Novel Encoding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
These results demonstrate that the proposed refinement process effectively enhances the overall quality of the generated music, especially for <span id="S7.SS3.p4.1.1" class="ltx_text ltx_font_italic">sigimsae</span> of each instrument.</p>
</div>
<figure id="S7.T2" class="ltx_table">
<table id="S7.T2.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S7.T2.8.9.1" class="ltx_tr">
<th id="S7.T2.8.9.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-top:0.25pt;padding-bottom:0.25pt;"></th>
<th id="S7.T2.8.9.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S7.T2.8.9.1.2.1" class="ltx_text ltx_font_italic">No</span> Refinement</th>
<th id="S7.T2.8.9.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.25pt;padding-bottom:0.25pt;">
<span id="S7.T2.8.9.1.3.1" class="ltx_text ltx_font_italic">With</span> Refinement</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S7.T2.2.2" class="ltx_tr">
<th id="S7.T2.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">Scale</th>
<td id="S7.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">4.0 (<math id="S7.T2.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T2.1.1.1.m1.1a"><mo id="S7.T2.1.1.1.m1.1.1" xref="S7.T2.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T2.1.1.1.m1.1b"><csymbol cd="latexml" id="S7.T2.1.1.1.m1.1.1.cmml" xref="S7.T2.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.1.1.1.m1.1c">\pm</annotation></semantics></math>0.53)</td>
<td id="S7.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.25pt;padding-bottom:0.25pt;">4.0 (<math id="S7.T2.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T2.2.2.2.m1.1a"><mo id="S7.T2.2.2.2.m1.1.1" xref="S7.T2.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T2.2.2.2.m1.1b"><csymbol cd="latexml" id="S7.T2.2.2.2.m1.1.1.cmml" xref="S7.T2.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.2.2.2.m1.1c">\pm</annotation></semantics></math>0.53)</td>
</tr>
<tr id="S7.T2.4.4" class="ltx_tr">
<th id="S7.T2.4.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.25pt;padding-bottom:0.25pt;">Sigimsae</th>
<td id="S7.T2.3.3.1" class="ltx_td ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;">3.4 (<math id="S7.T2.3.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T2.3.3.1.m1.1a"><mo id="S7.T2.3.3.1.m1.1.1" xref="S7.T2.3.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T2.3.3.1.m1.1b"><csymbol cd="latexml" id="S7.T2.3.3.1.m1.1.1.cmml" xref="S7.T2.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.3.3.1.m1.1c">\pm</annotation></semantics></math>0.73)</td>
<td id="S7.T2.4.4.2" class="ltx_td ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;">4.0 (<math id="S7.T2.4.4.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T2.4.4.2.m1.1a"><mo id="S7.T2.4.4.2.m1.1.1" xref="S7.T2.4.4.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T2.4.4.2.m1.1b"><csymbol cd="latexml" id="S7.T2.4.4.2.m1.1.1.cmml" xref="S7.T2.4.4.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.4.4.2.m1.1c">\pm</annotation></semantics></math>0.53)</td>
</tr>
<tr id="S7.T2.6.6" class="ltx_tr">
<th id="S7.T2.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.25pt;padding-bottom:0.25pt;">Rhythm</th>
<td id="S7.T2.5.5.1" class="ltx_td ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;">2.9 (<math id="S7.T2.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T2.5.5.1.m1.1a"><mo id="S7.T2.5.5.1.m1.1.1" xref="S7.T2.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T2.5.5.1.m1.1b"><csymbol cd="latexml" id="S7.T2.5.5.1.m1.1.1.cmml" xref="S7.T2.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.5.5.1.m1.1c">\pm</annotation></semantics></math>0.35)</td>
<td id="S7.T2.6.6.2" class="ltx_td ltx_align_center" style="padding-top:0.25pt;padding-bottom:0.25pt;">3.3 (<math id="S7.T2.6.6.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T2.6.6.2.m1.1a"><mo id="S7.T2.6.6.2.m1.1.1" xref="S7.T2.6.6.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T2.6.6.2.m1.1b"><csymbol cd="latexml" id="S7.T2.6.6.2.m1.1.1.cmml" xref="S7.T2.6.6.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.6.6.2.m1.1c">\pm</annotation></semantics></math>0.45)</td>
</tr>
<tr id="S7.T2.8.8" class="ltx_tr">
<th id="S7.T2.8.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-top:0.25pt;padding-bottom:0.25pt;">Harmony</th>
<td id="S7.T2.7.7.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.25pt;padding-bottom:0.25pt;">2.9 (<math id="S7.T2.7.7.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T2.7.7.1.m1.1a"><mo id="S7.T2.7.7.1.m1.1.1" xref="S7.T2.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T2.7.7.1.m1.1b"><csymbol cd="latexml" id="S7.T2.7.7.1.m1.1.1.cmml" xref="S7.T2.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.7.7.1.m1.1c">\pm</annotation></semantics></math>0.83)</td>
<td id="S7.T2.8.8.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.25pt;padding-bottom:0.25pt;">3.3 (<math id="S7.T2.8.8.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T2.8.8.2.m1.1a"><mo id="S7.T2.8.8.2.m1.1.1" xref="S7.T2.8.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T2.8.8.2.m1.1b"><csymbol cd="latexml" id="S7.T2.8.8.2.m1.1.1.cmml" xref="S7.T2.8.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.8.8.2.m1.1c">\pm</annotation></semantics></math>0.70)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S7.T2.10.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>the average and std of opinion scores from 7 judges for systems with and without refinement.</figcaption>
</figure>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">Throughout this work, we explored how music generation models can resurrect ancient melodies into new compositions that meet style of current-day Korean court music.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">Venturing into relatively uncharted territory, we approached each step meticulously—from data curation and parsing to model architecture design—while carefully considering the unique nuances of the musical tradition. To enhance the quality of the generated outputs, we proposed a novel encoding framework and validated its effectiveness through objective and subjective measures. This endeavour to tackle an underrepresented non-Western music genre through diverse MIR lenses hopefully expands the horizons of the field.</p>
</div>
<div id="S8.p3" class="ltx_para">
<p id="S8.p3.1" class="ltx_p">The <span id="S8.p3.1.1" class="ltx_text ltx_font_italic">Jeongganbo</span> dataset and its conversion to Western staff notation in MusicXML is available online, along with other code of this project, and video recording of the performance.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/MALerLab/SejongMusic" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/MALerLab/SejongMusic</a></span></span></span> To the best of our knowledge, this will be the first dataset of machine-readable <span id="S8.p3.1.2" class="ltx_text ltx_font_italic">Jeongganbo</span>. We believe that this dataset can significantly contribute to computational ethnomusicology beyond its usage as a training dataset for music generation demonstrated in this paper.</p>
</div>
<div id="S8.p4" class="ltx_para">
<p id="S8.p4.1" class="ltx_p">We also provide an interactive web demo<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://six-dragons-fly-again.site/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://six-dragons-fly-again.site/</a></span></span></span> that showcases our proposed generative model. While this project focused on reviving melodies from the 15th-century, the web demo allows users to input their own melodies and create orchestrations of Korean court music.
The interactive platform enables users to directly engage with the generative model in the web browser.</p>
</div>
<div id="S8.p5" class="ltx_para">
<p id="S8.p5.1" class="ltx_p">We hope that this project contributes to moving closer to leveraging machine learning to make traditional music more accessible and enjoyable for modern audiences.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Acknowledgements</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">We sincerely appreciate the National Gugak Center and its staff who supported this project, including director-general Kim Youngwoon (김영운), head of the research bureau Kim Myung-suk (김명석), research officers Park Jeonggyeong (박정경) and Han Jungwon (한정원). We are deeply grateful to the musicians of the Court Music Orchestra for their invaluable contributions and efforts to vitalize our humble results.
This research was also supported by the National R&amp;D Program through the National Research Foundation of Korea (NRF) funded by the Korean Government (MSIT) (RS-2023-00252944, Korean Traditional Gagok Generation Using Deep Learning).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y. Kim, “Chapter Ⅲ. critical assessment : The rhythmic interpretation of jeongganbo,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Korean Musicology Series, vol. 4</em>, 2010.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
National Gugak Center, “Publications,” <a target="_blank" href="https://www.gugak.go.kr/site/program/board/basicboard/view?menuid=001003002005&amp;pagesize=10&amp;boardtypeid=24&amp;boardid=13154&amp;lang=en" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.gugak.go.kr/site/program/board/basicboard/view?menuid=001003002005&amp;pagesize=10&amp;boardtypeid=24&amp;boardid=13154&amp;lang=en</a>, 2024.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
G. Micchi, M. Gotham, and M. Giraud, “Not all roads lead to Rome: Pitch representation and model architecture for automatic harmonic analysis,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Transactions of the Int. Society for Music Information Retrieval (TISMIR)</em>, vol. 3, no. 1, pp. 42–54, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
B. Sturm, “The Ai music generation challenge 2022: Summary and results,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proc. of the 4th Conference on AI Music Creativity (AIMC 2023)</em>, Brighton, UK, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
M. Gotham, K. Song, N. Böhlefeld, and A. Elgammal, “Beethoven X: Es könnte sein!(it could be!),” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proc. of the 3rd Conference on AI Music Creativity (AIMC 2022)</em>, Online, 2022, pp. 13–15.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
M. Ebrahimi, B. Majidi, and M. Eshghi, “Procedural composition of traditional Persian music using deep neural networks,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proc. of 5th Conference on Knowledge Based Engineering and Innovation (KBEI)</em>.   Tehran, Iran: IEEE, 2019, pp. 521–525.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J. Luo, X. Yang, S. Ji, and J. Li, “MG-VAE: Deep chinese folk songs generation with specific regional styles,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proc. of the 8th Conference on Sound and Music Technology (CSMT) Revised Selected Papers</em>.   Shanxi, China: Springer, 2020, pp. 93–106.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. E. Gnanadesikan, <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">The writing revolution: Cuneiform to the internet</em>.   John Wiley &amp; Sons, 2008, vol. 8.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Y. Kim, “ChapterⅡ. Korean notational systems,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Korean Musicology Series, vol. 4</em>, 2010.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
R. Koehler <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">et al.</em>, <em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic">Traditional music: sounds in harmony with nature</em>.   Seoul Selection, 2015.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
D. Kim, D. Han, D. Jeong, and J. J. Valero-Mas, “On the automatic recognition of jeongganbo music notation: dataset and approach,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Preprint on Research Square</em>, 2024.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
B. Sturm, J. F. Santos, and I. Korshunova, “Folk music style modelling by recurrent neural networks with long short term memory units,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">late-breaking demo session of 16th Int. Society for Music Information Retrieval Conf.</em>, Málaga, Spain, 2015.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
B. L. Sturm, J. F. Santos, O. Ben-Tal, and I. Korshunova, “Music transcription modelling and composition using deep learning,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1604.08723</em>, 2016.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Y.-S. Huang and Y.-H. Yang, “Pop music transformer: Beat-based modeling and generation of expressive pop piano compositions,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proc. of the 28th ACM International conference on multimedia</em>, New York, NY, USA, 2020, pp. 1180–1188.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proc. of 31st Conference on Neural Information Processing Systems (NIPS 2017)</em>, Long Beach, CA, USA, 2017.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, I. Simon, C. Hawthorne, N. Shazeer, A. M. Dai, M. D. Hoffman, M. Dinculescu, and D. Eck, “Music transformer,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proc. of The 7th International Conference on Learning Representations</em>, New Orleans, LA, USA, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
B. Yu, P. Lu, R. Wang, W. Hu, X. Tan, W. Ye, S. Zhang, T. Qin, and T.-Y. Liu, “Museformer: Transformer with fine- and coarse-grained attention for music generation,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proc. of The 36th Annual Conference on Neural Information Processing Systems</em>, New Orleans, LA, USA, 2022, pp. 1376–1388.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Y.-J. Shih, S.-L. Wu, F. Zalkow, M. Müller, and Y.-H. Yang, “Theme transformer: Symbolic music generation with theme-conditioned transformer,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Multimedia</em>, vol. 25, pp. 3495–3508, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin, “Convolutional sequence to sequence learning,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proc. of the 34th International Conference on Machine Learning</em>, Sydney, Australia, 2017, pp. 1243–1252.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Y. Ren, J. He, X. Tan, T. Qin, Z. Zhao, and T.-Y. Liu, “Popmag: Pop music accompaniment generation,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proc. of the 28th ACM Int. Conf. on Multimedia</em>, 2020, pp. 1198–1206.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
D. Jeong, “Virtuosotune: Hierarchical melody language model,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">IEIE Transactions on Smart Processing &amp; Computing</em>, vol. 12, no. 4, pp. 329–333, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Z. Wang and G. Xia, “MuseBERT: Pre-training music representation for music understanding and controllable generation,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proc. of the 22nd Int. Society for Music Information Retrieval Conf.</em>, Online, 2021, pp. 722–729.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Z. Guo, J. Kang, and D. Herremans, “A domain-knowledge-inspired music embedding space and a novel attention mechanism for symbolic music modeling,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 37, no. 4, 2023, pp. 5070–5077.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Mixed precision training,” <em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1710.03740</em>, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of deep bidirectional transformers for language understanding,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proc. of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, Minneapolis, Minnesota, USA, 2019, pp. 4171–4186.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
H. Flores García, P. Seetharaman, R. Kumar, and B. Pardo, “Vampnet: Music generation via masked acoustic token modeling,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proc. of the 24th Int. Society for Music Information Retrieval Conf.</em>, Milan, Italy, 2023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
R. Dahale, V. Talwadker, P. Rao, and P. Verma, “Generating coherent drum accompaniment with fills and improvisations,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proc. of the 23rd Int. Society for Music Information Retrieval Conf.</em>, Bengaluru, India, 2022.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
L. Casini, N. Jonason, and B. L. T. Sturm, “Investigating the viability of masked language modeling for symbolic music generation in abc-notation,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proc. of 13th International Conference on Computational Intelligence in Music, Sound, Art and Design</em>, Aberystwyth, UK, 2024, pp. 84–96.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
M. Zeng, X. Tan, R. Wang, Z. Ju, T. Qin, and T.-Y. Liu, “MusicBERT: Symbolic music understanding with large-scale pre-training,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</em>, Online, 2021, pp. 791–800.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
F. Liang, M. Gotham, M. Johnson, and J. Shotton, “Automatic stylistic composition of bach chorales with deep LSTM,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proc. of 18th Int. Society for Music Information Retrieval Conf.</em>, Suzhou, China, 2017, pp. 449–456.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
G. Hadjeres, F. Pachet, and F. Nielsen, “DeepBach: a steerable model for Bach chorales generation,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proc. of the 34th International Conference on Machine Learning</em>, Sydney, Australia, 2017, pp. 1362–1371.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
C.-Z. A. Huang, T. Cooijmans, A. Roberts, A. Courville, and D. Eck, “Counterpoint by convolution,” in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proc. of 18th Int. Society for Music Information Retrieval Conf.</em>, Suzhou, China, 2017.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
E. Choi, H. Kim, J. Nam, and D. Jeong, “Teaching chorale generation model to avoid parallel motions,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proc. of The 16th International Symposium on Computer Music Multidisciplinary Research (CMMR 2023)</em>, Tokyo, Japan, 2023.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:creator" content="D. Han"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="Proceedings of the 25th Int. Society for Music Information Retrieval Conf."></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.01094" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.01096" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.01096">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.01096" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.01097" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 12:42:59 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
