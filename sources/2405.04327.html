<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.04327] Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation</title><meta property="og:description" content="In the task of talking face generation, the objective is to generate a face video with lips synchronized to the corresponding audio while preserving visual details and identity information. Current methods face the cha…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.04327">

<!--Generated on Wed Jun  5 15:32:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Audio-Visual Speech Representation Expert for Enhanced 
<br class="ltx_break">Talking Face Video Generation and Evaluation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dogucan Yaman<sup id="id1.1.id1" class="ltx_sup">1</sup>   Fevziye Irem Eyiokur<sup id="id2.2.id2" class="ltx_sup">1</sup>   Leonard Bärmann<sup id="id3.3.id3" class="ltx_sup">1</sup>   Seymanur Aktı<sup id="id4.4.id4" class="ltx_sup">1</sup> 
<br class="ltx_break">Hazım Kemal Ekenel<sup id="id5.5.id5" class="ltx_sup">2</sup>   Alexander Waibel<sup id="id6.6.id6" class="ltx_sup">1,3</sup> 
<br class="ltx_break"><sup id="id7.7.id7" class="ltx_sup">1</sup>Karlsruhe Institute of Technology, <sup id="id8.8.id8" class="ltx_sup">2</sup>Istanbul Technical University, <sup id="id9.9.id9" class="ltx_sup">3</sup>Carnegie Mellon University
<br class="ltx_break"><span id="id10.10.id10" class="ltx_text ltx_font_typewriter" style="font-size:90%;">dogucan.yaman@kit.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.id1" class="ltx_p">In the task of talking face generation, the objective is to generate a face video with lips synchronized to the corresponding audio while preserving visual details and identity information. Current methods face the challenge of learning accurate lip synchronization while avoiding detrimental effects on visual quality, as well as robustly evaluating such synchronization. To tackle these problems, we propose utilizing an audio-visual speech representation expert (AV-HuBERT) for calculating lip synchronization loss during training. Moreover, leveraging AV-HuBERT’s features, we introduce three novel lip synchronization evaluation metrics, aiming to provide a comprehensive assessment of lip synchronization performance. Experimental results, along with a detailed ablation study, demonstrate the effectiveness of our approach and the utility of the proposed evaluation metrics.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The goal of talking face generation is to create a video based on provided face and audio sequences, seeking synchronized lip movements that match the given audio while maintaining the identity and visual details.
This task has gained considerable interest recently for its diverse applications, such as face dubbing and enhancement in video conferencing tools, film dubbing, and content creation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.04327/assets/x1.png" id="S1.F1.sf1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S1.F1.sf1.3.2" class="ltx_text" style="font-size:90%;">Cosine similarity</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.04327/assets/x2.png" id="S1.F1.sf2.g1" class="ltx_graphics ltx_img_landscape" width="332" height="249" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S1.F1.sf2.3.2" class="ltx_text" style="font-size:90%;">Lip-sync loss</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.04327/assets/x3.png" id="S1.F1.sf3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S1.F1.sf3.3.2" class="ltx_text" style="font-size:90%;">Cosine similarity</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F1.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.04327/assets/x4.png" id="S1.F1.sf4.g1" class="ltx_graphics ltx_img_landscape" width="332" height="249" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S1.F1.sf4.3.2" class="ltx_text" style="font-size:90%;">Lip-sync loss</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.4.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.5.2" class="ltx_text" style="font-size:90%;">Cosine similarity and lip-sync loss between GT audio-lip pairs on random LRS2 test samples, showcasing the instability of SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> <span id="S1.F1.5.2.1" class="ltx_text ltx_font_bold">(a, b)</span> and more robust performance of AV-HuBERT <span id="S1.F1.5.2.2" class="ltx_text ltx_font_bold">(c, d)</span>.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In the talking face generation task, visual quality of the generated faces and audio-lip synchronization (lip sync) are essential but also challenging aspects to have a natural video.
Since visual artifacts and out-of-sync lip movements are easily recognizable by the audience, they significantly diminish the naturalness of the dubbed video.
While visual quality is addressed with approaches across various domains, lip sync takes precedence as it is particular and crucial for talking face generation in maintaining the naturalness of dubbed videos.
Up to now, several different works tackled the challenges of learning lip sync.
The widely adopted method involves extracting features from both the audio and face sequences by a model that has been contrastively trained with audio-face pairs to learn lip sync.
These features are then compared to measure the synchronization between the two modalities (e.g., with cosine similarity).
Therefore, it is essential to extract meaningful as well as robust audio and visual features, as the speech representations from both modalities significantly influence the synchronization measurement.
For this, the most common method is using a <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">lip-expert</span>, which is a slightly modified and retrained version of SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Therefore, SyncNet name is being used for the original SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> and <span id="footnote1.1" class="ltx_text ltx_font_italic">lip-expert</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> interchangeably, as we also do it hereinafter.</span></span></span>.
Specifically, the features from the lips and audio sequence are extracted by the SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> and the cosine similarity along with the cross-entropy loss is computed.
This loss strategy is called <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">lip-sync loss</span>.
Significant advancements have been made since the introduction of using this SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> and most of the methods have benefited from this approach to learn lip sync in the literature.
Recently, TalkLip <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> proposes using a lip-reading expert, AV-HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>, for audio and visual feature extraction.
Then, contrastive learning is employed to learn lip sync, yielding superior performance.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In  <a href="#S1.F1.sf1" title="In Figure 1 ‣ 1 Introduction ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figs.</span> <span class="ltx_text ltx_ref_tag">1(a)</span></a> and <a href="#S1.F1.sf2" title="Figure 1(b) ‣ Figure 1 ‣ 1 Introduction ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(b)</span></a>, we share our analysis about the performance of SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> on LRS2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> ground-truth (GT) audio-lip pairs.
The cosine similarity and lip-sync loss show fluctuations even on GT samples, contrary to the anticipated stable and high performance.
This outcome implies that SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> has significant stability and reliability issues that lead to poor lip sync performance.
Moreover, we empirically find that using SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> causes severe visual quality issues and unstable training, despite enhanced lip sync performance.
To address these problems, inspired by TalkLip <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>, we employ a pretrained audio-visual speech representation learning model, AV-HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>, which was finetuned for the lip reading task, to extract audio and lip features. In contrast to TalkLip, we utilize cross-entropy-based lip-sync loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> to guide our model during training, providing a stabilized training signal (<a href="#S1.F1.sf3" title="In Figure 1 ‣ 1 Introduction ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figs.</span> <span class="ltx_text ltx_ref_tag">1(c)</span></a> and <a href="#S1.F1.sf4" title="Figure 1(d) ‣ Figure 1 ‣ 1 Introduction ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(d)</span></a>).
We refer to this as approach as <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">unsupervised</span>.
In addition to this, we investigate two further methods as loss functions.
Specifically, we employ AV-HuBERT features and compute the lip-sync loss by using the visual features of the generated faces and GT faces.
We call this approach <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">visual-visual</span>, as the audio is not involved.
We also obtain features from generated face-audio pairs and GT face-audio pairs with AV-HuBERT model for lip-sync loss and term this <span id="S1.p3.1.3" class="ltx_text ltx_font_italic">multimodal</span>, since we acquire the features from the multimodal representation (face-audio pairs).
We conduct an ablation study about lip sync learning by comparing these introduced approaches and present the results in <a href="#S4.SS5" title="4.5 Ablation Study ‣ 4 Experimental Results ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.5</span></a>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.3" class="ltx_p">Besides training a model with high-quality audio-lip synchronization, proper and robust evaluation of such capabilities is another key aspect of talking face generation, essential for analyzing and comparing different methods.
One of the first lip sync evaluation metrics is Mouth Landmark Distance (LMD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>, focusing on computing the distance between the landmarks in the mouth region of the generated faces and GT faces.
However, this metric does not disentangle the lip sync from visual factors, as it is sensitive against shifting in the spatial domain.
Additionally, lips with different articulatory parameters (e.g., aperture and spreading) yield poor LMD score, although they are still synchronized.
More recent metrics, LSE-C &amp; -D, are based on SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> features and measure the confidence and distance scores to represent lip sync.
The advantage of these metrics is that they do not require GT data, directly measuring the alignment between audio and faces.
However, unstable SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> performance makes these two metrics unreliable and also vulnerable to affine transformation.
One of the main reasons of this are the poor shift-invariant characteristics of SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>.
To tackle these problems in lip sync evaluation, we propose three novel complementary metrics:
Unsupervised Audio-Visual Synchronization (AVS<sub id="S1.p4.3.1" class="ltx_sub"><span id="S1.p4.3.1.1" class="ltx_text ltx_font_italic">u</span></sub>), Multimodal Audio-Visual Synchronization (AVS<sub id="S1.p4.3.2" class="ltx_sub"><span id="S1.p4.3.2.1" class="ltx_text ltx_font_italic">m</span></sub>), and Visual-only Lip Synchronization (AVS<sub id="S1.p4.3.3" class="ltx_sub"><span id="S1.p4.3.3.1" class="ltx_text ltx_font_italic">v</span></sub>).
We leverage the pretrained AV-HuBERT lip-reading expert for feature extraction and utilize cosine similarity for the score calculation.
The differences and details of these metrics are presented in <a href="#S4.SS1" title="4.1 Metrics ‣ 4 Experimental Results ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>.
Our contributions are summarized as follows:</p>
</div>
<div id="S1.p5" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose to use a pretrained audio-visual speech representation learning model (AV-HuBERT), finetuned for lip reading task, for feature extraction from the audio and face sequences for lip-sync loss in training.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We introduce three novel evaluation metrics by employing AV-HuBERT for feature extraction, yielding less vulnerable and more consistent assessments of performance.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We conduct extensive experiments and ablation studies to demonstrate the effectiveness of our contributions.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Talking face generation</span>
Traditional methods focus on achieving time-aligned videos by choosing the most fitting image-audio pairs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>.
Later on, the facial landmark representation for face generation is employed to obtain a synchronized lip <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">However, these methods suffer from poor lip sync, despite controllable face generation.
Wav2Lip <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> proposes a <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">lip-expert</span>, modified SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>, and also a lip-sync loss to guide the model for lip sync learning.
It shows superior lip sync performance. PC-AVS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite> introduces a pose-controllable 2D talking face generation without using any intermediate representation (e.g., facial landmarks, 3D head representation). GC-AVT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> employs a similar approach as PC-AVS but involves emotion-controllable face generation.
SyncTalkFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite> benefits from an audio-lip memory mechanism to store and retrieve lip motion representation to achieve enhanced lip sync.
On the other hand, VideoReTalking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> reveals fundamental problems in talking face generation, namely the effect of lip motion of the identity reference over the talking face generation.
To solve this problem, they transform the identity reference to have canonical expression with stable and flat lips, which yields improved training stability and lip sync.
DINet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>]</cite> employs a deformation module to improve the pose alignment and lip sync.
Recently, LipFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> introduced a pre-learned facial codebook-based method to learn HR video generation by overcoming existing challenges. TalkLip <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> employs a global audio encoder to capture the content in the speech and also introduces AV-HuBERT-based audio-visual feature extraction for lip sync learning along with contrastive learning.
However, TalkLip has severe visual artifacts, despite superior lip sync.
This method is the closest approach to our talking face generation.
However, we use lip-sync loss instead of contrastive learning and investigate two further methods for lip sync learning.
Moreover, we achieve better visual quality without artifacts.
On the contrary, in  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>, comprehensive analyses are provided regarding lip leakage observed in the reference image as well as the instability issues encountered with SyncNet. They propose a silent-lip generator to manipulate the reference image, mitigating lip leakage, and introduce stabilized synchronization loss to address the stability concerns within the SyncNet model and synchronization loss.
SIDGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> presents crucial analyses of learning synchronization.
They also provide a shift-invariant model, similar to the lip-expert &amp; SyncNet, for feature extraction to guide the model for lip sync learning.
On the other hand, one of the first attempts to generate synchronized lips as a part of a system was done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite> and the follow-up paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite> also contains an entire system that involves speech-to-speech translation and face dubbing.
These systems are also well-suited for utilization in meeting rooms, facilitating seamless communication among individuals conversing in different languages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite>.
In addition to the above 2D-based approaches, Neural Radiance Fields-based (NeRFs) and 3D-based methods aim at synthesizing the entire head by representing the head in the 3D space <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>, <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>, <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>, <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>, <a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>, <a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>.
Though they are capable of controlling the pose and emotion much better as well as achieving enhanced visual quality, they have severe lip sync performance, yielding unrealistic videos.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Lip sync evaluation</span>
The very first proposed metric is Lip Landmark Distance (LMD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>.
However, it has several issues, as we mentioned in <a href="#S1" title="1 Introduction ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S4.SS1" title="4.1 Metrics ‣ 4 Experimental Results ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>.
After the SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> and Sync scores <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> were proposed, they demonstrated a more convenient performance than LMD.
Later, Wav2Lip <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> proposed LSE-C and LSE-D metrics (confidence and distance) using SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> audio and visual features, which became the gold standard in the literature.
Despite the advantage of not requiring GT data, the unreliable performance of SyncNet makes these two metrics vulnerable.
The Word Error Rate (WER) has recently been proposed as an evaluation metric for reading intelligibility <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>.
In this work, we propose three novel lip sync evaluation metrics by utilizing a robust pretrained audio-visual speech representation learning model, AV-HuBERT.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2405.04327/assets/x5.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="428" height="206" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">Illustration of the proposed audio-driven talking face generation model and employed loss functions. </span></figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Talking Face Generation</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We propose a talking face generation approach to enhance lip sync and visual quality by leveraging an audio-visual lip-reading expert named AVHubert <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>.
<a href="#S2.F2" title="In 2 Related Work ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a> provides an overview of our model, which takes three inputs: an identity reference, a bottom-half masked pose reference, and an audio snippet.
The face generator is responsible for synthesizing a set of images to retain synchronized lips with respect to the given audio while preserving the identity and visual quality.
We extract features from the generated samples with the AV-HuBERT and then calculate lip-sync loss in the training along with the other losses.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Face Encoder</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In 2D talking face generation, the main approach is to provide an identity reference and a pose reference to the model, which aims to synthesize a modified version of the pose reference with lip movements matching the given audio.
Since the target image and the pose reference are the same, the mouth region of the pose reference has to be masked.
On the other hand, the identity reference is utilized to preserve the subject’s identity and is randomly sampled from a different part of the input video than the pose reference.
To encode the pose and identity references, we employ two individual encoders.
This approach demonstrates superior performance than using a single encoder for both modalities as in the traditional approaches, since each encoder follows its own objective more effectively, yielding better visual feature representation for both inputs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>.
Our identity and pose encoders share the same architecture.
They have consecutive convolutional blocks and each block involves one strided-convolution layer followed by two non-strided convolution layers.
After each layer, we employ a ReLU activation function <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite> and a batch normalization layer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Audio Encoder</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Our audio encoder embeds the mel-spectrogram representation of the audio snippet, acting as a condition for the face generator to drive the model to generate accurate lip movements, <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="E_{A}(A)=F^{A}\in\mathbb{R}^{1\times 1\times 512}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.2" xref="S3.SS2.p1.1.m1.1.2.cmml"><mrow id="S3.SS2.p1.1.m1.1.2.2" xref="S3.SS2.p1.1.m1.1.2.2.cmml"><msub id="S3.SS2.p1.1.m1.1.2.2.2" xref="S3.SS2.p1.1.m1.1.2.2.2.cmml"><mi id="S3.SS2.p1.1.m1.1.2.2.2.2" xref="S3.SS2.p1.1.m1.1.2.2.2.2.cmml">E</mi><mi id="S3.SS2.p1.1.m1.1.2.2.2.3" xref="S3.SS2.p1.1.m1.1.2.2.2.3.cmml">A</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.2.2.1" xref="S3.SS2.p1.1.m1.1.2.2.1.cmml">​</mo><mrow id="S3.SS2.p1.1.m1.1.2.2.3.2" xref="S3.SS2.p1.1.m1.1.2.2.cmml"><mo stretchy="false" id="S3.SS2.p1.1.m1.1.2.2.3.2.1" xref="S3.SS2.p1.1.m1.1.2.2.cmml">(</mo><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">A</mi><mo stretchy="false" id="S3.SS2.p1.1.m1.1.2.2.3.2.2" xref="S3.SS2.p1.1.m1.1.2.2.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p1.1.m1.1.2.3" xref="S3.SS2.p1.1.m1.1.2.3.cmml">=</mo><msup id="S3.SS2.p1.1.m1.1.2.4" xref="S3.SS2.p1.1.m1.1.2.4.cmml"><mi id="S3.SS2.p1.1.m1.1.2.4.2" xref="S3.SS2.p1.1.m1.1.2.4.2.cmml">F</mi><mi id="S3.SS2.p1.1.m1.1.2.4.3" xref="S3.SS2.p1.1.m1.1.2.4.3.cmml">A</mi></msup><mo id="S3.SS2.p1.1.m1.1.2.5" xref="S3.SS2.p1.1.m1.1.2.5.cmml">∈</mo><msup id="S3.SS2.p1.1.m1.1.2.6" xref="S3.SS2.p1.1.m1.1.2.6.cmml"><mi id="S3.SS2.p1.1.m1.1.2.6.2" xref="S3.SS2.p1.1.m1.1.2.6.2.cmml">ℝ</mi><mrow id="S3.SS2.p1.1.m1.1.2.6.3" xref="S3.SS2.p1.1.m1.1.2.6.3.cmml"><mn id="S3.SS2.p1.1.m1.1.2.6.3.2" xref="S3.SS2.p1.1.m1.1.2.6.3.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.1.m1.1.2.6.3.1" xref="S3.SS2.p1.1.m1.1.2.6.3.1.cmml">×</mo><mn id="S3.SS2.p1.1.m1.1.2.6.3.3" xref="S3.SS2.p1.1.m1.1.2.6.3.3.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.1.m1.1.2.6.3.1a" xref="S3.SS2.p1.1.m1.1.2.6.3.1.cmml">×</mo><mn id="S3.SS2.p1.1.m1.1.2.6.3.4" xref="S3.SS2.p1.1.m1.1.2.6.3.4.cmml">512</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.2"><and id="S3.SS2.p1.1.m1.1.2a.cmml" xref="S3.SS2.p1.1.m1.1.2"></and><apply id="S3.SS2.p1.1.m1.1.2b.cmml" xref="S3.SS2.p1.1.m1.1.2"><eq id="S3.SS2.p1.1.m1.1.2.3.cmml" xref="S3.SS2.p1.1.m1.1.2.3"></eq><apply id="S3.SS2.p1.1.m1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.2.2"><times id="S3.SS2.p1.1.m1.1.2.2.1.cmml" xref="S3.SS2.p1.1.m1.1.2.2.1"></times><apply id="S3.SS2.p1.1.m1.1.2.2.2.cmml" xref="S3.SS2.p1.1.m1.1.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.2.2.2.1.cmml" xref="S3.SS2.p1.1.m1.1.2.2.2">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.2.2.2.2.cmml" xref="S3.SS2.p1.1.m1.1.2.2.2.2">𝐸</ci><ci id="S3.SS2.p1.1.m1.1.2.2.2.3.cmml" xref="S3.SS2.p1.1.m1.1.2.2.2.3">𝐴</ci></apply><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝐴</ci></apply><apply id="S3.SS2.p1.1.m1.1.2.4.cmml" xref="S3.SS2.p1.1.m1.1.2.4"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.2.4.1.cmml" xref="S3.SS2.p1.1.m1.1.2.4">superscript</csymbol><ci id="S3.SS2.p1.1.m1.1.2.4.2.cmml" xref="S3.SS2.p1.1.m1.1.2.4.2">𝐹</ci><ci id="S3.SS2.p1.1.m1.1.2.4.3.cmml" xref="S3.SS2.p1.1.m1.1.2.4.3">𝐴</ci></apply></apply><apply id="S3.SS2.p1.1.m1.1.2c.cmml" xref="S3.SS2.p1.1.m1.1.2"><in id="S3.SS2.p1.1.m1.1.2.5.cmml" xref="S3.SS2.p1.1.m1.1.2.5"></in><share href="#S3.SS2.p1.1.m1.1.2.4.cmml" id="S3.SS2.p1.1.m1.1.2d.cmml" xref="S3.SS2.p1.1.m1.1.2"></share><apply id="S3.SS2.p1.1.m1.1.2.6.cmml" xref="S3.SS2.p1.1.m1.1.2.6"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.2.6.1.cmml" xref="S3.SS2.p1.1.m1.1.2.6">superscript</csymbol><ci id="S3.SS2.p1.1.m1.1.2.6.2.cmml" xref="S3.SS2.p1.1.m1.1.2.6.2">ℝ</ci><apply id="S3.SS2.p1.1.m1.1.2.6.3.cmml" xref="S3.SS2.p1.1.m1.1.2.6.3"><times id="S3.SS2.p1.1.m1.1.2.6.3.1.cmml" xref="S3.SS2.p1.1.m1.1.2.6.3.1"></times><cn type="integer" id="S3.SS2.p1.1.m1.1.2.6.3.2.cmml" xref="S3.SS2.p1.1.m1.1.2.6.3.2">1</cn><cn type="integer" id="S3.SS2.p1.1.m1.1.2.6.3.3.cmml" xref="S3.SS2.p1.1.m1.1.2.6.3.3">1</cn><cn type="integer" id="S3.SS2.p1.1.m1.1.2.6.3.4.cmml" xref="S3.SS2.p1.1.m1.1.2.6.3.4">512</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">E_{A}(A)=F^{A}\in\mathbb{R}^{1\times 1\times 512}</annotation></semantics></math>.
We employ the audio encoder of the SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> without finetuning, since it was trained in conjunction with a face encoder to learn lip synchronization.
In this way, the audio encoder provides a feature representation more suitable for the purpose of generating synchronized lips.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Video Generation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The talking face generator takes the combination of features of three encoders.
We first concatenate the identity features and pose features along the depth dimension.
Before feeding the face generator, we process these features through a convolution layer to reduce the depth.
Subsequently, we concatenate the output with the audio embedding to input the face generator.
Our face generator consists of consecutive transposed convolution layers.
Following each transposed convolution layer, we utilize two convolution layers with a stride one.
Similar to the image encoders, we employ the ReLU activation function <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite> and batch normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> after each layer.
We also apply residual connections <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> between the reciprocal layers of the face encoders (both identity and pose encoders) and the face generator.
This strategy yields the retention of high-level features and increased stabilization throughout the training.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Audio-Visual Speech Representation Expert</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Audio-Visual Hidden Unit BERT (AV-HuBERT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> is a self-supervised representation learning model for audio and visual data.
The model processes a face sequence (the mouth region) with a modified version of ResNet-18 and the Mel-frequency cepstral coefficients (MFCC) of an audio sequence with a linear projection layer followed by a normalization step based on per-frame statistics.
The fusion of audio and visual features is processed through transformer blocks to predict masked cluster assignments.
Thus, AV-HuBERT learned robust audio-visual speech representation.
We utilize the finetuned version of AV-HuBERT for lip reading, since it yields better lip sync as well as reading intelligibility <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>.
In <a href="#S1.F1" title="In 1 Introduction ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, we show the performance of AV-HuBERT on LRS2 GT data for measuring the cosine similarity and lip-sync loss.
The graphs clearly show that the AV-HuBERT has more stable performance and less fluctuation compared to SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> used in Wav2Lip <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Lip Synchronization Loss</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.2" class="ltx_p">We utilize the pretrained AV-HuBERT model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> and extract features from the final layer of the transformer encoder block for audio and video modalities: <math id="S3.SS5.p1.1.m1.2" class="ltx_Math" alttext="F^{A}_{AVH}\in\mathbb{R}^{T\times 768},F^{V}_{AVH}\in\mathbb{R}^{T\times 768}" display="inline"><semantics id="S3.SS5.p1.1.m1.2a"><mrow id="S3.SS5.p1.1.m1.2.2.2" xref="S3.SS5.p1.1.m1.2.2.3.cmml"><mrow id="S3.SS5.p1.1.m1.1.1.1.1" xref="S3.SS5.p1.1.m1.1.1.1.1.cmml"><msubsup id="S3.SS5.p1.1.m1.1.1.1.1.2" xref="S3.SS5.p1.1.m1.1.1.1.1.2.cmml"><mi id="S3.SS5.p1.1.m1.1.1.1.1.2.2.2" xref="S3.SS5.p1.1.m1.1.1.1.1.2.2.2.cmml">F</mi><mrow id="S3.SS5.p1.1.m1.1.1.1.1.2.3" xref="S3.SS5.p1.1.m1.1.1.1.1.2.3.cmml"><mi id="S3.SS5.p1.1.m1.1.1.1.1.2.3.2" xref="S3.SS5.p1.1.m1.1.1.1.1.2.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.1.m1.1.1.1.1.2.3.1" xref="S3.SS5.p1.1.m1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS5.p1.1.m1.1.1.1.1.2.3.3" xref="S3.SS5.p1.1.m1.1.1.1.1.2.3.3.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.1.m1.1.1.1.1.2.3.1a" xref="S3.SS5.p1.1.m1.1.1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS5.p1.1.m1.1.1.1.1.2.3.4" xref="S3.SS5.p1.1.m1.1.1.1.1.2.3.4.cmml">H</mi></mrow><mi id="S3.SS5.p1.1.m1.1.1.1.1.2.2.3" xref="S3.SS5.p1.1.m1.1.1.1.1.2.2.3.cmml">A</mi></msubsup><mo id="S3.SS5.p1.1.m1.1.1.1.1.1" xref="S3.SS5.p1.1.m1.1.1.1.1.1.cmml">∈</mo><msup id="S3.SS5.p1.1.m1.1.1.1.1.3" xref="S3.SS5.p1.1.m1.1.1.1.1.3.cmml"><mi id="S3.SS5.p1.1.m1.1.1.1.1.3.2" xref="S3.SS5.p1.1.m1.1.1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS5.p1.1.m1.1.1.1.1.3.3" xref="S3.SS5.p1.1.m1.1.1.1.1.3.3.cmml"><mi id="S3.SS5.p1.1.m1.1.1.1.1.3.3.2" xref="S3.SS5.p1.1.m1.1.1.1.1.3.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS5.p1.1.m1.1.1.1.1.3.3.1" xref="S3.SS5.p1.1.m1.1.1.1.1.3.3.1.cmml">×</mo><mn id="S3.SS5.p1.1.m1.1.1.1.1.3.3.3" xref="S3.SS5.p1.1.m1.1.1.1.1.3.3.3.cmml">768</mn></mrow></msup></mrow><mo id="S3.SS5.p1.1.m1.2.2.2.3" xref="S3.SS5.p1.1.m1.2.2.3a.cmml">,</mo><mrow id="S3.SS5.p1.1.m1.2.2.2.2" xref="S3.SS5.p1.1.m1.2.2.2.2.cmml"><msubsup id="S3.SS5.p1.1.m1.2.2.2.2.2" xref="S3.SS5.p1.1.m1.2.2.2.2.2.cmml"><mi id="S3.SS5.p1.1.m1.2.2.2.2.2.2.2" xref="S3.SS5.p1.1.m1.2.2.2.2.2.2.2.cmml">F</mi><mrow id="S3.SS5.p1.1.m1.2.2.2.2.2.3" xref="S3.SS5.p1.1.m1.2.2.2.2.2.3.cmml"><mi id="S3.SS5.p1.1.m1.2.2.2.2.2.3.2" xref="S3.SS5.p1.1.m1.2.2.2.2.2.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.1.m1.2.2.2.2.2.3.1" xref="S3.SS5.p1.1.m1.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS5.p1.1.m1.2.2.2.2.2.3.3" xref="S3.SS5.p1.1.m1.2.2.2.2.2.3.3.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.1.m1.2.2.2.2.2.3.1a" xref="S3.SS5.p1.1.m1.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS5.p1.1.m1.2.2.2.2.2.3.4" xref="S3.SS5.p1.1.m1.2.2.2.2.2.3.4.cmml">H</mi></mrow><mi id="S3.SS5.p1.1.m1.2.2.2.2.2.2.3" xref="S3.SS5.p1.1.m1.2.2.2.2.2.2.3.cmml">V</mi></msubsup><mo id="S3.SS5.p1.1.m1.2.2.2.2.1" xref="S3.SS5.p1.1.m1.2.2.2.2.1.cmml">∈</mo><msup id="S3.SS5.p1.1.m1.2.2.2.2.3" xref="S3.SS5.p1.1.m1.2.2.2.2.3.cmml"><mi id="S3.SS5.p1.1.m1.2.2.2.2.3.2" xref="S3.SS5.p1.1.m1.2.2.2.2.3.2.cmml">ℝ</mi><mrow id="S3.SS5.p1.1.m1.2.2.2.2.3.3" xref="S3.SS5.p1.1.m1.2.2.2.2.3.3.cmml"><mi id="S3.SS5.p1.1.m1.2.2.2.2.3.3.2" xref="S3.SS5.p1.1.m1.2.2.2.2.3.3.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS5.p1.1.m1.2.2.2.2.3.3.1" xref="S3.SS5.p1.1.m1.2.2.2.2.3.3.1.cmml">×</mo><mn id="S3.SS5.p1.1.m1.2.2.2.2.3.3.3" xref="S3.SS5.p1.1.m1.2.2.2.2.3.3.3.cmml">768</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.2b"><apply id="S3.SS5.p1.1.m1.2.2.3.cmml" xref="S3.SS5.p1.1.m1.2.2.2"><csymbol cd="ambiguous" id="S3.SS5.p1.1.m1.2.2.3a.cmml" xref="S3.SS5.p1.1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS5.p1.1.m1.1.1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1"><in id="S3.SS5.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1.1"></in><apply id="S3.SS5.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS5.p1.1.m1.1.1.1.1.2.1.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1.2">subscript</csymbol><apply id="S3.SS5.p1.1.m1.1.1.1.1.2.2.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS5.p1.1.m1.1.1.1.1.2.2.1.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1.2">superscript</csymbol><ci id="S3.SS5.p1.1.m1.1.1.1.1.2.2.2.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1.2.2.2">𝐹</ci><ci id="S3.SS5.p1.1.m1.1.1.1.1.2.2.3.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1.2.2.3">𝐴</ci></apply><apply id="S3.SS5.p1.1.m1.1.1.1.1.2.3.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1.2.3"><times id="S3.SS5.p1.1.m1.1.1.1.1.2.3.1.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1.2.3.1"></times><ci id="S3.SS5.p1.1.m1.1.1.1.1.2.3.2.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1.2.3.2">𝐴</ci><ci id="S3.SS5.p1.1.m1.1.1.1.1.2.3.3.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1.2.3.3">𝑉</ci><ci id="S3.SS5.p1.1.m1.1.1.1.1.2.3.4.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1.2.3.4">𝐻</ci></apply></apply><apply id="S3.SS5.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS5.p1.1.m1.1.1.1.1.3.1.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS5.p1.1.m1.1.1.1.1.3.2.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1.3.2">ℝ</ci><apply id="S3.SS5.p1.1.m1.1.1.1.1.3.3.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1.3.3"><times id="S3.SS5.p1.1.m1.1.1.1.1.3.3.1.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1.3.3.1"></times><ci id="S3.SS5.p1.1.m1.1.1.1.1.3.3.2.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1.3.3.2">𝑇</ci><cn type="integer" id="S3.SS5.p1.1.m1.1.1.1.1.3.3.3.cmml" xref="S3.SS5.p1.1.m1.1.1.1.1.3.3.3">768</cn></apply></apply></apply><apply id="S3.SS5.p1.1.m1.2.2.2.2.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2"><in id="S3.SS5.p1.1.m1.2.2.2.2.1.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2.1"></in><apply id="S3.SS5.p1.1.m1.2.2.2.2.2.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS5.p1.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2.2">subscript</csymbol><apply id="S3.SS5.p1.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS5.p1.1.m1.2.2.2.2.2.2.1.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2.2">superscript</csymbol><ci id="S3.SS5.p1.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2.2.2.2">𝐹</ci><ci id="S3.SS5.p1.1.m1.2.2.2.2.2.2.3.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2.2.2.3">𝑉</ci></apply><apply id="S3.SS5.p1.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2.2.3"><times id="S3.SS5.p1.1.m1.2.2.2.2.2.3.1.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2.2.3.1"></times><ci id="S3.SS5.p1.1.m1.2.2.2.2.2.3.2.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2.2.3.2">𝐴</ci><ci id="S3.SS5.p1.1.m1.2.2.2.2.2.3.3.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2.2.3.3">𝑉</ci><ci id="S3.SS5.p1.1.m1.2.2.2.2.2.3.4.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2.2.3.4">𝐻</ci></apply></apply><apply id="S3.SS5.p1.1.m1.2.2.2.2.3.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS5.p1.1.m1.2.2.2.2.3.1.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2.3">superscript</csymbol><ci id="S3.SS5.p1.1.m1.2.2.2.2.3.2.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2.3.2">ℝ</ci><apply id="S3.SS5.p1.1.m1.2.2.2.2.3.3.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2.3.3"><times id="S3.SS5.p1.1.m1.2.2.2.2.3.3.1.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2.3.3.1"></times><ci id="S3.SS5.p1.1.m1.2.2.2.2.3.3.2.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2.3.3.2">𝑇</ci><cn type="integer" id="S3.SS5.p1.1.m1.2.2.2.2.3.3.3.cmml" xref="S3.SS5.p1.1.m1.2.2.2.2.3.3.3">768</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.2c">F^{A}_{AVH}\in\mathbb{R}^{T\times 768},F^{V}_{AVH}\in\mathbb{R}^{T\times 768}</annotation></semantics></math>.
Along with  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>, we empirically found that extracting features from the entire video instead of a short sequence yields better audio-visual feature alignment.
Therefore, we specifically replace the corresponding interval of the ground-truth video with the generated face sequence (<a href="#S4.F3" title="In 4 Experimental Results ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>).
We then extract features from the video using the cropped lips from the face sequence and audio sequence to calculate the lip sync between audio and visual features.
However, since a major part of the video is ground-truth data, it is anticipated to have high audio-visual alignment, yielding insignificant effects of the generated samples on lip synchronization evaluation.
To tackle this problem, we only consider the generated samples in the feature space for lip sync loss, as the AV-HuBERT feature extractor provides the feature representation per time step, <math id="S3.SS5.p1.2.m2.1" class="ltx_Math" alttext="T\times D" display="inline"><semantics id="S3.SS5.p1.2.m2.1a"><mrow id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml"><mi id="S3.SS5.p1.2.m2.1.1.2" xref="S3.SS5.p1.2.m2.1.1.2.cmml">T</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS5.p1.2.m2.1.1.1" xref="S3.SS5.p1.2.m2.1.1.1.cmml">×</mo><mi id="S3.SS5.p1.2.m2.1.1.3" xref="S3.SS5.p1.2.m2.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><apply id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1"><times id="S3.SS5.p1.2.m2.1.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1.1"></times><ci id="S3.SS5.p1.2.m2.1.1.2.cmml" xref="S3.SS5.p1.2.m2.1.1.2">𝑇</ci><ci id="S3.SS5.p1.2.m2.1.1.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">T\times D</annotation></semantics></math>.
Specifically, we take the audio and visual AV-HuBERT features corresponding to the generated time interval.
Subsequently, we compute cosine similarity between these two feature representations followed by binary cross-entropy loss as follows:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="L_{sync}=-log(CS(F^{A_{t:t+k}}_{AVH},F^{V_{t:t+k}}_{AVH}))" display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><msub id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.3.2" xref="S3.E1.m1.1.1.3.2.cmml">L</mi><mrow id="S3.E1.m1.1.1.3.3" xref="S3.E1.m1.1.1.3.3.cmml"><mi id="S3.E1.m1.1.1.3.3.2" xref="S3.E1.m1.1.1.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.1" xref="S3.E1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.3.3" xref="S3.E1.m1.1.1.3.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.1a" xref="S3.E1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.3.4" xref="S3.E1.m1.1.1.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.3.3.1b" xref="S3.E1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.3.3.5" xref="S3.E1.m1.1.1.3.3.5.cmml">c</mi></mrow></msub><mo id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1a" xref="S3.E1.m1.1.1.1.cmml">−</mo><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2a" xref="S3.E1.m1.1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.5" xref="S3.E1.m1.1.1.1.1.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2b" xref="S3.E1.m1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.1.1.1.4.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.5" xref="S3.E1.m1.1.1.1.1.1.1.1.5.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.3a" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml">(</mo><msubsup id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">F</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1a" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.4" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.4.cmml">H</mi></mrow><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">A</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.2.cmml">t</mi><mo lspace="0.278em" rspace="0.278em" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.1.cmml">:</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.3.2.cmml">t</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.3.1.cmml">+</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.3.3.cmml">k</mi></mrow></mrow></msub></msubsup><mo id="S3.E1.m1.1.1.1.1.1.1.1.2.2.4" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml">,</mo><msubsup id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.2.cmml">F</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.3.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.1a" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.4" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.4.cmml">H</mi></mrow><msub id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.2.cmml">V</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.2.cmml">t</mi><mo lspace="0.278em" rspace="0.278em" id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.1.cmml">:</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.3.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.3.2.cmml">t</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.3.1" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.3.1.cmml">+</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.3.3" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.3.3.cmml">k</mi></mrow></mrow></msub></msubsup><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.1.2.2.5" xref="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><eq id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2"></eq><apply id="S3.E1.m1.1.1.3.cmml" xref="S3.E1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.3.2">𝐿</ci><apply id="S3.E1.m1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.3.3"><times id="S3.E1.m1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.3.3.1"></times><ci id="S3.E1.m1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.3.3.2">𝑠</ci><ci id="S3.E1.m1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.3.3.3">𝑦</ci><ci id="S3.E1.m1.1.1.3.3.4.cmml" xref="S3.E1.m1.1.1.3.3.4">𝑛</ci><ci id="S3.E1.m1.1.1.3.3.5.cmml" xref="S3.E1.m1.1.1.3.3.5">𝑐</ci></apply></apply><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><minus id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1"></minus><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3">𝑙</ci><ci id="S3.E1.m1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.4">𝑜</ci><ci id="S3.E1.m1.1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.1.5">𝑔</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3"></times><ci id="S3.E1.m1.1.1.1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.4">𝐶</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.5">𝑆</ci><interval closure="open" id="S3.E1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2"><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.2">𝐹</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.2">𝐴</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3"><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.1">:</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.2">𝑡</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.3"><plus id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.3.1"></plus><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.3.2">𝑡</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.2.3.3.3.3">𝑘</ci></apply></apply></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3"><times id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.2">𝐴</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.3">𝑉</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.3.4">𝐻</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2">superscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.2">𝐹</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.2">𝑉</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3"><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.1">:</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.2">𝑡</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.3"><plus id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.3.1"></plus><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.3.2">𝑡</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.2.3.3.3.3">𝑘</ci></apply></apply></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3"><times id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.1"></times><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.2">𝐴</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.3">𝑉</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.4.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2.2.2.3.4">𝐻</ci></apply></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">L_{sync}=-log(CS(F^{A_{t:t+k}}_{AVH},F^{V_{t:t+k}}_{AVH}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS5.p1.5" class="ltx_p">where <math id="S3.SS5.p1.3.m1.1" class="ltx_Math" alttext="CS" display="inline"><semantics id="S3.SS5.p1.3.m1.1a"><mrow id="S3.SS5.p1.3.m1.1.1" xref="S3.SS5.p1.3.m1.1.1.cmml"><mi id="S3.SS5.p1.3.m1.1.1.2" xref="S3.SS5.p1.3.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS5.p1.3.m1.1.1.1" xref="S3.SS5.p1.3.m1.1.1.1.cmml">​</mo><mi id="S3.SS5.p1.3.m1.1.1.3" xref="S3.SS5.p1.3.m1.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.3.m1.1b"><apply id="S3.SS5.p1.3.m1.1.1.cmml" xref="S3.SS5.p1.3.m1.1.1"><times id="S3.SS5.p1.3.m1.1.1.1.cmml" xref="S3.SS5.p1.3.m1.1.1.1"></times><ci id="S3.SS5.p1.3.m1.1.1.2.cmml" xref="S3.SS5.p1.3.m1.1.1.2">𝐶</ci><ci id="S3.SS5.p1.3.m1.1.1.3.cmml" xref="S3.SS5.p1.3.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.3.m1.1c">CS</annotation></semantics></math> indicates cosine similarity and <math id="S3.SS5.p1.4.m2.1" class="ltx_Math" alttext="t:t+k" display="inline"><semantics id="S3.SS5.p1.4.m2.1a"><mrow id="S3.SS5.p1.4.m2.1.1" xref="S3.SS5.p1.4.m2.1.1.cmml"><mi id="S3.SS5.p1.4.m2.1.1.2" xref="S3.SS5.p1.4.m2.1.1.2.cmml">t</mi><mo lspace="0.278em" rspace="0.278em" id="S3.SS5.p1.4.m2.1.1.1" xref="S3.SS5.p1.4.m2.1.1.1.cmml">:</mo><mrow id="S3.SS5.p1.4.m2.1.1.3" xref="S3.SS5.p1.4.m2.1.1.3.cmml"><mi id="S3.SS5.p1.4.m2.1.1.3.2" xref="S3.SS5.p1.4.m2.1.1.3.2.cmml">t</mi><mo id="S3.SS5.p1.4.m2.1.1.3.1" xref="S3.SS5.p1.4.m2.1.1.3.1.cmml">+</mo><mi id="S3.SS5.p1.4.m2.1.1.3.3" xref="S3.SS5.p1.4.m2.1.1.3.3.cmml">k</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.4.m2.1b"><apply id="S3.SS5.p1.4.m2.1.1.cmml" xref="S3.SS5.p1.4.m2.1.1"><ci id="S3.SS5.p1.4.m2.1.1.1.cmml" xref="S3.SS5.p1.4.m2.1.1.1">:</ci><ci id="S3.SS5.p1.4.m2.1.1.2.cmml" xref="S3.SS5.p1.4.m2.1.1.2">𝑡</ci><apply id="S3.SS5.p1.4.m2.1.1.3.cmml" xref="S3.SS5.p1.4.m2.1.1.3"><plus id="S3.SS5.p1.4.m2.1.1.3.1.cmml" xref="S3.SS5.p1.4.m2.1.1.3.1"></plus><ci id="S3.SS5.p1.4.m2.1.1.3.2.cmml" xref="S3.SS5.p1.4.m2.1.1.3.2">𝑡</ci><ci id="S3.SS5.p1.4.m2.1.1.3.3.cmml" xref="S3.SS5.p1.4.m2.1.1.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.4.m2.1c">t:t+k</annotation></semantics></math> represents the time interval of the generated part of the video.
<math id="S3.SS5.p1.5.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS5.p1.5.m3.1a"><mi id="S3.SS5.p1.5.m3.1.1" xref="S3.SS5.p1.5.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.5.m3.1b"><ci id="S3.SS5.p1.5.m3.1.1.cmml" xref="S3.SS5.p1.5.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.5.m3.1c">k</annotation></semantics></math> is the same as the length of the face sequence generated by the talking face generation model in a single forward pass.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Implementation Details</h3>

<div id="S3.SS6.p1" class="ltx_para ltx_noindent">
<p id="S3.SS6.p1.1" class="ltx_p"><span id="S3.SS6.p1.1.1" class="ltx_text ltx_font_bold">Adversarial loss</span>
We utilize the GAN (adversarial) loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite> to train our talking face generation model.
For this, we employ a discriminator model, which is responsible for distinguishing real (target data) and fake samples (generated data) to guide the generator.
Meanwhile, the generator attempts to synthesize appropriate images so that the discriminator cannot determine whether the sample is real or fake.
Our discriminator network contains <math id="S3.SS6.p1.1.m1.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S3.SS6.p1.1.m1.1a"><mn id="S3.SS6.p1.1.m1.1.1" xref="S3.SS6.p1.1.m1.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.1.m1.1b"><cn type="integer" id="S3.SS6.p1.1.m1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.1.m1.1c">7</annotation></semantics></math> consecutive strided-convolutional layers along with Leaky ReLU activation function and spectral normalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para ltx_noindent">
<p id="S3.SS6.p2.4" class="ltx_p"><span id="S3.SS6.p2.4.1" class="ltx_text ltx_font_bold">Perceptual loss</span>
In order to preserve the identity and textures, we utilize perceptual loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> as feature reconstruction loss by extracting features from the generated faces and GT faces from different layers of the pretrained VGG-19 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>.
Afterward, we calculate the L2 distance between extracted features, as shown below.
While <math id="S3.SS6.p2.1.m1.1" class="ltx_Math" alttext="c_{i}" display="inline"><semantics id="S3.SS6.p2.1.m1.1a"><msub id="S3.SS6.p2.1.m1.1.1" xref="S3.SS6.p2.1.m1.1.1.cmml"><mi id="S3.SS6.p2.1.m1.1.1.2" xref="S3.SS6.p2.1.m1.1.1.2.cmml">c</mi><mi id="S3.SS6.p2.1.m1.1.1.3" xref="S3.SS6.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.1.m1.1b"><apply id="S3.SS6.p2.1.m1.1.1.cmml" xref="S3.SS6.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p2.1.m1.1.1.1.cmml" xref="S3.SS6.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS6.p2.1.m1.1.1.2.cmml" xref="S3.SS6.p2.1.m1.1.1.2">𝑐</ci><ci id="S3.SS6.p2.1.m1.1.1.3.cmml" xref="S3.SS6.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.1.m1.1c">c_{i}</annotation></semantics></math> indicates weight coefficients, <math id="S3.SS6.p2.2.m2.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="S3.SS6.p2.2.m2.1a"><mi id="S3.SS6.p2.2.m2.1.1" xref="S3.SS6.p2.2.m2.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.2.m2.1b"><ci id="S3.SS6.p2.2.m2.1.1.cmml" xref="S3.SS6.p2.2.m2.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.2.m2.1c">\phi</annotation></semantics></math> states the selected layers for the feature extraction.
<math id="S3.SS6.p2.3.m3.1" class="ltx_Math" alttext="I^{G}" display="inline"><semantics id="S3.SS6.p2.3.m3.1a"><msup id="S3.SS6.p2.3.m3.1.1" xref="S3.SS6.p2.3.m3.1.1.cmml"><mi id="S3.SS6.p2.3.m3.1.1.2" xref="S3.SS6.p2.3.m3.1.1.2.cmml">I</mi><mi id="S3.SS6.p2.3.m3.1.1.3" xref="S3.SS6.p2.3.m3.1.1.3.cmml">G</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.3.m3.1b"><apply id="S3.SS6.p2.3.m3.1.1.cmml" xref="S3.SS6.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS6.p2.3.m3.1.1.1.cmml" xref="S3.SS6.p2.3.m3.1.1">superscript</csymbol><ci id="S3.SS6.p2.3.m3.1.1.2.cmml" xref="S3.SS6.p2.3.m3.1.1.2">𝐼</ci><ci id="S3.SS6.p2.3.m3.1.1.3.cmml" xref="S3.SS6.p2.3.m3.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.3.m3.1c">I^{G}</annotation></semantics></math> and <math id="S3.SS6.p2.4.m4.1" class="ltx_Math" alttext="I^{GT}" display="inline"><semantics id="S3.SS6.p2.4.m4.1a"><msup id="S3.SS6.p2.4.m4.1.1" xref="S3.SS6.p2.4.m4.1.1.cmml"><mi id="S3.SS6.p2.4.m4.1.1.2" xref="S3.SS6.p2.4.m4.1.1.2.cmml">I</mi><mrow id="S3.SS6.p2.4.m4.1.1.3" xref="S3.SS6.p2.4.m4.1.1.3.cmml"><mi id="S3.SS6.p2.4.m4.1.1.3.2" xref="S3.SS6.p2.4.m4.1.1.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p2.4.m4.1.1.3.1" xref="S3.SS6.p2.4.m4.1.1.3.1.cmml">​</mo><mi id="S3.SS6.p2.4.m4.1.1.3.3" xref="S3.SS6.p2.4.m4.1.1.3.3.cmml">T</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.4.m4.1b"><apply id="S3.SS6.p2.4.m4.1.1.cmml" xref="S3.SS6.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS6.p2.4.m4.1.1.1.cmml" xref="S3.SS6.p2.4.m4.1.1">superscript</csymbol><ci id="S3.SS6.p2.4.m4.1.1.2.cmml" xref="S3.SS6.p2.4.m4.1.1.2">𝐼</ci><apply id="S3.SS6.p2.4.m4.1.1.3.cmml" xref="S3.SS6.p2.4.m4.1.1.3"><times id="S3.SS6.p2.4.m4.1.1.3.1.cmml" xref="S3.SS6.p2.4.m4.1.1.3.1"></times><ci id="S3.SS6.p2.4.m4.1.1.3.2.cmml" xref="S3.SS6.p2.4.m4.1.1.3.2">𝐺</ci><ci id="S3.SS6.p2.4.m4.1.1.3.3.cmml" xref="S3.SS6.p2.4.m4.1.1.3.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.4.m4.1c">I^{GT}</annotation></semantics></math> are the generated image and the GT image, respectively.
We follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> for determining the coefficients and layers.</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="L_{per}=\sum_{i=1}^{5}c_{i}||VGG^{\phi_{i}}(I^{G})-VGG^{\phi_{i}}(I^{GT})||_{2}" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">L</mi><mrow id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml"><mi id="S3.E2.m1.1.1.3.3.2" xref="S3.E2.m1.1.1.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.1" xref="S3.E2.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.3.3" xref="S3.E2.m1.1.1.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.1a" xref="S3.E2.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.3.3.4" xref="S3.E2.m1.1.1.3.3.4.cmml">r</mi></mrow></msub><mo rspace="0.111em" id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><munderover id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E2.m1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E2.m1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.2.2.3.2" xref="S3.E2.m1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E2.m1.1.1.1.2.2.3.1" xref="S3.E2.m1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E2.m1.1.1.1.2.2.3.3" xref="S3.E2.m1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mn id="S3.E2.m1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.2.3.cmml">5</mn></munderover><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml">c</mi><mi id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">​</mo><msub id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.4.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2a" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><msup id="S3.E2.m1.1.1.1.1.1.1.1.1.1.5" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.2.cmml">G</mi><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.3.2.cmml">ϕ</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.3.3.cmml">i</mi></msub></msup><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2b" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msup id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">I</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">G</mi></msup><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml">−</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2.4" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.4.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2a" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.cmml">​</mo><msup id="S3.E2.m1.1.1.1.1.1.1.1.1.2.5" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.2.cmml">G</mi><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.3.2.cmml">ϕ</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.3.3.cmml">i</mi></msub></msup><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2b" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.cmml">​</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.cmml">(</mo><msup id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.2.cmml">I</mi><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3.1.cmml">​</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3.3.cmml">T</mi></mrow></msup><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3.cmml">2</mn></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"></eq><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">𝐿</ci><apply id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3"><times id="S3.E2.m1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3.1"></times><ci id="S3.E2.m1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.2">𝑝</ci><ci id="S3.E2.m1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3">𝑒</ci><ci id="S3.E2.m1.1.1.3.3.4.cmml" xref="S3.E2.m1.1.1.3.3.4">𝑟</ci></apply></apply><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><apply id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.1.1.1.2">subscript</csymbol><sum id="S3.E2.m1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.2.2.2"></sum><apply id="S3.E2.m1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.2.2.3"><eq id="S3.E2.m1.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E2.m1.1.1.1.2.2.3.3.cmml" xref="S3.E2.m1.1.1.1.2.2.3.3">1</cn></apply></apply><cn type="integer" id="S3.E2.m1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.2.3">5</cn></apply><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2">𝑐</ci><ci id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3">𝑖</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"><minus id="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3"></minus><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3">𝑉</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.4">𝐺</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.5">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.2">𝐺</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.3.2">italic-ϕ</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.5.3.3">𝑖</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝐼</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝐺</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.2"></times><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.3">𝑉</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.4.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.4">𝐺</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.5"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.5">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.2">𝐺</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.3.2">italic-ϕ</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.5.3.3">𝑖</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1">superscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.2">𝐼</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3"><times id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3.1"></times><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3.2">𝐺</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.1.1.3.3">𝑇</ci></apply></apply></apply></apply></apply><cn type="integer" id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">L_{per}=\sum_{i=1}^{5}c_{i}||VGG^{\phi_{i}}(I^{G})-VGG^{\phi_{i}}(I^{GT})||_{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS6.p3" class="ltx_para ltx_noindent">
<p id="S3.SS6.p3.1" class="ltx_p"><span id="S3.SS6.p3.1.1" class="ltx_text ltx_font_bold">Pixel reconstruction loss</span>
Despite perceptual loss to capture identity and textural details, pixel-level reconstruction loss is required to capture fine-grained details and generate consistent images.
Therefore, we employ a reconstruction loss in the pixel space: <math id="S3.SS6.p3.1.m1.1" class="ltx_Math" alttext="L_{pixel}=||I^{G}-I^{GT}||_{1}" display="inline"><semantics id="S3.SS6.p3.1.m1.1a"><mrow id="S3.SS6.p3.1.m1.1.1" xref="S3.SS6.p3.1.m1.1.1.cmml"><msub id="S3.SS6.p3.1.m1.1.1.3" xref="S3.SS6.p3.1.m1.1.1.3.cmml"><mi id="S3.SS6.p3.1.m1.1.1.3.2" xref="S3.SS6.p3.1.m1.1.1.3.2.cmml">L</mi><mrow id="S3.SS6.p3.1.m1.1.1.3.3" xref="S3.SS6.p3.1.m1.1.1.3.3.cmml"><mi id="S3.SS6.p3.1.m1.1.1.3.3.2" xref="S3.SS6.p3.1.m1.1.1.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p3.1.m1.1.1.3.3.1" xref="S3.SS6.p3.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.SS6.p3.1.m1.1.1.3.3.3" xref="S3.SS6.p3.1.m1.1.1.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p3.1.m1.1.1.3.3.1a" xref="S3.SS6.p3.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.SS6.p3.1.m1.1.1.3.3.4" xref="S3.SS6.p3.1.m1.1.1.3.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p3.1.m1.1.1.3.3.1b" xref="S3.SS6.p3.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.SS6.p3.1.m1.1.1.3.3.5" xref="S3.SS6.p3.1.m1.1.1.3.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p3.1.m1.1.1.3.3.1c" xref="S3.SS6.p3.1.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.SS6.p3.1.m1.1.1.3.3.6" xref="S3.SS6.p3.1.m1.1.1.3.3.6.cmml">l</mi></mrow></msub><mo id="S3.SS6.p3.1.m1.1.1.2" xref="S3.SS6.p3.1.m1.1.1.2.cmml">=</mo><msub id="S3.SS6.p3.1.m1.1.1.1" xref="S3.SS6.p3.1.m1.1.1.1.cmml"><mrow id="S3.SS6.p3.1.m1.1.1.1.1.1" xref="S3.SS6.p3.1.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS6.p3.1.m1.1.1.1.1.1.2" xref="S3.SS6.p3.1.m1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.SS6.p3.1.m1.1.1.1.1.1.1" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.cmml"><msup id="S3.SS6.p3.1.m1.1.1.1.1.1.1.2" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.SS6.p3.1.m1.1.1.1.1.1.1.2.2" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.2.2.cmml">I</mi><mi id="S3.SS6.p3.1.m1.1.1.1.1.1.1.2.3" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.2.3.cmml">G</mi></msup><mo id="S3.SS6.p3.1.m1.1.1.1.1.1.1.1" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.1.cmml">−</mo><msup id="S3.SS6.p3.1.m1.1.1.1.1.1.1.3" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.2" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.2.cmml">I</mi><mrow id="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.3" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.3.cmml"><mi id="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.3.2" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.3.1" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.3.1.cmml">​</mo><mi id="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.3.3" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.3.3.cmml">T</mi></mrow></msup></mrow><mo stretchy="false" id="S3.SS6.p3.1.m1.1.1.1.1.1.3" xref="S3.SS6.p3.1.m1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.SS6.p3.1.m1.1.1.1.3" xref="S3.SS6.p3.1.m1.1.1.1.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p3.1.m1.1b"><apply id="S3.SS6.p3.1.m1.1.1.cmml" xref="S3.SS6.p3.1.m1.1.1"><eq id="S3.SS6.p3.1.m1.1.1.2.cmml" xref="S3.SS6.p3.1.m1.1.1.2"></eq><apply id="S3.SS6.p3.1.m1.1.1.3.cmml" xref="S3.SS6.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS6.p3.1.m1.1.1.3.1.cmml" xref="S3.SS6.p3.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS6.p3.1.m1.1.1.3.2.cmml" xref="S3.SS6.p3.1.m1.1.1.3.2">𝐿</ci><apply id="S3.SS6.p3.1.m1.1.1.3.3.cmml" xref="S3.SS6.p3.1.m1.1.1.3.3"><times id="S3.SS6.p3.1.m1.1.1.3.3.1.cmml" xref="S3.SS6.p3.1.m1.1.1.3.3.1"></times><ci id="S3.SS6.p3.1.m1.1.1.3.3.2.cmml" xref="S3.SS6.p3.1.m1.1.1.3.3.2">𝑝</ci><ci id="S3.SS6.p3.1.m1.1.1.3.3.3.cmml" xref="S3.SS6.p3.1.m1.1.1.3.3.3">𝑖</ci><ci id="S3.SS6.p3.1.m1.1.1.3.3.4.cmml" xref="S3.SS6.p3.1.m1.1.1.3.3.4">𝑥</ci><ci id="S3.SS6.p3.1.m1.1.1.3.3.5.cmml" xref="S3.SS6.p3.1.m1.1.1.3.3.5">𝑒</ci><ci id="S3.SS6.p3.1.m1.1.1.3.3.6.cmml" xref="S3.SS6.p3.1.m1.1.1.3.3.6">𝑙</ci></apply></apply><apply id="S3.SS6.p3.1.m1.1.1.1.cmml" xref="S3.SS6.p3.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p3.1.m1.1.1.1.2.cmml" xref="S3.SS6.p3.1.m1.1.1.1">subscript</csymbol><apply id="S3.SS6.p3.1.m1.1.1.1.1.2.cmml" xref="S3.SS6.p3.1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS6.p3.1.m1.1.1.1.1.2.1.cmml" xref="S3.SS6.p3.1.m1.1.1.1.1.1.2">norm</csymbol><apply id="S3.SS6.p3.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1"><minus id="S3.SS6.p3.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.1"></minus><apply id="S3.SS6.p3.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS6.p3.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S3.SS6.p3.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.2.2">𝐼</ci><ci id="S3.SS6.p3.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.2.3">𝐺</ci></apply><apply id="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.2">𝐼</ci><apply id="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.3"><times id="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.3.1"></times><ci id="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.3.2">𝐺</ci><ci id="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.SS6.p3.1.m1.1.1.1.1.1.1.3.3.3">𝑇</ci></apply></apply></apply></apply><cn type="integer" id="S3.SS6.p3.1.m1.1.1.1.3.cmml" xref="S3.SS6.p3.1.m1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p3.1.m1.1c">L_{pixel}=||I^{G}-I^{GT}||_{1}</annotation></semantics></math></p>
</div>
<div id="S3.SS6.p4" class="ltx_para ltx_noindent">
<p id="S3.SS6.p4.4" class="ltx_p"><span id="S3.SS6.p4.4.1" class="ltx_text ltx_font_bold">Total loss</span>
By combining all the presented loss functions, the total loss is as follows:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.4" class="ltx_Math" alttext="L=L_{GAN}(G,D)+\lambda_{1}L_{pixel}(G)+\lambda_{2}L_{per}(G)+\lambda_{3}L_{sync}" display="block"><semantics id="S3.E3.m1.4a"><mrow id="S3.E3.m1.4.5" xref="S3.E3.m1.4.5.cmml"><mi id="S3.E3.m1.4.5.2" xref="S3.E3.m1.4.5.2.cmml">L</mi><mo id="S3.E3.m1.4.5.1" xref="S3.E3.m1.4.5.1.cmml">=</mo><mrow id="S3.E3.m1.4.5.3" xref="S3.E3.m1.4.5.3.cmml"><mrow id="S3.E3.m1.4.5.3.2" xref="S3.E3.m1.4.5.3.2.cmml"><msub id="S3.E3.m1.4.5.3.2.2" xref="S3.E3.m1.4.5.3.2.2.cmml"><mi id="S3.E3.m1.4.5.3.2.2.2" xref="S3.E3.m1.4.5.3.2.2.2.cmml">L</mi><mrow id="S3.E3.m1.4.5.3.2.2.3" xref="S3.E3.m1.4.5.3.2.2.3.cmml"><mi id="S3.E3.m1.4.5.3.2.2.3.2" xref="S3.E3.m1.4.5.3.2.2.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.5.3.2.2.3.1" xref="S3.E3.m1.4.5.3.2.2.3.1.cmml">​</mo><mi id="S3.E3.m1.4.5.3.2.2.3.3" xref="S3.E3.m1.4.5.3.2.2.3.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.5.3.2.2.3.1a" xref="S3.E3.m1.4.5.3.2.2.3.1.cmml">​</mo><mi id="S3.E3.m1.4.5.3.2.2.3.4" xref="S3.E3.m1.4.5.3.2.2.3.4.cmml">N</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.5.3.2.1" xref="S3.E3.m1.4.5.3.2.1.cmml">​</mo><mrow id="S3.E3.m1.4.5.3.2.3.2" xref="S3.E3.m1.4.5.3.2.3.1.cmml"><mo stretchy="false" id="S3.E3.m1.4.5.3.2.3.2.1" xref="S3.E3.m1.4.5.3.2.3.1.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">G</mi><mo id="S3.E3.m1.4.5.3.2.3.2.2" xref="S3.E3.m1.4.5.3.2.3.1.cmml">,</mo><mi id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml">D</mi><mo stretchy="false" id="S3.E3.m1.4.5.3.2.3.2.3" xref="S3.E3.m1.4.5.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.4.5.3.1" xref="S3.E3.m1.4.5.3.1.cmml">+</mo><mrow id="S3.E3.m1.4.5.3.3" xref="S3.E3.m1.4.5.3.3.cmml"><msub id="S3.E3.m1.4.5.3.3.2" xref="S3.E3.m1.4.5.3.3.2.cmml"><mi id="S3.E3.m1.4.5.3.3.2.2" xref="S3.E3.m1.4.5.3.3.2.2.cmml">λ</mi><mn id="S3.E3.m1.4.5.3.3.2.3" xref="S3.E3.m1.4.5.3.3.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.5.3.3.1" xref="S3.E3.m1.4.5.3.3.1.cmml">​</mo><msub id="S3.E3.m1.4.5.3.3.3" xref="S3.E3.m1.4.5.3.3.3.cmml"><mi id="S3.E3.m1.4.5.3.3.3.2" xref="S3.E3.m1.4.5.3.3.3.2.cmml">L</mi><mrow id="S3.E3.m1.4.5.3.3.3.3" xref="S3.E3.m1.4.5.3.3.3.3.cmml"><mi id="S3.E3.m1.4.5.3.3.3.3.2" xref="S3.E3.m1.4.5.3.3.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.5.3.3.3.3.1" xref="S3.E3.m1.4.5.3.3.3.3.1.cmml">​</mo><mi id="S3.E3.m1.4.5.3.3.3.3.3" xref="S3.E3.m1.4.5.3.3.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.5.3.3.3.3.1a" xref="S3.E3.m1.4.5.3.3.3.3.1.cmml">​</mo><mi id="S3.E3.m1.4.5.3.3.3.3.4" xref="S3.E3.m1.4.5.3.3.3.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.5.3.3.3.3.1b" xref="S3.E3.m1.4.5.3.3.3.3.1.cmml">​</mo><mi id="S3.E3.m1.4.5.3.3.3.3.5" xref="S3.E3.m1.4.5.3.3.3.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.5.3.3.3.3.1c" xref="S3.E3.m1.4.5.3.3.3.3.1.cmml">​</mo><mi id="S3.E3.m1.4.5.3.3.3.3.6" xref="S3.E3.m1.4.5.3.3.3.3.6.cmml">l</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.5.3.3.1a" xref="S3.E3.m1.4.5.3.3.1.cmml">​</mo><mrow id="S3.E3.m1.4.5.3.3.4.2" xref="S3.E3.m1.4.5.3.3.cmml"><mo stretchy="false" id="S3.E3.m1.4.5.3.3.4.2.1" xref="S3.E3.m1.4.5.3.3.cmml">(</mo><mi id="S3.E3.m1.3.3" xref="S3.E3.m1.3.3.cmml">G</mi><mo stretchy="false" id="S3.E3.m1.4.5.3.3.4.2.2" xref="S3.E3.m1.4.5.3.3.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.4.5.3.1a" xref="S3.E3.m1.4.5.3.1.cmml">+</mo><mrow id="S3.E3.m1.4.5.3.4" xref="S3.E3.m1.4.5.3.4.cmml"><msub id="S3.E3.m1.4.5.3.4.2" xref="S3.E3.m1.4.5.3.4.2.cmml"><mi id="S3.E3.m1.4.5.3.4.2.2" xref="S3.E3.m1.4.5.3.4.2.2.cmml">λ</mi><mn id="S3.E3.m1.4.5.3.4.2.3" xref="S3.E3.m1.4.5.3.4.2.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.5.3.4.1" xref="S3.E3.m1.4.5.3.4.1.cmml">​</mo><msub id="S3.E3.m1.4.5.3.4.3" xref="S3.E3.m1.4.5.3.4.3.cmml"><mi id="S3.E3.m1.4.5.3.4.3.2" xref="S3.E3.m1.4.5.3.4.3.2.cmml">L</mi><mrow id="S3.E3.m1.4.5.3.4.3.3" xref="S3.E3.m1.4.5.3.4.3.3.cmml"><mi id="S3.E3.m1.4.5.3.4.3.3.2" xref="S3.E3.m1.4.5.3.4.3.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.5.3.4.3.3.1" xref="S3.E3.m1.4.5.3.4.3.3.1.cmml">​</mo><mi id="S3.E3.m1.4.5.3.4.3.3.3" xref="S3.E3.m1.4.5.3.4.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.5.3.4.3.3.1a" xref="S3.E3.m1.4.5.3.4.3.3.1.cmml">​</mo><mi id="S3.E3.m1.4.5.3.4.3.3.4" xref="S3.E3.m1.4.5.3.4.3.3.4.cmml">r</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.5.3.4.1a" xref="S3.E3.m1.4.5.3.4.1.cmml">​</mo><mrow id="S3.E3.m1.4.5.3.4.4.2" xref="S3.E3.m1.4.5.3.4.cmml"><mo stretchy="false" id="S3.E3.m1.4.5.3.4.4.2.1" xref="S3.E3.m1.4.5.3.4.cmml">(</mo><mi id="S3.E3.m1.4.4" xref="S3.E3.m1.4.4.cmml">G</mi><mo stretchy="false" id="S3.E3.m1.4.5.3.4.4.2.2" xref="S3.E3.m1.4.5.3.4.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.4.5.3.1b" xref="S3.E3.m1.4.5.3.1.cmml">+</mo><mrow id="S3.E3.m1.4.5.3.5" xref="S3.E3.m1.4.5.3.5.cmml"><msub id="S3.E3.m1.4.5.3.5.2" xref="S3.E3.m1.4.5.3.5.2.cmml"><mi id="S3.E3.m1.4.5.3.5.2.2" xref="S3.E3.m1.4.5.3.5.2.2.cmml">λ</mi><mn id="S3.E3.m1.4.5.3.5.2.3" xref="S3.E3.m1.4.5.3.5.2.3.cmml">3</mn></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.5.3.5.1" xref="S3.E3.m1.4.5.3.5.1.cmml">​</mo><msub id="S3.E3.m1.4.5.3.5.3" xref="S3.E3.m1.4.5.3.5.3.cmml"><mi id="S3.E3.m1.4.5.3.5.3.2" xref="S3.E3.m1.4.5.3.5.3.2.cmml">L</mi><mrow id="S3.E3.m1.4.5.3.5.3.3" xref="S3.E3.m1.4.5.3.5.3.3.cmml"><mi id="S3.E3.m1.4.5.3.5.3.3.2" xref="S3.E3.m1.4.5.3.5.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.5.3.5.3.3.1" xref="S3.E3.m1.4.5.3.5.3.3.1.cmml">​</mo><mi id="S3.E3.m1.4.5.3.5.3.3.3" xref="S3.E3.m1.4.5.3.5.3.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.5.3.5.3.3.1a" xref="S3.E3.m1.4.5.3.5.3.3.1.cmml">​</mo><mi id="S3.E3.m1.4.5.3.5.3.3.4" xref="S3.E3.m1.4.5.3.5.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.4.5.3.5.3.3.1b" xref="S3.E3.m1.4.5.3.5.3.3.1.cmml">​</mo><mi id="S3.E3.m1.4.5.3.5.3.3.5" xref="S3.E3.m1.4.5.3.5.3.3.5.cmml">c</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.4b"><apply id="S3.E3.m1.4.5.cmml" xref="S3.E3.m1.4.5"><eq id="S3.E3.m1.4.5.1.cmml" xref="S3.E3.m1.4.5.1"></eq><ci id="S3.E3.m1.4.5.2.cmml" xref="S3.E3.m1.4.5.2">𝐿</ci><apply id="S3.E3.m1.4.5.3.cmml" xref="S3.E3.m1.4.5.3"><plus id="S3.E3.m1.4.5.3.1.cmml" xref="S3.E3.m1.4.5.3.1"></plus><apply id="S3.E3.m1.4.5.3.2.cmml" xref="S3.E3.m1.4.5.3.2"><times id="S3.E3.m1.4.5.3.2.1.cmml" xref="S3.E3.m1.4.5.3.2.1"></times><apply id="S3.E3.m1.4.5.3.2.2.cmml" xref="S3.E3.m1.4.5.3.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.5.3.2.2.1.cmml" xref="S3.E3.m1.4.5.3.2.2">subscript</csymbol><ci id="S3.E3.m1.4.5.3.2.2.2.cmml" xref="S3.E3.m1.4.5.3.2.2.2">𝐿</ci><apply id="S3.E3.m1.4.5.3.2.2.3.cmml" xref="S3.E3.m1.4.5.3.2.2.3"><times id="S3.E3.m1.4.5.3.2.2.3.1.cmml" xref="S3.E3.m1.4.5.3.2.2.3.1"></times><ci id="S3.E3.m1.4.5.3.2.2.3.2.cmml" xref="S3.E3.m1.4.5.3.2.2.3.2">𝐺</ci><ci id="S3.E3.m1.4.5.3.2.2.3.3.cmml" xref="S3.E3.m1.4.5.3.2.2.3.3">𝐴</ci><ci id="S3.E3.m1.4.5.3.2.2.3.4.cmml" xref="S3.E3.m1.4.5.3.2.2.3.4">𝑁</ci></apply></apply><interval closure="open" id="S3.E3.m1.4.5.3.2.3.1.cmml" xref="S3.E3.m1.4.5.3.2.3.2"><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝐺</ci><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">𝐷</ci></interval></apply><apply id="S3.E3.m1.4.5.3.3.cmml" xref="S3.E3.m1.4.5.3.3"><times id="S3.E3.m1.4.5.3.3.1.cmml" xref="S3.E3.m1.4.5.3.3.1"></times><apply id="S3.E3.m1.4.5.3.3.2.cmml" xref="S3.E3.m1.4.5.3.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.5.3.3.2.1.cmml" xref="S3.E3.m1.4.5.3.3.2">subscript</csymbol><ci id="S3.E3.m1.4.5.3.3.2.2.cmml" xref="S3.E3.m1.4.5.3.3.2.2">𝜆</ci><cn type="integer" id="S3.E3.m1.4.5.3.3.2.3.cmml" xref="S3.E3.m1.4.5.3.3.2.3">1</cn></apply><apply id="S3.E3.m1.4.5.3.3.3.cmml" xref="S3.E3.m1.4.5.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.4.5.3.3.3.1.cmml" xref="S3.E3.m1.4.5.3.3.3">subscript</csymbol><ci id="S3.E3.m1.4.5.3.3.3.2.cmml" xref="S3.E3.m1.4.5.3.3.3.2">𝐿</ci><apply id="S3.E3.m1.4.5.3.3.3.3.cmml" xref="S3.E3.m1.4.5.3.3.3.3"><times id="S3.E3.m1.4.5.3.3.3.3.1.cmml" xref="S3.E3.m1.4.5.3.3.3.3.1"></times><ci id="S3.E3.m1.4.5.3.3.3.3.2.cmml" xref="S3.E3.m1.4.5.3.3.3.3.2">𝑝</ci><ci id="S3.E3.m1.4.5.3.3.3.3.3.cmml" xref="S3.E3.m1.4.5.3.3.3.3.3">𝑖</ci><ci id="S3.E3.m1.4.5.3.3.3.3.4.cmml" xref="S3.E3.m1.4.5.3.3.3.3.4">𝑥</ci><ci id="S3.E3.m1.4.5.3.3.3.3.5.cmml" xref="S3.E3.m1.4.5.3.3.3.3.5">𝑒</ci><ci id="S3.E3.m1.4.5.3.3.3.3.6.cmml" xref="S3.E3.m1.4.5.3.3.3.3.6">𝑙</ci></apply></apply><ci id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3">𝐺</ci></apply><apply id="S3.E3.m1.4.5.3.4.cmml" xref="S3.E3.m1.4.5.3.4"><times id="S3.E3.m1.4.5.3.4.1.cmml" xref="S3.E3.m1.4.5.3.4.1"></times><apply id="S3.E3.m1.4.5.3.4.2.cmml" xref="S3.E3.m1.4.5.3.4.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.5.3.4.2.1.cmml" xref="S3.E3.m1.4.5.3.4.2">subscript</csymbol><ci id="S3.E3.m1.4.5.3.4.2.2.cmml" xref="S3.E3.m1.4.5.3.4.2.2">𝜆</ci><cn type="integer" id="S3.E3.m1.4.5.3.4.2.3.cmml" xref="S3.E3.m1.4.5.3.4.2.3">2</cn></apply><apply id="S3.E3.m1.4.5.3.4.3.cmml" xref="S3.E3.m1.4.5.3.4.3"><csymbol cd="ambiguous" id="S3.E3.m1.4.5.3.4.3.1.cmml" xref="S3.E3.m1.4.5.3.4.3">subscript</csymbol><ci id="S3.E3.m1.4.5.3.4.3.2.cmml" xref="S3.E3.m1.4.5.3.4.3.2">𝐿</ci><apply id="S3.E3.m1.4.5.3.4.3.3.cmml" xref="S3.E3.m1.4.5.3.4.3.3"><times id="S3.E3.m1.4.5.3.4.3.3.1.cmml" xref="S3.E3.m1.4.5.3.4.3.3.1"></times><ci id="S3.E3.m1.4.5.3.4.3.3.2.cmml" xref="S3.E3.m1.4.5.3.4.3.3.2">𝑝</ci><ci id="S3.E3.m1.4.5.3.4.3.3.3.cmml" xref="S3.E3.m1.4.5.3.4.3.3.3">𝑒</ci><ci id="S3.E3.m1.4.5.3.4.3.3.4.cmml" xref="S3.E3.m1.4.5.3.4.3.3.4">𝑟</ci></apply></apply><ci id="S3.E3.m1.4.4.cmml" xref="S3.E3.m1.4.4">𝐺</ci></apply><apply id="S3.E3.m1.4.5.3.5.cmml" xref="S3.E3.m1.4.5.3.5"><times id="S3.E3.m1.4.5.3.5.1.cmml" xref="S3.E3.m1.4.5.3.5.1"></times><apply id="S3.E3.m1.4.5.3.5.2.cmml" xref="S3.E3.m1.4.5.3.5.2"><csymbol cd="ambiguous" id="S3.E3.m1.4.5.3.5.2.1.cmml" xref="S3.E3.m1.4.5.3.5.2">subscript</csymbol><ci id="S3.E3.m1.4.5.3.5.2.2.cmml" xref="S3.E3.m1.4.5.3.5.2.2">𝜆</ci><cn type="integer" id="S3.E3.m1.4.5.3.5.2.3.cmml" xref="S3.E3.m1.4.5.3.5.2.3">3</cn></apply><apply id="S3.E3.m1.4.5.3.5.3.cmml" xref="S3.E3.m1.4.5.3.5.3"><csymbol cd="ambiguous" id="S3.E3.m1.4.5.3.5.3.1.cmml" xref="S3.E3.m1.4.5.3.5.3">subscript</csymbol><ci id="S3.E3.m1.4.5.3.5.3.2.cmml" xref="S3.E3.m1.4.5.3.5.3.2">𝐿</ci><apply id="S3.E3.m1.4.5.3.5.3.3.cmml" xref="S3.E3.m1.4.5.3.5.3.3"><times id="S3.E3.m1.4.5.3.5.3.3.1.cmml" xref="S3.E3.m1.4.5.3.5.3.3.1"></times><ci id="S3.E3.m1.4.5.3.5.3.3.2.cmml" xref="S3.E3.m1.4.5.3.5.3.3.2">𝑠</ci><ci id="S3.E3.m1.4.5.3.5.3.3.3.cmml" xref="S3.E3.m1.4.5.3.5.3.3.3">𝑦</ci><ci id="S3.E3.m1.4.5.3.5.3.3.4.cmml" xref="S3.E3.m1.4.5.3.5.3.3.4">𝑛</ci><ci id="S3.E3.m1.4.5.3.5.3.3.5.cmml" xref="S3.E3.m1.4.5.3.5.3.3.5">𝑐</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.4c">L=L_{GAN}(G,D)+\lambda_{1}L_{pixel}(G)+\lambda_{2}L_{per}(G)+\lambda_{3}L_{sync}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.SS6.p4.3" class="ltx_p">where <math id="S3.SS6.p4.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS6.p4.1.m1.1a"><mi id="S3.SS6.p4.1.m1.1.1" xref="S3.SS6.p4.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p4.1.m1.1b"><ci id="S3.SS6.p4.1.m1.1.1.cmml" xref="S3.SS6.p4.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p4.1.m1.1c">G</annotation></semantics></math> and <math id="S3.SS6.p4.2.m2.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS6.p4.2.m2.1a"><mi id="S3.SS6.p4.2.m2.1.1" xref="S3.SS6.p4.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p4.2.m2.1b"><ci id="S3.SS6.p4.2.m2.1.1.cmml" xref="S3.SS6.p4.2.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p4.2.m2.1c">D</annotation></semantics></math> denote the generator and discriminator. We determined the best coefficients through empirical analysis as follows: <math id="S3.SS6.p4.3.m3.6" class="ltx_Math" alttext="(\lambda_{1},\lambda_{2},\lambda_{3})=(10,1,0.5)" display="inline"><semantics id="S3.SS6.p4.3.m3.6a"><mrow id="S3.SS6.p4.3.m3.6.6" xref="S3.SS6.p4.3.m3.6.6.cmml"><mrow id="S3.SS6.p4.3.m3.6.6.3.3" xref="S3.SS6.p4.3.m3.6.6.3.4.cmml"><mo stretchy="false" id="S3.SS6.p4.3.m3.6.6.3.3.4" xref="S3.SS6.p4.3.m3.6.6.3.4.cmml">(</mo><msub id="S3.SS6.p4.3.m3.4.4.1.1.1" xref="S3.SS6.p4.3.m3.4.4.1.1.1.cmml"><mi id="S3.SS6.p4.3.m3.4.4.1.1.1.2" xref="S3.SS6.p4.3.m3.4.4.1.1.1.2.cmml">λ</mi><mn id="S3.SS6.p4.3.m3.4.4.1.1.1.3" xref="S3.SS6.p4.3.m3.4.4.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS6.p4.3.m3.6.6.3.3.5" xref="S3.SS6.p4.3.m3.6.6.3.4.cmml">,</mo><msub id="S3.SS6.p4.3.m3.5.5.2.2.2" xref="S3.SS6.p4.3.m3.5.5.2.2.2.cmml"><mi id="S3.SS6.p4.3.m3.5.5.2.2.2.2" xref="S3.SS6.p4.3.m3.5.5.2.2.2.2.cmml">λ</mi><mn id="S3.SS6.p4.3.m3.5.5.2.2.2.3" xref="S3.SS6.p4.3.m3.5.5.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS6.p4.3.m3.6.6.3.3.6" xref="S3.SS6.p4.3.m3.6.6.3.4.cmml">,</mo><msub id="S3.SS6.p4.3.m3.6.6.3.3.3" xref="S3.SS6.p4.3.m3.6.6.3.3.3.cmml"><mi id="S3.SS6.p4.3.m3.6.6.3.3.3.2" xref="S3.SS6.p4.3.m3.6.6.3.3.3.2.cmml">λ</mi><mn id="S3.SS6.p4.3.m3.6.6.3.3.3.3" xref="S3.SS6.p4.3.m3.6.6.3.3.3.3.cmml">3</mn></msub><mo stretchy="false" id="S3.SS6.p4.3.m3.6.6.3.3.7" xref="S3.SS6.p4.3.m3.6.6.3.4.cmml">)</mo></mrow><mo id="S3.SS6.p4.3.m3.6.6.4" xref="S3.SS6.p4.3.m3.6.6.4.cmml">=</mo><mrow id="S3.SS6.p4.3.m3.6.6.5.2" xref="S3.SS6.p4.3.m3.6.6.5.1.cmml"><mo stretchy="false" id="S3.SS6.p4.3.m3.6.6.5.2.1" xref="S3.SS6.p4.3.m3.6.6.5.1.cmml">(</mo><mn id="S3.SS6.p4.3.m3.1.1" xref="S3.SS6.p4.3.m3.1.1.cmml">10</mn><mo id="S3.SS6.p4.3.m3.6.6.5.2.2" xref="S3.SS6.p4.3.m3.6.6.5.1.cmml">,</mo><mn id="S3.SS6.p4.3.m3.2.2" xref="S3.SS6.p4.3.m3.2.2.cmml">1</mn><mo id="S3.SS6.p4.3.m3.6.6.5.2.3" xref="S3.SS6.p4.3.m3.6.6.5.1.cmml">,</mo><mn id="S3.SS6.p4.3.m3.3.3" xref="S3.SS6.p4.3.m3.3.3.cmml">0.5</mn><mo stretchy="false" id="S3.SS6.p4.3.m3.6.6.5.2.4" xref="S3.SS6.p4.3.m3.6.6.5.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p4.3.m3.6b"><apply id="S3.SS6.p4.3.m3.6.6.cmml" xref="S3.SS6.p4.3.m3.6.6"><eq id="S3.SS6.p4.3.m3.6.6.4.cmml" xref="S3.SS6.p4.3.m3.6.6.4"></eq><vector id="S3.SS6.p4.3.m3.6.6.3.4.cmml" xref="S3.SS6.p4.3.m3.6.6.3.3"><apply id="S3.SS6.p4.3.m3.4.4.1.1.1.cmml" xref="S3.SS6.p4.3.m3.4.4.1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p4.3.m3.4.4.1.1.1.1.cmml" xref="S3.SS6.p4.3.m3.4.4.1.1.1">subscript</csymbol><ci id="S3.SS6.p4.3.m3.4.4.1.1.1.2.cmml" xref="S3.SS6.p4.3.m3.4.4.1.1.1.2">𝜆</ci><cn type="integer" id="S3.SS6.p4.3.m3.4.4.1.1.1.3.cmml" xref="S3.SS6.p4.3.m3.4.4.1.1.1.3">1</cn></apply><apply id="S3.SS6.p4.3.m3.5.5.2.2.2.cmml" xref="S3.SS6.p4.3.m3.5.5.2.2.2"><csymbol cd="ambiguous" id="S3.SS6.p4.3.m3.5.5.2.2.2.1.cmml" xref="S3.SS6.p4.3.m3.5.5.2.2.2">subscript</csymbol><ci id="S3.SS6.p4.3.m3.5.5.2.2.2.2.cmml" xref="S3.SS6.p4.3.m3.5.5.2.2.2.2">𝜆</ci><cn type="integer" id="S3.SS6.p4.3.m3.5.5.2.2.2.3.cmml" xref="S3.SS6.p4.3.m3.5.5.2.2.2.3">2</cn></apply><apply id="S3.SS6.p4.3.m3.6.6.3.3.3.cmml" xref="S3.SS6.p4.3.m3.6.6.3.3.3"><csymbol cd="ambiguous" id="S3.SS6.p4.3.m3.6.6.3.3.3.1.cmml" xref="S3.SS6.p4.3.m3.6.6.3.3.3">subscript</csymbol><ci id="S3.SS6.p4.3.m3.6.6.3.3.3.2.cmml" xref="S3.SS6.p4.3.m3.6.6.3.3.3.2">𝜆</ci><cn type="integer" id="S3.SS6.p4.3.m3.6.6.3.3.3.3.cmml" xref="S3.SS6.p4.3.m3.6.6.3.3.3.3">3</cn></apply></vector><vector id="S3.SS6.p4.3.m3.6.6.5.1.cmml" xref="S3.SS6.p4.3.m3.6.6.5.2"><cn type="integer" id="S3.SS6.p4.3.m3.1.1.cmml" xref="S3.SS6.p4.3.m3.1.1">10</cn><cn type="integer" id="S3.SS6.p4.3.m3.2.2.cmml" xref="S3.SS6.p4.3.m3.2.2">1</cn><cn type="float" id="S3.SS6.p4.3.m3.3.3.cmml" xref="S3.SS6.p4.3.m3.3.3">0.5</cn></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p4.3.m3.6c">(\lambda_{1},\lambda_{2},\lambda_{3})=(10,1,0.5)</annotation></semantics></math>.</p>
</div>
<div id="S3.SS6.p5" class="ltx_para ltx_noindent">
<p id="S3.SS6.p5.8" class="ltx_p"><span id="S3.SS6.p5.8.1" class="ltx_text ltx_font_bold">Training details</span>
In each forward pass, we generate a set of images (denoted by <math id="S3.SS6.p5.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS6.p5.1.m1.1a"><mi id="S3.SS6.p5.1.m1.1.1" xref="S3.SS6.p5.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p5.1.m1.1b"><ci id="S3.SS6.p5.1.m1.1.1.cmml" xref="S3.SS6.p5.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p5.1.m1.1c">k</annotation></semantics></math>) to ensure temporal consistency.
Following the literature, we set <math id="S3.SS6.p5.2.m2.1" class="ltx_Math" alttext="k=5" display="inline"><semantics id="S3.SS6.p5.2.m2.1a"><mrow id="S3.SS6.p5.2.m2.1.1" xref="S3.SS6.p5.2.m2.1.1.cmml"><mi id="S3.SS6.p5.2.m2.1.1.2" xref="S3.SS6.p5.2.m2.1.1.2.cmml">k</mi><mo id="S3.SS6.p5.2.m2.1.1.1" xref="S3.SS6.p5.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS6.p5.2.m2.1.1.3" xref="S3.SS6.p5.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p5.2.m2.1b"><apply id="S3.SS6.p5.2.m2.1.1.cmml" xref="S3.SS6.p5.2.m2.1.1"><eq id="S3.SS6.p5.2.m2.1.1.1.cmml" xref="S3.SS6.p5.2.m2.1.1.1"></eq><ci id="S3.SS6.p5.2.m2.1.1.2.cmml" xref="S3.SS6.p5.2.m2.1.1.2">𝑘</ci><cn type="integer" id="S3.SS6.p5.2.m2.1.1.3.cmml" xref="S3.SS6.p5.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p5.2.m2.1c">k=5</annotation></semantics></math>.
We use FAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> for face detection and obtain tight crops as input.
We resize the faces to <math id="S3.SS6.p5.3.m3.1" class="ltx_Math" alttext="96\times 96" display="inline"><semantics id="S3.SS6.p5.3.m3.1a"><mrow id="S3.SS6.p5.3.m3.1.1" xref="S3.SS6.p5.3.m3.1.1.cmml"><mn id="S3.SS6.p5.3.m3.1.1.2" xref="S3.SS6.p5.3.m3.1.1.2.cmml">96</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS6.p5.3.m3.1.1.1" xref="S3.SS6.p5.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS6.p5.3.m3.1.1.3" xref="S3.SS6.p5.3.m3.1.1.3.cmml">96</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p5.3.m3.1b"><apply id="S3.SS6.p5.3.m3.1.1.cmml" xref="S3.SS6.p5.3.m3.1.1"><times id="S3.SS6.p5.3.m3.1.1.1.cmml" xref="S3.SS6.p5.3.m3.1.1.1"></times><cn type="integer" id="S3.SS6.p5.3.m3.1.1.2.cmml" xref="S3.SS6.p5.3.m3.1.1.2">96</cn><cn type="integer" id="S3.SS6.p5.3.m3.1.1.3.cmml" xref="S3.SS6.p5.3.m3.1.1.3">96</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p5.3.m3.1c">96\times 96</annotation></semantics></math> resolution since lip sync learning in high resolution holds further challenges <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> and the LRS2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> has low-resolution faces.
Then, we can apply GFPGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite> face enhancement method to the output video to increase the resolution for obtaining HR videos, if necessary.
Our audio encoder takes a mel-spectrogram of size <math id="S3.SS6.p5.4.m4.1" class="ltx_Math" alttext="16\times 80" display="inline"><semantics id="S3.SS6.p5.4.m4.1a"><mrow id="S3.SS6.p5.4.m4.1.1" xref="S3.SS6.p5.4.m4.1.1.cmml"><mn id="S3.SS6.p5.4.m4.1.1.2" xref="S3.SS6.p5.4.m4.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS6.p5.4.m4.1.1.1" xref="S3.SS6.p5.4.m4.1.1.1.cmml">×</mo><mn id="S3.SS6.p5.4.m4.1.1.3" xref="S3.SS6.p5.4.m4.1.1.3.cmml">80</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p5.4.m4.1b"><apply id="S3.SS6.p5.4.m4.1.1.cmml" xref="S3.SS6.p5.4.m4.1.1"><times id="S3.SS6.p5.4.m4.1.1.1.cmml" xref="S3.SS6.p5.4.m4.1.1.1"></times><cn type="integer" id="S3.SS6.p5.4.m4.1.1.2.cmml" xref="S3.SS6.p5.4.m4.1.1.2">16</cn><cn type="integer" id="S3.SS6.p5.4.m4.1.1.3.cmml" xref="S3.SS6.p5.4.m4.1.1.3">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p5.4.m4.1c">16\times 80</annotation></semantics></math> extracted from <math id="S3.SS6.p5.5.m5.1" class="ltx_Math" alttext="16kHz" display="inline"><semantics id="S3.SS6.p5.5.m5.1a"><mrow id="S3.SS6.p5.5.m5.1.1" xref="S3.SS6.p5.5.m5.1.1.cmml"><mn id="S3.SS6.p5.5.m5.1.1.2" xref="S3.SS6.p5.5.m5.1.1.2.cmml">16</mn><mo lspace="0em" rspace="0em" id="S3.SS6.p5.5.m5.1.1.1" xref="S3.SS6.p5.5.m5.1.1.1.cmml">​</mo><mi id="S3.SS6.p5.5.m5.1.1.3" xref="S3.SS6.p5.5.m5.1.1.3.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p5.5.m5.1.1.1a" xref="S3.SS6.p5.5.m5.1.1.1.cmml">​</mo><mi id="S3.SS6.p5.5.m5.1.1.4" xref="S3.SS6.p5.5.m5.1.1.4.cmml">H</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p5.5.m5.1.1.1b" xref="S3.SS6.p5.5.m5.1.1.1.cmml">​</mo><mi id="S3.SS6.p5.5.m5.1.1.5" xref="S3.SS6.p5.5.m5.1.1.5.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p5.5.m5.1b"><apply id="S3.SS6.p5.5.m5.1.1.cmml" xref="S3.SS6.p5.5.m5.1.1"><times id="S3.SS6.p5.5.m5.1.1.1.cmml" xref="S3.SS6.p5.5.m5.1.1.1"></times><cn type="integer" id="S3.SS6.p5.5.m5.1.1.2.cmml" xref="S3.SS6.p5.5.m5.1.1.2">16</cn><ci id="S3.SS6.p5.5.m5.1.1.3.cmml" xref="S3.SS6.p5.5.m5.1.1.3">𝑘</ci><ci id="S3.SS6.p5.5.m5.1.1.4.cmml" xref="S3.SS6.p5.5.m5.1.1.4">𝐻</ci><ci id="S3.SS6.p5.5.m5.1.1.5.cmml" xref="S3.SS6.p5.5.m5.1.1.5">𝑧</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p5.5.m5.1c">16kHz</annotation></semantics></math> audio.
The hop and the window sizes are <math id="S3.SS6.p5.6.m6.1" class="ltx_Math" alttext="200" display="inline"><semantics id="S3.SS6.p5.6.m6.1a"><mn id="S3.SS6.p5.6.m6.1.1" xref="S3.SS6.p5.6.m6.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S3.SS6.p5.6.m6.1b"><cn type="integer" id="S3.SS6.p5.6.m6.1.1.cmml" xref="S3.SS6.p5.6.m6.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p5.6.m6.1c">200</annotation></semantics></math> and <math id="S3.SS6.p5.7.m7.1" class="ltx_Math" alttext="800" display="inline"><semantics id="S3.SS6.p5.7.m7.1a"><mn id="S3.SS6.p5.7.m7.1.1" xref="S3.SS6.p5.7.m7.1.1.cmml">800</mn><annotation-xml encoding="MathML-Content" id="S3.SS6.p5.7.m7.1b"><cn type="integer" id="S3.SS6.p5.7.m7.1.1.cmml" xref="S3.SS6.p5.7.m7.1.1">800</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p5.7.m7.1c">800</annotation></semantics></math>, respectively.
We utilize the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>.
The learning rate is set to <math id="S3.SS6.p5.8.m8.1" class="ltx_Math" alttext="1\times 10^{-4}" display="inline"><semantics id="S3.SS6.p5.8.m8.1a"><mrow id="S3.SS6.p5.8.m8.1.1" xref="S3.SS6.p5.8.m8.1.1.cmml"><mn id="S3.SS6.p5.8.m8.1.1.2" xref="S3.SS6.p5.8.m8.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS6.p5.8.m8.1.1.1" xref="S3.SS6.p5.8.m8.1.1.1.cmml">×</mo><msup id="S3.SS6.p5.8.m8.1.1.3" xref="S3.SS6.p5.8.m8.1.1.3.cmml"><mn id="S3.SS6.p5.8.m8.1.1.3.2" xref="S3.SS6.p5.8.m8.1.1.3.2.cmml">10</mn><mrow id="S3.SS6.p5.8.m8.1.1.3.3" xref="S3.SS6.p5.8.m8.1.1.3.3.cmml"><mo id="S3.SS6.p5.8.m8.1.1.3.3a" xref="S3.SS6.p5.8.m8.1.1.3.3.cmml">−</mo><mn id="S3.SS6.p5.8.m8.1.1.3.3.2" xref="S3.SS6.p5.8.m8.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p5.8.m8.1b"><apply id="S3.SS6.p5.8.m8.1.1.cmml" xref="S3.SS6.p5.8.m8.1.1"><times id="S3.SS6.p5.8.m8.1.1.1.cmml" xref="S3.SS6.p5.8.m8.1.1.1"></times><cn type="integer" id="S3.SS6.p5.8.m8.1.1.2.cmml" xref="S3.SS6.p5.8.m8.1.1.2">1</cn><apply id="S3.SS6.p5.8.m8.1.1.3.cmml" xref="S3.SS6.p5.8.m8.1.1.3"><csymbol cd="ambiguous" id="S3.SS6.p5.8.m8.1.1.3.1.cmml" xref="S3.SS6.p5.8.m8.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS6.p5.8.m8.1.1.3.2.cmml" xref="S3.SS6.p5.8.m8.1.1.3.2">10</cn><apply id="S3.SS6.p5.8.m8.1.1.3.3.cmml" xref="S3.SS6.p5.8.m8.1.1.3.3"><minus id="S3.SS6.p5.8.m8.1.1.3.3.1.cmml" xref="S3.SS6.p5.8.m8.1.1.3.3"></minus><cn type="integer" id="S3.SS6.p5.8.m8.1.1.3.3.2.cmml" xref="S3.SS6.p5.8.m8.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p5.8.m8.1c">1\times 10^{-4}</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Results</h2>

<figure id="S4.F3" class="ltx_figure"><img src="/html/2405.04327/assets/x6.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="325" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">In the training, since extracting features from entire videos provides more informative features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>, we insert the generated faces into the corresponding part in the target video. Then, we use this video for feature extraction after cropping the mouth. </span></figcaption>
</figure>
<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text ltx_font_bold">Dataset</span>
We developed our talking face generator by utilizing the standard benchmark in the domain due to its diversity in terms of number of subjects: Lip Reading Sentence 2 (LRS2) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> training set.
The evaluation was conducted on the LRS2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>, LRW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>, and HDTF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite> datasets.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Metrics</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">For visual quality assessment, we utilize benchmark metrics in this field: FID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>, SSIM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>, and PSNR.
In evaluating lip sync, as in the literature, we employ Mouth Landmark Distance (LMD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> as well as LSE-C &amp; LSE-D metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>, which measure the confidence and distance scores via a pretrained SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>.
We also conduct a user study to perform human evaluation.
We compare our model with the state-of-the-art models that have publicly available codes and models, ensuring a fair comparison by evaluating under the same conditions.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The three widely utilized synchronization metrics in the literature have crucial problems.
The mouth landmark distance (LMD) measures the spatial difference between the mouth landmark points in the generated face and the target face.
However, this metric faces the following three issues:
(1) It is sensitive to errors in landmark detection.
(2) It cannot disentangle the synchronization and the generation stability.
For instance, if the model accurately generates lip movements but introduces errors in the mouth region (such as shifting the mouth/face), the landmark positions will subsequently change.
Although this does not affect the synchronization, it leads to a higher landmark distance, highlighting the need for disentanglement.
(3) LMD fails to consider lip movements or shapes properly.
Despite having the same lip shape, variation in the lips aperture and spreading cause misleading scores.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.28.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.29.2" class="ltx_text" style="font-size:90%;">Quantitative results on the test sets of LRS2 and LRW. While <span id="S4.T1.29.2.1" class="ltx_text" style="background-color:#D2F0AA;">green</span> indicates the best score, <span id="S4.T1.29.2.2" class="ltx_text" style="background-color:#FCEF9E;">yellow</span> shows the second best.</span></figcaption>
<div id="S4.T1.24" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:496.9pt;height:71.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-254.7pt,36.4pt) scale(0.493788356226049,0.493788356226049) ;">
<table id="S4.T1.24.24" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.24.24.25.1" class="ltx_tr">
<th id="S4.T1.24.24.25.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S4.T1.24.24.25.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="9"><span id="S4.T1.24.24.25.1.2.1" class="ltx_text ltx_font_bold">LRS2</span></th>
<th id="S4.T1.24.24.25.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="9"><span id="S4.T1.24.24.25.1.3.1" class="ltx_text ltx_font_bold">LRW</span></th>
</tr>
<tr id="S4.T1.24.24.24" class="ltx_tr">
<th id="S4.T1.24.24.24.25" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Method</th>
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">SSIM <math id="S4.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">PSNR <math id="S4.T1.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T1.2.2.2.2.m1.1.1" xref="S4.T1.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FID <math id="S4.T1.3.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T1.3.3.3.3.m1.1.1" xref="S4.T1.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T1.4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">LMD <math id="S4.T1.4.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.4.4.4.4.m1.1a"><mo stretchy="false" id="S4.T1.4.4.4.4.m1.1.1" xref="S4.T1.4.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.4.m1.1b"><ci id="S4.T1.4.4.4.4.m1.1.1.cmml" xref="S4.T1.4.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T1.5.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">LSE-C <math id="S4.T1.5.5.5.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.5.5.5.5.m1.1a"><mo stretchy="false" id="S4.T1.5.5.5.5.m1.1.1" xref="S4.T1.5.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.5.m1.1b"><ci id="S4.T1.5.5.5.5.m1.1.1.cmml" xref="S4.T1.5.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.5.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.6.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">LSE-D <math id="S4.T1.6.6.6.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.6.6.6.6.m1.1a"><mo stretchy="false" id="S4.T1.6.6.6.6.m1.1.1" xref="S4.T1.6.6.6.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.6.m1.1b"><ci id="S4.T1.6.6.6.6.m1.1.1.cmml" xref="S4.T1.6.6.6.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.6.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T1.8.8.8.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AVS<sub id="S4.T1.8.8.8.8.1" class="ltx_sub"><span id="S4.T1.8.8.8.8.1.1" class="ltx_text ltx_font_italic">u</span></sub> <math id="S4.T1.8.8.8.8.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.8.8.8.8.m2.1a"><mo stretchy="false" id="S4.T1.8.8.8.8.m2.1.1" xref="S4.T1.8.8.8.8.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.8.8.m2.1b"><ci id="S4.T1.8.8.8.8.m2.1.1.cmml" xref="S4.T1.8.8.8.8.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.8.8.m2.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.10.10.10.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AVS<sub id="S4.T1.10.10.10.10.1" class="ltx_sub"><span id="S4.T1.10.10.10.10.1.1" class="ltx_text ltx_font_italic">m</span></sub> <math id="S4.T1.10.10.10.10.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.10.10.10.10.m2.1a"><mo stretchy="false" id="S4.T1.10.10.10.10.m2.1.1" xref="S4.T1.10.10.10.10.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.10.10.m2.1b"><ci id="S4.T1.10.10.10.10.m2.1.1.cmml" xref="S4.T1.10.10.10.10.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.10.10.m2.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.12.12.12.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">AVS<sub id="S4.T1.12.12.12.12.1" class="ltx_sub"><span id="S4.T1.12.12.12.12.1.1" class="ltx_text ltx_font_italic">v</span></sub> <math id="S4.T1.12.12.12.12.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.12.12.12.12.m2.1a"><mo stretchy="false" id="S4.T1.12.12.12.12.m2.1.1" xref="S4.T1.12.12.12.12.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.12.12.12.12.m2.1b"><ci id="S4.T1.12.12.12.12.m2.1.1.cmml" xref="S4.T1.12.12.12.12.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.12.12.12.m2.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.13.13.13.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">SSIM <math id="S4.T1.13.13.13.13.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.13.13.13.13.m1.1a"><mo stretchy="false" id="S4.T1.13.13.13.13.m1.1.1" xref="S4.T1.13.13.13.13.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.13.13.13.13.m1.1b"><ci id="S4.T1.13.13.13.13.m1.1.1.cmml" xref="S4.T1.13.13.13.13.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.13.13.13.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.14.14.14.14" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">PSNR <math id="S4.T1.14.14.14.14.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.14.14.14.14.m1.1a"><mo stretchy="false" id="S4.T1.14.14.14.14.m1.1.1" xref="S4.T1.14.14.14.14.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.14.14.14.14.m1.1b"><ci id="S4.T1.14.14.14.14.m1.1.1.cmml" xref="S4.T1.14.14.14.14.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.14.14.14.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.15.15.15.15" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FID <math id="S4.T1.15.15.15.15.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.15.15.15.15.m1.1a"><mo stretchy="false" id="S4.T1.15.15.15.15.m1.1.1" xref="S4.T1.15.15.15.15.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.15.15.15.15.m1.1b"><ci id="S4.T1.15.15.15.15.m1.1.1.cmml" xref="S4.T1.15.15.15.15.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.15.15.15.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T1.16.16.16.16" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">LMD <math id="S4.T1.16.16.16.16.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.16.16.16.16.m1.1a"><mo stretchy="false" id="S4.T1.16.16.16.16.m1.1.1" xref="S4.T1.16.16.16.16.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.16.16.16.16.m1.1b"><ci id="S4.T1.16.16.16.16.m1.1.1.cmml" xref="S4.T1.16.16.16.16.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.16.16.16.16.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T1.17.17.17.17" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">LSE-C <math id="S4.T1.17.17.17.17.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.17.17.17.17.m1.1a"><mo stretchy="false" id="S4.T1.17.17.17.17.m1.1.1" xref="S4.T1.17.17.17.17.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.17.17.17.17.m1.1b"><ci id="S4.T1.17.17.17.17.m1.1.1.cmml" xref="S4.T1.17.17.17.17.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.17.17.17.17.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.18.18.18.18" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">LSE-D <math id="S4.T1.18.18.18.18.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.18.18.18.18.m1.1a"><mo stretchy="false" id="S4.T1.18.18.18.18.m1.1.1" xref="S4.T1.18.18.18.18.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.18.18.18.18.m1.1b"><ci id="S4.T1.18.18.18.18.m1.1.1.cmml" xref="S4.T1.18.18.18.18.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.18.18.18.18.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T1.20.20.20.20" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AVS<sub id="S4.T1.20.20.20.20.1" class="ltx_sub"><span id="S4.T1.20.20.20.20.1.1" class="ltx_text ltx_font_italic">u</span></sub> <math id="S4.T1.20.20.20.20.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.20.20.20.20.m2.1a"><mo stretchy="false" id="S4.T1.20.20.20.20.m2.1.1" xref="S4.T1.20.20.20.20.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.20.20.20.20.m2.1b"><ci id="S4.T1.20.20.20.20.m2.1.1.cmml" xref="S4.T1.20.20.20.20.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.20.20.20.20.m2.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.22.22.22.22" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">AVS<sub id="S4.T1.22.22.22.22.1" class="ltx_sub"><span id="S4.T1.22.22.22.22.1.1" class="ltx_text ltx_font_italic">m</span></sub> <math id="S4.T1.22.22.22.22.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.22.22.22.22.m2.1a"><mo stretchy="false" id="S4.T1.22.22.22.22.m2.1.1" xref="S4.T1.22.22.22.22.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.22.22.22.22.m2.1b"><ci id="S4.T1.22.22.22.22.m2.1.1.cmml" xref="S4.T1.22.22.22.22.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.22.22.22.22.m2.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T1.24.24.24.24" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t">AVS<sub id="S4.T1.24.24.24.24.1" class="ltx_sub"><span id="S4.T1.24.24.24.24.1.1" class="ltx_text ltx_font_italic">v</span></sub> <math id="S4.T1.24.24.24.24.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.24.24.24.24.m2.1a"><mo stretchy="false" id="S4.T1.24.24.24.24.m2.1.1" xref="S4.T1.24.24.24.24.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.24.24.24.24.m2.1b"><ci id="S4.T1.24.24.24.24.m2.1.1.cmml" xref="S4.T1.24.24.24.24.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.24.24.24.24.m2.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.24.24.26.1" class="ltx_tr">
<th id="S4.T1.24.24.26.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Wav2Lip <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>
</th>
<td id="S4.T1.24.24.26.1.2" class="ltx_td ltx_align_center ltx_border_t">0.865</td>
<td id="S4.T1.24.24.26.1.3" class="ltx_td ltx_align_center ltx_border_t">26.538</td>
<td id="S4.T1.24.24.26.1.4" class="ltx_td ltx_align_center ltx_border_t">7.05</td>
<td id="S4.T1.24.24.26.1.5" class="ltx_td ltx_align_center ltx_border_t">2.388</td>
<td id="S4.T1.24.24.26.1.6" class="ltx_td ltx_align_center ltx_border_t">7.594</td>
<td id="S4.T1.24.24.26.1.7" class="ltx_td ltx_align_center ltx_border_t">6.759</td>
<td id="S4.T1.24.24.26.1.8" class="ltx_td ltx_align_center ltx_border_t">0.248</td>
<td id="S4.T1.24.24.26.1.9" class="ltx_td ltx_align_center ltx_border_t">0.659</td>
<td id="S4.T1.24.24.26.1.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.289</td>
<td id="S4.T1.24.24.26.1.11" class="ltx_td ltx_align_center ltx_border_t">0.851</td>
<td id="S4.T1.24.24.26.1.12" class="ltx_td ltx_align_center ltx_border_t">25.144</td>
<td id="S4.T1.24.24.26.1.13" class="ltx_td ltx_align_center ltx_border_t">6.81</td>
<td id="S4.T1.24.24.26.1.14" class="ltx_td ltx_align_center ltx_border_t">2.147</td>
<td id="S4.T1.24.24.26.1.15" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.24.24.26.1.15.1" class="ltx_text" style="background-color:#FCEF9E;">7.490</span></td>
<td id="S4.T1.24.24.26.1.16" class="ltx_td ltx_align_center ltx_border_t">6.512</td>
<td id="S4.T1.24.24.26.1.17" class="ltx_td ltx_align_center ltx_border_t">0.242</td>
<td id="S4.T1.24.24.26.1.18" class="ltx_td ltx_align_center ltx_border_t">0.537</td>
<td id="S4.T1.24.24.26.1.19" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.268</td>
</tr>
<tr id="S4.T1.24.24.27.2" class="ltx_tr">
<th id="S4.T1.24.24.27.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VRT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>
</th>
<td id="S4.T1.24.24.27.2.2" class="ltx_td ltx_align_center">0.841</td>
<td id="S4.T1.24.24.27.2.3" class="ltx_td ltx_align_center">25.584</td>
<td id="S4.T1.24.24.27.2.4" class="ltx_td ltx_align_center">9.28</td>
<td id="S4.T1.24.24.27.2.5" class="ltx_td ltx_align_center">2.612</td>
<td id="S4.T1.24.24.27.2.6" class="ltx_td ltx_align_center">7.499</td>
<td id="S4.T1.24.24.27.2.7" class="ltx_td ltx_align_center">6.824</td>
<td id="S4.T1.24.24.27.2.8" class="ltx_td ltx_align_center">0.361</td>
<td id="S4.T1.24.24.27.2.9" class="ltx_td ltx_align_center">0.763</td>
<td id="S4.T1.24.24.27.2.10" class="ltx_td ltx_align_center ltx_border_r">0.426</td>
<td id="S4.T1.24.24.27.2.11" class="ltx_td ltx_align_center">0.873</td>
<td id="S4.T1.24.24.27.2.12" class="ltx_td ltx_align_center">27.110</td>
<td id="S4.T1.24.24.27.2.13" class="ltx_td ltx_align_center"><span id="S4.T1.24.24.27.2.13.1" class="ltx_text" style="background-color:#D2F0AA;">5.30</span></td>
<td id="S4.T1.24.24.27.2.14" class="ltx_td ltx_align_center">2.390</td>
<td id="S4.T1.24.24.27.2.15" class="ltx_td ltx_align_center">6.598</td>
<td id="S4.T1.24.24.27.2.16" class="ltx_td ltx_align_center">7.123</td>
<td id="S4.T1.24.24.27.2.17" class="ltx_td ltx_align_center">0.383</td>
<td id="S4.T1.24.24.27.2.18" class="ltx_td ltx_align_center">0.758</td>
<td id="S4.T1.24.24.27.2.19" class="ltx_td ltx_nopad_r ltx_align_center">0.557</td>
</tr>
<tr id="S4.T1.24.24.28.3" class="ltx_tr">
<th id="S4.T1.24.24.28.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">DINet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>]</cite>
</th>
<td id="S4.T1.24.24.28.3.2" class="ltx_td ltx_align_center">0.785</td>
<td id="S4.T1.24.24.28.3.3" class="ltx_td ltx_align_center">24.354</td>
<td id="S4.T1.24.24.28.3.4" class="ltx_td ltx_align_center"><span id="S4.T1.24.24.28.3.4.1" class="ltx_text" style="background-color:#FCEF9E;">4.26</span></td>
<td id="S4.T1.24.24.28.3.5" class="ltx_td ltx_align_center">2.301</td>
<td id="S4.T1.24.24.28.3.6" class="ltx_td ltx_align_center">5.376</td>
<td id="S4.T1.24.24.28.3.7" class="ltx_td ltx_align_center">8.376</td>
<td id="S4.T1.24.24.28.3.8" class="ltx_td ltx_align_center">0.291</td>
<td id="S4.T1.24.24.28.3.9" class="ltx_td ltx_align_center">0.758</td>
<td id="S4.T1.24.24.28.3.10" class="ltx_td ltx_align_center ltx_border_r">0.425</td>
<td id="S4.T1.24.24.28.3.11" class="ltx_td ltx_align_center">0.886</td>
<td id="S4.T1.24.24.28.3.12" class="ltx_td ltx_align_center">27.501</td>
<td id="S4.T1.24.24.28.3.13" class="ltx_td ltx_align_center">8.17</td>
<td id="S4.T1.24.24.28.3.14" class="ltx_td ltx_align_center">1.963</td>
<td id="S4.T1.24.24.28.3.15" class="ltx_td ltx_align_center">5.249</td>
<td id="S4.T1.24.24.28.3.16" class="ltx_td ltx_align_center">9.099</td>
<td id="S4.T1.24.24.28.3.17" class="ltx_td ltx_align_center">0.276</td>
<td id="S4.T1.24.24.28.3.18" class="ltx_td ltx_align_center">0.638</td>
<td id="S4.T1.24.24.28.3.19" class="ltx_td ltx_nopad_r ltx_align_center">0.360</td>
</tr>
<tr id="S4.T1.24.24.29.4" class="ltx_tr">
<th id="S4.T1.24.24.29.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TalkLip <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>
</th>
<td id="S4.T1.24.24.29.4.2" class="ltx_td ltx_align_center">0.860</td>
<td id="S4.T1.24.24.29.4.3" class="ltx_td ltx_align_center">26.112</td>
<td id="S4.T1.24.24.29.4.4" class="ltx_td ltx_align_center">4.94</td>
<td id="S4.T1.24.24.29.4.5" class="ltx_td ltx_align_center">2.344</td>
<td id="S4.T1.24.24.29.4.6" class="ltx_td ltx_align_center"><span id="S4.T1.24.24.29.4.6.1" class="ltx_text" style="background-color:#D2F0AA;">8.530</span></td>
<td id="S4.T1.24.24.29.4.7" class="ltx_td ltx_align_center"><span id="S4.T1.24.24.29.4.7.1" class="ltx_text" style="background-color:#D2F0AA;">6.086</span></td>
<td id="S4.T1.24.24.29.4.8" class="ltx_td ltx_align_center"><span id="S4.T1.24.24.29.4.8.1" class="ltx_text" style="background-color:#D2F0AA;">0.570</span></td>
<td id="S4.T1.24.24.29.4.9" class="ltx_td ltx_align_center"><span id="S4.T1.24.24.29.4.9.1" class="ltx_text" style="background-color:#FCEF9E;">0.895</span></td>
<td id="S4.T1.24.24.29.4.10" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.24.24.29.4.10.1" class="ltx_text" style="background-color:#FCEF9E;">0.702</span></td>
<td id="S4.T1.24.24.29.4.11" class="ltx_td ltx_align_center">0.868</td>
<td id="S4.T1.24.24.29.4.12" class="ltx_td ltx_align_center">26.349</td>
<td id="S4.T1.24.24.29.4.13" class="ltx_td ltx_align_center">15.73</td>
<td id="S4.T1.24.24.29.4.14" class="ltx_td ltx_align_center">1.836</td>
<td id="S4.T1.24.24.29.4.15" class="ltx_td ltx_align_center">7.281</td>
<td id="S4.T1.24.24.29.4.16" class="ltx_td ltx_align_center"><span id="S4.T1.24.24.29.4.16.1" class="ltx_text" style="background-color:#FCEF9E;">6.485</span></td>
<td id="S4.T1.24.24.29.4.17" class="ltx_td ltx_align_center"><span id="S4.T1.24.24.29.4.17.1" class="ltx_text" style="background-color:#D2F0AA;">0.581</span></td>
<td id="S4.T1.24.24.29.4.18" class="ltx_td ltx_align_center"><span id="S4.T1.24.24.29.4.18.1" class="ltx_text" style="background-color:#FCEF9E;">0.813</span></td>
<td id="S4.T1.24.24.29.4.19" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.24.24.29.4.19.1" class="ltx_text" style="background-color:#FCEF9E;">0.604</span></td>
</tr>
<tr id="S4.T1.24.24.30.5" class="ltx_tr">
<th id="S4.T1.24.24.30.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">IPLAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite>
</th>
<td id="S4.T1.24.24.30.5.2" class="ltx_td ltx_align_center"><span id="S4.T1.24.24.30.5.2.1" class="ltx_text" style="background-color:#FCEF9E;">0.877</span></td>
<td id="S4.T1.24.24.30.5.3" class="ltx_td ltx_align_center"><span id="S4.T1.24.24.30.5.3.1" class="ltx_text" style="background-color:#FCEF9E;">29.670</span></td>
<td id="S4.T1.24.24.30.5.4" class="ltx_td ltx_align_center"><span id="S4.T1.24.24.30.5.4.1" class="ltx_text" style="background-color:#D2F0AA;">4.10</span></td>
<td id="S4.T1.24.24.30.5.5" class="ltx_td ltx_align_center"><span id="S4.T1.24.24.30.5.5.1" class="ltx_text" style="background-color:#FCEF9E;">2.119</span></td>
<td id="S4.T1.24.24.30.5.6" class="ltx_td ltx_align_center">6.495</td>
<td id="S4.T1.24.24.30.5.7" class="ltx_td ltx_align_center">7.165</td>
<td id="S4.T1.24.24.30.5.8" class="ltx_td ltx_align_center">0.331</td>
<td id="S4.T1.24.24.30.5.9" class="ltx_td ltx_align_center">0.711</td>
<td id="S4.T1.24.24.30.5.10" class="ltx_td ltx_align_center ltx_border_r">0.479</td>
<td id="S4.T1.24.24.30.5.11" class="ltx_td ltx_align_center"><span id="S4.T1.24.24.30.5.11.1" class="ltx_text" style="background-color:#FCEF9E;">0.917</span></td>
<td id="S4.T1.24.24.30.5.12" class="ltx_td ltx_align_center"><span id="S4.T1.24.24.30.5.12.1" class="ltx_text" style="background-color:#D2F0AA;">30.456</span></td>
<td id="S4.T1.24.24.30.5.13" class="ltx_td ltx_align_center">8.40</td>
<td id="S4.T1.24.24.30.5.14" class="ltx_td ltx_align_center"><span id="S4.T1.24.24.30.5.14.1" class="ltx_text" style="background-color:#FCEF9E;">1.641</span></td>
<td id="S4.T1.24.24.30.5.15" class="ltx_td ltx_align_center">5.949</td>
<td id="S4.T1.24.24.30.5.16" class="ltx_td ltx_align_center">7.767</td>
<td id="S4.T1.24.24.30.5.17" class="ltx_td ltx_align_center">0.272</td>
<td id="S4.T1.24.24.30.5.18" class="ltx_td ltx_align_center">0.593</td>
<td id="S4.T1.24.24.30.5.19" class="ltx_td ltx_nopad_r ltx_align_center">0.331</td>
</tr>
<tr id="S4.T1.24.24.31.6" class="ltx_tr">
<th id="S4.T1.24.24.31.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">Ours</th>
<td id="S4.T1.24.24.31.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.24.24.31.6.2.1" class="ltx_text" style="background-color:#D2F0AA;">0.947</span></td>
<td id="S4.T1.24.24.31.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.24.24.31.6.3.1" class="ltx_text" style="background-color:#D2F0AA;">31.273</span></td>
<td id="S4.T1.24.24.31.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">4.51</td>
<td id="S4.T1.24.24.31.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.24.24.31.6.5.1" class="ltx_text" style="background-color:#D2F0AA;">1.188</span></td>
<td id="S4.T1.24.24.31.6.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.24.24.31.6.6.1" class="ltx_text" style="background-color:#FCEF9E;">7.958</span></td>
<td id="S4.T1.24.24.31.6.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.24.24.31.6.7.1" class="ltx_text" style="background-color:#FCEF9E;">6.301</span></td>
<td id="S4.T1.24.24.31.6.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.24.24.31.6.8.1" class="ltx_text" style="background-color:#FCEF9E;">0.508</span></td>
<td id="S4.T1.24.24.31.6.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.24.24.31.6.9.1" class="ltx_text" style="background-color:#D2F0AA;">0.939</span></td>
<td id="S4.T1.24.24.31.6.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T1.24.24.31.6.10.1" class="ltx_text" style="background-color:#D2F0AA;">0.879</span></td>
<td id="S4.T1.24.24.31.6.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.24.24.31.6.11.1" class="ltx_text" style="background-color:#D2F0AA;">0.919</span></td>
<td id="S4.T1.24.24.31.6.12" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.24.24.31.6.12.1" class="ltx_text" style="background-color:#FCEF9E;">30.185</span></td>
<td id="S4.T1.24.24.31.6.13" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.24.24.31.6.13.1" class="ltx_text" style="background-color:#FCEF9E;">6.21</span></td>
<td id="S4.T1.24.24.31.6.14" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.24.24.31.6.14.1" class="ltx_text" style="background-color:#D2F0AA;">1.487</span></td>
<td id="S4.T1.24.24.31.6.15" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.24.24.31.6.15.1" class="ltx_text" style="background-color:#D2F0AA;">7.738</span></td>
<td id="S4.T1.24.24.31.6.16" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.24.24.31.6.16.1" class="ltx_text" style="background-color:#D2F0AA;">6.456</span></td>
<td id="S4.T1.24.24.31.6.17" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.24.24.31.6.17.1" class="ltx_text" style="background-color:#FCEF9E;">0.554</span></td>
<td id="S4.T1.24.24.31.6.18" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.24.24.31.6.18.1" class="ltx_text" style="background-color:#D2F0AA;">0.856</span></td>
<td id="S4.T1.24.24.31.6.19" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.24.24.31.6.19.1" class="ltx_text" style="background-color:#D2F0AA;">0.762</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Recently proposed LSE-C and LSE-D metrics are more informative than LMD.
However, they also hold vital issues.
These metrics rely on audio and lip features extracted by the pretrained SyncNet model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>, which contains audio and image encoders, and was trained with audio-lip pairs for learning lip sync.
However, SyncNet is vulnerable against translations in the data due to not being properly shift invariant <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> (see <a href="#S4.F5.sf3" title="In Figure 5 ‣ 4.2 Quantitative Evaluation ‣ 4 Experimental Results ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5(c)</span></a>).
Therefore, small translations in the face affects LSE-C and LSE-D metrics, resulting in not fully disentangled lip synchronization evaluation.
Moreover, the margin around the faces has also impact on extracted features by SyncNet, yielding inconsistent LSE-C &amp; D scores.
To tackle these problems, we introduce three novel lip synchronization evaluation metrics.
We employ AV-HuBERT, a robust audio-visual speech representation learning model, to obtain superior audio and visual feature representations to measure synchronization.
</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Unsupervised Audio-Visual Synchronization (AVS<sub id="S4.SS1.p4.1.1.1" class="ltx_sub"><span id="S4.SS1.p4.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">u</span></sub>)</span>
In this metric, we measure the lip sync by only considering the given audio and the generated video.
Specifically, we extract audio and visual features (lips) from the transformer encoder block of AV-HuBERT.
Subsequently, we compute the cosine similarity between these two feature representations, as several works demonstrate the superiority of cosine similarity for audio-visual alignment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite> and speaker recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite> .
This metric does not require GT data and thus can flexibly be applied to any data, as LSE-C &amp; D.
As AV-HuBERT provides more robust feature representation than SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>, this metric is more consistent, reliable, and not vulnerable to translation (see <a href="#S1.F1" title="In 1 Introduction ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S4.F5" title="In 4.2 Quantitative Evaluation ‣ 4 Experimental Results ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>).</p>
<table id="S4.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E4.m1.4" class="ltx_Math" alttext="\text{AVS}_{u}=CS(TE(V^{G}_{1:T},\mathbf{0}),TE(\mathbf{0},A_{1:T}))" display="block"><semantics id="S4.E4.m1.4a"><mrow id="S4.E4.m1.4.4" xref="S4.E4.m1.4.4.cmml"><msub id="S4.E4.m1.4.4.4" xref="S4.E4.m1.4.4.4.cmml"><mtext id="S4.E4.m1.4.4.4.2" xref="S4.E4.m1.4.4.4.2a.cmml">AVS</mtext><mi id="S4.E4.m1.4.4.4.3" xref="S4.E4.m1.4.4.4.3.cmml">u</mi></msub><mo id="S4.E4.m1.4.4.3" xref="S4.E4.m1.4.4.3.cmml">=</mo><mrow id="S4.E4.m1.4.4.2" xref="S4.E4.m1.4.4.2.cmml"><mi id="S4.E4.m1.4.4.2.4" xref="S4.E4.m1.4.4.2.4.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.4.4.2.3" xref="S4.E4.m1.4.4.2.3.cmml">​</mo><mi id="S4.E4.m1.4.4.2.5" xref="S4.E4.m1.4.4.2.5.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.4.4.2.3a" xref="S4.E4.m1.4.4.2.3.cmml">​</mo><mrow id="S4.E4.m1.4.4.2.2.2" xref="S4.E4.m1.4.4.2.2.3.cmml"><mo stretchy="false" id="S4.E4.m1.4.4.2.2.2.3" xref="S4.E4.m1.4.4.2.2.3.cmml">(</mo><mrow id="S4.E4.m1.3.3.1.1.1.1" xref="S4.E4.m1.3.3.1.1.1.1.cmml"><mi id="S4.E4.m1.3.3.1.1.1.1.3" xref="S4.E4.m1.3.3.1.1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.1.1.1.1.2" xref="S4.E4.m1.3.3.1.1.1.1.2.cmml">​</mo><mi id="S4.E4.m1.3.3.1.1.1.1.4" xref="S4.E4.m1.3.3.1.1.1.1.4.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.3.3.1.1.1.1.2a" xref="S4.E4.m1.3.3.1.1.1.1.2.cmml">​</mo><mrow id="S4.E4.m1.3.3.1.1.1.1.1.1" xref="S4.E4.m1.3.3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E4.m1.3.3.1.1.1.1.1.1.2" xref="S4.E4.m1.3.3.1.1.1.1.1.2.cmml">(</mo><msubsup id="S4.E4.m1.3.3.1.1.1.1.1.1.1" xref="S4.E4.m1.3.3.1.1.1.1.1.1.1.cmml"><mi id="S4.E4.m1.3.3.1.1.1.1.1.1.1.2.2" xref="S4.E4.m1.3.3.1.1.1.1.1.1.1.2.2.cmml">V</mi><mrow id="S4.E4.m1.3.3.1.1.1.1.1.1.1.3" xref="S4.E4.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mn id="S4.E4.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S4.E4.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S4.E4.m1.3.3.1.1.1.1.1.1.1.3.1" xref="S4.E4.m1.3.3.1.1.1.1.1.1.1.3.1.cmml">:</mo><mi id="S4.E4.m1.3.3.1.1.1.1.1.1.1.3.3" xref="S4.E4.m1.3.3.1.1.1.1.1.1.1.3.3.cmml">T</mi></mrow><mi id="S4.E4.m1.3.3.1.1.1.1.1.1.1.2.3" xref="S4.E4.m1.3.3.1.1.1.1.1.1.1.2.3.cmml">G</mi></msubsup><mo id="S4.E4.m1.3.3.1.1.1.1.1.1.3" xref="S4.E4.m1.3.3.1.1.1.1.1.2.cmml">,</mo><mn id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml">𝟎</mn><mo stretchy="false" id="S4.E4.m1.3.3.1.1.1.1.1.1.4" xref="S4.E4.m1.3.3.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E4.m1.4.4.2.2.2.4" xref="S4.E4.m1.4.4.2.2.3.cmml">,</mo><mrow id="S4.E4.m1.4.4.2.2.2.2" xref="S4.E4.m1.4.4.2.2.2.2.cmml"><mi id="S4.E4.m1.4.4.2.2.2.2.3" xref="S4.E4.m1.4.4.2.2.2.2.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.4.4.2.2.2.2.2" xref="S4.E4.m1.4.4.2.2.2.2.2.cmml">​</mo><mi id="S4.E4.m1.4.4.2.2.2.2.4" xref="S4.E4.m1.4.4.2.2.2.2.4.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.E4.m1.4.4.2.2.2.2.2a" xref="S4.E4.m1.4.4.2.2.2.2.2.cmml">​</mo><mrow id="S4.E4.m1.4.4.2.2.2.2.1.1" xref="S4.E4.m1.4.4.2.2.2.2.1.2.cmml"><mo stretchy="false" id="S4.E4.m1.4.4.2.2.2.2.1.1.2" xref="S4.E4.m1.4.4.2.2.2.2.1.2.cmml">(</mo><mn id="S4.E4.m1.2.2" xref="S4.E4.m1.2.2.cmml">𝟎</mn><mo id="S4.E4.m1.4.4.2.2.2.2.1.1.3" xref="S4.E4.m1.4.4.2.2.2.2.1.2.cmml">,</mo><msub id="S4.E4.m1.4.4.2.2.2.2.1.1.1" xref="S4.E4.m1.4.4.2.2.2.2.1.1.1.cmml"><mi id="S4.E4.m1.4.4.2.2.2.2.1.1.1.2" xref="S4.E4.m1.4.4.2.2.2.2.1.1.1.2.cmml">A</mi><mrow id="S4.E4.m1.4.4.2.2.2.2.1.1.1.3" xref="S4.E4.m1.4.4.2.2.2.2.1.1.1.3.cmml"><mn id="S4.E4.m1.4.4.2.2.2.2.1.1.1.3.2" xref="S4.E4.m1.4.4.2.2.2.2.1.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S4.E4.m1.4.4.2.2.2.2.1.1.1.3.1" xref="S4.E4.m1.4.4.2.2.2.2.1.1.1.3.1.cmml">:</mo><mi id="S4.E4.m1.4.4.2.2.2.2.1.1.1.3.3" xref="S4.E4.m1.4.4.2.2.2.2.1.1.1.3.3.cmml">T</mi></mrow></msub><mo stretchy="false" id="S4.E4.m1.4.4.2.2.2.2.1.1.4" xref="S4.E4.m1.4.4.2.2.2.2.1.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E4.m1.4.4.2.2.2.5" xref="S4.E4.m1.4.4.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.4b"><apply id="S4.E4.m1.4.4.cmml" xref="S4.E4.m1.4.4"><eq id="S4.E4.m1.4.4.3.cmml" xref="S4.E4.m1.4.4.3"></eq><apply id="S4.E4.m1.4.4.4.cmml" xref="S4.E4.m1.4.4.4"><csymbol cd="ambiguous" id="S4.E4.m1.4.4.4.1.cmml" xref="S4.E4.m1.4.4.4">subscript</csymbol><ci id="S4.E4.m1.4.4.4.2a.cmml" xref="S4.E4.m1.4.4.4.2"><mtext id="S4.E4.m1.4.4.4.2.cmml" xref="S4.E4.m1.4.4.4.2">AVS</mtext></ci><ci id="S4.E4.m1.4.4.4.3.cmml" xref="S4.E4.m1.4.4.4.3">𝑢</ci></apply><apply id="S4.E4.m1.4.4.2.cmml" xref="S4.E4.m1.4.4.2"><times id="S4.E4.m1.4.4.2.3.cmml" xref="S4.E4.m1.4.4.2.3"></times><ci id="S4.E4.m1.4.4.2.4.cmml" xref="S4.E4.m1.4.4.2.4">𝐶</ci><ci id="S4.E4.m1.4.4.2.5.cmml" xref="S4.E4.m1.4.4.2.5">𝑆</ci><interval closure="open" id="S4.E4.m1.4.4.2.2.3.cmml" xref="S4.E4.m1.4.4.2.2.2"><apply id="S4.E4.m1.3.3.1.1.1.1.cmml" xref="S4.E4.m1.3.3.1.1.1.1"><times id="S4.E4.m1.3.3.1.1.1.1.2.cmml" xref="S4.E4.m1.3.3.1.1.1.1.2"></times><ci id="S4.E4.m1.3.3.1.1.1.1.3.cmml" xref="S4.E4.m1.3.3.1.1.1.1.3">𝑇</ci><ci id="S4.E4.m1.3.3.1.1.1.1.4.cmml" xref="S4.E4.m1.3.3.1.1.1.1.4">𝐸</ci><interval closure="open" id="S4.E4.m1.3.3.1.1.1.1.1.2.cmml" xref="S4.E4.m1.3.3.1.1.1.1.1.1"><apply id="S4.E4.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.3.3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E4.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.3.3.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E4.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.3.3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E4.m1.3.3.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E4.m1.3.3.1.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E4.m1.3.3.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E4.m1.3.3.1.1.1.1.1.1.1.2.2">𝑉</ci><ci id="S4.E4.m1.3.3.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E4.m1.3.3.1.1.1.1.1.1.1.2.3">𝐺</ci></apply><apply id="S4.E4.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.3.3.1.1.1.1.1.1.1.3"><ci id="S4.E4.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E4.m1.3.3.1.1.1.1.1.1.1.3.1">:</ci><cn type="integer" id="S4.E4.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E4.m1.3.3.1.1.1.1.1.1.1.3.2">1</cn><ci id="S4.E4.m1.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E4.m1.3.3.1.1.1.1.1.1.1.3.3">𝑇</ci></apply></apply><cn type="integer" id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1">0</cn></interval></apply><apply id="S4.E4.m1.4.4.2.2.2.2.cmml" xref="S4.E4.m1.4.4.2.2.2.2"><times id="S4.E4.m1.4.4.2.2.2.2.2.cmml" xref="S4.E4.m1.4.4.2.2.2.2.2"></times><ci id="S4.E4.m1.4.4.2.2.2.2.3.cmml" xref="S4.E4.m1.4.4.2.2.2.2.3">𝑇</ci><ci id="S4.E4.m1.4.4.2.2.2.2.4.cmml" xref="S4.E4.m1.4.4.2.2.2.2.4">𝐸</ci><interval closure="open" id="S4.E4.m1.4.4.2.2.2.2.1.2.cmml" xref="S4.E4.m1.4.4.2.2.2.2.1.1"><cn type="integer" id="S4.E4.m1.2.2.cmml" xref="S4.E4.m1.2.2">0</cn><apply id="S4.E4.m1.4.4.2.2.2.2.1.1.1.cmml" xref="S4.E4.m1.4.4.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.E4.m1.4.4.2.2.2.2.1.1.1.1.cmml" xref="S4.E4.m1.4.4.2.2.2.2.1.1.1">subscript</csymbol><ci id="S4.E4.m1.4.4.2.2.2.2.1.1.1.2.cmml" xref="S4.E4.m1.4.4.2.2.2.2.1.1.1.2">𝐴</ci><apply id="S4.E4.m1.4.4.2.2.2.2.1.1.1.3.cmml" xref="S4.E4.m1.4.4.2.2.2.2.1.1.1.3"><ci id="S4.E4.m1.4.4.2.2.2.2.1.1.1.3.1.cmml" xref="S4.E4.m1.4.4.2.2.2.2.1.1.1.3.1">:</ci><cn type="integer" id="S4.E4.m1.4.4.2.2.2.2.1.1.1.3.2.cmml" xref="S4.E4.m1.4.4.2.2.2.2.1.1.1.3.2">1</cn><ci id="S4.E4.m1.4.4.2.2.2.2.1.1.1.3.3.cmml" xref="S4.E4.m1.4.4.2.2.2.2.1.1.1.3.3">𝑇</ci></apply></apply></interval></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.4c">\text{AVS}_{u}=CS(TE(V^{G}_{1:T},\mathbf{0}),TE(\mathbf{0},A_{1:T}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.p4.5" class="ltx_p">where <math id="S4.SS1.p4.2.m1.1" class="ltx_Math" alttext="CS" display="inline"><semantics id="S4.SS1.p4.2.m1.1a"><mrow id="S4.SS1.p4.2.m1.1.1" xref="S4.SS1.p4.2.m1.1.1.cmml"><mi id="S4.SS1.p4.2.m1.1.1.2" xref="S4.SS1.p4.2.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.2.m1.1.1.1" xref="S4.SS1.p4.2.m1.1.1.1.cmml">​</mo><mi id="S4.SS1.p4.2.m1.1.1.3" xref="S4.SS1.p4.2.m1.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m1.1b"><apply id="S4.SS1.p4.2.m1.1.1.cmml" xref="S4.SS1.p4.2.m1.1.1"><times id="S4.SS1.p4.2.m1.1.1.1.cmml" xref="S4.SS1.p4.2.m1.1.1.1"></times><ci id="S4.SS1.p4.2.m1.1.1.2.cmml" xref="S4.SS1.p4.2.m1.1.1.2">𝐶</ci><ci id="S4.SS1.p4.2.m1.1.1.3.cmml" xref="S4.SS1.p4.2.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m1.1c">CS</annotation></semantics></math> indicates the cosine similarity and <math id="S4.SS1.p4.3.m2.1" class="ltx_Math" alttext="TE" display="inline"><semantics id="S4.SS1.p4.3.m2.1a"><mrow id="S4.SS1.p4.3.m2.1.1" xref="S4.SS1.p4.3.m2.1.1.cmml"><mi id="S4.SS1.p4.3.m2.1.1.2" xref="S4.SS1.p4.3.m2.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p4.3.m2.1.1.1" xref="S4.SS1.p4.3.m2.1.1.1.cmml">​</mo><mi id="S4.SS1.p4.3.m2.1.1.3" xref="S4.SS1.p4.3.m2.1.1.3.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m2.1b"><apply id="S4.SS1.p4.3.m2.1.1.cmml" xref="S4.SS1.p4.3.m2.1.1"><times id="S4.SS1.p4.3.m2.1.1.1.cmml" xref="S4.SS1.p4.3.m2.1.1.1"></times><ci id="S4.SS1.p4.3.m2.1.1.2.cmml" xref="S4.SS1.p4.3.m2.1.1.2">𝑇</ci><ci id="S4.SS1.p4.3.m2.1.1.3.cmml" xref="S4.SS1.p4.3.m2.1.1.3">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m2.1c">TE</annotation></semantics></math> represents the transformer encoder block from which we extract features.
<math id="S4.SS1.p4.4.m3.1" class="ltx_Math" alttext="V^{G}" display="inline"><semantics id="S4.SS1.p4.4.m3.1a"><msup id="S4.SS1.p4.4.m3.1.1" xref="S4.SS1.p4.4.m3.1.1.cmml"><mi id="S4.SS1.p4.4.m3.1.1.2" xref="S4.SS1.p4.4.m3.1.1.2.cmml">V</mi><mi id="S4.SS1.p4.4.m3.1.1.3" xref="S4.SS1.p4.4.m3.1.1.3.cmml">G</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.4.m3.1b"><apply id="S4.SS1.p4.4.m3.1.1.cmml" xref="S4.SS1.p4.4.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.4.m3.1.1.1.cmml" xref="S4.SS1.p4.4.m3.1.1">superscript</csymbol><ci id="S4.SS1.p4.4.m3.1.1.2.cmml" xref="S4.SS1.p4.4.m3.1.1.2">𝑉</ci><ci id="S4.SS1.p4.4.m3.1.1.3.cmml" xref="S4.SS1.p4.4.m3.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.4.m3.1c">V^{G}</annotation></semantics></math> and <math id="S4.SS1.p4.5.m4.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S4.SS1.p4.5.m4.1a"><mi id="S4.SS1.p4.5.m4.1.1" xref="S4.SS1.p4.5.m4.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.5.m4.1b"><ci id="S4.SS1.p4.5.m4.1.1.cmml" xref="S4.SS1.p4.5.m4.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.5.m4.1c">A</annotation></semantics></math> state the lips of the generated faces and audio sequence, respectively.
Please note that we give lips and audio to the AV-HuBERT model individually as we need to extract separate features to calculate the similarity between them.
However, the audio-visual transformer encoder of AV-HuBERT requires both modalities together.
Therefore, we follow the approach proposed by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Shi et al.</span> [<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite> and provide a placeholder of zeros with equivalent dimensions.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para ltx_noindent">
<p id="S4.SS1.p5.1" class="ltx_p"><span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_bold">Multimodal Audio-Visual Synchronization (AVS<sub id="S4.SS1.p5.1.1.1" class="ltx_sub"><span id="S4.SS1.p5.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">m</span></sub>)</span>
In this metric, we employ the generated and GT videos as pairs to measure the synchronization.
We first extract features from the generated video using the mouth region of the faces and audio.
We then repeat the same procedure with the GT video.
In the end, we calculate the cosine similarity between these two embeddings.
Intuitively, this metric considers the similarity between the alignment of the generated lips-audio and GT lips-audio pairs.</p>
<table id="S4.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E5.m1.2" class="ltx_Math" alttext="\text{AVS}_{m}=CS(TE(V^{G}_{1:T},A_{1:T}),TE(V^{GT}_{1:T},A_{1:T}))" display="block"><semantics id="S4.E5.m1.2a"><mrow id="S4.E5.m1.2.2" xref="S4.E5.m1.2.2.cmml"><msub id="S4.E5.m1.2.2.4" xref="S4.E5.m1.2.2.4.cmml"><mtext id="S4.E5.m1.2.2.4.2" xref="S4.E5.m1.2.2.4.2a.cmml">AVS</mtext><mi id="S4.E5.m1.2.2.4.3" xref="S4.E5.m1.2.2.4.3.cmml">m</mi></msub><mo id="S4.E5.m1.2.2.3" xref="S4.E5.m1.2.2.3.cmml">=</mo><mrow id="S4.E5.m1.2.2.2" xref="S4.E5.m1.2.2.2.cmml"><mi id="S4.E5.m1.2.2.2.4" xref="S4.E5.m1.2.2.2.4.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.2.2.2.3" xref="S4.E5.m1.2.2.2.3.cmml">​</mo><mi id="S4.E5.m1.2.2.2.5" xref="S4.E5.m1.2.2.2.5.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.2.2.2.3a" xref="S4.E5.m1.2.2.2.3.cmml">​</mo><mrow id="S4.E5.m1.2.2.2.2.2" xref="S4.E5.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.E5.m1.2.2.2.2.2.3" xref="S4.E5.m1.2.2.2.2.3.cmml">(</mo><mrow id="S4.E5.m1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.cmml"><mi id="S4.E5.m1.1.1.1.1.1.1.4" xref="S4.E5.m1.1.1.1.1.1.1.4.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.1.1.3.cmml">​</mo><mi id="S4.E5.m1.1.1.1.1.1.1.5" xref="S4.E5.m1.1.1.1.1.1.1.5.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.1.1.1.1.1.1.3a" xref="S4.E5.m1.1.1.1.1.1.1.3.cmml">​</mo><mrow id="S4.E5.m1.1.1.1.1.1.1.2.2" xref="S4.E5.m1.1.1.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S4.E5.m1.1.1.1.1.1.1.2.2.3" xref="S4.E5.m1.1.1.1.1.1.1.2.3.cmml">(</mo><msubsup id="S4.E5.m1.1.1.1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E5.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">V</mi><mrow id="S4.E5.m1.1.1.1.1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mn id="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.1" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">:</mo><mi id="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">T</mi></mrow><mi id="S4.E5.m1.1.1.1.1.1.1.1.1.1.2.3" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">G</mi></msubsup><mo id="S4.E5.m1.1.1.1.1.1.1.2.2.4" xref="S4.E5.m1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S4.E5.m1.1.1.1.1.1.1.2.2.2" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S4.E5.m1.1.1.1.1.1.1.2.2.2.2" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.cmml">A</mi><mrow id="S4.E5.m1.1.1.1.1.1.1.2.2.2.3" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.3.cmml"><mn id="S4.E5.m1.1.1.1.1.1.1.2.2.2.3.2" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S4.E5.m1.1.1.1.1.1.1.2.2.2.3.1" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.3.1.cmml">:</mo><mi id="S4.E5.m1.1.1.1.1.1.1.2.2.2.3.3" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.3.3.cmml">T</mi></mrow></msub><mo stretchy="false" id="S4.E5.m1.1.1.1.1.1.1.2.2.5" xref="S4.E5.m1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S4.E5.m1.2.2.2.2.2.4" xref="S4.E5.m1.2.2.2.2.3.cmml">,</mo><mrow id="S4.E5.m1.2.2.2.2.2.2" xref="S4.E5.m1.2.2.2.2.2.2.cmml"><mi id="S4.E5.m1.2.2.2.2.2.2.4" xref="S4.E5.m1.2.2.2.2.2.2.4.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.2.2.2.2.2.2.3" xref="S4.E5.m1.2.2.2.2.2.2.3.cmml">​</mo><mi id="S4.E5.m1.2.2.2.2.2.2.5" xref="S4.E5.m1.2.2.2.2.2.2.5.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.2.2.2.2.2.2.3a" xref="S4.E5.m1.2.2.2.2.2.2.3.cmml">​</mo><mrow id="S4.E5.m1.2.2.2.2.2.2.2.2" xref="S4.E5.m1.2.2.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.E5.m1.2.2.2.2.2.2.2.2.3" xref="S4.E5.m1.2.2.2.2.2.2.2.3.cmml">(</mo><msubsup id="S4.E5.m1.2.2.2.2.2.2.1.1.1" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1.cmml"><mi id="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.2" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.2.cmml">V</mi><mrow id="S4.E5.m1.2.2.2.2.2.2.1.1.1.3" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1.3.cmml"><mn id="S4.E5.m1.2.2.2.2.2.2.1.1.1.3.2" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S4.E5.m1.2.2.2.2.2.2.1.1.1.3.1" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1.3.1.cmml">:</mo><mi id="S4.E5.m1.2.2.2.2.2.2.1.1.1.3.3" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1.3.3.cmml">T</mi></mrow><mrow id="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.3" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.3.cmml"><mi id="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.3.2" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.3.1" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.3.1.cmml">​</mo><mi id="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.3.3" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.3.3.cmml">T</mi></mrow></msubsup><mo id="S4.E5.m1.2.2.2.2.2.2.2.2.4" xref="S4.E5.m1.2.2.2.2.2.2.2.3.cmml">,</mo><msub id="S4.E5.m1.2.2.2.2.2.2.2.2.2" xref="S4.E5.m1.2.2.2.2.2.2.2.2.2.cmml"><mi id="S4.E5.m1.2.2.2.2.2.2.2.2.2.2" xref="S4.E5.m1.2.2.2.2.2.2.2.2.2.2.cmml">A</mi><mrow id="S4.E5.m1.2.2.2.2.2.2.2.2.2.3" xref="S4.E5.m1.2.2.2.2.2.2.2.2.2.3.cmml"><mn id="S4.E5.m1.2.2.2.2.2.2.2.2.2.3.2" xref="S4.E5.m1.2.2.2.2.2.2.2.2.2.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S4.E5.m1.2.2.2.2.2.2.2.2.2.3.1" xref="S4.E5.m1.2.2.2.2.2.2.2.2.2.3.1.cmml">:</mo><mi id="S4.E5.m1.2.2.2.2.2.2.2.2.2.3.3" xref="S4.E5.m1.2.2.2.2.2.2.2.2.2.3.3.cmml">T</mi></mrow></msub><mo stretchy="false" id="S4.E5.m1.2.2.2.2.2.2.2.2.5" xref="S4.E5.m1.2.2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E5.m1.2.2.2.2.2.5" xref="S4.E5.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E5.m1.2b"><apply id="S4.E5.m1.2.2.cmml" xref="S4.E5.m1.2.2"><eq id="S4.E5.m1.2.2.3.cmml" xref="S4.E5.m1.2.2.3"></eq><apply id="S4.E5.m1.2.2.4.cmml" xref="S4.E5.m1.2.2.4"><csymbol cd="ambiguous" id="S4.E5.m1.2.2.4.1.cmml" xref="S4.E5.m1.2.2.4">subscript</csymbol><ci id="S4.E5.m1.2.2.4.2a.cmml" xref="S4.E5.m1.2.2.4.2"><mtext id="S4.E5.m1.2.2.4.2.cmml" xref="S4.E5.m1.2.2.4.2">AVS</mtext></ci><ci id="S4.E5.m1.2.2.4.3.cmml" xref="S4.E5.m1.2.2.4.3">𝑚</ci></apply><apply id="S4.E5.m1.2.2.2.cmml" xref="S4.E5.m1.2.2.2"><times id="S4.E5.m1.2.2.2.3.cmml" xref="S4.E5.m1.2.2.2.3"></times><ci id="S4.E5.m1.2.2.2.4.cmml" xref="S4.E5.m1.2.2.2.4">𝐶</ci><ci id="S4.E5.m1.2.2.2.5.cmml" xref="S4.E5.m1.2.2.2.5">𝑆</ci><interval closure="open" id="S4.E5.m1.2.2.2.2.3.cmml" xref="S4.E5.m1.2.2.2.2.2"><apply id="S4.E5.m1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1"><times id="S4.E5.m1.1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.3"></times><ci id="S4.E5.m1.1.1.1.1.1.1.4.cmml" xref="S4.E5.m1.1.1.1.1.1.1.4">𝑇</ci><ci id="S4.E5.m1.1.1.1.1.1.1.5.cmml" xref="S4.E5.m1.1.1.1.1.1.1.5">𝐸</ci><interval closure="open" id="S4.E5.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2.2"><apply id="S4.E5.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E5.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E5.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.2.2">𝑉</ci><ci id="S4.E5.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.2.3">𝐺</ci></apply><apply id="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.3"><ci id="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.1">:</ci><cn type="integer" id="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.2">1</cn><ci id="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.3">𝑇</ci></apply></apply><apply id="S4.E5.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.2">𝐴</ci><apply id="S4.E5.m1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.3"><ci id="S4.E5.m1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.3.1">:</ci><cn type="integer" id="S4.E5.m1.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.3.2">1</cn><ci id="S4.E5.m1.1.1.1.1.1.1.2.2.2.3.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.3.3">𝑇</ci></apply></apply></interval></apply><apply id="S4.E5.m1.2.2.2.2.2.2.cmml" xref="S4.E5.m1.2.2.2.2.2.2"><times id="S4.E5.m1.2.2.2.2.2.2.3.cmml" xref="S4.E5.m1.2.2.2.2.2.2.3"></times><ci id="S4.E5.m1.2.2.2.2.2.2.4.cmml" xref="S4.E5.m1.2.2.2.2.2.2.4">𝑇</ci><ci id="S4.E5.m1.2.2.2.2.2.2.5.cmml" xref="S4.E5.m1.2.2.2.2.2.2.5">𝐸</ci><interval closure="open" id="S4.E5.m1.2.2.2.2.2.2.2.3.cmml" xref="S4.E5.m1.2.2.2.2.2.2.2.2"><apply id="S4.E5.m1.2.2.2.2.2.2.1.1.1.cmml" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.E5.m1.2.2.2.2.2.2.1.1.1.1.cmml" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1">subscript</csymbol><apply id="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.cmml" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.1.cmml" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1">superscript</csymbol><ci id="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.2.cmml" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.2">𝑉</ci><apply id="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.3.cmml" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.3"><times id="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.3.1.cmml" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.3.1"></times><ci id="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.3.2.cmml" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.3.2">𝐺</ci><ci id="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.3.3.cmml" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1.2.3.3">𝑇</ci></apply></apply><apply id="S4.E5.m1.2.2.2.2.2.2.1.1.1.3.cmml" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1.3"><ci id="S4.E5.m1.2.2.2.2.2.2.1.1.1.3.1.cmml" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1.3.1">:</ci><cn type="integer" id="S4.E5.m1.2.2.2.2.2.2.1.1.1.3.2.cmml" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1.3.2">1</cn><ci id="S4.E5.m1.2.2.2.2.2.2.1.1.1.3.3.cmml" xref="S4.E5.m1.2.2.2.2.2.2.1.1.1.3.3">𝑇</ci></apply></apply><apply id="S4.E5.m1.2.2.2.2.2.2.2.2.2.cmml" xref="S4.E5.m1.2.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E5.m1.2.2.2.2.2.2.2.2.2.1.cmml" xref="S4.E5.m1.2.2.2.2.2.2.2.2.2">subscript</csymbol><ci id="S4.E5.m1.2.2.2.2.2.2.2.2.2.2.cmml" xref="S4.E5.m1.2.2.2.2.2.2.2.2.2.2">𝐴</ci><apply id="S4.E5.m1.2.2.2.2.2.2.2.2.2.3.cmml" xref="S4.E5.m1.2.2.2.2.2.2.2.2.2.3"><ci id="S4.E5.m1.2.2.2.2.2.2.2.2.2.3.1.cmml" xref="S4.E5.m1.2.2.2.2.2.2.2.2.2.3.1">:</ci><cn type="integer" id="S4.E5.m1.2.2.2.2.2.2.2.2.2.3.2.cmml" xref="S4.E5.m1.2.2.2.2.2.2.2.2.2.3.2">1</cn><ci id="S4.E5.m1.2.2.2.2.2.2.2.2.2.3.3.cmml" xref="S4.E5.m1.2.2.2.2.2.2.2.2.2.3.3">𝑇</ci></apply></apply></interval></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.2c">\text{AVS}_{m}=CS(TE(V^{G}_{1:T},A_{1:T}),TE(V^{GT}_{1:T},A_{1:T}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS1.p6" class="ltx_para ltx_noindent">
<p id="S4.SS1.p6.1" class="ltx_p"><span id="S4.SS1.p6.1.1" class="ltx_text ltx_font_bold">Visual-only Lip Synchronization (AVS<sub id="S4.SS1.p6.1.1.1" class="ltx_sub"><span id="S4.SS1.p6.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">v</span></sub>)</span>
We only employ the lips of the generated faces and GT faces without involving audio.
Therefore, this metric only focuses on the visual shape similarity of the lips.
As AV-HuBERT is finetuned for lip-reading purpose, the extracted visual features hold the information for lip reading, yielding meaningful representation for lip synchronization and intelligibility.</p>
<table id="S4.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E6.m1.4" class="ltx_Math" alttext="\text{AVS}_{v}=CS(TE(V^{G}_{1:T},\mathbf{0}),TE(V^{GT}_{1:T},\mathbf{0}))" display="block"><semantics id="S4.E6.m1.4a"><mrow id="S4.E6.m1.4.4" xref="S4.E6.m1.4.4.cmml"><msub id="S4.E6.m1.4.4.4" xref="S4.E6.m1.4.4.4.cmml"><mtext id="S4.E6.m1.4.4.4.2" xref="S4.E6.m1.4.4.4.2a.cmml">AVS</mtext><mi id="S4.E6.m1.4.4.4.3" xref="S4.E6.m1.4.4.4.3.cmml">v</mi></msub><mo id="S4.E6.m1.4.4.3" xref="S4.E6.m1.4.4.3.cmml">=</mo><mrow id="S4.E6.m1.4.4.2" xref="S4.E6.m1.4.4.2.cmml"><mi id="S4.E6.m1.4.4.2.4" xref="S4.E6.m1.4.4.2.4.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.2.3" xref="S4.E6.m1.4.4.2.3.cmml">​</mo><mi id="S4.E6.m1.4.4.2.5" xref="S4.E6.m1.4.4.2.5.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.2.3a" xref="S4.E6.m1.4.4.2.3.cmml">​</mo><mrow id="S4.E6.m1.4.4.2.2.2" xref="S4.E6.m1.4.4.2.2.3.cmml"><mo stretchy="false" id="S4.E6.m1.4.4.2.2.2.3" xref="S4.E6.m1.4.4.2.2.3.cmml">(</mo><mrow id="S4.E6.m1.3.3.1.1.1.1" xref="S4.E6.m1.3.3.1.1.1.1.cmml"><mi id="S4.E6.m1.3.3.1.1.1.1.3" xref="S4.E6.m1.3.3.1.1.1.1.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.3.3.1.1.1.1.2" xref="S4.E6.m1.3.3.1.1.1.1.2.cmml">​</mo><mi id="S4.E6.m1.3.3.1.1.1.1.4" xref="S4.E6.m1.3.3.1.1.1.1.4.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.3.3.1.1.1.1.2a" xref="S4.E6.m1.3.3.1.1.1.1.2.cmml">​</mo><mrow id="S4.E6.m1.3.3.1.1.1.1.1.1" xref="S4.E6.m1.3.3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E6.m1.3.3.1.1.1.1.1.1.2" xref="S4.E6.m1.3.3.1.1.1.1.1.2.cmml">(</mo><msubsup id="S4.E6.m1.3.3.1.1.1.1.1.1.1" xref="S4.E6.m1.3.3.1.1.1.1.1.1.1.cmml"><mi id="S4.E6.m1.3.3.1.1.1.1.1.1.1.2.2" xref="S4.E6.m1.3.3.1.1.1.1.1.1.1.2.2.cmml">V</mi><mrow id="S4.E6.m1.3.3.1.1.1.1.1.1.1.3" xref="S4.E6.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mn id="S4.E6.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S4.E6.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S4.E6.m1.3.3.1.1.1.1.1.1.1.3.1" xref="S4.E6.m1.3.3.1.1.1.1.1.1.1.3.1.cmml">:</mo><mi id="S4.E6.m1.3.3.1.1.1.1.1.1.1.3.3" xref="S4.E6.m1.3.3.1.1.1.1.1.1.1.3.3.cmml">T</mi></mrow><mi id="S4.E6.m1.3.3.1.1.1.1.1.1.1.2.3" xref="S4.E6.m1.3.3.1.1.1.1.1.1.1.2.3.cmml">G</mi></msubsup><mo id="S4.E6.m1.3.3.1.1.1.1.1.1.3" xref="S4.E6.m1.3.3.1.1.1.1.1.2.cmml">,</mo><mn id="S4.E6.m1.1.1" xref="S4.E6.m1.1.1.cmml">𝟎</mn><mo stretchy="false" id="S4.E6.m1.3.3.1.1.1.1.1.1.4" xref="S4.E6.m1.3.3.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E6.m1.4.4.2.2.2.4" xref="S4.E6.m1.4.4.2.2.3.cmml">,</mo><mrow id="S4.E6.m1.4.4.2.2.2.2" xref="S4.E6.m1.4.4.2.2.2.2.cmml"><mi id="S4.E6.m1.4.4.2.2.2.2.3" xref="S4.E6.m1.4.4.2.2.2.2.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.2.2.2.2.2" xref="S4.E6.m1.4.4.2.2.2.2.2.cmml">​</mo><mi id="S4.E6.m1.4.4.2.2.2.2.4" xref="S4.E6.m1.4.4.2.2.2.2.4.cmml">E</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.2.2.2.2.2a" xref="S4.E6.m1.4.4.2.2.2.2.2.cmml">​</mo><mrow id="S4.E6.m1.4.4.2.2.2.2.1.1" xref="S4.E6.m1.4.4.2.2.2.2.1.2.cmml"><mo stretchy="false" id="S4.E6.m1.4.4.2.2.2.2.1.1.2" xref="S4.E6.m1.4.4.2.2.2.2.1.2.cmml">(</mo><msubsup id="S4.E6.m1.4.4.2.2.2.2.1.1.1" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1.cmml"><mi id="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.2" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.2.cmml">V</mi><mrow id="S4.E6.m1.4.4.2.2.2.2.1.1.1.3" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1.3.cmml"><mn id="S4.E6.m1.4.4.2.2.2.2.1.1.1.3.2" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S4.E6.m1.4.4.2.2.2.2.1.1.1.3.1" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1.3.1.cmml">:</mo><mi id="S4.E6.m1.4.4.2.2.2.2.1.1.1.3.3" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1.3.3.cmml">T</mi></mrow><mrow id="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.3" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.3.cmml"><mi id="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.3.2" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.3.1" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.3.1.cmml">​</mo><mi id="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.3.3" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.3.3.cmml">T</mi></mrow></msubsup><mo id="S4.E6.m1.4.4.2.2.2.2.1.1.3" xref="S4.E6.m1.4.4.2.2.2.2.1.2.cmml">,</mo><mn id="S4.E6.m1.2.2" xref="S4.E6.m1.2.2.cmml">𝟎</mn><mo stretchy="false" id="S4.E6.m1.4.4.2.2.2.2.1.1.4" xref="S4.E6.m1.4.4.2.2.2.2.1.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E6.m1.4.4.2.2.2.5" xref="S4.E6.m1.4.4.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E6.m1.4b"><apply id="S4.E6.m1.4.4.cmml" xref="S4.E6.m1.4.4"><eq id="S4.E6.m1.4.4.3.cmml" xref="S4.E6.m1.4.4.3"></eq><apply id="S4.E6.m1.4.4.4.cmml" xref="S4.E6.m1.4.4.4"><csymbol cd="ambiguous" id="S4.E6.m1.4.4.4.1.cmml" xref="S4.E6.m1.4.4.4">subscript</csymbol><ci id="S4.E6.m1.4.4.4.2a.cmml" xref="S4.E6.m1.4.4.4.2"><mtext id="S4.E6.m1.4.4.4.2.cmml" xref="S4.E6.m1.4.4.4.2">AVS</mtext></ci><ci id="S4.E6.m1.4.4.4.3.cmml" xref="S4.E6.m1.4.4.4.3">𝑣</ci></apply><apply id="S4.E6.m1.4.4.2.cmml" xref="S4.E6.m1.4.4.2"><times id="S4.E6.m1.4.4.2.3.cmml" xref="S4.E6.m1.4.4.2.3"></times><ci id="S4.E6.m1.4.4.2.4.cmml" xref="S4.E6.m1.4.4.2.4">𝐶</ci><ci id="S4.E6.m1.4.4.2.5.cmml" xref="S4.E6.m1.4.4.2.5">𝑆</ci><interval closure="open" id="S4.E6.m1.4.4.2.2.3.cmml" xref="S4.E6.m1.4.4.2.2.2"><apply id="S4.E6.m1.3.3.1.1.1.1.cmml" xref="S4.E6.m1.3.3.1.1.1.1"><times id="S4.E6.m1.3.3.1.1.1.1.2.cmml" xref="S4.E6.m1.3.3.1.1.1.1.2"></times><ci id="S4.E6.m1.3.3.1.1.1.1.3.cmml" xref="S4.E6.m1.3.3.1.1.1.1.3">𝑇</ci><ci id="S4.E6.m1.3.3.1.1.1.1.4.cmml" xref="S4.E6.m1.3.3.1.1.1.1.4">𝐸</ci><interval closure="open" id="S4.E6.m1.3.3.1.1.1.1.1.2.cmml" xref="S4.E6.m1.3.3.1.1.1.1.1.1"><apply id="S4.E6.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.3.3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E6.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.3.3.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E6.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S4.E6.m1.3.3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E6.m1.3.3.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E6.m1.3.3.1.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E6.m1.3.3.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E6.m1.3.3.1.1.1.1.1.1.1.2.2">𝑉</ci><ci id="S4.E6.m1.3.3.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E6.m1.3.3.1.1.1.1.1.1.1.2.3">𝐺</ci></apply><apply id="S4.E6.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S4.E6.m1.3.3.1.1.1.1.1.1.1.3"><ci id="S4.E6.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E6.m1.3.3.1.1.1.1.1.1.1.3.1">:</ci><cn type="integer" id="S4.E6.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E6.m1.3.3.1.1.1.1.1.1.1.3.2">1</cn><ci id="S4.E6.m1.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E6.m1.3.3.1.1.1.1.1.1.1.3.3">𝑇</ci></apply></apply><cn type="integer" id="S4.E6.m1.1.1.cmml" xref="S4.E6.m1.1.1">0</cn></interval></apply><apply id="S4.E6.m1.4.4.2.2.2.2.cmml" xref="S4.E6.m1.4.4.2.2.2.2"><times id="S4.E6.m1.4.4.2.2.2.2.2.cmml" xref="S4.E6.m1.4.4.2.2.2.2.2"></times><ci id="S4.E6.m1.4.4.2.2.2.2.3.cmml" xref="S4.E6.m1.4.4.2.2.2.2.3">𝑇</ci><ci id="S4.E6.m1.4.4.2.2.2.2.4.cmml" xref="S4.E6.m1.4.4.2.2.2.2.4">𝐸</ci><interval closure="open" id="S4.E6.m1.4.4.2.2.2.2.1.2.cmml" xref="S4.E6.m1.4.4.2.2.2.2.1.1"><apply id="S4.E6.m1.4.4.2.2.2.2.1.1.1.cmml" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.E6.m1.4.4.2.2.2.2.1.1.1.1.cmml" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1">subscript</csymbol><apply id="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.cmml" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.1.cmml" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1">superscript</csymbol><ci id="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.2.cmml" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.2">𝑉</ci><apply id="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.3.cmml" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.3"><times id="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.3.1.cmml" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.3.1"></times><ci id="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.3.2.cmml" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.3.2">𝐺</ci><ci id="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.3.3.cmml" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1.2.3.3">𝑇</ci></apply></apply><apply id="S4.E6.m1.4.4.2.2.2.2.1.1.1.3.cmml" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1.3"><ci id="S4.E6.m1.4.4.2.2.2.2.1.1.1.3.1.cmml" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1.3.1">:</ci><cn type="integer" id="S4.E6.m1.4.4.2.2.2.2.1.1.1.3.2.cmml" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1.3.2">1</cn><ci id="S4.E6.m1.4.4.2.2.2.2.1.1.1.3.3.cmml" xref="S4.E6.m1.4.4.2.2.2.2.1.1.1.3.3">𝑇</ci></apply></apply><cn type="integer" id="S4.E6.m1.2.2.cmml" xref="S4.E6.m1.2.2">0</cn></interval></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m1.4c">\text{AVS}_{v}=CS(TE(V^{G}_{1:T},\mathbf{0}),TE(V^{GT}_{1:T},\mathbf{0}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Quantitative Evaluation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We demonstrate quantitative results on the test data of LRS2 and LRW in <a href="#S4.T1" title="In 4.1 Metrics ‣ 4 Experimental Results ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a> and HDTF in <a href="#S4.T2" title="In 4.2 Quantitative Evaluation ‣ 4 Experimental Results ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>.
On LRS2 and HDTF, we surpass all other methods in the visual quality, except FID on LRS2, where we are slightly behind IPLAP and DINet.
On LRW, we achieve similar visual quality scores with IPLAP for SSIM and PSNR, and with VRT for FID.
In summary, we surpass other methods with a large margin in most of the visual quality metrics.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">In LMD, we achieve state-of-the-art results on all three datasets.
This outcome does not only show the synchronization performance but also demonstrate the visual stability of our generated outputs.
In LSE-C and LSE-D metrics, we have state-of-the-art performance on LRW dataset.
While TalkLip has better scores on LRS2, Wav2Lip achieves the highest score on HDTF.
However, when considering our user study (<a href="#S4.SS4" title="4.4 User Study ‣ 4 Experimental Results ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.4</span></a>), Wav2Lip is far behind our method on HDTF in synchronization performance, as in LRS2 and LRW datasets.
This shows the vulnerability and inconsistency of LSE-C &amp; D metrics.
<a href="#S4.F5.sf3" title="In Figure 5 ‣ 4.2 Quantitative Evaluation ‣ 4 Experimental Results ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5(c)</span></a> illustrates the sensitive performance of LSE-C &amp; D metrics when the visual data is horizontally shifted <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>.
It clearly proves that these metrics are extremely sensitive to translation in the data, measuring poor lip synchronization performance when the face is shifted while preserving the same lip shape.
Moreover, SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> demonstrates similar fluctuation on GT data as in <a href="#S1.F1.sf1" title="In Figure 1 ‣ 1 Introduction ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1(a)</span></a>.
All these analyses and outcomes validate the motivation of our proposed new lip synchronization metrics.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2405.04327/assets/x7.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="191" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">Qualitative comparison of our approach with state-of-the-art models and ground-truth data on HDTF</span></figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.3" class="ltx_p">According to our AV-HuBERT-based lip sync metrics, we achieve state-of-the-art results on all three datasets in AVS<sub id="S4.SS2.p3.3.1" class="ltx_sub"><span id="S4.SS2.p3.3.1.1" class="ltx_text ltx_font_italic">m</span></sub> and AVS<sub id="S4.SS2.p3.3.2" class="ltx_sub"><span id="S4.SS2.p3.3.2.1" class="ltx_text ltx_font_italic">v</span></sub>.
On the other hand, TalkLip surpasses us with a small difference on all three datasets in the AVS<sub id="S4.SS2.p3.3.3" class="ltx_sub"><span id="S4.SS2.p3.3.3.1" class="ltx_text ltx_font_italic">u</span></sub> metric.
In <a href="#S1.F1.sf3" title="In Figure 1 ‣ 1 Introduction ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1(c)</span></a>, we illustrate the analysis of cosine similarity between AV-HuBERT lip and audio features on LRS2 GT data (similar to <a href="#S1.F1.sf1" title="In Figure 1 ‣ 1 Introduction ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1(a)</span></a>).
The graph shows that the AV-HuBERT features are more stable than SyncNet features.
Similarly, <a href="#S4.F5.sf3" title="In Figure 5 ‣ 4.2 Quantitative Evaluation ‣ 4 Experimental Results ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5(c)</span></a> and <a href="#S4.F5.sf4" title="In Figure 5 ‣ 4.2 Quantitative Evaluation ‣ 4 Experimental Results ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5(d)</span></a> also confirm the stability of our three novel metrics compared to LSE-C &amp; D.
Considering these analyses as well as the results of our user study, it is clear that our proposed lip synchronization evaluation metrics provide more insight about the lip synchronization and also show more reliable performance.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.14.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.15.2" class="ltx_text" style="font-size:90%;">Quantitative results on HDTF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite>.</span></figcaption>
<div id="S4.T2.12" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:48.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-132.7pt,29.7pt) scale(0.449569917543654,0.449569917543654) ;">
<table id="S4.T2.12.12" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.12.12.12" class="ltx_tr">
<th id="S4.T2.12.12.12.13" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Method</th>
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">SSIM <math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">PSNR <math id="S4.T2.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">FID <math id="S4.T2.3.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T2.3.3.3.3.m1.1.1" xref="S4.T2.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T2.4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">LMD <math id="S4.T2.4.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.4.4.4.4.m1.1a"><mo stretchy="false" id="S4.T2.4.4.4.4.m1.1.1" xref="S4.T2.4.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.4.m1.1b"><ci id="S4.T2.4.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T2.5.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">LSE-C <math id="S4.T2.5.5.5.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.5.5.5.5.m1.1a"><mo stretchy="false" id="S4.T2.5.5.5.5.m1.1.1" xref="S4.T2.5.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.5.m1.1b"><ci id="S4.T2.5.5.5.5.m1.1.1.cmml" xref="S4.T2.5.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.5.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T2.6.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">LSE-D <math id="S4.T2.6.6.6.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.6.6.6.6.m1.1a"><mo stretchy="false" id="S4.T2.6.6.6.6.m1.1.1" xref="S4.T2.6.6.6.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.6.m1.1b"><ci id="S4.T2.6.6.6.6.m1.1.1.cmml" xref="S4.T2.6.6.6.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.6.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T2.8.8.8.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AVS<sub id="S4.T2.8.8.8.8.1" class="ltx_sub"><span id="S4.T2.8.8.8.8.1.1" class="ltx_text ltx_font_italic">u</span></sub> <math id="S4.T2.8.8.8.8.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.8.8.8.8.m2.1a"><mo stretchy="false" id="S4.T2.8.8.8.8.m2.1.1" xref="S4.T2.8.8.8.8.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.8.8.m2.1b"><ci id="S4.T2.8.8.8.8.m2.1.1.cmml" xref="S4.T2.8.8.8.8.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.8.8.m2.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T2.10.10.10.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AVS<sub id="S4.T2.10.10.10.10.1" class="ltx_sub"><span id="S4.T2.10.10.10.10.1.1" class="ltx_text ltx_font_italic">m</span></sub> <math id="S4.T2.10.10.10.10.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.10.10.10.10.m2.1a"><mo stretchy="false" id="S4.T2.10.10.10.10.m2.1.1" xref="S4.T2.10.10.10.10.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.10.10.m2.1b"><ci id="S4.T2.10.10.10.10.m2.1.1.cmml" xref="S4.T2.10.10.10.10.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.10.10.m2.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T2.12.12.12.12" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">AVS<sub id="S4.T2.12.12.12.12.1" class="ltx_sub"><span id="S4.T2.12.12.12.12.1.1" class="ltx_text ltx_font_italic">v</span></sub> <math id="S4.T2.12.12.12.12.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.12.12.12.12.m2.1a"><mo stretchy="false" id="S4.T2.12.12.12.12.m2.1.1" xref="S4.T2.12.12.12.12.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.12.12.12.12.m2.1b"><ci id="S4.T2.12.12.12.12.m2.1.1.cmml" xref="S4.T2.12.12.12.12.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.12.12.12.m2.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.12.12.13.1" class="ltx_tr">
<th id="S4.T2.12.12.13.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Wav2Lip</th>
<td id="S4.T2.12.12.13.1.2" class="ltx_td ltx_align_center ltx_border_t">0.841</td>
<td id="S4.T2.12.12.13.1.3" class="ltx_td ltx_align_center ltx_border_t">24.812</td>
<td id="S4.T2.12.12.13.1.4" class="ltx_td ltx_align_center ltx_border_t">35.41</td>
<td id="S4.T2.12.12.13.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.12.12.13.1.5.1" class="ltx_text" style="background-color:#FCEF9E;">1.341</span></td>
<td id="S4.T2.12.12.13.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.12.12.13.1.6.1" class="ltx_text" style="background-color:#D2F0AA;">9.054</span></td>
<td id="S4.T2.12.12.13.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.12.12.13.1.7.1" class="ltx_text" style="background-color:#D2F0AA;">6.414</span></td>
<td id="S4.T2.12.12.13.1.8" class="ltx_td ltx_align_center ltx_border_t">0.297</td>
<td id="S4.T2.12.12.13.1.9" class="ltx_td ltx_align_center ltx_border_t">0.514</td>
<td id="S4.T2.12.12.13.1.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.358</td>
</tr>
<tr id="S4.T2.12.12.14.2" class="ltx_tr">
<th id="S4.T2.12.12.14.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VideoReTalking</th>
<td id="S4.T2.12.12.14.2.2" class="ltx_td ltx_align_center">0.830</td>
<td id="S4.T2.12.12.14.2.3" class="ltx_td ltx_align_center">24.551</td>
<td id="S4.T2.12.12.14.2.4" class="ltx_td ltx_align_center">29.77</td>
<td id="S4.T2.12.12.14.2.5" class="ltx_td ltx_align_center">3.085</td>
<td id="S4.T2.12.12.14.2.6" class="ltx_td ltx_align_center">6.121</td>
<td id="S4.T2.12.12.14.2.7" class="ltx_td ltx_align_center">7.368</td>
<td id="S4.T2.12.12.14.2.8" class="ltx_td ltx_align_center">0.384</td>
<td id="S4.T2.12.12.14.2.9" class="ltx_td ltx_align_center">0.677</td>
<td id="S4.T2.12.12.14.2.10" class="ltx_td ltx_nopad_r ltx_align_center">0.570</td>
</tr>
<tr id="S4.T2.12.12.15.3" class="ltx_tr">
<th id="S4.T2.12.12.15.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TalkLip</th>
<td id="S4.T2.12.12.15.3.2" class="ltx_td ltx_align_center">0.820</td>
<td id="S4.T2.12.12.15.3.3" class="ltx_td ltx_align_center">25.229</td>
<td id="S4.T2.12.12.15.3.4" class="ltx_td ltx_align_center">25.10</td>
<td id="S4.T2.12.12.15.3.5" class="ltx_td ltx_align_center">2.981</td>
<td id="S4.T2.12.12.15.3.6" class="ltx_td ltx_align_center">6.189</td>
<td id="S4.T2.12.12.15.3.7" class="ltx_td ltx_align_center">7.276</td>
<td id="S4.T2.12.12.15.3.8" class="ltx_td ltx_align_center"><span id="S4.T2.12.12.15.3.8.1" class="ltx_text" style="background-color:#D2F0AA;">0.591</span></td>
<td id="S4.T2.12.12.15.3.9" class="ltx_td ltx_align_center"><span id="S4.T2.12.12.15.3.9.1" class="ltx_text" style="background-color:#FCEF9E;">0.823</span></td>
<td id="S4.T2.12.12.15.3.10" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T2.12.12.15.3.10.1" class="ltx_text" style="background-color:#FCEF9E;">0.730</span></td>
</tr>
<tr id="S4.T2.12.12.16.4" class="ltx_tr">
<th id="S4.T2.12.12.16.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">IPLAP</th>
<td id="S4.T2.12.12.16.4.2" class="ltx_td ltx_align_center"><span id="S4.T2.12.12.16.4.2.1" class="ltx_text" style="background-color:#FCEF9E;">0.869</span></td>
<td id="S4.T2.12.12.16.4.3" class="ltx_td ltx_align_center"><span id="S4.T2.12.12.16.4.3.1" class="ltx_text" style="background-color:#FCEF9E;">27.801</span></td>
<td id="S4.T2.12.12.16.4.4" class="ltx_td ltx_align_center"><span id="S4.T2.12.12.16.4.4.1" class="ltx_text" style="background-color:#FCEF9E;">22.09</span></td>
<td id="S4.T2.12.12.16.4.5" class="ltx_td ltx_align_center">2.217</td>
<td id="S4.T2.12.12.16.4.6" class="ltx_td ltx_align_center">5.563</td>
<td id="S4.T2.12.12.16.4.7" class="ltx_td ltx_align_center">8.495</td>
<td id="S4.T2.12.12.16.4.8" class="ltx_td ltx_align_center">0.459</td>
<td id="S4.T2.12.12.16.4.9" class="ltx_td ltx_align_center">0.661</td>
<td id="S4.T2.12.12.16.4.10" class="ltx_td ltx_nopad_r ltx_align_center">0.528</td>
</tr>
<tr id="S4.T2.12.12.17.5" class="ltx_tr">
<th id="S4.T2.12.12.17.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Ours</th>
<td id="S4.T2.12.12.17.5.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.12.12.17.5.2.1" class="ltx_text" style="background-color:#D2F0AA;">0.933</span></td>
<td id="S4.T2.12.12.17.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.12.12.17.5.3.1" class="ltx_text" style="background-color:#D2F0AA;">30.579</span></td>
<td id="S4.T2.12.12.17.5.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.12.12.17.5.4.1" class="ltx_text" style="background-color:#D2F0AA;">16.76</span></td>
<td id="S4.T2.12.12.17.5.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.12.12.17.5.5.1" class="ltx_text" style="background-color:#D2F0AA;">1.292</span></td>
<td id="S4.T2.12.12.17.5.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.12.12.17.5.6.1" class="ltx_text" style="background-color:#FCEF9E;">8.106</span></td>
<td id="S4.T2.12.12.17.5.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.12.12.17.5.7.1" class="ltx_text" style="background-color:#FCEF9E;">6.765</span></td>
<td id="S4.T2.12.12.17.5.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.12.12.17.5.8.1" class="ltx_text" style="background-color:#FCEF9E;">0.538</span></td>
<td id="S4.T2.12.12.17.5.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.12.12.17.5.9.1" class="ltx_text" style="background-color:#D2F0AA;">0.892</span></td>
<td id="S4.T2.12.12.17.5.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T2.12.12.17.5.10.1" class="ltx_text" style="background-color:#D2F0AA;">0.783</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.04327/assets/x8.png" id="S4.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="332" height="218" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">Horizontal shift analysis</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.04327/assets/x9.png" id="S4.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="332" height="218" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">Rotation analysis</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.04327/assets/x10.png" id="S4.F5.sf3.g1" class="ltx_graphics ltx_img_landscape" width="332" height="165" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F5.sf3.3.2" class="ltx_text" style="font-size:90%;">Comparison of evaluation metrics</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F5.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.04327/assets/x11.png" id="S4.F5.sf4.g1" class="ltx_graphics ltx_img_landscape" width="308" height="213" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S4.F5.sf4.3.2" class="ltx_text" style="font-size:90%;">Analyses of our metrics</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.14.5.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.8.4" class="ltx_text ltx_font_bold" style="font-size:90%;">(a,b)<span id="S4.F5.8.4.5" class="ltx_text ltx_font_medium"> shows the performance analyses of SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> and AV-HuBERT features for lip-sync loss on GT LRS2 data with the horizontal shift and rotation in spatial space. It clearly shows that SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> is not shift invariant and vulnerable to the affine transformation, while AV-HuBERT demonstrates robust performance. </span>(c)<span id="S4.F5.6.2.2" class="ltx_text ltx_font_medium"> compares the LSE-C &amp; D metrics with our <math id="S4.F5.5.1.1.m1.1" class="ltx_Math" alttext="AVS_{u}" display="inline"><semantics id="S4.F5.5.1.1.m1.1b"><mrow id="S4.F5.5.1.1.m1.1.1" xref="S4.F5.5.1.1.m1.1.1.cmml"><mi id="S4.F5.5.1.1.m1.1.1.2" xref="S4.F5.5.1.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.F5.5.1.1.m1.1.1.1" xref="S4.F5.5.1.1.m1.1.1.1.cmml">​</mo><mi id="S4.F5.5.1.1.m1.1.1.3" xref="S4.F5.5.1.1.m1.1.1.3.cmml">V</mi><mo lspace="0em" rspace="0em" id="S4.F5.5.1.1.m1.1.1.1b" xref="S4.F5.5.1.1.m1.1.1.1.cmml">​</mo><msub id="S4.F5.5.1.1.m1.1.1.4" xref="S4.F5.5.1.1.m1.1.1.4.cmml"><mi id="S4.F5.5.1.1.m1.1.1.4.2" xref="S4.F5.5.1.1.m1.1.1.4.2.cmml">S</mi><mi id="S4.F5.5.1.1.m1.1.1.4.3" xref="S4.F5.5.1.1.m1.1.1.4.3.cmml">u</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.5.1.1.m1.1c"><apply id="S4.F5.5.1.1.m1.1.1.cmml" xref="S4.F5.5.1.1.m1.1.1"><times id="S4.F5.5.1.1.m1.1.1.1.cmml" xref="S4.F5.5.1.1.m1.1.1.1"></times><ci id="S4.F5.5.1.1.m1.1.1.2.cmml" xref="S4.F5.5.1.1.m1.1.1.2">𝐴</ci><ci id="S4.F5.5.1.1.m1.1.1.3.cmml" xref="S4.F5.5.1.1.m1.1.1.3">𝑉</ci><apply id="S4.F5.5.1.1.m1.1.1.4.cmml" xref="S4.F5.5.1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S4.F5.5.1.1.m1.1.1.4.1.cmml" xref="S4.F5.5.1.1.m1.1.1.4">subscript</csymbol><ci id="S4.F5.5.1.1.m1.1.1.4.2.cmml" xref="S4.F5.5.1.1.m1.1.1.4.2">𝑆</ci><ci id="S4.F5.5.1.1.m1.1.1.4.3.cmml" xref="S4.F5.5.1.1.m1.1.1.4.3">𝑢</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.5.1.1.m1.1d">AVS_{u}</annotation></semantics></math> metric while applying horizontal shifting in the spatial space. While LSE-C &amp; D scores are aligned with the left axis, <math id="S4.F5.6.2.2.m2.1" class="ltx_Math" alttext="AVS_{u}" display="inline"><semantics id="S4.F5.6.2.2.m2.1b"><mrow id="S4.F5.6.2.2.m2.1.1" xref="S4.F5.6.2.2.m2.1.1.cmml"><mi id="S4.F5.6.2.2.m2.1.1.2" xref="S4.F5.6.2.2.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.F5.6.2.2.m2.1.1.1" xref="S4.F5.6.2.2.m2.1.1.1.cmml">​</mo><mi id="S4.F5.6.2.2.m2.1.1.3" xref="S4.F5.6.2.2.m2.1.1.3.cmml">V</mi><mo lspace="0em" rspace="0em" id="S4.F5.6.2.2.m2.1.1.1b" xref="S4.F5.6.2.2.m2.1.1.1.cmml">​</mo><msub id="S4.F5.6.2.2.m2.1.1.4" xref="S4.F5.6.2.2.m2.1.1.4.cmml"><mi id="S4.F5.6.2.2.m2.1.1.4.2" xref="S4.F5.6.2.2.m2.1.1.4.2.cmml">S</mi><mi id="S4.F5.6.2.2.m2.1.1.4.3" xref="S4.F5.6.2.2.m2.1.1.4.3.cmml">u</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.6.2.2.m2.1c"><apply id="S4.F5.6.2.2.m2.1.1.cmml" xref="S4.F5.6.2.2.m2.1.1"><times id="S4.F5.6.2.2.m2.1.1.1.cmml" xref="S4.F5.6.2.2.m2.1.1.1"></times><ci id="S4.F5.6.2.2.m2.1.1.2.cmml" xref="S4.F5.6.2.2.m2.1.1.2">𝐴</ci><ci id="S4.F5.6.2.2.m2.1.1.3.cmml" xref="S4.F5.6.2.2.m2.1.1.3">𝑉</ci><apply id="S4.F5.6.2.2.m2.1.1.4.cmml" xref="S4.F5.6.2.2.m2.1.1.4"><csymbol cd="ambiguous" id="S4.F5.6.2.2.m2.1.1.4.1.cmml" xref="S4.F5.6.2.2.m2.1.1.4">subscript</csymbol><ci id="S4.F5.6.2.2.m2.1.1.4.2.cmml" xref="S4.F5.6.2.2.m2.1.1.4.2">𝑆</ci><ci id="S4.F5.6.2.2.m2.1.1.4.3.cmml" xref="S4.F5.6.2.2.m2.1.1.4.3">𝑢</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.6.2.2.m2.1d">AVS_{u}</annotation></semantics></math> is alifned with the right one. </span>(d)<span id="S4.F5.8.4.4" class="ltx_text ltx_font_medium"> analyses our three metrics under the shifting conditions. Since <math id="S4.F5.7.3.3.m1.1" class="ltx_Math" alttext="AVS_{v}" display="inline"><semantics id="S4.F5.7.3.3.m1.1b"><mrow id="S4.F5.7.3.3.m1.1.1" xref="S4.F5.7.3.3.m1.1.1.cmml"><mi id="S4.F5.7.3.3.m1.1.1.2" xref="S4.F5.7.3.3.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.F5.7.3.3.m1.1.1.1" xref="S4.F5.7.3.3.m1.1.1.1.cmml">​</mo><mi id="S4.F5.7.3.3.m1.1.1.3" xref="S4.F5.7.3.3.m1.1.1.3.cmml">V</mi><mo lspace="0em" rspace="0em" id="S4.F5.7.3.3.m1.1.1.1b" xref="S4.F5.7.3.3.m1.1.1.1.cmml">​</mo><msub id="S4.F5.7.3.3.m1.1.1.4" xref="S4.F5.7.3.3.m1.1.1.4.cmml"><mi id="S4.F5.7.3.3.m1.1.1.4.2" xref="S4.F5.7.3.3.m1.1.1.4.2.cmml">S</mi><mi id="S4.F5.7.3.3.m1.1.1.4.3" xref="S4.F5.7.3.3.m1.1.1.4.3.cmml">v</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.7.3.3.m1.1c"><apply id="S4.F5.7.3.3.m1.1.1.cmml" xref="S4.F5.7.3.3.m1.1.1"><times id="S4.F5.7.3.3.m1.1.1.1.cmml" xref="S4.F5.7.3.3.m1.1.1.1"></times><ci id="S4.F5.7.3.3.m1.1.1.2.cmml" xref="S4.F5.7.3.3.m1.1.1.2">𝐴</ci><ci id="S4.F5.7.3.3.m1.1.1.3.cmml" xref="S4.F5.7.3.3.m1.1.1.3">𝑉</ci><apply id="S4.F5.7.3.3.m1.1.1.4.cmml" xref="S4.F5.7.3.3.m1.1.1.4"><csymbol cd="ambiguous" id="S4.F5.7.3.3.m1.1.1.4.1.cmml" xref="S4.F5.7.3.3.m1.1.1.4">subscript</csymbol><ci id="S4.F5.7.3.3.m1.1.1.4.2.cmml" xref="S4.F5.7.3.3.m1.1.1.4.2">𝑆</ci><ci id="S4.F5.7.3.3.m1.1.1.4.3.cmml" xref="S4.F5.7.3.3.m1.1.1.4.3">𝑣</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.7.3.3.m1.1d">AVS_{v}</annotation></semantics></math> and <math id="S4.F5.8.4.4.m2.1" class="ltx_Math" alttext="AVS_{m}" display="inline"><semantics id="S4.F5.8.4.4.m2.1b"><mrow id="S4.F5.8.4.4.m2.1.1" xref="S4.F5.8.4.4.m2.1.1.cmml"><mi id="S4.F5.8.4.4.m2.1.1.2" xref="S4.F5.8.4.4.m2.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.F5.8.4.4.m2.1.1.1" xref="S4.F5.8.4.4.m2.1.1.1.cmml">​</mo><mi id="S4.F5.8.4.4.m2.1.1.3" xref="S4.F5.8.4.4.m2.1.1.3.cmml">V</mi><mo lspace="0em" rspace="0em" id="S4.F5.8.4.4.m2.1.1.1b" xref="S4.F5.8.4.4.m2.1.1.1.cmml">​</mo><msub id="S4.F5.8.4.4.m2.1.1.4" xref="S4.F5.8.4.4.m2.1.1.4.cmml"><mi id="S4.F5.8.4.4.m2.1.1.4.2" xref="S4.F5.8.4.4.m2.1.1.4.2.cmml">S</mi><mi id="S4.F5.8.4.4.m2.1.1.4.3" xref="S4.F5.8.4.4.m2.1.1.4.3.cmml">m</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.8.4.4.m2.1c"><apply id="S4.F5.8.4.4.m2.1.1.cmml" xref="S4.F5.8.4.4.m2.1.1"><times id="S4.F5.8.4.4.m2.1.1.1.cmml" xref="S4.F5.8.4.4.m2.1.1.1"></times><ci id="S4.F5.8.4.4.m2.1.1.2.cmml" xref="S4.F5.8.4.4.m2.1.1.2">𝐴</ci><ci id="S4.F5.8.4.4.m2.1.1.3.cmml" xref="S4.F5.8.4.4.m2.1.1.3">𝑉</ci><apply id="S4.F5.8.4.4.m2.1.1.4.cmml" xref="S4.F5.8.4.4.m2.1.1.4"><csymbol cd="ambiguous" id="S4.F5.8.4.4.m2.1.1.4.1.cmml" xref="S4.F5.8.4.4.m2.1.1.4">subscript</csymbol><ci id="S4.F5.8.4.4.m2.1.1.4.2.cmml" xref="S4.F5.8.4.4.m2.1.1.4.2">𝑆</ci><ci id="S4.F5.8.4.4.m2.1.1.4.3.cmml" xref="S4.F5.8.4.4.m2.1.1.4.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.8.4.4.m2.1d">AVS_{m}</annotation></semantics></math> require generated data-GT pairs, we use GT LRS2 data and our model’s output. On the other hand, for </span>(a,b,c)<span id="S4.F5.8.4.6" class="ltx_text ltx_font_medium">, we only use GT LRS2 data.
</span></span></figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Qualitative Evaluation</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In <a href="#S4.F4" title="In 4.2 Quantitative Evaluation ‣ 4 Experimental Results ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, we present a qualitative comparison with SOTA models.
We employ the respective publicly available models of the compared methods while generating videos.
We choose the HDTF dataset to present results from an unseen dataset, except for DINet since it was trained on the HDTF dataset.
The results clearly show that our model surpasses all other methods in terms of having the most similar lip shapes with the GT face.
Besides, mouth region and teeth have lower quality in Wav2Lip.
Moreover, TalkLip has a severe pose stability issues and shows artifacts around the face.
This clearly degrades the naturalness of a video.
Our user study in <a href="#S4.T3" title="In 4.4 User Study ‣ 4 Experimental Results ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a> validates the poor visual quality of TalkLip, DINet, and Wav2Lip.
In summary, the qualitative analysis and user study demonstrate the superiority of our model in terms of the lip synchronization and visual quality, resulting in natural talking face generation.
Moreover, we present results in <a href="#S4.F4" title="In 4.2 Quantitative Evaluation ‣ 4 Experimental Results ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a> by employing a face enhancement model, GFPGAN, to show that our model’s results can be improved with a post-processing step to produce high resolution faces for high-resolution talking face video generation.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>User Study</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.2" class="ltx_p">We conduct a user study to explore how the generated videos look to humans.
We randomly select ten videos from the HDTF dataset and generate these videos with each model to use in the user study along with the GT videos.
Users were shown aligned videos of multiple faces generated by different methods and asked to rank them on lip synchronization, visual quality and overall quality.
In total, ten different participants joined the user study and we present the results in <a href="#S4.T3" title="In 4.4 User Study ‣ 4 Experimental Results ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a>.
The scores are scaled between <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mn id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><cn type="integer" id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">1</annotation></semantics></math> (worst) and <math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><mn id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><cn type="integer" id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">5</annotation></semantics></math> (best).
According to the results, we outperform all other models in lip synchronization, visual quality, and overall quality.
TalkLip demonstrates the second-best synchronization performance.
However, its visual quality issues and artifacts yield the lowest visual quality score.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.5.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.6.2" class="ltx_text" style="font-size:90%;">User study on randomly selected HDTF videos <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite>.</span></figcaption>
<table id="S4.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.3.3" class="ltx_tr">
<th id="S4.T3.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T3.3.3.4.1" class="ltx_text" style="font-size:80%;">Method</span></th>
<th id="S4.T3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T3.1.1.1.1" class="ltx_text" style="font-size:80%;">Sync </span><math id="S4.T3.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.1.1.1.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T3.2.2.2.1" class="ltx_text" style="font-size:80%;">Visual </span><math id="S4.T3.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.2.2.2.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T3.2.2.2.m1.1.1" xref="S4.T3.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T3.3.3.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T3.3.3.3.1" class="ltx_text" style="font-size:80%;">Overall </span><math id="S4.T3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.3.3.3.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T3.3.3.3.m1.1.1" xref="S4.T3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.3.4.1" class="ltx_tr">
<th id="S4.T3.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">
<span id="S4.T3.3.4.1.1.1" class="ltx_text" style="font-size:80%;">Wav2Lip </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.3.4.1.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a><span id="S4.T3.3.4.1.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.4.1.2.1" class="ltx_text" style="font-size:80%;">2.91</span></td>
<td id="S4.T3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.4.1.3.1" class="ltx_text" style="font-size:80%;">2.88</span></td>
<td id="S4.T3.3.4.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="S4.T3.3.4.1.4.1" class="ltx_text" style="font-size:80%;">2.73</span></td>
</tr>
<tr id="S4.T3.3.5.2" class="ltx_tr">
<th id="S4.T3.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T3.3.5.2.1.1" class="ltx_text" style="font-size:80%;">VideoReTalking w/ FR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.3.5.2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a><span id="S4.T3.3.5.2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T3.3.5.2.2" class="ltx_td ltx_align_center"><span id="S4.T3.3.5.2.2.1" class="ltx_text" style="font-size:80%;">3.05</span></td>
<td id="S4.T3.3.5.2.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.5.2.3.1" class="ltx_text" style="font-size:80%;">3.70</span></td>
<td id="S4.T3.3.5.2.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.3.5.2.4.1" class="ltx_text" style="font-size:80%;">3.46</span></td>
</tr>
<tr id="S4.T3.3.6.3" class="ltx_tr">
<th id="S4.T3.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T3.3.6.3.1.1" class="ltx_text" style="font-size:80%;">DINet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.3.6.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a><span id="S4.T3.3.6.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T3.3.6.3.2" class="ltx_td ltx_align_center"><span id="S4.T3.3.6.3.2.1" class="ltx_text" style="font-size:80%;">2.50</span></td>
<td id="S4.T3.3.6.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.6.3.3.1" class="ltx_text" style="font-size:80%;">2.34</span></td>
<td id="S4.T3.3.6.3.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.3.6.3.4.1" class="ltx_text" style="font-size:80%;">2.48</span></td>
</tr>
<tr id="S4.T3.3.7.4" class="ltx_tr">
<th id="S4.T3.3.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T3.3.7.4.1.1" class="ltx_text" style="font-size:80%;">TalkLip </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.3.7.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a><span id="S4.T3.3.7.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T3.3.7.4.2" class="ltx_td ltx_align_center"><span id="S4.T3.3.7.4.2.1" class="ltx_text" style="font-size:80%;">3.32</span></td>
<td id="S4.T3.3.7.4.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.7.4.3.1" class="ltx_text" style="font-size:80%;">2.05</span></td>
<td id="S4.T3.3.7.4.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.3.7.4.4.1" class="ltx_text" style="font-size:80%;">2.08</span></td>
</tr>
<tr id="S4.T3.3.8.5" class="ltx_tr">
<th id="S4.T3.3.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">
<span id="S4.T3.3.8.5.1.1" class="ltx_text" style="font-size:80%;">IPLAP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.3.8.5.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a><span id="S4.T3.3.8.5.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T3.3.8.5.2" class="ltx_td ltx_align_center"><span id="S4.T3.3.8.5.2.1" class="ltx_text" style="font-size:80%;">2.62</span></td>
<td id="S4.T3.3.8.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.8.5.3.1" class="ltx_text" style="font-size:80%;">3.86</span></td>
<td id="S4.T3.3.8.5.4" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T3.3.8.5.4.1" class="ltx_text" style="font-size:80%;">3.27</span></td>
</tr>
<tr id="S4.T3.3.9.6" class="ltx_tr">
<th id="S4.T3.3.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T3.3.9.6.1.1" class="ltx_text" style="font-size:80%;">Ours</span></th>
<td id="S4.T3.3.9.6.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.3.9.6.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">3.92</span></td>
<td id="S4.T3.3.9.6.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.3.9.6.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">4.02</span></td>
<td id="S4.T3.3.9.6.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T3.3.9.6.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">3.95</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Ablation Study</h3>

<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.14.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.15.2" class="ltx_text" style="font-size:90%;">Ablation study on LRS2 dataset for AV-HuBERT-based synchronization losses.</span></figcaption>
<div id="S4.T4.12" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:216.8pt;height:41.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-127.2pt,24.3pt) scale(0.460172129195869,0.460172129195869) ;">
<table id="S4.T4.12.12" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.12.12.12" class="ltx_tr">
<th id="S4.T4.12.12.12.13" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Method</th>
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">SSIM <math id="S4.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">PSNR <math id="S4.T4.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T4.2.2.2.2.m1.1.1" xref="S4.T4.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.2.m1.1b"><ci id="S4.T4.2.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T4.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">FID <math id="S4.T4.3.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T4.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T4.3.3.3.3.m1.1.1" xref="S4.T4.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.3.m1.1b"><ci id="S4.T4.3.3.3.3.m1.1.1.cmml" xref="S4.T4.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T4.4.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">LMD <math id="S4.T4.4.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T4.4.4.4.4.m1.1a"><mo stretchy="false" id="S4.T4.4.4.4.4.m1.1.1" xref="S4.T4.4.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.4.4.m1.1b"><ci id="S4.T4.4.4.4.4.m1.1.1.cmml" xref="S4.T4.4.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T4.5.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">LSE-C <math id="S4.T4.5.5.5.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.5.5.5.5.m1.1a"><mo stretchy="false" id="S4.T4.5.5.5.5.m1.1.1" xref="S4.T4.5.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.5.5.m1.1b"><ci id="S4.T4.5.5.5.5.m1.1.1.cmml" xref="S4.T4.5.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.5.5.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T4.6.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">LSE-D <math id="S4.T4.6.6.6.6.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T4.6.6.6.6.m1.1a"><mo stretchy="false" id="S4.T4.6.6.6.6.m1.1.1" xref="S4.T4.6.6.6.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.6.6.m1.1b"><ci id="S4.T4.6.6.6.6.m1.1.1.cmml" xref="S4.T4.6.6.6.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.6.6.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T4.8.8.8.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AVS<sub id="S4.T4.8.8.8.8.1" class="ltx_sub"><span id="S4.T4.8.8.8.8.1.1" class="ltx_text ltx_font_italic">u</span></sub> <math id="S4.T4.8.8.8.8.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.8.8.8.8.m2.1a"><mo stretchy="false" id="S4.T4.8.8.8.8.m2.1.1" xref="S4.T4.8.8.8.8.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.8.8.8.8.m2.1b"><ci id="S4.T4.8.8.8.8.m2.1.1.cmml" xref="S4.T4.8.8.8.8.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.8.8.8.8.m2.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T4.10.10.10.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AVS<sub id="S4.T4.10.10.10.10.1" class="ltx_sub"><span id="S4.T4.10.10.10.10.1.1" class="ltx_text ltx_font_italic">m</span></sub> <math id="S4.T4.10.10.10.10.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.10.10.10.10.m2.1a"><mo stretchy="false" id="S4.T4.10.10.10.10.m2.1.1" xref="S4.T4.10.10.10.10.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.10.10.10.10.m2.1b"><ci id="S4.T4.10.10.10.10.m2.1.1.cmml" xref="S4.T4.10.10.10.10.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.10.10.10.10.m2.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T4.12.12.12.12" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">AVS<sub id="S4.T4.12.12.12.12.1" class="ltx_sub"><span id="S4.T4.12.12.12.12.1.1" class="ltx_text ltx_font_italic">v</span></sub> <math id="S4.T4.12.12.12.12.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.12.12.12.12.m2.1a"><mo stretchy="false" id="S4.T4.12.12.12.12.m2.1.1" xref="S4.T4.12.12.12.12.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.12.12.12.12.m2.1b"><ci id="S4.T4.12.12.12.12.m2.1.1.cmml" xref="S4.T4.12.12.12.12.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.12.12.12.12.m2.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.12.12.13.1" class="ltx_tr">
<th id="S4.T4.12.12.13.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Baseline</th>
<td id="S4.T4.12.12.13.1.2" class="ltx_td ltx_align_center ltx_border_t">0.864</td>
<td id="S4.T4.12.12.13.1.3" class="ltx_td ltx_align_center ltx_border_t">26.424</td>
<td id="S4.T4.12.12.13.1.4" class="ltx_td ltx_align_center ltx_border_t">12.25</td>
<td id="S4.T4.12.12.13.1.5" class="ltx_td ltx_align_center ltx_border_t">2.423</td>
<td id="S4.T4.12.12.13.1.6" class="ltx_td ltx_align_center ltx_border_t">7.116</td>
<td id="S4.T4.12.12.13.1.7" class="ltx_td ltx_align_center ltx_border_t">7.396</td>
<td id="S4.T4.12.12.13.1.8" class="ltx_td ltx_align_center ltx_border_t">0.301</td>
<td id="S4.T4.12.12.13.1.9" class="ltx_td ltx_align_center ltx_border_t">0.637</td>
<td id="S4.T4.12.12.13.1.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.423</td>
</tr>
<tr id="S4.T4.12.12.14.2" class="ltx_tr">
<th id="S4.T4.12.12.14.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Visual-visual</th>
<td id="S4.T4.12.12.14.2.2" class="ltx_td ltx_align_center">0.905</td>
<td id="S4.T4.12.12.14.2.3" class="ltx_td ltx_align_center">29.248</td>
<td id="S4.T4.12.12.14.2.4" class="ltx_td ltx_align_center">15.14</td>
<td id="S4.T4.12.12.14.2.5" class="ltx_td ltx_align_center">1.798</td>
<td id="S4.T4.12.12.14.2.6" class="ltx_td ltx_align_center">7.481</td>
<td id="S4.T4.12.12.14.2.7" class="ltx_td ltx_align_center">6.556</td>
<td id="S4.T4.12.12.14.2.8" class="ltx_td ltx_align_center">0.381</td>
<td id="S4.T4.12.12.14.2.9" class="ltx_td ltx_align_center">0.765</td>
<td id="S4.T4.12.12.14.2.10" class="ltx_td ltx_nopad_r ltx_align_center">0.545</td>
</tr>
<tr id="S4.T4.12.12.15.3" class="ltx_tr">
<th id="S4.T4.12.12.15.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Multimodal</th>
<td id="S4.T4.12.12.15.3.2" class="ltx_td ltx_align_center">0.910</td>
<td id="S4.T4.12.12.15.3.3" class="ltx_td ltx_align_center">30.014</td>
<td id="S4.T4.12.12.15.3.4" class="ltx_td ltx_align_center">6.11</td>
<td id="S4.T4.12.12.15.3.5" class="ltx_td ltx_align_center">1.774</td>
<td id="S4.T4.12.12.15.3.6" class="ltx_td ltx_align_center">6.998</td>
<td id="S4.T4.12.12.15.3.7" class="ltx_td ltx_align_center">6.794</td>
<td id="S4.T4.12.12.15.3.8" class="ltx_td ltx_align_center">0.395</td>
<td id="S4.T4.12.12.15.3.9" class="ltx_td ltx_align_center">0.789</td>
<td id="S4.T4.12.12.15.3.10" class="ltx_td ltx_nopad_r ltx_align_center">0.575</td>
</tr>
<tr id="S4.T4.12.12.16.4" class="ltx_tr">
<th id="S4.T4.12.12.16.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Unsupervised</th>
<td id="S4.T4.12.12.16.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.12.12.16.4.2.1" class="ltx_text ltx_font_bold">0.947</span></td>
<td id="S4.T4.12.12.16.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.12.12.16.4.3.1" class="ltx_text ltx_font_bold">31.273</span></td>
<td id="S4.T4.12.12.16.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.12.12.16.4.4.1" class="ltx_text ltx_font_bold">4.51</span></td>
<td id="S4.T4.12.12.16.4.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.12.12.16.4.5.1" class="ltx_text ltx_font_bold">1.188</span></td>
<td id="S4.T4.12.12.16.4.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.12.12.16.4.6.1" class="ltx_text ltx_font_bold">7.958</span></td>
<td id="S4.T4.12.12.16.4.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.12.12.16.4.7.1" class="ltx_text ltx_font_bold">6.301</span></td>
<td id="S4.T4.12.12.16.4.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.12.12.16.4.8.1" class="ltx_text ltx_font_bold">0.508</span></td>
<td id="S4.T4.12.12.16.4.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.12.12.16.4.9.1" class="ltx_text ltx_font_bold">0.939</span></td>
<td id="S4.T4.12.12.16.4.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T4.12.12.16.4.10.1" class="ltx_text ltx_font_bold">0.879</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.2" class="ltx_p">We conduct an ablation study to show the effect of using AV-HuBERT features in lip-sync loss throughout training.
For this, we first train our model with <span id="S4.SS5.p1.2.1" class="ltx_text ltx_font_italic">lip-expert</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> features for calculating lip-sync loss.
We call this model <span id="S4.SS5.p1.2.2" class="ltx_text ltx_font_italic">baseline</span> in <a href="#S4.T4" title="In 4.5 Ablation Study ‣ 4 Experimental Results ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>.
<span id="S4.SS5.p1.2.3" class="ltx_text ltx_font_italic">Unsupervised</span> represents the method that we propose to use.
Specifically, we extract features from the audio and lip sequences using the AV-HuBERT transformer encoder.
Afterward, we calculate lip-sync loss to explicitly measure the synchronization performance of the model in the training.
We also employ AV-HuBERT in two more ways to compare with our approach, following the AVS<sub id="S4.SS5.p1.2.4" class="ltx_sub"><span id="S4.SS5.p1.2.4.1" class="ltx_text ltx_font_italic">v</span></sub> and AVS<sub id="S4.SS5.p1.2.5" class="ltx_sub"><span id="S4.SS5.p1.2.5.1" class="ltx_text ltx_font_italic">m</span></sub> metrics.
In the <span id="S4.SS5.p1.2.6" class="ltx_text ltx_font_italic">visual-visual</span> approach, we extract only visual features by feeding the AV-HuBERT model with the generated lips and GT lips, individually.
Then, we apply lip-sync loss between these two embeddings without involving audio.
This method obtains better scores than <span id="S4.SS5.p1.2.7" class="ltx_text ltx_font_italic">baseline</span>.
We further apply the <span id="S4.SS5.p1.2.8" class="ltx_text ltx_font_italic">multimodal</span> strategy, extracting features from generated lips-audio pairs and also from GT lips-audio pairs, and then applying lip-sync loss thereupon.
The <span id="S4.SS5.p1.2.9" class="ltx_text ltx_font_italic">multimodal</span> approach enhances the visual quality compared to <span id="S4.SS5.p1.2.10" class="ltx_text ltx_font_italic">baseline</span> and <span id="S4.SS5.p1.2.11" class="ltx_text ltx_font_italic">visual-visual</span> approaches but decreases the LSE-C &amp; D scores even below the <span id="S4.SS5.p1.2.12" class="ltx_text ltx_font_italic">baseline</span>.
On the other hand, in our synchronization metrics, <span id="S4.SS5.p1.2.13" class="ltx_text ltx_font_italic">multimodal</span> outperforms the <span id="S4.SS5.p1.2.14" class="ltx_text ltx_font_italic">baseline</span> as well as <span id="S4.SS5.p1.2.15" class="ltx_text ltx_font_italic">visual-visual</span> method.
Finally, the best results in visual quality and lip synchronization are achieved by employing the <span id="S4.SS5.p1.2.16" class="ltx_text ltx_font_italic">unsupervised</span> approach according to the ablation study.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">In <a href="#S4.F6" title="In 4.5 Ablation Study ‣ 4 Experimental Results ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>, we share the sample images from the approaches presented in <a href="#S4.T4" title="In 4.5 Ablation Study ‣ 4 Experimental Results ‣ Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a>.
The <span id="S4.SS5.p2.1.1" class="ltx_text ltx_font_italic">baseline</span> has distinguishable face borders, artifacts in the mouth region, and not fully aligned lip shape.
Although <span id="S4.SS5.p2.1.2" class="ltx_text ltx_font_italic">visual-visual</span> shows enhanced visual quality and improved lip shape, <span id="S4.SS5.p2.1.3" class="ltx_text ltx_font_italic">multimodal</span> approach generates more appropriate lip shapes.
Finally, <span id="S4.SS5.p2.1.4" class="ltx_text ltx_font_italic">unsupervised</span> is able to generate the best fitting lip shapes as well as enhanced visual quality.
Specifically, the teeth have better visual quality and are more similar to the GT in terms of the characteristic features of the subject (identity).</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2405.04327/assets/x12.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="106" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Qualitative samples from the ablation study</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We propose to use the pretrained audio-visual speech representation expert AV-HuBERT for training a talking face generation network with high-quality audio-lip synchronization.
Furthermore, we utilize this network to obtain three complementary and robust metrics for evaluating lip synchronization.
Our experimental results demonstrate the effectiveness of our approach.
We also analyze the proposed metrics for robustness and validate their alignment with human preferences through a user study.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Limitations</span>
AV-HuBERT and its features should be investigated further to employ them more efficiently for lip synchronization, despite increased performance and stability in the training.
Furthermore, the sample size of our user study could be increased to gain more statistical power.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">Ethics &amp; Social Impact</span>
Talking face generation is essential for a wide range of applications.
However, its vulnerability and potential for misuse (e.g., deepfake) pose significant risks.
We will apply Watermarking and take necessary precautions to prevent unauthorized usage of our model.</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_bold">Acknowledgement</span>
This work was supported in part by the European Commission project Meetween (101135798) under the call HORIZON-CL4-2023-HUMAN-01-03.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Afouras et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Triantafyllos Afouras, Joon Son Chung, Andrew Senior, Oriol Vinyals, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Deep audio-visual speech recognition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib1.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</em><span id="bib.bib1.10.2" class="ltx_text" style="font-size:90%;">, 44(12):8717–8727, 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.4.4.1" class="ltx_text" style="font-size:90%;">Blanz and Vetter [1999]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.6.1" class="ltx_text" style="font-size:90%;">
Volker Blanz and Thomas Vetter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">A morphable model for the synthesis of 3d faces.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib2.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</em><span id="bib.bib2.10.3" class="ltx_text" style="font-size:90%;">, pages 187–194, 1999.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Booth et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
James Booth, Anastasios Roussos, Allan Ponniah, David Dunaway, and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">Large scale 3d morphable models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib3.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</em><span id="bib.bib3.10.2" class="ltx_text" style="font-size:90%;">, 126(2):233–254, 2018.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.4.4.1" class="ltx_text" style="font-size:90%;">Brand [1999]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.6.1" class="ltx_text" style="font-size:90%;">
Matthew Brand.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">Voice puppetry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib4.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</em><span id="bib.bib4.10.3" class="ltx_text" style="font-size:90%;">, pages 21–28, 1999.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.4.4.1" class="ltx_text" style="font-size:90%;">Bulat and Tzimiropoulos [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.6.1" class="ltx_text" style="font-size:90%;">
Adrian Bulat and Georgios Tzimiropoulos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib5.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Computer Vision</em><span id="bib.bib5.10.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Lele Chen, Zhiheng Li, Ross K Maddox, Zhiyao Duan, and Chenliang Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">Lip movements generation at a glance.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib6.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the European conference on computer vision (ECCV)</em><span id="bib.bib6.11.3" class="ltx_text" style="font-size:90%;">, pages 520–535, 2018.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
Lele Chen, Ross K Maddox, Zhiyao Duan, and Chenliang Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">Hierarchical cross-modal talking face generation with dynamic pixel-wise loss.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib7.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span id="bib.bib7.11.3" class="ltx_text" style="font-size:90%;">, pages 7832–7841, 2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Cheng et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Kun Cheng, Xiaodong Cun, Yong Zhang, Menghan Xia, Fei Yin, Mingrui Zhu, Xuan Wang, Jue Wang, and Nannan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">Videoretalking: Audio-based lip synchronization for talking head video editing in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib8.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">SIGGRAPH Asia 2022 Conference Papers</em><span id="bib.bib8.11.3" class="ltx_text" style="font-size:90%;">, pages 1–9, 2022.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Chung et al. [2003]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
JS Chung, J Huh, S Mun, M Lee, HS Heo, S Choe, C Ham, S Jung, BJ Lee, and I Han.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">In defence of metric learning for speaker recognition. arxiv 2020.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib9.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2003.11982</em><span id="bib.bib9.10.2" class="ltx_text" style="font-size:90%;">, 2003.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.4.4.1" class="ltx_text" style="font-size:90%;">Chung and Zisserman [2017a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.6.1" class="ltx_text" style="font-size:90%;">
Joon Son Chung and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">Lip reading in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib10.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision–ACCV 2016: 13th Asian Conference on Computer Vision, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part II 13</em><span id="bib.bib10.10.3" class="ltx_text" style="font-size:90%;">, pages 87–103. Springer, 2017a.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.4.4.1" class="ltx_text" style="font-size:90%;">Chung and Zisserman [2017b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.6.1" class="ltx_text" style="font-size:90%;">
Joon Son Chung and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">Out of time: automated lip sync in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib11.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision–ACCV 2016 Workshops: ACCV 2016 International Workshops, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part II 13</em><span id="bib.bib11.10.3" class="ltx_text" style="font-size:90%;">, pages 251–263. Springer, 2017b.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Das et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Dipanjan Das, Sandika Biswas, Sanjana Sinha, and Brojeshwar Bhowmick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Speech-driven facial animation using cascaded gans for learning of motion and texture.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib12.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX 16</em><span id="bib.bib12.11.3" class="ltx_text" style="font-size:90%;">, pages 408–424. Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Goodfellow et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">Generative adversarial nets.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib13.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib13.10.2" class="ltx_text" style="font-size:90%;">, 27, 2014.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text" style="font-size:90%;">Guo et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">
Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun Bao, and Juyong Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">Ad-nerf: Audio driven neural radiance fields for talking head synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib14.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib14.11.3" class="ltx_text" style="font-size:90%;">, pages 5784–5794, 2021.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">He et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib15.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span id="bib.bib15.11.3" class="ltx_text" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Heusel et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">Gans trained by a two time-scale update rule converge to a local nash equilibrium.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib16.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib16.10.2" class="ltx_text" style="font-size:90%;">, 30, 2017.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.4.4.1" class="ltx_text" style="font-size:90%;">Ioffe and Szegedy [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.6.1" class="ltx_text" style="font-size:90%;">
Sergey Ioffe and Christian Szegedy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">Batch normalization: Accelerating deep network training by reducing internal covariate shift.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib17.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International conference on machine learning</em><span id="bib.bib17.10.3" class="ltx_text" style="font-size:90%;">, pages 448–456. pmlr, 2015.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">Johnson et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Justin Johnson, Alexandre Alahi, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">Perceptual losses for real-time style transfer and super-resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib18.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14</em><span id="bib.bib18.11.3" class="ltx_text" style="font-size:90%;">, pages 694–711. Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.4.4.1" class="ltx_text" style="font-size:90%;">Kingma and Ba [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.6.1" class="ltx_text" style="font-size:90%;">
Diederik P Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib19.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1412.6980</em><span id="bib.bib19.9.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Krizhevsky et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">Imagenet classification with deep convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib20.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib20.10.2" class="ltx_text" style="font-size:90%;">, 25, 2012.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Liang et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Borong Liang, Yan Pan, Zhizhi Guo, Hang Zhou, Zhibin Hong, Xiaoguang Han, Junyu Han, Jingtuo Liu, Errui Ding, and Jingdong Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">Expressive talking head generation with granular audio-visual control.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib21.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib21.11.3" class="ltx_text" style="font-size:90%;">, pages 3387–3396, 2022.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Xian Liu, Yinghao Xu, Qianyi Wu, Hang Zhou, Wayne Wu, and Bolei Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Semantic-aware implicit neural audio-driven video portrait generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib22.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXVII</em><span id="bib.bib22.11.3" class="ltx_text" style="font-size:90%;">, pages 106–125. Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Miyato et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">Spectral normalization for generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib23.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1802.05957</em><span id="bib.bib23.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Muaz et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Urwa Muaz, Wondong Jang, Rohun Tripathi, Santhosh Mani, Wenbin Ouyang, Ravi Teja Gadde, Baris Gecer, Sergio Elizondo, Reza Madad, and Naveen Nair.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Sidgan: High-resolution dubbed video generation via shift-invariant learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib24.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib24.11.3" class="ltx_text" style="font-size:90%;">, pages 7833–7842, 2023.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.4.4.1" class="ltx_text" style="font-size:90%;">Nair and Hinton [2010]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.6.1" class="ltx_text" style="font-size:90%;">
Vinod Nair and Geoffrey E Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">Rectified linear units improve restricted boltzmann machines.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib25.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 27th international conference on machine learning (ICML-10)</em><span id="bib.bib25.10.3" class="ltx_text" style="font-size:90%;">, pages 807–814, 2010.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Papantoniou et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Foivos Paraperas Papantoniou, Panagiotis P Filntisis, Petros Maragos, and Anastasios Roussos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Neural emotion director: Speech-preserving semantic control of facial expressions in” in-the-wild” videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib26.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib26.11.3" class="ltx_text" style="font-size:90%;">, pages 18781–18790, 2022.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Park et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Se Jin Park, Minsu Kim, Joanna Hong, Jeongsoo Choi, and Yong Man Ro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">Synctalkface: Talking face generation with precise lip-syncing via audio-lip memory.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib27.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial Intelligence</em><span id="bib.bib27.11.3" class="ltx_text" style="font-size:90%;">, pages 2062–2070, 2022.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Prajwal et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
KR Prajwal, Rudrabha Mukhopadhyay, Vinay P Namboodiri, and CV Jawahar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">A lip sync expert is all you need for speech to lip generation in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib28.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 28th ACM International Conference on Multimedia</em><span id="bib.bib28.11.3" class="ltx_text" style="font-size:90%;">, pages 484–492, 2020.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Ritter et al. [1999]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Max Ritter, Uwe Meier, Jie Yang, and Alex Waibel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Face translation: A multimodal translation agent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib29.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">AVSP’99-International Conference on Auditory-Visual Speech Processing</em><span id="bib.bib29.11.3" class="ltx_text" style="font-size:90%;">. Citeseer, 1999.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="font-size:90%;">Shen et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">
Shuai Shen, Wanhua Li, Zheng Zhu, Yueqi Duan, Jie Zhou, and Jiwen Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">Learning dynamic facial radiance fields for few-shot talking head synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib30.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XII</em><span id="bib.bib30.11.3" class="ltx_text" style="font-size:90%;">, pages 666–682. Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text" style="font-size:90%;">Shi et al. [2022a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">
Bowen Shi, Wei-Ning Hsu, Kushal Lakhotia, and Abdelrahman Mohamed.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="font-size:90%;">Learning audio-visual speech representation by masked multimodal cluster prediction.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib31.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2201.02184</em><span id="bib.bib31.10.2" class="ltx_text" style="font-size:90%;">, 2022a.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text" style="font-size:90%;">Shi et al. [2022b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">
Bowen Shi, Wei-Ning Hsu, and Abdelrahman Mohamed.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">Robust self-supervised audio-visual speech recognition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib32.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2201.01763</em><span id="bib.bib32.10.2" class="ltx_text" style="font-size:90%;">, 2022b.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.4.4.1" class="ltx_text" style="font-size:90%;">Simonyan and Zisserman [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.6.1" class="ltx_text" style="font-size:90%;">
Karen Simonyan and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">Very deep convolutional networks for large-scale image recognition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib33.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1409.1556</em><span id="bib.bib33.9.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">Song et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
Linsen Song, Wayne Wu, Chen Qian, Ran He, and Chen Change Loy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">Everybody’s talkin’: Let me talk as you want.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib34.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Information Forensics and Security</em><span id="bib.bib34.10.2" class="ltx_text" style="font-size:90%;">, 17:585–598, 2022.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text" style="font-size:90%;">Suwajanakorn et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">
Supasorn Suwajanakorn, Steven M Seitz, and Ira Kemelmacher-Shlizerman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">Synthesizing obama: learning lip sync from audio.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib35.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM Transactions on Graphics (ToG)</em><span id="bib.bib35.10.2" class="ltx_text" style="font-size:90%;">, 36(4):1–13, 2017.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.5.5.1" class="ltx_text" style="font-size:90%;">Tang et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">
Jiaxiang Tang, Kaisiyuan Wang, Hang Zhou, Xiaokang Chen, Dongliang He, Tianshu Hu, Jingtuo Liu, Gang Zeng, and Jingdong Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="font-size:90%;">Real-time neural radiance talking portrait synthesis via audio-spatial decomposition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib36.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2211.12368</em><span id="bib.bib36.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text" style="font-size:90%;">Thies et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">
Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian Theobalt, and Matthias Nießner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">Neural voice puppetry: Audio-driven facial reenactment.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib37.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVI 16</em><span id="bib.bib37.11.3" class="ltx_text" style="font-size:90%;">, pages 716–731. Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text" style="font-size:90%;">Waibel et al. [2003]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">
Alex Waibel, Tanja Schultz, Michael Bett, Matthias Denecke, Robert Malkin, Ivica Rogina, Rainer Stiefelhagen, and Jie Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">Smart: The smart meeting room task at isl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib38.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP’03).</em><span id="bib.bib38.11.3" class="ltx_text" style="font-size:90%;">, pages IV–752. IEEE, 2003.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.5.5.1" class="ltx_text" style="font-size:90%;">Waibel et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">
Alexander Waibel, Moritz Behr, Dogucan Yaman, Fevziye Irem Eyiokur, Tuan-Nam Nguyen, Carlos Mullov, Mehmet Arif Demirtas, Alperen Kantarci, Stefan Constantin, and Hazim Kemal Ekenel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">Face-dubbing++: Lip-synchronous, voice preserving translation of videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib39.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)</em><span id="bib.bib39.11.3" class="ltx_text" style="font-size:90%;">, pages 1–5. IEEE, 2023.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">
Duomin Wang, Yu Deng, Zixin Yin, Heung-Yeung Shum, and Baoyuan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">Progressive disentangled representation learning for fine-grained controllable talking head synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib40.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib40.11.3" class="ltx_text" style="font-size:90%;">, pages 17979–17989, 2023a.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
Jiadong Wang, Xinyuan Qian, Malu Zhang, Robby T Tan, and Haizhou Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">Seeing what you said: Talking face generation guided by a lip reading expert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib41.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib41.11.3" class="ltx_text" style="font-size:90%;">, pages 14653–14662, 2023b.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2023c]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
Jiayu Wang, Kang Zhao, Shiwei Zhang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">Lipformer: High-fidelity and generalizable talking face generation with a pre-learned facial codebook.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib42.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib42.11.3" class="ltx_text" style="font-size:90%;">, pages 13844–13853, 2023c.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">
Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="font-size:90%;">Towards real-world blind face restoration with generative facial prior.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib43.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib43.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2004]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text" style="font-size:90%;">Image quality assessment: from error visibility to structural similarity.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib44.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE transactions on image processing</em><span id="bib.bib44.10.2" class="ltx_text" style="font-size:90%;">, 13(4):600–612, 2004.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text" style="font-size:90%;">Wu et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="font-size:90%;">
Haozhe Wu, Jia Jia, Haoyu Wang, Yishun Dou, Chao Duan, and Qingshan Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="font-size:90%;">Imitating arbitrary talking style for realistic audio-driven talking face synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib45.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 29th ACM International Conference on Multimedia</em><span id="bib.bib45.11.3" class="ltx_text" style="font-size:90%;">, pages 1478–1486, 2021.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.5.5.1" class="ltx_text" style="font-size:90%;">Yaman et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="font-size:90%;">
Dogucan Yaman, Fevziye Irem Eyiokur, Leonard Bärmann, Hazim Kemal Ekenel, and Alexander Waibel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.8.1" class="ltx_text" style="font-size:90%;">Audio-driven talking face generation by overcoming unintended information flow, 2023.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.5.5.1" class="ltx_text" style="font-size:90%;">Yao et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text" style="font-size:90%;">
Shunyu Yao, RuiZhe Zhong, Yichao Yan, Guangtao Zhai, and Xiaokang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.8.1" class="ltx_text" style="font-size:90%;">Dfa-nerf: personalized talking head generation via disentangled face attributes neural rendering.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib47.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2201.00791</em><span id="bib.bib47.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text" style="font-size:90%;">Ye et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text" style="font-size:90%;">
Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, JinZheng He, and Zhou Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text" style="font-size:90%;">Geneface: Generalized and high-fidelity audio-driven 3d talking face synthesis.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib48.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2301.13430</em><span id="bib.bib48.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.5.5.1" class="ltx_text" style="font-size:90%;">Yehia et al. [1998]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.7.1" class="ltx_text" style="font-size:90%;">
Hani Yehia, Philip Rubin, and Eric Vatikiotis-Bateson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.8.1" class="ltx_text" style="font-size:90%;">Quantitative association of vocal-tract and facial behavior.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib49.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Speech Communication</em><span id="bib.bib49.10.2" class="ltx_text" style="font-size:90%;">, 26(1-2):23–43, 1998.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.5.5.1" class="ltx_text" style="font-size:90%;">Yin et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.7.1" class="ltx_text" style="font-size:90%;">
Fei Yin, Yong Zhang, Xiaodong Cun, Mingdeng Cao, Yanbo Fan, Xuan Wang, Qingyan Bai, Baoyuan Wu, Jue Wang, and Yujiu Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.8.1" class="ltx_text" style="font-size:90%;">Styleheat: One-shot high-resolution editable talking face generation via pre-trained stylegan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib50.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XVII</em><span id="bib.bib50.11.3" class="ltx_text" style="font-size:90%;">, pages 85–101. Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.5.5.1" class="ltx_text" style="font-size:90%;">Zhan et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.7.1" class="ltx_text" style="font-size:90%;">
Fangneng Zhan, Yingchen Yu, Rongliang Wu, Jiahui Zhang, and Shijian Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.8.1" class="ltx_text" style="font-size:90%;">Multimodal image synthesis and editing: A survey.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib51.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2112.13592</em><span id="bib.bib51.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2021a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.7.1" class="ltx_text" style="font-size:90%;">
Chenxu Zhang, Yifan Zhao, Yifei Huang, Ming Zeng, Saifeng Ni, Madhukar Budagavi, and Xiaohu Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.8.1" class="ltx_text" style="font-size:90%;">Facial: Synthesizing dynamic talking face with implicit attribute learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib52.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</em><span id="bib.bib52.11.3" class="ltx_text" style="font-size:90%;">, pages 3867–3876, 2021a.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2021b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.7.1" class="ltx_text" style="font-size:90%;">
Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.8.1" class="ltx_text" style="font-size:90%;">Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib53.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib53.11.3" class="ltx_text" style="font-size:90%;">, pages 3661–3670, 2021b.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.7.1" class="ltx_text" style="font-size:90%;">
Zhimeng Zhang, Zhipeng Hu, Wenjin Deng, Changjie Fan, Tangjie Lv, and Yu Ding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.8.1" class="ltx_text" style="font-size:90%;">Dinet: Deformation inpainting network for realistic face visually dubbing on high resolution video.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib54.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.03988</em><span id="bib.bib54.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.5.5.1" class="ltx_text" style="font-size:90%;">Zhen et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.7.1" class="ltx_text" style="font-size:90%;">
Rui Zhen, Wenchao Song, Qiang He, Juan Cao, Lei Shi, and Jia Luo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.8.1" class="ltx_text" style="font-size:90%;">Human-computer interaction system: A survey of talking-head generation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib55.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Electronics</em><span id="bib.bib55.10.2" class="ltx_text" style="font-size:90%;">, 12(1):218, 2023.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.5.5.1" class="ltx_text" style="font-size:90%;">Zhong et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.7.1" class="ltx_text" style="font-size:90%;">
Weizhi Zhong, Chaowei Fang, Yinqi Cai, Pengxu Wei, Gangming Zhao, Liang Lin, and Guanbin Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.8.1" class="ltx_text" style="font-size:90%;">Identity-preserving talking face generation with landmark and appearance priors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib56.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib56.11.3" class="ltx_text" style="font-size:90%;">, pages 9729–9738, 2023.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.5.5.1" class="ltx_text" style="font-size:90%;">Zhou et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.7.1" class="ltx_text" style="font-size:90%;">
Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, and Xiaogang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.8.1" class="ltx_text" style="font-size:90%;">Talking face generation by adversarially disentangled audio-visual representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib57.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI conference on artificial intelligence</em><span id="bib.bib57.11.3" class="ltx_text" style="font-size:90%;">, pages 9299–9306, 2019.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.5.5.1" class="ltx_text" style="font-size:90%;">Zhou et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.7.1" class="ltx_text" style="font-size:90%;">
Hang Zhou, Yasheng Sun, Wayne Wu, Chen Change Loy, Xiaogang Wang, and Ziwei Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.8.1" class="ltx_text" style="font-size:90%;">Pose-controllable talking face generation by implicitly modularized audio-visual representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib58.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span id="bib.bib58.11.3" class="ltx_text" style="font-size:90%;">, pages 4176–4186, 2021.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.5.5.1" class="ltx_text" style="font-size:90%;">Zhou et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.7.1" class="ltx_text" style="font-size:90%;">
Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos Kalogerakis, and Dingzeyu Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.8.1" class="ltx_text" style="font-size:90%;">Makelttalk: speaker-aware talking-head animation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib59.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM Transactions On Graphics (TOG)</em><span id="bib.bib59.10.2" class="ltx_text" style="font-size:90%;">, 39(6):1–15, 2020.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.04326" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.04327" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.04327">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.04327" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.04328" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 15:32:18 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
