<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.12425] Multimodal Fusion Method with Spatiotemporal Sequences and Relationship Learning for Valence-Arousal Estimation</title><meta property="og:description" content="This paper presents our approach for the VA (Valence-Arousal) estimation task in the ABAW6 competition. We devised a comprehensive model by preprocessing video frames and audio segments to extract visual and audio feat…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multimodal Fusion Method with Spatiotemporal Sequences and Relationship Learning for Valence-Arousal Estimation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multimodal Fusion Method with Spatiotemporal Sequences and Relationship Learning for Valence-Arousal Estimation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.12425">

<!--Generated on Fri Apr  5 16:00:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Multimodal Fusion Method with Spatiotemporal Sequences and Relationship Learning for Valence-Arousal Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jun Yu<sup id="id12.12.id1" class="ltx_sup">1</sup>, Gongpeng Zhao<sup id="id13.13.id2" class="ltx_sup">1</sup>, Yongqi Wang<sup id="id14.14.id3" class="ltx_sup">1</sup> , Zhihong Wei<sup id="id15.15.id4" class="ltx_sup">1</sup> , Zerui Zhang<sup id="id16.16.id5" class="ltx_sup">1</sup>, Zhongpeng Cai<sup id="id17.17.id6" class="ltx_sup">1</sup>,
<br class="ltx_break">Guochen Xie<sup id="id18.18.id7" class="ltx_sup">1</sup>, Jichao Zhu<sup id="id19.19.id8" class="ltx_sup">1</sup>, Wangyuan Zhu<sup id="id20.20.id9" class="ltx_sup">1</sup>, Yang Zheng<sup id="id21.21.id10" class="ltx_sup">1</sup>
<br class="ltx_break"><sup id="id22.22.id11" class="ltx_sup">1</sup>University of Science and Technology of China
<br class="ltx_break"><span id="id23.23.id12" class="ltx_text ltx_font_typewriter" style="font-size:90%;">harryjun@ustc.edu.cn
<br class="ltx_break">{zgp0531,wangyognqi,weizh588,igodrr,zpcai,
<br class="ltx_break">xiegc,jichaozhu,zhuwangyuan,zhengyang}@mail.ustc.edu.cn 
<br class="ltx_break"></span>
</span><span class="ltx_author_notes">Corresponding authorCorresponding author</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id24.id1" class="ltx_p">This paper presents our approach for the VA (Valence-Arousal) estimation task in the ABAW6 competition. We devised a comprehensive model by preprocessing video frames and audio segments to extract visual and audio features. Through the utilization of Temporal Convolutional Network (TCN) module, we effectively captured the temporal and spatial correlations between these features. Subsequently, we employed a Transformer encoder structure to learn long-range dependencies, thereby enhancing the model’s performance and generalization ability. Additionally, we proposed the LA-SE module to better capture local image information and enhance channel selection and suppression. Our method leverages a multimodal data fusion approach, integrating pre-trained audio and video backbones for feature extraction, followed by TCN-based spatiotemporal encoding and Transformer-based temporal information capture. Experimental results demonstrate the effectiveness of our approach, achieving competitive performance in VA estimation on the AffWild2 dataset.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With the continuous development of artificial intelligence technology and society, human-computer interaction has become a research field of great concern. An excellent human-computer interaction system not only needs to have efficient functionality but also needs to consider the user’s emotions and experiences. Emotion analysis plays an important role in this regard, as it is expected to provide a more accurate understanding of human emotions, thereby designing human-computer interaction systems that are more user-friendly and closer to user needs.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In recent years, human emotional behavior analysis has received increasing attention. Commonly used human expression modalities<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> in this field include Action Units (AU)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> basic expression categories (EXPR)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>, and Valence-Arousal (VA)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>. Action Units describe the movement of specific facial regions and are the smallest units for describing expressions. Basic expression categories classify expressions into several emotional categories such as happiness, sadness, etc. VA is a model that includes two continuous values representing the valence and arousal of emotions, which can better describe human emotional states.
The ABAW workshop<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>, <a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite> aims to address the challenges of in-the-wild emotional behavior analysis, which reflects a key characteristic of real-world Human-Computer Interaction (HCI) systems. The objective is to develop intelligent machines and robots capable of understanding human emotions, moods, and behaviors. By interacting with humans in a ”human-centered” manner, these systems provide engaging experiences and effectively serve as digital assistants. Such interactions should not be constrained by individual backgrounds, age, gender, race, education level, occupation, or social status. Therefore, the development of intelligent systems capable of accurately analyzing human behavior in the wild is essential for fostering trust, understanding, and intimacy between humans and machines in real-life environments. The advancement of this technology holds promise for the future of HCI, enabling more sophisticated and human-centric experiences in everyday life. To promote the development of human emotional behavior analysis, the Affective Behavior Analysis in-the-wild (ABAW) competition has proposed a series of challenges aimed at overcoming obstacles in this field. The ABAW competition has constructed large-scale multimodal video datasets, such as Affwild <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>and Affwild2<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>, <a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>, which provide valuable resources for researchers, greatly advancing the progress of in-the-wild facial expression analysis and accelerating the application in related industries. The Affwild2 dataset contains a large number of videos, most of which are annotated frame by frame with AU, basic expression categories, VA, and other labels, providing researchers with rich annotated data.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Emotion recognition<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>, <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>, as a critical task, plays a crucial role in human emotion understanding and human-computer interaction. Among them, the estimation of Valence and Arousal is a key component of emotion recognition. Through the dimensional model<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>, we can view emotional states as points in continuous space, with Valence and Arousal serving as axes. However, achieving accurate VA estimation faces numerous challenges. Firstly, the expression of emotions is subjective and varies between individuals, leading to uncertainty in emotion recognition results due to differences in how different individuals perceive emotions. Secondly, emotional expression is typically dynamic and influenced by time, necessitating consideration of long-term temporal dependencies to better capture the process of emotional change. Additionally, emotional expression takes various forms and may be influenced by external factors and individual experiences, requiring emotion recognition models to have a certain level of robustness to adapt to different contexts and individual characteristics. Addressing these challenges requires integrating multimodal data and advanced models to improve the accuracy and robustness of VA estimation, thereby advancing the application and development of emotion recognition technology in the fields of human-computer interaction and affective computing.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To address the challenges above, we employ a multimodal data fusion approach. Firstly, we utilize a pre-trained audio VGGnet as the backbone for extracting dynamic audio features from VGGish<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>, and an IResnet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> pre-trained with the arcface<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> method as the backbone for extracting video frames. Consequently, the expressiveness of deep features in both visual and audio modalities can be further enhanced through fine-tuning. Subsequently, for each branch, we utilize CNN backbones to extract dynamic spatial depth features from video frames and log-spectrograms, followed by the use of Temporal Convolutional Networks<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite> to further learn spatiotemporal encoding. Additionally, we integrate the LA-SE module before the TCN module to better capture local image information and enhance channel selection and suppression. The LA-SE module combines LANet for spatial aggregation and SENet for channel-wise selection, improving the effectiveness of feature extraction. Lastly, these features are concatenated and fed into a Transformer structure to capture temporal information for downstream tasks. During training, we employ a strategy of large-window resampling, where the window size determines the amount of contextual information the model considers for predicting each time step.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The main contribution of the proposed method can be summarized as:</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">1. We introduce a novel approach for fusing multimodal data, leveraging pre-trained audio and video backbones to extract dynamic features from both visual and audio modalities.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">2. To enhance feature extraction and selection, we propose the LA-SE network, combining LANet for spatial aggregation and SENet for channel-wise selection. This module captures local image information effectively, improving model performance.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">3. Utilizing Temporal Convolutional Networks (TCN), we further learn spatiotemporal encoding to capture temporal dependencies and patterns in the data.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">4. We utilize a Transformer structure to effectively capture temporal information for downstream tasks, ensuring robust performance in real-world scenarios.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In the field of affective behavior analysis, multimodal feature fusion is a key issue. Most studies<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite> adopt multimodal methods to improve model performance by fusing visual and audio features. In previous research, researchers have proposed various fusion methods, such as directly fusing multimodal features into a common feature vector for analysis<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>, or separately analyzing the features of each modality and then fusing the final outputs to obtain better prediction results<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>, or even combining the above two fusion methods to achieve better results<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>combined visual and audio information in videos, constructing a dual-stream network for emotion recognition, achieving high performance. Currently, convolution-based methods<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> have made significant progress. At the same time, the application of Transformer in multimodal learning has become mainstream, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>achieving excellent performance using the Transformer structure. In terms of audio features<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>, <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, energy features, temporal features, etc., are widely used to improve the ability of emotion recognition.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Recently, researchers have proposed many new methods and frameworks<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>, <a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>, <a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>, including tasks such as AU detection, expression recognition, and VA estimation. Some of these methods leverage the correlation between VA and AU or VA and EXPR, proposing multi-task frameworks. These methods can extract complementary information from other tasks. To address the high cost of annotating emotion/AU/VA labels from real-world facial images, some researchers have proposed using self-supervised learning (SSL) methods to leverage knowledge from existing large-scale unlabeled data. These methods provide new avenues for improving the accuracy of affective analysis.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In conclusion, multimodal features play an important role in affective behavior analysis, and various fusion methods and self-supervised learning methods provide new ideas and approaches for improving the accuracy and efficiency of emotion recognition.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2403.12425/assets/model.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="193" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">Our proposed framework for VA estimation. The model begins with preprocessing of video frames and audio segments, followed by feature extraction using pre-trained audio and video backbones. The LA-SE module enhances local image information capture and channel selection. Temporal Convolutional Network (TCN) modules capture temporal and spatial correlations in the features, while a Transformer encoder structure learns long-range dependencies. </span></figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed approach</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we focus on introducing our method. To better accomplish the VA estimation task, we have designed an efficient network. As shown in Figure <span id="S3.p1.1.1" class="ltx_text" style="color:#FF0000;">1</span>, we use a CNN backbone to extract dynamic spatial depth features from video frames and audio. Subsequently, we utilize TCN to further learn spatiotemporal encoding. Afterward, we concatenate the video and audio features and feed the output into a fully connected layer to apply to the VA estimation task.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Feature Extraction</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">VGGish:</span>VGGish<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite> is a pre-trained deep neural network model developed by Google, specifically designed for audio feature extraction. It is commonly used for tasks such as audio classification, audio event detection, and other audio-related tasks. The model architecture is based on the VGG network architecture, which consists of multiple convolutional layers followed by max-pooling layers. Its output is a 128-dimensional feature vector that can be used for speech-related tasks.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">MFCC:</span>MFCC<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> is a widely used method for audio feature extraction in tasks such as speech recognition and audio processing. It was proposed by American engineer Harvey Fletcher and Japanese engineer Takeo Yamazaki in the 1960s. Based on the perceptual characteristics of the human ear to sound frequencies, MFCC transforms audio signals into a set of feature vectors to represent the spectral characteristics of the audio. The process of extracting MFCC involves segmenting the audio signal into frames, applying windowing, performing a fast Fourier transform to compute the power spectrum, applying a Mel filterbank, taking the logarithm, and finally applying discrete cosine transform to obtain the MFCC coefficients. These coefficients can be used to train machine learning models, such as speech recognition and speaker recognition, for the analysis and processing of audio data. Its output is a 39-dimensional feature vector that can be used for speech-related tasks.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">IResNet-50:</span>IResNet-50<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite> is a variant of the ResNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> architecture. IResNet-50 incorporates the idea of residual connections to enable the training of very deep neural networks more effectively. In this study, we trained IResNet-50 using the AffectNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite> dataset and obtained a 512-dimensional visual feature vector.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.3.2" class="ltx_text" style="font-size:90%;">The dimensions of features.</span></figcaption>
<table id="S3.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.4.1.1" class="ltx_tr">
<th id="S3.T1.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.4.1.1.1.1" class="ltx_text ltx_font_bold">Feature Modality</span></th>
<th id="S3.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.4.1.1.2.1" class="ltx_text ltx_font_bold">Dimension</span></th>
<th id="S3.T1.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.4.1.1.3.1" class="ltx_text ltx_font_bold">Description</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.4.2.1" class="ltx_tr">
<td id="S3.T1.4.2.1.1" class="ltx_td ltx_align_center ltx_border_t">VGGish</td>
<td id="S3.T1.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t">128</td>
<td id="S3.T1.4.2.1.3" class="ltx_td ltx_align_center ltx_border_t">A</td>
</tr>
<tr id="S3.T1.4.3.2" class="ltx_tr">
<td id="S3.T1.4.3.2.1" class="ltx_td ltx_align_center">MFCC</td>
<td id="S3.T1.4.3.2.2" class="ltx_td ltx_align_center">39</td>
<td id="S3.T1.4.3.2.3" class="ltx_td ltx_align_center">A</td>
</tr>
<tr id="S3.T1.4.4.3" class="ltx_tr">
<td id="S3.T1.4.4.3.1" class="ltx_td ltx_align_center ltx_border_bb">IResNet-50</td>
<td id="S3.T1.4.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">512</td>
<td id="S3.T1.4.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">V</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Preprocessing</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Our preprocessing approach is primarily based on the solution proposed by Su Zhang, whoes were last year’s third-place winners<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite>. In our experiments, the input consists of one visual feature and two audio features, but their sizes are typically different, and there may even be significant discrepancies. The sizes of the features are shown in Table <span id="S3.SS2.p1.1.1" class="ltx_text" style="color:#FF0000;">1</span>. The visual preprocessing first resizes all images to a size of 48×48×3, based on the cropped-aligned image data provided by the organizer. For each trial in the training or validation set, its length is determined by the number of rows in the annotation text file, excluding those marked as -5. In the test set, the length is determined by the frame count of the original video. A zero matrix of size N×48×48×3 is initialized, and then each row is assigned to the corresponding jpg image if it exists. The audio preprocessing converts all videos to mono with a 16K sampling rate in WAV format initially. Then, the logmelspectrogram is extracted using preprocessing code from the Vggish repository, with the hop length specified to synchronize with other modalities and annotations. Emotion label processing involves excluding rows containing -5. To ensure consistency in length between features and annotations, the feature matrix is either padded repeatedly (using the last feature points) or trimmed (from the rear), depending on whether the feature length is shorter or longer than the trial length.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Training Details</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.4" class="ltx_p">In this study, we employed a six-fold cross-validation method to fully utilize the AffWild2 database. The database consists of 360 training trials, 72 validation trials, and 162 testing trials, covering various emotional expressions and contexts. Through cross-validation, we were able to leverage the information in the dataset effectively, reducing the model’s dependence on specific data distributions, thereby better assessing the performance and robustness of the model. Given the video frames <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="V_{i}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">V</mi><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">𝑉</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">V_{i}</annotation></semantics></math> and the corresponding audio segments <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="A_{i}" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><msub id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.2" xref="S3.SS3.p1.2.m2.1.1.2.cmml">A</mi><mi id="S3.SS3.p1.2.m2.1.1.3" xref="S3.SS3.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">𝐴</ci><ci id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">A_{i}</annotation></semantics></math> obtained through preprocessing, we employ pre-trained video feature extraction models (IResNet-50) and audio feature extraction models (VGGish) to extract visual features <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="F_{\text{vis}}^{i}" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><msubsup id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2.2" xref="S3.SS3.p1.3.m3.1.1.2.2.cmml">F</mi><mtext id="S3.SS3.p1.3.m3.1.1.2.3" xref="S3.SS3.p1.3.m3.1.1.2.3a.cmml">vis</mtext><mi id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">superscript</csymbol><apply id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.2.1.cmml" xref="S3.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.2.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2.2">𝐹</ci><ci id="S3.SS3.p1.3.m3.1.1.2.3a.cmml" xref="S3.SS3.p1.3.m3.1.1.2.3"><mtext mathsize="70%" id="S3.SS3.p1.3.m3.1.1.2.3.cmml" xref="S3.SS3.p1.3.m3.1.1.2.3">vis</mtext></ci></apply><ci id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">F_{\text{vis}}^{i}</annotation></semantics></math> and audio features <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="F_{\text{aud}}^{i}" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><msubsup id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml"><mi id="S3.SS3.p1.4.m4.1.1.2.2" xref="S3.SS3.p1.4.m4.1.1.2.2.cmml">F</mi><mtext id="S3.SS3.p1.4.m4.1.1.2.3" xref="S3.SS3.p1.4.m4.1.1.2.3a.cmml">aud</mtext><mi id="S3.SS3.p1.4.m4.1.1.3" xref="S3.SS3.p1.4.m4.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><apply id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">superscript</csymbol><apply id="S3.SS3.p1.4.m4.1.1.2.cmml" xref="S3.SS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.4.m4.1.1.2.1.cmml" xref="S3.SS3.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p1.4.m4.1.1.2.2.cmml" xref="S3.SS3.p1.4.m4.1.1.2.2">𝐹</ci><ci id="S3.SS3.p1.4.m4.1.1.2.3a.cmml" xref="S3.SS3.p1.4.m4.1.1.2.3"><mtext mathsize="70%" id="S3.SS3.p1.4.m4.1.1.2.3.cmml" xref="S3.SS3.p1.4.m4.1.1.2.3">aud</mtext></ci></apply><ci id="S3.SS3.p1.4.m4.1.1.3.cmml" xref="S3.SS3.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">F_{\text{aud}}^{i}</annotation></semantics></math> for each frame separately. Subsequently, we send them to TCN modules for processing to capture the temporal and spatial correlations between them. As shown in Equation <span id="S3.SS3.p1.4.1" class="ltx_text" style="color:#FF0000;">1</span>, we concatenate the extracted visual frame feature branch and the audio feature branch. Next, the processed features are fed into a Transformer encoder structure, which helps learn long-range dependencies between features and improves the performance and generalization ability of the model. The Transformer encoder consists of four encoder layers, each with a dropout rate of 0.4. The output of the encoder is then passed through a fully connected layer, a Batch Normalization layer, and another fully connected layer. Finally, a tanh activation function is used for prediction, yielding the Valence (V) or Arousal (A) values for each video frame. This design enhances the model’s expressive power and stability, enabling it to better adapt to various tasks and data distributions. When computing the loss, we utilize the CCCLoss as shown in Equation <span id="S3.SS3.p1.4.2" class="ltx_text" style="color:#FF0000;">2</span>.Due to the frame-wise prediction process, we adopt a smoothing strategy to enhance the stability of the model. We observed occasional omissions of frames in the cropped face images extracted from the videos. Since video frames are continuous, we replace the missing frames with the last available frame.</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="f=\text{concat}(\hat{f}_{1},\hat{f}_{2},\hat{f}_{3})" display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><mi id="S3.E1.m1.3.3.5" xref="S3.E1.m1.3.3.5.cmml">f</mi><mo id="S3.E1.m1.3.3.4" xref="S3.E1.m1.3.3.4.cmml">=</mo><mrow id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.3.cmml"><mtext id="S3.E1.m1.3.3.3.5" xref="S3.E1.m1.3.3.3.5a.cmml">concat</mtext><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.4" xref="S3.E1.m1.3.3.3.4.cmml">​</mo><mrow id="S3.E1.m1.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.4.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.3.3.3.4" xref="S3.E1.m1.3.3.3.3.4.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.cmml">f</mi><mo id="S3.E1.m1.1.1.1.1.1.1.2.1" xref="S3.E1.m1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mn id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.E1.m1.3.3.3.3.3.5" xref="S3.E1.m1.3.3.3.3.4.cmml">,</mo><msub id="S3.E1.m1.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.cmml"><mover accent="true" id="S3.E1.m1.2.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.2.2.cmml">f</mi><mo id="S3.E1.m1.2.2.2.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.2.2.2.1.cmml">^</mo></mover><mn id="S3.E1.m1.2.2.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.E1.m1.3.3.3.3.3.6" xref="S3.E1.m1.3.3.3.3.4.cmml">,</mo><msub id="S3.E1.m1.3.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.3.cmml"><mover accent="true" id="S3.E1.m1.3.3.3.3.3.3.2" xref="S3.E1.m1.3.3.3.3.3.3.2.cmml"><mi id="S3.E1.m1.3.3.3.3.3.3.2.2" xref="S3.E1.m1.3.3.3.3.3.3.2.2.cmml">f</mi><mo id="S3.E1.m1.3.3.3.3.3.3.2.1" xref="S3.E1.m1.3.3.3.3.3.3.2.1.cmml">^</mo></mover><mn id="S3.E1.m1.3.3.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.3.3.cmml">3</mn></msub><mo stretchy="false" id="S3.E1.m1.3.3.3.3.3.7" xref="S3.E1.m1.3.3.3.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><eq id="S3.E1.m1.3.3.4.cmml" xref="S3.E1.m1.3.3.4"></eq><ci id="S3.E1.m1.3.3.5.cmml" xref="S3.E1.m1.3.3.5">𝑓</ci><apply id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.3"><times id="S3.E1.m1.3.3.3.4.cmml" xref="S3.E1.m1.3.3.3.4"></times><ci id="S3.E1.m1.3.3.3.5a.cmml" xref="S3.E1.m1.3.3.3.5"><mtext id="S3.E1.m1.3.3.3.5.cmml" xref="S3.E1.m1.3.3.3.5">concat</mtext></ci><vector id="S3.E1.m1.3.3.3.3.4.cmml" xref="S3.E1.m1.3.3.3.3.3"><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2"><ci id="S3.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2">𝑓</ci></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.E1.m1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2">subscript</csymbol><apply id="S3.E1.m1.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2"><ci id="S3.E1.m1.2.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.1">^</ci><ci id="S3.E1.m1.2.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.2.2">𝑓</ci></apply><cn type="integer" id="S3.E1.m1.2.2.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.3">2</cn></apply><apply id="S3.E1.m1.3.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3">subscript</csymbol><apply id="S3.E1.m1.3.3.3.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3.2"><ci id="S3.E1.m1.3.3.3.3.3.3.2.1.cmml" xref="S3.E1.m1.3.3.3.3.3.3.2.1">^</ci><ci id="S3.E1.m1.3.3.3.3.3.3.2.2.cmml" xref="S3.E1.m1.3.3.3.3.3.3.2.2">𝑓</ci></apply><cn type="integer" id="S3.E1.m1.3.3.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3.3">3</cn></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">f=\text{concat}(\hat{f}_{1},\hat{f}_{2},\hat{f}_{3})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.4" class="ltx_Math" alttext="\text{CCCLoss}=1-CCC(\hat{v}_{\text{frame}_{i}},v_{\text{frame}_{i}})+1-CCC(\hat{a}_{\text{frame}_{i}},a_{\text{frame}_{i}})" display="block"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml"><mtext id="S3.E2.m1.4.4.6" xref="S3.E2.m1.4.4.6a.cmml">CCCLoss</mtext><mo id="S3.E2.m1.4.4.5" xref="S3.E2.m1.4.4.5.cmml">=</mo><mrow id="S3.E2.m1.4.4.4" xref="S3.E2.m1.4.4.4.cmml"><mrow id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml"><mrow id="S3.E2.m1.2.2.2.2.2" xref="S3.E2.m1.2.2.2.2.2.cmml"><mn id="S3.E2.m1.2.2.2.2.2.4" xref="S3.E2.m1.2.2.2.2.2.4.cmml">1</mn><mo id="S3.E2.m1.2.2.2.2.2.3" xref="S3.E2.m1.2.2.2.2.2.3.cmml">−</mo><mrow id="S3.E2.m1.2.2.2.2.2.2" xref="S3.E2.m1.2.2.2.2.2.2.cmml"><mi id="S3.E2.m1.2.2.2.2.2.2.4" xref="S3.E2.m1.2.2.2.2.2.2.4.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.2.2.3" xref="S3.E2.m1.2.2.2.2.2.2.3.cmml">​</mo><mi id="S3.E2.m1.2.2.2.2.2.2.5" xref="S3.E2.m1.2.2.2.2.2.2.5.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.2.2.3a" xref="S3.E2.m1.2.2.2.2.2.2.3.cmml">​</mo><mi id="S3.E2.m1.2.2.2.2.2.2.6" xref="S3.E2.m1.2.2.2.2.2.2.6.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.2.2.2.3b" xref="S3.E2.m1.2.2.2.2.2.2.3.cmml">​</mo><mrow id="S3.E2.m1.2.2.2.2.2.2.2.2" xref="S3.E2.m1.2.2.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.2.2.2.2.2.2.2.2.3" xref="S3.E2.m1.2.2.2.2.2.2.2.3.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">v</mi><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mtext id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2a.cmml">frame</mtext><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></msub><mo id="S3.E2.m1.2.2.2.2.2.2.2.2.4" xref="S3.E2.m1.2.2.2.2.2.2.2.3.cmml">,</mo><msub id="S3.E2.m1.2.2.2.2.2.2.2.2.2" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.cmml"><mi id="S3.E2.m1.2.2.2.2.2.2.2.2.2.2" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.2.cmml">v</mi><msub id="S3.E2.m1.2.2.2.2.2.2.2.2.2.3" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.3.cmml"><mtext id="S3.E2.m1.2.2.2.2.2.2.2.2.2.3.2" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.3.2a.cmml">frame</mtext><mi id="S3.E2.m1.2.2.2.2.2.2.2.2.2.3.3" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.3.3.cmml">i</mi></msub></msub><mo stretchy="false" id="S3.E2.m1.2.2.2.2.2.2.2.2.5" xref="S3.E2.m1.2.2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.2.2.2.2.3" xref="S3.E2.m1.2.2.2.2.3.cmml">+</mo><mn id="S3.E2.m1.2.2.2.2.4" xref="S3.E2.m1.2.2.2.2.4.cmml">1</mn></mrow><mo id="S3.E2.m1.4.4.4.5" xref="S3.E2.m1.4.4.4.5.cmml">−</mo><mrow id="S3.E2.m1.4.4.4.4" xref="S3.E2.m1.4.4.4.4.cmml"><mi id="S3.E2.m1.4.4.4.4.4" xref="S3.E2.m1.4.4.4.4.4.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.4.4.3" xref="S3.E2.m1.4.4.4.4.3.cmml">​</mo><mi id="S3.E2.m1.4.4.4.4.5" xref="S3.E2.m1.4.4.4.4.5.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.4.4.3a" xref="S3.E2.m1.4.4.4.4.3.cmml">​</mo><mi id="S3.E2.m1.4.4.4.4.6" xref="S3.E2.m1.4.4.4.4.6.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.4.4.3b" xref="S3.E2.m1.4.4.4.4.3.cmml">​</mo><mrow id="S3.E2.m1.4.4.4.4.2.2" xref="S3.E2.m1.4.4.4.4.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.4.4.2.2.3" xref="S3.E2.m1.4.4.4.4.2.3.cmml">(</mo><msub id="S3.E2.m1.3.3.3.3.1.1.1" xref="S3.E2.m1.3.3.3.3.1.1.1.cmml"><mover accent="true" id="S3.E2.m1.3.3.3.3.1.1.1.2" xref="S3.E2.m1.3.3.3.3.1.1.1.2.cmml"><mi id="S3.E2.m1.3.3.3.3.1.1.1.2.2" xref="S3.E2.m1.3.3.3.3.1.1.1.2.2.cmml">a</mi><mo id="S3.E2.m1.3.3.3.3.1.1.1.2.1" xref="S3.E2.m1.3.3.3.3.1.1.1.2.1.cmml">^</mo></mover><msub id="S3.E2.m1.3.3.3.3.1.1.1.3" xref="S3.E2.m1.3.3.3.3.1.1.1.3.cmml"><mtext id="S3.E2.m1.3.3.3.3.1.1.1.3.2" xref="S3.E2.m1.3.3.3.3.1.1.1.3.2a.cmml">frame</mtext><mi id="S3.E2.m1.3.3.3.3.1.1.1.3.3" xref="S3.E2.m1.3.3.3.3.1.1.1.3.3.cmml">i</mi></msub></msub><mo id="S3.E2.m1.4.4.4.4.2.2.4" xref="S3.E2.m1.4.4.4.4.2.3.cmml">,</mo><msub id="S3.E2.m1.4.4.4.4.2.2.2" xref="S3.E2.m1.4.4.4.4.2.2.2.cmml"><mi id="S3.E2.m1.4.4.4.4.2.2.2.2" xref="S3.E2.m1.4.4.4.4.2.2.2.2.cmml">a</mi><msub id="S3.E2.m1.4.4.4.4.2.2.2.3" xref="S3.E2.m1.4.4.4.4.2.2.2.3.cmml"><mtext id="S3.E2.m1.4.4.4.4.2.2.2.3.2" xref="S3.E2.m1.4.4.4.4.2.2.2.3.2a.cmml">frame</mtext><mi id="S3.E2.m1.4.4.4.4.2.2.2.3.3" xref="S3.E2.m1.4.4.4.4.2.2.2.3.3.cmml">i</mi></msub></msub><mo stretchy="false" id="S3.E2.m1.4.4.4.4.2.2.5" xref="S3.E2.m1.4.4.4.4.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4"><eq id="S3.E2.m1.4.4.5.cmml" xref="S3.E2.m1.4.4.5"></eq><ci id="S3.E2.m1.4.4.6a.cmml" xref="S3.E2.m1.4.4.6"><mtext id="S3.E2.m1.4.4.6.cmml" xref="S3.E2.m1.4.4.6">CCCLoss</mtext></ci><apply id="S3.E2.m1.4.4.4.cmml" xref="S3.E2.m1.4.4.4"><minus id="S3.E2.m1.4.4.4.5.cmml" xref="S3.E2.m1.4.4.4.5"></minus><apply id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2"><plus id="S3.E2.m1.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2.3"></plus><apply id="S3.E2.m1.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2"><minus id="S3.E2.m1.2.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2.2.3"></minus><cn type="integer" id="S3.E2.m1.2.2.2.2.2.4.cmml" xref="S3.E2.m1.2.2.2.2.2.4">1</cn><apply id="S3.E2.m1.2.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2"><times id="S3.E2.m1.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2.2.2.3"></times><ci id="S3.E2.m1.2.2.2.2.2.2.4.cmml" xref="S3.E2.m1.2.2.2.2.2.2.4">𝐶</ci><ci id="S3.E2.m1.2.2.2.2.2.2.5.cmml" xref="S3.E2.m1.2.2.2.2.2.2.5">𝐶</ci><ci id="S3.E2.m1.2.2.2.2.2.2.6.cmml" xref="S3.E2.m1.2.2.2.2.2.2.6">𝐶</ci><interval closure="open" id="S3.E2.m1.2.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2"><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2"><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2">𝑣</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2a.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2"><mtext mathsize="70%" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2">frame</mtext></ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply><apply id="S3.E2.m1.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.2.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.2.2.2.2.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.2">𝑣</ci><apply id="S3.E2.m1.2.2.2.2.2.2.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.2.2.2.2.2.2.3.1.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.3">subscript</csymbol><ci id="S3.E2.m1.2.2.2.2.2.2.2.2.2.3.2a.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.3.2"><mtext mathsize="70%" id="S3.E2.m1.2.2.2.2.2.2.2.2.2.3.2.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.3.2">frame</mtext></ci><ci id="S3.E2.m1.2.2.2.2.2.2.2.2.2.3.3.cmml" xref="S3.E2.m1.2.2.2.2.2.2.2.2.2.3.3">𝑖</ci></apply></apply></interval></apply></apply><cn type="integer" id="S3.E2.m1.2.2.2.2.4.cmml" xref="S3.E2.m1.2.2.2.2.4">1</cn></apply><apply id="S3.E2.m1.4.4.4.4.cmml" xref="S3.E2.m1.4.4.4.4"><times id="S3.E2.m1.4.4.4.4.3.cmml" xref="S3.E2.m1.4.4.4.4.3"></times><ci id="S3.E2.m1.4.4.4.4.4.cmml" xref="S3.E2.m1.4.4.4.4.4">𝐶</ci><ci id="S3.E2.m1.4.4.4.4.5.cmml" xref="S3.E2.m1.4.4.4.4.5">𝐶</ci><ci id="S3.E2.m1.4.4.4.4.6.cmml" xref="S3.E2.m1.4.4.4.4.6">𝐶</ci><interval closure="open" id="S3.E2.m1.4.4.4.4.2.3.cmml" xref="S3.E2.m1.4.4.4.4.2.2"><apply id="S3.E2.m1.3.3.3.3.1.1.1.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.3.1.1.1.1.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1">subscript</csymbol><apply id="S3.E2.m1.3.3.3.3.1.1.1.2.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.2"><ci id="S3.E2.m1.3.3.3.3.1.1.1.2.1.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.2.1">^</ci><ci id="S3.E2.m1.3.3.3.3.1.1.1.2.2.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.2.2">𝑎</ci></apply><apply id="S3.E2.m1.3.3.3.3.1.1.1.3.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.3.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.3.3.3.3.1.1.1.3.2a.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.3.2"><mtext mathsize="70%" id="S3.E2.m1.3.3.3.3.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.3.2">frame</mtext></ci><ci id="S3.E2.m1.3.3.3.3.1.1.1.3.3.cmml" xref="S3.E2.m1.3.3.3.3.1.1.1.3.3">𝑖</ci></apply></apply><apply id="S3.E2.m1.4.4.4.4.2.2.2.cmml" xref="S3.E2.m1.4.4.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.4.2.2.2.1.cmml" xref="S3.E2.m1.4.4.4.4.2.2.2">subscript</csymbol><ci id="S3.E2.m1.4.4.4.4.2.2.2.2.cmml" xref="S3.E2.m1.4.4.4.4.2.2.2.2">𝑎</ci><apply id="S3.E2.m1.4.4.4.4.2.2.2.3.cmml" xref="S3.E2.m1.4.4.4.4.2.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.4.4.2.2.2.3.1.cmml" xref="S3.E2.m1.4.4.4.4.2.2.2.3">subscript</csymbol><ci id="S3.E2.m1.4.4.4.4.2.2.2.3.2a.cmml" xref="S3.E2.m1.4.4.4.4.2.2.2.3.2"><mtext mathsize="70%" id="S3.E2.m1.4.4.4.4.2.2.2.3.2.cmml" xref="S3.E2.m1.4.4.4.4.2.2.2.3.2">frame</mtext></ci><ci id="S3.E2.m1.4.4.4.4.2.2.2.3.3.cmml" xref="S3.E2.m1.4.4.4.4.2.2.2.3.3">𝑖</ci></apply></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">\text{CCCLoss}=1-CCC(\hat{v}_{\text{frame}_{i}},v_{\text{frame}_{i}})+1-CCC(\hat{a}_{\text{frame}_{i}},a_{\text{frame}_{i}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>LA-SE Module</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">To better capture local image information and enhance channel selection and suppression, we propose the LA-SE network, which combines the LANet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> and SENet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>. In Figure <span id="S3.SS4.p1.1.1" class="ltx_text" style="color:#FF0000;">2</span>, LANet is employed to aggregate local information, accomplished through a sequence of two consecutive <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mrow id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mn id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS4.p1.1.m1.1.1.1" xref="S3.SS4.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><times id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">1</cn><cn type="integer" id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">1\times 1</annotation></semantics></math> convolutional layers. These layers consolidate spatial information into a unified channel, which is then subjected to scaling using a sigmoid function to produce spatial attention.In contrast, Figure <span id="S3.SS4.p1.1.2" class="ltx_text" style="color:#FF0000;">3</span> illustrates the utilization of SENet for channel-wise selection and suppression. SENet comprises two primary operations: squeeze and excitation.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">The squeeze operation compresses global channel information into a single-channel descriptor using global average pooling:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.2" class="ltx_Math" alttext="z_{t}=\frac{1}{w\times h}\sum_{i=1}^{w}\sum_{j=1}^{h}u_{t}(i,j)" display="block"><semantics id="S3.E3.m1.2a"><mrow id="S3.E3.m1.2.3" xref="S3.E3.m1.2.3.cmml"><msub id="S3.E3.m1.2.3.2" xref="S3.E3.m1.2.3.2.cmml"><mi id="S3.E3.m1.2.3.2.2" xref="S3.E3.m1.2.3.2.2.cmml">z</mi><mi id="S3.E3.m1.2.3.2.3" xref="S3.E3.m1.2.3.2.3.cmml">t</mi></msub><mo id="S3.E3.m1.2.3.1" xref="S3.E3.m1.2.3.1.cmml">=</mo><mrow id="S3.E3.m1.2.3.3" xref="S3.E3.m1.2.3.3.cmml"><mfrac id="S3.E3.m1.2.3.3.2" xref="S3.E3.m1.2.3.3.2.cmml"><mn id="S3.E3.m1.2.3.3.2.2" xref="S3.E3.m1.2.3.3.2.2.cmml">1</mn><mrow id="S3.E3.m1.2.3.3.2.3" xref="S3.E3.m1.2.3.3.2.3.cmml"><mi id="S3.E3.m1.2.3.3.2.3.2" xref="S3.E3.m1.2.3.3.2.3.2.cmml">w</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.2.3.3.2.3.1" xref="S3.E3.m1.2.3.3.2.3.1.cmml">×</mo><mi id="S3.E3.m1.2.3.3.2.3.3" xref="S3.E3.m1.2.3.3.2.3.3.cmml">h</mi></mrow></mfrac><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.3.3.1" xref="S3.E3.m1.2.3.3.1.cmml">​</mo><mrow id="S3.E3.m1.2.3.3.3" xref="S3.E3.m1.2.3.3.3.cmml"><munderover id="S3.E3.m1.2.3.3.3.1" xref="S3.E3.m1.2.3.3.3.1.cmml"><mo movablelimits="false" rspace="0em" id="S3.E3.m1.2.3.3.3.1.2.2" xref="S3.E3.m1.2.3.3.3.1.2.2.cmml">∑</mo><mrow id="S3.E3.m1.2.3.3.3.1.2.3" xref="S3.E3.m1.2.3.3.3.1.2.3.cmml"><mi id="S3.E3.m1.2.3.3.3.1.2.3.2" xref="S3.E3.m1.2.3.3.3.1.2.3.2.cmml">i</mi><mo id="S3.E3.m1.2.3.3.3.1.2.3.1" xref="S3.E3.m1.2.3.3.3.1.2.3.1.cmml">=</mo><mn id="S3.E3.m1.2.3.3.3.1.2.3.3" xref="S3.E3.m1.2.3.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E3.m1.2.3.3.3.1.3" xref="S3.E3.m1.2.3.3.3.1.3.cmml">w</mi></munderover><mrow id="S3.E3.m1.2.3.3.3.2" xref="S3.E3.m1.2.3.3.3.2.cmml"><munderover id="S3.E3.m1.2.3.3.3.2.1" xref="S3.E3.m1.2.3.3.3.2.1.cmml"><mo movablelimits="false" id="S3.E3.m1.2.3.3.3.2.1.2.2" xref="S3.E3.m1.2.3.3.3.2.1.2.2.cmml">∑</mo><mrow id="S3.E3.m1.2.3.3.3.2.1.2.3" xref="S3.E3.m1.2.3.3.3.2.1.2.3.cmml"><mi id="S3.E3.m1.2.3.3.3.2.1.2.3.2" xref="S3.E3.m1.2.3.3.3.2.1.2.3.2.cmml">j</mi><mo id="S3.E3.m1.2.3.3.3.2.1.2.3.1" xref="S3.E3.m1.2.3.3.3.2.1.2.3.1.cmml">=</mo><mn id="S3.E3.m1.2.3.3.3.2.1.2.3.3" xref="S3.E3.m1.2.3.3.3.2.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E3.m1.2.3.3.3.2.1.3" xref="S3.E3.m1.2.3.3.3.2.1.3.cmml">h</mi></munderover><mrow id="S3.E3.m1.2.3.3.3.2.2" xref="S3.E3.m1.2.3.3.3.2.2.cmml"><msub id="S3.E3.m1.2.3.3.3.2.2.2" xref="S3.E3.m1.2.3.3.3.2.2.2.cmml"><mi id="S3.E3.m1.2.3.3.3.2.2.2.2" xref="S3.E3.m1.2.3.3.3.2.2.2.2.cmml">u</mi><mi id="S3.E3.m1.2.3.3.3.2.2.2.3" xref="S3.E3.m1.2.3.3.3.2.2.2.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.2.3.3.3.2.2.1" xref="S3.E3.m1.2.3.3.3.2.2.1.cmml">​</mo><mrow id="S3.E3.m1.2.3.3.3.2.2.3.2" xref="S3.E3.m1.2.3.3.3.2.2.3.1.cmml"><mo stretchy="false" id="S3.E3.m1.2.3.3.3.2.2.3.2.1" xref="S3.E3.m1.2.3.3.3.2.2.3.1.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">i</mi><mo id="S3.E3.m1.2.3.3.3.2.2.3.2.2" xref="S3.E3.m1.2.3.3.3.2.2.3.1.cmml">,</mo><mi id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml">j</mi><mo stretchy="false" id="S3.E3.m1.2.3.3.3.2.2.3.2.3" xref="S3.E3.m1.2.3.3.3.2.2.3.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.2b"><apply id="S3.E3.m1.2.3.cmml" xref="S3.E3.m1.2.3"><eq id="S3.E3.m1.2.3.1.cmml" xref="S3.E3.m1.2.3.1"></eq><apply id="S3.E3.m1.2.3.2.cmml" xref="S3.E3.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.3.2.1.cmml" xref="S3.E3.m1.2.3.2">subscript</csymbol><ci id="S3.E3.m1.2.3.2.2.cmml" xref="S3.E3.m1.2.3.2.2">𝑧</ci><ci id="S3.E3.m1.2.3.2.3.cmml" xref="S3.E3.m1.2.3.2.3">𝑡</ci></apply><apply id="S3.E3.m1.2.3.3.cmml" xref="S3.E3.m1.2.3.3"><times id="S3.E3.m1.2.3.3.1.cmml" xref="S3.E3.m1.2.3.3.1"></times><apply id="S3.E3.m1.2.3.3.2.cmml" xref="S3.E3.m1.2.3.3.2"><divide id="S3.E3.m1.2.3.3.2.1.cmml" xref="S3.E3.m1.2.3.3.2"></divide><cn type="integer" id="S3.E3.m1.2.3.3.2.2.cmml" xref="S3.E3.m1.2.3.3.2.2">1</cn><apply id="S3.E3.m1.2.3.3.2.3.cmml" xref="S3.E3.m1.2.3.3.2.3"><times id="S3.E3.m1.2.3.3.2.3.1.cmml" xref="S3.E3.m1.2.3.3.2.3.1"></times><ci id="S3.E3.m1.2.3.3.2.3.2.cmml" xref="S3.E3.m1.2.3.3.2.3.2">𝑤</ci><ci id="S3.E3.m1.2.3.3.2.3.3.cmml" xref="S3.E3.m1.2.3.3.2.3.3">ℎ</ci></apply></apply><apply id="S3.E3.m1.2.3.3.3.cmml" xref="S3.E3.m1.2.3.3.3"><apply id="S3.E3.m1.2.3.3.3.1.cmml" xref="S3.E3.m1.2.3.3.3.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.3.3.3.1.1.cmml" xref="S3.E3.m1.2.3.3.3.1">superscript</csymbol><apply id="S3.E3.m1.2.3.3.3.1.2.cmml" xref="S3.E3.m1.2.3.3.3.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.3.3.3.1.2.1.cmml" xref="S3.E3.m1.2.3.3.3.1">subscript</csymbol><sum id="S3.E3.m1.2.3.3.3.1.2.2.cmml" xref="S3.E3.m1.2.3.3.3.1.2.2"></sum><apply id="S3.E3.m1.2.3.3.3.1.2.3.cmml" xref="S3.E3.m1.2.3.3.3.1.2.3"><eq id="S3.E3.m1.2.3.3.3.1.2.3.1.cmml" xref="S3.E3.m1.2.3.3.3.1.2.3.1"></eq><ci id="S3.E3.m1.2.3.3.3.1.2.3.2.cmml" xref="S3.E3.m1.2.3.3.3.1.2.3.2">𝑖</ci><cn type="integer" id="S3.E3.m1.2.3.3.3.1.2.3.3.cmml" xref="S3.E3.m1.2.3.3.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E3.m1.2.3.3.3.1.3.cmml" xref="S3.E3.m1.2.3.3.3.1.3">𝑤</ci></apply><apply id="S3.E3.m1.2.3.3.3.2.cmml" xref="S3.E3.m1.2.3.3.3.2"><apply id="S3.E3.m1.2.3.3.3.2.1.cmml" xref="S3.E3.m1.2.3.3.3.2.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.3.3.3.2.1.1.cmml" xref="S3.E3.m1.2.3.3.3.2.1">superscript</csymbol><apply id="S3.E3.m1.2.3.3.3.2.1.2.cmml" xref="S3.E3.m1.2.3.3.3.2.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.3.3.3.2.1.2.1.cmml" xref="S3.E3.m1.2.3.3.3.2.1">subscript</csymbol><sum id="S3.E3.m1.2.3.3.3.2.1.2.2.cmml" xref="S3.E3.m1.2.3.3.3.2.1.2.2"></sum><apply id="S3.E3.m1.2.3.3.3.2.1.2.3.cmml" xref="S3.E3.m1.2.3.3.3.2.1.2.3"><eq id="S3.E3.m1.2.3.3.3.2.1.2.3.1.cmml" xref="S3.E3.m1.2.3.3.3.2.1.2.3.1"></eq><ci id="S3.E3.m1.2.3.3.3.2.1.2.3.2.cmml" xref="S3.E3.m1.2.3.3.3.2.1.2.3.2">𝑗</ci><cn type="integer" id="S3.E3.m1.2.3.3.3.2.1.2.3.3.cmml" xref="S3.E3.m1.2.3.3.3.2.1.2.3.3">1</cn></apply></apply><ci id="S3.E3.m1.2.3.3.3.2.1.3.cmml" xref="S3.E3.m1.2.3.3.3.2.1.3">ℎ</ci></apply><apply id="S3.E3.m1.2.3.3.3.2.2.cmml" xref="S3.E3.m1.2.3.3.3.2.2"><times id="S3.E3.m1.2.3.3.3.2.2.1.cmml" xref="S3.E3.m1.2.3.3.3.2.2.1"></times><apply id="S3.E3.m1.2.3.3.3.2.2.2.cmml" xref="S3.E3.m1.2.3.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.2.3.3.3.2.2.2.1.cmml" xref="S3.E3.m1.2.3.3.3.2.2.2">subscript</csymbol><ci id="S3.E3.m1.2.3.3.3.2.2.2.2.cmml" xref="S3.E3.m1.2.3.3.3.2.2.2.2">𝑢</ci><ci id="S3.E3.m1.2.3.3.3.2.2.2.3.cmml" xref="S3.E3.m1.2.3.3.3.2.2.2.3">𝑡</ci></apply><interval closure="open" id="S3.E3.m1.2.3.3.3.2.2.3.1.cmml" xref="S3.E3.m1.2.3.3.3.2.2.3.2"><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝑖</ci><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">𝑗</ci></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.2c">z_{t}=\frac{1}{w\times h}\sum_{i=1}^{w}\sum_{j=1}^{h}u_{t}(i,j)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.5" class="ltx_p">where <math id="S3.SS4.p3.1.m1.1" class="ltx_Math" alttext="z_{t}" display="inline"><semantics id="S3.SS4.p3.1.m1.1a"><msub id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml"><mi id="S3.SS4.p3.1.m1.1.1.2" xref="S3.SS4.p3.1.m1.1.1.2.cmml">z</mi><mi id="S3.SS4.p3.1.m1.1.1.3" xref="S3.SS4.p3.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><apply id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.1.m1.1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p3.1.m1.1.1.2.cmml" xref="S3.SS4.p3.1.m1.1.1.2">𝑧</ci><ci id="S3.SS4.p3.1.m1.1.1.3.cmml" xref="S3.SS4.p3.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">z_{t}</annotation></semantics></math> represents the signal of channel <math id="S3.SS4.p3.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS4.p3.2.m2.1a"><mi id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><ci id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">t</annotation></semantics></math> and <math id="S3.SS4.p3.3.m3.2" class="ltx_Math" alttext="u_{t}(i,j)" display="inline"><semantics id="S3.SS4.p3.3.m3.2a"><mrow id="S3.SS4.p3.3.m3.2.3" xref="S3.SS4.p3.3.m3.2.3.cmml"><msub id="S3.SS4.p3.3.m3.2.3.2" xref="S3.SS4.p3.3.m3.2.3.2.cmml"><mi id="S3.SS4.p3.3.m3.2.3.2.2" xref="S3.SS4.p3.3.m3.2.3.2.2.cmml">u</mi><mi id="S3.SS4.p3.3.m3.2.3.2.3" xref="S3.SS4.p3.3.m3.2.3.2.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS4.p3.3.m3.2.3.1" xref="S3.SS4.p3.3.m3.2.3.1.cmml">​</mo><mrow id="S3.SS4.p3.3.m3.2.3.3.2" xref="S3.SS4.p3.3.m3.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS4.p3.3.m3.2.3.3.2.1" xref="S3.SS4.p3.3.m3.2.3.3.1.cmml">(</mo><mi id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml">i</mi><mo id="S3.SS4.p3.3.m3.2.3.3.2.2" xref="S3.SS4.p3.3.m3.2.3.3.1.cmml">,</mo><mi id="S3.SS4.p3.3.m3.2.2" xref="S3.SS4.p3.3.m3.2.2.cmml">j</mi><mo stretchy="false" id="S3.SS4.p3.3.m3.2.3.3.2.3" xref="S3.SS4.p3.3.m3.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.2b"><apply id="S3.SS4.p3.3.m3.2.3.cmml" xref="S3.SS4.p3.3.m3.2.3"><times id="S3.SS4.p3.3.m3.2.3.1.cmml" xref="S3.SS4.p3.3.m3.2.3.1"></times><apply id="S3.SS4.p3.3.m3.2.3.2.cmml" xref="S3.SS4.p3.3.m3.2.3.2"><csymbol cd="ambiguous" id="S3.SS4.p3.3.m3.2.3.2.1.cmml" xref="S3.SS4.p3.3.m3.2.3.2">subscript</csymbol><ci id="S3.SS4.p3.3.m3.2.3.2.2.cmml" xref="S3.SS4.p3.3.m3.2.3.2.2">𝑢</ci><ci id="S3.SS4.p3.3.m3.2.3.2.3.cmml" xref="S3.SS4.p3.3.m3.2.3.2.3">𝑡</ci></apply><interval closure="open" id="S3.SS4.p3.3.m3.2.3.3.1.cmml" xref="S3.SS4.p3.3.m3.2.3.3.2"><ci id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1">𝑖</ci><ci id="S3.SS4.p3.3.m3.2.2.cmml" xref="S3.SS4.p3.3.m3.2.2">𝑗</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.2c">u_{t}(i,j)</annotation></semantics></math> denotes the element of channel <math id="S3.SS4.p3.4.m4.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS4.p3.4.m4.1a"><mi id="S3.SS4.p3.4.m4.1.1" xref="S3.SS4.p3.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.4.m4.1b"><ci id="S3.SS4.p3.4.m4.1.1.cmml" xref="S3.SS4.p3.4.m4.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.4.m4.1c">t</annotation></semantics></math> at position <math id="S3.SS4.p3.5.m5.2" class="ltx_Math" alttext="(i,j)" display="inline"><semantics id="S3.SS4.p3.5.m5.2a"><mrow id="S3.SS4.p3.5.m5.2.3.2" xref="S3.SS4.p3.5.m5.2.3.1.cmml"><mo stretchy="false" id="S3.SS4.p3.5.m5.2.3.2.1" xref="S3.SS4.p3.5.m5.2.3.1.cmml">(</mo><mi id="S3.SS4.p3.5.m5.1.1" xref="S3.SS4.p3.5.m5.1.1.cmml">i</mi><mo id="S3.SS4.p3.5.m5.2.3.2.2" xref="S3.SS4.p3.5.m5.2.3.1.cmml">,</mo><mi id="S3.SS4.p3.5.m5.2.2" xref="S3.SS4.p3.5.m5.2.2.cmml">j</mi><mo stretchy="false" id="S3.SS4.p3.5.m5.2.3.2.3" xref="S3.SS4.p3.5.m5.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.5.m5.2b"><interval closure="open" id="S3.SS4.p3.5.m5.2.3.1.cmml" xref="S3.SS4.p3.5.m5.2.3.2"><ci id="S3.SS4.p3.5.m5.1.1.cmml" xref="S3.SS4.p3.5.m5.1.1">𝑖</ci><ci id="S3.SS4.p3.5.m5.2.2.cmml" xref="S3.SS4.p3.5.m5.2.2">𝑗</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.5.m5.2c">(i,j)</annotation></semantics></math>. The excitation operation models channel-wise dependencies through two fully connected (FC) layers:</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="s=\sigma\left(w_{2}\cdot g(w_{1}\cdot z)\right)" display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mi id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml">s</mi><mo id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.3" xref="S3.E4.m1.1.1.1.3.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.3.cmml"><msub id="S3.E4.m1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.3.2.2" xref="S3.E4.m1.1.1.1.1.1.1.3.2.2.cmml">w</mi><mn id="S3.E4.m1.1.1.1.1.1.1.3.2.3" xref="S3.E4.m1.1.1.1.1.1.1.3.2.3.cmml">2</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.1.1.1.1.1.1.3.1" xref="S3.E4.m1.1.1.1.1.1.1.3.1.cmml">⋅</mo><mi id="S3.E4.m1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.1.3.3.cmml">g</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">w</mi><mn id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">1</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml">⋅</mo><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.cmml">z</mi></mrow><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><eq id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2"></eq><ci id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3">𝑠</ci><apply id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><times id="S3.E4.m1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.2"></times><ci id="S3.E4.m1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.3">𝜎</ci><apply id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"><times id="S3.E4.m1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2"></times><apply id="S3.E4.m1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3"><ci id="S3.E4.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.1">⋅</ci><apply id="S3.E4.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.2.2">𝑤</ci><cn type="integer" id="S3.E4.m1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.2.3">2</cn></apply><ci id="S3.E4.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.3">𝑔</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1"><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1">⋅</ci><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2">𝑤</ci><cn type="integer" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3">1</cn></apply><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3">𝑧</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">s=\sigma\left(w_{2}\cdot g(w_{1}\cdot z)\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p3.9" class="ltx_p">where <math id="S3.SS4.p3.6.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS4.p3.6.m1.1a"><mi id="S3.SS4.p3.6.m1.1.1" xref="S3.SS4.p3.6.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.6.m1.1b"><ci id="S3.SS4.p3.6.m1.1.1.cmml" xref="S3.SS4.p3.6.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.6.m1.1c">\sigma</annotation></semantics></math> refers to the sigmoid function, <math id="S3.SS4.p3.7.m2.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S3.SS4.p3.7.m2.1a"><mi id="S3.SS4.p3.7.m2.1.1" xref="S3.SS4.p3.7.m2.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.7.m2.1b"><ci id="S3.SS4.p3.7.m2.1.1.cmml" xref="S3.SS4.p3.7.m2.1.1">𝑔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.7.m2.1c">g</annotation></semantics></math> is the ReLU function, and <math id="S3.SS4.p3.8.m3.1" class="ltx_Math" alttext="w_{1}" display="inline"><semantics id="S3.SS4.p3.8.m3.1a"><msub id="S3.SS4.p3.8.m3.1.1" xref="S3.SS4.p3.8.m3.1.1.cmml"><mi id="S3.SS4.p3.8.m3.1.1.2" xref="S3.SS4.p3.8.m3.1.1.2.cmml">w</mi><mn id="S3.SS4.p3.8.m3.1.1.3" xref="S3.SS4.p3.8.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.8.m3.1b"><apply id="S3.SS4.p3.8.m3.1.1.cmml" xref="S3.SS4.p3.8.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.8.m3.1.1.1.cmml" xref="S3.SS4.p3.8.m3.1.1">subscript</csymbol><ci id="S3.SS4.p3.8.m3.1.1.2.cmml" xref="S3.SS4.p3.8.m3.1.1.2">𝑤</ci><cn type="integer" id="S3.SS4.p3.8.m3.1.1.3.cmml" xref="S3.SS4.p3.8.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.8.m3.1c">w_{1}</annotation></semantics></math> and <math id="S3.SS4.p3.9.m4.1" class="ltx_Math" alttext="w_{2}" display="inline"><semantics id="S3.SS4.p3.9.m4.1a"><msub id="S3.SS4.p3.9.m4.1.1" xref="S3.SS4.p3.9.m4.1.1.cmml"><mi id="S3.SS4.p3.9.m4.1.1.2" xref="S3.SS4.p3.9.m4.1.1.2.cmml">w</mi><mn id="S3.SS4.p3.9.m4.1.1.3" xref="S3.SS4.p3.9.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.9.m4.1b"><apply id="S3.SS4.p3.9.m4.1.1.cmml" xref="S3.SS4.p3.9.m4.1.1"><csymbol cd="ambiguous" id="S3.SS4.p3.9.m4.1.1.1.cmml" xref="S3.SS4.p3.9.m4.1.1">subscript</csymbol><ci id="S3.SS4.p3.9.m4.1.1.2.cmml" xref="S3.SS4.p3.9.m4.1.1.2">𝑤</ci><cn type="integer" id="S3.SS4.p3.9.m4.1.1.3.cmml" xref="S3.SS4.p3.9.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.9.m4.1c">w_{2}</annotation></semantics></math> are the weights of the FC layers.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.5" class="ltx_p">Finally, the scaling operation adjusts each channel dynamically based on learned activations:</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.1" class="ltx_Math" alttext="x_{i}=s_{i}\times u_{i}" display="block"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><msub id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml"><mi id="S3.E5.m1.1.1.2.2" xref="S3.E5.m1.1.1.2.2.cmml">x</mi><mi id="S3.E5.m1.1.1.2.3" xref="S3.E5.m1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.cmml">=</mo><mrow id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3.cmml"><msub id="S3.E5.m1.1.1.3.2" xref="S3.E5.m1.1.1.3.2.cmml"><mi id="S3.E5.m1.1.1.3.2.2" xref="S3.E5.m1.1.1.3.2.2.cmml">s</mi><mi id="S3.E5.m1.1.1.3.2.3" xref="S3.E5.m1.1.1.3.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E5.m1.1.1.3.1" xref="S3.E5.m1.1.1.3.1.cmml">×</mo><msub id="S3.E5.m1.1.1.3.3" xref="S3.E5.m1.1.1.3.3.cmml"><mi id="S3.E5.m1.1.1.3.3.2" xref="S3.E5.m1.1.1.3.3.2.cmml">u</mi><mi id="S3.E5.m1.1.1.3.3.3" xref="S3.E5.m1.1.1.3.3.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"><eq id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"></eq><apply id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.2.1.cmml" xref="S3.E5.m1.1.1.2">subscript</csymbol><ci id="S3.E5.m1.1.1.2.2.cmml" xref="S3.E5.m1.1.1.2.2">𝑥</ci><ci id="S3.E5.m1.1.1.2.3.cmml" xref="S3.E5.m1.1.1.2.3">𝑖</ci></apply><apply id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3"><times id="S3.E5.m1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.3.1"></times><apply id="S3.E5.m1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.3.2.1.cmml" xref="S3.E5.m1.1.1.3.2">subscript</csymbol><ci id="S3.E5.m1.1.1.3.2.2.cmml" xref="S3.E5.m1.1.1.3.2.2">𝑠</ci><ci id="S3.E5.m1.1.1.3.2.3.cmml" xref="S3.E5.m1.1.1.3.2.3">𝑖</ci></apply><apply id="S3.E5.m1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.3.3.1.cmml" xref="S3.E5.m1.1.1.3.3">subscript</csymbol><ci id="S3.E5.m1.1.1.3.3.2.cmml" xref="S3.E5.m1.1.1.3.3.2">𝑢</ci><ci id="S3.E5.m1.1.1.3.3.3.cmml" xref="S3.E5.m1.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">x_{i}=s_{i}\times u_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p4.4" class="ltx_p">where <math id="S3.SS4.p4.1.m1.1" class="ltx_Math" alttext="s_{i}" display="inline"><semantics id="S3.SS4.p4.1.m1.1a"><msub id="S3.SS4.p4.1.m1.1.1" xref="S3.SS4.p4.1.m1.1.1.cmml"><mi id="S3.SS4.p4.1.m1.1.1.2" xref="S3.SS4.p4.1.m1.1.1.2.cmml">s</mi><mi id="S3.SS4.p4.1.m1.1.1.3" xref="S3.SS4.p4.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.1.m1.1b"><apply id="S3.SS4.p4.1.m1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.1.m1.1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p4.1.m1.1.1.2.cmml" xref="S3.SS4.p4.1.m1.1.1.2">𝑠</ci><ci id="S3.SS4.p4.1.m1.1.1.3.cmml" xref="S3.SS4.p4.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.1.m1.1c">s_{i}</annotation></semantics></math> represents a scalar related to channel <math id="S3.SS4.p4.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS4.p4.2.m2.1a"><mi id="S3.SS4.p4.2.m2.1.1" xref="S3.SS4.p4.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.2.m2.1b"><ci id="S3.SS4.p4.2.m2.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.2.m2.1c">i</annotation></semantics></math>, and <math id="S3.SS4.p4.3.m3.1" class="ltx_Math" alttext="u_{i}" display="inline"><semantics id="S3.SS4.p4.3.m3.1a"><msub id="S3.SS4.p4.3.m3.1.1" xref="S3.SS4.p4.3.m3.1.1.cmml"><mi id="S3.SS4.p4.3.m3.1.1.2" xref="S3.SS4.p4.3.m3.1.1.2.cmml">u</mi><mi id="S3.SS4.p4.3.m3.1.1.3" xref="S3.SS4.p4.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.3.m3.1b"><apply id="S3.SS4.p4.3.m3.1.1.cmml" xref="S3.SS4.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS4.p4.3.m3.1.1.1.cmml" xref="S3.SS4.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS4.p4.3.m3.1.1.2.cmml" xref="S3.SS4.p4.3.m3.1.1.2">𝑢</ci><ci id="S3.SS4.p4.3.m3.1.1.3.cmml" xref="S3.SS4.p4.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.3.m3.1c">u_{i}</annotation></semantics></math> is the input for channel <math id="S3.SS4.p4.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS4.p4.4.m4.1a"><mi id="S3.SS4.p4.4.m4.1.1" xref="S3.SS4.p4.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.4.m4.1b"><ci id="S3.SS4.p4.4.m4.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.4.m4.1c">i</annotation></semantics></math>.</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<p id="S3.SS4.p5.1" class="ltx_p">The LA-SE network integrates the advantages of LANet and SENet to extract more representative and robust features for image understanding tasks, thereby improving model performance and generalization capability.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>TCN Module</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">The Temporal Convolutional Network (TCN) module is utilized to capture temporal dependencies between frames and features. It consists of a stack of dilated convolutional layers followed by activation functions and pooling operations. The dilated convolutional layers have exponentially increasing dilation rates, enabling them to capture information from a wider range of temporal contexts.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.3" class="ltx_p">Formally, given an input sequence <math id="S3.SS5.p2.1.m1.4" class="ltx_Math" alttext="X=\{x_{1},x_{2},...,x_{T}\}" display="inline"><semantics id="S3.SS5.p2.1.m1.4a"><mrow id="S3.SS5.p2.1.m1.4.4" xref="S3.SS5.p2.1.m1.4.4.cmml"><mi id="S3.SS5.p2.1.m1.4.4.5" xref="S3.SS5.p2.1.m1.4.4.5.cmml">X</mi><mo id="S3.SS5.p2.1.m1.4.4.4" xref="S3.SS5.p2.1.m1.4.4.4.cmml">=</mo><mrow id="S3.SS5.p2.1.m1.4.4.3.3" xref="S3.SS5.p2.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS5.p2.1.m1.4.4.3.3.4" xref="S3.SS5.p2.1.m1.4.4.3.4.cmml">{</mo><msub id="S3.SS5.p2.1.m1.2.2.1.1.1" xref="S3.SS5.p2.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS5.p2.1.m1.2.2.1.1.1.2" xref="S3.SS5.p2.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S3.SS5.p2.1.m1.2.2.1.1.1.3" xref="S3.SS5.p2.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS5.p2.1.m1.4.4.3.3.5" xref="S3.SS5.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS5.p2.1.m1.3.3.2.2.2" xref="S3.SS5.p2.1.m1.3.3.2.2.2.cmml"><mi id="S3.SS5.p2.1.m1.3.3.2.2.2.2" xref="S3.SS5.p2.1.m1.3.3.2.2.2.2.cmml">x</mi><mn id="S3.SS5.p2.1.m1.3.3.2.2.2.3" xref="S3.SS5.p2.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS5.p2.1.m1.4.4.3.3.6" xref="S3.SS5.p2.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml">…</mi><mo id="S3.SS5.p2.1.m1.4.4.3.3.7" xref="S3.SS5.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS5.p2.1.m1.4.4.3.3.3" xref="S3.SS5.p2.1.m1.4.4.3.3.3.cmml"><mi id="S3.SS5.p2.1.m1.4.4.3.3.3.2" xref="S3.SS5.p2.1.m1.4.4.3.3.3.2.cmml">x</mi><mi id="S3.SS5.p2.1.m1.4.4.3.3.3.3" xref="S3.SS5.p2.1.m1.4.4.3.3.3.3.cmml">T</mi></msub><mo stretchy="false" id="S3.SS5.p2.1.m1.4.4.3.3.8" xref="S3.SS5.p2.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.4b"><apply id="S3.SS5.p2.1.m1.4.4.cmml" xref="S3.SS5.p2.1.m1.4.4"><eq id="S3.SS5.p2.1.m1.4.4.4.cmml" xref="S3.SS5.p2.1.m1.4.4.4"></eq><ci id="S3.SS5.p2.1.m1.4.4.5.cmml" xref="S3.SS5.p2.1.m1.4.4.5">𝑋</ci><set id="S3.SS5.p2.1.m1.4.4.3.4.cmml" xref="S3.SS5.p2.1.m1.4.4.3.3"><apply id="S3.SS5.p2.1.m1.2.2.1.1.1.cmml" xref="S3.SS5.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS5.p2.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS5.p2.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS5.p2.1.m1.2.2.1.1.1.2">𝑥</ci><cn type="integer" id="S3.SS5.p2.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS5.p2.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS5.p2.1.m1.3.3.2.2.2.cmml" xref="S3.SS5.p2.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS5.p2.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS5.p2.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.SS5.p2.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS5.p2.1.m1.3.3.2.2.2.2">𝑥</ci><cn type="integer" id="S3.SS5.p2.1.m1.3.3.2.2.2.3.cmml" xref="S3.SS5.p2.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">…</ci><apply id="S3.SS5.p2.1.m1.4.4.3.3.3.cmml" xref="S3.SS5.p2.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS5.p2.1.m1.4.4.3.3.3.1.cmml" xref="S3.SS5.p2.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.SS5.p2.1.m1.4.4.3.3.3.2.cmml" xref="S3.SS5.p2.1.m1.4.4.3.3.3.2">𝑥</ci><ci id="S3.SS5.p2.1.m1.4.4.3.3.3.3.cmml" xref="S3.SS5.p2.1.m1.4.4.3.3.3.3">𝑇</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.4c">X=\{x_{1},x_{2},...,x_{T}\}</annotation></semantics></math>, where <math id="S3.SS5.p2.2.m2.1" class="ltx_Math" alttext="x_{t}" display="inline"><semantics id="S3.SS5.p2.2.m2.1a"><msub id="S3.SS5.p2.2.m2.1.1" xref="S3.SS5.p2.2.m2.1.1.cmml"><mi id="S3.SS5.p2.2.m2.1.1.2" xref="S3.SS5.p2.2.m2.1.1.2.cmml">x</mi><mi id="S3.SS5.p2.2.m2.1.1.3" xref="S3.SS5.p2.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.2.m2.1b"><apply id="S3.SS5.p2.2.m2.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p2.2.m2.1.1.1.cmml" xref="S3.SS5.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS5.p2.2.m2.1.1.2.cmml" xref="S3.SS5.p2.2.m2.1.1.2">𝑥</ci><ci id="S3.SS5.p2.2.m2.1.1.3.cmml" xref="S3.SS5.p2.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.2.m2.1c">x_{t}</annotation></semantics></math> represents the feature representation at time step <math id="S3.SS5.p2.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS5.p2.3.m3.1a"><mi id="S3.SS5.p2.3.m3.1.1" xref="S3.SS5.p2.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.3.m3.1b"><ci id="S3.SS5.p2.3.m3.1.1.cmml" xref="S3.SS5.p2.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.3.m3.1c">t</annotation></semantics></math>, the output of the TCN module can be calculated as follows:</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.2" class="ltx_Math" alttext="y_{t}=\text{ReLU}\left(\sum_{i=1}^{N}W_{i}*x_{t+(i-1)d}\right)" display="block"><semantics id="S3.E6.m1.2a"><mrow id="S3.E6.m1.2.2" xref="S3.E6.m1.2.2.cmml"><msub id="S3.E6.m1.2.2.3" xref="S3.E6.m1.2.2.3.cmml"><mi id="S3.E6.m1.2.2.3.2" xref="S3.E6.m1.2.2.3.2.cmml">y</mi><mi id="S3.E6.m1.2.2.3.3" xref="S3.E6.m1.2.2.3.3.cmml">t</mi></msub><mo id="S3.E6.m1.2.2.2" xref="S3.E6.m1.2.2.2.cmml">=</mo><mrow id="S3.E6.m1.2.2.1" xref="S3.E6.m1.2.2.1.cmml"><mtext id="S3.E6.m1.2.2.1.3" xref="S3.E6.m1.2.2.1.3a.cmml">ReLU</mtext><mo lspace="0em" rspace="0em" id="S3.E6.m1.2.2.1.2" xref="S3.E6.m1.2.2.1.2.cmml">​</mo><mrow id="S3.E6.m1.2.2.1.1.1" xref="S3.E6.m1.2.2.1.1.1.1.cmml"><mo id="S3.E6.m1.2.2.1.1.1.2" xref="S3.E6.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.2.2.1.1.1.1" xref="S3.E6.m1.2.2.1.1.1.1.cmml"><munderover id="S3.E6.m1.2.2.1.1.1.1.1" xref="S3.E6.m1.2.2.1.1.1.1.1.cmml"><mo lspace="0em" movablelimits="false" id="S3.E6.m1.2.2.1.1.1.1.1.2.2" xref="S3.E6.m1.2.2.1.1.1.1.1.2.2.cmml">∑</mo><mrow id="S3.E6.m1.2.2.1.1.1.1.1.2.3" xref="S3.E6.m1.2.2.1.1.1.1.1.2.3.cmml"><mi id="S3.E6.m1.2.2.1.1.1.1.1.2.3.2" xref="S3.E6.m1.2.2.1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S3.E6.m1.2.2.1.1.1.1.1.2.3.1" xref="S3.E6.m1.2.2.1.1.1.1.1.2.3.1.cmml">=</mo><mn id="S3.E6.m1.2.2.1.1.1.1.1.2.3.3" xref="S3.E6.m1.2.2.1.1.1.1.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E6.m1.2.2.1.1.1.1.1.3" xref="S3.E6.m1.2.2.1.1.1.1.1.3.cmml">N</mi></munderover><mrow id="S3.E6.m1.2.2.1.1.1.1.2" xref="S3.E6.m1.2.2.1.1.1.1.2.cmml"><msub id="S3.E6.m1.2.2.1.1.1.1.2.2" xref="S3.E6.m1.2.2.1.1.1.1.2.2.cmml"><mi id="S3.E6.m1.2.2.1.1.1.1.2.2.2" xref="S3.E6.m1.2.2.1.1.1.1.2.2.2.cmml">W</mi><mi id="S3.E6.m1.2.2.1.1.1.1.2.2.3" xref="S3.E6.m1.2.2.1.1.1.1.2.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E6.m1.2.2.1.1.1.1.2.1" xref="S3.E6.m1.2.2.1.1.1.1.2.1.cmml">∗</mo><msub id="S3.E6.m1.2.2.1.1.1.1.2.3" xref="S3.E6.m1.2.2.1.1.1.1.2.3.cmml"><mi id="S3.E6.m1.2.2.1.1.1.1.2.3.2" xref="S3.E6.m1.2.2.1.1.1.1.2.3.2.cmml">x</mi><mrow id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.cmml"><mi id="S3.E6.m1.1.1.1.3" xref="S3.E6.m1.1.1.1.3.cmml">t</mi><mo id="S3.E6.m1.1.1.1.2" xref="S3.E6.m1.1.1.1.2.cmml">+</mo><mrow id="S3.E6.m1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><mrow id="S3.E6.m1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E6.m1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.1.2.cmml">i</mi><mo id="S3.E6.m1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mn id="S3.E6.m1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.E6.m1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.2.cmml">​</mo><mi id="S3.E6.m1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.3.cmml">d</mi></mrow></mrow></msub></mrow></mrow><mo id="S3.E6.m1.2.2.1.1.1.3" xref="S3.E6.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.2b"><apply id="S3.E6.m1.2.2.cmml" xref="S3.E6.m1.2.2"><eq id="S3.E6.m1.2.2.2.cmml" xref="S3.E6.m1.2.2.2"></eq><apply id="S3.E6.m1.2.2.3.cmml" xref="S3.E6.m1.2.2.3"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.3.1.cmml" xref="S3.E6.m1.2.2.3">subscript</csymbol><ci id="S3.E6.m1.2.2.3.2.cmml" xref="S3.E6.m1.2.2.3.2">𝑦</ci><ci id="S3.E6.m1.2.2.3.3.cmml" xref="S3.E6.m1.2.2.3.3">𝑡</ci></apply><apply id="S3.E6.m1.2.2.1.cmml" xref="S3.E6.m1.2.2.1"><times id="S3.E6.m1.2.2.1.2.cmml" xref="S3.E6.m1.2.2.1.2"></times><ci id="S3.E6.m1.2.2.1.3a.cmml" xref="S3.E6.m1.2.2.1.3"><mtext id="S3.E6.m1.2.2.1.3.cmml" xref="S3.E6.m1.2.2.1.3">ReLU</mtext></ci><apply id="S3.E6.m1.2.2.1.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1"><apply id="S3.E6.m1.2.2.1.1.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1">superscript</csymbol><apply id="S3.E6.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1">subscript</csymbol><sum id="S3.E6.m1.2.2.1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.2.2"></sum><apply id="S3.E6.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.2.3"><eq id="S3.E6.m1.2.2.1.1.1.1.1.2.3.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.2.3.1"></eq><ci id="S3.E6.m1.2.2.1.1.1.1.1.2.3.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.2.3.2">𝑖</ci><cn type="integer" id="S3.E6.m1.2.2.1.1.1.1.1.2.3.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.2.3.3">1</cn></apply></apply><ci id="S3.E6.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.1.3">𝑁</ci></apply><apply id="S3.E6.m1.2.2.1.1.1.1.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2"><times id="S3.E6.m1.2.2.1.1.1.1.2.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2.1"></times><apply id="S3.E6.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.2.2.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E6.m1.2.2.1.1.1.1.2.2.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2.2.2">𝑊</ci><ci id="S3.E6.m1.2.2.1.1.1.1.2.2.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2.2.3">𝑖</ci></apply><apply id="S3.E6.m1.2.2.1.1.1.1.2.3.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.2.3.1.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E6.m1.2.2.1.1.1.1.2.3.2.cmml" xref="S3.E6.m1.2.2.1.1.1.1.2.3.2">𝑥</ci><apply id="S3.E6.m1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"><plus id="S3.E6.m1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.2"></plus><ci id="S3.E6.m1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.3">𝑡</ci><apply id="S3.E6.m1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1"><times id="S3.E6.m1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.2"></times><apply id="S3.E6.m1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1"><minus id="S3.E6.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.1"></minus><ci id="S3.E6.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.2">𝑖</ci><cn type="integer" id="S3.E6.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.1.3">1</cn></apply><ci id="S3.E6.m1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.3">𝑑</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.2c">y_{t}=\text{ReLU}\left(\sum_{i=1}^{N}W_{i}*x_{t+(i-1)d}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS5.p4" class="ltx_para">
<p id="S3.SS5.p4.5" class="ltx_p">where <math id="S3.SS5.p4.1.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S3.SS5.p4.1.m1.1a"><mo id="S3.SS5.p4.1.m1.1.1" xref="S3.SS5.p4.1.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.1.m1.1b"><times id="S3.SS5.p4.1.m1.1.1.cmml" xref="S3.SS5.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.1.m1.1c">*</annotation></semantics></math> denotes the convolution operation, <math id="S3.SS5.p4.2.m2.1" class="ltx_Math" alttext="W_{i}" display="inline"><semantics id="S3.SS5.p4.2.m2.1a"><msub id="S3.SS5.p4.2.m2.1.1" xref="S3.SS5.p4.2.m2.1.1.cmml"><mi id="S3.SS5.p4.2.m2.1.1.2" xref="S3.SS5.p4.2.m2.1.1.2.cmml">W</mi><mi id="S3.SS5.p4.2.m2.1.1.3" xref="S3.SS5.p4.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.2.m2.1b"><apply id="S3.SS5.p4.2.m2.1.1.cmml" xref="S3.SS5.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p4.2.m2.1.1.1.cmml" xref="S3.SS5.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS5.p4.2.m2.1.1.2.cmml" xref="S3.SS5.p4.2.m2.1.1.2">𝑊</ci><ci id="S3.SS5.p4.2.m2.1.1.3.cmml" xref="S3.SS5.p4.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.2.m2.1c">W_{i}</annotation></semantics></math> represents the learnable weights of the <math id="S3.SS5.p4.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS5.p4.3.m3.1a"><mi id="S3.SS5.p4.3.m3.1.1" xref="S3.SS5.p4.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.3.m3.1b"><ci id="S3.SS5.p4.3.m3.1.1.cmml" xref="S3.SS5.p4.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.3.m3.1c">i</annotation></semantics></math>-th convolutional layer, <math id="S3.SS5.p4.4.m4.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS5.p4.4.m4.1a"><mi id="S3.SS5.p4.4.m4.1.1" xref="S3.SS5.p4.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.4.m4.1b"><ci id="S3.SS5.p4.4.m4.1.1.cmml" xref="S3.SS5.p4.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.4.m4.1c">N</annotation></semantics></math> is the number of layers, and <math id="S3.SS5.p4.5.m5.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.SS5.p4.5.m5.1a"><mi id="S3.SS5.p4.5.m5.1.1" xref="S3.SS5.p4.5.m5.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.5.m5.1b"><ci id="S3.SS5.p4.5.m5.1.1.cmml" xref="S3.SS5.p4.5.m5.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.5.m5.1c">d</annotation></semantics></math> is the dilation rate. The ReLU function introduces non-linearity to the model, allowing it to learn complex temporal patterns.</p>
</div>
<div id="S3.SS5.p5" class="ltx_para">
<p id="S3.SS5.p5.1" class="ltx_p">The output of the TCN module, denoted as <math id="S3.SS5.p5.1.m1.4" class="ltx_Math" alttext="Y=\{y_{1},y_{2},...,y_{T}\}" display="inline"><semantics id="S3.SS5.p5.1.m1.4a"><mrow id="S3.SS5.p5.1.m1.4.4" xref="S3.SS5.p5.1.m1.4.4.cmml"><mi id="S3.SS5.p5.1.m1.4.4.5" xref="S3.SS5.p5.1.m1.4.4.5.cmml">Y</mi><mo id="S3.SS5.p5.1.m1.4.4.4" xref="S3.SS5.p5.1.m1.4.4.4.cmml">=</mo><mrow id="S3.SS5.p5.1.m1.4.4.3.3" xref="S3.SS5.p5.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS5.p5.1.m1.4.4.3.3.4" xref="S3.SS5.p5.1.m1.4.4.3.4.cmml">{</mo><msub id="S3.SS5.p5.1.m1.2.2.1.1.1" xref="S3.SS5.p5.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS5.p5.1.m1.2.2.1.1.1.2" xref="S3.SS5.p5.1.m1.2.2.1.1.1.2.cmml">y</mi><mn id="S3.SS5.p5.1.m1.2.2.1.1.1.3" xref="S3.SS5.p5.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS5.p5.1.m1.4.4.3.3.5" xref="S3.SS5.p5.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS5.p5.1.m1.3.3.2.2.2" xref="S3.SS5.p5.1.m1.3.3.2.2.2.cmml"><mi id="S3.SS5.p5.1.m1.3.3.2.2.2.2" xref="S3.SS5.p5.1.m1.3.3.2.2.2.2.cmml">y</mi><mn id="S3.SS5.p5.1.m1.3.3.2.2.2.3" xref="S3.SS5.p5.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS5.p5.1.m1.4.4.3.3.6" xref="S3.SS5.p5.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS5.p5.1.m1.1.1" xref="S3.SS5.p5.1.m1.1.1.cmml">…</mi><mo id="S3.SS5.p5.1.m1.4.4.3.3.7" xref="S3.SS5.p5.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS5.p5.1.m1.4.4.3.3.3" xref="S3.SS5.p5.1.m1.4.4.3.3.3.cmml"><mi id="S3.SS5.p5.1.m1.4.4.3.3.3.2" xref="S3.SS5.p5.1.m1.4.4.3.3.3.2.cmml">y</mi><mi id="S3.SS5.p5.1.m1.4.4.3.3.3.3" xref="S3.SS5.p5.1.m1.4.4.3.3.3.3.cmml">T</mi></msub><mo stretchy="false" id="S3.SS5.p5.1.m1.4.4.3.3.8" xref="S3.SS5.p5.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.1.m1.4b"><apply id="S3.SS5.p5.1.m1.4.4.cmml" xref="S3.SS5.p5.1.m1.4.4"><eq id="S3.SS5.p5.1.m1.4.4.4.cmml" xref="S3.SS5.p5.1.m1.4.4.4"></eq><ci id="S3.SS5.p5.1.m1.4.4.5.cmml" xref="S3.SS5.p5.1.m1.4.4.5">𝑌</ci><set id="S3.SS5.p5.1.m1.4.4.3.4.cmml" xref="S3.SS5.p5.1.m1.4.4.3.3"><apply id="S3.SS5.p5.1.m1.2.2.1.1.1.cmml" xref="S3.SS5.p5.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p5.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS5.p5.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS5.p5.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS5.p5.1.m1.2.2.1.1.1.2">𝑦</ci><cn type="integer" id="S3.SS5.p5.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS5.p5.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS5.p5.1.m1.3.3.2.2.2.cmml" xref="S3.SS5.p5.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS5.p5.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS5.p5.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.SS5.p5.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS5.p5.1.m1.3.3.2.2.2.2">𝑦</ci><cn type="integer" id="S3.SS5.p5.1.m1.3.3.2.2.2.3.cmml" xref="S3.SS5.p5.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS5.p5.1.m1.1.1.cmml" xref="S3.SS5.p5.1.m1.1.1">…</ci><apply id="S3.SS5.p5.1.m1.4.4.3.3.3.cmml" xref="S3.SS5.p5.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS5.p5.1.m1.4.4.3.3.3.1.cmml" xref="S3.SS5.p5.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.SS5.p5.1.m1.4.4.3.3.3.2.cmml" xref="S3.SS5.p5.1.m1.4.4.3.3.3.2">𝑦</ci><ci id="S3.SS5.p5.1.m1.4.4.3.3.3.3.cmml" xref="S3.SS5.p5.1.m1.4.4.3.3.3.3">𝑇</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.1.m1.4c">Y=\{y_{1},y_{2},...,y_{T}\}</annotation></semantics></math>, contains features that capture temporal dependencies and can be further processed by subsequent layers for downstream tasks such as classification or regression.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Transformer Encode</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">The Transformer Encoder is a crucial component in many sequence modeling tasks, including natural language processing and video analysis. It utilizes self-attention mechanisms to capture long-range dependencies within sequences effectively.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.3" class="ltx_p">Formally, given an input sequence <math id="S3.SS6.p2.1.m1.4" class="ltx_Math" alttext="\mathbf{X}=\{\mathbf{x}_{1},\mathbf{x}_{2},...,\mathbf{x}_{T}\}" display="inline"><semantics id="S3.SS6.p2.1.m1.4a"><mrow id="S3.SS6.p2.1.m1.4.4" xref="S3.SS6.p2.1.m1.4.4.cmml"><mi id="S3.SS6.p2.1.m1.4.4.5" xref="S3.SS6.p2.1.m1.4.4.5.cmml">𝐗</mi><mo id="S3.SS6.p2.1.m1.4.4.4" xref="S3.SS6.p2.1.m1.4.4.4.cmml">=</mo><mrow id="S3.SS6.p2.1.m1.4.4.3.3" xref="S3.SS6.p2.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS6.p2.1.m1.4.4.3.3.4" xref="S3.SS6.p2.1.m1.4.4.3.4.cmml">{</mo><msub id="S3.SS6.p2.1.m1.2.2.1.1.1" xref="S3.SS6.p2.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS6.p2.1.m1.2.2.1.1.1.2" xref="S3.SS6.p2.1.m1.2.2.1.1.1.2.cmml">𝐱</mi><mn id="S3.SS6.p2.1.m1.2.2.1.1.1.3" xref="S3.SS6.p2.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS6.p2.1.m1.4.4.3.3.5" xref="S3.SS6.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS6.p2.1.m1.3.3.2.2.2" xref="S3.SS6.p2.1.m1.3.3.2.2.2.cmml"><mi id="S3.SS6.p2.1.m1.3.3.2.2.2.2" xref="S3.SS6.p2.1.m1.3.3.2.2.2.2.cmml">𝐱</mi><mn id="S3.SS6.p2.1.m1.3.3.2.2.2.3" xref="S3.SS6.p2.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS6.p2.1.m1.4.4.3.3.6" xref="S3.SS6.p2.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS6.p2.1.m1.1.1" xref="S3.SS6.p2.1.m1.1.1.cmml">…</mi><mo id="S3.SS6.p2.1.m1.4.4.3.3.7" xref="S3.SS6.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS6.p2.1.m1.4.4.3.3.3" xref="S3.SS6.p2.1.m1.4.4.3.3.3.cmml"><mi id="S3.SS6.p2.1.m1.4.4.3.3.3.2" xref="S3.SS6.p2.1.m1.4.4.3.3.3.2.cmml">𝐱</mi><mi id="S3.SS6.p2.1.m1.4.4.3.3.3.3" xref="S3.SS6.p2.1.m1.4.4.3.3.3.3.cmml">T</mi></msub><mo stretchy="false" id="S3.SS6.p2.1.m1.4.4.3.3.8" xref="S3.SS6.p2.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.1.m1.4b"><apply id="S3.SS6.p2.1.m1.4.4.cmml" xref="S3.SS6.p2.1.m1.4.4"><eq id="S3.SS6.p2.1.m1.4.4.4.cmml" xref="S3.SS6.p2.1.m1.4.4.4"></eq><ci id="S3.SS6.p2.1.m1.4.4.5.cmml" xref="S3.SS6.p2.1.m1.4.4.5">𝐗</ci><set id="S3.SS6.p2.1.m1.4.4.3.4.cmml" xref="S3.SS6.p2.1.m1.4.4.3.3"><apply id="S3.SS6.p2.1.m1.2.2.1.1.1.cmml" xref="S3.SS6.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p2.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS6.p2.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS6.p2.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS6.p2.1.m1.2.2.1.1.1.2">𝐱</ci><cn type="integer" id="S3.SS6.p2.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS6.p2.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS6.p2.1.m1.3.3.2.2.2.cmml" xref="S3.SS6.p2.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS6.p2.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS6.p2.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.SS6.p2.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS6.p2.1.m1.3.3.2.2.2.2">𝐱</ci><cn type="integer" id="S3.SS6.p2.1.m1.3.3.2.2.2.3.cmml" xref="S3.SS6.p2.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS6.p2.1.m1.1.1.cmml" xref="S3.SS6.p2.1.m1.1.1">…</ci><apply id="S3.SS6.p2.1.m1.4.4.3.3.3.cmml" xref="S3.SS6.p2.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS6.p2.1.m1.4.4.3.3.3.1.cmml" xref="S3.SS6.p2.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.SS6.p2.1.m1.4.4.3.3.3.2.cmml" xref="S3.SS6.p2.1.m1.4.4.3.3.3.2">𝐱</ci><ci id="S3.SS6.p2.1.m1.4.4.3.3.3.3.cmml" xref="S3.SS6.p2.1.m1.4.4.3.3.3.3">𝑇</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.1.m1.4c">\mathbf{X}=\{\mathbf{x}_{1},\mathbf{x}_{2},...,\mathbf{x}_{T}\}</annotation></semantics></math>, where <math id="S3.SS6.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{x}_{t}" display="inline"><semantics id="S3.SS6.p2.2.m2.1a"><msub id="S3.SS6.p2.2.m2.1.1" xref="S3.SS6.p2.2.m2.1.1.cmml"><mi id="S3.SS6.p2.2.m2.1.1.2" xref="S3.SS6.p2.2.m2.1.1.2.cmml">𝐱</mi><mi id="S3.SS6.p2.2.m2.1.1.3" xref="S3.SS6.p2.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.2.m2.1b"><apply id="S3.SS6.p2.2.m2.1.1.cmml" xref="S3.SS6.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS6.p2.2.m2.1.1.1.cmml" xref="S3.SS6.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS6.p2.2.m2.1.1.2.cmml" xref="S3.SS6.p2.2.m2.1.1.2">𝐱</ci><ci id="S3.SS6.p2.2.m2.1.1.3.cmml" xref="S3.SS6.p2.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.2.m2.1c">\mathbf{x}_{t}</annotation></semantics></math> represents the feature representation at time step <math id="S3.SS6.p2.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS6.p2.3.m3.1a"><mi id="S3.SS6.p2.3.m3.1.1" xref="S3.SS6.p2.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.3.m3.1b"><ci id="S3.SS6.p2.3.m3.1.1.cmml" xref="S3.SS6.p2.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.3.m3.1c">t</annotation></semantics></math>, the Transformer Encoder processes the sequence through multiple layers of self-attention and feed-forward neural networks. Each layer consists of two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.</p>
</div>
<div id="S3.SS6.p3" class="ltx_para">
<p id="S3.SS6.p3.1" class="ltx_p">The multi-head self-attention mechanism computes attention scores between all pairs of positions in the input sequence, allowing the model to focus on relevant information from distant positions. This is achieved by linearly projecting the input into multiple lower-dimensional representations, computing attention scores, and then aggregating them across different heads.</p>
</div>
<div id="S3.SS6.p4" class="ltx_para">
<p id="S3.SS6.p4.1" class="ltx_p">The position-wise feed-forward network applies a two-layer fully connected neural network independently to each position in the sequence. This operation introduces non-linearity and enables the model to capture complex interactions between features.</p>
</div>
<div id="S3.SS6.p5" class="ltx_para">
<p id="S3.SS6.p5.1" class="ltx_p">After processing through multiple layers of self-attention and feed-forward networks, the Transformer Encoder produces a sequence of output representations <math id="S3.SS6.p5.1.m1.4" class="ltx_Math" alttext="\mathbf{Y}=\{\mathbf{y}_{1},\mathbf{y}_{2},...,\mathbf{y}_{T}\}" display="inline"><semantics id="S3.SS6.p5.1.m1.4a"><mrow id="S3.SS6.p5.1.m1.4.4" xref="S3.SS6.p5.1.m1.4.4.cmml"><mi id="S3.SS6.p5.1.m1.4.4.5" xref="S3.SS6.p5.1.m1.4.4.5.cmml">𝐘</mi><mo id="S3.SS6.p5.1.m1.4.4.4" xref="S3.SS6.p5.1.m1.4.4.4.cmml">=</mo><mrow id="S3.SS6.p5.1.m1.4.4.3.3" xref="S3.SS6.p5.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S3.SS6.p5.1.m1.4.4.3.3.4" xref="S3.SS6.p5.1.m1.4.4.3.4.cmml">{</mo><msub id="S3.SS6.p5.1.m1.2.2.1.1.1" xref="S3.SS6.p5.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS6.p5.1.m1.2.2.1.1.1.2" xref="S3.SS6.p5.1.m1.2.2.1.1.1.2.cmml">𝐲</mi><mn id="S3.SS6.p5.1.m1.2.2.1.1.1.3" xref="S3.SS6.p5.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS6.p5.1.m1.4.4.3.3.5" xref="S3.SS6.p5.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS6.p5.1.m1.3.3.2.2.2" xref="S3.SS6.p5.1.m1.3.3.2.2.2.cmml"><mi id="S3.SS6.p5.1.m1.3.3.2.2.2.2" xref="S3.SS6.p5.1.m1.3.3.2.2.2.2.cmml">𝐲</mi><mn id="S3.SS6.p5.1.m1.3.3.2.2.2.3" xref="S3.SS6.p5.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS6.p5.1.m1.4.4.3.3.6" xref="S3.SS6.p5.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.SS6.p5.1.m1.1.1" xref="S3.SS6.p5.1.m1.1.1.cmml">…</mi><mo id="S3.SS6.p5.1.m1.4.4.3.3.7" xref="S3.SS6.p5.1.m1.4.4.3.4.cmml">,</mo><msub id="S3.SS6.p5.1.m1.4.4.3.3.3" xref="S3.SS6.p5.1.m1.4.4.3.3.3.cmml"><mi id="S3.SS6.p5.1.m1.4.4.3.3.3.2" xref="S3.SS6.p5.1.m1.4.4.3.3.3.2.cmml">𝐲</mi><mi id="S3.SS6.p5.1.m1.4.4.3.3.3.3" xref="S3.SS6.p5.1.m1.4.4.3.3.3.3.cmml">T</mi></msub><mo stretchy="false" id="S3.SS6.p5.1.m1.4.4.3.3.8" xref="S3.SS6.p5.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p5.1.m1.4b"><apply id="S3.SS6.p5.1.m1.4.4.cmml" xref="S3.SS6.p5.1.m1.4.4"><eq id="S3.SS6.p5.1.m1.4.4.4.cmml" xref="S3.SS6.p5.1.m1.4.4.4"></eq><ci id="S3.SS6.p5.1.m1.4.4.5.cmml" xref="S3.SS6.p5.1.m1.4.4.5">𝐘</ci><set id="S3.SS6.p5.1.m1.4.4.3.4.cmml" xref="S3.SS6.p5.1.m1.4.4.3.3"><apply id="S3.SS6.p5.1.m1.2.2.1.1.1.cmml" xref="S3.SS6.p5.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS6.p5.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS6.p5.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS6.p5.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS6.p5.1.m1.2.2.1.1.1.2">𝐲</ci><cn type="integer" id="S3.SS6.p5.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS6.p5.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS6.p5.1.m1.3.3.2.2.2.cmml" xref="S3.SS6.p5.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS6.p5.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS6.p5.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.SS6.p5.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS6.p5.1.m1.3.3.2.2.2.2">𝐲</ci><cn type="integer" id="S3.SS6.p5.1.m1.3.3.2.2.2.3.cmml" xref="S3.SS6.p5.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS6.p5.1.m1.1.1.cmml" xref="S3.SS6.p5.1.m1.1.1">…</ci><apply id="S3.SS6.p5.1.m1.4.4.3.3.3.cmml" xref="S3.SS6.p5.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS6.p5.1.m1.4.4.3.3.3.1.cmml" xref="S3.SS6.p5.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.SS6.p5.1.m1.4.4.3.3.3.2.cmml" xref="S3.SS6.p5.1.m1.4.4.3.3.3.2">𝐲</ci><ci id="S3.SS6.p5.1.m1.4.4.3.3.3.3.cmml" xref="S3.SS6.p5.1.m1.4.4.3.3.3.3">𝑇</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p5.1.m1.4c">\mathbf{Y}=\{\mathbf{y}_{1},\mathbf{y}_{2},...,\mathbf{y}_{T}\}</annotation></semantics></math>, which captures rich contextual information and dependencies within the input sequence. These representations can be further used for downstream tasks such as classification, regression, or sequence generation.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F3.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:433.6pt;"><img src="/html/2403.12425/assets/LANet.png" id="S3.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="379" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.1.1.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F3.1.2.2" class="ltx_text" style="font-size:90%;">The LANet module, where H, W and C refer to height, width and number of channels, respectively.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F3.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:433.6pt;"><img src="/html/2403.12425/assets/SENet.png" id="S3.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="380" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.2.2.2" class="ltx_text" style="font-size:90%;">The SENet module, where H, W and C refer to height, width and number of channels, respectively.</span></figcaption>
</figure>
</div>
</div>
</figure>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S3.T2.3.2" class="ltx_text" style="font-size:90%;">Module Ablation Experiment Results for LA-SE, TCN, and Transformer Encode</span></figcaption>
<table id="S3.T2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.4.1.1" class="ltx_tr">
<th id="S3.T2.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S3.T2.4.1.1.1.1" class="ltx_text ltx_font_bold">Experimental Combination</span></th>
<th id="S3.T2.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.4.1.1.2.1" class="ltx_text ltx_font_bold">LA-SE</span></th>
<th id="S3.T2.4.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.4.1.1.3.1" class="ltx_text ltx_font_bold">TCN</span></th>
<th id="S3.T2.4.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.4.1.1.4.1" class="ltx_text ltx_font_bold">Transformer Encode</span></th>
<th id="S3.T2.4.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.4.1.1.5.1" class="ltx_text ltx_font_bold">CCC Score (%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.4.2.1" class="ltx_tr">
<th id="S3.T2.4.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Baseline</th>
<td id="S3.T2.4.2.1.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.4.2.1.3" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.4.2.1.4" class="ltx_td ltx_border_t"></td>
<td id="S3.T2.4.2.1.5" class="ltx_td ltx_align_center ltx_border_t">31.08</td>
</tr>
<tr id="S3.T2.4.3.2" class="ltx_tr">
<th id="S3.T2.4.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">+LA-SE</th>
<td id="S3.T2.4.3.2.2" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T2.4.3.2.3" class="ltx_td"></td>
<td id="S3.T2.4.3.2.4" class="ltx_td"></td>
<td id="S3.T2.4.3.2.5" class="ltx_td ltx_align_center">33.96</td>
</tr>
<tr id="S3.T2.4.4.3" class="ltx_tr">
<th id="S3.T2.4.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">+TCN</th>
<td id="S3.T2.4.4.3.2" class="ltx_td"></td>
<td id="S3.T2.4.4.3.3" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T2.4.4.3.4" class="ltx_td"></td>
<td id="S3.T2.4.4.3.5" class="ltx_td ltx_align_center">51.57</td>
</tr>
<tr id="S3.T2.4.5.4" class="ltx_tr">
<th id="S3.T2.4.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">+Transformer Encode</th>
<td id="S3.T2.4.5.4.2" class="ltx_td"></td>
<td id="S3.T2.4.5.4.3" class="ltx_td"></td>
<td id="S3.T2.4.5.4.4" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T2.4.5.4.5" class="ltx_td ltx_align_center">38.77</td>
</tr>
<tr id="S3.T2.4.6.5" class="ltx_tr">
<th id="S3.T2.4.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">+LA-SE + TCN</th>
<td id="S3.T2.4.6.5.2" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T2.4.6.5.3" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T2.4.6.5.4" class="ltx_td"></td>
<td id="S3.T2.4.6.5.5" class="ltx_td ltx_align_center">52.71</td>
</tr>
<tr id="S3.T2.4.7.6" class="ltx_tr">
<th id="S3.T2.4.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">+LA-SE + Transformer Encode</th>
<td id="S3.T2.4.7.6.2" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T2.4.7.6.3" class="ltx_td"></td>
<td id="S3.T2.4.7.6.4" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T2.4.7.6.5" class="ltx_td ltx_align_center">40.12</td>
</tr>
<tr id="S3.T2.4.8.7" class="ltx_tr">
<th id="S3.T2.4.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">+TCN + Transformer Encode</th>
<td id="S3.T2.4.8.7.2" class="ltx_td"></td>
<td id="S3.T2.4.8.7.3" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T2.4.8.7.4" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T2.4.8.7.5" class="ltx_td ltx_align_center">60.21</td>
</tr>
<tr id="S3.T2.4.9.8" class="ltx_tr">
<th id="S3.T2.4.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">+LA-SE + TCN + Transformer Encode</th>
<td id="S3.T2.4.9.8.2" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S3.T2.4.9.8.3" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S3.T2.4.9.8.4" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S3.T2.4.9.8.5" class="ltx_td ltx_align_center ltx_border_bb">63.56</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we provided an overview of the dataset, evaluation protocol, and experimental results used in the study.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset and Evaluation</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Affwild2 dataset.</span> Aff-Wild2 is an extension of the Aff-Wild dataset for affect recognition. It approximately doubles the number of included video frames and the number of subjects; thus, improving the variability of the included behaviors and of the involved persons. Aff-Wild2 is a significant research asset, being the only database annotated for all three main behavioral tasks in the wild. It is a large-scale database and the first one to feature AU annotations alongside both audio and video. Annotations in Aff-Wild2 are frame-based, encompassing seven basic expressions, twelve action units, as well as valence and arousal. In total, Aff-Wild2 comprises 564 videos, around 2.8 million frames, involving 554 subjects. These subjects represent diverse demographics in terms of age, ethnicity, and nationality, while also presenting a wide array of environmental and situational variations.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Evaluation.</span>To estimate the VA, we computed the Concordance Correlation Coefficient (CCC) separately for arousal and valence. The evaluation metric for this competition is represented by Equation <span id="S4.SS1.p2.1.2" class="ltx_text" style="color:#FF0000;">7</span>.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<table id="S4.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E7.m1.1" class="ltx_Math" alttext="P=0.5\times(CCC_{\text{arousal}}+CCC_{\text{valence}})" display="block"><semantics id="S4.E7.m1.1a"><mrow id="S4.E7.m1.1.1" xref="S4.E7.m1.1.1.cmml"><mi id="S4.E7.m1.1.1.3" xref="S4.E7.m1.1.1.3.cmml">P</mi><mo id="S4.E7.m1.1.1.2" xref="S4.E7.m1.1.1.2.cmml">=</mo><mrow id="S4.E7.m1.1.1.1" xref="S4.E7.m1.1.1.1.cmml"><mn id="S4.E7.m1.1.1.1.3" xref="S4.E7.m1.1.1.1.3.cmml">0.5</mn><mo lspace="0.222em" rspace="0.222em" id="S4.E7.m1.1.1.1.2" xref="S4.E7.m1.1.1.1.2.cmml">×</mo><mrow id="S4.E7.m1.1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E7.m1.1.1.1.1.1.2" xref="S4.E7.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E7.m1.1.1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.1.1.cmml"><mrow id="S4.E7.m1.1.1.1.1.1.1.2" xref="S4.E7.m1.1.1.1.1.1.1.2.cmml"><mi id="S4.E7.m1.1.1.1.1.1.1.2.2" xref="S4.E7.m1.1.1.1.1.1.1.2.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.1.1.1.1.2.1" xref="S4.E7.m1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S4.E7.m1.1.1.1.1.1.1.2.3" xref="S4.E7.m1.1.1.1.1.1.1.2.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.1.1.1.1.2.1a" xref="S4.E7.m1.1.1.1.1.1.1.2.1.cmml">​</mo><msub id="S4.E7.m1.1.1.1.1.1.1.2.4" xref="S4.E7.m1.1.1.1.1.1.1.2.4.cmml"><mi id="S4.E7.m1.1.1.1.1.1.1.2.4.2" xref="S4.E7.m1.1.1.1.1.1.1.2.4.2.cmml">C</mi><mtext id="S4.E7.m1.1.1.1.1.1.1.2.4.3" xref="S4.E7.m1.1.1.1.1.1.1.2.4.3a.cmml">arousal</mtext></msub></mrow><mo id="S4.E7.m1.1.1.1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S4.E7.m1.1.1.1.1.1.1.3" xref="S4.E7.m1.1.1.1.1.1.1.3.cmml"><mi id="S4.E7.m1.1.1.1.1.1.1.3.2" xref="S4.E7.m1.1.1.1.1.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.1.1.1.1.3.1" xref="S4.E7.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S4.E7.m1.1.1.1.1.1.1.3.3" xref="S4.E7.m1.1.1.1.1.1.1.3.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.1.1.1.1.3.1a" xref="S4.E7.m1.1.1.1.1.1.1.3.1.cmml">​</mo><msub id="S4.E7.m1.1.1.1.1.1.1.3.4" xref="S4.E7.m1.1.1.1.1.1.1.3.4.cmml"><mi id="S4.E7.m1.1.1.1.1.1.1.3.4.2" xref="S4.E7.m1.1.1.1.1.1.1.3.4.2.cmml">C</mi><mtext id="S4.E7.m1.1.1.1.1.1.1.3.4.3" xref="S4.E7.m1.1.1.1.1.1.1.3.4.3a.cmml">valence</mtext></msub></mrow></mrow><mo stretchy="false" id="S4.E7.m1.1.1.1.1.1.3" xref="S4.E7.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E7.m1.1b"><apply id="S4.E7.m1.1.1.cmml" xref="S4.E7.m1.1.1"><eq id="S4.E7.m1.1.1.2.cmml" xref="S4.E7.m1.1.1.2"></eq><ci id="S4.E7.m1.1.1.3.cmml" xref="S4.E7.m1.1.1.3">𝑃</ci><apply id="S4.E7.m1.1.1.1.cmml" xref="S4.E7.m1.1.1.1"><times id="S4.E7.m1.1.1.1.2.cmml" xref="S4.E7.m1.1.1.1.2"></times><cn type="float" id="S4.E7.m1.1.1.1.3.cmml" xref="S4.E7.m1.1.1.1.3">0.5</cn><apply id="S4.E7.m1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1.1.1"><plus id="S4.E7.m1.1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.1"></plus><apply id="S4.E7.m1.1.1.1.1.1.1.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.2"><times id="S4.E7.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.2.1"></times><ci id="S4.E7.m1.1.1.1.1.1.1.2.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.2.2">𝐶</ci><ci id="S4.E7.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.E7.m1.1.1.1.1.1.1.2.3">𝐶</ci><apply id="S4.E7.m1.1.1.1.1.1.1.2.4.cmml" xref="S4.E7.m1.1.1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.1.1.2.4.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.2.4">subscript</csymbol><ci id="S4.E7.m1.1.1.1.1.1.1.2.4.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.2.4.2">𝐶</ci><ci id="S4.E7.m1.1.1.1.1.1.1.2.4.3a.cmml" xref="S4.E7.m1.1.1.1.1.1.1.2.4.3"><mtext mathsize="70%" id="S4.E7.m1.1.1.1.1.1.1.2.4.3.cmml" xref="S4.E7.m1.1.1.1.1.1.1.2.4.3">arousal</mtext></ci></apply></apply><apply id="S4.E7.m1.1.1.1.1.1.1.3.cmml" xref="S4.E7.m1.1.1.1.1.1.1.3"><times id="S4.E7.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.3.1"></times><ci id="S4.E7.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.3.2">𝐶</ci><ci id="S4.E7.m1.1.1.1.1.1.1.3.3.cmml" xref="S4.E7.m1.1.1.1.1.1.1.3.3">𝐶</ci><apply id="S4.E7.m1.1.1.1.1.1.1.3.4.cmml" xref="S4.E7.m1.1.1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.1.1.3.4.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.3.4">subscript</csymbol><ci id="S4.E7.m1.1.1.1.1.1.1.3.4.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.3.4.2">𝐶</ci><ci id="S4.E7.m1.1.1.1.1.1.1.3.4.3a.cmml" xref="S4.E7.m1.1.1.1.1.1.1.3.4.3"><mtext mathsize="70%" id="S4.E7.m1.1.1.1.1.1.1.3.4.3.cmml" xref="S4.E7.m1.1.1.1.1.1.1.3.4.3">valence</mtext></ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E7.m1.1c">P=0.5\times(CCC_{\text{arousal}}+CCC_{\text{valence}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Module Ablation Experiment Results</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In this section, we present the results of the module ablation experiments for LA-SE, TCN, and Transformer Encode, along with their impact on the CCC (Concordance Correlation Coefficient) scores.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The table <span id="S4.SS2.p2.1.1" class="ltx_text" style="color:#FF0000;">2</span> presents the CCC scores (%) obtained from different experimental combinations. Each row represents a specific experimental configuration, including individual modules and their combinations. Starting with the Baseline model, which directly concatenates features extracted from video and audio, yielding a CCC score of 31.08. Subsequently, we gradually added different modules to investigate their effects on model performance. By adding the LA-SE module, the CCC score increased to 33.96, indicating an improvement in model performance. When the TCN module was added alone, the CCC score significantly increased to 51.57, demonstrating its effectiveness in capturing temporal dependencies between frames. The introduction of the Transformer Encode module further improved performance, increasing the CCC score to 38.77. This suggests that Transformer Encode helps to learn long-range dependencies between features, enhancing the model’s generalization capability. When combining the LA-SE and TCN modules, the CCC score further improved to 52.71, demonstrating the synergistic effect of these two modules. Similarly, the combination of LA-SE and Transformer Encode also led to a performance improvement, with a CCC score of 40.12. However, the most significant improvement occurred with the combination of TCN and Transformer Encode, resulting in a CCC score of 60.21. This indicates that TCN and Transformer Encode modules complement each other well in capturing temporal and feature relationships. Finally, when all three modules were combined, the model achieved the highest performance, with a CCC score of 63.71. This further validates the importance and effectiveness of the LA-SE, TCN, and Transformer Encode modules in improving model performance.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results on Validation set</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">For the estimation of VA, we evaluate the models based on the average CCC values for valence and arousal. To enhance the model’s generalization, we further conduct a six-fold cross-validation on randomly segmented annotated data. Detailed experimental results are provided in Table <span id="S4.SS3.p1.1.1" class="ltx_text" style="color:#FF0000;">3</span>. This table presents the Valence and Arousal scores for different validation sets (fold-0 to fold-5). Each validation set corresponds to a specific cross-validation fold. Each row represents a validation set, including its Valence and Arousal scores. The last row shows the best Valence and Arousal scores obtained across all folds, highlighted in bold.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.2.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.3.2" class="ltx_text" style="font-size:90%;">Valence and Arousal Scores for Different Validation Sets</span></figcaption>
<table id="S4.T3.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.4.1.1" class="ltx_tr">
<th id="S4.T3.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.4.1.1.1.1" class="ltx_text ltx_font_bold">Val Set</span></th>
<th id="S4.T3.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.4.1.1.2.1" class="ltx_text ltx_font_bold">Valence</span></th>
<th id="S4.T3.4.1.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.4.1.1.3.1" class="ltx_text ltx_font_bold">Arousal</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.4.2.1" class="ltx_tr">
<td id="S4.T3.4.2.1.1" class="ltx_td ltx_align_center ltx_border_t">fold-0</td>
<td id="S4.T3.4.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.5121</td>
<td id="S4.T3.4.2.1.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.5507</td>
</tr>
<tr id="S4.T3.4.3.2" class="ltx_tr">
<td id="S4.T3.4.3.2.1" class="ltx_td ltx_align_center">fold-1</td>
<td id="S4.T3.4.3.2.2" class="ltx_td ltx_align_center">0.5789</td>
<td id="S4.T3.4.3.2.3" class="ltx_td ltx_nopad_r ltx_align_center">0.5992</td>
</tr>
<tr id="S4.T3.4.4.3" class="ltx_tr">
<td id="S4.T3.4.4.3.1" class="ltx_td ltx_align_center">fold-2</td>
<td id="S4.T3.4.4.3.2" class="ltx_td ltx_align_center">0.5547</td>
<td id="S4.T3.4.4.3.3" class="ltx_td ltx_nopad_r ltx_align_center">0.5862</td>
</tr>
<tr id="S4.T3.4.5.4" class="ltx_tr">
<td id="S4.T3.4.5.4.1" class="ltx_td ltx_align_center">fold-3</td>
<td id="S4.T3.4.5.4.2" class="ltx_td ltx_align_center">0.5743</td>
<td id="S4.T3.4.5.4.3" class="ltx_td ltx_nopad_r ltx_align_center">0.5959</td>
</tr>
<tr id="S4.T3.4.6.5" class="ltx_tr">
<td id="S4.T3.4.6.5.1" class="ltx_td ltx_align_center">fold-4</td>
<td id="S4.T3.4.6.5.2" class="ltx_td ltx_align_center">0.5971</td>
<td id="S4.T3.4.6.5.3" class="ltx_td ltx_nopad_r ltx_align_center">0.6156</td>
</tr>
<tr id="S4.T3.4.7.6" class="ltx_tr">
<td id="S4.T3.4.7.6.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.4.7.6.1.1" class="ltx_text ltx_font_bold">fold-5</span></td>
<td id="S4.T3.4.7.6.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.4.7.6.2.1" class="ltx_text ltx_font_bold">0.6123</span></td>
<td id="S4.T3.4.7.6.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T3.4.7.6.3.1" class="ltx_text ltx_font_bold">0.6589</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This paper introduces our submission for the VA estimation task in ABAW6 competition. We conducted preprocessing on video frames and audio segments to extract visual and audio features, constructing a comprehensive model for VA (Valence-Arousal) estimation task. Utilizing TCN modules, we captured the temporal and spatial correlations between features. Finally, we fed the extracted features into a Transformer encoder structure to learn long-range dependencies and enhance the model’s performance and generalization ability.


<span id="S5.p1.1.1" class="ltx_text" style="font-size:90%;"></span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Bai et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Shaojie Bai, J. Zico Kolter, and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling, 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Behrmann et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Jens Behrmann, Will Grathwohl, Ricky T. Q. Chen, David Duvenaud, and Jörn-Henrik Jacobsen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">Invertible residual networks, 2019.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.4.4.1" class="ltx_text" style="font-size:90%;">Davis and Mermelstein [1980]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.6.1" class="ltx_text" style="font-size:90%;">
Steven Davis and Paul Mermelstein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib3.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE transactions on acoustics, speech, and signal processing</em><span id="bib.bib3.9.2" class="ltx_text" style="font-size:90%;">, 28(4):357–366, 1980.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Deng et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Didan Deng, Liang Wu, and Bertram E Shi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">Iterative distillation for better uncertainty estimates in multitask emotion recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib4.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib4.11.3" class="ltx_text" style="font-size:90%;">, pages 3557–3566, 2021.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Deng et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Jiankang Deng, Jia Guo, Jing Yang, Niannan Xue, Irene Kotsia, and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Arcface: Additive angular margin loss for deep face recognition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib5.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</em><span id="bib.bib5.10.2" class="ltx_text" style="font-size:90%;">, 44(10):5962–5979, 2022.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Duta et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Ionut Cosmin Duta, Li Liu, Fan Zhu, and Ling Shao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">Improved residual networks for image and video recognition, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Han et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">A survey on vision transformer.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib7.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</em><span id="bib.bib7.10.2" class="ltx_text" style="font-size:90%;">, 45(1):87–110, 2022.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">He et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition, 2015.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Hershey et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Shawn Hershey, Sourish Chaudhuri, Daniel P. W. Ellis, Jort F. Gemmeke, Aren Jansen, R. Channing Moore, Manoj Plakal, Devin Platt, Rif A. Saurous, Bryan Seybold, Malcolm Slaney, Ron J. Weiss, and Kevin Wilson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">Cnn architectures for large-scale audio classification, 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Hu et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">Squeeze-and-excitation networks, 2019.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Khare et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Smith K. Khare, Victoria Blanes-Vidal, Esmaeil S. Nadimi, and U. Rajendra Acharya.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">Emotion recognition and artificial intelligence: A systematic review (2014–2023) and research recommendations.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib11.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Information Fusion</em><span id="bib.bib11.10.2" class="ltx_text" style="font-size:90%;">, 102:102019, 2024.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Kim et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Jun-Hwa Kim, Namho Kim, and Chee Sun Won.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Facial expression recognition with swin transformer.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib12.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2203.13472</em><span id="bib.bib12.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.4.4.1" class="ltx_text" style="font-size:90%;">Kollias [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.6.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">Abaw: Valence-arousal estimation, expression recognition, action unit detection &amp; multi-task learning challenges.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib13.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib13.10.3" class="ltx_text" style="font-size:90%;">, pages 2328–2336, 2022.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.4.4.1" class="ltx_text" style="font-size:90%;">Kollias [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.6.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">Multi-label compound expression recognition: C-expr database &amp; network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib14.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib14.10.3" class="ltx_text" style="font-size:90%;">, pages 5589–5598, 2023.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.4.4.1" class="ltx_text" style="font-size:90%;">Kollias and Zafeiriou [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.6.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib15.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1910.04855</em><span id="bib.bib15.9.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.4.4.1" class="ltx_text" style="font-size:90%;">Kollias and Zafeiriou [2021a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.6.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework, 2021a.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.4.4.1" class="ltx_text" style="font-size:90%;">Kollias and Zafeiriou [2021b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.6.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">Analysing affective behavior in the second abaw2 competition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib17.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib17.10.3" class="ltx_text" style="font-size:90%;">, pages 3652–3660, 2021b.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">Kollias et al. [2019a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
D Kollias, A Schulc, E Hajiyev, and S Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">Analysing affective behavior in the first abaw 2020 competition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib18.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)</em><span id="bib.bib18.11.3" class="ltx_text" style="font-size:90%;">, pages 794–800, 2019a.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Kollias et al. [2019b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Face behavior a la carte: Expressions, affect and action units in a single network.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib19.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1910.11111</em><span id="bib.bib19.10.2" class="ltx_text" style="font-size:90%;">, 2019b.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Kollias et al. [2019c]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A. Nicolaou, Athanasios Papaioannou, Guoying Zhao, Björn Schuller, Irene Kotsia, and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib20.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</em><span id="bib.bib20.10.2" class="ltx_text" style="font-size:90%;">, 127(6–7):907–929, 2019c.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Kollias et al. [2019d]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou, Athanasios Papaioannou, Guoying Zhao, Björn Schuller, Irene Kotsia, and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib21.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</em><span id="bib.bib21.10.2" class="ltx_text" style="font-size:90%;">, pages 1–23, 2019d.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Kollias et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Face behavior a la carte: Expressions, affect and action units in a single network, 2020.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Kollias et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias, Viktoriia Sharmanska, and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">Distribution matching for heterogeneous multi-task learning: a large-scale face study.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib23.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2105.03790</em><span id="bib.bib23.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Kollias et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias, Panagiotis Tzirakis, Alice Baird, Alan Cowen, and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Abaw: Valence-arousal estimation, expression recognition, action unit detection &amp; emotional reaction intensity estimation challenges.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib24.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib24.11.3" class="ltx_text" style="font-size:90%;">, pages 5888–5897, 2023.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="font-size:90%;">Kollias et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">
Dimitrios Kollias, Panagiotis Tzirakis, Alan Cowen, Stefanos Zafeiriou, Chunchang Shao, and Guanyu Hu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">The 6th affective behavior analysis in-the-wild (abaw) competition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib25.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2402.19344</em><span id="bib.bib25.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Kuhnke et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Felix Kuhnke, Lars Rumberg, and Jörn Ostermann.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Two-stream aural-visual affect analysis in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib26.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)</em><span id="bib.bib26.11.3" class="ltx_text" style="font-size:90%;">, pages 600–605. IEEE, 2020.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Lieskovská et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Eva Lieskovská, Maroš Jakubec, Roman Jarina, and Michal Chmulík.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">A review on speech emotion recognition using deep learning and attention mechanism.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib27.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Electronics</em><span id="bib.bib27.10.2" class="ltx_text" style="font-size:90%;">, 10(10):1163, 2021.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Meng et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
Liyu Meng, Yuchen Liu, Xiaolong Liu, Zhaopei Huang, Yuan Cheng, Meng Wang, Chuanhe Liu, and Qin Jin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">Multi-modal emotion estimation for in-the-wild videos.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib28.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2203.13032</em><span id="bib.bib28.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Mollahosseini et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Ali Mollahosseini, Behzad Hasani, and Mohammad H. Mahoor.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Affectnet: A database for facial expression, valence, and arousal computing in the wild.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib29.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Affective Computing</em><span id="bib.bib29.10.2" class="ltx_text" style="font-size:90%;">, 10(1):18–31, 2019.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="font-size:90%;">Nagrani et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">
Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">Attention bottlenecks for multimodal fusion.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib30.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib30.10.2" class="ltx_text" style="font-size:90%;">, 34:14200–14213, 2021.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text" style="font-size:90%;">Ortega et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">
Juan DS Ortega, Patrick Cardinal, and Alessandro L Koerich.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="font-size:90%;">Emotion recognition using fusion of audio and video features.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib31.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)</em><span id="bib.bib31.11.3" class="ltx_text" style="font-size:90%;">, pages 3847–3852. IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.4.4.1" class="ltx_text" style="font-size:90%;">Parthasarathy and Sundaram [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.6.1" class="ltx_text" style="font-size:90%;">
Srinivas Parthasarathy and Shiva Sundaram.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">Detecting expressions with multimodal transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib32.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 IEEE Spoken Language Technology Workshop (SLT)</em><span id="bib.bib32.10.3" class="ltx_text" style="font-size:90%;">, pages 636–643. IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.5.5.1" class="ltx_text" style="font-size:90%;">Poria et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">
Soujanya Poria, Iti Chaturvedi, Erik Cambria, and Amir Hussain.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">Convolutional mkl based multimodal emotion recognition and sentiment analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib33.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2016 IEEE 16th international conference on data mining (ICDM)</em><span id="bib.bib33.11.3" class="ltx_text" style="font-size:90%;">, pages 439–448. IEEE, 2016.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">Sandbach et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
Georgia Sandbach, Stefanos Zafeiriou, Maja Pantic, and Lijun Yin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">Static and dynamic 3d facial expression recognition: A comprehensive survey.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib34.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Image Vis. Comput.</em><span id="bib.bib34.10.2" class="ltx_text" style="font-size:90%;">, 30:683–697, 2012.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text" style="font-size:90%;">Stuhlsatz et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">
André Stuhlsatz, Christine Meyer, Florian Eyben, Thomas Zielke, Günter Meier, and Björn Schuller.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">Deep neural networks for acoustic emotion recognition: Raising the benchmarks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib35.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2011 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em><span id="bib.bib35.11.3" class="ltx_text" style="font-size:90%;">, pages 5688–5691. IEEE, 2011.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.5.5.1" class="ltx_text" style="font-size:90%;">Tzirakis et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">
Panagiotis Tzirakis, George Trigeorgis, Mihalis A Nicolaou, Björn W Schuller, and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="font-size:90%;">End-to-end multimodal emotion recognition using deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib36.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Journal of selected topics in signal processing</em><span id="bib.bib36.10.2" class="ltx_text" style="font-size:90%;">, 11(8):1301–1309, 2017.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text" style="font-size:90%;">Usman et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">
Muhammad Usman, Siddique Latif, and Junaid Qadir.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">Using deep autoencoders for facial expression recognition, 2018.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text" style="font-size:90%;">Vu et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">
Manh Tu Vu, Marie Beurton-Aimar, and Serge Marchand.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">Multitask multi-database emotion recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib38.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib38.11.3" class="ltx_text" style="font-size:90%;">, pages 3637–3644, 2021.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">
Lingfeng Wang, Shisen Wang, Jin Qi, and Kenji Suzuki.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">A multi-task mean teacher for semi-supervised facial affective behavior analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib39.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on computer vision</em><span id="bib.bib39.11.3" class="ltx_text" style="font-size:90%;">, pages 3603–3608, 2021.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">
Zhiwei Yang, Peng Wu, Jing Liu, and Xiaotao Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">Dynamic local aggregation network with adaptive clusterer for anomaly detection, 2022.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Zafeiriou et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
Stefanos Zafeiriou, Dimitrios D. Kollias, Mihalis A. Nicolaou, A. Papaioannou, Guoying Zhao, and Irene Kotsia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">Aff-wild: Valence and arousal ‘in-the-wild’ challenge.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib41.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em><span id="bib.bib41.10.2" class="ltx_text" style="font-size:90%;">, pages 1980–1987, 2017.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
Su Zhang, Ziyuan Zhao, and Cuntai Guan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">Multimodal continuous emotion recognition: A technical report for abaw5, 2023.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">
Wei Zhang, Zunhu Guo, Keyu Chen, Lincheng Li, Zhimeng Zhang, Yu Ding, Runze Wu, Tangjie Lv, and Changjie Fan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="font-size:90%;">Prior aided streaming network for multi-task affective analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib43.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib43.11.3" class="ltx_text" style="font-size:90%;">, pages 3539–3549, 2021.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">
Wei Zhang, Feng Qiu, Suzhen Wang, Hao Zeng, Zhimeng Zhang, Rudong An, Bowen Ma, and Yu Ding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text" style="font-size:90%;">Transformer-based multimodal information fusion for facial expression analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib44.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib44.11.3" class="ltx_text" style="font-size:90%;">, pages 2428–2437, 2022.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="font-size:90%;">
Yuanyuan Zhang, Jun Du, Zirui Wang, Jianshu Zhang, and Yanhui Tu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="font-size:90%;">Attention based fully convolutional network for speech emotion recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib45.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</em><span id="bib.bib45.11.3" class="ltx_text" style="font-size:90%;">, pages 1771–1775. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="font-size:90%;">
Yuan-Hang Zhang, Rulin Huang, Jiabei Zeng, and Shiguang Shan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.8.1" class="ltx_text" style="font-size:90%;">M 3 f: Multi-modal continuous valence-arousal estimation in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib46.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)</em><span id="bib.bib46.11.3" class="ltx_text" style="font-size:90%;">, pages 632–636. IEEE, 2020.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.12424" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.12425" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.12425">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.12425" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.12427" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 16:00:32 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
