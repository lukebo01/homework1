<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.07827] Bridging Paintings and Music – Exploring Emotion based Music Generation through Paintings</title><meta property="og:description" content="Rapid advancements in artificial intelligence have significantly enhanced generative tasks involving music and images, employing both unimodal and multimodal approaches. This research develops a model capable of genera…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bridging Paintings and Music – Exploring Emotion based Music Generation through Paintings">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Bridging Paintings and Music – Exploring Emotion based Music Generation through Paintings">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.07827">

<!--Generated on Sat Oct  5 23:47:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Music generation,  Generative AI,  Transformers,  Images,  Emotions
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Bridging Paintings and Music – Exploring Emotion based Music Generation through Paintings
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Tanisha Hisariya   Huan Zhang   Jinhua Liang 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_italic"> Queen Mary University of London</span>, Centre for Digital Music, London, United Kingdom 
<br class="ltx_break">t.hisariya@se23.qmul.ac.uk, huan.zhang@qmul.ac.uk, jinhua.liang@qmul.ac.uk
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Rapid advancements in artificial intelligence have significantly enhanced generative tasks involving music and images, employing both unimodal and multimodal approaches. This research develops a model capable of generating music that resonates with the emotions depicted in visual arts, integrating emotion labeling, image captioning, and language models to transform visual inputs into musical compositions. Addressing the scarcity of aligned art and music data, we curated the Emotion Painting Music Dataset, pairing paintings with corresponding music for effective training and evaluation. Our dual-stage framework converts images to text descriptions of emotional content and then transforms these descriptions into music, facilitating efficient learning with minimal data. Performance is evaluated using metrics such as Fréchet Audio Distance (FAD), Total Harmonic Distortion (THD), Inception Score (IS), and KL divergence, with audio-emotion text similarity confirmed by the pre-trained CLAP model to demonstrate high alignment between generated music and text. This synthesis tool bridges visual art and music, enhancing accessibility for the visually impaired and opening avenues in educational and therapeutic applications by providing enriched multi-sensory experiences.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Music generation, Generative AI, Transformers, Images, Emotions

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">”Art is not what you see but what you make others see.” - Edgar Degas. Visual art communicates information and emotions from the artist to the observer, encapsulating cultural and innovative influences from different eras. Similarly, music as an art form evokes a broad spectrum of emotions through its composition and style, paralleling the expressive power of visual arts. This paper explores the innovative intersection of these two art forms by generating music that reflects the emotions perceived in visual artworks such as paintings. This approach not only aims to make art more accessible to the visually impaired by translating visual cues into auditory signals but also extends the research frontier in AI-driven generative models conditioned on images.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Advancements in AI, particularly in generative models, have shown remarkable achievements in mimicking human creativity and generating content that aligns with user expectations across various media, leveraging deep learning techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. The models capture complex patterns from large datasets and generate outputs that often surpass traditional methods. Music generation is one significant application of AI, utilizing methods to create compositions previously thought unattainable by machines, with techniques categorized into symbolic representation and waveform generation. Symbolic generation focuses on note sequences and events, mainly used by musicians <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, while waveform generation produces continuous audio signals interpretable by a general audience, offering a broader application for everyday interactions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Waveform generation’s effectiveness heavily relies on the sampling rate to fully capture the audio structure, requiring models that can train effectively on high-dimensional datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The challenge of cross-modal generative AI, particularly converting images to music, lies in identifying relationships between these diverse modalities and the scarcity of paired data necessary for training. Despite these challenges, using transfer learning to apply a pre-trained music generation model fine-tuned to specific needs reduces the reliance on large datasets, allowing for the integration of different architectures and modalities to create efficient and seamless hybrid models. This paper aims to bridge the gap between visual and auditory arts, enhancing accessibility and merging distinct forms of artistic expression. The contribution of this work is summarized as follows:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We proposed a visual-guided music synthesis system that generates music by interpreting the emotions conveyed by images.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Our framework is decomposed into image-to-text and text-to-music tasks, facilitating efficient learning with minimal data. We further enhance training efficiency by exclusively training the decoder in the latent space.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We explore the influence of text descriptions by employing diverse textual conditions. To this end, we have curated for both training and evaluation purposes the Emotion Painting Music Dataset, which is publicly available at this URL <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://zenodo.org/records/13717256</span></span></span>.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our generated music is qualitatively
measured across various metrics using the Fréchet Audio
Distance (FAD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, Total Harmonic Distortion (THD), Inception Score
(IS), and KL divergence. Audio-emotion text similarity has also been measured by pre-trained CLAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> model to demonstrate high alignment between generated music and text. This tool, bridging art and music, holds promise for enhancing learning experiences in educational environments or therapeutic contexts, providing unique multi-sensory engagements.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This research covers three key areas in generative AI: image feature extraction and conversion, deep learning techniques for music generation, and multi-modality in audio and music generation.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">
Image feature extraction and conversion</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Feature extraction from images can utilize either unimodal or multimodal approaches. The unimodal approach offers straightforward single-label classifications, while multimodal strategies employ language models like LLMs to process visual representations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Deep neural networks (DNN)s, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, laid the groundwork for the unimodal strategy by categorizing an image to a fixed set of labels. Subsequently, DNNs have been proven to yield the promising performance in other fields, such audio and graphs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The evolution of image captioning leveraged these CNN architectures alongside natural language processing, significantly advanced by the introduction of contrastive learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.Contrastive Language-Image Pretraining (CLIP) enhanced the alignment between textual and visual data using dual encoders to calculate cosine similarities between text and image vectors. Following CLIP, BLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> further refined the model by integrating a flexible multimodal encoder-decoder framework that excelled in both understanding and generating visual-language tasks with high accuracy. The subsequent introduction of Multimodal large language model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> marked a major advancement, optimizing the handling of more complex contextual and visual information, setting new standards for image captioning capabilities.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Deep Learning methods for Music Generation</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The use of Recurrent Neural Networks (RNNs) to generate musical melodies began with Todd <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, but their limited memory capacity led to the development of Long Short-Term Memory (LSTM) units <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, which improved the ability to retain musical sequences over time. The evolution continued with models incorporating Restricted Boltzmann Machines (RBM) for better melody generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, although challenges in long-term memory persisted until Google introduced advancements in RNNs for music with enhanced dependency handling in 2016.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The introduction of generative architectures such as Convolutional Neural Networks (CNNs), Variational Auto Encoders (VAEs), and Generative Adversarial Networks (GANs) marked significant progress. MusicVAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and MidiNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, a CNN-based GAN, expanded capabilities for creating coherent musical sequences, although GANs sometimes suffered from mode collapse. MuseGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> further advanced multi-track music generation, improving the coherence of generated compositions. The advent of WaveNet by Google in 2016 introduced a CNN-based model that employed an autoregressive approach for dynamic raw audio generation, heavily used in both text-to-speech and music generation tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Significant advances were also seen with the introduction of Music Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, which utilized transformers and relative attention models to enhance long-term music generation, surpassing earlier RNN models in efficiency but encountering issues with note redundancy. MuseNet in 2019 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and later developments like seqGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and models by Jacek and Teodara using Conditional VAEs and RNNs for emotion-driven music generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> highlighted the ongoing challenges of computational demands and complex architecture interpretability while pushing the boundaries of music generation technology.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Multi-Modality Audio and Music generation</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">In 2022, cMelGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, a conditional GAN model based on Mel Spectrogram, was introduced to improve music generation efficiency, though it faced challenges with GAN training instability. JukePix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, a model for converting paintings to music using Convolutional GANs, showed potential in generating multi-track music but was limited by solo performance evaluations. Chen et al. developed MusicLDM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, a complex model integrating CLAP, VAE, Hifi-GAN, and diffusion models, excelling in music generation but constrained by data sampling rates and computational resources.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Further advancements include AudioLDM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, which uses CLAP embeddings and a latent diffusion model to achieve high-quality audio generation. Building on this, AudioLDM 2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> leveraged GPT-2 to handle various input modalities, showing significant improvements in accuracy. Mousai <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> and MusicLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> further explored text-conditioned music generation, with MusicLM providing consistent output despite challenges in processing complex text structures.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">MeLoDy by Lam et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, used a dual-path diffusion model combined with a language model to enhance semantic modeling and music generation. Despite its innovative approach, training data biases limited its diversity. Sheffer and Adi’s im2wav model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, based on a transformer architecture and CLIP, aimed to generate high-fidelity audio from image inputs but faced computational inefficiencies. Recently, Chowdhury et al. introduced MelFusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, synthesizing music from images and text via advanced deep-learning diffusion models, setting new benchmarks in performance and opening avenues for further research in cross-modal generative AI.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Method</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The method of generating music from images based on
emotions consists of integrating deep learning models.
Firstly, we will convert the images into their textual
format, and then text along with its associated music will be
used to fine-tune the MusicGen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> model to
generate the required music. By doing this, we are not only
generating the music conditioned on emotions from images,
but we are also exploring the effect of various textual
descriptions during musical generation. In this research
paper, we are exploring four models: an emotion labeling
model, an image description model, a large language model,
and a Music Generation model. The overview of our
methodology can be seen in Figure 2.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Image Emotion Labelling Model</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To effectively perceive the emotions of images, we have
introduced a classification model to label the emotions. The
model will play an important part in determining the
emotions during inference, helping to improve the
consistency and relevance of generated music. Pre-trained
ImageNet ResNet50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> has been chosen as the best model
for this approach due to its ability to manage diverse and
complex datasets with dense layers. Leveraging the technique
of transfer learning, the model has been adapted to a given
dataset with the additional two GRU layers along with a
multi-head Attention layer before the fully connected layer.
The output layer of ResNet50 architecture has been flattening
out to meet the output classes of the dataset. Further, to enhance the performance and to prevent overfitting, some
dropping out of non-essential neurons have been
incorporated before fully connected layers.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2409.07827/assets/asset/fig2.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="110" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.3.2" class="ltx_text" style="font-size:90%;">Architecture of Emotion Labelling model with pretrained ResNet50 and additional two bidirectional GRU and one Attention layer proceeding with dropout along fully connected layer.</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Image Description Model</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The Image Captioning Model is very crucial as it is responsible for generating the captions of images reflecting emotions perceived by them. By doing this, we aimed to enhance the description of images by extracting more word tokens. We employed BLIP [11], a current state-of-the-art model for Image Captioning, due to its superior performance in generating diverse and descriptive texts aligning closely with visual information.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The model is being conditioned on the emotion labels obtained from the emotion classifier enabling it to give better relevant emotional descriptions. The model is trained on a large amount of highly diversified data so we can directly incorporate the BLIP Large Captioning model to generate the caption.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2409.07827/assets/asset/fig3.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="191" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.14.5.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.8.4" class="ltx_text" style="font-size:90%;">Overview architecture of our model , which encompasses working flow of all the four models represented as different colour giving the output as MS-G<sub id="S3.F2.8.4.1" class="ltx_sub"><span id="S3.F2.8.4.1.1" class="ltx_text ltx_font_italic">E</span></sub> (single label text from ResNet50+ MusicGen), MS-G<sub id="S3.F2.8.4.2" class="ltx_sub"><span id="S3.F2.8.4.2.1" class="ltx_text ltx_font_italic">N</span></sub> (image descritive text from BLIP+MusicGen), MS-G<sub id="S3.F2.8.4.3" class="ltx_sub"><span id="S3.F2.8.4.3.1" class="ltx_text ltx_font_italic">L</span></sub> (Enhanced description from Falcon+ MusicGen), MS-G<sub id="S3.F2.8.4.4" class="ltx_sub"><span id="S3.F2.8.4.4.1" class="ltx_text ltx_font_italic">O</span></sub> (Enhanced description from Falcon+enhanced finetuning method of MusicGen).</span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">LLM model</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">This model plays an integral role in the evolution of visual and musical information. It further enhances the description generated by the captioning model by incorporating some musical terms that reflect the mood, themes, and musical understanding terms that are very useful for generating the music. There is a need to enhance the description because the Image Captioning Model gives us descriptions based on image features such as objects, colors, and more, while the models for generating music need some music related component details in that to optimally perform. Providing this type of description leads to a better quality and resemblance of music. That’s why we are integrating the LLM model into our framework to fill the gap between the provided description and the expected inputs. The model not only works with the optimization of description but also ensures that the information about the visual image, especially the essence of emotion, is not lost.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The LLM model we have incorporated here is FalconRW-1B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> because it is fully open-source, and able to perform well even with restricted computational power. It consists of language modelling decoder-based architecture that is only incorporated with many advanced techniques like Attention. The input requirement of this model is characterized into three parts as shown in Table 1: system message (intent to tell the behavior of the model), instructions (intent to give the proper input), and response (the response model is providing).</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.2.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S3.T1.3.2" class="ltx_text" style="font-size:90%;">Description of input format given to falcon 1B model, emphasizing the system message and instruction, followed by a description provided.</span></figcaption>
<table id="S3.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.4.1.1" class="ltx_tr">
<th id="S3.T1.4.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.4.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.1.1.1.1.1" class="ltx_p" style="width:56.9pt;">Role</span>
</span>
</th>
<th id="S3.T1.4.1.1.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.4.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.1.1.2.1.1" class="ltx_p" style="width:184.9pt;">Content</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.4.2.1" class="ltx_tr">
<td id="S3.T1.4.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.4.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.2.1.1.1.1" class="ltx_p" style="width:56.9pt;">System Message</span>
</span>
</td>
<td id="S3.T1.4.2.1.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.4.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.2.1.2.1.1" class="ltx_p" style="width:184.9pt;">You are an enhanced description generator. You will be given with image description and you have to enhance those in musical terms.</span>
</span>
</td>
</tr>
<tr id="S3.T1.4.3.2" class="ltx_tr">
<td id="S3.T1.4.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T1.4.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.3.2.1.1.1" class="ltx_p" style="width:56.9pt;">Instruction</span>
</span>
</td>
<td id="S3.T1.4.3.2.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S3.T1.4.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.3.2.2.1.1" class="ltx_p" style="width:184.9pt;">Generate a musical theme description for the following image description: ”<span id="S3.T1.4.3.2.2.1.1.1" class="ltx_text ltx_font_italic">sad man in a sailor’s hat sitting at a table</span>”. Include details like mood, genre, tempo, and melody in 2 lines.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic"> Music Generation </span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The MusicGen-small model, abbreviated as MG-S, was fine-tuned for generating music based on various textual inputs derived from corresponding image-to-text models and audio files. Text and audio files were encoded into tokens using specialized encoder models, followed by a conditioning and fusion process involving an Attention mechanism. This was further processed by a transformer model using masking techniques to generate relevant tokens for loss computation and parameter refinement.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Our experimental process involved several iterations of the MG-S model to enhance music generation capabilities:</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p"><span id="S3.SS4.p3.1.1" class="ltx_text ltx_font_bold">MG-S Emotive</span>: Utilized emotion descriptions from the Image Emotion Labeling model to adjust the model’s parameters.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p"><span id="S3.SS4.p4.1.1" class="ltx_text ltx_font_bold">MG-S Narrative</span>: Conditioned the music generation on image descriptions enriched with emotional cues, with comprehensive parameter tuning in the model.</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<p id="S3.SS4.p5.1" class="ltx_p"><span id="S3.SS4.p5.1.1" class="ltx_text ltx_font_bold">MG-S Lyrical</span>: Enhanced the descriptive content of images using a Language Model that incorporates musical knowledge, thoroughly fine-tuning the model.</p>
</div>
<div id="S3.SS4.p6" class="ltx_para">
<p id="S3.SS4.p6.1" class="ltx_p"><span id="S3.SS4.p6.1.1" class="ltx_text ltx_font_bold">MG-S Optimized</span>: Introduced architectural improvements to optimize model performance. These improvements included pre-processing music and text files prior to training to stabilize and streamline the training process. The approach involved storing precomputed tensors, freezing initial layers to enhance stability, and modifying input handling for consistent learning. This version also adjusted how input descriptions were managed to ensure accurate performance evaluation on our dataset.</p>
</div>
<div id="S3.SS4.p7" class="ltx_para">
<p id="S3.SS4.p7.1" class="ltx_p">These versions of MG-S represent a progression in the methodical enhancement of music generation, conditioned on textual and emotional cues, leading to a refined and efficient training methodology.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Datasets</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Data collection and preparation are elementary steps in developing any model. Due to the lack of an existing paired dataset encompassing both art and music, which share the same emotional attribute, we proposed to make our own bespoke paired dataset integrating two different art forms, painting and music, while conveying the same emotion.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">For the image paintings dataset, we utilized WIKIART EMOTION DATASET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, an openly available dataset of wiki art depicting various emotions. Wikiart is a collection of various paintings that evolved from different eras of the past to present, symbolizing different art forms and meanings. These paintings have been analyzed and further categorized into more than ten emotions in the emotion dataset. Based on these, we have manually analyzed datasets for five particular emotions - Happy, Angry, Sad, Fun, and Neutral and collected 1200 such paintings, which will serve as one part of our paired dataset. The manual selection process depicts the accurate representation of each dataset conveying the emotion.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">The next part contains the collection of music datasets that should convey the same emotion. For this purpose, we selected MIREX EMOTION DATASET, a dataset of 193 MIDI files of music depicting various emotions. Initially, we preprocess the musical dataset from its raw form of MIDI and convert it into .wav form at 32KHz making them compatible frequency for the MusicGen model. We further combined the emotional aspects of several parts to categorize it into five similar emotions as with paintings finally. Furthermore, as we are using a fixed 30s audio segment chunk in our music generation model, the music has been trimmed into various 30s without any overlapping between two different audios. By doing these preprocessing steps, we can generate more music samples that are uniquely identified from each other.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Later, to form the final paired dataset, the music and paintings have been associated with each other randomly, depicting the same emotions. In this way, we can have 1200 different pairs of paintings and music depicting five different emotions. Furthermore, for the training purpose of the model, we took around 80% of data from each section of emotion, with the remaining 20% split evenly between evaluation and testing sets.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Evaluation metrics</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To evaluate the quality and resemblance of generated music, we used a set of objective evaluation metrics to measure quality, smoothness, noise, and distortion in the generated music:</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Frechet Audio Distance (FAD)</span>.
FAD, inspired by the Frechet Inception Distance, measures the similarity between the statistical distributions of generated and reference music sets using the VGGish model for feature extraction. A lower FAD score indicates greater similarity to the reference set.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Contrastive Language Audio Pretraining (CLAP)</span>.
CLAP score calculates the similarity between text descriptions and audio using embeddings from the pre-trained LAION CLAP model’s text and audio encoders, computing cosine similarity between them. A higher CLAP score indicates better alignment between text and audio.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">Total Harmonic Distortion (THD) score</span>.
THD measures the harmonic distortion present in the generated music, assessing the purity of the audio signal. Lower THD scores signify less distortion and higher audio quality.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">Inception score (ISc)</span>.
IS reflects the variety and diversity of the generated audio group. A higher Isc indicates a diverser distribution of synthesise music.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p"><span id="S4.SS2.p6.1.1" class="ltx_text ltx_font_bold">Kullback-Leibler (KL) divergence</span>.
KL divergence quantifies the difference between the probability distributions of reference and generated features. A lower KL score indicates closer resemblance between two distributions, suggesting better generation fidelity.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Training</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">All our experiments were performed on one NVIDIA A40 GPU with 48 GB of memory. This setup allowed us to perform training with a batch size of 16, using the small version of the MusicGen model over 40 epochs. We used an AdamW optimizer with early stopping and a learning rate of 1e-5, a cosine scheduler with warmup steps of 100. During inference, we took the <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="\text{top}_{k}" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><msub id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mtext id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2a.cmml">top</mtext><mi id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p1.1.m1.1.1.2a.cmml" xref="S4.SS3.p1.1.m1.1.1.2"><mtext id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">top</mtext></ci><ci id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\text{top}_{k}</annotation></semantics></math> as 250, selecting the top 250 most resembling audio tokens at a temperature of 1.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Results</span>
</h3>

<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.8.1.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S4.T2.9.2" class="ltx_text" style="font-size:90%;">Objective comparison of test set data for emotion based image to music generation across all the models. Here the best results are made <span id="S4.T2.9.2.1" class="ltx_text ltx_font_bold">bold</span>.</span></figcaption>
<table id="S4.T2.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.5.5" class="ltx_tr">
<th id="S4.T2.5.5.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Model</th>
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">FAD<math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mo stretchy="false" id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">CLAP<math id="S4.T2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T2.3.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">KL<math id="S4.T2.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.3.3.3.m1.1a"><mo stretchy="false" id="S4.T2.3.3.3.m1.1.1" xref="S4.T2.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T2.4.4.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">THD<math id="S4.T2.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.4.4.4.m1.1a"><mo stretchy="false" id="S4.T2.4.4.4.m1.1.1" xref="S4.T2.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.m1.1b"><ci id="S4.T2.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T2.5.5.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">ISc<math id="S4.T2.5.5.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.5.5.5.m1.1a"><mo stretchy="false" id="S4.T2.5.5.5.m1.1.1" xref="S4.T2.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.m1.1b"><ci id="S4.T2.5.5.5.m1.1.1.cmml" xref="S4.T2.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.5.6.1" class="ltx_tr">
<td id="S4.T2.5.6.1.1" class="ltx_td ltx_align_left ltx_border_t">MG-S Emotive</td>
<td id="S4.T2.5.6.1.2" class="ltx_td ltx_align_left ltx_border_t">7.02</td>
<td id="S4.T2.5.6.1.3" class="ltx_td ltx_align_left ltx_border_t">0.075</td>
<td id="S4.T2.5.6.1.4" class="ltx_td ltx_align_left ltx_border_t">0.054</td>
<td id="S4.T2.5.6.1.5" class="ltx_td ltx_align_left ltx_border_t">1.79</td>
<td id="S4.T2.5.6.1.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.5.6.1.6.1" class="ltx_text ltx_font_bold">1.044</span></td>
</tr>
<tr id="S4.T2.5.7.2" class="ltx_tr">
<td id="S4.T2.5.7.2.1" class="ltx_td ltx_align_left">MG-S Narrative</td>
<td id="S4.T2.5.7.2.2" class="ltx_td ltx_align_left">5.22</td>
<td id="S4.T2.5.7.2.3" class="ltx_td ltx_align_left">0.096</td>
<td id="S4.T2.5.7.2.4" class="ltx_td ltx_align_left">0.045</td>
<td id="S4.T2.5.7.2.5" class="ltx_td ltx_align_left"><span id="S4.T2.5.7.2.5.1" class="ltx_text ltx_font_bold">1.73</span></td>
<td id="S4.T2.5.7.2.6" class="ltx_td ltx_align_left">1.032</td>
</tr>
<tr id="S4.T2.5.8.3" class="ltx_tr">
<td id="S4.T2.5.8.3.1" class="ltx_td ltx_align_left">MG-S Lyrical</td>
<td id="S4.T2.5.8.3.2" class="ltx_td ltx_align_left"><span id="S4.T2.5.8.3.2.1" class="ltx_text ltx_font_bold">5.06</span></td>
<td id="S4.T2.5.8.3.3" class="ltx_td ltx_align_left">0.11</td>
<td id="S4.T2.5.8.3.4" class="ltx_td ltx_align_left">0.046</td>
<td id="S4.T2.5.8.3.5" class="ltx_td ltx_align_left">1.92</td>
<td id="S4.T2.5.8.3.6" class="ltx_td ltx_align_left">1.031</td>
</tr>
<tr id="S4.T2.5.9.4" class="ltx_tr">
<td id="S4.T2.5.9.4.1" class="ltx_td ltx_align_left ltx_border_bb">MG-S Optimized</td>
<td id="S4.T2.5.9.4.2" class="ltx_td ltx_align_left ltx_border_bb">5.54</td>
<td id="S4.T2.5.9.4.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T2.5.9.4.3.1" class="ltx_text ltx_font_bold">0.13</span></td>
<td id="S4.T2.5.9.4.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T2.5.9.4.4.1" class="ltx_text ltx_font_bold">0.012</span></td>
<td id="S4.T2.5.9.4.5" class="ltx_td ltx_align_left ltx_border_bb">1.75</td>
<td id="S4.T2.5.9.4.6" class="ltx_td ltx_align_left ltx_border_bb">1.033</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The architectural experiments with the four variants of the MusicGen model, as reflected in Table <a href="#S4.T2" title="TABLE II ‣ IV-D Results ‣ IV Experiments ‣ Bridging Paintings and Music – Exploring Emotion based Music Generation through Paintings" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, offer detailed insights into each model’s performance improvements and challenges. MG-S Emotive, our baseline model, uses ResNet50 and GRU to extract single-word emotion labels, demonstrating limitations with high FAD and KL scores, and low CLAP scores indicating poor text-to-music alignment and outputs with significant noise. MG-S Narrative advances this by using the BLIP model for richer emotional captioning, improving text-music alignment as seen in the higher CLAP scores and reduced noise, though it struggles with complex emotions like anger. MG-S Lyrical incorporates an LLM to enrich musical context in text descriptions, enhancing semantic appropriateness and improving FAD and CLAP scores but facing challenges in balancing complexity with fidelity as indicated by the KL and THD scores. Finally, MG-S Optimized integrates enriched contextual descriptions with a modified tuning pipeline, significantly reducing training times and achieving the highest CLAP scores while minimizing distortion and noise, demonstrating the most effective architecture in complex emotional contexts as evidenced by the spectrogram analysis. These progressive refinements highlight the enhanced capabilities of MusicGen in generating high-fidelity music aligned with complex textual descriptions.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2409.07827/assets/asset/fig4.jpeg" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="359" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">CLAP analysis of generated song across model with their emotions. It is being meausred with providing “emotion song” as text during CLAP calculation.</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This work introduced a generative model to produce music conditioned on the emotions depicted in paintings, serving as a step towards integrating visual art and music through technology. The model demonstrates the feasibility of using generative AI to create emotionally resonant music, addressing a notable gap in modality conversion. Our evaluation focused on the quality, diversity, and presence of noise in the generated music, highlighting the discrepancies between ideal model inputs and typical user-provided data. Addressing these discrepancies is crucial for improving output quality.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">The research also points out the limited availability of datasets appropriate for training art-music generation models and suggests enhancing dataset diversity for better model training. The findings reveal significant gaps in how the models interpret single-label and non-musical descriptions compared to user expectations, underscoring the need for more sophisticated handling of input data. Moreover, the study identifies the high inference time of the model as a challenge for real-time application, suggesting further optimization is needed. Future work should explore developing specific evaluation metrics tailored to this multimodal context to enhance the precision of assessments, potentially advancing the field of text-conditioned generative models.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J.-P. Briot, “From artificial neural networks to deep learning for music generation: history, concepts and trends,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Neural Computing and Applications</em>, vol. 33, no. 1, pp. 39–65, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
X. Liu, Z. Zhu, H. Liu, Y. Yuan, M. Cui, Q. Huang, J. Liang, Y. Cao, Q. Kong, M. D. Plumbley <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Wavjourney: Compositional audio creation with large language models,” <em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.14335</em>, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Liang, H. Zhang, H. Liu, Y. Cao, Q. Kong, X. Liu, W. Wang, M. D. Plumbley, H. Phan, and E. Benetos, “Wavcraft: Audio editing and generation with large language models,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.09527</em>, 2024.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
H. Vinet, “The representation levels of music information,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International Symposium on Computer Music Modeling and Retrieval</em>.   Springer, 2003, pp. 193–209.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
H. Zhang, S. Chowdhury, C. E. Cancino-Chacón, J. Liang, S. Dixon, and G. Widmer, “Dexter: Learning and controlling performance expression with diffusion models,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, vol. 14, no. 15, p. 6543, 2024.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
H. Liu, Y. Yuan, X. Liu, X. Mei, Q. Kong, Q. Tian, Y. Wang, W. Wang, Y. Wang, and M. D. Plumbley, “Audioldm 2: Learning holistic audio generation with self-supervised pretraining,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Trans. Audio, Speech and Lang. Proc.</em>, vol. 32, p. 2871–2883, may 2024.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y. Yuan, H. Liu, J. Liang, X. Liu, M. D. Plumbley, and W. Wang, “Leveraging pre-trained audioldm for sound generation: A benchmark study,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">2023 31st European Signal Processing Conference (EUSIPCO)</em>.   IEEE, 2023, pp. 765–769.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
P. Colarusso, L. H. Kidder, I. W. Levin, and E. N. Lewis, “Raman and infrared microspectroscopy,” 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
K. Kilgour, M. Zuluaga, D. Roblek, and M. Sharifi, “Frechet audio distance: A metric for evaluating music enhancement algorithms,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.08466</em>, 2018.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
B. Elizalde, S. Deshmukh, M. Al Ismail, and H. Wang, “Clap learning audio concepts from natural language supervision,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J. Li, D. Li, C. Xiong, and S. Hoi, “Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.   PMLR, 2022, pp. 12 888–12 900.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. Liang, X. Liu, H. Liu, H. Phan, E. Benetos, M. D. Plumbley, and W. Wang, “Adapting language-audio models as few-shot audio learners,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.17719</em>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2016, pp. 770–778.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J. Liang, I. Nolasco, B. Ghani, H. Phan, E. Benetos, and D. Stowell, “Mind the domain gap: a systematic analysis on bioacoustic sound event detection,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2403.18638</em>, 2024.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
B. Ding, T. Zhang, C. Wang, G. Liu, J. Liang, R. Hu, Y. Wu, and D. Guo, “Acoustic scene classification: a comprehensive survey,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Expert Systems with Applications</em>, p. 121902, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
H. Zhang, E. Karystinaios, S. Dixon, G. Widmer, and C. E. Cancino-Chacón, “Symbolic music representations for classification tasks: A systematic evaluation,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceeding of the 24th International Society on Music Information Retrieval (ISMIR)</em>, Milan, Italy, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Learning transferable visual models from natural language supervision,” in <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.   PMLR, 2021, pp. 8748–8763.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Gpt-4 technical report,” <em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J. Liang, X. Liu, W. Wang, M. D. Plumbley, H. Phan, and E. Benetos, “Acoustic prompt tuning: Empowering large language models with audition capabilities,” 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
H. Liu, C. Li, Y. Li, and Y. J. Lee, “Improved baselines with visual instruction tuning,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2024, pp. 26 296–26 306.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
P. M. Todd, “A connectionist approach to algorithmic composition,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Computer Music Journal</em>, vol. 13, no. 4, pp. 27–43, 1989.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
S. Hochreiter, “Long short-term memory,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Neural Computation MIT-Press</em>, 1997.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S. van Herwaarden, M. Grachten, W. de Haas, and W. Bas de Haas, “Predicting expressive dynamics in piano performances using neural networks,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Society for Music Information Retrieval Conference (ISMIR)</em>, 2014.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A. Roberts, J. Engel, C. Raffel, C. Hawthorne, and D. Eck, “A hierarchical latent vector model for learning long-term structure in music,” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.   PMLR, 2018, pp. 4364–4373.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
L.-C. Yang, S.-Y. Chou, and Y.-H. Yang, “Midinet: A convolutional generative adversarial network for symbolic-domain music generation,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1703.10847</em>, 2017.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
H.-W. Dong, W.-Y. Hsiao, L.-C. Yang, and Y.-H. Yang, “Musegan: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 32, no. 1, 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
A. Van Den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, K. Kavukcuoglu <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Wavenet: A generative model for raw audio,” <em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1609.03499</em>, vol. 12, 2016.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, N. Shazeer, I. Simon, C. Hawthorne, A. M. Dai, M. D. Hoffman, M. Dinculescu, and D. Eck, “Music transformer,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference of Learning Representations (ICLR)</em>, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
C. Payne, “Musenet,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">OpenAI Blog</em>, vol. 3, 2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
L. Yu, W. Zhang, J. Wang, and Y. Yu, “Seqgan: Sequence generative adversarial nets with policy gradient,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI conference on artificial intelligence</em>, vol. 31, no. 1, 2017.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
J. Mycka, A. Żychowski, and J. Mańdziuk, “Toward human-level tonal and modal melody harmonizations,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Journal of Computational Science</em>, vol. 67, no. October 2022, p. 101963, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
T. Qian, J. Kaunismaa, and T. Chung, “Cmelgan: An efficient conditional generative model based on mel spectrograms,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.07319</em>, 2022.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
X. Wang, Z. Gao, H. Qian, and Y. Xu, “Jukepix: A cross-modality approach to transform paintings into music segments,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">2018 IEEE International Conference on Robotics and Biomimetics (ROBIO)</em>.   IEEE, 2018, pp. 456–461.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
K. Chen, Y. Wu, H. Liu, M. Nezhurina, T. Berg-Kirkpatrick, and S. Dubnov, “Musicldm: Enhancing novelty in text-to-music generation using beat-synchronous mixup strategies,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2024, pp. 1206–1210.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, and M. D. Plumbley, “AudioLDM: Text-to-audio generation with latent diffusion models,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine Learning (ICML)</em>, Hawaii, USA, 2023.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
F. Schneider, O. Kamal, Z. Jin, and B. Schölkopf, “Mo<math id="bib.bib36.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="bib.bib36.1.m1.1a"><mo id="bib.bib36.1.m1.1.1" xref="bib.bib36.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="bib.bib36.1.m1.1b"><ci id="bib.bib36.1.m1.1.1.cmml" xref="bib.bib36.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib36.1.m1.1c">\backslash</annotation></semantics></math>^ usai: Text-to-music generation with long-context latent diffusion,” <em id="bib.bib36.2.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.11757</em>, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Musiclm: Generating music from text,” <em id="bib.bib37.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.11325</em>, 2023.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
M. W. Lam, Q. Tian, T. Li, Z. Yin, S. Feng, M. Tu, Y. Ji, R. Xia, M. Ma, X. Song <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Efficient neural music generation,” <em id="bib.bib38.2.2" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 36, 2024.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
R. Sheffer and Y. Adi, “I hear your true colors: Image guided audio generation,” in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
S. Chowdhury, S. Nag, K. Joseph, B. V. Srinivasan, and D. Manocha, “Melfusion: Synthesizing music from image and language cues using diffusion models,” in <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2024, pp. 26 826–26 835.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. Défossez, “Simple and controllable music generation,” no. i, 2023.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, H. Alobeidli, A. Cappelli, B. Pannier, E. Almazrouei, and J. Launay, “The refinedweb dataset for falcon LLM: Outperforming curated corpora with web data only,” in <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track</em>, 2023. [Online]. Available: https://openreview.net/forum?id=kM5eGcdCzq

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
S. Mohammad and S. Kiritchenko, “Wikiart emotions: An annotated dataset of emotions evoked by art,” in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the eleventh international conference on language resources and evaluation (LREC 2018)</em>, 2018.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.07826" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.07827" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.07827">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.07827" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.07828" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 23:47:43 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
