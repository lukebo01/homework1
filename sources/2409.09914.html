<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.09914] A Study on Zero-shot Non-intrusive Speech Assessment using Large Language Models</title><meta property="og:description" content="This work investigates two strategies for zero-shot non-intrusive speech assessment leveraging large language models. First, we explore the audio analysis capabilities of GPT-4o. Second, we propose GPT-Whisper, which u…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Study on Zero-shot Non-intrusive Speech Assessment using Large Language Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Study on Zero-shot Non-intrusive Speech Assessment using Large Language Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.09914">

<!--Generated on Sat Oct  5 21:32:36 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
speech assessment,  zero-shot,  non-intrusive,  whisper,  ChatGPT,  large language model
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Study on Zero-shot Non-intrusive Speech Assessment using Large Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ryandhimas E. Zezario
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">Academia Sinica
<br class="ltx_break"></span>Taipei, Taiwan 
<br class="ltx_break">ryandhimas@citi.sinica.edu.tw
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sabato M. Siniscalchi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.1.id1" class="ltx_text ltx_font_italic">University of Palermo
<br class="ltx_break"></span>Sicily, Italy 
<br class="ltx_break">sabatomarco.siniscalchi@unipa.it
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hsin-Min Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_font_italic">Academia Sinica
<br class="ltx_break"></span>Taipei, Taiwan 
<br class="ltx_break">whm@iis.sinica.edu.tw
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yu Tsao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_font_italic">Academia Sinica
<br class="ltx_break"></span>Taipei, Taiwan 
<br class="ltx_break">yu.tsao@citi.sinica.edu.tw
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">This work investigates two strategies for zero-shot non-intrusive speech assessment leveraging large language models. First, we explore the audio analysis capabilities of GPT-4o. Second, we propose GPT-Whisper, which uses Whisper as an audio-to-text module and evaluates the text’s naturalness via targeted prompt engineering. We evaluate assessment metrics predicted by GPT-4o and GPT-Whisper examining their correlations with human-based quality and intelligibility assessments, and character error rate (CER) of automatic speech recognition. Experimental results show that GPT-4o alone is not effective for audio analysis; whereas, GPT-Whisper demonstrates higher prediction, showing moderate correlation with speech quality and intelligibility, and high correlation with CER. Compared to supervised non-intrusive neural speech assessment models, namely MOS-SSL and MTI-Net, GPT-Whisper yields a notably higher Spearman’s rank correlation with Whisper’s CER. These findings validate GPT-Whisper as a reliable method for accurate zero-shot speech assessment without requiring additional training data (speech data and corresponding assessment scores).</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
speech assessment, zero-shot, non-intrusive, whisper, ChatGPT, large language model

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Speech assessment metrics play a critical role in evaluating a variety of speech-related applications, including speech enhancement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, hearing aid (HA) devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, and telecommunications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. With the advancement of deep learning and the need for accurate non-intrusive speech assessment metrics, researchers have increasingly adopted deep learning models for speech assessment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. To achieve accurate automatic assessments, various strategies have been explored, such as reducing listener bias <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, integrating large pre-trained models (e.g., self-supervised learning (SSL) models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, and speech language models<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>), utilizing ensemble learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, and incorporating pseudo labels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Despite significant performance improvements, achieving satisfactory generalization with limited training samples remains challenging.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recently, with the development of large-scale conversational agents like ChatGPT, there has been an increasing interest in evaluating their ability to perform more extensive reasoning and understanding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. The capabilities of GPT-4, especially its latest extension GPT-4o <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://platform.openai.com/docs/models/gpt-4o</span></span></span>, have been significantly expanded to enable not only advanced text understanding but also multimodal integration, such as merging visual and audio understanding. In the case of image understanding, ChatGPT has been successfully integrated with visual language models to perform tasks such as deep fake detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. For audio understanding, a noteworthy integration is AudioGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. This approach integrates ChatGPT with multiple pre-trained audio models. Based on specific prompt input, the most appropriate audio model is selected to generate the response. While AudioGPT excels at integrating audio understanding with prompt engineering, it does not cover speech assessment. Given the interest in reliable non-intrusive speech assessment with minimal training samples, we intend to investigate whether ChatGPT can effectively perform speech assessment in a zero-shot setting and identify optimal strategies to ensure accurate and unbiased results.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we explore two strategies for zero-shot non-intrusive speech assessment by leveraging large language models (LLM). First, we directly leverage the audio analysis capabilities of GPT-4o for speech assessment. Second, we propose a more advanced approach, namely GPT-Whisper. Specifically, we use Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> as an audio-to-text module to create text representations, which are then assessed using targeted prompt engineering focused on evaluating the naturalness of the predicted text. To the best of our knowledge, this is the first attempt to leverage ChatGPT for speech assessment. Furthermore, to gain a deeper insight into our new assessment metric computed by GPT-Whisper, we also evaluate its correlation with several metrics, including human-based quality assessment, human-based intelligibility assessment, and the character error rate (CER) of automatic speech recognition (ASR) models. We use two open-source ASR models: Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, and Google ASR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> to generate CER scores. Finally, we compare our approach with two supervised neural speech assessment models: MOS-SSL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and MTI-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our experimental results first confirm that using ChatGPT alone for audio analysis is not optimal because it mainly relies on factors such as amplitude range, standard deviation, and signal-to-noise ratio (SNR) to assess the quality or intelligibility of audio data. Next, in terms of Spearman’s rank correlation coefficient (SRCC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, our GPT-Whisper metric yields moderate correlation with human-based quality assessment (0.4360), human-based intelligibility assessment (0.5485), and Google ASR’s CER (0.5415) and high correlation with Whisper’s CER (0.7784). This further validates our new metric as a viable assessment for evaluating speech in a zero-shot manner, especially given its higher correlation with CER. Finally, when comparing GPT-Whisper with MOS-SSL and all variants of MTI-Net (wav2vec and Whisper) in predicting Whisper’s CER, experimental results show that GPT-Whisper outperforms them in terms of SRCC (0.7784 vs. 0.7482, 0.7418, and 0.7655).</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The remainder of this paper is organized as follows. Section <a href="#S2" title="II Methodology ‣ A Study on Zero-shot Non-intrusive Speech Assessment using Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> presents the proposed methodology. Section <a href="#S3" title="III Experiments ‣ A Study on Zero-shot Non-intrusive Speech Assessment using Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> describes the experimental setup and results. Finally, Section <a href="#S4" title="IV Conclusions ‣ A Study on Zero-shot Non-intrusive Speech Assessment using Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> presents the conclusions and future work.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.09914/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="181" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Zero-shot speech assessment with GPT-4o.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">GPT-4o for speech assessment</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.3" class="ltx_p">The main intention behind using GPT-4o for speech assessment is to validate ChatGPT’s reasoning capabilities in understanding speech characteristics. The overall framework for zero-shot speech assessment using GPT-4o is shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ A Study on Zero-shot Non-intrusive Speech Assessment using Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Specifically, given an audio input <span id="S2.SS1.p1.3.1" class="ltx_text ltx_markedasmath ltx_font_italic">A</span> and a prompt <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="P" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">P</annotation></semantics></math>, ChatGPT utilizes this information to estimate the assessment metric <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="S_{GPT-4o}" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><msub id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><mi id="S2.SS1.p1.3.m3.1.1.2" xref="S2.SS1.p1.3.m3.1.1.2.cmml">S</mi><mrow id="S2.SS1.p1.3.m3.1.1.3" xref="S2.SS1.p1.3.m3.1.1.3.cmml"><mrow id="S2.SS1.p1.3.m3.1.1.3.2" xref="S2.SS1.p1.3.m3.1.1.3.2.cmml"><mi id="S2.SS1.p1.3.m3.1.1.3.2.2" xref="S2.SS1.p1.3.m3.1.1.3.2.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.3.m3.1.1.3.2.1" xref="S2.SS1.p1.3.m3.1.1.3.2.1.cmml">​</mo><mi id="S2.SS1.p1.3.m3.1.1.3.2.3" xref="S2.SS1.p1.3.m3.1.1.3.2.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.3.m3.1.1.3.2.1a" xref="S2.SS1.p1.3.m3.1.1.3.2.1.cmml">​</mo><mi id="S2.SS1.p1.3.m3.1.1.3.2.4" xref="S2.SS1.p1.3.m3.1.1.3.2.4.cmml">T</mi></mrow><mo id="S2.SS1.p1.3.m3.1.1.3.1" xref="S2.SS1.p1.3.m3.1.1.3.1.cmml">−</mo><mrow id="S2.SS1.p1.3.m3.1.1.3.3" xref="S2.SS1.p1.3.m3.1.1.3.3.cmml"><mn id="S2.SS1.p1.3.m3.1.1.3.3.2" xref="S2.SS1.p1.3.m3.1.1.3.3.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S2.SS1.p1.3.m3.1.1.3.3.1" xref="S2.SS1.p1.3.m3.1.1.3.3.1.cmml">​</mo><mi id="S2.SS1.p1.3.m3.1.1.3.3.3" xref="S2.SS1.p1.3.m3.1.1.3.3.3.cmml">o</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2">𝑆</ci><apply id="S2.SS1.p1.3.m3.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3"><minus id="S2.SS1.p1.3.m3.1.1.3.1.cmml" xref="S2.SS1.p1.3.m3.1.1.3.1"></minus><apply id="S2.SS1.p1.3.m3.1.1.3.2.cmml" xref="S2.SS1.p1.3.m3.1.1.3.2"><times id="S2.SS1.p1.3.m3.1.1.3.2.1.cmml" xref="S2.SS1.p1.3.m3.1.1.3.2.1"></times><ci id="S2.SS1.p1.3.m3.1.1.3.2.2.cmml" xref="S2.SS1.p1.3.m3.1.1.3.2.2">𝐺</ci><ci id="S2.SS1.p1.3.m3.1.1.3.2.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3.2.3">𝑃</ci><ci id="S2.SS1.p1.3.m3.1.1.3.2.4.cmml" xref="S2.SS1.p1.3.m3.1.1.3.2.4">𝑇</ci></apply><apply id="S2.SS1.p1.3.m3.1.1.3.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3.3"><times id="S2.SS1.p1.3.m3.1.1.3.3.1.cmml" xref="S2.SS1.p1.3.m3.1.1.3.3.1"></times><cn type="integer" id="S2.SS1.p1.3.m3.1.1.3.3.2.cmml" xref="S2.SS1.p1.3.m3.1.1.3.3.2">4</cn><ci id="S2.SS1.p1.3.m3.1.1.3.3.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3.3.3">𝑜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">S_{GPT-4o}</annotation></semantics></math>, as follows:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.3" class="ltx_Math" alttext="\small\begin{array}[]{c}S_{GPT-4o}=ChatGPT(A,P).\end{array}" display="block"><semantics id="S2.E1.m1.3a"><mtable displaystyle="true" id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml"><mtr id="S2.E1.m1.3.3a" xref="S2.E1.m1.3.3.cmml"><mtd id="S2.E1.m1.3.3b" xref="S2.E1.m1.3.3.cmml"><mrow id="S2.E1.m1.3.3.3.3.3.3" xref="S2.E1.m1.3.3.3.3.3.3.1.cmml"><mrow id="S2.E1.m1.3.3.3.3.3.3.1" xref="S2.E1.m1.3.3.3.3.3.3.1.cmml"><msub id="S2.E1.m1.3.3.3.3.3.3.1.2" xref="S2.E1.m1.3.3.3.3.3.3.1.2.cmml"><mi mathsize="90%" id="S2.E1.m1.3.3.3.3.3.3.1.2.2" xref="S2.E1.m1.3.3.3.3.3.3.1.2.2.cmml">S</mi><mrow id="S2.E1.m1.3.3.3.3.3.3.1.2.3" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.cmml"><mrow id="S2.E1.m1.3.3.3.3.3.3.1.2.3.2" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.cmml"><mi mathsize="90%" id="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.2" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.1" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.1.cmml">​</mo><mi mathsize="90%" id="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.3" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.1a" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.1.cmml">​</mo><mi mathsize="90%" id="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.4" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.4.cmml">T</mi></mrow><mo mathsize="90%" id="S2.E1.m1.3.3.3.3.3.3.1.2.3.1" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.1.cmml">−</mo><mrow id="S2.E1.m1.3.3.3.3.3.3.1.2.3.3" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.3.cmml"><mn mathsize="90%" id="S2.E1.m1.3.3.3.3.3.3.1.2.3.3.2" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.3.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.3.3.3.3.1.2.3.3.1" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E1.m1.3.3.3.3.3.3.1.2.3.3.3" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.3.3.cmml">o</mi></mrow></mrow></msub><mo mathsize="90%" id="S2.E1.m1.3.3.3.3.3.3.1.1" xref="S2.E1.m1.3.3.3.3.3.3.1.1.cmml">=</mo><mrow id="S2.E1.m1.3.3.3.3.3.3.1.3" xref="S2.E1.m1.3.3.3.3.3.3.1.3.cmml"><mi mathsize="90%" id="S2.E1.m1.3.3.3.3.3.3.1.3.2" xref="S2.E1.m1.3.3.3.3.3.3.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.3.3.3.3.1.3.1" xref="S2.E1.m1.3.3.3.3.3.3.1.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E1.m1.3.3.3.3.3.3.1.3.3" xref="S2.E1.m1.3.3.3.3.3.3.1.3.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.3.3.3.3.1.3.1a" xref="S2.E1.m1.3.3.3.3.3.3.1.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E1.m1.3.3.3.3.3.3.1.3.4" xref="S2.E1.m1.3.3.3.3.3.3.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.3.3.3.3.1.3.1b" xref="S2.E1.m1.3.3.3.3.3.3.1.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E1.m1.3.3.3.3.3.3.1.3.5" xref="S2.E1.m1.3.3.3.3.3.3.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.3.3.3.3.1.3.1c" xref="S2.E1.m1.3.3.3.3.3.3.1.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E1.m1.3.3.3.3.3.3.1.3.6" xref="S2.E1.m1.3.3.3.3.3.3.1.3.6.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.3.3.3.3.1.3.1d" xref="S2.E1.m1.3.3.3.3.3.3.1.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E1.m1.3.3.3.3.3.3.1.3.7" xref="S2.E1.m1.3.3.3.3.3.3.1.3.7.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.3.3.3.3.1.3.1e" xref="S2.E1.m1.3.3.3.3.3.3.1.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E1.m1.3.3.3.3.3.3.1.3.8" xref="S2.E1.m1.3.3.3.3.3.3.1.3.8.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.3.3.3.3.1.3.1f" xref="S2.E1.m1.3.3.3.3.3.3.1.3.1.cmml">​</mo><mrow id="S2.E1.m1.3.3.3.3.3.3.1.3.9.2" xref="S2.E1.m1.3.3.3.3.3.3.1.3.9.1.cmml"><mo maxsize="90%" minsize="90%" id="S2.E1.m1.3.3.3.3.3.3.1.3.9.2.1" xref="S2.E1.m1.3.3.3.3.3.3.1.3.9.1.cmml">(</mo><mi mathsize="90%" id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml">A</mi><mo mathsize="90%" id="S2.E1.m1.3.3.3.3.3.3.1.3.9.2.2" xref="S2.E1.m1.3.3.3.3.3.3.1.3.9.1.cmml">,</mo><mi mathsize="90%" id="S2.E1.m1.2.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.2.2.cmml">P</mi><mo maxsize="90%" minsize="90%" id="S2.E1.m1.3.3.3.3.3.3.1.3.9.2.3" xref="S2.E1.m1.3.3.3.3.3.3.1.3.9.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" mathsize="90%" id="S2.E1.m1.3.3.3.3.3.3.2" xref="S2.E1.m1.3.3.3.3.3.3.1.cmml">.</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><matrix id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3"><matrixrow id="S2.E1.m1.3.3a.cmml" xref="S2.E1.m1.3.3"><apply id="S2.E1.m1.3.3.3.3.3.3.1.cmml" xref="S2.E1.m1.3.3.3.3.3.3"><eq id="S2.E1.m1.3.3.3.3.3.3.1.1.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.1"></eq><apply id="S2.E1.m1.3.3.3.3.3.3.1.2.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.3.3.3.3.1.2.1.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.2">subscript</csymbol><ci id="S2.E1.m1.3.3.3.3.3.3.1.2.2.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.2.2">𝑆</ci><apply id="S2.E1.m1.3.3.3.3.3.3.1.2.3.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3"><minus id="S2.E1.m1.3.3.3.3.3.3.1.2.3.1.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.1"></minus><apply id="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.2"><times id="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.1.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.1"></times><ci id="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.2.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.2">𝐺</ci><ci id="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.3.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.3">𝑃</ci><ci id="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.4.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.2.4">𝑇</ci></apply><apply id="S2.E1.m1.3.3.3.3.3.3.1.2.3.3.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.3"><times id="S2.E1.m1.3.3.3.3.3.3.1.2.3.3.1.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.3.1"></times><cn type="integer" id="S2.E1.m1.3.3.3.3.3.3.1.2.3.3.2.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.3.2">4</cn><ci id="S2.E1.m1.3.3.3.3.3.3.1.2.3.3.3.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.2.3.3.3">𝑜</ci></apply></apply></apply><apply id="S2.E1.m1.3.3.3.3.3.3.1.3.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.3"><times id="S2.E1.m1.3.3.3.3.3.3.1.3.1.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.3.1"></times><ci id="S2.E1.m1.3.3.3.3.3.3.1.3.2.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.3.2">𝐶</ci><ci id="S2.E1.m1.3.3.3.3.3.3.1.3.3.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.3.3">ℎ</ci><ci id="S2.E1.m1.3.3.3.3.3.3.1.3.4.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.3.4">𝑎</ci><ci id="S2.E1.m1.3.3.3.3.3.3.1.3.5.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.3.5">𝑡</ci><ci id="S2.E1.m1.3.3.3.3.3.3.1.3.6.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.3.6">𝐺</ci><ci id="S2.E1.m1.3.3.3.3.3.3.1.3.7.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.3.7">𝑃</ci><ci id="S2.E1.m1.3.3.3.3.3.3.1.3.8.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.3.8">𝑇</ci><interval closure="open" id="S2.E1.m1.3.3.3.3.3.3.1.3.9.1.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1.3.9.2"><ci id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1">𝐴</ci><ci id="S2.E1.m1.2.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.2.2">𝑃</ci></interval></apply></apply></matrixrow></matrix></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">\small\begin{array}[]{c}S_{GPT-4o}=ChatGPT(A,P).\end{array}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2409.09914/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="181" height="246" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Zero-shot speech assessment with GPT-Whisper.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">GPT-Whisper</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.3" class="ltx_p">The overall framework of GPT-Whisper is shown in Fig. <a href="#S2.F2" title="Figure 2 ‣ II-A GPT-4o for speech assessment ‣ II Methodology ‣ A Study on Zero-shot Non-intrusive Speech Assessment using Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The main idea behind GPT-Whisper is to leverage the reasoning capabilities of GPT-4o to assess the predicted word sequence <span id="S2.SS2.p1.3.1" class="ltx_text ltx_markedasmath ltx_font_italic">T</span> from the audio input <span id="S2.SS2.p1.3.2" class="ltx_text ltx_markedasmath ltx_font_italic">A</span>. We specifically chose Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> as the audio-to-text module because of its outstanding capability to convert audio into text across a variety of acoustic environments. We assume that this capability enables Whisper to capture subtle nuances of spoken language. Specifically, given an input audio <span id="S2.SS2.p1.3.3" class="ltx_text ltx_markedasmath ltx_font_italic">A</span>, the audio-to-text conversion using Whisper ASR is defined as follows:</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.2" class="ltx_Math" alttext="\small\begin{array}[]{c}T=Whisper(A).\end{array}" display="block"><semantics id="S2.E2.m1.2a"><mtable displaystyle="true" id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml"><mtr id="S2.E2.m1.2.2a" xref="S2.E2.m1.2.2.cmml"><mtd id="S2.E2.m1.2.2b" xref="S2.E2.m1.2.2.cmml"><mrow id="S2.E2.m1.2.2.2.2.2.2" xref="S2.E2.m1.2.2.2.2.2.2.1.cmml"><mrow id="S2.E2.m1.2.2.2.2.2.2.1" xref="S2.E2.m1.2.2.2.2.2.2.1.cmml"><mi mathsize="90%" id="S2.E2.m1.2.2.2.2.2.2.1.2" xref="S2.E2.m1.2.2.2.2.2.2.1.2.cmml">T</mi><mo mathsize="90%" id="S2.E2.m1.2.2.2.2.2.2.1.1" xref="S2.E2.m1.2.2.2.2.2.2.1.1.cmml">=</mo><mrow id="S2.E2.m1.2.2.2.2.2.2.1.3" xref="S2.E2.m1.2.2.2.2.2.2.1.3.cmml"><mi mathsize="90%" id="S2.E2.m1.2.2.2.2.2.2.1.3.2" xref="S2.E2.m1.2.2.2.2.2.2.1.3.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.2.2.2.2.1.3.1" xref="S2.E2.m1.2.2.2.2.2.2.1.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E2.m1.2.2.2.2.2.2.1.3.3" xref="S2.E2.m1.2.2.2.2.2.2.1.3.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.2.2.2.2.1.3.1a" xref="S2.E2.m1.2.2.2.2.2.2.1.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E2.m1.2.2.2.2.2.2.1.3.4" xref="S2.E2.m1.2.2.2.2.2.2.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.2.2.2.2.1.3.1b" xref="S2.E2.m1.2.2.2.2.2.2.1.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E2.m1.2.2.2.2.2.2.1.3.5" xref="S2.E2.m1.2.2.2.2.2.2.1.3.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.2.2.2.2.1.3.1c" xref="S2.E2.m1.2.2.2.2.2.2.1.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E2.m1.2.2.2.2.2.2.1.3.6" xref="S2.E2.m1.2.2.2.2.2.2.1.3.6.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.2.2.2.2.1.3.1d" xref="S2.E2.m1.2.2.2.2.2.2.1.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E2.m1.2.2.2.2.2.2.1.3.7" xref="S2.E2.m1.2.2.2.2.2.2.1.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.2.2.2.2.1.3.1e" xref="S2.E2.m1.2.2.2.2.2.2.1.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E2.m1.2.2.2.2.2.2.1.3.8" xref="S2.E2.m1.2.2.2.2.2.2.1.3.8.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.2.2.2.2.1.3.1f" xref="S2.E2.m1.2.2.2.2.2.2.1.3.1.cmml">​</mo><mrow id="S2.E2.m1.2.2.2.2.2.2.1.3.9.2" xref="S2.E2.m1.2.2.2.2.2.2.1.3.cmml"><mo maxsize="90%" minsize="90%" id="S2.E2.m1.2.2.2.2.2.2.1.3.9.2.1" xref="S2.E2.m1.2.2.2.2.2.2.1.3.cmml">(</mo><mi mathsize="90%" id="S2.E2.m1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.cmml">A</mi><mo maxsize="90%" minsize="90%" id="S2.E2.m1.2.2.2.2.2.2.1.3.9.2.2" xref="S2.E2.m1.2.2.2.2.2.2.1.3.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" mathsize="90%" id="S2.E2.m1.2.2.2.2.2.2.2" xref="S2.E2.m1.2.2.2.2.2.2.1.cmml">.</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S2.E2.m1.2b"><matrix id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2"><matrixrow id="S2.E2.m1.2.2a.cmml" xref="S2.E2.m1.2.2"><apply id="S2.E2.m1.2.2.2.2.2.2.1.cmml" xref="S2.E2.m1.2.2.2.2.2.2"><eq id="S2.E2.m1.2.2.2.2.2.2.1.1.cmml" xref="S2.E2.m1.2.2.2.2.2.2.1.1"></eq><ci id="S2.E2.m1.2.2.2.2.2.2.1.2.cmml" xref="S2.E2.m1.2.2.2.2.2.2.1.2">𝑇</ci><apply id="S2.E2.m1.2.2.2.2.2.2.1.3.cmml" xref="S2.E2.m1.2.2.2.2.2.2.1.3"><times id="S2.E2.m1.2.2.2.2.2.2.1.3.1.cmml" xref="S2.E2.m1.2.2.2.2.2.2.1.3.1"></times><ci id="S2.E2.m1.2.2.2.2.2.2.1.3.2.cmml" xref="S2.E2.m1.2.2.2.2.2.2.1.3.2">𝑊</ci><ci id="S2.E2.m1.2.2.2.2.2.2.1.3.3.cmml" xref="S2.E2.m1.2.2.2.2.2.2.1.3.3">ℎ</ci><ci id="S2.E2.m1.2.2.2.2.2.2.1.3.4.cmml" xref="S2.E2.m1.2.2.2.2.2.2.1.3.4">𝑖</ci><ci id="S2.E2.m1.2.2.2.2.2.2.1.3.5.cmml" xref="S2.E2.m1.2.2.2.2.2.2.1.3.5">𝑠</ci><ci id="S2.E2.m1.2.2.2.2.2.2.1.3.6.cmml" xref="S2.E2.m1.2.2.2.2.2.2.1.3.6">𝑝</ci><ci id="S2.E2.m1.2.2.2.2.2.2.1.3.7.cmml" xref="S2.E2.m1.2.2.2.2.2.2.1.3.7">𝑒</ci><ci id="S2.E2.m1.2.2.2.2.2.2.1.3.8.cmml" xref="S2.E2.m1.2.2.2.2.2.2.1.3.8">𝑟</ci><ci id="S2.E2.m1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1">𝐴</ci></apply></apply></matrixrow></matrix></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.2c">\small\begin{array}[]{c}T=Whisper(A).\end{array}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.2" class="ltx_p">Based on the predicted word sequence <span id="S2.SS2.p2.2.1" class="ltx_text ltx_markedasmath ltx_font_italic">T</span>, we use ChatGPT to estimate the GPT-Whisper score <math id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="S_{GPT-Whisper}" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><msub id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml"><mi id="S2.SS2.p2.2.m2.1.1.2" xref="S2.SS2.p2.2.m2.1.1.2.cmml">S</mi><mrow id="S2.SS2.p2.2.m2.1.1.3" xref="S2.SS2.p2.2.m2.1.1.3.cmml"><mrow id="S2.SS2.p2.2.m2.1.1.3.2" xref="S2.SS2.p2.2.m2.1.1.3.2.cmml"><mi id="S2.SS2.p2.2.m2.1.1.3.2.2" xref="S2.SS2.p2.2.m2.1.1.3.2.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p2.2.m2.1.1.3.2.1" xref="S2.SS2.p2.2.m2.1.1.3.2.1.cmml">​</mo><mi id="S2.SS2.p2.2.m2.1.1.3.2.3" xref="S2.SS2.p2.2.m2.1.1.3.2.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p2.2.m2.1.1.3.2.1a" xref="S2.SS2.p2.2.m2.1.1.3.2.1.cmml">​</mo><mi id="S2.SS2.p2.2.m2.1.1.3.2.4" xref="S2.SS2.p2.2.m2.1.1.3.2.4.cmml">T</mi></mrow><mo id="S2.SS2.p2.2.m2.1.1.3.1" xref="S2.SS2.p2.2.m2.1.1.3.1.cmml">−</mo><mrow id="S2.SS2.p2.2.m2.1.1.3.3" xref="S2.SS2.p2.2.m2.1.1.3.3.cmml"><mi id="S2.SS2.p2.2.m2.1.1.3.3.2" xref="S2.SS2.p2.2.m2.1.1.3.3.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p2.2.m2.1.1.3.3.1" xref="S2.SS2.p2.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S2.SS2.p2.2.m2.1.1.3.3.3" xref="S2.SS2.p2.2.m2.1.1.3.3.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p2.2.m2.1.1.3.3.1a" xref="S2.SS2.p2.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S2.SS2.p2.2.m2.1.1.3.3.4" xref="S2.SS2.p2.2.m2.1.1.3.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p2.2.m2.1.1.3.3.1b" xref="S2.SS2.p2.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S2.SS2.p2.2.m2.1.1.3.3.5" xref="S2.SS2.p2.2.m2.1.1.3.3.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p2.2.m2.1.1.3.3.1c" xref="S2.SS2.p2.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S2.SS2.p2.2.m2.1.1.3.3.6" xref="S2.SS2.p2.2.m2.1.1.3.3.6.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p2.2.m2.1.1.3.3.1d" xref="S2.SS2.p2.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S2.SS2.p2.2.m2.1.1.3.3.7" xref="S2.SS2.p2.2.m2.1.1.3.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p2.2.m2.1.1.3.3.1e" xref="S2.SS2.p2.2.m2.1.1.3.3.1.cmml">​</mo><mi id="S2.SS2.p2.2.m2.1.1.3.3.8" xref="S2.SS2.p2.2.m2.1.1.3.3.8.cmml">r</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><apply id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.2.m2.1.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p2.2.m2.1.1.2.cmml" xref="S2.SS2.p2.2.m2.1.1.2">𝑆</ci><apply id="S2.SS2.p2.2.m2.1.1.3.cmml" xref="S2.SS2.p2.2.m2.1.1.3"><minus id="S2.SS2.p2.2.m2.1.1.3.1.cmml" xref="S2.SS2.p2.2.m2.1.1.3.1"></minus><apply id="S2.SS2.p2.2.m2.1.1.3.2.cmml" xref="S2.SS2.p2.2.m2.1.1.3.2"><times id="S2.SS2.p2.2.m2.1.1.3.2.1.cmml" xref="S2.SS2.p2.2.m2.1.1.3.2.1"></times><ci id="S2.SS2.p2.2.m2.1.1.3.2.2.cmml" xref="S2.SS2.p2.2.m2.1.1.3.2.2">𝐺</ci><ci id="S2.SS2.p2.2.m2.1.1.3.2.3.cmml" xref="S2.SS2.p2.2.m2.1.1.3.2.3">𝑃</ci><ci id="S2.SS2.p2.2.m2.1.1.3.2.4.cmml" xref="S2.SS2.p2.2.m2.1.1.3.2.4">𝑇</ci></apply><apply id="S2.SS2.p2.2.m2.1.1.3.3.cmml" xref="S2.SS2.p2.2.m2.1.1.3.3"><times id="S2.SS2.p2.2.m2.1.1.3.3.1.cmml" xref="S2.SS2.p2.2.m2.1.1.3.3.1"></times><ci id="S2.SS2.p2.2.m2.1.1.3.3.2.cmml" xref="S2.SS2.p2.2.m2.1.1.3.3.2">𝑊</ci><ci id="S2.SS2.p2.2.m2.1.1.3.3.3.cmml" xref="S2.SS2.p2.2.m2.1.1.3.3.3">ℎ</ci><ci id="S2.SS2.p2.2.m2.1.1.3.3.4.cmml" xref="S2.SS2.p2.2.m2.1.1.3.3.4">𝑖</ci><ci id="S2.SS2.p2.2.m2.1.1.3.3.5.cmml" xref="S2.SS2.p2.2.m2.1.1.3.3.5">𝑠</ci><ci id="S2.SS2.p2.2.m2.1.1.3.3.6.cmml" xref="S2.SS2.p2.2.m2.1.1.3.3.6">𝑝</ci><ci id="S2.SS2.p2.2.m2.1.1.3.3.7.cmml" xref="S2.SS2.p2.2.m2.1.1.3.3.7">𝑒</ci><ci id="S2.SS2.p2.2.m2.1.1.3.3.8.cmml" xref="S2.SS2.p2.2.m2.1.1.3.3.8">𝑟</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">S_{GPT-Whisper}</annotation></semantics></math>. Our strategy involves using naturalness in prompt engineering, which refers to how similar the predicted text is to human-generated text in terms of fluency, coherence, and context. This approach measures the degree to which text reflects the natural flow and nuances of human speech. The process of estimating the GPT-Whisper score is defined as follows:</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.3" class="ltx_Math" alttext="\small\begin{array}[]{c}S_{GPT-Whisper}=ChatGPT(T,P).\end{array}" display="block"><semantics id="S2.E3.m1.3a"><mtable displaystyle="true" id="S2.E3.m1.3.3" xref="S2.E3.m1.3.3.cmml"><mtr id="S2.E3.m1.3.3a" xref="S2.E3.m1.3.3.cmml"><mtd id="S2.E3.m1.3.3b" xref="S2.E3.m1.3.3.cmml"><mrow id="S2.E3.m1.3.3.3.3.3.3" xref="S2.E3.m1.3.3.3.3.3.3.1.cmml"><mrow id="S2.E3.m1.3.3.3.3.3.3.1" xref="S2.E3.m1.3.3.3.3.3.3.1.cmml"><msub id="S2.E3.m1.3.3.3.3.3.3.1.2" xref="S2.E3.m1.3.3.3.3.3.3.1.2.cmml"><mi mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.2.2" xref="S2.E3.m1.3.3.3.3.3.3.1.2.2.cmml">S</mi><mrow id="S2.E3.m1.3.3.3.3.3.3.1.2.3" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.cmml"><mrow id="S2.E3.m1.3.3.3.3.3.3.1.2.3.2" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.cmml"><mi mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.2" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.1" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.1.cmml">​</mo><mi mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.3" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.1a" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.1.cmml">​</mo><mi mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.4" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.4.cmml">T</mi></mrow><mo mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.2.3.1" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.1.cmml">−</mo><mrow id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.cmml"><mi mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.2" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.2.cmml">W</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.1" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.3" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.1a" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.4" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.1b" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.5" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.1c" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.6" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.6.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.1d" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.7" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.1e" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.8" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.8.cmml">r</mi></mrow></mrow></msub><mo mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.1" xref="S2.E3.m1.3.3.3.3.3.3.1.1.cmml">=</mo><mrow id="S2.E3.m1.3.3.3.3.3.3.1.3" xref="S2.E3.m1.3.3.3.3.3.3.1.3.cmml"><mi mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.3.2" xref="S2.E3.m1.3.3.3.3.3.3.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.3.3.3.3.1.3.1" xref="S2.E3.m1.3.3.3.3.3.3.1.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.3.3" xref="S2.E3.m1.3.3.3.3.3.3.1.3.3.cmml">h</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.3.3.3.3.1.3.1a" xref="S2.E3.m1.3.3.3.3.3.3.1.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.3.4" xref="S2.E3.m1.3.3.3.3.3.3.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.3.3.3.3.1.3.1b" xref="S2.E3.m1.3.3.3.3.3.3.1.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.3.5" xref="S2.E3.m1.3.3.3.3.3.3.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.3.3.3.3.1.3.1c" xref="S2.E3.m1.3.3.3.3.3.3.1.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.3.6" xref="S2.E3.m1.3.3.3.3.3.3.1.3.6.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.3.3.3.3.1.3.1d" xref="S2.E3.m1.3.3.3.3.3.3.1.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.3.7" xref="S2.E3.m1.3.3.3.3.3.3.1.3.7.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.3.3.3.3.1.3.1e" xref="S2.E3.m1.3.3.3.3.3.3.1.3.1.cmml">​</mo><mi mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.3.8" xref="S2.E3.m1.3.3.3.3.3.3.1.3.8.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.3.3.3.3.1.3.1f" xref="S2.E3.m1.3.3.3.3.3.3.1.3.1.cmml">​</mo><mrow id="S2.E3.m1.3.3.3.3.3.3.1.3.9.2" xref="S2.E3.m1.3.3.3.3.3.3.1.3.9.1.cmml"><mo maxsize="90%" minsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.3.9.2.1" xref="S2.E3.m1.3.3.3.3.3.3.1.3.9.1.cmml">(</mo><mi mathsize="90%" id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml">T</mi><mo mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.3.9.2.2" xref="S2.E3.m1.3.3.3.3.3.3.1.3.9.1.cmml">,</mo><mi mathsize="90%" id="S2.E3.m1.2.2.2.2.2.2" xref="S2.E3.m1.2.2.2.2.2.2.cmml">P</mi><mo maxsize="90%" minsize="90%" id="S2.E3.m1.3.3.3.3.3.3.1.3.9.2.3" xref="S2.E3.m1.3.3.3.3.3.3.1.3.9.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" mathsize="90%" id="S2.E3.m1.3.3.3.3.3.3.2" xref="S2.E3.m1.3.3.3.3.3.3.1.cmml">.</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S2.E3.m1.3b"><matrix id="S2.E3.m1.3.3.cmml" xref="S2.E3.m1.3.3"><matrixrow id="S2.E3.m1.3.3a.cmml" xref="S2.E3.m1.3.3"><apply id="S2.E3.m1.3.3.3.3.3.3.1.cmml" xref="S2.E3.m1.3.3.3.3.3.3"><eq id="S2.E3.m1.3.3.3.3.3.3.1.1.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.1"></eq><apply id="S2.E3.m1.3.3.3.3.3.3.1.2.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.3.3.3.3.1.2.1.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.2">subscript</csymbol><ci id="S2.E3.m1.3.3.3.3.3.3.1.2.2.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.2.2">𝑆</ci><apply id="S2.E3.m1.3.3.3.3.3.3.1.2.3.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3"><minus id="S2.E3.m1.3.3.3.3.3.3.1.2.3.1.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.1"></minus><apply id="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.2"><times id="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.1.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.1"></times><ci id="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.2.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.2">𝐺</ci><ci id="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.3.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.3">𝑃</ci><ci id="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.4.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.2.4">𝑇</ci></apply><apply id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3"><times id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.1.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.1"></times><ci id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.2.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.2">𝑊</ci><ci id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.3.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.3">ℎ</ci><ci id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.4.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.4">𝑖</ci><ci id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.5.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.5">𝑠</ci><ci id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.6.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.6">𝑝</ci><ci id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.7.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.7">𝑒</ci><ci id="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.8.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.2.3.3.8">𝑟</ci></apply></apply></apply><apply id="S2.E3.m1.3.3.3.3.3.3.1.3.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.3"><times id="S2.E3.m1.3.3.3.3.3.3.1.3.1.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.3.1"></times><ci id="S2.E3.m1.3.3.3.3.3.3.1.3.2.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.3.2">𝐶</ci><ci id="S2.E3.m1.3.3.3.3.3.3.1.3.3.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.3.3">ℎ</ci><ci id="S2.E3.m1.3.3.3.3.3.3.1.3.4.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.3.4">𝑎</ci><ci id="S2.E3.m1.3.3.3.3.3.3.1.3.5.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.3.5">𝑡</ci><ci id="S2.E3.m1.3.3.3.3.3.3.1.3.6.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.3.6">𝐺</ci><ci id="S2.E3.m1.3.3.3.3.3.3.1.3.7.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.3.7">𝑃</ci><ci id="S2.E3.m1.3.3.3.3.3.3.1.3.8.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.3.8">𝑇</ci><interval closure="open" id="S2.E3.m1.3.3.3.3.3.3.1.3.9.1.cmml" xref="S2.E3.m1.3.3.3.3.3.3.1.3.9.2"><ci id="S2.E3.m1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1">𝑇</ci><ci id="S2.E3.m1.2.2.2.2.2.2.cmml" xref="S2.E3.m1.2.2.2.2.2.2">𝑃</ci></interval></apply></apply></matrixrow></matrix></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.3c">\small\begin{array}[]{c}S_{GPT-Whisper}=ChatGPT(T,P).\end{array}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Experimental setup</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The two proposed approaches are evaluated on the TMHINT-QI(S) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, which is an extension of the TMHINT-QI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, by including additional unseen noises, speakers, and enhancement systems. The TMHINT-QI(S) dataset is also a benchmark track in the VoiceMOS Challenge 2023 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. The evaluation set consists of noisy, clean, and enhanced utterances, covering three seen noise conditions (babble, white, and pink) and one unseen noise condition (street). It also includes three seen speech enhancement systems (Minimum-Mean Square Error (MMSE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, Fully Convolutional Network (FCN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, and Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>), and introduces two new, unseen speech enhancement systems: Conformer-based Metric Generative Adversarial Network (CMGAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and DEMUCS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. In total, there are 1,960 utterances with corresponding quality scores (0-5) and intelligibility scores (0-1). An additional metric, CER (0-1), is produced by two ASR models (Google ASR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>). Note that we reverse the CER score so that higher values indicate better performance, ensuring consistency with other metrics and simplifying interpretation.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Two evaluation metrics, linear correlation coefficient (LCC) and Spearman’s rank correlation coefficient (SRCC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, are used to measure the performance of the proposed methods. Higher LCC and SRCC values indicate a stronger correlation between the predicted score and the ground-truth score, indicating better assessment prediction performance.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2409.09914/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="266" height="280" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example of GPT-4o used for speech assessment.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Experimental results of GPT-4o</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In the first experiment, we intend to leverage prompt engineering on GPT-4o for speech assessment. As shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ III-A Experimental setup ‣ III Experiments ‣ A Study on Zero-shot Non-intrusive Speech Assessment using Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we specifically use keywords to evaluate the quality and intelligibility of a given audio input. The audio input in Fig. <a href="#S3.F3" title="Figure 3 ‣ III-A Experimental setup ‣ III Experiments ‣ A Study on Zero-shot Non-intrusive Speech Assessment using Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> is an MMSE-enhanced speech of noisy speech containing babble noise at -2dB SNR, which is used as a representative scenario. The results show that GPT-4o determines the quality or intelligibility based only on metrics such as SNR, clipping, amplitude, or loudness. However, when we compare these results to the corresponding ground-truth information (quality score: 1.4/5, intelligibility score: 0.6/1, SNR: -2dB), GPT-4o makes inaccurate predictions. For example, as shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ III-A Experimental setup ‣ III Experiments ‣ A Study on Zero-shot Non-intrusive Speech Assessment using Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, GPT-4o believes that the audio input is unclipped and has a high SNR, whereas in fact the audio has a low SNR (MMSE’s noise reduction effect is noticeably poor in this example) and moderate distortion. Furthermore, while the intelligibility estimate is fairly close to the true value (7/10 vs 0.6/1), the quality estimate is quite off (7/10 vs 1.4/5). Experimental results on the complete test set covering different audio characteristics show that it is difficult for the current GPT-4o to precisely predict the quality and intelligibility of the audio input. Therefore, the current GPT-4o may not serve as a reliable zero-shot non-intrusive speech assessment method.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Experimental results of GPT-Whisper</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In this section, we aim to evaluate the capabilities of our proposed GPT-Whisper for performing zero-shot non-intrusive speech assessment.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS1.4.1.1" class="ltx_text">III-C</span>1 </span>Correlation analysis</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">In initial experiments, we estimate GPT-Whisper scores using the approach outlined in Section <a href="#S2" title="II Methodology ‣ A Study on Zero-shot Non-intrusive Speech Assessment using Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. Experimental results show that GPT-4o exhibits notable reasoning capabilities in text comprehension and demonstrates solid contextual understanding, as shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ III-C1 Correlation analysis ‣ III-C Experimental results of GPT-Whisper ‣ III Experiments ‣ A Study on Zero-shot Non-intrusive Speech Assessment using Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. GPT-4o effectively evaluates the coherence of text inputs and determines whether connections are natural or unnatural. Based on these results, we conclude that the scores produced by GPT-Whisper are more reliable than those produced previously by GPT-4o for speech assessment.</p>
</div>
<div id="S3.SS3.SSS1.p2" class="ltx_para">
<p id="S3.SS3.SSS1.p2.1" class="ltx_p">Based on the promising performance of GPT-Whisper, we intend to further investigate which assessment metrics are more correlated with the GPT-Whisper score. We assume that GPT-Whisper can be used as an alternative in scenarios where there is no corresponding label, and the training data is insufficient to establish a deep learning-based speech assessment model. In our evaluation, we compare GPT-Whisper scores with human-based quality, human-based intelligibility scores, and the CER of two ASR models (Whisper and Google ASR). The results in Fig. <a href="#S3.F5" title="Figure 5 ‣ III-C1 Correlation analysis ‣ III-C Experimental results of GPT-Whisper ‣ III Experiments ‣ A Study on Zero-shot Non-intrusive Speech Assessment using Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> show that the GPT-Whisper metric has a higher correlation with intelligibility-based metrics than the quality one and has the highest correlation with the CER of Whisper ASR. These results further confirm GPT-Whisper’s ability to perform zero-shot speech assessment and serve as a surrogate metric for evaluating speech recognition performance, given its high correlation score with the CER of Whisper (SRCC of 0.7784).</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2409.09914/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="266" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Example of GPT-Whisper used for speech assessment.</figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2409.09914/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="257" height="151" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Correlation of GPT-Whisper scores with other metrics.</figcaption>
</figure>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS2.4.1.1" class="ltx_text">III-C</span>2 </span>Comparison with supervised methods</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">We now compare GPT-Whisper with two supervised speech assessment models, namely MOS-SSL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, and MTI-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, on the task of predicting the CER of Whisper. We use Whisper’s CER as the ground truth because the GPT-Whisper score has a higher correlation with this CER score than with human-based quality and intelligibility. Therefore, it is interesting to compare GPT-Whisper with these supervised models on this metric. Specifically, MOS-SSL incorporates wav2vec 2.0 to predict MOS scores, while MTI-Net employs cross-domain features (raw waveform, spectral features, and SSL features) to predict intelligibility metrics (intelligibility, CER, and STOI) using multi-task learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. We trained MOS-SSL and MTI-Net using the TMHINT-QI(s) training set. MOS-SSL was trained to predict Whisper’s CER, while MTI-Net was trained to predict intelligibility, Google ASR’s CER, Whisper’s CER, and STOI. Given the significant efficacy of integrating Whisper representations into robust speech assessment models, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, we also integrated Whisper into the MTI-Net model. Therefore, we prepared two versions: MTI-Net (wav2vec) and MTI-Net (Whisper).</p>
</div>
<div id="S3.SS3.SSS2.p2" class="ltx_para">
<p id="S3.SS3.SSS2.p2.1" class="ltx_p">The results in Table <a href="#S3.T1" title="TABLE I ‣ III-C2 Comparison with supervised methods ‣ III-C Experimental results of GPT-Whisper ‣ III Experiments ‣ A Study on Zero-shot Non-intrusive Speech Assessment using Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> confirm that GPT-Whisper performs comparably to supervised non-intrusive speech assessment models. Interestingly, GPT-Whisper outperforms MOS-SSL and MTI-Net (wav2vec) in LCC and SRCC. Additionally, GPT-Whisper has higher SRCC than MTI-Net (Whisper), although its LCC is slightly lower. These results demonstrate the effectiveness of GPT-Whisper as a zero-shot speech assessment model, showing comparable or even superior performance to supervised methods.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison of GPT-Whisper with supervised methods.</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt" style="padding-bottom:1.72221pt;"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">System</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt" style="padding-bottom:1.72221pt;"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Supervised</span></th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt" style="padding-bottom:1.72221pt;"><span id="S3.T1.1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">LCC</span></th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-bottom:1.72221pt;"><span id="S3.T1.1.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">SRCC</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt"><span id="S3.T1.1.2.1.1.1" class="ltx_text" style="font-size:80%;">GPT-Whisper</span></td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt"><span id="S3.T1.1.2.1.2.1" class="ltx_text" style="font-size:80%;">No</span></td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt"><span id="S3.T1.1.2.1.3.1" class="ltx_text" style="font-size:80%;">0.7541</span></td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T1.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.7784</span></td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">
<span id="S3.T1.1.3.2.1.1" class="ltx_text" style="font-size:80%;">MOS-SSL </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T1.1.3.2.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S3.T1.1.3.2.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.T1.1.3.2.2.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.T1.1.3.2.3.1" class="ltx_text" style="font-size:80%;">0.7328</span></td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.3.2.4.1" class="ltx_text" style="font-size:80%;">0.7482</span></td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">
<span id="S3.T1.1.4.3.1.1" class="ltx_text" style="font-size:80%;">MTI-Net (wav2vec) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T1.1.4.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S3.T1.1.4.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.T1.1.4.3.2.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.T1.1.4.3.3.1" class="ltx_text" style="font-size:80%;">0.7344</span></td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.4.3.4.1" class="ltx_text" style="font-size:80%;">0.7418</span></td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<td id="S3.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr ltx_border_t">
<span id="S3.T1.1.5.4.1.1" class="ltx_text" style="font-size:80%;">MTI-Net (Whisper) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T1.1.5.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S3.T1.1.5.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr ltx_border_t"><span id="S3.T1.1.5.4.2.1" class="ltx_text" style="font-size:80%;">Yes</span></td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr ltx_border_t"><span id="S3.T1.1.5.4.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">0.8194</span></td>
<td id="S3.T1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.1.5.4.4.1" class="ltx_text" style="font-size:80%;">0.7655</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2409.09914/assets/x6.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="272" height="174" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Distribution of predicted CER scores of GPT-Whisper and MTI-Net (Whisper) and Whisper’s ground-truth CER scores.</figcaption>
</figure>
</section>
<section id="S3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS3.SSS3.4.1.1" class="ltx_text">III-C</span>3 </span>Score distribution analysis</h4>

<div id="S3.SS3.SSS3.p1" class="ltx_para">
<p id="S3.SS3.SSS3.p1.1" class="ltx_p">In this section, we further analyze the distribution of scores evaluated by GPT-Whisper and MTI-Net (Whisper) and the corresponding CER labels of Whisper. This analysis aims to evaluate whether GPT-Whisper provides accurate score estimations across the entire score range. As shown in Fig. <a href="#S3.F6" title="Figure 6 ‣ III-C2 Comparison with supervised methods ‣ III-C Experimental results of GPT-Whisper ‣ III Experiments ‣ A Study on Zero-shot Non-intrusive Speech Assessment using Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, GPT-Whisper successfully predicts the entire range of scores and shows a more similar distribution to that of CER generated by the Whisper ASR, especially for CER less than 0.25.
The closeness of the distribution of predicted CER scores to the distribution of true CER scores validates GPT-Whisper as a new assessment metric and the potential of using ChatGPT for speech assessment by implementing the GPT-Whisper method.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Conclusions</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we have proposed two strategies for zero-shot non-intrusive speech assessment. The first strategy leverages the audio analysis capabilities of GPT-4o. The second strategy introduces a more advanced approach that uses Whisper as an audio-to-text module and evaluates the naturalness of the generated text through targeted prompt engineering, so the model is called GPT-Whisper. To the best of our knowledge, this is the first attempt to leverage ChatGPT for speech assessment. Experimental results confirm that GPT-4o alone is insufficient for accurate speech assessment. In contrast, GPT-Whisper demonstrates potential as a zero-shot speech assessment method, with yielding moderate correlation with human-based quality assessment (SRCC of 0.4360), human-based intelligibility assessment (SRCC of 0.5485), and Google ASR’s CER (SRCC of 0.5415) and notably high correlation with Whisper’s CER (SRCC of 0.7784). Furthermore, in predicting Whisper’s CER, GPT-Whisper outperforms the supervised models MOS-SSL, MTI-Net (wav2vec), and MTI-Net (Whisper) in terms of SRCC (0.7784 vs. 0.7482, 0.7418, and 0.7655). This confirms the ability of GPT-Whisper as a reliable zero-shot speech assessment method and the potential of using ChatGPT for speech assessment through the GPT-Whisper method. In the future, we plan to explore more advanced prompt engineering to further leverage ChatGPT to estimate other speech assessment metrics. We also intend to investigate direct integration of GPT-Whisper with speech-processing applications to enhance their performance.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
P. C. Loizou,

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Speech enhancement: theory and practice</span>,

</span>
<span class="ltx_bibblock">CRC press, 2007.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Rix, J. Beerends, M. Hollier, and A. Hekstra,

</span>
<span class="ltx_bibblock">“Perceptual evaluation of speech quality (PESQ), an objective method for end-to-end speech quality assessment of narrow-band telephone networks and speech codecs,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">ITU-T Recommendation</span>, 2001, p. 862.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. M. Kates and K. H. Arehart,

</span>
<span class="ltx_bibblock">“The hearing-aid speech perception index (HASPI) version 2,”

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Speech Communication</span>, vol. 131, pp. 35–46, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J. M. Kates and K. H. Arehart,

</span>
<span class="ltx_bibblock">“The hearing-aid speech quality index (HASQI) version 2,”

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Journal of the Audio Engineering Society</span>, vol. 62, no. 3, pp. 99–117, 2014.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
H.-T. Chiang, S.-W. Fu, H.-M. Wang, Y. Tsao, and J. H. L. Hansen,

</span>
<span class="ltx_bibblock">“Multi-objective non-intrusive hearing-aid speech assessment model,”

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">arXiv:2311.08878</span>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J.G Beerends, C. Schmidmer, J. Berger, M. Obermann, R. Ullmann, J. Pomy, and M. Keyhl,

</span>
<span class="ltx_bibblock">“Perceptual objective listening quality assessment (POLQA), the third generation ITU-T standard for end-to-end speech quality measurement part i—temporal alignment,”

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Journal of The Audio Engineering Society</span>, vol. 61, no. 6, pp. 366–384, 2013.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Z. Yang, W. Zhou, C. Chu, S. Li, R. Dabre, R. Rubino, and Y. Zhao,

</span>
<span class="ltx_bibblock">“Fusion of self-supervised learned models for MOS prediction,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, 2022, pp. 5443–5447.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S. Cuervo and R. Marxer,

</span>
<span class="ltx_bibblock">“Speech foundation models on intelligibility prediction for hearing-impaired listeners,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2024, pp. 1421–1425.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
R. Mogridge, G. Close, R. Sutherland, T. Hain, J. Barker, S. Goetze, and A. Ragni,

</span>
<span class="ltx_bibblock">“Non-intrusive speech intelligibility prediction for hearing-impaired users using intermediate ASR features and human memory models,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2024, pp. 306–310.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
C. O. Mawalim, B. A. Titalim, S. Okada, and M. Unoki,

</span>
<span class="ltx_bibblock">“Non-intrusive speech intelligibility prediction using an auditory periphery model with hearing loss,”

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Applied Acoustics</span>, vol. 214, pp. 109663, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
R. E. Zezario, Yu-Wen Chen, Szu-Wei Fu, Yu Tsao, Hsin-Min Wang, and Chiou-Shann Fuh,

</span>
<span class="ltx_bibblock">“A study on incorporating Whisper for robust speech assessment,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proc. ICME</span>, 2024.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
P. Manocha, D. Williamson, and A. Finkelstein,

</span>
<span class="ltx_bibblock">“Corn: Co-trained full- and no-reference speech quality assessment,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2024, pp. 376–380.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
E. Cooper, W.-C. Huang, Y. Tsao, H.-M. Wang, T. Toda, and J. Yamagishi,

</span>
<span class="ltx_bibblock">“A review on subjective and objective evaluation of synthetic speech,”

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Acoustical Science and Technology</span>, pp. e24–12, 2024.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
W.-C. Huang, E. Cooper, J. Yamagishi, and T. Toda,

</span>
<span class="ltx_bibblock">“LDNet: Unified listener dependent modeling in MOS prediction for synthetic speech,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2022, pp. 896–900.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
E. Cooper, W.-H. Huang, T. Toda, and J. Yamagishi,

</span>
<span class="ltx_bibblock">“Generalization ability of MOS prediction networks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
R.E Zezario, S.-W Fu, F. Chen, C.-S Fuh, H.-M. Wang, and Y. Tsao,

</span>
<span class="ltx_bibblock">“Deep learning-based non-intrusive multi-objective speech assessment model with cross-domain features,”

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span>, vol. 31, pp. 54–70, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
S. Maiti, Y. Peng, T. Saeki, and S. Watanabe,

</span>
<span class="ltx_bibblock">“Speechlmscore: Evaluating speech generation using speech language model,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP 2023</span>, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
R. E. Zezario, B.-R. Brian Bai, C.-S. Fuh, H.-M. Wang, and Y. Tsao,

</span>
<span class="ltx_bibblock">“Multi-task pseudo-label learning for non-intrusive speech quality assessment model,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2024, pp. 831–835.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
X. Wei, X. Cui, N. Cheng, X. Wang, X. Zhang, S. Huang, P. Xie, J. Xu, Y. Chen, M. Zhang, Y. Jiang, and W. Han,

</span>
<span class="ltx_bibblock">“Chatie: Zero-shot information extraction via chatting with chatgpt,”

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">arXiv:2302.10205</span>, 2024.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S. Jia, R. Lyu, K. Zhao, Y. Chen, Z. Yan, Y. Ju, C. Hu, X. Li, B. Wu, and S. Lyu,

</span>
<span class="ltx_bibblock">“Can chatgpt detect deepfakes? a study of using multimodal large language models for media forensics,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proc. of the IEEE/CVF CVPR Workshops</span>, 2024, pp. 4324–4333.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
R. Huang, M. Li, D. Yang, J. Shi, X. Chang, Z. Ye, Y. Wu, Z. Hong, J. Huang, J. Liu, Y. Ren, Y. Zou, Z. Zhao, and S. Watanabe,

</span>
<span class="ltx_bibblock">“AudioGPT: Understanding and generating speech, music, sound, and talking head,”

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proc. the AAAI Conference on Artificial Intelligence</span>, vol. 38, no. 21, pp. 23802–23804, 2024.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever,

</span>
<span class="ltx_bibblock">“Robust speech recognition via large-scale weak supervision,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proc. ICML</span>, 2023, pp. 28492–28518.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A. Zhang,

</span>
<span class="ltx_bibblock">“Speech recognition (version 3.6) [software], available: https://github.com/uberi/speech_recognition#readme,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proc. ICCC</span>, 2017.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
R. Zezario, S.-W. Fu, F. Chen, C. S. Fuh, H.-M. Wang, and Y. Tsao,

</span>
<span class="ltx_bibblock">“MTI-Net: A multi-target speech intelligibility prediction model,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, 2022, pp. 5463–5467.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
C. Spearman,

</span>
<span class="ltx_bibblock">“The proof and measurement of association between two things,”

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">The American Journal of Psychology</span>, vol. 15, no. 1, pp. 72–101, 1904.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Y.-W. Chen and Y. Tsao,

</span>
<span class="ltx_bibblock">“InQSS: a speech intelligibility assessment model using a multi-task learning network,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech</span>, 2022, pp. 3088–3092.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
E. Cooper, W.-C. Huang, Y. Tsao, H.-M. Wang, T. Toda, and J. Yamagishi,

</span>
<span class="ltx_bibblock">“The VoiceMOS Challenge 2023: Zero-shot subjective speech quality prediction for multiple domains,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Proc. ASRU</span>, 2023, pp. 1–7.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Y. Ephraim and D. Malah,

</span>
<span class="ltx_bibblock">“Speech enhancement using a minimum mean-square error log-spectral amplitude estimator,”

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Acoustics, Speech, and Signal Processing</span>, vol. 33, no. 2, pp. 443–445, 1985.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
S.-W. Fu, Y. Tsao, X. Lu, and H. Kawai,

</span>
<span class="ltx_bibblock">“Raw waveform-based speech enhancement by fully convolutional networks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Proc. APSIPA ASC</span>, 2017.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J. Kim, M. El-Khamy, and J. Lee,

</span>
<span class="ltx_bibblock">“T-GSA: Transformer with Gaussian-weighted self-attention for speech enhancement,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Proc. ICASSP</span>, 2020, pp. 6649–6653.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
R. Cao, S. Abdulatif, and B. Yang,

</span>
<span class="ltx_bibblock">“CMGAN: Conformer-based Metric GAN for speech enhancement,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech 2022</span>, 2022, pp. 936–940.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
A. Défossez, N. Usunier, L. Bottou, and F. Bach,

</span>
<span class="ltx_bibblock">“Music source separation in the waveform domain,”

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">arXiv 1911.13254</span>, 2021.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.09913" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.09914" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.09914">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.09914" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.09915" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 21:32:36 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
