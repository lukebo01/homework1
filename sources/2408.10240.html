<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.10240] AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People</title><meta property="og:description" content="People with visual impairments often struggle to create content that relies heavily on visual elements, particularly when conveying spatial and structural information. Existing accessible drawing tools, which constructâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.10240">

<!--Generated on Thu Sep  5 17:04:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or <span id="id1.id1" class="ltx_text" style="color:#000000;">Visually</span> Impaired People</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Seonghee Lee
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id2.1.id1" class="ltx_text ltx_affiliation_institution">Stanford University</span><span id="id3.2.id2" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:shlee@cs.stanford.edu">shlee@cs.stanford.edu</a>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maho Kohga
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_affiliation_institution">Stanford University</span><span id="id5.2.id2" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:mkohga@stanford.edu">mkohga@stanford.edu</a>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Steve Landau
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id6.1.id1" class="ltx_text ltx_affiliation_institution">Touch Graphics</span><span id="id7.2.id2" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:sl@touchgraphics.com">sl@touchgraphics.com</a>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sile Oâ€™Modhrain
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id8.1.id1" class="ltx_text ltx_affiliation_institution">University of Michigan</span><span id="id9.2.id2" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:sileo@umich.edu">sileo@umich.edu</a>
</span></span></span>
<span class="ltx_author_before">Â andÂ </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hari Subramonyam
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id10.1.id1" class="ltx_text ltx_affiliation_institution">Stanford University</span><span id="id11.2.id2" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:harihars@stanford.edu">harihars@stanford.edu</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id12.id1" class="ltx_p">People with visual impairments often struggle to create content that relies heavily on visual elements, particularly when conveying spatial and structural information. Existing accessible drawing tools, which construct images line by line, are suitable for simple tasks like math but not for more expressive artwork. On the other hand, emerging generative AI-based text-to-image tools can produce expressive illustrations from descriptions in natural language, but they lack precise control over image composition and properties. To address this gap, our work integrates generative AI with a constructive approach that provides users with enhanced control and editing capabilities. Our system, AltCanvas, features a tile-based interface enabling users to construct visual scenes incrementally, with each tile representing an object within the scene. Users can add, edit, move, and arrange objects while receiving speech and audio feedback. Once completed, the scene can be rendered as a color illustration or as a vector for tactile graphic generation. Involving 14 blind or low-vision users in design and evaluation, we found that participants effectively used the AltCanvasâ€™s workflow to create illustrations.</p>
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">conference: </span>The 26th International
ACM SIGACCESS Conference on Computers and Accessibility; October 27â€“30,
2024; St. Johnâ€™s, NL, Canada</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_booktitle"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">booktitle: </span>The 26th International ACM SIGACCESS Conference on
Computers and Accessibility (ASSETS â€™24), October 27â€“30, 2024, St. Johnâ€™s,
NL, Canada</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">doi: </span>10.1145/3663548.3675600</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">isbn: </span>979-8-4007-0677-6/24/10</span></span></span>
<figure id="S0.F1" class="ltx_figure ltx_teaserfigure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2408.10240/assets/images/teaser.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="286" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span><span id="S0.F1.2.1" class="ltx_text ltx_font_bold"> AltCanvas image authoring steps: (A) Image Generation with AltCanvas using a tile-based interface. (B) Image Editing Process in AltCanvas, incorporating sonification and verbal feedback. (C) Final Rendering Process of AltCanvas, producing tactile graphics or enhanced renderings for general audiences.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S0.F1.3" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S0.F1.4" class="ltx_p ltx_figure_panel ltx_align_center">[]A diagram of AltCanvasâ€™ three steps image authoring steps. A on the left: The tile-based interface can add images by talking to them. B in the middle: Images canÂ be editedÂ through sonification, verbal feedback, and arrow keys. C on the right: The rendering process of black-and-white and color graphics. Step A: It shows a tile-based interface with a highlighted square, suggesting the selection of a tile, and an instruction bubble that reads, â€Add an image of a running dog,â€ prompting the start of the image generation. Step B: The interface now includes a line drawing of a running dog next to a ball, with a directional arrow indicating the ability to move the ball. Below, thereâ€™s a legend explaining the editing controls, such as using arrow keys to adjust the location of objects. Step C: This step is divided into two parts. The upper part shows the final black-and-white tactile graphic, a simple and clear line drawing of a dog chasing a flying disc with a tree in the background. The lower part displays the color graphic with final rendering, where the same scene is depicted in full color with a blue sky, a lush green tree, and the dog joyfully leaping towards the blue flying disc.</p>
</div>
</div>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Spatial constructs play a crucial role in human cognition and communicationÂ <cite class="ltx_cite ltx_citemacro_citep">(Laurini and Thompson, <a href="#bib.bib38" title="" class="ltx_ref">1992</a>)</cite>. We rely on spatial information for describing scenes, learning complex concepts in science and mathematics, and artistically expressing ourselves. For instance, when describing a pet-friendly home in a blog article, we can sketch out where to place pet beds, food stations, and play areas showing dogs and cats at play. For sighted users, several tools exist for creating visual graphics across different fidelity, ranging from digital pen-based drawing applications (e.g., SketchPadÂ <cite class="ltx_cite ltx_citemacro_citep">(Sutherland, <a href="#bib.bib60" title="" class="ltx_ref">1963</a>)</cite>) to direct manipulation tools such as Adobe IllustratorÂ <cite class="ltx_cite ltx_citemacro_citep">(Adobe Systems Incorporated, <a href="#bib.bib4" title="" class="ltx_ref">2023a</a>)</cite> for authoring rich vector graphics. However, based on a recent surveyÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2023</a>)</cite>, while blind or vision impaired (BVI) people are motivated to create visual content for school, work, and art, current access barriers in content creation tools hinder their ability to do so. Although linguistic references such as â€œnext toâ€ or â€œjaggedâ€ are commonly used for spatial and structural descriptions, they can also be abstract and ambiguous. The act of <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">visualizing</span> allows us to be more precise in articulating spatial information.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Existing drawing tools designed for BVI users take a guided constructive approach (i.e., line-by-line drawing) and primarily concentrate on basic graphics, such as shapes and geometric figuresÂ <cite class="ltx_cite ltx_citemacro_citep">(Bornschein and Weber, <a href="#bib.bib10" title="" class="ltx_ref">2017</a>)</cite>. While this allows a high degree of control within specific drawing tasks, it can be tedious to create more expressive open-ended graphics, for instance, the illustration of the pet-friendly home in the earlier example. On the other hand, tools based on emergent text-to-image generative modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Dhariwal and Nichol, <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite> allow for rich, expressive illustration and digital artwork using text promptsÂ <cite class="ltx_cite ltx_citemacro_citep">(Chang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite>. However, the trade-off is in the <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">degree of control</span>. With a generative AI approach, users may have to describe entire scenes in natural language and have less control over specific spatial and visual attributes such as composition, size, color, relative distance, orientation, etc. In other words, it would be difficult to generate a highly specific illustration of the intended pet-friendly home. Our own exploration has shown that the models donâ€™t always accurately interpret text prompts (e.g., a prompt for <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">â€œimage with five apples and one apple sliced in halfâ€</span> resulted in one apple sliced into five pieces). In fact, recent work in generative AI-based image tools for BVI users has looked at question-answering mechanisms to validate the generated output across complex scenesÂ <cite class="ltx_cite ltx_citemacro_citep">(Huh etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2023a</a>)</cite>. Alternatively, creating accessible visual content like printed tactile graphics is also challenging, as direct conversion is often impossible, making iterative design difficult.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Ideally, we would like tools that can leverage generative AI capabilities to create expressive content but also support a constructive approach for better control and composition. In this regard, for sighted users, researchers have explored techniques such as users providing a rough doodle of scene composition to inform the semantics and style of generationÂ <cite class="ltx_cite ltx_citemacro_citep">(Park etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2019</a>)</cite>. Turning our thoughts into tangible visuals requires such forms of control through iterative representation, feedback, and adjustmentsÂ <cite class="ltx_cite ltx_citemacro_citep">(Millar, <a href="#bib.bib43" title="" class="ltx_ref">1975a</a>)</cite>. Thus, our motivating question for this work is <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">â€œHow might we support a constructive approach to authoring visual content while also leveraging the generative capabilities of AI models?â€</span> By conducting a formative study with five blind participants who have created visual content in the past, we learned about current content authoring workflows, associated challenges, and opinions about generative text-to-image models. Specifically, experienced users expressed (1) a need for precise control and guidance to optimize the design, (2) the ability to use pre-existing graphics with options for deleting backgrounds and tools for rescaling and editing, and (3) enhanced usability of generative AI features through verbal descriptions and explanations of graphics without needing to print each iteration. Based on these requirements, we developed <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">AltCanvas</span>, using an iterative design and evaluation approach.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">AltCanvas is a generative AI-powered illustration tool that implements a <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">dynamic tile-based interface</span> and sonification features to support the authoring of visual graphics. It consists of a side-by-side layout of a tile view (i.e., an alternative to a direct manipulation drawing canvas â€” AltCanvas) and an image view to render the image under construction. The tile view is designed based on an understanding of blind spatial cognition and relational aspects of objects in images (i.e., <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">â€œthe vast majority of visual information is really spatial informationÂ <cite class="ltx_cite ltx_citemacro_citep">(Giudice, <a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite>
â€</span>). At the start of creating a new illustration, the tile view consists of a single tile, and dynamically expands along eight directions to allow adding additional objects to relative locations. As shown in FigureÂ <a href="#S0.F1" title="Figure 1 â€£ AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the user starts by adding a ball object to the canvas using speech-to-text and text-to-image. They then add a dog to the right of it by navigating to the adjacent tile on the right. AltCanvas implements speech-based descriptions of the generated image and, through keyboard navigation and sonification, allows creators to compose complex images through resizing, repositioning, and other edit features. Finally, once the image is composed, AltCanvas allows creators to generate a representation suitable for printing as tactile graphics or can render full-color graphics for sharing with sighted people. In developing AltCanvas, we prototyped a range of interaction techniques for different authoring tasks and conducted a preliminary study to understand the preferred interactions of BVI content creators. Based on feedback, we revised our designs and evaluated the final prototype through a user study. Across the formative and iterative evaluation studies, we worked with 14 participants.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our main contributions include (1) a tile-based interaction paradigm that provides an alternative representation of the visual canvas, (2) a novel constructive and generative image authoring workflow using speech and sonification, and (3) results from authoring content with AltCanvas and tactile graphic evaluation from BVI users.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Image Editing Challenges for BVI users</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">When editing images, users must continually adjust their input based on visual feedback. However, many current editing software lack the capability to offer real-time visual feedback through screen-readersÂ <cite class="ltx_cite ltx_citemacro_citep">(Acosta etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2018</a>; Kamel and Landay, <a href="#bib.bib35" title="" class="ltx_ref">2000a</a>; Schaadhardt etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2021</a>)</cite>. This is a serious limitation as visual feedback is crucial for editing operations such as alignment and overlapÂ <cite class="ltx_cite ltx_citemacro_citep">(Millar, <a href="#bib.bib44" title="" class="ltx_ref">1975b</a>; Peng etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2021</a>)</cite>. Additionally, complex image editors like Adobe Photoshop, with its 71 keyboard commands, compound the challenge, especially when custom menus or dialog boxes arenâ€™t screen reader-friendlyÂ <cite class="ltx_cite ltx_citemacro_citep">(Adobe Systems Incorporated, <a href="#bib.bib5" title="" class="ltx_ref">2023b</a>; Acosta etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2018</a>; Pandey etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2020</a>; Li etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2021</a>)</cite>. Further, image editing tools should be equipped with â€œgrammar toolsâ€ that assist visually impaired users in navigating the spatial layout and understanding the interactive drawing space, enhancing their grasp of concepts or subject matterÂ <cite class="ltx_cite ltx_citemacro_citep">(Spector etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2016</a>)</cite>. These tools should incorporate elements that can be adjusted or combined over time, enabling users to locate drawn objects, identify critical control points, and make modifications or additions to these points as necessaryÂ <cite class="ltx_cite ltx_citemacro_citep">(Kamel and Landay, <a href="#bib.bib35" title="" class="ltx_ref">2000a</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>BVI Image Editing Interfaces</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Grid-based interfaces can help BVI users make more precise point selections <cite class="ltx_cite ltx_citemacro_citep">(Kamel and Landay, <a href="#bib.bib34" title="" class="ltx_ref">1999</a>)</cite>. Prior work has explored variations such as IC2Dâ€™s grid-based system <cite class="ltx_cite ltx_citemacro_citep">(Kamel and Landay, <a href="#bib.bib34" title="" class="ltx_ref">1999</a>)</cite> and haptic display with a grid system that divides the drawing surface into <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="m\times n" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mrow id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mi id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p1.1.m1.1.1.1" xref="S2.SS2.p1.1.m1.1.1.1.cmml">Ã—</mo><mi id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><times id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1.1"></times><ci id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">ğ‘š</ci><ci id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">m\times n</annotation></semantics></math> sections <cite class="ltx_cite ltx_citemacro_citep">(Headley and Pawluk, <a href="#bib.bib27" title="" class="ltx_ref">2010</a>)</cite>. To navigate around different regions on the grid, keyboard commands coupled with verbalizations of grid locations were used. Alternately, the use of a table structure to convey information has also been used in prior work to make data flow diagrams accessible for BVI students <cite class="ltx_cite ltx_citemacro_citep">(Sauter, <a href="#bib.bib55" title="" class="ltx_ref">2015</a>)</cite>. Considerations for table-based interfaces include screen-reader accessibility, but it is challenging to operationalize well even if tables have been properly marked up <cite class="ltx_cite ltx_citemacro_citep">(Williams etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2019</a>; Gardiner etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2016</a>)</cite>. Innovative navigational aids like those in TextSL offer BVI users the ability to move through virtual spaces safely and efficiently using natural language for collision-free navigation and relative location information, replacing traditional coordinate systems with directional cues such as â€œnorthâ€ or â€œnortheastâ€ <cite class="ltx_cite ltx_citemacro_citep">(Folmer etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2009</a>)</cite>. This suite of tools and methodologies reflects a growing commitment to enhancing the usability of image editing software for BVI users through multi-sensory and accessible interfaces. Yet there is more to be explored in terms of effective guidelines for creating BVI-accessible editing software.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Image description and tactile feedback are crucial components of accessible image editors for visually impaired users. Early innovations in this field include TDraw, which offered tactile feedback without erasure capabilitiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Kurze, <a href="#bib.bib37" title="" class="ltx_ref">1996</a>)</cite>, and Watanabeâ€™s system, which introduced full erasure functionality via a Braille display and stylusÂ <cite class="ltx_cite ltx_citemacro_citep">(Watanabe and Kobayashi, <a href="#bib.bib64" title="" class="ltx_ref">2002</a>)</cite>. Recent advancements have expanded these capabilities: the Draw and Drag Touchscreen provides interactive manipulation with text-to-speech output for geometric shapesÂ <cite class="ltx_cite ltx_citemacro_citep">(Grussenmeyer, <a href="#bib.bib25" title="" class="ltx_ref">2015</a>)</cite>, â€œPlaying with Geometryâ€ utilizes vibrotactile feedback for freehand drawing Â <cite class="ltx_cite ltx_citemacro_citep">(Buzzi etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2015</a>)</cite>, and AudioDraw employs audio feedback to assist in creating educational diagrams Â <cite class="ltx_cite ltx_citemacro_citep">(Grussenmeyer and Folmer, <a href="#bib.bib26" title="" class="ltx_ref">2016</a>)</cite>. Parallel developments in image description systems have further enhanced accessibility. The â€˜RegionSpeakâ€™ system enables users to crowdsource object labels and navigate spatial layoutsÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhong etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib69" title="" class="ltx_ref">2015</a>)</cite>, while â€˜TactileMapsâ€™ introduces a novel tactile exploration method using a raised-line map overlaid on a multi-touch screen with auditory feedback Â <cite class="ltx_cite ltx_citemacro_citep">(Brock and Jouffrais, <a href="#bib.bib12" title="" class="ltx_ref">2015</a>)</cite>. Leveraging advanced technology, â€˜Image Explorerâ€™ employs deep learning to create a multi-layered, touch-based exploration systemÂ <cite class="ltx_cite ltx_citemacro_citep">(Lee etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2021</a>)</cite>. These innovative tools, spanning from tactile interfaces to sophisticated image description systems, significantly improve visually impaired usersâ€™ ability to interact with and understand visual content through a combination of touch and auditory feedback.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Previous research has highlighted the efficacy of data sonification for rapid data analysis. Notable examples include the sonification of the 2011 Tohoku Earthquake in Japan, various astronomical objects by NASA, and solar winds <cite class="ltx_cite ltx_citemacro_citep">(Arcand etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2024</a>; Candey etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2006</a>; SÃ¸ndergaard and VandsÃ¸, <a href="#bib.bib58" title="" class="ltx_ref">2017</a>)</cite>. MathGraphs developed a series of sonification of math graphs <cite class="ltx_cite ltx_citemacro_citep">(Ohshiro etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2021</a>)</cite>. For BVI users who are more sensitive and trained to recognize the sound, the use of sonification provides an intuitive way for users to interact beyond immediate visualization <cite class="ltx_cite ltx_citemacro_citep">(Vallejo-Pinto etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2011</a>)</cite>. Sonification has also been applied to enhance object recognition within interactive experiences. For example, EdgeSonic employs sonification techniques to emphasize image edge features and distance-to-edge maps <cite class="ltx_cite ltx_citemacro_citep">(Yoshida etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib67" title="" class="ltx_ref">2011</a>)</cite>. Similarly, Melodie has enriched the crafting process for weavers by incorporating sonification <cite class="ltx_cite ltx_citemacro_citep">(Borgos-Rodriguez etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite>. More recently, Symphony introduced an audio-tactile system specifically designed to aid blind weavers. This system uses sounds to denote warping and pitch variations, thus facilitating the creation and perception of textile patterns <cite class="ltx_cite ltx_citemacro_citep">(Das etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite>. Extending beyond these applications, sonification has found a place in digital gaming for the blind. â€œHeroâ€™s Callâ€ utilizes arrow keys for movement and incorporates a sound radar to help players navigate their surroundings <cite class="ltx_cite ltx_citemacro_citep">(Out of Sight Games, <a href="#bib.bib48" title="" class="ltx_ref">2023</a>)</cite>. Another game, â€œAliens,â€ employs pitch variations to inform players about the positions of targets, allowing for quicker navigation and interaction using arrow keys <cite class="ltx_cite ltx_citemacro_citep">(Jantrid, <a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite>. Overall, sonification has proven to be a versatile tool in assisting BVI users, enhancing data analysis, accessibility, and user interaction through intuitive auditory cues.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Image Generation and Editing with AI tools</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Generative AI has opened new avenues for creating accessible technology. Recent research has explored its use in making short-form videos accessible through video summaries, as demonstrated in projects like AVscript and ShortScribe Â <cite class="ltx_cite ltx_citemacro_citep">(Huh etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2023b</a>; VanÂ Daele etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2024</a>)</cite>. It has also been employed to enhance image generation processes (GenAssist) and to design effective tactile graphics using image-to-image generation techniques (Text to Haptics) Â <cite class="ltx_cite ltx_citemacro_citep">(Huh etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2023a</a>; Tanaka etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2023</a>; Huh etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2023b</a>)</cite>. Complementary tools such as BeMyEyes, Apple VoiceOver, Android Talkback, Chromebook Chromevox, and Braille displays further assist users in navigating and understanding various visual mediums Â <cite class="ltx_cite ltx_citemacro_citep">(Eyes, <a href="#bib.bib20" title="" class="ltx_ref">2020</a>; Leporini etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2012</a>; Singh etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2023</a>)</cite>. An innovative approach is exemplified by WorldSmith, a multimodal tool that enables users to design and refine complex fictional worlds through layered editing and hierarchical compositions, incorporating a prompt-based model that integrates text, sketches, and region masks as inputs Â <cite class="ltx_cite ltx_citemacro_citep">(Dang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>. Similarly, Crosspower explores leveraging language structure to facilitate graphic content creation, highlighting both the potential and limitations of language as a primary medium for image editing Â <cite class="ltx_cite ltx_citemacro_citep">(Xia, <a href="#bib.bib66" title="" class="ltx_ref">2020</a>)</cite>. While natural language processing offers precision, its inherent limitations underscore the need for tools that provide greater control and flexibility in visual content creation and editing for visually impaired users.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Formative Study with Blind Visual Content Creators</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To better understand existing visual content authoring workflows and associated challenges with representation, feedback, and iteration, we conducted semi-structured interviews with five blind experienced visual content creators.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Participant ID</span></span>
</span>
</td>
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.2.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Eyesight</span></span>
</span>
</td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.3.1.1" class="ltx_p" style="width:71.1pt;"><span id="S3.T1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Image Editing Experience</span></span>
</span>
</td>
<td id="S3.T1.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.4.1.1" class="ltx_p" style="width:71.1pt;"><span id="S3.T1.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Participated in Formative Study?</span></span>
</span>
</td>
<td id="S3.T1.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.5.1.1" class="ltx_p" style="width:85.4pt;"><span id="S3.T1.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Participated in Design Study?</span></span>
</span>
</td>
<td id="S3.T1.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.6.1.1" class="ltx_p" style="width:85.4pt;"><span id="S3.T1.1.1.1.6.1.1.1" class="ltx_text ltx_font_bold">Participated in Final Evaluation Study?</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.2.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.1.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.1.1.1" class="ltx_p" style="width:56.9pt;">P1</span>
</span>
</td>
<td id="S3.T1.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.2.1.1" class="ltx_p" style="width:56.9pt;">Blind</span>
</span>
</td>
<td id="S3.T1.1.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.3.1.1" class="ltx_p" style="width:71.1pt;">Experienced</span>
</span>
</td>
<td id="S3.T1.1.2.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.4.1.1" class="ltx_p" style="width:71.1pt;">yes</span>
</span>
</td>
<td id="S3.T1.1.2.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.5.1.1" class="ltx_p" style="width:85.4pt;">no</span>
</span>
</td>
<td id="S3.T1.1.2.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.6.1.1" class="ltx_p" style="width:85.4pt;">no</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.3.3" class="ltx_tr">
<td id="S3.T1.1.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="S3.T1.1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.3.1.1.1" class="ltx_p" style="width:56.9pt;">P2</span>
</span>
</td>
<td id="S3.T1.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.3.2.1.1" class="ltx_p" style="width:56.9pt;">Blind</span>
</span>
</td>
<td id="S3.T1.1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.3.3.1.1" class="ltx_p" style="width:71.1pt;">Experienced</span>
</span>
</td>
<td id="S3.T1.1.3.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.3.4.1.1" class="ltx_p" style="width:71.1pt;">yes</span>
</span>
</td>
<td id="S3.T1.1.3.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.3.5.1.1" class="ltx_p" style="width:85.4pt;">no</span>
</span>
</td>
<td id="S3.T1.1.3.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.3.6.1.1" class="ltx_p" style="width:85.4pt;">no</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.4.4" class="ltx_tr">
<td id="S3.T1.1.4.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="S3.T1.1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.4.1.1.1" class="ltx_p" style="width:56.9pt;">P3</span>
</span>
</td>
<td id="S3.T1.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.4.2.1.1" class="ltx_p" style="width:56.9pt;">Blind</span>
</span>
</td>
<td id="S3.T1.1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.4.3.1.1" class="ltx_p" style="width:71.1pt;">Experienced</span>
</span>
</td>
<td id="S3.T1.1.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.4.4.1.1" class="ltx_p" style="width:71.1pt;">yes</span>
</span>
</td>
<td id="S3.T1.1.4.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.4.5.1.1" class="ltx_p" style="width:85.4pt;">no</span>
</span>
</td>
<td id="S3.T1.1.4.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.4.6.1.1" class="ltx_p" style="width:85.4pt;">yes</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.5.5" class="ltx_tr">
<td id="S3.T1.1.5.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="S3.T1.1.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.5.1.1.1" class="ltx_p" style="width:56.9pt;">P4</span>
</span>
</td>
<td id="S3.T1.1.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.5.2.1.1" class="ltx_p" style="width:56.9pt;">Low Vision</span>
</span>
</td>
<td id="S3.T1.1.5.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.5.3.1.1" class="ltx_p" style="width:71.1pt;">Experienced</span>
</span>
</td>
<td id="S3.T1.1.5.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.5.4.1.1" class="ltx_p" style="width:71.1pt;">yes</span>
</span>
</td>
<td id="S3.T1.1.5.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.5.5.1.1" class="ltx_p" style="width:85.4pt;">no</span>
</span>
</td>
<td id="S3.T1.1.5.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.5.6.1.1" class="ltx_p" style="width:85.4pt;">no</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.6.6" class="ltx_tr">
<td id="S3.T1.1.6.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="S3.T1.1.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.6.1.1.1" class="ltx_p" style="width:56.9pt;">P5</span>
</span>
</td>
<td id="S3.T1.1.6.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.6.2.1.1" class="ltx_p" style="width:56.9pt;">Blind</span>
</span>
</td>
<td id="S3.T1.1.6.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.6.3.1.1" class="ltx_p" style="width:71.1pt;">Experienced</span>
</span>
</td>
<td id="S3.T1.1.6.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.6.4.1.1" class="ltx_p" style="width:71.1pt;">yes</span>
</span>
</td>
<td id="S3.T1.1.6.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.6.5.1.1" class="ltx_p" style="width:85.4pt;">no</span>
</span>
</td>
<td id="S3.T1.1.6.6.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.6.6.1.1" class="ltx_p" style="width:85.4pt;">yes</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.7.7" class="ltx_tr">
<td id="S3.T1.1.7.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="S3.T1.1.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.7.1.1.1" class="ltx_p" style="width:56.9pt;">P6</span>
</span>
</td>
<td id="S3.T1.1.7.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.7.2.1.1" class="ltx_p" style="width:56.9pt;">Blind</span>
</span>
</td>
<td id="S3.T1.1.7.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.7.3.1.1" class="ltx_p" style="width:71.1pt;">Novice</span>
</span>
</td>
<td id="S3.T1.1.7.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.7.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.7.4.1.1" class="ltx_p" style="width:71.1pt;">no</span>
</span>
</td>
<td id="S3.T1.1.7.7.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.7.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.7.5.1.1" class="ltx_p" style="width:85.4pt;">yes</span>
</span>
</td>
<td id="S3.T1.1.7.7.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.7.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.7.6.1.1" class="ltx_p" style="width:85.4pt;">yes</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.8.8" class="ltx_tr">
<td id="S3.T1.1.8.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="S3.T1.1.8.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.8.1.1.1" class="ltx_p" style="width:56.9pt;">P7</span>
</span>
</td>
<td id="S3.T1.1.8.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.8.2.1.1" class="ltx_p" style="width:56.9pt;">Blind</span>
</span>
</td>
<td id="S3.T1.1.8.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.8.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.8.3.1.1" class="ltx_p" style="width:71.1pt;">Novice</span>
</span>
</td>
<td id="S3.T1.1.8.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.8.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.8.4.1.1" class="ltx_p" style="width:71.1pt;">no</span>
</span>
</td>
<td id="S3.T1.1.8.8.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.8.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.8.5.1.1" class="ltx_p" style="width:85.4pt;">yes</span>
</span>
</td>
<td id="S3.T1.1.8.8.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.8.8.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.8.8.6.1.1" class="ltx_p" style="width:85.4pt;">yes</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.9.9" class="ltx_tr">
<td id="S3.T1.1.9.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="S3.T1.1.9.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.9.1.1.1" class="ltx_p" style="width:56.9pt;">P8</span>
</span>
</td>
<td id="S3.T1.1.9.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.9.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.9.2.1.1" class="ltx_p" style="width:56.9pt;">Low Vision</span>
</span>
</td>
<td id="S3.T1.1.9.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.9.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.9.3.1.1" class="ltx_p" style="width:71.1pt;">Novice</span>
</span>
</td>
<td id="S3.T1.1.9.9.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.9.9.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.9.4.1.1" class="ltx_p" style="width:71.1pt;">no</span>
</span>
</td>
<td id="S3.T1.1.9.9.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.9.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.9.5.1.1" class="ltx_p" style="width:85.4pt;">yes</span>
</span>
</td>
<td id="S3.T1.1.9.9.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.9.9.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.9.9.6.1.1" class="ltx_p" style="width:85.4pt;">no</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.10.10" class="ltx_tr">
<td id="S3.T1.1.10.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="S3.T1.1.10.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.10.10.1.1.1" class="ltx_p" style="width:56.9pt;">P9</span>
</span>
</td>
<td id="S3.T1.1.10.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.10.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.10.10.2.1.1" class="ltx_p" style="width:56.9pt;">Low Vision</span>
</span>
</td>
<td id="S3.T1.1.10.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.10.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.10.10.3.1.1" class="ltx_p" style="width:71.1pt;">Novice</span>
</span>
</td>
<td id="S3.T1.1.10.10.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.10.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.10.10.4.1.1" class="ltx_p" style="width:71.1pt;">no</span>
</span>
</td>
<td id="S3.T1.1.10.10.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.10.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.10.10.5.1.1" class="ltx_p" style="width:85.4pt;">yes</span>
</span>
</td>
<td id="S3.T1.1.10.10.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.10.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.10.10.6.1.1" class="ltx_p" style="width:85.4pt;">no</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.11.11" class="ltx_tr">
<td id="S3.T1.1.11.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="S3.T1.1.11.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.11.11.1.1.1" class="ltx_p" style="width:56.9pt;">P10</span>
</span>
</td>
<td id="S3.T1.1.11.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.11.11.2.1.1" class="ltx_p" style="width:56.9pt;">Low Vision</span>
</span>
</td>
<td id="S3.T1.1.11.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.11.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.11.11.3.1.1" class="ltx_p" style="width:71.1pt;">Novice</span>
</span>
</td>
<td id="S3.T1.1.11.11.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.11.11.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.11.11.4.1.1" class="ltx_p" style="width:71.1pt;">no</span>
</span>
</td>
<td id="S3.T1.1.11.11.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.11.11.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.11.11.5.1.1" class="ltx_p" style="width:85.4pt;">yes</span>
</span>
</td>
<td id="S3.T1.1.11.11.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.11.11.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.11.11.6.1.1" class="ltx_p" style="width:85.4pt;">yes</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.12.12" class="ltx_tr">
<td id="S3.T1.1.12.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="S3.T1.1.12.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.12.12.1.1.1" class="ltx_p" style="width:56.9pt;">P11</span>
</span>
</td>
<td id="S3.T1.1.12.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.12.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.12.12.2.1.1" class="ltx_p" style="width:56.9pt;">Low Vision</span>
</span>
</td>
<td id="S3.T1.1.12.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.12.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.12.12.3.1.1" class="ltx_p" style="width:71.1pt;">Novice</span>
</span>
</td>
<td id="S3.T1.1.12.12.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.12.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.12.12.4.1.1" class="ltx_p" style="width:71.1pt;">no</span>
</span>
</td>
<td id="S3.T1.1.12.12.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.12.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.12.12.5.1.1" class="ltx_p" style="width:85.4pt;">yes</span>
</span>
</td>
<td id="S3.T1.1.12.12.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.12.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.12.12.6.1.1" class="ltx_p" style="width:85.4pt;">no</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.13.13" class="ltx_tr">
<td id="S3.T1.1.13.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="S3.T1.1.13.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.13.13.1.1.1" class="ltx_p" style="width:56.9pt;">P12</span>
</span>
</td>
<td id="S3.T1.1.13.13.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.13.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.13.13.2.1.1" class="ltx_p" style="width:56.9pt;">Blind</span>
</span>
</td>
<td id="S3.T1.1.13.13.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.13.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.13.13.3.1.1" class="ltx_p" style="width:71.1pt;">Novice</span>
</span>
</td>
<td id="S3.T1.1.13.13.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.13.13.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.13.13.4.1.1" class="ltx_p" style="width:71.1pt;">no</span>
</span>
</td>
<td id="S3.T1.1.13.13.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.13.13.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.13.13.5.1.1" class="ltx_p" style="width:85.4pt;">no</span>
</span>
</td>
<td id="S3.T1.1.13.13.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.13.13.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.13.13.6.1.1" class="ltx_p" style="width:85.4pt;">yes</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.14.14" class="ltx_tr">
<td id="S3.T1.1.14.14.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="S3.T1.1.14.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.14.14.1.1.1" class="ltx_p" style="width:56.9pt;">P13</span>
</span>
</td>
<td id="S3.T1.1.14.14.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.14.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.14.14.2.1.1" class="ltx_p" style="width:56.9pt;">Blind</span>
</span>
</td>
<td id="S3.T1.1.14.14.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.14.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.14.14.3.1.1" class="ltx_p" style="width:71.1pt;">Novice</span>
</span>
</td>
<td id="S3.T1.1.14.14.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.14.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.14.14.4.1.1" class="ltx_p" style="width:71.1pt;">no</span>
</span>
</td>
<td id="S3.T1.1.14.14.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.14.14.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.14.14.5.1.1" class="ltx_p" style="width:85.4pt;">no</span>
</span>
</td>
<td id="S3.T1.1.14.14.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.1.14.14.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.14.14.6.1.1" class="ltx_p" style="width:85.4pt;">yes</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.15.15" class="ltx_tr">
<td id="S3.T1.1.15.15.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r">
<span id="S3.T1.1.15.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.15.15.1.1.1" class="ltx_p" style="width:56.9pt;">P14</span>
</span>
</td>
<td id="S3.T1.1.15.15.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S3.T1.1.15.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.15.15.2.1.1" class="ltx_p" style="width:56.9pt;">Low Vision</span>
</span>
</td>
<td id="S3.T1.1.15.15.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S3.T1.1.15.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.15.15.3.1.1" class="ltx_p" style="width:71.1pt;">Novice</span>
</span>
</td>
<td id="S3.T1.1.15.15.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S3.T1.1.15.15.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.15.15.4.1.1" class="ltx_p" style="width:71.1pt;">no</span>
</span>
</td>
<td id="S3.T1.1.15.15.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S3.T1.1.15.15.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.15.15.5.1.1" class="ltx_p" style="width:85.4pt;">no</span>
</span>
</td>
<td id="S3.T1.1.15.15.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S3.T1.1.15.15.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.15.15.6.1.1" class="ltx_p" style="width:85.4pt;">yes</span>
</span>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Participant characteristics across the three studies (formative need finding, design feedback, and user evaluation study). <span id="S3.T1.3.1" class="ltx_text" style="color:#000000;"> â€œBlindâ€ refers to participants who are totally blind, meaning they have no functional vision. â€œLow Visionâ€ refers to participants who have some degree of vision impairment, typically using a combination of a screen magnifier and a screen reader. â€œPast Experienceâ€ refers to experience with image editing software using a screen reader or other accessibility tools. â€œExperiencedâ€ indicates extensive prior experience, while â€œNoviceâ€ indicates limited or no prior experience.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S3.T1.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.T1.5" class="ltx_p ltx_figure_panel ltx_align_center">This table summarizes the characteristics of participants across three different studies focused on formative need finding, design feedback, and user evaluation. It consists of six columns: Participant ID (P1 through P14), Eyesight status (Blind or Low Vision), Past Experience (Experienced or Novice), and three columns indicating participation in Interviews, Design Study, and Evaluation with â€™yesâ€™ or â€™noâ€™ responses. â€Blindâ€ refers to participants who are totally blind with no functional vision, while â€Low Visionâ€ refers to participants with partial vision impairment or related eyesight illnesses who typically use a combination of a screen magnifier and a screen reader. â€Experiencedâ€ indicates prior experience with image editing using editing software or through SVG code using a screen reader or other accessibility tools. â€Noviceâ€ indicates limited or no prior experience. Participants P1 to P5 are experienced and have participated in interviews, but their participation varies in design studies and evaluations. Participants P6 to P11 are a mix of blind or low vision novices, with varying participation in the design study and evaluation but no interviews. The last three participants (P12 to P14) are novices with different eyesight statuses and have only participated in the evaluation.</p>
</div>
</div>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Participant Recruitment</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We recruited five participants by email using contacts in our immediate network. All participants self-identified as blind and had extensive experience authoring tactile graphics as part of their professional work or personal interest. Each semi-structured interview lasted between 45 and 60 minutes, and participants were compensated with a $50 Amazon Gift Card for their time. The interviews were conducted over the Zoom video-conference platformÂ <cite class="ltx_cite ltx_citemacro_citep">(Zoom Video Communications, <a href="#bib.bib70" title="" class="ltx_ref">2023</a>)</cite>. TableÂ <a href="#S3.T1" title="Table 1 â€£ 3. Formative Study with Blind Visual Content Creators â€£ AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides a consolidated summary of all participants across different studies with specific columns indicating whether or not they were involved in each of the studies. Many of our participants engaged throughout the iterative design process.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Procedure</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Our semi-structured interviews consisted of three parts. In the first part, we asked participants questions about their visual content authoring workflow. Example questions include â€œCan you tell us about the last time you authored a tactile graphic? What was the graphic about?â€, â€œCan you walk us through your process for authoring that tactile graphics using your current tools?â€, â€œCan you tell us about the tools you usually use and how they help you with the authoring process?â€ Based on this context and common ground, we proceeded to ask them questions to understand current challenges and needs for visual content authoring tools. We asked, â€œWhat are some pain points or challenges in your authoring workflow?â€ and asked follow-up questions depending on their pain points. In the last phase of the interview, we asked participants about their experience of co-creating tactile graphics with AI, if any. We then brainstormed with the participants using guiding questions such as, â€œWhat is the input you would initially provide to ChatGPT, and what output would you expect it to return?â€
and â€œhow do you imagine iterating and editing on the graphics using ChatGPT?â€ in order to understand their expectation and perceptions towards AI. All sessions were video recorded, and in a few instances, participants also emailed us artifacts of their creations.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Analysis</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">After the interviews, video recordings of the sessions were transcribed using the Zoom transcription features, and we read through the transcripts to ensure correctness (approximately 300 minutes of recordings). Using an open-coding approachÂ <cite class="ltx_cite ltx_citemacro_citep">(Denzin and Lincoln, <a href="#bib.bib18" title="" class="ltx_ref">2011</a>)</cite>, two of the authors then independently coded all of the transcripts using ATLAS.tiÂ <cite class="ltx_cite ltx_citemacro_citep">(Inc., <a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite>. Over multiple cycles of discussion amongst the authors, we synthesized the key emergent themes around visual content authoring. We continued refining these themes until every category and subtopic was addressed, and no additional themes surfaced. Specific themes included qualitative descriptions and characteristics of good tactile graphics, resources used to learn about visual content, tooling, color, visual attributes, specific features for editing, perception during authoring, limitations of current workflows, unexpected outputs from generative AI, and imagined uses.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2408.10240/assets/x1.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="145" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>The participantsâ€™ previous works, including a pen and crayon drawing, the original grid layout for placing images, and an artwork of a bridge written in SVG code. From the left, P2 showed us drawings using ballpoint pens and crayola crayons on paper. P3 showed us the process of using a grid layout to create shape cutouts. On the right, P5 showed us an example artwork of the Golden Gate Bridge using SVG Code.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S3.F2.1" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F2.2" class="ltx_p ltx_figure_panel ltx_align_center">The participantsâ€™ previous works including a pen and crayon drawing, the original grid layout for placing images, and an artwork of a bridge written in SVG code. From the left, P2 showed us drawings using ballpoint pens and crayola crayons on paper. P3 showed us the process of using a grid layout to create shape cutouts. On the right, P5 showed us an example artwork of the Golden Gate Bridge using SVG Code. From the right there is an image created by P2 â€” A sketched artwork of three figures, possibly a family, with two larger figures and a smaller one in the middle. They appear to be dancing or interacting playfully. The image is created with ballpoint pen and colored with Crayola crayons on paper. P3 â€” A photo of a hands-on crafting process showing a black greeting card with a cut-out pattern placed on a white grid layout. The card features a star-like pattern made from the cut-outs. P5 â€” A digital illustration of a red suspension bridge, possibly resembling the Golden Gate Bridge, against a white background. This artwork is noted as being created by writing SVG code.</p>
</div>
</div>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Findings</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Across all sessions, participants reported needing to navigate multiple tools to author spatial content (both visual and tactile). Depending on the fidelity of the graphics they were creating, participants made use of low-fidelity tools, such as Wikki Stix and Sensational Blackboards. In many cases, they would later switch to more high-fidelity vector graphics authoring tools, but that often required assistance from sighted users. When digitally authoring, participants reported frequently printing out the in-progress content as tactile graphics to assess and provide feedback or revise on their own. Based on these authoring workflows, here we focus our findings on key low-level steps during authoring and associated challenges:</p>
</div>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1. </span>Composition and Layout</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">In many of the content that participants reported creating, they used existing graphic elements as a starting point, and the focus was more on composition. For instance, in one session, P3 designed holiday-themed greeting cards: <span id="S3.SS4.SSS1.p1.1.1" class="ltx_text ltx_font_italic">â€œI was given a bunch of images, and I am using a physical grid to position them. I then work with a sighted individual to run it through Inkscape and print it out to create a holiday greeting card.â€</span> Other participants also described using a similar grid-based approach for layout composition. In such cases, they reported needing to perform several calculations about the dimensions of the image and grid coordinates to place the items in the correct position. According to P5: <span id="S3.SS4.SSS1.p1.1.2" class="ltx_text ltx_font_italic">â€œSo okay, 50 units is half an inch. 25 units is a quarter of an inch, and with that kind of spatial reasoning and a lot of time in practice, you kind of get a sense of what the numbers youâ€™re dialing in are going to create on your canvasâ€¦however, this becomes complicated to re-calibrate when editing the location many times.â€</span> During composition, participants reported needing agency in specifying in detail where different elements will go. As P1 mentioned: <span id="S3.SS4.SSS1.p1.1.3" class="ltx_text ltx_font_italic">â€œSay I am creating a seascape, in my mind, I can imagine how it will look, I know where everything goes, it is all in my head, this is what I want.â€</span> Based on these insights, we infer that authoring tools that combine constructive and generative approaches should <span id="S3.SS4.SSS1.p1.1.4" class="ltx_text ltx_font_bold">provide precise control over the composition and layout of generated content without needing to manually calculate specific coordinates (D1)</span>.</p>
</div>
<div id="S3.SS4.SSS1.p2" class="ltx_para">
<p id="S3.SS4.SSS1.p2.1" class="ltx_p">Specific to using existing graphics, participants regularly looked for online content as a starting point to create their illustrations. According to P3: <span id="S3.SS4.SSS1.p2.1.1" class="ltx_text ltx_font_italic">â€œmy first process is to look for images online, and Iâ€™ll look for keywords like black and white line art coloring pages and outline drawings.â€</span> Further, P3 adds that they often need additional processing to make the visual compatible with tactile graphics: <span id="S3.SS4.SSS1.p2.1.2" class="ltx_text ltx_font_italic">â€œI did re-scale and do some color management with a software called Tactile view to eliminate some unwanted color. And I use the latest version of Windows photos to zap some backgrounds that were creating distracting background dotsâ€</span>. As an alternative, two participants also reported having learned how to code support vector graphics (SVG) manually to create digital content but the process can be cognitively intensive. P1 also added that they donâ€™t always want a hand-drawn look and feel. Therefore, we want tools <span id="S3.SS4.SSS1.p2.1.3" class="ltx_text ltx_font_bold">to make it easy for creators to access tactile graphics compatible illustrations (D2)</span>.</p>
</div>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2. </span>Iterative Editing</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p">Across sessions, participants mentioned a range of intents for editing the graphics being created. The majority of editing was done to resize elements. When digitally authoring, participants reported frequently printing out the in-progress graphics to assess the level of detail and whether elements were too close to each other, or if the element was too small to perceive the detail. In the seascape example, P1 mentioned: <span id="S3.SS4.SSS2.p1.1.1" class="ltx_text ltx_font_italic">â€œWe want it to be fun when we interact with it, feeling the curve of the shell or legs of the crab. So if the image is too small, those details get lost, and I have to make it bigger.â€</span> Other edit operations included changing the appearance of the objects, such as curves and edges, or making some details less prominent through the use of primary and secondary lines to clarify which details should stand out. Based on these insights authoring tools should support <span id="S3.SS4.SSS2.p1.1.2" class="ltx_text ltx_font_bold">â€œdimension-based editing as well as feature-based editing (D3)â€</span></p>
</div>
</section>
<section id="S3.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3. </span>Feedback During Authoring</h4>

<div id="S3.SS4.SSS3.p1" class="ltx_para">
<p id="S3.SS4.SSS3.p1.1" class="ltx_p">A key challenge in digital authoring tools is the lack of feedback during authoring. All participants reported seeking inputs from sighted individuals to assess and describe the graphic being authored. Alternatively, they have to print it out as tactile graphics to assess on their own. According to P5: <span id="S3.SS4.SSS3.p1.1.1" class="ltx_text ltx_font_italic">â€œâ€¦, she [sighted spouse] can look at it and tell me you know where to move it or if it needs to be adjusted. But if Iâ€™m by myself, then yeah, itâ€™s a lot of iteration, just a lot of printing back and forth.â€</span> Participants aspired future tools would require fewer print cycles. Specific to editing tasks, participants commented that it would be helpful for tools to provide feedback on well-known constraints such as overlapping edges or extending beyond the border. According to P1: <span id="S3.SS4.SSS3.p1.1.2" class="ltx_text ltx_font_italic">â€œhave visual spell-checksâ€¦have categories of things like what lines extending beyond borders.â€</span> Participants also mentioned detailed verbal descriptions would be helpful (e.g., <span id="S3.SS4.SSS3.p1.1.3" class="ltx_text ltx_font_italic">â€œon the top left corner you have a sea shell (P1)â€</span>). Moreover, participants described such verbal descriptions should be collaborative and through a dialog where the creator can go back and forth to get into different kinds of details. Lastly, participants also reported that by nature, image editors rely on visual cues to understand the state of an image, making them inaccessible to those with visual impairments. It would be helpful if the tool had both verbal descriptions and sound feedback to convey the state of the graphics. Drawing from these insights, tools should <span id="S3.SS4.SSS3.p1.1.4" class="ltx_text ltx_font_bold">aim to minimize the number of print iterations (D4)</span> by <span id="S3.SS4.SSS3.p1.1.5" class="ltx_text ltx_font_bold">providing verbal and auditory feedback through dialogic interactions (D5).</span></p>
</div>
</section>
<section id="S3.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.4. </span>Use of Generative AI</h4>

<div id="S3.SS4.SSS4.p1" class="ltx_para">
<p id="S3.SS4.SSS4.p1.1" class="ltx_p">Three of the five participants reported having explored different ways to use ChatGPT in their authoring workflow. For instance P3 reporting using GPT to get an initial SVG representation of an object and then iterated on their own. P1 was enthusiastic but also cautioned about its limitations based on their experience. According to P3: <span id="S3.SS4.SSS4.p1.1.1" class="ltx_text ltx_font_italic">â€œItâ€™s fun and interesting to write a description and get a sense of what AI has created, but it is also challenging. It doesnâ€™t do a good job of spatially putting them where you want them to be. Thing A and Thing B and how you want them to be related to each other. Being able to specify that would be very helpful.â€</span> They further added that describing takes a lot of work; it can be cognitively intensive and ambiguous. Along similar lines, P4 expressed that AI might help with creating some initial shapes to help guide their own authoring but not doing anything elaborate. Lastly, participants felt that if they were trying to create something unique, AI may not be able to accurately generate the illustration.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>User Experience</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Based on the guidelines from the formative study, we developed AltCanvas to help BVI users create visual content. As shown in FigureÂ <a href="#S4.F3" title="Figure 3 â€£ 4. User Experience â€£ AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, AltCanvasâ€™s interface consists of two main regions: (1) on the left half of the interface is a tile view for authoring, and (2) on the right is an image view that will render the image being authored.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2408.10240/assets/images/ux_onLoadHori.png" id="S4.F3.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="179" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3. </span> <span id="S4.F3.2.1" class="ltx_text ltx_font_bold">Main Interface</span> (A) Region of the Tile Based Interface. (B) Region of the Canvas Interface. (C) Region of the Keyboard shortcut commands (D) System Settings with Canvas Size, Image style, and Speech Speed. The screen (D) will pop up initially for the user to set settings. Users can access the keyboard commands screen C when pressing SHIFT + K. Users can navigate through the screen (A) and (B) regions while editing the image.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F3.3" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F3.4" class="ltx_p ltx_figure_panel">A series of interface screenshots labeled from A to D illustrate different sections of the AltCanvas. A. Region of the Tile Based Interface. B. Region of the Canvas Interface. C. Region of the Keyboard shortcut commands D. System Settings with Image Style and Speech Speed. The screen D will pop up initially for the user for settings. Users can access the keyboard commands screen C when pressing SHIFT + K. Users can navigate through the screen A and B region while editing the image. In part A, titled â€TileRegionâ€ has a matrix of tiles, which are used to select and place images within the canvas. The tile currently selected is highlighted. Part B, called â€Canvas Region,â€ displays a canvas area with a 600x600 pixel grid where a line drawing of a running dog and a tree has been placed. Part C features a pop-up window with â€Keyboard Instructions,â€ listing shortcuts like â€™Shift + Kâ€™ for keyboard command instructions, and other combinations for various editing actions. Finally, part D shows the â€System Settingsâ€ window, where users can adjust the canvas size width and height, image style between tactile and color graphics and control speech speed for audio feedback.</p>
</div>
</div>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The tile view itself is comprised of <span id="S4.p2.1.1" class="ltx_text ltx_font_italic">dynamically expanding</span> interactive tiles that the user can navigate through directional commands (up, down, left, right). Each tile represents a single object on the image canvas. It provides a functional proxy for the creation and manipulation of that object, enabling users to control the objectâ€™s properties effortlessly. The layout of the tiles on the tile view closely aligns with the relative positions of objects on the canvas. The tile view starts with a single tile, and once an object is associated with that tile, eight new tiles are added adjacent to that tile (left, right, top, down, top-left, top-right, bottom-left, bottom-right) to place objects relative to that tile object. Note that the tiles are all <span id="S4.p2.1.2" class="ltx_text ltx_font_italic">uniform</span> and do not represent the distance or size of the object on the canvas. Rather, they provide users with an easy way to navigate through images on a canvas and to select regions on the canvas for new generations by utilizing relative locations of image objects. In an earlier iteration, we experimented with absolute positioning based on the object arrangement on canvas by encoding size and distance (similar to the current grid-based techniques our participants reported). For instance, we can imagine dividing the canvas into a <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="5\times 5" display="inline"><semantics id="S4.p2.1.m1.1a"><mrow id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mn id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p2.1.m1.1.1.1" xref="S4.p2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><times id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">5</cn><cn type="integer" id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">5\times 5</annotation></semantics></math> grid and assigning one or more tiles to an object depending on how much space it occupies. However, this made the navigation less consistent and required large changes in the tile positioning as objects moved around on the canvas. In the final design, we opted to encode just the relative position as tiles (based on design consideration D1). Size and distance are encoded using sonification and speech, which we will describe later in this section (based on design consideration D5).</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">To better understand how AltCanvas supports a constructive and generative image authoring workflow, let us look at how Otto, a blind user, can create an artwork of his <span id="S4.p3.1.1" class="ltx_text ltx_font_italic">dog playing with a Frisbee in a park</span>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Setup and Tutorial</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To begin with, Otto opens AltCanvas on his web browser. Because this is the first time Otto is using the system, he presses SHIFT + K to hear the keyboard commands in the system. This will open a popup with the keyboard commands listed. <span id="S4.SS1.p1.1.1" class="ltx_text" style="color:#000000;">These shortcuts were designed with reference to existing blind-accessible games and industry-defined accessibility practices, ensuring consistency and learnability (e.g., using SHIFT+ [LETTER] of the function name).</span> Using the arrow keys, Otto can navigate through the 10 commands in order (see Appendix for a list of keyboard shortcuts). The first down arrow key will read the first command SHIFT + G Global canvas description to Otto. Further, AltCanvas supports stereo audio with panning across the left and right microphone to support directional navigation on the canvas.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>System Setting: Speech Rate</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">Since users have different speeds at which they recognize speech, depending on their proficiency with a screen reader, a system should allow the user to control different levels of speech rate. Before starting the prototype, users can choose their desired speech rate. Users can select from three distinct speech rate options within the system. The highest setting, rated as 3, matches the rapid delivery typical of screen readers. The intermediate option moderates the speed to a level between typical human speech and the swift pace of screen readers. The lowest setting, rated as 1, is calibrated to the comfortable listening speed for the everyday user.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>System Setting: Image Style</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">As different users have varying needs for image creation with editors, our system offers primarily two methods of image authoring support: images to share with general audiences and tactile graphics. Users can select the type of image they wish to createâ€”a colored version to share with general audiences or a tactile version that can assist with tactile graphic generation.</p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3. </span>System Setting: Canvas Size</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">While the canvas size is set to default based on the userâ€™s screen width, on the system setting pop-up, users have the option to change the size of the canvas through keyboard input.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Adding Objects to the Canvas</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Once familiar with the keyboard navigation commands, Otto exists the tutorial view and sees AltCanvasâ€™s the main authoring interface. By default, AltCanvas focuses on the single new tile on the tile view and greets Otto with an auditory guide: <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">â€œYou are currently focused on the first tile. Press Enter to generate the image on the 600 by 600 canvas.â€</span> Otto presses Enter, and a distinct beep earcon confirms the system is ready for voice input. Following the beep, Otto voices his request, <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">â€œCreate an image of a dog.â€</span> The system quickly processes his command and seeks confirmation, audibly prompting, <span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_italic">â€œDetected: Create an image of a dog. Press Enter to confirm or the Escape key to cancel.â€</span> Otto confirms by pressing Enter, signaling the system to generate the image. Based on design consideration D2, we use a text-to-image model to generate the image.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Once the image generation is complete, it is rendered on the canvas. By default, the first image is positioned at the center of the canvas. AltCanvas provided Otto with a detailed description of the generated image (D4, D5): <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_italic">â€œDog has been generated. The coordinates of the image are 250 by 250. The dog is a golden retriever with golden hair and a smiling expression. The image measures 100 by 100.â€</span> Initially, all images are generated at a size of 100 by 100. The coordinates of these images correspond to their relative positions on the canvas. Otto can retain this initial generation and proceed with further image creation and editing, or he can press the Enter key to regenerate the image with a different description or press Shift + X to delete the image if he wishes to try other objects for his artwork.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Once the dog image is generated, 8 new tiles are added adjacent to the dog image tile. This allows Otto to begin adding other objects to the scene relative to the dog object. During keyboard navigation of the tiles, when Otto reaches the edge of the existing tiles, he will hear an audio cue, a â€œthumpâ€ sound indicating he has reached the edge of the tile blocks. On a current empty tile, Otto will hear a navigation sound depending on whether he is moving up, down, left, or right. Based on iterative feedback, users preferred different directional sounds as they were navigating the tile view. When Otto enters a tile with an image, he will hear the name of the image itself. With the initial image accepted, Otto can now continue authoring the scene. He decides to add more elements to his canvas. By focusing on the tile to the right of the dog tile, he voices another command,<span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_italic"> â€œAdd an image of a tree with a thick trunkâ€</span>, and follows the same confirmation process as before. The system seamlessly integrates this new element into the existing canvas, maintaining spatial awareness and providing Otto with real-time updates.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span><span id="S4.SS3.1.1" class="ltx_text" style="color:#000000;">Perception of AI Generated Content</span>
</h3>

<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2408.10240/assets/images/ux_describe.png" id="S4.F4.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="323" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4. </span> <span id="S4.F4.2.1" class="ltx_text ltx_font_bold">Image Descriptions</span> (A) Global Description of the canvas with multiple objects on it. The user can press the SHIFT +G command to hear the global description of the canvas. (B) Local Description: the user presses the SHIFT +I command to activate the local information. This will describe the image on the current tile to the user. (C) The Image Chat function. This command can be accessed through SHIFT +C. This feature will answer the question the user has about the image on the current tile to the user</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F4.3" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F4.4" class="ltx_p ltx_figure_panel">The two screenshots of interface to describe the AltCanvasâ€™s image descriptions. On the left, A. Global Description of the canvas is available. The user can press the SHIFT +G command to hear the global description of the canvas. On the right, B. Local Description activates the local information explanation by pressing the SHIFT +I command. This will describe the image on the current tile to the user. C. The Image Chat function. This command can be accessed through SHIFT +C. This feature will answer the question the user asks about the image on the current tile. In detail, the section A displays the â€™Global Canvas Descriptionâ€™ where a user has given a general description of a scene featuring a dog leaping towards the left to catch a frisbee, with a tree to the right of the dog. In section B, called â€™Local Canvas Description,â€™ thereâ€™s a focused description of a particular element dubbed â€™dog.â€™ It describes the dogâ€™s action, the art style, and provides specific coordinates and size for the drawing on the canvas. Section C shows â€™Image Chat Modeâ€™ where a voice command asking for the appearance of the dogâ€™s tail is given, and the software responds that the tail is narrow, pointy, and pointing upwards.</p>
</div>
</div>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Now that Otto has generated a few images, he wishes to understand the layout and orientation of objects on the canvas. There are a total of four commands that Otto can use to understand the image: <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Global Canvas Descriptions, Local Information, Radar Scan, and Chat </span>(to support design consideration D5). By using the SHIFT + G command, Otto activates the Global Canvas Description feature. This command provides an auditory overview of the entire canvas. The system describes the overall layout, any overlapping images, their relative positions, and the overall ambiance of the canvas, giving Otto a sense of visual aesthetics and composition. Once Otto presses this command, he hears the global description: <span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_italic">â€œYour canvas currently features a golden retriever in the center with a medium-sized tree to its right. Both images are well-separated with no overlap, set against a clear background.â€</span> At any time, Otto can press the Escape key to exit the narration. Second, the SHIFT + I command allows Otto to obtain detailed information about a specific image or tile he navigates to. When this command is activated, the system provides a description that includes the imageâ€™s precise coordinates on the canvas and its current dimensions. This localized information helps Otto understand the specific details of individual elements within his artwork. Otto focuses on the dog tile and presses the SHIFT + I command. The system speaks: <span id="S4.SS3.p1.1.3" class="ltx_text ltx_font_italic">â€œThe image depicts a golden retriever sitting upright and facing the viewer. The fur of the retriever is long, and it has a smiling face, with its tongue sticking out.â€</span></p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Third, to get a sense of objects surrounding the active tile, Otto can activate the Radar Scan feature by pressing the SHIFT+R keys. Based on the userâ€™s current location, the radar scan identifies the nearest objects by name and measures the distance from the current image to others sequentially. For instance, pressing SHIFT+R while focusing on a dog would result in the scan announcing <span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_italic">â€œTree, 20 pixels away.â€</span> Finally, for inquiries beyond the scope of automated descriptions, Otto can use the SHIFT + C command. This feature is designed for interactive engagement with the canvas (D5). Upon pressing SHIFT + C at an image tile, the system prompts, <span id="S4.SS3.p2.1.2" class="ltx_text ltx_font_italic">â€œAsk a question about the image and I will answer.â€</span> Following a beep earcon, Otto can vocalize specific questions about the image. This could include inquiries about color, style, or any other detailed attributes not covered in standard descriptions. Otto asks: <span id="S4.SS3.p2.1.3" class="ltx_text ltx_font_italic">â€œDescribe the shape of the dogâ€™s tail?â€</span> and the system responds, <span id="S4.SS3.p2.1.4" class="ltx_text ltx_font_italic">â€œThe dogâ€™s tail in this image is raised and curved upwards, reflecting a sense of excitement or alertness.â€</span></p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Editing and Composition</h3>

<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2408.10240/assets/images/ux_interact.png" id="S4.F5.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="176" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5. </span> <span id="S4.F5.3.1" class="ltx_text ltx_font_bold">Image Editing Operations</span> Edit image locations with SHIFT + L and arrow keys: A thump sound indicates edge collision (A), speech notifies object collision (B), spatial sounds and coordinates describe movement (C). SHIFT + S adjusts size with variable frequency tones (D). <span id="S4.F5.4.2" class="ltx_text" style="color:#000000;">When the size increases, the frequency increases, while as the size decreases, the frequency decreases.</span> Tile manipulation and radar scan reveal layout and distances (E, F). </figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F5.5" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F5.6" class="ltx_p ltx_figure_panel">Edit image locations with SHIFT + L and arrow keys: A thump sound indicates edge collision (A), speech notifies object collision (B), spatial sounds and coordinates describe movement (C). SHIFT + S adjusts size with variable frequency tones (D). Tile manipulation and radar scan reveal layout and distances (E, F).
Panel A features a simple line drawing of a dog in different positions, with a speech bubble indicating sound feedback when interacting with the image.
Panel B depicts the same dog with a speech bubble saying â€Overlapping with Frisbeeâ€ to signal a conflict in the image layout.
Panel C shows an arrow pointing to the right with a speech bubble, and text indicating that the current location is 200 by 300, suggesting a feature to move elements on the canvas.
Panel D provides audio feedback information where the frequency of the sound increases with the size of the image element, which in this case is a drawing of the dog.
<span id="S4.F5.6.1" class="ltx_text" style="color:#000000;">When the user presses the up arrow key to increase the size of an element, the system will play a sound with an increasing frequency to reflect the growth in size. Conversely, when the user presses the down arrow key to decrease the size of the element, the system will play a sound with a decreasing frequency to indicate the reduction in size.</span>
Panel E has a grid similar to panel A, with a highlighted tile indicating that it has been pushed to the left, showing how to move tiles within the interface.
Finally, panel F uses a radar scan metaphor to describe the spatial relationship between the elements on the canvas, with the tree being 20 pixels to the right, the frisbee 50 pixels up and 30 pixels left from the dogâ€™s position.</p>
</div>
</div>
</figure>
<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Using the above perceptual feedback features, Otto can iteratively and flexibly edit images as he adds them to the canvas. AltCanvas provides Otto with four essential editing operations to refine his artistic creations. These include<span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_bold"> modifying the location and size of objects, pushing images around the canvas, and deleting unwanted elements.</span> These operations were most commonly described by the formative study participants to compose objects in the scene. We deliberately opted not to support fine-grain edits of individual objects through direct manipulation, such as enlarging the size of the dogâ€™s ear. Instead, we use generative features and natural language prompts for such modifications. In sectionÂ <a href="#S8" title="8. Discussion â€£ AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we discuss these design trade-offs.</p>
</div>
<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1. </span>Editing the Location of an Image</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">To adjust the position of an image, Otto presses SHIFT + L to enter location edit mode. In this mode, using the arrow keys moves the image across the canvas. Each movement triggers an auditory notification confirming the action. If Ottoâ€™s adjustments cause the image to overlap with another object or reach the canvas edge, he receives specific audio cues. These cues include warnings of overlaps (<span id="S4.SS4.SSS1.p1.1.1" class="ltx_text ltx_font_italic">â€œOverlapping with Image Xâ€</span>) and sounds indicating the edge of the canvas. To check the imageâ€™s coordinates during editing, Otto can press SHIFT, which tells him the current position, with each press of an arrow key shifting the image by 20 units.</p>
</div>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2. </span>Editing the Size of an Image</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">To modify an imageâ€™s size, Otto initiates the process by pressing SHIFT + S, activating the size edit mode. Here, the UP and DOWN arrow keys adjust the imageâ€™s size. Increases in size are accompanied by a rising earcon frequency, while decreases produce a lowering frequency. This sensory feedback helps Otto visualize the changes in real-time. If he needs to know the exact dimensions during resizing, pressing SHIFT provides this information. Adjustments are made in increments of 10 units per arrow key press while maintaining the overall aspect ratio of the generated image.</p>
</div>
</section>
<section id="S4.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3. </span>Rearranging the Image</h4>

<div id="S4.SS4.SSS3.p1" class="ltx_para">
<p id="S4.SS4.SSS3.p1.1" class="ltx_p">When Otto needs to make space between images or reorganize the layout, he can use the push operation. By selecting an image and pressing SHIFT + ARROW KEY in the desired direction, the image shifts, clearing space for additional elements. This operation is confirmed audibly, <span id="S4.SS4.SSS3.p1.1.1" class="ltx_text ltx_font_italic">â€œPushed Image to the [direction],â€</span> providing Otto clear feedback on his action. This allows Otto to add new images in between existing images (e.g., adding a Frisbee object between the dog and the tree). The tile view serves as a persistent reference to augment his spatial cognition as he makes the edits.</p>
</div>
</section>
<section id="S4.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.4. </span>Deleting the Image</h4>

<div id="S4.SS4.SSS4.p1" class="ltx_para">
<p id="S4.SS4.SSS4.p1.1" class="ltx_p">If an image no longer fits Ottoâ€™s vision, he can remove it by pressing SHIFT + X. This command deletes the selected image, and Otto immediately hears, <span id="S4.SS4.SSS4.p1.1.1" class="ltx_text ltx_font_italic">â€œDeleted Image on the tile.â€</span> The removal of the image results in an empty tile, which alters the auditory navigation landscape, helping Otto understand that the space is now available for new creations.</p>
</div>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5. </span>Rendering the Final Image</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">After Otto completes editing his images on AltCanvas, he moves to the final stage, where the images are rendered into their finished forms. This section includes two specialized rendering options tailored to enhance tactile and visual experiences. For tactile graphic rendering, AltCanvas uses a dedicated model designed to optimize the image for tactile graphics. This rendering process adjusts the texture and relief of the image, making it suitable for tactile perception and interpretation. This option is particularly valuable for BVI users like Otto, as it transforms the digital image into a vector graphics format that can be printed using a tactile graphics printer. Additionally, Otto can choose to re-render the image to enhance its visual qualities, such as incorporating more naturalistic backgrounds or adjusting the color palette. This operation refines the visual elements of the image, making them more appealing and realistic. To do this, Otto provides a speech description of the type of background rendering he wishes to do on this final image. The re-rendering process might include adding shadow effects, lighting adjustments, and blending elements to create a cohesive scene that visually communicates Ottoâ€™s artistic intent. This version can be shared with a sighted audience or embedded into other content, such as talk slides or blog articles on the web. Example results of tactile and color graphics using our AltCanvas are shown in FigureÂ <a href="#S4.F6" title="Figure 6 â€£ 4.5. Rendering the Final Image â€£ 4. User Experience â€£ AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2408.10240/assets/images/render.png" id="S4.F6.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="292" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6. </span><span id="S4.F6.2.1" class="ltx_text ltx_font_bold">Final Image Rendering</span> The figure illustrates the background rendering options and edge detection features in AltCanvas. Four different rendered backgrounds showcase how users can select various scenes for their final image composition. The second rendering operation illustrates the tactile rendering operations available to the user.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F6.3" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F6.4" class="ltx_p ltx_figure_panel">The figure shows a two-tiered illustration. Once the user has generated the final image through AltCanvas, the figure depicts the rendering options that are available to the user. The first option is, â€™Background Rendering,â€™ The images display four examples of a rendered scene featuring a dog, a tree, and a frisbee (the image the user has generated) in different environments: a sunny day with blue skies, a night scene with a moonlit backdrop, an autumn setting with leaves on the ground, and a dusk scene with a sunset. Each background has a the prompt that the user inputs to generate the final image. The lower tier, â€™Edge Detection for Tactile Rendering Option,â€™ presents a black and white outline of the same scene for tactile rendering purposes, with a list of technical options for edge detection algorithms including Sobel and Canny.</p>
</div>
</div>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>System Implementation</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">AltCanvas is a web-based application employing a client-server architecture. FigureÂ <a href="#S5.F7" title="Figure 7 â€£ 5.3.4. Dynamic Tiles â€£ 5.3. Image Editing â€£ 5. System Implementation â€£ AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates the input-output pipeline, highlighting the systemâ€™s two primary components: the image generation module and the image description module. The process begins when a user issues a speech command, which is then parsed to create an image prompt for the Large Language Model (LLM) image generation model. Following image creation, the image description module generates a comprehensive, accessible description of the generated image.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Image Generation</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p"><span id="S5.SS1.p1.1.1" class="ltx_text" style="color:#000000;">The userâ€™s speech input (e.g., <span id="S5.SS1.p1.1.1.1" class="ltx_text ltx_font_italic">â€œcreate an image of a catâ€</span>) is initially transcribed and then passed along to a prompt rewriting pipeline (see Appendix sectionÂ <a href="#A1.SS5" title="A.5. Prompts used in the system â€£ Appendix A Appendix â€£ AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.5</span></a>). For tactile graphics generation, our base prompts align with the Braille Authority of North America (BANA) guidelines Â <cite class="ltx_cite ltx_citemacro_citep">(Braille Authority of North America (2022), <a href="#bib.bib11" title="" class="ltx_ref">BANA</a>)</cite>, emphasizing key features such as the absence of perspective, clear outlines, simplification, and elimination of unnecessary details. For other image types, our prompt focuses on limiting generation to a single object and excluding text. Once generated, the image undergoes background removal. The resulting image is then placed on the canvas. This refined prompt is passed to the GPT-4o model (parameters: n=1, style=natural, quality=hd) for image generation. Due to model limitations in infographic generation, our system currently focuses on object images.</span></p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Image Descriptions</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">After image generation, the resulting URL is input into the GPT-4o model to create a description tailored for visually impaired users. When the global canvas description mode is activated, the entire canvas is captured and processed through the GPT-4 API to generate a comprehensive description. The canvasâ€™s HTML element is captured using the html2canvas library and rendered as an image before being sent to the system for description generationÂ <cite class="ltx_cite ltx_citemacro_citep">(htm, <a href="#bib.bib2" title="" class="ltx_ref">2024</a>)</cite>. Users can employ this feature during the editing process to understand the current canvas state, checking for image overlap and positioning. The prompt used for description generation is provided in the Appendix (sectionÂ <a href="#A1.SS5" title="A.5. Prompts used in the system â€£ Appendix A Appendix â€£ AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.5</span></a>). During location and size editing, users require more rapid system responses for efficient manipulation.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Image Editing</h3>

<section id="S5.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1. </span>Size Editing</h4>

<div id="S5.SS3.SSS1.p1" class="ltx_para">
<p id="S5.SS3.SSS1.p1.1" class="ltx_p"><span id="S5.SS3.SSS1.p1.1.1" class="ltx_text" style="color:#000000;">Sound frequency maps to object size - higher frequencies for larger objects, lower for smaller ones as preferred by participants. Each time the user presses the up arrow key, the size of the object increases by a specified amount maintaining height and width ratio. Users can press the SHIFT+I key after pressing the up arrow key to understand the change in the size of the object. It will give them verbal feedback â€œsize 30â€ (height 30, width 30).</span></p>
</div>
</section>
<section id="S5.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.2. </span>Location Editing</h4>

<div id="S5.SS3.SSS2.p1" class="ltx_para">
<p id="S5.SS3.SSS2.p1.1" class="ltx_p"><span id="S5.SS3.SSS2.p1.1.1" class="ltx_text" style="color:#000000;">The location of the object can be adjusted using the keyboard arrows. Users can move elements in four directions: up, down, left, and right. Spatial sonification provides audio cues as the object is moved - users will hear an up, down, left, and right spatial sound as they move around the objects. If a user bumps into another element, they hear a thump sound indicating the collision. Users can press the SHIFT+I key to check the objectâ€™s current location in coordinates (X,Y) after moving it. The centerpoint of the object is called.</span></p>
</div>
</section>
<section id="S5.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.3. </span>Radar Scan</h4>

<div id="S5.SS3.SSS3.p1" class="ltx_para">
<p id="S5.SS3.SSS3.p1.1" class="ltx_p"><span id="S5.SS3.SSS3.p1.1.1" class="ltx_text" style="color:#000000;">Radar Scan calculates the distance between the current image that the user has selected and the surrounding elements. The center point of the object is used to determine the direction the object is located in. The descriptions include both proximity and direction information (e.g., â€œThe tree is 20 pixels to the right, the frisbee is 50 pixels up and 30 pixels leftâ€).</span></p>
</div>
</section>
<section id="S5.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.4. </span>Dynamic Tiles</h4>

<div id="S5.SS3.SSS4.p1" class="ltx_para">
<p id="S5.SS3.SSS4.p1.1" class="ltx_p"><span id="S5.SS3.SSS4.p1.1.1" class="ltx_text" style="color:#000000;">Tiles and images maintain direct correspondence, reflecting changes in positioning during editing. If an element is moved on the canvas, the corresponding tileâ€™s position will be updated to match this new layout.</span></p>
</div>
<figure id="S5.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2408.10240/assets/images/imageGenPipeline.png.png" id="S5.F7.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="224" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7. </span> <span id="S5.F7.2.1" class="ltx_text ltx_font_bold">Image Generation Pipeline</span> The audio of the user is captured and parsed to single out the main components of the prompt using a wink-pos-tagger. The parsed prompt is processed into Tactile Graphic Generation and Visual Color Generation Prompts based on the type of image the user wishes to generate. This to final prompt is passed into the image generation model (model: gpt-4-turbo, parameters: n=1, style=natural, quality= hd) based on the initial prompt and image type requirements. The final image is passed through a background removal API and the url is used to generate descriptions of the given image. </figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S5.F7.3" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.F7.4" class="ltx_p ltx_figure_panel">Pipeline for Image Generation : The audio of the user is captured and parsed to single out the main components of the prompt. Theprompt is processed into Tactile Graphic Generation and Visual Color Generation Prompts based on the type of image the user wishes to generate. This final prompt is passed into the image generation model and goes through a verification process based on the initial prompt and image type requirements. The final image is passed through a background removal API and the url is used to generate descriptions of the given image. The image is a flowchart detailing the process of generating an image from an audio recording. The flow starts with an audio recording, which goes through â€™Prompt <span id="S5.F7.4.1" class="ltx_text" style="color:#000000;">Rewritingâ€™, where we make the user prompt command more comprehensive to create detailed graphics.</span> â€™Tactile Graphic Generation Promptâ€™ and â€™Visual Color Image Generation Prompt,â€™ both leading into an â€™Image Generation Model.â€™ The output of the model is an â€™Image Description Generation,â€™ which presumably describes the generated image. This description is linked to â€™Saved Images,â€™ suggesting that the generated images are stored. Additionally, there is a â€™Background Removal APIâ€™ that processes the saved images to remove their backgrounds, illustrated by a transition from a full image to one where only the subject remains.</p>
</div>
</div>
</figure>
</section>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Implementation Details</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">The web-based client is developed using React and hosted on AWS Amplify <cite class="ltx_cite ltx_citemacro_citep">(Amazon Web Services, Inc., <a href="#bib.bib7" title="" class="ltx_ref">2024b</a>)</cite>. It incorporates the Web Speech API for speech recognition and utilizes the SpeechSynthesisUtterance interface for audio feedback to users<cite class="ltx_cite ltx_citemacro_citep">(Network, <a href="#bib.bib45" title="" class="ltx_ref">2024</a>)</cite>. An initial onboarding section guides users through various audio checks, facilitating interaction with spatial audio generated by Tone.js for navigation in four directions (left, right, top, and bottom) <cite class="ltx_cite ltx_citemacro_citep">(Mann, <a href="#bib.bib42" title="" class="ltx_ref">2024</a>)</cite>. Audio files, primarily MP3s, are loaded from an AWS S3 bucket and played using Tone.jsâ€™s spatial audio capabilities. AltCanvasâ€™s backend server is implemented using Node.js and Express.js <cite class="ltx_cite ltx_citemacro_citep">(Amazon Web Services, Inc., <a href="#bib.bib6" title="" class="ltx_ref">2024a</a>; Holowaychuk and other contributors, <a href="#bib.bib28" title="" class="ltx_ref">2024</a>)</cite>. It integrates API calls to the OpenAI API and a Background Removal API (removal.ai) for enhanced functionality <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib47" title="" class="ltx_ref">2024</a>; Removal.AI, <a href="#bib.bib53" title="" class="ltx_ref">2024</a>)</cite>. Additionally, the wink-pos-tagger library was used to parse the initial user prompt before sending it to the model <cite class="ltx_cite ltx_citemacro_citep">(GRAYPE Systems Private Limited, <a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite>. Sound effects for sonification, stored as MP3 files in a public AWS S3 bucket, were sourced from Pixabay under an open-access agreement. The backend infrastructure is hosted on AWS Elastic Beanstalk with a Caddy reverse proxy <cite class="ltx_cite ltx_citemacro_citep">(Pixabay, <a href="#bib.bib52" title="" class="ltx_ref">2024</a>; Holt and other contributors, <a href="#bib.bib29" title="" class="ltx_ref">2024</a>)</cite>. Images generated are transiently handled, only being stored on the server if the user opts to save them in the final Render Canvas page. To render the final image, we finetuned a model that does tactile image conversion. This model takes in an image that is a non-tactile graphic and converts it into a tactile graphic by stripping out color and creating more pronounced outlines. To render the final image, we used the stable-diffusion model.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Iterative Design Feedback</h2>

<figure id="S6.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2408.10240/assets/images/pilot.png" id="S6.F8.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="479" height="266" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8. </span> <span id="S6.F8.2.1" class="ltx_text ltx_font_bold">Overview of Interactions Reviewed in Preliminary Study for Design Feedback</span> The four main types of interactions that were tested in the pilot study. (A) Tile Navigation: here, we tested if users were able to gain a spatial understanding of the canvas through navigating through the tiles. (B) Global Info, Local Info, here we tested the descriptions of the image to gain an understanding of the different types of descriptions wanted. (C) Location Edit: Here, we studied how users navigated through changing the location of images and the types of information they wanted. (D) Size Edit: here, we studied how users changed the size of objects on the canvas.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S6.F8.3" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S6.F8.4" class="ltx_p ltx_figure_panel">The main four types of interactions that were tested in the pilot study. (A)Tile Navigation: here, we tested if users were able to gain a spatial understanding of the canvas by navigating through the tiles. (B) Global Info, Local Info, here we tested the descriptions of the image to gain an understanding of the different types of descriptions wanted. (C) Location Edit: Here, we studied how users navigated through changing the location of images and the types of information they wanted. (D) Size Edit, here, we studied how users changed the size of objects on the canvas. Panel A is titled â€œTile Navigationâ€ and illustrates a grid where each tile represents an element of the image. The panel shows a music note and a sun icon, with a label â€œDogâ€ on one of the tiles. A speech bubble indicates that sonification is used for tile navigation and that speech feedback is provided on the image content.
Panel B is labeled â€œGlobal Info, Local Infoâ€ and depicts two speech bubbles: one for a local description stating â€œThe dog is facing the left. The image depicts a line drawing.â€ and another for a global description mentioning a sun in the top right corner and a small dog in the center.
Panel C, â€œLocation Edit,â€ shows an interface where the position of image elements can be edited. A selected tile with a dog is highlighted, and a keyboard shortcut is suggested for editing the location.
Panel D, â€œSize Edit,â€ demonstrates how the size of an image element can be adjusted. It shows a tile with a sun, and an arrow icon implies the scaling function, with a keyboard shortcut for size editing mentioned.</p>
</div>
</div>
</figure>
<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">To design AltCanvasâ€™s features, we conducted a preliminary design study with six participants (2 blind and 4 with low vision). Using early working prototypes of AltCanvas design alternatives, we aimed to gather feedback on the intuitiveness of a tile-based interaction interface and to explore the intricacies of speech recognition, sound models, and sonification techniques. Concretely, we evaluated four categories of interactions. : (1) Tile Navigation: to understand whether or not the tile interaction was intuitive to users, (2) Global Info, Local Info: to understand if image descriptions were sufficient (3) Location Edit: To understand the location edit details of moving objects (4) Size Edit: To understand the size edit details of changing around sizes of objects (see FigureÂ <a href="#S6.F8" title="Figure 8 â€£ 6. Iterative Design Feedback â€£ AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>). Participants were recruited by reaching out to local organizations that work with BVI individuals, including the Vista Center for the BlindÂ <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://vistacenter.org/</span></span></span>, and LightHouse for the Blind and Visually ImpairedÂ <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://lighthouse-sf.org/</span></span></span>. All sessions were conducted over Zoom using the web-based prototypes. The sessions lasted between 30-45 minutes and participants received $25 for the time. Details about the participants can be found in TableÂ <a href="#S3.T1" title="Table 1 â€£ 3. Formative Study with Blind Visual Content Creators â€£ AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Here, we report the specific tasks along with summaries of participant feedback, which we incorporated into the final system design and implementation.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Interaction Alternatives and Participant Feedback</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">For the study, we started with a simple illustration that we created that comprised a dog, a dog bowl, and the sun. A total of three objects in the scene.</p>
</div>
<section id="S6.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1. </span>Tile Navigation</h4>

<div id="S6.SS1.SSS1.p1" class="ltx_para">
<p id="S6.SS1.SSS1.p1.1" class="ltx_p">To assess our tile-based interactions with authoring, we instructed participants to explore the scene through the tiles with an arrow key and, at the end, provide us a description of the location of objects on the canvas. As the users moved through the tiles, they were able to hear sonified feedback and image descriptions when they landed on objects. For instance, the dog tile would make a subtle barking sound, the bowl the sound of metal, and the sun a â€˜flareâ€™ sound. Further, we added a short navigation sound as feedback, indicating the focus was on the new tile.</p>
</div>
<div id="S6.SS1.SSS1.p2" class="ltx_para">
<p id="S6.SS1.SSS1.p2.1" class="ltx_p"><span id="S6.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Feedback from Participants:</span> All participants achieved a comprehensive understanding of the spatial orientation of the canvas via the tile-based interface. Each participant could accurately articulate the orientation of objects on the canvas, although the identical sounds used for different directional interactions (top, down, bottom, up) caused some confusion despite spatial audio enhancements. P4 suggested, <span id="S6.SS1.SSS1.p2.1.2" class="ltx_text ltx_font_italic">â€œSounds are like colors to us; I want to hear more dynamic and varied sounds for each direction.â€</span> The majority favored hearing the direct names of objects on each tile rather than personalized sonified notes. P2 noted that unique sounds for each object might lead to cognitive overload due to the mental effort needed to link distinct sounds with specific objects.</p>
</div>
</section>
<section id="S6.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2. </span>Global and Local Information</h4>

<div id="S6.SS1.SSS2.p1" class="ltx_para">
<p id="S6.SS1.SSS2.p1.1" class="ltx_p">Secondly, we wanted to understand usersâ€™ ability to process and utilize the descriptive information presented globally (whole system overview) versus locally (detailed segment information). Our goal was to determine the most effective method for conveying complex information to BVI users, which could significantly enhance their authoring and editing experience. In our prototype, users pressed the SHIFT + G and SHIFT + I commands to activate the global and local information and gave feedback on the descriptions they heard. The global description gave a description of the layout of the dog, the dog bowl, and the sun. The system said <span id="S6.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_italic">â€œThere is a dog in the center of the canvas, a dog bowl to the left of the dog, and then a sun on top of the dog.â€</span> For the local description, the system says, <span id="S6.SS1.SSS2.p1.1.2" class="ltx_text ltx_font_italic">â€œThis is a line drawing of a dog. The dog is facing the left.â€ </span></p>
</div>
<div id="S6.SS1.SSS2.p2" class="ltx_para">
<p id="S6.SS1.SSS2.p2.1" class="ltx_p"><span id="S6.SS1.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Feedback from Participants:</span> In the descriptions, users found it essential to have both coordinate information and visual location descriptions available. They requested more detailed global descriptions to better understand the overlapping and relative positions of objects. Additionally, there was a call for more interactive features to query information about the canvas that was not covered in the descriptions, such as asking, <span id="S6.SS1.SSS2.p2.1.2" class="ltx_text ltx_font_italic">â€œWhat color is the dog?â€</span> or <span id="S6.SS1.SSS2.p2.1.3" class="ltx_text ltx_font_italic">â€œWhat does the dog bowl look like?â€</span> For the local information, users requested an accessible way to easily retrieve coordinate or size information quickly without having to go through the entire image description.</p>
</div>
</section>
<section id="S6.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.3. </span>Location Edit</h4>

<div id="S6.SS1.SSS3.p1" class="ltx_para">
<p id="S6.SS1.SSS3.p1.1" class="ltx_p">We gathered feedback on interactions related to editing and repositioning graphical elements within the interface. We aimed to assess the intuitiveness, accuracy, and responsiveness of the system when users engage in such modifications, which are critical for ensuring effective interaction. Users pressed the SHIFT + L command to activate the location edit mode, once they were focused on the object they wished to edit. Then, they used the arrow keys to change the locations of objects. Users heard speech feedback to know that they entered the mode and heard either verbal descriptions of object movement such as â€œup 10â€ or sonification earcons. For this task, we asked participants to move the bowl closer to the bottom edge of the screen.</p>
</div>
<div id="S6.SS1.SSS3.p2" class="ltx_para">
<p id="S6.SS1.SSS3.p2.1" class="ltx_p"><span id="S6.SS1.SSS3.p2.1.1" class="ltx_text ltx_font_bold">Feedback from Participants:</span> Through the use of spatial audio, users were introduced to a variety of earcons that effectively communicated changes in location. The initial earcon consisted of a simple chime that emitted a spatial sound with each keystroke. This was followed by direct speech feedback, which provided precise information on their movements across the canvasâ€”for instance, â€œup 10.â€ Users commented on the lag of the speech feedback of the up 10 and how it could cause more confusion when the keyboard was pressed multiple times. Users commented that if they had a way to quickly know the changes of movement, the simple chime earcon would be sufficient. Overall, during location edits, users primarily sought two pieces of information: (1) the exact canvas location of objects and (2) the magnitude of their movements. For users adept with the grid coordinate system, navigating this setup was intuitive and straightforward. In contrast, those less familiar with the coordinates preferred a broader description of the objectâ€™s position, such as <span id="S6.SS1.SSS3.p2.1.2" class="ltx_text ltx_font_italic">â€œslightly to the center-right.â€</span> Moreover, users effectively discerned bump sounds at the edges of the canvas and identified sounds indicative of overlapping objects.</p>
</div>
</section>
<section id="S6.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.4. </span>Size Edit</h4>

<div id="S6.SS1.SSS4.p1" class="ltx_para">
<p id="S6.SS1.SSS4.p1.1" class="ltx_p">The final task is on how users adjust the size of visual elements within the interface. We wanted to understand the level of control and ease with which users can customize the visual aspects of the interface to suit their individual needs and preferences. Users pressed the SHIFT +S command to activate the size edit mode, once they were focused on the object they wished to edit. Then, they used the up and down arrow keys to change the sizes of objects on the canvas. Users heard speech feedback to know that they entered the mode, and heard a mixture of â€œincrease 10â€ size feedback or increase size frequency sound feedback. For this task, we asked users to increase the size of the dog to make it larger than the bowl.</p>
</div>
<div id="S6.SS1.SSS4.p2" class="ltx_para">
<p id="S6.SS1.SSS4.p2.1" class="ltx_p"><span id="S6.SS1.SSS4.p2.1.1" class="ltx_text ltx_font_bold">Feedback from Participants:</span> Contrary to typical size increase feedback, users identified an increase in frequency as an indication of larger object sizes, while a decrease in frequency signaled a reduction in size. As with location edits, users sought precise numerical details regarding size changes. Additionally, they expressed interest in understanding the relative sizes of objects on the canvas, achieved by activating the global information mode (SHIFT + G).</p>
</div>
</section>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Usability Evaluation</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We conducted a user study to evaluate the overall usability and effectiveness of our tile-based paradigm and the combined constructive and generative authoring workflow. The study was conducted via Zoom with 8 BLV users (6 blind, 2 low vision) as indicated in TableÂ <a href="#S3.T1" title="Table 1 â€£ 3. Formative Study with Blind Visual Content Creators â€£ AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. In addition to participants in prior studies, we recruited participants from local community organizations, including the Vista Center for the BlindÂ <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://vistacenter.org/</span></span></span>, and LightHouse for the Blind and Visually ImpairedÂ <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://lighthouse-sf.org/</span></span></span>. The study took approximately 90 minutes for each one-on-one session, and participants were compensated with a $50 dollar gift card for their time.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>Procedure</h3>

<section id="S7.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.1. </span>System Checks (5 minutes)</h4>

<div id="S7.SS1.SSS1.p1" class="ltx_para">
<p id="S7.SS1.SSS1.p1.1" class="ltx_p">At the start of each session, the participant conducted a system check using a calibration page in our system to confirm that the audio and speech were working in their system. This page was compatible with a screen reader, and participants went through the process on their own. For participants who were not familiar with their screen readers, we helped them navigate through this system and check through the remote control of the screen through Zoom. Participants tested their sound by pressing the left, right, and top buttons to hear the sounds that will be used in AltCanvas. The audio was tested through a practice round of screen recording. This process helped participants turn on browser settings for instances where the audio was not activated.</p>
</div>
</section>
<section id="S7.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.2. </span>Onboarding Tutorial (20 minutes)</h4>

<div id="S7.SS1.SSS2.p1" class="ltx_para">
<p id="S7.SS1.SSS2.p1.1" class="ltx_p">Once the audio setup was complete, we gave participants a detailed hands-on tutorial to help them gain familiarity with AltCanvasâ€™s features. We first explained the Tile Navigation and Image generation process and asked them to generate an image of a spoon. We then proceeded to go through the image understanding and editing commands. Participants were instructed to create an image of a fork. Next, we helped them understand how to edit the location of images using the Location Edit (SHIFT +L) command. Using this, the participants were instructed to move the fork to the left of the spoon, thereby experiencing the overlap interaction feedback. Then, we asked them to use Size Edit mode (SHIFT + S) to experience increasing and decreasing the size of the image. Finally, participants tested deleting the Image (SHIFT +X) and pushing the image on a tile to create tile space (SHIFT + ARROWKEY)</p>
</div>
</section>
<section id="S7.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.3. </span>Study Tasks (45 minutes)</h4>

<div id="S7.SS1.SSS3.p1" class="ltx_para">
<p id="S7.SS1.SSS3.p1.1" class="ltx_p">Once participants indicated familiarity, they proceeded to work on three illustration tasks. We provided specific scene descriptions for the first two tasks with varying complexity, and the third task was open-ended. In <span id="S7.SS1.SSS3.p1.1.1" class="ltx_text ltx_font_bold">Task 1</span>, we asked participants to (1) Create an image of a dog. Make the dog above 150 pixels in size. (2) Create an image of a dog bowl. Move the dog bowl to the left of the dog and make sure they do not overlap. (3) Create an image of a clock above the dog and place it on the top of the canvas. <span id="S7.SS1.SSS3.p1.1.2" class="ltx_text ltx_font_bold">Task 2</span> required them to generate five objects and perform eight different edit interactions. Specifically, participants were instructed to: (1) Create an image of a dining table. Make the table above 200 pixels in size. Place it on the bottom of the Canvas. (2) Create an image of a potted plant. Place it on top of the table. (3) Create an image of a window. Place the window in the top left corner. Make the window above 150 pixels in size. (4) Create an image of a clock. Place the clock in the top right corner. Make the clock above 150 in size. (5) Create an image of a cat. Place the cat on the bottom left corner of the canvas. In <span id="S7.SS1.SSS3.p1.1.3" class="ltx_text ltx_font_bold">Task 3</span>, an open-ended task, we asked participants to create an illustration of their choosing without any guidelines. We collected system log data, including all of the prompts and actions taken for each task for each participant. Refer to Appendix X to see some samples of the generated images across all tasks.</p>
</div>
</section>
<section id="S7.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.4. </span>Feedback Elicitation (20 minutes)</h4>

<div id="S7.SS1.SSS4.p1" class="ltx_para">
<p id="S7.SS1.SSS4.p1.1" class="ltx_p">After the study, participants were asked to respond to a set of nine usability questions on a scale of 1-7. Then we followed up with an open-ended discussion with some initial guiding prompts: â€œWhat do you think of using this tool for authoring illustrations? â€, â€œHow does AltCanvas compare to other tools or your conventional ways of making illustrations? â€, â€œAre there features that are missing or what would like to see in future iterations? Do you have any feedback to improve its usefulness?â€ â€œHow do you envision that this tool can be useful in your daily life?â€</p>
</div>
</section>
<section id="S7.SS1.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.5. </span>Evaluation of Created Illustrations</h4>

<div id="S7.SS1.SSS5.p1" class="ltx_para">
<p id="S7.SS1.SSS5.p1.1" class="ltx_p">After the final study, we printed out the graphics created by each participant and mailed it to them for feedback. We printed the graphics on a Swell Paper and used a PIAF printer to generate the tactile graphics. We added descriptions to each of the tactile graphics about the images that the graphic included. We followed up with them over email with questions to evaluate their printed graphics. Questions included â€œDoes the printed tactile graphic align with what you think you created in our tool?â€, â€œIf not, what are the specific misalignments between the generated image and your perception of using our tool?â€, â€œwhat information is currently missing about the image generation but could have been helpful during authoring and editing?â€, â€œwhat do you think you want to change about the images?â€, and â€œdo you have any feedback to improve its usefulness?â€ Participants responded to these questions by email.</p>
</div>
</section>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2. </span>Results</h3>

<figure id="S7.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2408.10240/assets/images/likert.png" id="S7.F9.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="305" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9. </span> <span id="S7.F9.2.1" class="ltx_text ltx_font_bold">LIKERT Scale responses for the final study</span> LIKERT Scale responses for the final study on a 7-point scale. Highest satisfaction can be observed for the questions â€œI could easily understand how to use this toolâ€ (avg = 6.5, std= 0.75), â€œThe voice command feature was intuitive to useâ€ (avg = 6.5, std=1.06). â€œThe tile-based interface was intuitive to useâ€ (avg= 5.8, std=2.03) and â€œI could easily edit the image as I was creating itâ€(avg= 6.3, std=0.7). Mixed responses were received for â€œI want to create more images with this toolâ€(avg=5, std= 2.3) and â€œI could easily generate the image using this toolâ€ (avg= 4.8, std=1.3).</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S7.F9.3" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S7.F9.4" class="ltx_p ltx_figure_panel">The image is a bar chart displaying user feedback on a tool, with each statement rated on a Likert scale ranging from â€œStrongly Disagreeâ€ to â€œStrongly Agree.â€ The vertical y-axis lists user statements, and the horizontal x-axis represents the percentage of responses. Darker shades of blue indicate stronger agreement, while red tones indicate disagreement. The statements, from top to bottom, are: I could easily understand how to use the tool. I want to create more images with this tool. I could easily generate the image using this tool. The voice command feature was intuitive to use. The sonification feature was intuitive to use. The tile-based interface was intuitive to use. I could easily edit the image as I was creating it. I could easily understand the state of the canvas through voice explanations. I could easily understand the state of the canvas through sounds. LIKERT Scale responses for the final study on a 7 point scale. Highest satisfaction can be observed for the questions â€œI could easily understand how to use this toolâ€ (avg = 6.5, std= 0.75), â€œThe voice command feature was intuitve to useâ€ (avg = 6.5, std=1.06). â€œThe tile based interface was intuitve to useâ€ (avg= 5.8, std=2.03) and â€œI could easily edit the image as I was creating itâ€(avg= 6.3, std=0.7). Mixed responses were received for â€œI want to create more images with this toolâ€(avg=5, std= 2.3) and â€œI could easily generate the image using this toolâ€ (avg= 4.8, std=1.3). The primary reason behind this is because of the unreliability of image generations from the AI model. The majority strongly agree that they could easily understand how to use the tool and that the voice command feature was intuitive. Most users agree or strongly agree that they want to create more images with the tool. Opinions vary more on the tile-based interface and the ease of editing images, with a spread across neutral to agree.</p>
</div>
</div>
</figure>
<section id="S7.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.1. </span>Image Generations</h4>

<div id="S7.SS2.SSS1.p1" class="ltx_para">
<p id="S7.SS2.SSS1.p1.5" class="ltx_p">Regarding the ease of use of the voice command feature, users responded to the question â€œThe voice command feature was intuitive to useâ€ with an overall high satisfaction (<math id="S7.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="avg=6.5" display="inline"><semantics id="S7.SS2.SSS1.p1.1.m1.1a"><mrow id="S7.SS2.SSS1.p1.1.m1.1.1" xref="S7.SS2.SSS1.p1.1.m1.1.1.cmml"><mrow id="S7.SS2.SSS1.p1.1.m1.1.1.2" xref="S7.SS2.SSS1.p1.1.m1.1.1.2.cmml"><mi id="S7.SS2.SSS1.p1.1.m1.1.1.2.2" xref="S7.SS2.SSS1.p1.1.m1.1.1.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS1.p1.1.m1.1.1.2.1" xref="S7.SS2.SSS1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS1.p1.1.m1.1.1.2.3" xref="S7.SS2.SSS1.p1.1.m1.1.1.2.3.cmml">v</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS1.p1.1.m1.1.1.2.1a" xref="S7.SS2.SSS1.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS1.p1.1.m1.1.1.2.4" xref="S7.SS2.SSS1.p1.1.m1.1.1.2.4.cmml">g</mi></mrow><mo id="S7.SS2.SSS1.p1.1.m1.1.1.1" xref="S7.SS2.SSS1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S7.SS2.SSS1.p1.1.m1.1.1.3" xref="S7.SS2.SSS1.p1.1.m1.1.1.3.cmml">6.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS1.p1.1.m1.1b"><apply id="S7.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S7.SS2.SSS1.p1.1.m1.1.1"><eq id="S7.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S7.SS2.SSS1.p1.1.m1.1.1.1"></eq><apply id="S7.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S7.SS2.SSS1.p1.1.m1.1.1.2"><times id="S7.SS2.SSS1.p1.1.m1.1.1.2.1.cmml" xref="S7.SS2.SSS1.p1.1.m1.1.1.2.1"></times><ci id="S7.SS2.SSS1.p1.1.m1.1.1.2.2.cmml" xref="S7.SS2.SSS1.p1.1.m1.1.1.2.2">ğ‘</ci><ci id="S7.SS2.SSS1.p1.1.m1.1.1.2.3.cmml" xref="S7.SS2.SSS1.p1.1.m1.1.1.2.3">ğ‘£</ci><ci id="S7.SS2.SSS1.p1.1.m1.1.1.2.4.cmml" xref="S7.SS2.SSS1.p1.1.m1.1.1.2.4">ğ‘”</ci></apply><cn type="float" id="S7.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S7.SS2.SSS1.p1.1.m1.1.1.3">6.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS1.p1.1.m1.1c">avg=6.5</annotation></semantics></math>) (FigureÂ <a href="#S7.F9" title="Figure 9 â€£ 7.2. Results â€£ 7. Usability Evaluation â€£ AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>). Responses to the statement, â€œI want to create more images with this toolâ€ were varied (<math id="S7.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="avg=5" display="inline"><semantics id="S7.SS2.SSS1.p1.2.m2.1a"><mrow id="S7.SS2.SSS1.p1.2.m2.1.1" xref="S7.SS2.SSS1.p1.2.m2.1.1.cmml"><mrow id="S7.SS2.SSS1.p1.2.m2.1.1.2" xref="S7.SS2.SSS1.p1.2.m2.1.1.2.cmml"><mi id="S7.SS2.SSS1.p1.2.m2.1.1.2.2" xref="S7.SS2.SSS1.p1.2.m2.1.1.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS1.p1.2.m2.1.1.2.1" xref="S7.SS2.SSS1.p1.2.m2.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS1.p1.2.m2.1.1.2.3" xref="S7.SS2.SSS1.p1.2.m2.1.1.2.3.cmml">v</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS1.p1.2.m2.1.1.2.1a" xref="S7.SS2.SSS1.p1.2.m2.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS1.p1.2.m2.1.1.2.4" xref="S7.SS2.SSS1.p1.2.m2.1.1.2.4.cmml">g</mi></mrow><mo id="S7.SS2.SSS1.p1.2.m2.1.1.1" xref="S7.SS2.SSS1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S7.SS2.SSS1.p1.2.m2.1.1.3" xref="S7.SS2.SSS1.p1.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS1.p1.2.m2.1b"><apply id="S7.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S7.SS2.SSS1.p1.2.m2.1.1"><eq id="S7.SS2.SSS1.p1.2.m2.1.1.1.cmml" xref="S7.SS2.SSS1.p1.2.m2.1.1.1"></eq><apply id="S7.SS2.SSS1.p1.2.m2.1.1.2.cmml" xref="S7.SS2.SSS1.p1.2.m2.1.1.2"><times id="S7.SS2.SSS1.p1.2.m2.1.1.2.1.cmml" xref="S7.SS2.SSS1.p1.2.m2.1.1.2.1"></times><ci id="S7.SS2.SSS1.p1.2.m2.1.1.2.2.cmml" xref="S7.SS2.SSS1.p1.2.m2.1.1.2.2">ğ‘</ci><ci id="S7.SS2.SSS1.p1.2.m2.1.1.2.3.cmml" xref="S7.SS2.SSS1.p1.2.m2.1.1.2.3">ğ‘£</ci><ci id="S7.SS2.SSS1.p1.2.m2.1.1.2.4.cmml" xref="S7.SS2.SSS1.p1.2.m2.1.1.2.4">ğ‘”</ci></apply><cn type="integer" id="S7.SS2.SSS1.p1.2.m2.1.1.3.cmml" xref="S7.SS2.SSS1.p1.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS1.p1.2.m2.1c">avg=5</annotation></semantics></math>, <math id="S7.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="std=2.3" display="inline"><semantics id="S7.SS2.SSS1.p1.3.m3.1a"><mrow id="S7.SS2.SSS1.p1.3.m3.1.1" xref="S7.SS2.SSS1.p1.3.m3.1.1.cmml"><mrow id="S7.SS2.SSS1.p1.3.m3.1.1.2" xref="S7.SS2.SSS1.p1.3.m3.1.1.2.cmml"><mi id="S7.SS2.SSS1.p1.3.m3.1.1.2.2" xref="S7.SS2.SSS1.p1.3.m3.1.1.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS1.p1.3.m3.1.1.2.1" xref="S7.SS2.SSS1.p1.3.m3.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS1.p1.3.m3.1.1.2.3" xref="S7.SS2.SSS1.p1.3.m3.1.1.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS1.p1.3.m3.1.1.2.1a" xref="S7.SS2.SSS1.p1.3.m3.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS1.p1.3.m3.1.1.2.4" xref="S7.SS2.SSS1.p1.3.m3.1.1.2.4.cmml">d</mi></mrow><mo id="S7.SS2.SSS1.p1.3.m3.1.1.1" xref="S7.SS2.SSS1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S7.SS2.SSS1.p1.3.m3.1.1.3" xref="S7.SS2.SSS1.p1.3.m3.1.1.3.cmml">2.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS1.p1.3.m3.1b"><apply id="S7.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S7.SS2.SSS1.p1.3.m3.1.1"><eq id="S7.SS2.SSS1.p1.3.m3.1.1.1.cmml" xref="S7.SS2.SSS1.p1.3.m3.1.1.1"></eq><apply id="S7.SS2.SSS1.p1.3.m3.1.1.2.cmml" xref="S7.SS2.SSS1.p1.3.m3.1.1.2"><times id="S7.SS2.SSS1.p1.3.m3.1.1.2.1.cmml" xref="S7.SS2.SSS1.p1.3.m3.1.1.2.1"></times><ci id="S7.SS2.SSS1.p1.3.m3.1.1.2.2.cmml" xref="S7.SS2.SSS1.p1.3.m3.1.1.2.2">ğ‘ </ci><ci id="S7.SS2.SSS1.p1.3.m3.1.1.2.3.cmml" xref="S7.SS2.SSS1.p1.3.m3.1.1.2.3">ğ‘¡</ci><ci id="S7.SS2.SSS1.p1.3.m3.1.1.2.4.cmml" xref="S7.SS2.SSS1.p1.3.m3.1.1.2.4">ğ‘‘</ci></apply><cn type="float" id="S7.SS2.SSS1.p1.3.m3.1.1.3.cmml" xref="S7.SS2.SSS1.p1.3.m3.1.1.3">2.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS1.p1.3.m3.1c">std=2.3</annotation></semantics></math>) (FigureÂ <a href="#S7.F9" title="Figure 9 â€£ 7.2. Results â€£ 7. Usability Evaluation â€£ AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>). P10 responded that they wanted to further explore â€œHow creative this could become.â€ The feedback regarding image generation with the model was generally positive, though there were some limitations. To the question, â€œI could easily generate images with this tool,â€ 6 users agreed, while 2 users disagreed (<math id="S7.SS2.SSS1.p1.4.m4.1" class="ltx_Math" alttext="avg=4.8" display="inline"><semantics id="S7.SS2.SSS1.p1.4.m4.1a"><mrow id="S7.SS2.SSS1.p1.4.m4.1.1" xref="S7.SS2.SSS1.p1.4.m4.1.1.cmml"><mrow id="S7.SS2.SSS1.p1.4.m4.1.1.2" xref="S7.SS2.SSS1.p1.4.m4.1.1.2.cmml"><mi id="S7.SS2.SSS1.p1.4.m4.1.1.2.2" xref="S7.SS2.SSS1.p1.4.m4.1.1.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS1.p1.4.m4.1.1.2.1" xref="S7.SS2.SSS1.p1.4.m4.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS1.p1.4.m4.1.1.2.3" xref="S7.SS2.SSS1.p1.4.m4.1.1.2.3.cmml">v</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS1.p1.4.m4.1.1.2.1a" xref="S7.SS2.SSS1.p1.4.m4.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS1.p1.4.m4.1.1.2.4" xref="S7.SS2.SSS1.p1.4.m4.1.1.2.4.cmml">g</mi></mrow><mo id="S7.SS2.SSS1.p1.4.m4.1.1.1" xref="S7.SS2.SSS1.p1.4.m4.1.1.1.cmml">=</mo><mn id="S7.SS2.SSS1.p1.4.m4.1.1.3" xref="S7.SS2.SSS1.p1.4.m4.1.1.3.cmml">4.8</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS1.p1.4.m4.1b"><apply id="S7.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S7.SS2.SSS1.p1.4.m4.1.1"><eq id="S7.SS2.SSS1.p1.4.m4.1.1.1.cmml" xref="S7.SS2.SSS1.p1.4.m4.1.1.1"></eq><apply id="S7.SS2.SSS1.p1.4.m4.1.1.2.cmml" xref="S7.SS2.SSS1.p1.4.m4.1.1.2"><times id="S7.SS2.SSS1.p1.4.m4.1.1.2.1.cmml" xref="S7.SS2.SSS1.p1.4.m4.1.1.2.1"></times><ci id="S7.SS2.SSS1.p1.4.m4.1.1.2.2.cmml" xref="S7.SS2.SSS1.p1.4.m4.1.1.2.2">ğ‘</ci><ci id="S7.SS2.SSS1.p1.4.m4.1.1.2.3.cmml" xref="S7.SS2.SSS1.p1.4.m4.1.1.2.3">ğ‘£</ci><ci id="S7.SS2.SSS1.p1.4.m4.1.1.2.4.cmml" xref="S7.SS2.SSS1.p1.4.m4.1.1.2.4">ğ‘”</ci></apply><cn type="float" id="S7.SS2.SSS1.p1.4.m4.1.1.3.cmml" xref="S7.SS2.SSS1.p1.4.m4.1.1.3">4.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS1.p1.4.m4.1c">avg=4.8</annotation></semantics></math>, <math id="S7.SS2.SSS1.p1.5.m5.1" class="ltx_Math" alttext="std=1.3" display="inline"><semantics id="S7.SS2.SSS1.p1.5.m5.1a"><mrow id="S7.SS2.SSS1.p1.5.m5.1.1" xref="S7.SS2.SSS1.p1.5.m5.1.1.cmml"><mrow id="S7.SS2.SSS1.p1.5.m5.1.1.2" xref="S7.SS2.SSS1.p1.5.m5.1.1.2.cmml"><mi id="S7.SS2.SSS1.p1.5.m5.1.1.2.2" xref="S7.SS2.SSS1.p1.5.m5.1.1.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS1.p1.5.m5.1.1.2.1" xref="S7.SS2.SSS1.p1.5.m5.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS1.p1.5.m5.1.1.2.3" xref="S7.SS2.SSS1.p1.5.m5.1.1.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS1.p1.5.m5.1.1.2.1a" xref="S7.SS2.SSS1.p1.5.m5.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS1.p1.5.m5.1.1.2.4" xref="S7.SS2.SSS1.p1.5.m5.1.1.2.4.cmml">d</mi></mrow><mo id="S7.SS2.SSS1.p1.5.m5.1.1.1" xref="S7.SS2.SSS1.p1.5.m5.1.1.1.cmml">=</mo><mn id="S7.SS2.SSS1.p1.5.m5.1.1.3" xref="S7.SS2.SSS1.p1.5.m5.1.1.3.cmml">1.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS1.p1.5.m5.1b"><apply id="S7.SS2.SSS1.p1.5.m5.1.1.cmml" xref="S7.SS2.SSS1.p1.5.m5.1.1"><eq id="S7.SS2.SSS1.p1.5.m5.1.1.1.cmml" xref="S7.SS2.SSS1.p1.5.m5.1.1.1"></eq><apply id="S7.SS2.SSS1.p1.5.m5.1.1.2.cmml" xref="S7.SS2.SSS1.p1.5.m5.1.1.2"><times id="S7.SS2.SSS1.p1.5.m5.1.1.2.1.cmml" xref="S7.SS2.SSS1.p1.5.m5.1.1.2.1"></times><ci id="S7.SS2.SSS1.p1.5.m5.1.1.2.2.cmml" xref="S7.SS2.SSS1.p1.5.m5.1.1.2.2">ğ‘ </ci><ci id="S7.SS2.SSS1.p1.5.m5.1.1.2.3.cmml" xref="S7.SS2.SSS1.p1.5.m5.1.1.2.3">ğ‘¡</ci><ci id="S7.SS2.SSS1.p1.5.m5.1.1.2.4.cmml" xref="S7.SS2.SSS1.p1.5.m5.1.1.2.4">ğ‘‘</ci></apply><cn type="float" id="S7.SS2.SSS1.p1.5.m5.1.1.3.cmml" xref="S7.SS2.SSS1.p1.5.m5.1.1.3">1.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS1.p1.5.m5.1c">std=1.3</annotation></semantics></math>) (FigureÂ <a href="#S7.F9" title="Figure 9 â€£ 7.2. Results â€£ 7. Usability Evaluation â€£ AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>). Most users were pleased with the quality of the generated images. The model often produced creative results from simple prompts, such as â€œan image of a dog,â€ leading to variations like â€œbeagle,â€ â€œgolden retriever,â€ or â€œcocker spaniel.â€ This unpredictability added a touch of fun for some users, with P7 and P5 commenting, â€œReally fun, you never know what youâ€™ll get out of the model.â€ and â€œEasy to create another image, and itâ€™s fun to see what the AI creates.â€ However, a few users felt frustrated when they couldnâ€™t generate exactly what they wanted. For P5, who had experience in generating SVG graphics, the lack of control over model responses was a source of concern. The participant commented that they had â€œlimited control over the creativity of the toolâ€ compared to generating free-form graphics on their own through SVG code. Despite this, the participant commented â€œI was impressed with the owl and the coffee mug, and for blind folks who may not know how to draw certain items, this could be a good way to help educate them and build up their image context and knowledge.â€</p>
</div>
</section>
<section id="S7.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.2. </span>Image Descriptions</h4>

<div id="S7.SS2.SSS2.p1" class="ltx_para">
<p id="S7.SS2.SSS2.p1.2" class="ltx_p">Users were overall satisfied with the generated image descriptions. To the question, â€œI could easily understand the state of the canvas through voice explanations.â€ users responded positively (<math id="S7.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="avg=6" display="inline"><semantics id="S7.SS2.SSS2.p1.1.m1.1a"><mrow id="S7.SS2.SSS2.p1.1.m1.1.1" xref="S7.SS2.SSS2.p1.1.m1.1.1.cmml"><mrow id="S7.SS2.SSS2.p1.1.m1.1.1.2" xref="S7.SS2.SSS2.p1.1.m1.1.1.2.cmml"><mi id="S7.SS2.SSS2.p1.1.m1.1.1.2.2" xref="S7.SS2.SSS2.p1.1.m1.1.1.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS2.p1.1.m1.1.1.2.1" xref="S7.SS2.SSS2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS2.p1.1.m1.1.1.2.3" xref="S7.SS2.SSS2.p1.1.m1.1.1.2.3.cmml">v</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS2.p1.1.m1.1.1.2.1a" xref="S7.SS2.SSS2.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS2.p1.1.m1.1.1.2.4" xref="S7.SS2.SSS2.p1.1.m1.1.1.2.4.cmml">g</mi></mrow><mo id="S7.SS2.SSS2.p1.1.m1.1.1.1" xref="S7.SS2.SSS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S7.SS2.SSS2.p1.1.m1.1.1.3" xref="S7.SS2.SSS2.p1.1.m1.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS2.p1.1.m1.1b"><apply id="S7.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S7.SS2.SSS2.p1.1.m1.1.1"><eq id="S7.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S7.SS2.SSS2.p1.1.m1.1.1.1"></eq><apply id="S7.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S7.SS2.SSS2.p1.1.m1.1.1.2"><times id="S7.SS2.SSS2.p1.1.m1.1.1.2.1.cmml" xref="S7.SS2.SSS2.p1.1.m1.1.1.2.1"></times><ci id="S7.SS2.SSS2.p1.1.m1.1.1.2.2.cmml" xref="S7.SS2.SSS2.p1.1.m1.1.1.2.2">ğ‘</ci><ci id="S7.SS2.SSS2.p1.1.m1.1.1.2.3.cmml" xref="S7.SS2.SSS2.p1.1.m1.1.1.2.3">ğ‘£</ci><ci id="S7.SS2.SSS2.p1.1.m1.1.1.2.4.cmml" xref="S7.SS2.SSS2.p1.1.m1.1.1.2.4">ğ‘”</ci></apply><cn type="integer" id="S7.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S7.SS2.SSS2.p1.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS2.p1.1.m1.1c">avg=6</annotation></semantics></math>, <math id="S7.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="std=0.5" display="inline"><semantics id="S7.SS2.SSS2.p1.2.m2.1a"><mrow id="S7.SS2.SSS2.p1.2.m2.1.1" xref="S7.SS2.SSS2.p1.2.m2.1.1.cmml"><mrow id="S7.SS2.SSS2.p1.2.m2.1.1.2" xref="S7.SS2.SSS2.p1.2.m2.1.1.2.cmml"><mi id="S7.SS2.SSS2.p1.2.m2.1.1.2.2" xref="S7.SS2.SSS2.p1.2.m2.1.1.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS2.p1.2.m2.1.1.2.1" xref="S7.SS2.SSS2.p1.2.m2.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS2.p1.2.m2.1.1.2.3" xref="S7.SS2.SSS2.p1.2.m2.1.1.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS2.p1.2.m2.1.1.2.1a" xref="S7.SS2.SSS2.p1.2.m2.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS2.p1.2.m2.1.1.2.4" xref="S7.SS2.SSS2.p1.2.m2.1.1.2.4.cmml">d</mi></mrow><mo id="S7.SS2.SSS2.p1.2.m2.1.1.1" xref="S7.SS2.SSS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S7.SS2.SSS2.p1.2.m2.1.1.3" xref="S7.SS2.SSS2.p1.2.m2.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS2.p1.2.m2.1b"><apply id="S7.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S7.SS2.SSS2.p1.2.m2.1.1"><eq id="S7.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S7.SS2.SSS2.p1.2.m2.1.1.1"></eq><apply id="S7.SS2.SSS2.p1.2.m2.1.1.2.cmml" xref="S7.SS2.SSS2.p1.2.m2.1.1.2"><times id="S7.SS2.SSS2.p1.2.m2.1.1.2.1.cmml" xref="S7.SS2.SSS2.p1.2.m2.1.1.2.1"></times><ci id="S7.SS2.SSS2.p1.2.m2.1.1.2.2.cmml" xref="S7.SS2.SSS2.p1.2.m2.1.1.2.2">ğ‘ </ci><ci id="S7.SS2.SSS2.p1.2.m2.1.1.2.3.cmml" xref="S7.SS2.SSS2.p1.2.m2.1.1.2.3">ğ‘¡</ci><ci id="S7.SS2.SSS2.p1.2.m2.1.1.2.4.cmml" xref="S7.SS2.SSS2.p1.2.m2.1.1.2.4">ğ‘‘</ci></apply><cn type="float" id="S7.SS2.SSS2.p1.2.m2.1.1.3.cmml" xref="S7.SS2.SSS2.p1.2.m2.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS2.p1.2.m2.1c">std=0.5</annotation></semantics></math>) (FigureÂ <a href="#S7.F9" title="Figure 9 â€£ 7.2. Results â€£ 7. Usability Evaluation â€£ AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>). Through the interaction data, we observed users utilizing the image description commands accessed by SHIFT +G (Global), SHIFT+I (Information), and SHIFT+C (Chat) after consecutive edit interactions and towards the end of image editing (FigureÂ <a href="#S7.F10" title="Figure 10 â€£ 7.2.2. Image Descriptions â€£ 7.2. Results â€£ 7. Usability Evaluation â€£ AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>). Image descriptions were also accessed by users during system edits as found in the Pilot Study. As users were editing the location and size, they often referred back to the coordinates and current width and height information to confirm their generations using the SHIFT key. Participants commented on certain confusions with the location coordinates and â€œhaving to do my own math.â€ We note that there is a trade-off with qualitative descriptors such as â€œtop-leftâ€ and exact coordinates when participants desire precision. In future iterations, we aim to look at using the questioning features to help with computation. Additionally, users utilized the image descriptions after image generations to confirm the visuals that they had created.</p>
</div>
<figure id="S7.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2408.10240/assets/images/interaction.png" id="S7.F10.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="332" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10. </span> <span id="S7.F10.2.1" class="ltx_text ltx_font_bold">Sequence of Edit Interactions</span> Sequence of Edit Interactions for Task One, Simple Image and Task Two, Difficult Image. The boxes correspond to the editing durations. A general trend of global and local image descriptions used after multiple edit interactions and towards the end of image editing can be observed. The green image descriptions refer to the separate image description commands accessed by SHIFT +G (Global), SHIFT+I (Information), and SHIFT+C (Chat). </figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S7.F10.3" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S7.F10.4" class="ltx_p ltx_figure_panel">The image is a grid-based chart representing the actions taken by participants in two separate tasks. Each row corresponds to a participant labeled P3, P5, P6, P7, P10, P12, P13, and P14. The columns represent different actions: Image Generation (blue), Location Edit (pink), Size Edit (yellow), Image Descriptions (green), and Delete Image (gray). Task 1 and Task 2 are shown in two separate sections of the grid. The participantsâ€™ actions are marked on the grid with colored squares corresponding to the actionsâ€™ legend, illustrating the sequence and frequency of each action performed by the participants across the tasks.A visualized Sequence of Edit Interactions for three tasks. The blue refers to image generation tasks, the pink refers to location edits, yellow refers to size edits, green refers to image descriptions, and gray refers to delete image. The boxes correspond to the editing durations. A general trend of global and local image descriptions used after multiple edit interactions and towards the end of image editing can be observed. For task one, a total of three images were generated, whereas in task two, a total of five images have been generated. Image generations are followed by an image delete for incorrect generations. The green image descriptions refer to the separate image description commands accessed by SHIFT +G (Global), SHIFT+I (Information), and SHIFT+C (Chat). The general trend shown in the chart is that participants engage in various image editing activities, with some actions being more common than others. In both Task 1 and Task 2, â€™Image Generationâ€™ (blue) and â€™Location Editâ€™ (pink) are quite prevalent across most participants, indicating these are frequent steps in the tasks. â€™Size Editâ€™ (yellow) also appears regularly, but less often than image generation or location editing. â€™Image Descriptionsâ€™ (green) are used moderately throughout, and â€™Delete Imageâ€™ (gray) is the least frequent action, suggesting that participants do not often delete images once they have been generated or edited. The pattern of actions seems consistent between Task 1 and Task 2, indicating similar behavior across tasks by the participants.</p>
</div>
</div>
</figure>
</section>
<section id="S7.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.3. </span>Image Editing Interactions</h4>

<div id="S7.SS2.SSS3.p1" class="ltx_para">
<p id="S7.SS2.SSS3.p1.4" class="ltx_p"><span id="S7.SS2.SSS3.p1.4.1" class="ltx_text ltx_font_bold">Tile-based system as an effective tool for relative image location navigation:</span>
Overall, users expressed satisfaction with the tile-based editing interface. To the question â€œThe tile-based interface was intuitive to use.â€ participants responded positively (<math id="S7.SS2.SSS3.p1.1.m1.1" class="ltx_Math" alttext="avg=5.8" display="inline"><semantics id="S7.SS2.SSS3.p1.1.m1.1a"><mrow id="S7.SS2.SSS3.p1.1.m1.1.1" xref="S7.SS2.SSS3.p1.1.m1.1.1.cmml"><mrow id="S7.SS2.SSS3.p1.1.m1.1.1.2" xref="S7.SS2.SSS3.p1.1.m1.1.1.2.cmml"><mi id="S7.SS2.SSS3.p1.1.m1.1.1.2.2" xref="S7.SS2.SSS3.p1.1.m1.1.1.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS3.p1.1.m1.1.1.2.1" xref="S7.SS2.SSS3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS3.p1.1.m1.1.1.2.3" xref="S7.SS2.SSS3.p1.1.m1.1.1.2.3.cmml">v</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS3.p1.1.m1.1.1.2.1a" xref="S7.SS2.SSS3.p1.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS3.p1.1.m1.1.1.2.4" xref="S7.SS2.SSS3.p1.1.m1.1.1.2.4.cmml">g</mi></mrow><mo id="S7.SS2.SSS3.p1.1.m1.1.1.1" xref="S7.SS2.SSS3.p1.1.m1.1.1.1.cmml">=</mo><mn id="S7.SS2.SSS3.p1.1.m1.1.1.3" xref="S7.SS2.SSS3.p1.1.m1.1.1.3.cmml">5.8</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS3.p1.1.m1.1b"><apply id="S7.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S7.SS2.SSS3.p1.1.m1.1.1"><eq id="S7.SS2.SSS3.p1.1.m1.1.1.1.cmml" xref="S7.SS2.SSS3.p1.1.m1.1.1.1"></eq><apply id="S7.SS2.SSS3.p1.1.m1.1.1.2.cmml" xref="S7.SS2.SSS3.p1.1.m1.1.1.2"><times id="S7.SS2.SSS3.p1.1.m1.1.1.2.1.cmml" xref="S7.SS2.SSS3.p1.1.m1.1.1.2.1"></times><ci id="S7.SS2.SSS3.p1.1.m1.1.1.2.2.cmml" xref="S7.SS2.SSS3.p1.1.m1.1.1.2.2">ğ‘</ci><ci id="S7.SS2.SSS3.p1.1.m1.1.1.2.3.cmml" xref="S7.SS2.SSS3.p1.1.m1.1.1.2.3">ğ‘£</ci><ci id="S7.SS2.SSS3.p1.1.m1.1.1.2.4.cmml" xref="S7.SS2.SSS3.p1.1.m1.1.1.2.4">ğ‘”</ci></apply><cn type="float" id="S7.SS2.SSS3.p1.1.m1.1.1.3.cmml" xref="S7.SS2.SSS3.p1.1.m1.1.1.3">5.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS3.p1.1.m1.1c">avg=5.8</annotation></semantics></math>, <math id="S7.SS2.SSS3.p1.2.m2.1" class="ltx_Math" alttext="std=2.03" display="inline"><semantics id="S7.SS2.SSS3.p1.2.m2.1a"><mrow id="S7.SS2.SSS3.p1.2.m2.1.1" xref="S7.SS2.SSS3.p1.2.m2.1.1.cmml"><mrow id="S7.SS2.SSS3.p1.2.m2.1.1.2" xref="S7.SS2.SSS3.p1.2.m2.1.1.2.cmml"><mi id="S7.SS2.SSS3.p1.2.m2.1.1.2.2" xref="S7.SS2.SSS3.p1.2.m2.1.1.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS3.p1.2.m2.1.1.2.1" xref="S7.SS2.SSS3.p1.2.m2.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS3.p1.2.m2.1.1.2.3" xref="S7.SS2.SSS3.p1.2.m2.1.1.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS3.p1.2.m2.1.1.2.1a" xref="S7.SS2.SSS3.p1.2.m2.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS3.p1.2.m2.1.1.2.4" xref="S7.SS2.SSS3.p1.2.m2.1.1.2.4.cmml">d</mi></mrow><mo id="S7.SS2.SSS3.p1.2.m2.1.1.1" xref="S7.SS2.SSS3.p1.2.m2.1.1.1.cmml">=</mo><mn id="S7.SS2.SSS3.p1.2.m2.1.1.3" xref="S7.SS2.SSS3.p1.2.m2.1.1.3.cmml">2.03</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS3.p1.2.m2.1b"><apply id="S7.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S7.SS2.SSS3.p1.2.m2.1.1"><eq id="S7.SS2.SSS3.p1.2.m2.1.1.1.cmml" xref="S7.SS2.SSS3.p1.2.m2.1.1.1"></eq><apply id="S7.SS2.SSS3.p1.2.m2.1.1.2.cmml" xref="S7.SS2.SSS3.p1.2.m2.1.1.2"><times id="S7.SS2.SSS3.p1.2.m2.1.1.2.1.cmml" xref="S7.SS2.SSS3.p1.2.m2.1.1.2.1"></times><ci id="S7.SS2.SSS3.p1.2.m2.1.1.2.2.cmml" xref="S7.SS2.SSS3.p1.2.m2.1.1.2.2">ğ‘ </ci><ci id="S7.SS2.SSS3.p1.2.m2.1.1.2.3.cmml" xref="S7.SS2.SSS3.p1.2.m2.1.1.2.3">ğ‘¡</ci><ci id="S7.SS2.SSS3.p1.2.m2.1.1.2.4.cmml" xref="S7.SS2.SSS3.p1.2.m2.1.1.2.4">ğ‘‘</ci></apply><cn type="float" id="S7.SS2.SSS3.p1.2.m2.1.1.3.cmml" xref="S7.SS2.SSS3.p1.2.m2.1.1.3">2.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS3.p1.2.m2.1c">std=2.03</annotation></semantics></math>) (Fig.Â <a href="#S7.F9" title="Figure 9 â€£ 7.2. Results â€£ 7. Usability Evaluation â€£ AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>). P14 commented that the tile interface â€œhelps with figuring out spatial awareness.â€ By navigating to tile locations with images, participants liked being able to locate the tiles on the interface and then move onto the canvas for further edits.
On the systemâ€™s intuitiveness, â€œI could easily understand how to use the tool.â€ Participants responded that (<math id="S7.SS2.SSS3.p1.3.m3.1" class="ltx_Math" alttext="avg=6.5" display="inline"><semantics id="S7.SS2.SSS3.p1.3.m3.1a"><mrow id="S7.SS2.SSS3.p1.3.m3.1.1" xref="S7.SS2.SSS3.p1.3.m3.1.1.cmml"><mrow id="S7.SS2.SSS3.p1.3.m3.1.1.2" xref="S7.SS2.SSS3.p1.3.m3.1.1.2.cmml"><mi id="S7.SS2.SSS3.p1.3.m3.1.1.2.2" xref="S7.SS2.SSS3.p1.3.m3.1.1.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS3.p1.3.m3.1.1.2.1" xref="S7.SS2.SSS3.p1.3.m3.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS3.p1.3.m3.1.1.2.3" xref="S7.SS2.SSS3.p1.3.m3.1.1.2.3.cmml">v</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS3.p1.3.m3.1.1.2.1a" xref="S7.SS2.SSS3.p1.3.m3.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS3.p1.3.m3.1.1.2.4" xref="S7.SS2.SSS3.p1.3.m3.1.1.2.4.cmml">g</mi></mrow><mo id="S7.SS2.SSS3.p1.3.m3.1.1.1" xref="S7.SS2.SSS3.p1.3.m3.1.1.1.cmml">=</mo><mn id="S7.SS2.SSS3.p1.3.m3.1.1.3" xref="S7.SS2.SSS3.p1.3.m3.1.1.3.cmml">6.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS3.p1.3.m3.1b"><apply id="S7.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S7.SS2.SSS3.p1.3.m3.1.1"><eq id="S7.SS2.SSS3.p1.3.m3.1.1.1.cmml" xref="S7.SS2.SSS3.p1.3.m3.1.1.1"></eq><apply id="S7.SS2.SSS3.p1.3.m3.1.1.2.cmml" xref="S7.SS2.SSS3.p1.3.m3.1.1.2"><times id="S7.SS2.SSS3.p1.3.m3.1.1.2.1.cmml" xref="S7.SS2.SSS3.p1.3.m3.1.1.2.1"></times><ci id="S7.SS2.SSS3.p1.3.m3.1.1.2.2.cmml" xref="S7.SS2.SSS3.p1.3.m3.1.1.2.2">ğ‘</ci><ci id="S7.SS2.SSS3.p1.3.m3.1.1.2.3.cmml" xref="S7.SS2.SSS3.p1.3.m3.1.1.2.3">ğ‘£</ci><ci id="S7.SS2.SSS3.p1.3.m3.1.1.2.4.cmml" xref="S7.SS2.SSS3.p1.3.m3.1.1.2.4">ğ‘”</ci></apply><cn type="float" id="S7.SS2.SSS3.p1.3.m3.1.1.3.cmml" xref="S7.SS2.SSS3.p1.3.m3.1.1.3">6.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS3.p1.3.m3.1c">avg=6.5</annotation></semantics></math>, <math id="S7.SS2.SSS3.p1.4.m4.1" class="ltx_Math" alttext="std=0.75" display="inline"><semantics id="S7.SS2.SSS3.p1.4.m4.1a"><mrow id="S7.SS2.SSS3.p1.4.m4.1.1" xref="S7.SS2.SSS3.p1.4.m4.1.1.cmml"><mrow id="S7.SS2.SSS3.p1.4.m4.1.1.2" xref="S7.SS2.SSS3.p1.4.m4.1.1.2.cmml"><mi id="S7.SS2.SSS3.p1.4.m4.1.1.2.2" xref="S7.SS2.SSS3.p1.4.m4.1.1.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS3.p1.4.m4.1.1.2.1" xref="S7.SS2.SSS3.p1.4.m4.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS3.p1.4.m4.1.1.2.3" xref="S7.SS2.SSS3.p1.4.m4.1.1.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS3.p1.4.m4.1.1.2.1a" xref="S7.SS2.SSS3.p1.4.m4.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS3.p1.4.m4.1.1.2.4" xref="S7.SS2.SSS3.p1.4.m4.1.1.2.4.cmml">d</mi></mrow><mo id="S7.SS2.SSS3.p1.4.m4.1.1.1" xref="S7.SS2.SSS3.p1.4.m4.1.1.1.cmml">=</mo><mn id="S7.SS2.SSS3.p1.4.m4.1.1.3" xref="S7.SS2.SSS3.p1.4.m4.1.1.3.cmml">0.75</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS3.p1.4.m4.1b"><apply id="S7.SS2.SSS3.p1.4.m4.1.1.cmml" xref="S7.SS2.SSS3.p1.4.m4.1.1"><eq id="S7.SS2.SSS3.p1.4.m4.1.1.1.cmml" xref="S7.SS2.SSS3.p1.4.m4.1.1.1"></eq><apply id="S7.SS2.SSS3.p1.4.m4.1.1.2.cmml" xref="S7.SS2.SSS3.p1.4.m4.1.1.2"><times id="S7.SS2.SSS3.p1.4.m4.1.1.2.1.cmml" xref="S7.SS2.SSS3.p1.4.m4.1.1.2.1"></times><ci id="S7.SS2.SSS3.p1.4.m4.1.1.2.2.cmml" xref="S7.SS2.SSS3.p1.4.m4.1.1.2.2">ğ‘ </ci><ci id="S7.SS2.SSS3.p1.4.m4.1.1.2.3.cmml" xref="S7.SS2.SSS3.p1.4.m4.1.1.2.3">ğ‘¡</ci><ci id="S7.SS2.SSS3.p1.4.m4.1.1.2.4.cmml" xref="S7.SS2.SSS3.p1.4.m4.1.1.2.4">ğ‘‘</ci></apply><cn type="float" id="S7.SS2.SSS3.p1.4.m4.1.1.3.cmml" xref="S7.SS2.SSS3.p1.4.m4.1.1.3">0.75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS3.p1.4.m4.1c">std=0.75</annotation></semantics></math>). Despite the number of keyboard commands, users were able to easily navigate to image generation, location edits, and size edit commands throughout the completion of the tasks.</p>
</div>
<div id="S7.SS2.SSS3.p2" class="ltx_para">
<p id="S7.SS2.SSS3.p2.3" class="ltx_p"><span id="S7.SS2.SSS3.p2.3.3" class="ltx_text" style="color:#000000;">It took an average of 10.4 minutes (ranging from 5 to 22.83 minutes) to complete Task 1 and an average of 21.5 minutes (ranging from 13 to 28 minutes) to complete Task 2. For the freeform task, we gave a time limit of 15 minutes. We also calculated the average time users spent on generation and editing operations across sessions. On average, users spent 6 minutes and 28 seconds (<math id="S7.SS2.SSS3.p2.1.1.m1.1" class="ltx_Math" alttext="std=3:01" display="inline"><semantics id="S7.SS2.SSS3.p2.1.1.m1.1a"><mrow id="S7.SS2.SSS3.p2.1.1.m1.1.1" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.cmml"><mrow id="S7.SS2.SSS3.p2.1.1.m1.1.1.2" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.2.cmml"><mrow id="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.cmml"><mi mathcolor="#000000" id="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.2" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.1" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.1.cmml">â€‹</mo><mi mathcolor="#000000" id="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.3" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.1a" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.1.cmml">â€‹</mo><mi mathcolor="#000000" id="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.4" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.4.cmml">d</mi></mrow><mo mathcolor="#000000" id="S7.SS2.SSS3.p2.1.1.m1.1.1.2.1" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.2.1.cmml">=</mo><mn mathcolor="#000000" id="S7.SS2.SSS3.p2.1.1.m1.1.1.2.3" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.2.3.cmml">3</mn></mrow><mo lspace="0.278em" mathcolor="#000000" rspace="0.278em" id="S7.SS2.SSS3.p2.1.1.m1.1.1.1" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.1.cmml">:</mo><mn mathcolor="#000000" id="S7.SS2.SSS3.p2.1.1.m1.1.1.3" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.3.cmml">01</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS3.p2.1.1.m1.1b"><apply id="S7.SS2.SSS3.p2.1.1.m1.1.1.cmml" xref="S7.SS2.SSS3.p2.1.1.m1.1.1"><ci id="S7.SS2.SSS3.p2.1.1.m1.1.1.1.cmml" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.1">:</ci><apply id="S7.SS2.SSS3.p2.1.1.m1.1.1.2.cmml" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.2"><eq id="S7.SS2.SSS3.p2.1.1.m1.1.1.2.1.cmml" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.2.1"></eq><apply id="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.cmml" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2"><times id="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.1.cmml" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.1"></times><ci id="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.2.cmml" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.2">ğ‘ </ci><ci id="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.3.cmml" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.3">ğ‘¡</ci><ci id="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.4.cmml" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.2.2.4">ğ‘‘</ci></apply><cn type="integer" id="S7.SS2.SSS3.p2.1.1.m1.1.1.2.3.cmml" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.2.3">3</cn></apply><cn type="integer" id="S7.SS2.SSS3.p2.1.1.m1.1.1.3.cmml" xref="S7.SS2.SSS3.p2.1.1.m1.1.1.3">01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS3.p2.1.1.m1.1c">std=3:01</annotation></semantics></math>) on image generation. Specific to editing, for size editing, the average total time was 5 minutes and 39 seconds (<math id="S7.SS2.SSS3.p2.2.2.m2.1" class="ltx_Math" alttext="std=3:01" display="inline"><semantics id="S7.SS2.SSS3.p2.2.2.m2.1a"><mrow id="S7.SS2.SSS3.p2.2.2.m2.1.1" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.cmml"><mrow id="S7.SS2.SSS3.p2.2.2.m2.1.1.2" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.2.cmml"><mrow id="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.cmml"><mi mathcolor="#000000" id="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.2" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.1" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.1.cmml">â€‹</mo><mi mathcolor="#000000" id="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.3" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.1a" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.1.cmml">â€‹</mo><mi mathcolor="#000000" id="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.4" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.4.cmml">d</mi></mrow><mo mathcolor="#000000" id="S7.SS2.SSS3.p2.2.2.m2.1.1.2.1" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.2.1.cmml">=</mo><mn mathcolor="#000000" id="S7.SS2.SSS3.p2.2.2.m2.1.1.2.3" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.2.3.cmml">3</mn></mrow><mo lspace="0.278em" mathcolor="#000000" rspace="0.278em" id="S7.SS2.SSS3.p2.2.2.m2.1.1.1" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.1.cmml">:</mo><mn mathcolor="#000000" id="S7.SS2.SSS3.p2.2.2.m2.1.1.3" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.3.cmml">01</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS3.p2.2.2.m2.1b"><apply id="S7.SS2.SSS3.p2.2.2.m2.1.1.cmml" xref="S7.SS2.SSS3.p2.2.2.m2.1.1"><ci id="S7.SS2.SSS3.p2.2.2.m2.1.1.1.cmml" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.1">:</ci><apply id="S7.SS2.SSS3.p2.2.2.m2.1.1.2.cmml" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.2"><eq id="S7.SS2.SSS3.p2.2.2.m2.1.1.2.1.cmml" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.2.1"></eq><apply id="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.cmml" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2"><times id="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.1.cmml" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.1"></times><ci id="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.2.cmml" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.2">ğ‘ </ci><ci id="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.3.cmml" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.3">ğ‘¡</ci><ci id="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.4.cmml" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.2.2.4">ğ‘‘</ci></apply><cn type="integer" id="S7.SS2.SSS3.p2.2.2.m2.1.1.2.3.cmml" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.2.3">3</cn></apply><cn type="integer" id="S7.SS2.SSS3.p2.2.2.m2.1.1.3.cmml" xref="S7.SS2.SSS3.p2.2.2.m2.1.1.3">01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS3.p2.2.2.m2.1c">std=3:01</annotation></semantics></math>). The location edit action had an average total time of 6 minutes and 19 seconds (<math id="S7.SS2.SSS3.p2.3.3.m3.1" class="ltx_Math" alttext="std=1:38" display="inline"><semantics id="S7.SS2.SSS3.p2.3.3.m3.1a"><mrow id="S7.SS2.SSS3.p2.3.3.m3.1.1" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.cmml"><mrow id="S7.SS2.SSS3.p2.3.3.m3.1.1.2" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.2.cmml"><mrow id="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.cmml"><mi mathcolor="#000000" id="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.2" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.1" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.1.cmml">â€‹</mo><mi mathcolor="#000000" id="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.3" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.1a" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.1.cmml">â€‹</mo><mi mathcolor="#000000" id="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.4" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.4.cmml">d</mi></mrow><mo mathcolor="#000000" id="S7.SS2.SSS3.p2.3.3.m3.1.1.2.1" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.2.1.cmml">=</mo><mn mathcolor="#000000" id="S7.SS2.SSS3.p2.3.3.m3.1.1.2.3" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.2.3.cmml">1</mn></mrow><mo lspace="0.278em" mathcolor="#000000" rspace="0.278em" id="S7.SS2.SSS3.p2.3.3.m3.1.1.1" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.1.cmml">:</mo><mn mathcolor="#000000" id="S7.SS2.SSS3.p2.3.3.m3.1.1.3" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.3.cmml">38</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS3.p2.3.3.m3.1b"><apply id="S7.SS2.SSS3.p2.3.3.m3.1.1.cmml" xref="S7.SS2.SSS3.p2.3.3.m3.1.1"><ci id="S7.SS2.SSS3.p2.3.3.m3.1.1.1.cmml" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.1">:</ci><apply id="S7.SS2.SSS3.p2.3.3.m3.1.1.2.cmml" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.2"><eq id="S7.SS2.SSS3.p2.3.3.m3.1.1.2.1.cmml" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.2.1"></eq><apply id="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.cmml" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2"><times id="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.1.cmml" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.1"></times><ci id="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.2.cmml" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.2">ğ‘ </ci><ci id="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.3.cmml" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.3">ğ‘¡</ci><ci id="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.4.cmml" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.2.2.4">ğ‘‘</ci></apply><cn type="integer" id="S7.SS2.SSS3.p2.3.3.m3.1.1.2.3.cmml" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.2.3">1</cn></apply><cn type="integer" id="S7.SS2.SSS3.p2.3.3.m3.1.1.3.cmml" xref="S7.SS2.SSS3.p2.3.3.m3.1.1.3">38</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS3.p2.3.3.m3.1c">std=1:38</annotation></semantics></math>).</span></p>
</div>
<div id="S7.SS2.SSS3.p3" class="ltx_para">
<p id="S7.SS2.SSS3.p3.2" class="ltx_p"><span id="S7.SS2.SSS3.p3.2.1" class="ltx_text ltx_font_bold">Sonification for Edit Interactions:</span>
To the question, â€œI could easily understand the state of the canvas through soundsâ€ (to gather feedback on sonification), users responded (<math id="S7.SS2.SSS3.p3.1.m1.1" class="ltx_Math" alttext="avg=6" display="inline"><semantics id="S7.SS2.SSS3.p3.1.m1.1a"><mrow id="S7.SS2.SSS3.p3.1.m1.1.1" xref="S7.SS2.SSS3.p3.1.m1.1.1.cmml"><mrow id="S7.SS2.SSS3.p3.1.m1.1.1.2" xref="S7.SS2.SSS3.p3.1.m1.1.1.2.cmml"><mi id="S7.SS2.SSS3.p3.1.m1.1.1.2.2" xref="S7.SS2.SSS3.p3.1.m1.1.1.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS3.p3.1.m1.1.1.2.1" xref="S7.SS2.SSS3.p3.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS3.p3.1.m1.1.1.2.3" xref="S7.SS2.SSS3.p3.1.m1.1.1.2.3.cmml">v</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS3.p3.1.m1.1.1.2.1a" xref="S7.SS2.SSS3.p3.1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS3.p3.1.m1.1.1.2.4" xref="S7.SS2.SSS3.p3.1.m1.1.1.2.4.cmml">g</mi></mrow><mo id="S7.SS2.SSS3.p3.1.m1.1.1.1" xref="S7.SS2.SSS3.p3.1.m1.1.1.1.cmml">=</mo><mn id="S7.SS2.SSS3.p3.1.m1.1.1.3" xref="S7.SS2.SSS3.p3.1.m1.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS3.p3.1.m1.1b"><apply id="S7.SS2.SSS3.p3.1.m1.1.1.cmml" xref="S7.SS2.SSS3.p3.1.m1.1.1"><eq id="S7.SS2.SSS3.p3.1.m1.1.1.1.cmml" xref="S7.SS2.SSS3.p3.1.m1.1.1.1"></eq><apply id="S7.SS2.SSS3.p3.1.m1.1.1.2.cmml" xref="S7.SS2.SSS3.p3.1.m1.1.1.2"><times id="S7.SS2.SSS3.p3.1.m1.1.1.2.1.cmml" xref="S7.SS2.SSS3.p3.1.m1.1.1.2.1"></times><ci id="S7.SS2.SSS3.p3.1.m1.1.1.2.2.cmml" xref="S7.SS2.SSS3.p3.1.m1.1.1.2.2">ğ‘</ci><ci id="S7.SS2.SSS3.p3.1.m1.1.1.2.3.cmml" xref="S7.SS2.SSS3.p3.1.m1.1.1.2.3">ğ‘£</ci><ci id="S7.SS2.SSS3.p3.1.m1.1.1.2.4.cmml" xref="S7.SS2.SSS3.p3.1.m1.1.1.2.4">ğ‘”</ci></apply><cn type="integer" id="S7.SS2.SSS3.p3.1.m1.1.1.3.cmml" xref="S7.SS2.SSS3.p3.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS3.p3.1.m1.1c">avg=6</annotation></semantics></math>, <math id="S7.SS2.SSS3.p3.2.m2.1" class="ltx_Math" alttext="std=0.75" display="inline"><semantics id="S7.SS2.SSS3.p3.2.m2.1a"><mrow id="S7.SS2.SSS3.p3.2.m2.1.1" xref="S7.SS2.SSS3.p3.2.m2.1.1.cmml"><mrow id="S7.SS2.SSS3.p3.2.m2.1.1.2" xref="S7.SS2.SSS3.p3.2.m2.1.1.2.cmml"><mi id="S7.SS2.SSS3.p3.2.m2.1.1.2.2" xref="S7.SS2.SSS3.p3.2.m2.1.1.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS3.p3.2.m2.1.1.2.1" xref="S7.SS2.SSS3.p3.2.m2.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS3.p3.2.m2.1.1.2.3" xref="S7.SS2.SSS3.p3.2.m2.1.1.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS3.p3.2.m2.1.1.2.1a" xref="S7.SS2.SSS3.p3.2.m2.1.1.2.1.cmml">â€‹</mo><mi id="S7.SS2.SSS3.p3.2.m2.1.1.2.4" xref="S7.SS2.SSS3.p3.2.m2.1.1.2.4.cmml">d</mi></mrow><mo id="S7.SS2.SSS3.p3.2.m2.1.1.1" xref="S7.SS2.SSS3.p3.2.m2.1.1.1.cmml">=</mo><mn id="S7.SS2.SSS3.p3.2.m2.1.1.3" xref="S7.SS2.SSS3.p3.2.m2.1.1.3.cmml">0.75</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS3.p3.2.m2.1b"><apply id="S7.SS2.SSS3.p3.2.m2.1.1.cmml" xref="S7.SS2.SSS3.p3.2.m2.1.1"><eq id="S7.SS2.SSS3.p3.2.m2.1.1.1.cmml" xref="S7.SS2.SSS3.p3.2.m2.1.1.1"></eq><apply id="S7.SS2.SSS3.p3.2.m2.1.1.2.cmml" xref="S7.SS2.SSS3.p3.2.m2.1.1.2"><times id="S7.SS2.SSS3.p3.2.m2.1.1.2.1.cmml" xref="S7.SS2.SSS3.p3.2.m2.1.1.2.1"></times><ci id="S7.SS2.SSS3.p3.2.m2.1.1.2.2.cmml" xref="S7.SS2.SSS3.p3.2.m2.1.1.2.2">ğ‘ </ci><ci id="S7.SS2.SSS3.p3.2.m2.1.1.2.3.cmml" xref="S7.SS2.SSS3.p3.2.m2.1.1.2.3">ğ‘¡</ci><ci id="S7.SS2.SSS3.p3.2.m2.1.1.2.4.cmml" xref="S7.SS2.SSS3.p3.2.m2.1.1.2.4">ğ‘‘</ci></apply><cn type="float" id="S7.SS2.SSS3.p3.2.m2.1.1.3.cmml" xref="S7.SS2.SSS3.p3.2.m2.1.1.3">0.75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS3.p3.2.m2.1c">std=0.75</annotation></semantics></math>). P10 mentioned that the sonification feature in tile navigation and location edits â€œhelps with figuring out spatial awarenessâ€ during the image editing. P3 commented that â€œup and down sounds were intuitive for the tile navigation.â€ Participants were satisfied with the sonification changes as image sizes increased in frequency when the image size increased and decreased when the size was decreasing. Users also expressed satisfaction with the Canvas Edge and Image overlapping sounds and were able to complete the overlap interactions (placing a potted plant on a table) and placing the window and clock successfully on the top corners of the canvas as seen in the Appendix.</p>
</div>
<div id="S7.SS2.SSS3.p4" class="ltx_para">
<p id="S7.SS2.SSS3.p4.1" class="ltx_p"><span id="S7.SS2.SSS3.p4.1.1" class="ltx_text ltx_font_bold">Verbal Feedback for Editing:</span>
Overall, participants effectively used verbal descriptions and image information throughout the editing process. The image descriptions were utilized to confirm image generations, verify current canvas states, and ensure that their desired interactions were displayed correctly (Fig.Â <a href="#S7.F10" title="Figure 10 â€£ 7.2.2. Image Descriptions â€£ 7.2. Results â€£ 7. Usability Evaluation â€£ AltCanvas: A Tile-Based Image Editor with Generative AI for Blind or Visually Impaired People" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>). For example, participants frequently used the SHIFT +G Global description to hear the global description of the canvas after placing the potted plant on the table. While participants had heard that the image was overlapping to confirm the final image, they listened to the global description before confirming completion.</p>
</div>
</section>
<section id="S7.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.4. </span>Quality of Final Illustration</h4>

<div id="S7.SS2.SSS4.p1" class="ltx_para">
<p id="S7.SS2.SSS4.p1.1" class="ltx_p">Five out of eight participants responded by email, offering feedback on the printed tactile graphics that were shipped to them. All participants responded positively to the match between their perceived illustration (using feedback from the tool) during the authoring and the final printed output. P3 responded that â€œalmost precisely how I imagined it would be placed.â€ Participants also suggested additional features that could enhance the toolâ€™s functionality, such as â€œoptions to know the location of the picture through degrees like around the clockâ€”2 oâ€™clock, 3 oâ€™clock,â€ and â€œoptions to add backgrounds and adjust them with the editing tool.â€</p>
</div>
<div id="S7.SS2.SSS4.p2" class="ltx_para">
<p id="S7.SS2.SSS4.p2.1" class="ltx_p"><span id="S7.SS2.SSS4.p2.1.1" class="ltx_text" style="color:#000000;">Participants recommended several enhancements to improve both functionality and accessibility. They suggested expanding editing capabilities by adding background adjustment, image rotation/flipping, and multi-edit functionality. To increase accessibility, they proposed improving assistive technology integration, particularly enhancing compatibility with screen readers. Optimizing keyboard accessibility through streamlined navigation and editing shortcuts was also emphasized. Additionally, participants recommended implementing stereo audio spaces to facilitate more intuitive spatial navigation of the canvas. These proposed improvements aim to create a more comprehensive and accessible tool catering to users with varying levels of expertise and visual abilities.</span></p>
</div>
</section>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Discussion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">AltCanvas introduces a novel workflow for creating visual content through generative AI, offering enhanced control over editing interactions and scene composition. Our study reveals that users value control over their creative outputs and understanding of the underlying processes. AltCanvasâ€™s tile-based paradigm, serving as an alternative view of the drawing canvas, provides visually impaired users with diverse creative editing options, including dimension and location edits, image regeneration, and compositions. This approach offers a level of controllability previously unavailable, enabling a more engaged and informed creative experience.</p>
</div>
<section id="S8.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1. </span>Broader Utility of AltCanvas</h3>

<div id="S8.SS1.p1" class="ltx_para">
<p id="S8.SS1.p1.1" class="ltx_p"><span id="S8.SS1.p1.1.1" class="ltx_text" style="color:#000000;">More generally, AltCanvasâ€™s tile-based paradigm can support spatial understanding and manipulation of a variety of visual content. Participants in our study, including those with no prior image editing experience, identified various potential applications for AltCanvas. These ranged from editing childrenâ€™s books to creating visual graphics and developing educational materials. Notably, participants suggested integrating the tile-based editing approach into mainstream software like Microsoft PowerPoint and Appleâ€™s publishing tools, indicating its potential to enhance spatial content editing for a wider range of users. </span></p>
</div>
<div id="S8.SS1.p2" class="ltx_para">
<p id="S8.SS1.p2.1" class="ltx_p"><span id="S8.SS1.p2.1.1" class="ltx_text" style="color:#000000;">In enabling access to such applications, AltCanvasâ€™s tile-based interaction offers an alternative to traditional linear text descriptions and aligns more closely with natural spatial cognition. For instance, in educational settings, the spatial exploration facilitated by our tile navigation could enhance studentsâ€™ understanding of complex diagrams. Rather than relying on sequential descriptions like â€œthe heart is above the stomach, which is left of the liver,â€ students using AltCanvas can navigate the tile grid using directional commands, building a more intuitive mental map. They might start at the heart tile, move down to the stomach, and then right to the liver, experiencing spatial relationships directly. This non-linear exploration allows for immediate comparison of element positions and sizes, fostering a more comprehensive understanding of the overall layout. </span></p>
</div>
<div id="S8.SS1.p3" class="ltx_para">
<p id="S8.SS1.p3.1" class="ltx_p"><span id="S8.SS1.p3.1.1" class="ltx_text" style="color:#000000;">Additionally, AltCanvasâ€™s sonification features, such as distinct sounds for tile navigation and object overlap, provide multi-sensory feedback that further reinforces spatial awareness, offering a richer, more interactive learning experience than traditional text-to-speech descriptions. For visually impaired users, sonification features serve a role similar to colors in visual imagery. Sonification provides real-time auditory feedback, creating aural landmarks that aid in forming mental maps of canvas layouts. Keyboard controls with specific sound feedback reinforce spatial changes, allowing users to discern object positions and relationships through varied tones. For instance, in music production, the tile-based system could be adapted for spatial audio mixing, with sound sources placed and manipulated on a virtual soundstage. For urban planning, sonification could represent traffic flow or population density, providing an intuitive way to understand city dynamics. Our study found that sonification effectively communicated spatial information and canvas state, providing an enjoyable editing experience. Users could adjust image locations using spatial sounds and detect overlaps through audio cues. This approach addresses the challenge of alternating between task interfaces, screen readers, and additional plugins often required for spatial navigationÂ <cite class="ltx_cite ltx_citemacro_citep">(Saha etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2023</a>)</cite>. </span></p>
</div>
<div id="S8.SS1.p4" class="ltx_para">
<p id="S8.SS1.p4.1" class="ltx_p"><span id="S8.SS1.p4.1.1" class="ltx_text" style="color:#000000;">The broader implications of AltCanvas extend to the field of accessible technology and AI-assisted creative tools. For instance, in professional contexts such as graphic design, AltCanvasâ€™s approach can enable visually impaired designers to create and edit complex layouts independently. In educational publishing, it could allow visually impaired educators to generate and arrange visual elements in presentations or digital textbooks. For game development, this approach could enable the creation of audio-centric games with rich, spatially-aware environments. These examples illustrate how AltCanvasâ€™s integration of generative AI with intuitive interaction methods points towards more inclusive and adaptable creative tools, potentially transforming workflows and expanding creative possibilities across various professional contexts and disciplines.</span></p>
</div>
</section>
<section id="S8.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2. </span>Limitations and Future Work</h3>

<section id="S8.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.2.1. </span>Complex Editing Operations</h4>

<div id="S8.SS2.SSS1.p1" class="ltx_para">
<p id="S8.SS2.SSS1.p1.1" class="ltx_p">In designing AltCanvas, our primary focus was to utilize blind spatial cognition, allowing for a streamlined and iterative authoring process without adding complexity. Therefore, our current implementation does not support advanced editing features like one may find in line-by-line drawing tools or vector graphics authoring tools such as Adobe Illustrator. In our earlier iterations, we prototyped nested tile views at the object level (tiles representing parts of a dog) and scene level inspired by the nested grid drawing method by Kamel and LandayÂ <cite class="ltx_cite ltx_citemacro_citep">(Kamel and Landay, <a href="#bib.bib36" title="" class="ltx_ref">2000b</a>)</cite>. However, since the tile views are configured dynamically, we found that the effort required to acquire the spatial representations of generated elements can become challenging for users to remember. Similarly, for our tile-based authoring, we opted for eight directions for relative object placement as opposed to fine-grained navigation, such as segments of 10-degree angles. Our directional implementation, while allowing for â€œput it thereâ€ spatial cognition, the degree of freedom can be constrained, requiring multiple steps to position objects. Future work can look at offloading more editing functionality to generative AI while exploring better and more accurate perceptual feedback.</p>
</div>
</section>
<section id="S8.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.2.2. </span>Sonification Experiences</h4>

<div id="S8.SS2.SSS2.p1" class="ltx_para">
<p id="S8.SS2.SSS2.p1.1" class="ltx_p">Our implementation is limited in the range of expressive sonification interactions. Future work should look into generating more diverse and interactive sounds for editing interactions. Based on participant feedback, there was a desire to represent the entire canvas in stereo space. Future developments should focus on creating more diverse and interactive sounds for editing interactions. This direction in development would not only enhance the functionality of the tools but also significantly improve the user experience by providing a more intuitive and immersive editing environment.</p>
</div>
</section>
<section id="S8.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.2.3. </span>Use of Generative AI</h4>

<div id="S8.SS2.SSS3.p1" class="ltx_para">
<p id="S8.SS2.SSS3.p1.1" class="ltx_p"><span id="S8.SS2.SSS3.p1.1.1" class="ltx_text" style="color:#000000;">While generative AI offers powerful capabilities for image creation, it also presents significant limitations in the context of illustration tools for visually impaired users. The unpredictability of AI-generated outputs can be challenging, as users may not always obtain the precise image they envision. This lack of fine-grained control over specific details can be particularly frustrating for users who have a clear mental image of their desired illustration. These constraints highlight the need for continued research into more controllable and interpretable AI models for image generation, as well as the development of hybrid approaches that combine AI generation with more traditional, user-controlled editing techniques. For instance, by incorporating features like user image uploading or Retrieval Augmented Generation (RAG), we can provide more control over image generation. Image uploading would allow users to work with existing visuals, potentially enhancing or modifying them within the AltCanvas environment. This could be particularly useful for tasks like adapting educational materials or refining pre-existing designs. RAG, on the other hand, could improve the accuracy and relevance of generated images by leveraging a curated knowledge base. For instance, when creating illustrations for specific subjects or styles, RAG could ensure that generated content aligns more closely with user intent by drawing from relevant visual and contextual information.</span></p>
</div>
</section>
</section>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9. </span>Conclusion</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">In conclusion, our work presents a novel visual content authoring paradigm for BVI users, enhancing their ability to create, interact with, and understand visual content. By introducing a tile-based interface that provides an alternative view of the drawing canvas, we enable spatial comprehension and interactions for authoring visual scenes. Additionally, the use of generative text-to-image AI models allows for the generation of complex images from simple voice commands and natural language descriptions, providing a way for creative expression previously dominated by basic, less expressive authoring tools. Our approach combines the expressiveness of generative AI with a constructive method that respects user agency, enabling users to incrementally build and modify visual scenes with heightened control and flexibility. Throughout this project, our engagements with 14 BVI participantsâ€”through interviews, feedback sessions, and usability evaluationsâ€”have been crucial. The findings from our evaluation show that participants were successfully able to create illustrations using AltCanvasâ€™s tile-based paradigm and authoring workflow. Our work contributes to a more inclusive environment for visual arts and digital content creation with generative AI.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
We thank our participants for their time and valuable inputs in the design of AltCanvas. We also thank the reviewers for their feedback on the paper. This work is supported through the AI Research Institutes program by the National Science Foundation and the Institute of Education Sciences, U.S. Department of Education through Award <math id="S9.1.m1.1" class="ltx_Math" alttext="\#2229873" display="inline"><semantics id="S9.1.m1.1a"><mrow id="S9.1.m1.1.1" xref="S9.1.m1.1.1.cmml"><mi mathvariant="normal" id="S9.1.m1.1.1.2" xref="S9.1.m1.1.1.2.cmml">#</mi><mo lspace="0em" rspace="0em" id="S9.1.m1.1.1.1" xref="S9.1.m1.1.1.1.cmml">â€‹</mo><mn id="S9.1.m1.1.1.3" xref="S9.1.m1.1.1.3.cmml">2229873</mn></mrow><annotation-xml encoding="MathML-Content" id="S9.1.m1.1b"><apply id="S9.1.m1.1.1.cmml" xref="S9.1.m1.1.1"><times id="S9.1.m1.1.1.1.cmml" xref="S9.1.m1.1.1.1"></times><ci id="S9.1.m1.1.1.2.cmml" xref="S9.1.m1.1.1.2">#</ci><cn type="integer" id="S9.1.m1.1.1.3.cmml" xref="S9.1.m1.1.1.3">2229873</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S9.1.m1.1c">\#2229873</annotation></semantics></math> - National AI Institute for Exceptional Education.

</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">htm (2024)</span>
<span class="ltx_bibblock">
2024.

</span>
<span class="ltx_bibblock">html2canvas: Screenshots with JavaScript.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://html2canvas.hertzen.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://html2canvas.hertzen.com/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-04-24.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Acosta etÂ al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Tania Acosta, Patricia Acosta-Vargas, Luis Salvador-Ullauri, and Sergio LujÃ¡n-Mora. 2018.

</span>
<span class="ltx_bibblock">Method for accessibility assessment of online content editors. In <em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Information Technology &amp; Systems (ICITS 2018)</em>. Springer, 538â€“551.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adobe Systems Incorporated (2023a)</span>
<span class="ltx_bibblock">
Adobe Systems Incorporated. 2023a.

</span>
<span class="ltx_bibblock">Adobe Illustrator.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.adobe.com/products/illustrator.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.adobe.com/products/illustrator.html</a>

</span>
<span class="ltx_bibblock">Accessed: 2023-04-22.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adobe Systems Incorporated (2023b)</span>
<span class="ltx_bibblock">
Adobe Systems Incorporated. 2023b.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Adobe Photoshop</em> (version 24.0 ed.).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.adobe.com/products/photoshop.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.adobe.com/products/photoshop.html</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amazon Web Services, Inc. (2024a)</span>
<span class="ltx_bibblock">
Amazon Web Services, Inc. 2024a.

</span>
<span class="ltx_bibblock">Amazon S3: Object storage built to retrieve any amount of data from anywhere.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aws.amazon.com/s3/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aws.amazon.com/s3/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-04-24.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amazon Web Services, Inc. (2024b)</span>
<span class="ltx_bibblock">
Amazon Web Services, Inc. 2024b.

</span>
<span class="ltx_bibblock">AWS Amplify - Build full-stack web and mobile apps.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aws.amazon.com/amplify/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aws.amazon.com/amplify/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-04-24.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arcand etÂ al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
KimberlyÂ Kowal Arcand, JessicaÂ Sarah Schonhut-Stasik, SarahÂ G Kane, Gwynn Sturdevant, Matt Russo, Megan Watzke, Brian Hsu, and LisaÂ F Smith. 2024.

</span>
<span class="ltx_bibblock">A Universe of Sound: processing NASA data into sonifications to explore participant response.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">Frontiers in Communication</em> 9 (2024), 1288896.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgos-Rodriguez etÂ al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Katya Borgos-Rodriguez, Maitraye Das, and AnneÂ Marie Piper. 2021.

</span>
<span class="ltx_bibblock">Melodie: A Design Inquiry into Accessible Crafting through Audio-enhanced Weaving.

</span>
<span class="ltx_bibblock">14, 1, Article 5 (mar 2021), 30Â pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3444699" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3444699</a>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bornschein and Weber (2017)</span>
<span class="ltx_bibblock">
Jens Bornschein and Gerhard Weber. 2017.

</span>
<span class="ltx_bibblock">Digital drawing tools for blind users: A state-of-the-art and requirement analysis. In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 10th International Conference on Pervasive Technologies Related to Assistive Environments</em>. 21â€“28.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Braille Authority of North America (2022) (BANA)</span>
<span class="ltx_bibblock">Braille Authority of North America (BANA). 2022.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Guidelines and Standards for Tactile Graphics, 2022</em>.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.brailleauthority.org/sites/default/files/tg/Tactile%20Graphics%20Standards%20and%20Guidelines%202022.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.brailleauthority.org/sites/default/files/tg/Tactile%20Graphics%20Standards%20and%20Guidelines%202022.pdf</a>

</span>
<span class="ltx_bibblock">Available for download from the BANA Website.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brock and Jouffrais (2015)</span>
<span class="ltx_bibblock">
Anke Brock and Christophe Jouffrais. 2015.

</span>
<span class="ltx_bibblock">Interactive audio-tactile maps for visually impaired people.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ACM SIGACCESS Accessibility and Computing</em> 113 (Nov. 2015), 3â€“12.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/2850440.2850441" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2850440.2850441</a>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buzzi etÂ al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
MariaÂ Claudia Buzzi, Marina Buzzi, Barbara Leporini, and Caterina Senette. 2015.

</span>
<span class="ltx_bibblock">Playing with geometry: a Multimodal Android App for Blind Children.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Candey etÂ al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2006)</span>
<span class="ltx_bibblock">
RobertÂ M Candey, AntonÂ M Schertenleib, and WL DiazÂ Merced. 2006.

</span>
<span class="ltx_bibblock">Xsonify sonification tool for space physics. Citeseer.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang etÂ al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Minsuk Chang, Stefania Druga, AlexanderÂ J Fiannaca, Pedro Vergani, Chinmay Kulkarni, CarrieÂ J Cai, and Michael Terry. 2023.

</span>
<span class="ltx_bibblock">The prompt artists. In <em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 15th Conference on Creativity and Cognition</em>. 75â€“87.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dang etÂ al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Hai Dang, Frederik Brudy, George Fitzmaurice, and Fraser Anderson. 2023.

</span>
<span class="ltx_bibblock">WorldSmith: Iterative and Expressive Prompting for World Building with a Generative AI. In <em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology</em>. 1â€“17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das etÂ al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Maitraye Das, Darren Gergle, and AnneÂ Marie Piper. 2023.

</span>
<span class="ltx_bibblock">Simphony: Enhancing Accessible Pattern Design Practices among Blind Weavers. In <em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em> (Â¡conf-locÂ¿, Â¡cityÂ¿HamburgÂ¡/cityÂ¿, Â¡countryÂ¿GermanyÂ¡/countryÂ¿, Â¡/conf-locÂ¿) <em id="bib.bib17.4.2" class="ltx_emph ltx_font_italic">(CHI â€™23)</em>. Association for Computing Machinery, New York, NY, USA, Article 132, 19Â pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3544548.3581047" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3544548.3581047</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Denzin and Lincoln (2011)</span>
<span class="ltx_bibblock">
NormanÂ K Denzin and YvonnaÂ S Lincoln. 2011.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">The Sage handbook of qualitative research</em>.

</span>
<span class="ltx_bibblock">sage.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhariwal and Nichol (2021)</span>
<span class="ltx_bibblock">
Prafulla Dhariwal and Alexander Nichol. 2021.

</span>
<span class="ltx_bibblock">Diffusion models beat gans on image synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 34 (2021), 8780â€“8794.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eyes (2020)</span>
<span class="ltx_bibblock">
BeÂ My Eyes. 2020.

</span>
<span class="ltx_bibblock">Be my eyes.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Retrieved May</em> 29 (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Folmer etÂ al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2009)</span>
<span class="ltx_bibblock">
Eelke Folmer, Bei Yuan, Dave Carr, and Manjari Sapre. 2009.

</span>
<span class="ltx_bibblock">TextSL: a command-based virtual world interface for the visually impaired. In <em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 11th International ACM SIGACCESS Conference on Computers and Accessibility</em> (Pittsburgh, Pennsylvania, USA) <em id="bib.bib21.4.2" class="ltx_emph ltx_font_italic">(Assets â€™09)</em>. Association for Computing Machinery, New York, NY, USA, 59â€“66.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/1639642.1639654" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/1639642.1639654</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gardiner etÂ al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Steven Gardiner, Anthony Tomasic, and John Zimmerman. 2016.

</span>
<span class="ltx_bibblock">The utility of tables for screen reader users. In <em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">2016 13th IEEE Annual Consumer Communications &amp; Networking Conference (CCNC)</em>. 1135â€“1140.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/CCNC.2016.7444949" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/CCNC.2016.7444949</a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Giudice (2018)</span>
<span class="ltx_bibblock">
NicholasÂ A Giudice. 2018.

</span>
<span class="ltx_bibblock">Navigating without vision: Principles of blind spatial cognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Handbook of behavioral and cognitive geography</em>. Edward Elgar Publishing, 260â€“288.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">GRAYPE Systems Private Limited (2017)</span>
<span class="ltx_bibblock">
GRAYPE Systems Private Limited. 2017.

</span>
<span class="ltx_bibblock">wink-pos-tagger: English Part-of-speech tagger.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/winkjs/wink-pos-tagger" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/winkjs/wink-pos-tagger</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Node.js package for natural language processing.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grussenmeyer (2015)</span>
<span class="ltx_bibblock">
William Grussenmeyer. 2015.

</span>
<span class="ltx_bibblock">Draw and drag.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">ACM SIGACCESS Accessibility and Computing</em> (07 2015), 10â€“13.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/2809904.2809907" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2809904.2809907</a>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grussenmeyer and Folmer (2016)</span>
<span class="ltx_bibblock">
William Grussenmeyer and Eelke Folmer. 2016.

</span>
<span class="ltx_bibblock">AudioDraw: user preferences in non-visual diagram drawing for touchscreens. In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 13th International Web for All Conference</em> (Montreal, Canada) <em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic">(W4A â€™16)</em>. Association for Computing Machinery, New York, NY, USA, Article 22, 8Â pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/2899475.2899483" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2899475.2899483</a>

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Headley and Pawluk (2010)</span>
<span class="ltx_bibblock">
Patrick Headley and D. Pawluk. 2010.

</span>
<span class="ltx_bibblock">A low-cost, variable-amplitude haptic distributed display for persons who are blind and visually impaired.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">ASSETSâ€™10 - Proceedings of the 12th International ACM SIGACCESS Conference on Computers and Accessibility</em>.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/1878803.1878844" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/1878803.1878844</a>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holowaychuk and other contributors (2024)</span>
<span class="ltx_bibblock">
TJ Holowaychuk and other contributors. 2024.

</span>
<span class="ltx_bibblock">Express - Fast, unopinionated, minimalist web framework for Node.js.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://expressjs.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://expressjs.com/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-04-24.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holt and other contributors (2024)</span>
<span class="ltx_bibblock">
Matthew Holt and other contributors. 2024.

</span>
<span class="ltx_bibblock">Caddy Web Server.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://caddyserver.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://caddyserver.com/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-04-24.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huh etÂ al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Mina Huh, Yi-Hao Peng, and Amy Pavel. 2023a.

</span>
<span class="ltx_bibblock">GenAssist: Making Image Generation Accessible. In <em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology</em>. 1â€“17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huh etÂ al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Mina Huh, Saelyne Yang, Yi-Hao Peng, XiangÂ â€œAnthonyâ€ Chen, Young-Ho Kim, and Amy Pavel. 2023b.

</span>
<span class="ltx_bibblock">AVscript: Accessible Video Editing with Audio-Visual Scripts. In <em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em> <em id="bib.bib31.4.2" class="ltx_emph ltx_font_italic">(CHI â€™23)</em>. ACM.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3544548.3581494" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3544548.3581494</a>

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Inc. (2019)</span>
<span class="ltx_bibblock">
Azumio Inc. 2019.

</span>
<span class="ltx_bibblock">Calorie Mama Food AI: Instant food Recognition and Calorie Counter using Deep Learning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://www.caloriemama.ai/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.caloriemama.ai/</a>

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jantrid (2023)</span>
<span class="ltx_bibblock">
Jantrid. 2023.

</span>
<span class="ltx_bibblock">Aliens.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://files.jantrid.net/aliens/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://files.jantrid.net/aliens/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-04-24.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamel and Landay (1999)</span>
<span class="ltx_bibblock">
Hesham Kamel and James Landay. 1999.

</span>
<span class="ltx_bibblock">The integrated communication 2 draw (IC2D): a drawing program for the visually impaired.

</span>
<span class="ltx_bibblock">(01 1999).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/632716.632854" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/632716.632854</a>

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamel and Landay (2000a)</span>
<span class="ltx_bibblock">
HeshamÂ M. Kamel and JamesÂ A. Landay. 2000a.

</span>
<span class="ltx_bibblock">A study of blind drawing practice: creating graphical information without the visual channel. In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">International ACM SIGACCESS Conference on Computers and Accessibility</em>.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:17581574" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:17581574</a>

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamel and Landay (2000b)</span>
<span class="ltx_bibblock">
HeshamÂ M Kamel and JamesÂ A Landay. 2000b.

</span>
<span class="ltx_bibblock">A study of blind drawing practice: creating graphical information without the visual channel. In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the fourth international ACM conference on Assistive technologies</em>. 34â€“41.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kurze (1996)</span>
<span class="ltx_bibblock">
Martin Kurze. 1996.

</span>
<span class="ltx_bibblock">TDraw: a computer-based tactile drawing tool for blind people. In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">International ACM SIGACCESS Conference on Computers and Accessibility</em>.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:14172507" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:14172507</a>

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laurini and Thompson (1992)</span>
<span class="ltx_bibblock">
Robert Laurini and Derek Thompson. 1992.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Fundamentals of spatial information systems</em>. Vol.Â 37.

</span>
<span class="ltx_bibblock">Academic press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee etÂ al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jaewook Lee, Yi-Hao Peng, Jaylin Herskovitz, and Anhong Guo. 2021.

</span>
<span class="ltx_bibblock">Image Explorer: Multi-Layered Touch Exploration to Make Images Accessible. Association for Computing Machinery, New York, NY, USA.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3441852.3476548" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3441852.3476548</a>

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leporini etÂ al<span id="bib.bib40.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Barbara Leporini, MariaÂ Claudia Buzzi, and Marina Buzzi. 2012.

</span>
<span class="ltx_bibblock">Interacting with mobile devices via VoiceOver: usability and accessibility issues. In <em id="bib.bib40.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 24th Australian computer-human interaction conference</em>. 339â€“348.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Junchen Li, Garreth Tigwell, and Kristen Shinohara. 2021.

</span>
<span class="ltx_bibblock">Accessibility of High-Fidelity Prototyping Tools.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mann (2024)</span>
<span class="ltx_bibblock">
Yotam Mann. 2024.

</span>
<span class="ltx_bibblock">Tone.js: A Web Audio framework.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://tonejs.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://tonejs.github.io/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-04-24.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Millar (1975a)</span>
<span class="ltx_bibblock">
Susanna Millar. 1975a.

</span>
<span class="ltx_bibblock">Visual experience or translation rules? Drawing the human figure by blind and sighted children.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Perception</em> 4, 4 (1975), 363â€“371.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Millar (1975b)</span>
<span class="ltx_bibblock">
Susanna Millar. 1975b.

</span>
<span class="ltx_bibblock">Visual Experience or Translation Rules? Drawing the Human Figure by Blind and Sighted Children.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Perception</em> 4 (1975), 363 â€“ 371.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:145306265" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:145306265</a>

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Network (2024)</span>
<span class="ltx_bibblock">
MozillaÂ Developer Network. 2024.

</span>
<span class="ltx_bibblock">SpeechSynthesisUtterance - Web APIs â€” MDN.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesisUtterance" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesisUtterance</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-04-24.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ohshiro etÂ al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Keita Ohshiro, Amy Hurst, and Luke DuBois. 2021.

</span>
<span class="ltx_bibblock">Making Math Graphs More Accessible in Remote Learning: Using Sonification to Introduce Discontinuity in Calculus. In <em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility</em> (Â¡conf-locÂ¿, Â¡cityÂ¿Virtual EventÂ¡/cityÂ¿, Â¡countryÂ¿USAÂ¡/countryÂ¿, Â¡/conf-locÂ¿) <em id="bib.bib46.4.2" class="ltx_emph ltx_font_italic">(ASSETS â€™21)</em>. Association for Computing Machinery, New York, NY, USA, Article 77, 4Â pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3441852.3476533" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3441852.3476533</a>

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)</span>
<span class="ltx_bibblock">
OpenAI. 2024.

</span>
<span class="ltx_bibblock">OpenAI API.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openai.com/api/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/api/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-04-24.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Out of Sight Games (2023)</span>
<span class="ltx_bibblock">
Out of Sight Games. 2023.

</span>
<span class="ltx_bibblock">A Heroâ€™s Call.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://outofsightgames.com/a-heros-call/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://outofsightgames.com/a-heros-call/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-04-24.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pandey etÂ al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Maulishree Pandey, Hariharan Subramonyam, Brooke Sasia, Steve Oney, and Sile Oâ€™Modhrain. 2020.

</span>
<span class="ltx_bibblock">Explore, Create, Annotate: Designing Digital Drawing Tools with Visually Impaired People. In <em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</em> (Â¡conf-locÂ¿, Â¡cityÂ¿HonoluluÂ¡/cityÂ¿, Â¡stateÂ¿HIÂ¡/stateÂ¿, Â¡countryÂ¿USAÂ¡/countryÂ¿, Â¡/conf-locÂ¿) <em id="bib.bib49.4.2" class="ltx_emph ltx_font_italic">(CHI â€™20)</em>. Association for Computing Machinery, New York, NY, USA, 1â€“12.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3313831.3376349" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3313831.3376349</a>

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park etÂ al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. 2019.

</span>
<span class="ltx_bibblock">Semantic image synthesis with spatially-adaptive normalization. In <em id="bib.bib50.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 2337â€“2346.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng etÂ al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Yi-Hao Peng, JiWoong Jang, JeffreyÂ P Bigham, and Amy Pavel. 2021.

</span>
<span class="ltx_bibblock">Say It All: Feedback for Improving Non-Visual Presentation Accessibility. In <em id="bib.bib51.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em> (Â¡conf-locÂ¿, Â¡cityÂ¿YokohamaÂ¡/cityÂ¿, Â¡countryÂ¿JapanÂ¡/countryÂ¿, Â¡/conf-locÂ¿) <em id="bib.bib51.4.2" class="ltx_emph ltx_font_italic">(CHI â€™21)</em>. Association for Computing Machinery, New York, NY, USA, Article 276, 12Â pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3411764.3445572" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3411764.3445572</a>

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pixabay (2024)</span>
<span class="ltx_bibblock">
Pixabay. 2024.

</span>
<span class="ltx_bibblock">4.4 million+ Stunning Free Images to Use Anywhere - Pixabay.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://pixabay.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pixabay.com</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-04-24.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Removal.AI (2024)</span>
<span class="ltx_bibblock">
Removal.AI. 2024.

</span>
<span class="ltx_bibblock">Removal.AI - Free Background Remover.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://removal.ai/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://removal.ai/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-04-24.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saha etÂ al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Abir Saha, ThomasÂ Barlow McHugh, and AnneÂ Marie Piper. 2023.

</span>
<span class="ltx_bibblock">Tutoria11y: Enhancing Accessible Interactive Tutorial Creation by Blind Audio Producers. In <em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em> (Hamburg, Germany) <em id="bib.bib54.4.2" class="ltx_emph ltx_font_italic">(CHI â€™23)</em>. Association for Computing Machinery, New York, NY, USA, Article 220, 14Â pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3544548.3580698" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3544548.3580698</a>

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sauter (2015)</span>
<span class="ltx_bibblock">
VickiÂ L. Sauter. 2015.

</span>
<span class="ltx_bibblock">Teaching Tip: Making Data Flow Diagrams Accessible for Visually Impaired Students Using Excel Tables.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">J. Inf. Syst. Educ.</em> 26 (2015), 9â€“20.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:61300084" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:61300084</a>

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schaadhardt etÂ al<span id="bib.bib56.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Anastasia Schaadhardt, Alexis Hiniker, and JacobÂ O. Wobbrock. 2021.

</span>
<span class="ltx_bibblock">Understanding Blind Screen-Reader Usersâ€™ Experiences of Digital Artboards. In <em id="bib.bib56.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em> (Â¡conf-locÂ¿, Â¡cityÂ¿YokohamaÂ¡/cityÂ¿, Â¡countryÂ¿JapanÂ¡/countryÂ¿, Â¡/conf-locÂ¿) <em id="bib.bib56.4.2" class="ltx_emph ltx_font_italic">(CHI â€™21)</em>. Association for Computing Machinery, New York, NY, USA, Article 270, 19Â pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3411764.3445242" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3411764.3445242</a>

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh etÂ al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Mandhatya Singh, MuhammadÂ Suhaib Kanroo, HadiaÂ Showkat Kawoosa, and Puneet Goyal. 2023.

</span>
<span class="ltx_bibblock">Towards accessible chart visualizations for the non-visuals: Research, applications and gaps.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">Computer science review</em> 48 (2023), 100555.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SÃ¸ndergaard and VandsÃ¸ (2017)</span>
<span class="ltx_bibblock">
Morten SÃ¸ndergaard and Anette VandsÃ¸. 2017.

</span>
<span class="ltx_bibblock">Sonification and Audification as Means of Representing Data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">The Aesthetics of Scientific Data Representation</em>. Routledge, 70â€“78.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spector etÂ al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
J.Â Michael Spector, Chin-Chung Tsai, DemetriosÂ G. Sampson, Kinshuk, Ronghuai Huang, Nian-Shing Chen, and Paul Resta (Eds.). 2016.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.3.1" class="ltx_emph ltx_font_italic">16th IEEE International Conference on Advanced Learning Technologies, ICALT 2016, Austin, TX, USA, July 25-28, 2016</em>. IEEE Computer Society.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=7756054" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=7756054</a>

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sutherland (1963)</span>
<span class="ltx_bibblock">
IvanÂ E Sutherland. 1963.

</span>
<span class="ltx_bibblock">Sketchpad: A man-machine graphical communication system. In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Proceedings of the May 21-23, 1963, spring joint computer conference</em>. 329â€“346.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tanaka etÂ al<span id="bib.bib61.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Kengo Tanaka, Tatsuki Fushimi, Ayaka Tsutsui, and Yoichi Ochiai. 2023.

</span>
<span class="ltx_bibblock">Text to Haptics: Method and Case Studies of Designing Tactile Graphics for Inclusive Tactile Picture Books by Digital Fabrication and Generative AI. In <em id="bib.bib61.3.1" class="ltx_emph ltx_font_italic">ACM SIGGRAPH 2023 Labs</em> (Los Angeles, CA, USA) <em id="bib.bib61.4.2" class="ltx_emph ltx_font_italic">(SIGGRAPH â€™23)</em>. Association for Computing Machinery, New York, NY, USA, Article 10, 2Â pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3588029.3595471" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3588029.3595471</a>

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vallejo-Pinto etÂ al<span id="bib.bib62.2.2.1" class="ltx_text">.</span> (2011)</span>
<span class="ltx_bibblock">
JosÃ© Vallejo-Pinto, Javier Torrente, and Baltasar FernÃ¡ndez-ManjÃ³n. 2011.

</span>
<span class="ltx_bibblock">Applying sonification to improve accessibility of point-and-click computer games for people with limited vision.

</span>
<span class="ltx_bibblock"><em id="bib.bib62.3.1" class="ltx_emph ltx_font_italic">Proceedings of HCI 2011 - 25th BCS Conference on Human Computer Interaction</em> (01 2011).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">VanÂ Daele etÂ al<span id="bib.bib63.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Tess VanÂ Daele, Akhil Iyer, Yuning Zhang, JalynÂ C Derry, Mina Huh, and Amy Pavel. 2024.

</span>
<span class="ltx_bibblock">Making Short-Form Videos Accessible with Hierarchical Video Summaries.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.10382</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Watanabe and Kobayashi (2002)</span>
<span class="ltx_bibblock">
Tetsuya Watanabe and Makoto Kobayashi. 2002.

</span>
<span class="ltx_bibblock">A prototype of the Freely Rewritable Tactile Drawing System for Persons Who are Blind.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Journal of Vision Impairment and Blindness</em> 96 (06 2002).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1177/0145482X0209600611" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1177/0145482X0209600611</a>

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Williams etÂ al<span id="bib.bib65.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Kristin Williams, Taylor Clarke, Steve Gardiner, John Zimmerman, and Anthony Tomasic. 2019.

</span>
<span class="ltx_bibblock">Find and Seek: Assessing the Impact of Table Navigation on Information Look-up with a Screen Reader.

</span>
<span class="ltx_bibblock">12, 3, Article 11 (aug 2019), 23Â pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3342282" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3342282</a>

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia (2020)</span>
<span class="ltx_bibblock">
Haijun Xia. 2020.

</span>
<span class="ltx_bibblock">Crosspower: Bridging Graphics and Linguistics.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology</em> (2020).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:222834679" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:222834679</a>

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoshida etÂ al<span id="bib.bib67.2.2.1" class="ltx_text">.</span> (2011)</span>
<span class="ltx_bibblock">
Tsubasa Yoshida, KrisÂ M. Kitani, Hideki Koike, Serge Belongie, and Kevin Schlei. 2011.

</span>
<span class="ltx_bibblock">EdgeSonic: image feature sonification for the visually impaired. In <em id="bib.bib67.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2nd Augmented Human International Conference</em> (Tokyo, Japan) <em id="bib.bib67.4.2" class="ltx_emph ltx_font_italic">(AH â€™11)</em>. Association for Computing Machinery, New York, NY, USA, Article 11, 4Â pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/1959826.1959837" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/1959826.1959837</a>

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al<span id="bib.bib68.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Lotus Zhang, Simon Sun, and Leah Findlater. 2023.

</span>
<span class="ltx_bibblock">Understanding Digital Content Creation Needs of Blind and Low Vision People. In <em id="bib.bib68.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility</em>. 1â€“15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong etÂ al<span id="bib.bib69.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Yu Zhong, WalterÂ S. Lasecki, Erin Brady, and JeffreyÂ P. Bigham. 2015.

</span>
<span class="ltx_bibblock">RegionSpeak: Quick Comprehensive Spatial Descriptions of Complex Images for Blind Users. In <em id="bib.bib69.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</em>. Association for Computing Machinery, New York, NY, USA, 2353â€“2362.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/2702123.2702437" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2702123.2702437</a>

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zoom Video Communications (2023)</span>
<span class="ltx_bibblock">
Zoom Video Communications. 2023.

</span>
<span class="ltx_bibblock">Zoom: Secure Video Conferencing and Web Conferencing Service.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://zoom.us" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://zoom.us</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2023-04-22.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1. </span>Keyboard Shortcuts</h3>

<figure id="A1.T2" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="A1.T2.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T2.1.1.1" class="ltx_tr">
<th id="A1.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="A1.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Keyboard Commands</span></th>
<th id="A1.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="A1.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Function</span></th>
<th id="A1.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="A1.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Main Interaction</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T2.1.2.1" class="ltx_tr">
<td id="A1.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">ENTER</td>
<td id="A1.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Image Generation / Regeneration</td>
<td id="A1.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Generate a new image with speech</td>
</tr>
<tr id="A1.T2.1.3.2" class="ltx_tr">
<td id="A1.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SHIFT + G</td>
<td id="A1.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Global Canvas Description</td>
<td id="A1.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Hear global description about the canvas</td>
</tr>
<tr id="A1.T2.1.4.3" class="ltx_tr">
<td id="A1.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SHIFT + I</td>
<td id="A1.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Local Image Description</td>
<td id="A1.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Hear local description about the image on tile</td>
</tr>
<tr id="A1.T2.1.5.4" class="ltx_tr">
<td id="A1.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SHIFT + R</td>
<td id="A1.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Radar Scan for Surrounding Objects</td>
<td id="A1.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Hear the name of the object and numerical distance</td>
</tr>
<tr id="A1.T2.1.6.5" class="ltx_tr">
<td id="A1.T2.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SHIFT + C</td>
<td id="A1.T2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Image Chat</td>
<td id="A1.T2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Ask a question about the image</td>
</tr>
<tr id="A1.T2.1.7.6" class="ltx_tr">
<td id="A1.T2.1.7.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SHIFT + L</td>
<td id="A1.T2.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Location Edit</td>
<td id="A1.T2.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Edit the location of an image</td>
</tr>
<tr id="A1.T2.1.8.7" class="ltx_tr">
<td id="A1.T2.1.8.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SHIFT + S</td>
<td id="A1.T2.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Size Edit</td>
<td id="A1.T2.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Increase / Decrease the size of an image</td>
</tr>
<tr id="A1.T2.1.9.8" class="ltx_tr">
<td id="A1.T2.1.9.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SHIFT + Arrow Key</td>
<td id="A1.T2.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Push Image</td>
<td id="A1.T2.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Push image tiles to create tile space</td>
</tr>
<tr id="A1.T2.1.10.9" class="ltx_tr">
<td id="A1.T2.1.10.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">SHIFT + X</td>
<td id="A1.T2.1.10.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Delete Image</td>
<td id="A1.T2.1.10.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Delete an Image on the tile</td>
</tr>
<tr id="A1.T2.1.11.10" class="ltx_tr">
<td id="A1.T2.1.11.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Arrow Keys</td>
<td id="A1.T2.1.11.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Tile Navigation, Location/Size Edit</td>
<td id="A1.T2.1.11.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Navigate through different spaces on tiles</td>
</tr>
<tr id="A1.T2.1.12.11" class="ltx_tr">
<td id="A1.T2.1.12.11.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">ESC</td>
<td id="A1.T2.1.12.11.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Quit / Stop</td>
<td id="A1.T2.1.12.11.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Exit an editing model, stop model speech</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>The 10 main keyboard commands that are used in this system. This table lists the 10 main keyboard commands used in a system and their respective functions and interactions. It has three columns: â€˜Keyboard Commandsâ€™, â€˜Functionâ€™, and â€˜Main Interactionâ€™. For example, pressing â€˜ENTERâ€™ generates a new image with speech, while â€™SHIFT + Gâ€™ provides a global description of the canvas. Local descriptions, radar scans for nearby objects, image chats, and editing image location or size are performed with various â€˜SHIFTâ€™ plus a letter key combinations. The â€˜SHIFT + Arrow Keyâ€™ pushes image tiles, â€™SHIFT + Xâ€™ deletes an image, and the arrow keys alone navigate through tiles. â€™ESCâ€™ is used to quit or stop actions within the system. Each command aligns with a specific interaction for image editing and navigation within the system, aimed at enhancing user experience.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="A1.T2.2" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="A1.T2.3" class="ltx_p ltx_figure_panel ltx_align_center">This table lists the 10 main keyboard commands used in a system and their respective functions and interactions. It has three columns: â€™Keyboard Commandsâ€™, â€™Functionâ€™, and â€™Main Interactionâ€™. For example, pressing â€™ENTERâ€™ generates a new image with speech, while â€™SHIFT + Gâ€™ provides a global description of the canvas. Local descriptions, radar scans for nearby objects, image chats, and editing image location or size are performed with various â€™SHIFTâ€™ plus a letter key combinations. The â€™SHIFT + Arrow Keyâ€™ pushes image tiles, â€™SHIFT + Xâ€™ deletes an image, and the arrow keys alone navigate through tiles. â€™ESCâ€™ is used to quit or stop actions within the system. Each command aligns with a specific interaction for image editing and navigation within the system, aimed at enhancing user experience.</p>
</div>
</div>
</figure>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2. </span>TaskOne: Sample User Graphic Generations</h3>

<div id="A1.SS2.p1" class="ltx_para">
<img src="/html/2408.10240/assets/images/userGraphic_Tone.png" id="A1.SS2.p1.g1" class="ltx_graphics ltx_img_portrait" width="198" height="256" alt="[Uncaptioned image]"><img src="/html/2408.10240/assets/images/userGraphic_Ttwo.png" id="A1.SS2.p1.g2" class="ltx_graphics ltx_img_portrait" width="198" height="256" alt="[Uncaptioned image]"><img src="/html/2408.10240/assets/images/userGraphic_Tthree.png" id="A1.SS2.p1.g3" class="ltx_graphics ltx_img_portrait" width="198" height="256" alt="[Uncaptioned image]"><span id="A1.SS2.p1.1" class="ltx_ERROR undefined">\Description</span>
<p id="A1.SS2.p1.2" class="ltx_p">The image presents three sample user graphic generations for â€™Task Oneâ€™. Each sample depicts three items: a dog, a dog bowl, and a clock. The first graphic shows a simple, cartoon-style representation with a puppy in front of a dog bowl that has a few pieces of dog food scattered around, and a wall clock with roman numerals above. In the second graphic, there is a more detailed drawing of a dachshund with a wagging tail next to an empty dog bowl, and a wall clock with hands pointing at eleven and one.
The third graphic features a corgi sitting proudly between two dog bowls decorated with paw prints, and a wall clock with a second hand above it.</p>
</div>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3. </span>TaskTwo: Sample User Graphic Generations</h3>

<div id="A1.SS3.p1" class="ltx_para">
<img src="/html/2408.10240/assets/images/userGraphic_one.png" id="A1.SS3.p1.g1" class="ltx_graphics ltx_img_portrait" width="198" height="256" alt="[Uncaptioned image]"><img src="/html/2408.10240/assets/images/userGraphic_two.png" id="A1.SS3.p1.g2" class="ltx_graphics ltx_img_portrait" width="198" height="256" alt="[Uncaptioned image]"><img src="/html/2408.10240/assets/images/userGraphic_three.png" id="A1.SS3.p1.g3" class="ltx_graphics ltx_img_portrait" width="198" height="256" alt="[Uncaptioned image]"><span id="A1.SS3.p1.1" class="ltx_ERROR undefined">\Description</span>
<p id="A1.SS3.p1.2" class="ltx_p">The image showcases three sets of line-drawn graphics created by users as part of â€™Task Twoâ€™. Each set includes a window, a clock, a cat, a table, and a potted plant. In the first graphic, the window is styled with curtains pulled aside, the clock has simple numbers and hands, the cat appears seated and alert, the table is square with a flowerpot that has several flowers.
The second graphic features a grid-style window, a clock with Roman numerals, a lounging cat, and a round table with a large potted plant.
The third graphic depicts an arched window with a divided pane, a clock marked with lines instead of numbers, a fluffy cat seated in front of a slender table supporting a potted plant with long leaves.
Each graphic presents these common elements with varying artistic interpretations, showcasing different styles and details in the usersâ€™ renditions.</p>
</div>
</section>
<section id="A1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4. </span>TaskThree: Sample User Graphic Generations</h3>

<div id="A1.SS4.p1" class="ltx_para">
<img src="/html/2408.10240/assets/images/userGraphic_Fone.png" id="A1.SS4.p1.g1" class="ltx_graphics ltx_img_portrait" width="198" height="256" alt="[Uncaptioned image]"><img src="/html/2408.10240/assets/images/userGraphic_Ftwo.png" id="A1.SS4.p1.g2" class="ltx_graphics ltx_img_portrait" width="198" height="256" alt="[Uncaptioned image]"><img src="/html/2408.10240/assets/images/userGraphic_Fthree.png" id="A1.SS4.p1.g3" class="ltx_graphics ltx_img_portrait" width="198" height="256" alt="[Uncaptioned image]"><span id="A1.SS4.p1.1" class="ltx_ERROR undefined">\Description</span>
<p id="A1.SS4.p1.2" class="ltx_p">The image is a compilation of three sets of graphics generated by users for â€™Task Threeâ€™. Each graphic represents a different theme described by the users: The first graphic depicts a family of three (mom, dad, and daughter), standing next to a turtle and a unicorn, showcasing a combination of real and mythical creatures.
The second graphic shows a scene with three items: a decorated Christmas tree, a racing car, and a pickup truck, suggesting a contrast between the stationary and the dynamic.
The third graphic features a playful set with a character that resembles a Minion wearing glasses and holding a spatula grilling things for a barbecue, a round character that could be Kirby floating on water, and a simple basket, highlighting characters from popular culture and a common object.
Each set of images captures unique elements as specified by the users, reflecting their individual creative interpretations of the given task.</p>
</div>
</section>
<section id="A1.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5. </span>Prompts used in the system</h3>

<section id="A1.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.5.1. </span>Tactile Image Generation Prompt</h4>

<div id="A1.SS5.SSS1.p1" class="ltx_para">
<span id="A1.SS5.SSS1.p1.1" class="ltx_inline-block ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><pre id="A1.SS5.SSS1.p1.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:90%;">
Create ONLY ONE DIGITAL graphic of the \texttt{${mainObject}}
This graphic should be VERY SIMPLE and MINIMAL, focusing on the core shape and essence of the object.
Avoid adding any perspectives, intricate details, or text to the design.
The goal is to capture the simplicity and clarity of the object in a minimalistic style.
Ensure that the lines are clean and the overall design is straightforward.
The user has specifically requested the following object: \texttt{${voiceText}}.
Please adhere strictly to these guidelines to achieve the desired outcome.
</pre>
</span>
</div>
</section>
<section id="A1.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.5.2. </span>Image Description Prompt</h4>

<div id="A1.SS5.SSS2.p1" class="ltx_para">
<span id="A1.SS5.SSS2.p1.1" class="ltx_inline-block ltx_framed ltx_framed_rectangle" style="border-color: #000000;">
<span id="A1.SS5.SSS2.p1.1.1" class="ltx_p"><span id="A1.SS5.SSS2.p1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">GLOBAL DESCRIPTION:<span id="A1.SS5.SSS2.p1.1.1.1.1" class="ltx_text ltx_font_medium"></span></span></span><pre id="A1.SS5.SSS2.p1.1.2" class="ltx_verbatim ltx_font_typewriter" style="font-size:90%;">
Provide a one-line brief description of what the image looks like.
Describe the layout of the images on a canvas based on their coordinates
and sizes verbally, without using exact numbers. Avoid stating specific
shapes like "square." Mention if one image appears to be placed on top
of another, noting any spaces above or below the image.
</pre>
<span id="A1.SS5.SSS2.p1.1.3" class="ltx_p"><span id="A1.SS5.SSS2.p1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">LOCAL DESCRIPTION:<span id="A1.SS5.SSS2.p1.1.3.1.1" class="ltx_text ltx_font_medium"></span></span></span><pre id="A1.SS5.SSS2.p1.1.4" class="ltx_verbatim ltx_font_typewriter" style="font-size:90%;">
The image is called \texttt{\$\{image.name\}}. It is located at x-coordinate
\texttt{\$\{image.coordinate.x\}} and y-coordinate \texttt{\$\{image.coordinate.y\}}.
The size of the image is \texttt{\$\{image.sizeParts.width\}} in width
and \texttt{\$\{image.sizeParts.height\}} in height.
Additional description: \texttt{\$\{image.descriptions\}}.
</pre>
<span id="A1.SS5.SSS2.p1.1.5" class="ltx_p"><span id="A1.SS5.SSS2.p1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">CHAT BASED DESCRIPTION:<span id="A1.SS5.SSS2.p1.1.5.1.1" class="ltx_text ltx_font_medium"></span></span></span><pre id="A1.SS5.SSS2.p1.1.6" class="ltx_verbatim ltx_font_typewriter" style="font-size:90%;">
You are describing an image to a Visually Impaired Person. Keep the description brief
and straightforward. Generate the given image description according to the
following criteria: \texttt{\$\{voiceText\}}.
</pre>
</span>
</div>
</section>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.10239" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.10240" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.10240">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.10240" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.10241" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 17:04:47 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
