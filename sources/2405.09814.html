<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.09814] Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis</title><meta property="og:description" content="In this work, we present Semantic Gesticulator, a novel framework designed to synthesize realistic gestures accompanying speech with strong semantic correspondence. Semantically meaningful gestures are crucial for effe…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.09814">

<!--Generated on Wed Jun  5 13:36:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="co-speech gesture synthesis,  multi-modality,  retrieval augmentation">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zeyi Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:illusence1@gmail.com">illusence1@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">School of Electronics Engineering and Computer Science, Peking University</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tenglong Ao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:aubrey.tenglong.ao@gmail.com">aubrey.tenglong.ao@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_affiliation_institution">School of Computer Science, Peking University</span><span id="id4.2.id2" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuyao Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:2020201710@ruc.edu.cn">2020201710@ruc.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">Renmin University of China</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qingzhe Gao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:gaoqingzhe97@gmail.com">gaoqingzhe97@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">Shandong University</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id9.3.id1" class="ltx_text ltx_affiliation_institution">Peking University</span><span id="id10.4.id2" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chuan Lin
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:chuanlin2015@foxmail.com">chuanlin2015@foxmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id11.1.id1" class="ltx_text ltx_affiliation_institution">Peking University</span><span id="id12.2.id2" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Baoquan Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:baoquan@pku.edu.cn">baoquan@pku.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-4702-036X" title="ORCID identifier" class="ltx_ref">0000-0003-4702-036X</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id13.1.id1" class="ltx_text ltx_affiliation_institution">Peking University</span><span id="id14.2.id2" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id15.3.id1" class="ltx_text ltx_affiliation_institution">State Key Lab of General AI</span><span id="id16.4.id2" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Libin Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:libin.liu@pku.edu.cn">libin.liu@pku.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-2280-6817" title="ORCID identifier" class="ltx_ref">0000-0003-2280-6817</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id17.1.id1" class="ltx_text ltx_affiliation_institution">Peking University</span><span id="id18.2.id2" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id19.3.id1" class="ltx_text ltx_affiliation_institution">State Key Lab of General AI</span><span id="id20.4.id2" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id21.id1" class="ltx_p">In this work, we present <em id="id21.id1.1" class="ltx_emph ltx_font_italic">Semantic Gesticulator</em>, a novel framework designed to synthesize realistic gestures accompanying speech with strong semantic correspondence. Semantically meaningful gestures are crucial for effective non-verbal communication, but such gestures often fall within the long tail of the distribution of natural human motion. The sparsity of these movements makes it challenging for deep learning-based systems, trained on moderately sized datasets, to capture the relationship between the movements and the corresponding speech semantics. To address this challenge, we develop a generative retrieval framework based on a large language model. This framework efficiently retrieves suitable semantic gesture candidates from a motion library in response to the input speech. To construct this motion library, we summarize a comprehensive list of commonly used semantic gestures based on findings in linguistics, and we collect a high-quality motion dataset encompassing both body and hand movements. We also design a novel GPT-based model with strong generalization capabilities to audio, capable of generating high-quality gestures that match the rhythm of speech. Furthermore, we propose a semantic alignment mechanism to efficiently align the retrieved semantic gestures with the GPT’s output, ensuring the naturalness of the final animation. Our system demonstrates robustness in generating gestures that are rhythmically coherent and semantically explicit, as evidenced by a comprehensive collection of examples. User studies confirm the quality and human-likeness of our results, and show that our system outperforms state-of-the-art systems in terms of semantic appropriateness by a clear margin. We will release the code and dataset for academic research.</p>
</div>
<div class="ltx_keywords">co-speech gesture synthesis, multi-modality, retrieval augmentation
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_journal"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>TOG</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_journalvolume"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalvolume: </span>43</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_journalnumber"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalnumber: </span>4</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_publicationmonth"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">publicationmonth: </span>7</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3658134</span></span></span><span id="id8" class="ltx_note ltx_note_frontmatter ltx_role_submissionid"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">submissionid: </span>0</span></span></span><span id="id9" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Animation</span></span></span><span id="id10" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Natural language processing</span></span></span><span id="id11" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Neural networks</span></span></span>
<figure id="S0.F1" class="ltx_figure ltx_teaserfigure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2405.09814/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="177" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>. </span><span id="S0.F1.3.2" class="ltx_text" style="font-size:90%;">Our system can synthesize realistic co-speech gestures with strong rhythmic coherence and semantic correspondence.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S0.F1.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Gestures, which are hand and body movements that are non-verbal and non-manipulative, often accompany speech and play an important role in communication <cite class="ltx_cite ltx_citemacro_citep">(McNeill, <a href="#bib.bib57" title="" class="ltx_ref">1992</a>)</cite>. They not only enhance expression, making it more dynamic, especially through rhythmic movements known as <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">beat gestures</em>, but also increase communicative efficacy. This includes pointing or deictic gestures that convey specific references, metaphoric and iconic gestures that depict concepts, and emblems that symbolize specific words or phrases <cite class="ltx_cite ltx_citemacro_citep">(Nyatsanga et al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2023</a>)</cite>. In this work, we refer to gestures with such communicative functions as <em id="S1.p1.1.2" class="ltx_emph ltx_font_italic">semantic gestures</em>. The occurrence of such gestures is much less frequent than that of beat gestures and falls within the long tail distribution of gestures. <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2022d</a>, <a href="#bib.bib50" title="" class="ltx_ref">a</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Deep learning-based approaches have recently achieved significant success in synthesizing natural-looking gestures from speech input. These methods often excel at producing beat gestures that are in rhythmic harmony with the speech, but tend to stuggle with semantic gestures that reflect the meaning of the spoken words. Many previous deep learning-based systems adopt textual features extracted from transcripts as inputs to perceive semantics <cite class="ltx_cite ltx_citemacro_citep">(Kucherenko et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2020</a>; Yoon
et al<span class="ltx_text">.</span>, <a href="#bib.bib97" title="" class="ltx_ref">2020</a>; Ao
et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2022</a>; Pang et al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2023</a>; Zhi
et al<span class="ltx_text">.</span>, <a href="#bib.bib105" title="" class="ltx_ref">2023</a>)</cite>. However, since the majority of words in transcripts are not accompanied by semantically meaningful gestures, the learning process often tends to overlook these sparse semantic gestures and treat them as noises in the training data.
There are methods attempting to refine the training procedure to address such issues. For example, <cite class="ltx_cite ltx_citemacro_citet">Liu et al<span class="ltx_text">.</span> (<a href="#bib.bib50" title="" class="ltx_ref">2022a</a>)</cite> cluster gestures and resample within each cluster to improve data balance. <cite class="ltx_cite ltx_citemacro_citet">Ao et al<span class="ltx_text">.</span> (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite> attempt to discover implicit correspondence between semantic gestures and semantically salient words using contrastive learning. Meanwhile, <cite class="ltx_cite ltx_citemacro_citet">Liang
et al<span class="ltx_text">.</span> (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite> train classifiers to classify gestures into a small number of labels and use these classifiers as extra perceptual objectives. Although these methods show effectiveness in certain scenarios, they still rely on sufficient coverage of semantic gesture-speech pairs within training data. Given the sparsity of such data, obtaining this coverage is challenging. Consequently, these methods struggle in practice to reliably produce appropriate semantic gestures at the correct moments, especially when dealing with a diverse range of semantic gesture categories.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">A widely-used and robust strategy for enabling neural systems to perceive sparse and hard-to-mine data clues is retrieval augmentation, which has achieved great success in natural language processing <cite class="ltx_cite ltx_citemacro_citep">(Lewis
et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2020</a>; Izacard et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2022</a>; Ram et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">2023</a>)</cite>. The core idea involves retrieving useful items from an external database to enhance generation. When such retrieval is conducted by a powerful model, like a large language model, the external database effectively becomes augmented in return by the knowledge embedded in the retrieval model. This knowledge, originating from a much larger volume of data and potentially from different modalities, extends the limits of the data being retrieved.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Adapting this idea to semantic gestures is non-trivial, containing two challenges: 1) establishing a high-quality motion dataset with sufficient diversity to cover commonly used semantic gestures; 2) developing a retrieval model adept at choosing appropriate semantic gestures according to context and determining their timing. To address the first challenge, we compile a comprehensive set of semantic gestures commonly used in human communication, drawing on relevant linguistic and human behavioral studies (e.g. <cite class="ltx_cite ltx_citemacro_citep">(Morris, <a href="#bib.bib59" title="" class="ltx_ref">1994</a>; Wagner and
Armstrong, <a href="#bib.bib82" title="" class="ltx_ref">2003</a>; Kipp, <a href="#bib.bib38" title="" class="ltx_ref">2005</a>; World
Federation of the Deaf, <a href="#bib.bib86" title="" class="ltx_ref">1975</a>)</cite>). Based on this collection, we record a high-quality dataset of body, hand, and finger movement using motion capture. This dataset contains over 200 types of semantic gestures, each offering a variety of gestures. As for the second challenge, large language models like ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib64" title="" class="ltx_ref">2022</a>)</cite> possess strong contextual understanding and generalization capabilities. We construct a generative retrieval model by fine-tuning a large language model, enabling it to efficiently retrieve appropriate semantic gestures based on input speech in an end-to-end manner and determine their timing.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We refer to this framework as <em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">Semantic Gesticulator</em>, a co-speech gesture synthesis neural system designed to generate diverse and appropriate semantic gestures while ensuring rhythmic coherence with speech. We learn a GPT-based <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib71" title="" class="ltx_ref">2018</a>)</cite> gesture generative model to accommodate the retrieved gestures. This model is built upon discrete tokens extracted by a scalable, body part-aware Residual VQ-VAE <cite class="ltx_cite ltx_citemacro_citep">(Zeghidour et al<span class="ltx_text">.</span>, <a href="#bib.bib100" title="" class="ltx_ref">2022</a>)</cite>. It generalizes to a broad range of audio inputs, resulting in gestures that are both realistic and rhythmically in sync with the input speech. Moreover, we develop a semantics-aware gesture alignment mechanism. This mechanism fuses semantic and rhythmic gestures at the latent space level, ensuring that the generated gestures are not only meaningful but also exhibit rhythmic harmony.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In summary, the technical contributions of this work include:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce a novel semantics-aware co-speech gesture synthesis system that produces natural and semantically rich gestures. The GPT-based generator and the semantics-aware alignment mechanism effectively ensure motion quality and generalization across different audio inputs.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We develop an LLM-based generative semantic gesture retrieval framework capable of efficiently retrieving semantic gestures from a gesture library.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We compile a comprehensive list of commonly used semantic gestures and capture a high-quality dataset according to it. The list and the dataset will be released to the community for academic research.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Co-Speech Gesture Synthesis</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Early methods for co-speech gesture synthesis is rule-based approaches, which primarily employ linguistic mapping rules to convert speech into sequences of pre-defined gesture clips <cite class="ltx_cite ltx_citemacro_citep">(Cassell et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">1994</a>, <a href="#bib.bib15" title="" class="ltx_ref">2001</a>; Kipp, <a href="#bib.bib37" title="" class="ltx_ref">2004</a>; Kopp et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2006</a>)</cite>. A thorough review of these methods is presented by <cite class="ltx_cite ltx_citemacro_citep">(Wagner
et al<span class="ltx_text">.</span>, <a href="#bib.bib83" title="" class="ltx_ref">2014</a>)</cite>. Although rule-based methods yield interpretable and controllable outcomes, the construction of mapping rules and gesture databases is labor-intensive and movements generated by these methods are not realistic. To reduce the manual effort involved in creating rules and improve motion quality, data-driven approaches have increasingly become the dominant methodology in this field. A comprehensive survey of these methods is provided by <cite class="ltx_cite ltx_citemacro_citep">(Nyatsanga et al<span class="ltx_text">.</span>, <a href="#bib.bib63" title="" class="ltx_ref">2023</a>)</cite>. Statistical models are initially employed to extract mapping rules from data <cite class="ltx_cite ltx_citemacro_citep">(Neff
et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2008</a>; Levine
et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2009</a>; Levine et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2010</a>)</cite>, but they still require a meticulously curated library of gesture units to generate the final motion. Subsequently, the advanced capabilities of deep neural networks enable the training of complex end-to-end models directly using raw gesture data. Deep learning-based systems can be categorized based on the randomness of their output. One choice is deterministic models, which are mainly based on MLPs <cite class="ltx_cite ltx_citemacro_citep">(Kucherenko et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2020</a>)</cite>, CNNs <cite class="ltx_cite ltx_citemacro_citep">(Habibie et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite>, RNNs <cite class="ltx_cite ltx_citemacro_citep">(Yoon
et al<span class="ltx_text">.</span>, <a href="#bib.bib98" title="" class="ltx_ref">2019</a>, <a href="#bib.bib97" title="" class="ltx_ref">2020</a>; Bhattacharya et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2021a</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2022c</a>; Ghorbani et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2023</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2022a</a>, <a href="#bib.bib50" title="" class="ltx_ref">a</a>; Xu
et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2023</a>; Liang
et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>, and Transformer <cite class="ltx_cite ltx_citemacro_citep">(Bhattacharya et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2021b</a>; Voß and Kopp, <a href="#bib.bib81" title="" class="ltx_ref">2023</a>; Qi
et al<span class="ltx_text">.</span>, <a href="#bib.bib69" title="" class="ltx_ref">2023a</a>; Qi et al<span class="ltx_text">.</span>, <a href="#bib.bib70" title="" class="ltx_ref">2023b</a>)</cite>. Gestures generated by deterministic methods easily converge to “mean” poses due to the inherent many-to-many relationship between speech and gestures. Generative models are widely used to alleviate this problem, such as VAEs <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2021</a>; Ghorbani et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite>, VQ-VAEs <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a href="#bib.bib95" title="" class="ltx_ref">2022</a>; Yazdian
et al<span class="ltx_text">.</span>, <a href="#bib.bib93" title="" class="ltx_ref">2022</a>; Liu
et al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2022b</a>; Zhou
et al<span class="ltx_text">.</span>, <a href="#bib.bib108" title="" class="ltx_ref">2023</a>; Zhang
et al<span class="ltx_text">.</span>, <a href="#bib.bib103" title="" class="ltx_ref">2023d</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2023</a>; Ao
et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2022</a>; Ng et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2024</a>)</cite>, flow-based models <cite class="ltx_cite ltx_citemacro_citep">(Alexanderson et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2020</a>; Ye
et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2022</a>; Kucherenko et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2021</a>)</cite>, and diffusion-based models <cite class="ltx_cite ltx_citemacro_citep">(Alexanderson et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2023</a>; Zhang
et al<span class="ltx_text">.</span>, <a href="#bib.bib101" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib102" title="" class="ltx_ref">b</a>; Xue
et al<span class="ltx_text">.</span>, <a href="#bib.bib88" title="" class="ltx_ref">2023</a>; Ji
et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2023</a>; Ao et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2023</a>; Mehta et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2023</a>; Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib89" title="" class="ltx_ref">2023b</a>; Deichler et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2023</a>; Yin et al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2023</a>; Chhatre et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2023</a>; Zhi
et al<span class="ltx_text">.</span>, <a href="#bib.bib105" title="" class="ltx_ref">2023</a>; Zhu
et al<span class="ltx_text">.</span>, <a href="#bib.bib109" title="" class="ltx_ref">2023a</a>; Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2023</a>; Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2024</a>)</cite>. Additionally, some hybrid systems integrate deep features with motion graphs <cite class="ltx_cite ltx_citemacro_citep">(Zhou
et al<span class="ltx_text">.</span>, <a href="#bib.bib106" title="" class="ltx_ref">2022a</a>)</cite> or motion matching <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib90" title="" class="ltx_ref">2023a</a>; Habibie et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2022</a>; Ferstl
et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2021</a>)</cite> to enhance the controllability of the system. Generating meaningful gestures in sync with speech robustly is challenging for neural systems <cite class="ltx_cite ltx_citemacro_citep">(Yoon et al<span class="ltx_text">.</span>, <a href="#bib.bib99" title="" class="ltx_ref">2022</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Liang
et al<span class="ltx_text">.</span> (<a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite> design specific semantic gesture classifiers to explicitly guide the generator. Some systems opt to mine implicit content as a representation of semantics <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2022a</a>; Ao et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Gao
et al<span class="ltx_text">.</span> (<a href="#bib.bib28" title="" class="ltx_ref">2023</a>)</cite> define six general semantic gestures and employ a Large Language Model (LLM) as a simple classifier. Through prompt engineering, it identifies corresponding gestures for each sentence of text. Finally, the identified gestures are merged with gestures generated by the neural system through linear interpolation. There are there key differences between this work with our system: (a) our method enables generating over 200 types of semantic gestures, which covers commonly used scenarios; (b) we develop a generative retrieval framework through fine-tuning the LLM, capable of efficiently retrieving semantic gestures from a large gesture library; and (c) a semantics-aware gesture alignment mechanism is proposed to fuse semantic and rhythmic gestures at the latent space
level, ensuring that the generated gestures are both meaningful and rhythm-coherent.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Speech-Gesture Dataset</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Current co-speech gesture datasets could be divided into two types: pose-estimated and motion-captured. For the former, <cite class="ltx_cite ltx_citemacro_citet">Ginosar et al<span class="ltx_text">.</span> (<a href="#bib.bib30" title="" class="ltx_ref">2019</a>)</cite> propose the Speech2Gesture Dataset, which employs OpenPose <cite class="ltx_cite ltx_citemacro_citep">(Cao
et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite> to extract 2D poses from News and Teaching videos. This dataset is then lifted to 3D pose <cite class="ltx_cite ltx_citemacro_citep">(Habibie et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite> and SMPL-X <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a href="#bib.bib95" title="" class="ltx_ref">2022</a>)</cite>. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Yoon
et al<span class="ltx_text">.</span> (<a href="#bib.bib98" title="" class="ltx_ref">2019</a>)</cite> estimate 2D poses from TED videos and build the TED Dataset, which is also extended to 3D pose <cite class="ltx_cite ltx_citemacro_citep">(Yoon
et al<span class="ltx_text">.</span>, <a href="#bib.bib97" title="" class="ltx_ref">2020</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2022c</a>)</cite> and SMPL-X <cite class="ltx_cite ltx_citemacro_citep">(Lu et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2023</a>)</cite>. Although pose-estimation methods facilitate the extraction of vast amounts of data from videos, their accuracy remains constrained. Motion-captured methods can yield high-quality motion datasets but are often costly. The Trinity Dataset <cite class="ltx_cite ltx_citemacro_citep">(Ferstl and
McDonnell, <a href="#bib.bib25" title="" class="ltx_ref">2018</a>)</cite> showcases a male actor with <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mn id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><cn type="integer" id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">4</annotation></semantics></math> hours of data, and the TalkingWithHands Dataset <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2019</a>)</cite> gathers data from conversational scenarios involving two speakers. The BEAT Dataset <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2022d</a>)</cite> first incorporates both 3D pose and facial blendshapes. <cite class="ltx_cite ltx_citemacro_citet">Liu et al<span class="ltx_text">.</span> (<a href="#bib.bib51" title="" class="ltx_ref">2023</a>)</cite> add mesh-level data to this dataset. ZEGGS <cite class="ltx_cite ltx_citemacro_citep">(Ghorbani et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite> focuses on a single speaker across 12 different styles. In this work, we collect a motion-captured gesture dataset encompassing commonly used semantic gestures, designed to augment and enrich existing datasets.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Generative Retrieval</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Unlike the traditional index-retrieval-rank paradigm <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib110" title="" class="ltx_ref">2023b</a>)</cite>, generative retrieval involves storing knowledge in model parameters <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib110" title="" class="ltx_ref">2023b</a>; Tay et al<span class="ltx_text">.</span>, <a href="#bib.bib76" title="" class="ltx_ref">2022</a>)</cite>. It focuses on generating index identifiers, such as numbers and titles, sequentially through an autoregressive fashion to achieve end-to-end retrieval <cite class="ltx_cite ltx_citemacro_citep">(Cao
et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>. This model-based information retrieval has recently gained significant attention in academia <cite class="ltx_cite ltx_citemacro_citep">(Zhou
et al<span class="ltx_text">.</span>, <a href="#bib.bib107" title="" class="ltx_ref">2022b</a>; Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>.
Methods like DSI <cite class="ltx_cite ltx_citemacro_citep">(Tay et al<span class="ltx_text">.</span>, <a href="#bib.bib76" title="" class="ltx_ref">2022</a>)</cite> and NCI <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2023a</a>)</cite> encode document content into vectors and use hierarchical clustering to generate numeric, semantically-based identifiers for retrieval. These number-based identifiers implicitly capture the document’s hierarchical information and demonstrate efficient and effective performance. However, converting text to number-based identifiers can result in a loss of semantic detail, posing challenges for neural networks in learning this mapping function.
To transfer the knowledge compressed in Pre-trained Language Models (PLMs), GENRE <cite class="ltx_cite ltx_citemacro_citep">(Cao
et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite> focuses on retrieving entities by finetuning a T5 model <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al<span class="ltx_text">.</span>, <a href="#bib.bib74" title="" class="ltx_ref">2020</a>)</cite> to generate term-based identifiers such as each entity’s unique title, progressing from left to right and token by token.
Additionally, there are studies exploring the integration of multiple identifiers and how to use Large Language Models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">(Li
et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2023</a>; Bevilacqua et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2022</a>; Ziems
et al<span class="ltx_text">.</span>, <a href="#bib.bib111" title="" class="ltx_ref">2023</a>)</cite>.
We can interpret our semantic gesture synthesis as a specific retrieval task. It uses speech text as a query to retrieve and decode appropriate semantic gesture identifiers at relevant locations using an autoregressive fashion, fulfilling the overall system’s requirements. In contrast to sequence labeling tasks, this approach enables cross-encoding of context and considers semantics of gestures.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>System Overview</h2>

<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2405.09814/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="143" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>. </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Our system is composed of three principal components: (a) an end-to-end neural generator, adept at handling a wide array of speech audio inputs to create gesture animations that are in rhythm with the speech; (b) a generative retrieval framework based on a large language model (LLM), adept at interpreting transcript context and selecting suitable semantic gestures from an extensive library covering commonly used gestures; and (c) a semantics-aware alignment mechanism, which amalgamates the chosen semantic gestures with the rhythmically produced motion, culminating in gestures that are semantically enriched.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S3.F2.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our system processes audio and speech transcripts as inputs to generate realistic full-body gestures, including finger motion, that are both rhythmically and semantically aligned with the speech content. It is capable of robustly synthesizing sparse semantic gestures, vital for effective communication.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Our system is built upon a discrete latent motion space, learned through the use of a residual VQ-VAE <cite class="ltx_cite ltx_citemacro_citep">(Zeghidour et al<span class="ltx_text">.</span>, <a href="#bib.bib100" title="" class="ltx_ref">2022</a>)</cite>. This approach tokenize a sequence of gestures into hierarchical and compact motion tokens, ensuring both motion quality and diversity. As illustrated in Figure <a href="#S3.F2" title="Figure 2 ‣ 3. System Overview ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, our system comprises three key modules: (a) an end-to-end neural generator capable of processing a diverse range of speech audio inputs to produce rhythm-matched gesture animations utilizing the GPT architecture <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib72" title="" class="ltx_ref">2019</a>)</cite>; (b) a large language model (LLM)-based generative retrieval framework that comprehends the context of the transcript input and retrieves appropriate semantic gestures from a high-quality motion library covering commonly used semantic gestures; and (c) a semantics-aware alignment mechanism that integrates the retrieved semantic gestures with the rhythmic motion generated, resulting in semantically-enhanced gesture animation. In subsequent sections, we detail the components of our system and their respective training processes.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Co-Speech Gesture GPT Model</h2>

<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2405.09814/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="415" height="181" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>. </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">The process of synthesizing a rhythm-coherent gesture segment consists of: (a) a residual VQ-VAE <cite class="ltx_cite ltx_citemacro_citep">(Zeghidour et al<span class="ltx_text">.</span>, <a href="#bib.bib100" title="" class="ltx_ref">2022</a>)</cite> learns a hierarchical categorical space to represent motion as discrete tokens; (b) a powerful GPT-based <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib72" title="" class="ltx_ref">2019</a>)</cite> generator predicts the future gesture token conditioned on the preceding gesture tokens and synchronized audio features in an autoregressive manner.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F3.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.12" class="ltx_p">We design a gesture generative model <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S4.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\mathcal{G}</annotation></semantics></math> enabling synthesizing rhythm-matched gestures as the foundation for the following semantic enhancement. The generator <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S4.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><ci id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">\mathcal{G}</annotation></semantics></math> predicts a sequence of discrete gesture tokens <math id="S4.p1.3.m3.1" class="ltx_Math" alttext="\bm{\hat{Z}}=[[\bm{\hat{z}}^{r}_{l}]_{r=1}^{R}]_{l=1}^{L}" display="inline"><semantics id="S4.p1.3.m3.1a"><mrow id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml"><mover accent="true" id="S4.p1.3.m3.1.1.3" xref="S4.p1.3.m3.1.1.3.cmml"><mi id="S4.p1.3.m3.1.1.3.2" xref="S4.p1.3.m3.1.1.3.2.cmml">𝒁</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.p1.3.m3.1.1.3.1" xref="S4.p1.3.m3.1.1.3.1.cmml">^</mo></mover><mo id="S4.p1.3.m3.1.1.2" xref="S4.p1.3.m3.1.1.2.cmml">=</mo><msubsup id="S4.p1.3.m3.1.1.1" xref="S4.p1.3.m3.1.1.1.cmml"><mrow id="S4.p1.3.m3.1.1.1.1.1.1" xref="S4.p1.3.m3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.p1.3.m3.1.1.1.1.1.1.2" xref="S4.p1.3.m3.1.1.1.1.1.2.1.cmml">[</mo><msubsup id="S4.p1.3.m3.1.1.1.1.1.1.1" xref="S4.p1.3.m3.1.1.1.1.1.1.1.cmml"><mrow id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.2.1.cmml">[</mo><msubsup id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.2.2.1" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mi id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.3.cmml">l</mi><mi id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">r</mi></msubsup><mo stretchy="false" id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.3" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.2.1.cmml">]</mo></mrow><mrow id="S4.p1.3.m3.1.1.1.1.1.1.1.1.3" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.p1.3.m3.1.1.1.1.1.1.1.1.3.2" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.3.2.cmml">r</mi><mo id="S4.p1.3.m3.1.1.1.1.1.1.1.1.3.1" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.3.1.cmml">=</mo><mn id="S4.p1.3.m3.1.1.1.1.1.1.1.1.3.3" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S4.p1.3.m3.1.1.1.1.1.1.1.3" xref="S4.p1.3.m3.1.1.1.1.1.1.1.3.cmml">R</mi></msubsup><mo stretchy="false" id="S4.p1.3.m3.1.1.1.1.1.1.3" xref="S4.p1.3.m3.1.1.1.1.1.2.1.cmml">]</mo></mrow><mrow id="S4.p1.3.m3.1.1.1.1.3" xref="S4.p1.3.m3.1.1.1.1.3.cmml"><mi id="S4.p1.3.m3.1.1.1.1.3.2" xref="S4.p1.3.m3.1.1.1.1.3.2.cmml">l</mi><mo id="S4.p1.3.m3.1.1.1.1.3.1" xref="S4.p1.3.m3.1.1.1.1.3.1.cmml">=</mo><mn id="S4.p1.3.m3.1.1.1.1.3.3" xref="S4.p1.3.m3.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S4.p1.3.m3.1.1.1.3" xref="S4.p1.3.m3.1.1.1.3.cmml">L</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><apply id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1"><eq id="S4.p1.3.m3.1.1.2.cmml" xref="S4.p1.3.m3.1.1.2"></eq><apply id="S4.p1.3.m3.1.1.3.cmml" xref="S4.p1.3.m3.1.1.3"><ci id="S4.p1.3.m3.1.1.3.1.cmml" xref="S4.p1.3.m3.1.1.3.1">bold-^</ci><ci id="S4.p1.3.m3.1.1.3.2.cmml" xref="S4.p1.3.m3.1.1.3.2">𝒁</ci></apply><apply id="S4.p1.3.m3.1.1.1.cmml" xref="S4.p1.3.m3.1.1.1"><csymbol cd="ambiguous" id="S4.p1.3.m3.1.1.1.2.cmml" xref="S4.p1.3.m3.1.1.1">superscript</csymbol><apply id="S4.p1.3.m3.1.1.1.1.cmml" xref="S4.p1.3.m3.1.1.1"><csymbol cd="ambiguous" id="S4.p1.3.m3.1.1.1.1.2.cmml" xref="S4.p1.3.m3.1.1.1">subscript</csymbol><apply id="S4.p1.3.m3.1.1.1.1.1.2.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.p1.3.m3.1.1.1.1.1.2.1.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S4.p1.3.m3.1.1.1.1.1.1.1.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p1.3.m3.1.1.1.1.1.1.1.2.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1">superscript</csymbol><apply id="S4.p1.3.m3.1.1.1.1.1.1.1.1.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p1.3.m3.1.1.1.1.1.1.1.1.2.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.2.2"><ci id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.2.2.1">bold-^</ci><ci id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝒛</ci></apply><ci id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑟</ci></apply><ci id="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.1.1.1.3">𝑙</ci></apply></apply><apply id="S4.p1.3.m3.1.1.1.1.1.1.1.1.3.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.3"><eq id="S4.p1.3.m3.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.3.1"></eq><ci id="S4.p1.3.m3.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.3.2">𝑟</ci><cn type="integer" id="S4.p1.3.m3.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply><ci id="S4.p1.3.m3.1.1.1.1.1.1.1.3.cmml" xref="S4.p1.3.m3.1.1.1.1.1.1.1.3">𝑅</ci></apply></apply><apply id="S4.p1.3.m3.1.1.1.1.3.cmml" xref="S4.p1.3.m3.1.1.1.1.3"><eq id="S4.p1.3.m3.1.1.1.1.3.1.cmml" xref="S4.p1.3.m3.1.1.1.1.3.1"></eq><ci id="S4.p1.3.m3.1.1.1.1.3.2.cmml" xref="S4.p1.3.m3.1.1.1.1.3.2">𝑙</ci><cn type="integer" id="S4.p1.3.m3.1.1.1.1.3.3.cmml" xref="S4.p1.3.m3.1.1.1.1.3.3">1</cn></apply></apply><ci id="S4.p1.3.m3.1.1.1.3.cmml" xref="S4.p1.3.m3.1.1.1.3">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">\bm{\hat{Z}}=[[\bm{\hat{z}}^{r}_{l}]_{r=1}^{R}]_{l=1}^{L}</annotation></semantics></math> conditioned on a speech, where the space of gesture tokens is pre-learned by a Residual VQ-VAE (RVQ) <cite class="ltx_cite ltx_citemacro_citep">(Zeghidour et al<span class="ltx_text">.</span>, <a href="#bib.bib100" title="" class="ltx_ref">2022</a>)</cite>, <math id="S4.p1.4.m4.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S4.p1.4.m4.1a"><mi id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><ci id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">R</annotation></semantics></math> is the number of the residual quantization layers, <math id="S4.p1.5.m5.1" class="ltx_Math" alttext="\bm{\hat{z}}^{r}_{l}\in\mathbb{R}^{C}" display="inline"><semantics id="S4.p1.5.m5.1a"><mrow id="S4.p1.5.m5.1.1" xref="S4.p1.5.m5.1.1.cmml"><msubsup id="S4.p1.5.m5.1.1.2" xref="S4.p1.5.m5.1.1.2.cmml"><mover accent="true" id="S4.p1.5.m5.1.1.2.2.2" xref="S4.p1.5.m5.1.1.2.2.2.cmml"><mi id="S4.p1.5.m5.1.1.2.2.2.2" xref="S4.p1.5.m5.1.1.2.2.2.2.cmml">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.p1.5.m5.1.1.2.2.2.1" xref="S4.p1.5.m5.1.1.2.2.2.1.cmml">^</mo></mover><mi id="S4.p1.5.m5.1.1.2.3" xref="S4.p1.5.m5.1.1.2.3.cmml">l</mi><mi id="S4.p1.5.m5.1.1.2.2.3" xref="S4.p1.5.m5.1.1.2.2.3.cmml">r</mi></msubsup><mo id="S4.p1.5.m5.1.1.1" xref="S4.p1.5.m5.1.1.1.cmml">∈</mo><msup id="S4.p1.5.m5.1.1.3" xref="S4.p1.5.m5.1.1.3.cmml"><mi id="S4.p1.5.m5.1.1.3.2" xref="S4.p1.5.m5.1.1.3.2.cmml">ℝ</mi><mi id="S4.p1.5.m5.1.1.3.3" xref="S4.p1.5.m5.1.1.3.3.cmml">C</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.5.m5.1b"><apply id="S4.p1.5.m5.1.1.cmml" xref="S4.p1.5.m5.1.1"><in id="S4.p1.5.m5.1.1.1.cmml" xref="S4.p1.5.m5.1.1.1"></in><apply id="S4.p1.5.m5.1.1.2.cmml" xref="S4.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.p1.5.m5.1.1.2.1.cmml" xref="S4.p1.5.m5.1.1.2">subscript</csymbol><apply id="S4.p1.5.m5.1.1.2.2.cmml" xref="S4.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.p1.5.m5.1.1.2.2.1.cmml" xref="S4.p1.5.m5.1.1.2">superscript</csymbol><apply id="S4.p1.5.m5.1.1.2.2.2.cmml" xref="S4.p1.5.m5.1.1.2.2.2"><ci id="S4.p1.5.m5.1.1.2.2.2.1.cmml" xref="S4.p1.5.m5.1.1.2.2.2.1">bold-^</ci><ci id="S4.p1.5.m5.1.1.2.2.2.2.cmml" xref="S4.p1.5.m5.1.1.2.2.2.2">𝒛</ci></apply><ci id="S4.p1.5.m5.1.1.2.2.3.cmml" xref="S4.p1.5.m5.1.1.2.2.3">𝑟</ci></apply><ci id="S4.p1.5.m5.1.1.2.3.cmml" xref="S4.p1.5.m5.1.1.2.3">𝑙</ci></apply><apply id="S4.p1.5.m5.1.1.3.cmml" xref="S4.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.p1.5.m5.1.1.3.1.cmml" xref="S4.p1.5.m5.1.1.3">superscript</csymbol><ci id="S4.p1.5.m5.1.1.3.2.cmml" xref="S4.p1.5.m5.1.1.3.2">ℝ</ci><ci id="S4.p1.5.m5.1.1.3.3.cmml" xref="S4.p1.5.m5.1.1.3.3">𝐶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.5.m5.1c">\bm{\hat{z}}^{r}_{l}\in\mathbb{R}^{C}</annotation></semantics></math>, and <math id="S4.p1.6.m6.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S4.p1.6.m6.1a"><mi id="S4.p1.6.m6.1.1" xref="S4.p1.6.m6.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.p1.6.m6.1b"><ci id="S4.p1.6.m6.1.1.cmml" xref="S4.p1.6.m6.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.6.m6.1c">C</annotation></semantics></math> is the dimension of the latent space. Then, the token sequence is decoded into gesture motion <math id="S4.p1.7.m7.1" class="ltx_Math" alttext="\bm{M}=[\bm{m}_{k}]_{k=1}^{K}" display="inline"><semantics id="S4.p1.7.m7.1a"><mrow id="S4.p1.7.m7.1.1" xref="S4.p1.7.m7.1.1.cmml"><mi id="S4.p1.7.m7.1.1.3" xref="S4.p1.7.m7.1.1.3.cmml">𝑴</mi><mo id="S4.p1.7.m7.1.1.2" xref="S4.p1.7.m7.1.1.2.cmml">=</mo><msubsup id="S4.p1.7.m7.1.1.1" xref="S4.p1.7.m7.1.1.1.cmml"><mrow id="S4.p1.7.m7.1.1.1.1.1.1" xref="S4.p1.7.m7.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.p1.7.m7.1.1.1.1.1.1.2" xref="S4.p1.7.m7.1.1.1.1.1.2.1.cmml">[</mo><msub id="S4.p1.7.m7.1.1.1.1.1.1.1" xref="S4.p1.7.m7.1.1.1.1.1.1.1.cmml"><mi id="S4.p1.7.m7.1.1.1.1.1.1.1.2" xref="S4.p1.7.m7.1.1.1.1.1.1.1.2.cmml">𝒎</mi><mi id="S4.p1.7.m7.1.1.1.1.1.1.1.3" xref="S4.p1.7.m7.1.1.1.1.1.1.1.3.cmml">k</mi></msub><mo stretchy="false" id="S4.p1.7.m7.1.1.1.1.1.1.3" xref="S4.p1.7.m7.1.1.1.1.1.2.1.cmml">]</mo></mrow><mrow id="S4.p1.7.m7.1.1.1.1.3" xref="S4.p1.7.m7.1.1.1.1.3.cmml"><mi id="S4.p1.7.m7.1.1.1.1.3.2" xref="S4.p1.7.m7.1.1.1.1.3.2.cmml">k</mi><mo id="S4.p1.7.m7.1.1.1.1.3.1" xref="S4.p1.7.m7.1.1.1.1.3.1.cmml">=</mo><mn id="S4.p1.7.m7.1.1.1.1.3.3" xref="S4.p1.7.m7.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S4.p1.7.m7.1.1.1.3" xref="S4.p1.7.m7.1.1.1.3.cmml">K</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.7.m7.1b"><apply id="S4.p1.7.m7.1.1.cmml" xref="S4.p1.7.m7.1.1"><eq id="S4.p1.7.m7.1.1.2.cmml" xref="S4.p1.7.m7.1.1.2"></eq><ci id="S4.p1.7.m7.1.1.3.cmml" xref="S4.p1.7.m7.1.1.3">𝑴</ci><apply id="S4.p1.7.m7.1.1.1.cmml" xref="S4.p1.7.m7.1.1.1"><csymbol cd="ambiguous" id="S4.p1.7.m7.1.1.1.2.cmml" xref="S4.p1.7.m7.1.1.1">superscript</csymbol><apply id="S4.p1.7.m7.1.1.1.1.cmml" xref="S4.p1.7.m7.1.1.1"><csymbol cd="ambiguous" id="S4.p1.7.m7.1.1.1.1.2.cmml" xref="S4.p1.7.m7.1.1.1">subscript</csymbol><apply id="S4.p1.7.m7.1.1.1.1.1.2.cmml" xref="S4.p1.7.m7.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.p1.7.m7.1.1.1.1.1.2.1.cmml" xref="S4.p1.7.m7.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S4.p1.7.m7.1.1.1.1.1.1.1.cmml" xref="S4.p1.7.m7.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p1.7.m7.1.1.1.1.1.1.1.1.cmml" xref="S4.p1.7.m7.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.p1.7.m7.1.1.1.1.1.1.1.2.cmml" xref="S4.p1.7.m7.1.1.1.1.1.1.1.2">𝒎</ci><ci id="S4.p1.7.m7.1.1.1.1.1.1.1.3.cmml" xref="S4.p1.7.m7.1.1.1.1.1.1.1.3">𝑘</ci></apply></apply><apply id="S4.p1.7.m7.1.1.1.1.3.cmml" xref="S4.p1.7.m7.1.1.1.1.3"><eq id="S4.p1.7.m7.1.1.1.1.3.1.cmml" xref="S4.p1.7.m7.1.1.1.1.3.1"></eq><ci id="S4.p1.7.m7.1.1.1.1.3.2.cmml" xref="S4.p1.7.m7.1.1.1.1.3.2">𝑘</ci><cn type="integer" id="S4.p1.7.m7.1.1.1.1.3.3.cmml" xref="S4.p1.7.m7.1.1.1.1.3.3">1</cn></apply></apply><ci id="S4.p1.7.m7.1.1.1.3.cmml" xref="S4.p1.7.m7.1.1.1.3">𝐾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.7.m7.1c">\bm{M}=[\bm{m}_{k}]_{k=1}^{K}</annotation></semantics></math> using the RVQ decoder <math id="S4.p1.8.m8.1" class="ltx_Math" alttext="\mathcal{D}_{{\text{VQ}}}" display="inline"><semantics id="S4.p1.8.m8.1a"><msub id="S4.p1.8.m8.1.1" xref="S4.p1.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p1.8.m8.1.1.2" xref="S4.p1.8.m8.1.1.2.cmml">𝒟</mi><mtext id="S4.p1.8.m8.1.1.3" xref="S4.p1.8.m8.1.1.3a.cmml">VQ</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.p1.8.m8.1b"><apply id="S4.p1.8.m8.1.1.cmml" xref="S4.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S4.p1.8.m8.1.1.1.cmml" xref="S4.p1.8.m8.1.1">subscript</csymbol><ci id="S4.p1.8.m8.1.1.2.cmml" xref="S4.p1.8.m8.1.1.2">𝒟</ci><ci id="S4.p1.8.m8.1.1.3a.cmml" xref="S4.p1.8.m8.1.1.3"><mtext mathsize="70%" id="S4.p1.8.m8.1.1.3.cmml" xref="S4.p1.8.m8.1.1.3">VQ</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.8.m8.1c">\mathcal{D}_{{\text{VQ}}}</annotation></semantics></math>, where the RVQ’s downsampling rate is <math id="S4.p1.9.m9.1" class="ltx_Math" alttext="d=K/L" display="inline"><semantics id="S4.p1.9.m9.1a"><mrow id="S4.p1.9.m9.1.1" xref="S4.p1.9.m9.1.1.cmml"><mi id="S4.p1.9.m9.1.1.2" xref="S4.p1.9.m9.1.1.2.cmml">d</mi><mo id="S4.p1.9.m9.1.1.1" xref="S4.p1.9.m9.1.1.1.cmml">=</mo><mrow id="S4.p1.9.m9.1.1.3" xref="S4.p1.9.m9.1.1.3.cmml"><mi id="S4.p1.9.m9.1.1.3.2" xref="S4.p1.9.m9.1.1.3.2.cmml">K</mi><mo id="S4.p1.9.m9.1.1.3.1" xref="S4.p1.9.m9.1.1.3.1.cmml">/</mo><mi id="S4.p1.9.m9.1.1.3.3" xref="S4.p1.9.m9.1.1.3.3.cmml">L</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.9.m9.1b"><apply id="S4.p1.9.m9.1.1.cmml" xref="S4.p1.9.m9.1.1"><eq id="S4.p1.9.m9.1.1.1.cmml" xref="S4.p1.9.m9.1.1.1"></eq><ci id="S4.p1.9.m9.1.1.2.cmml" xref="S4.p1.9.m9.1.1.2">𝑑</ci><apply id="S4.p1.9.m9.1.1.3.cmml" xref="S4.p1.9.m9.1.1.3"><divide id="S4.p1.9.m9.1.1.3.1.cmml" xref="S4.p1.9.m9.1.1.3.1"></divide><ci id="S4.p1.9.m9.1.1.3.2.cmml" xref="S4.p1.9.m9.1.1.3.2">𝐾</ci><ci id="S4.p1.9.m9.1.1.3.3.cmml" xref="S4.p1.9.m9.1.1.3.3">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.9.m9.1c">d=K/L</annotation></semantics></math>. Each pose <math id="S4.p1.10.m10.1" class="ltx_Math" alttext="\bm{m}_{k}\in\mathbb{R}^{3+6J}" display="inline"><semantics id="S4.p1.10.m10.1a"><mrow id="S4.p1.10.m10.1.1" xref="S4.p1.10.m10.1.1.cmml"><msub id="S4.p1.10.m10.1.1.2" xref="S4.p1.10.m10.1.1.2.cmml"><mi id="S4.p1.10.m10.1.1.2.2" xref="S4.p1.10.m10.1.1.2.2.cmml">𝒎</mi><mi id="S4.p1.10.m10.1.1.2.3" xref="S4.p1.10.m10.1.1.2.3.cmml">k</mi></msub><mo id="S4.p1.10.m10.1.1.1" xref="S4.p1.10.m10.1.1.1.cmml">∈</mo><msup id="S4.p1.10.m10.1.1.3" xref="S4.p1.10.m10.1.1.3.cmml"><mi id="S4.p1.10.m10.1.1.3.2" xref="S4.p1.10.m10.1.1.3.2.cmml">ℝ</mi><mrow id="S4.p1.10.m10.1.1.3.3" xref="S4.p1.10.m10.1.1.3.3.cmml"><mn id="S4.p1.10.m10.1.1.3.3.2" xref="S4.p1.10.m10.1.1.3.3.2.cmml">3</mn><mo id="S4.p1.10.m10.1.1.3.3.1" xref="S4.p1.10.m10.1.1.3.3.1.cmml">+</mo><mrow id="S4.p1.10.m10.1.1.3.3.3" xref="S4.p1.10.m10.1.1.3.3.3.cmml"><mn id="S4.p1.10.m10.1.1.3.3.3.2" xref="S4.p1.10.m10.1.1.3.3.3.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="S4.p1.10.m10.1.1.3.3.3.1" xref="S4.p1.10.m10.1.1.3.3.3.1.cmml">​</mo><mi id="S4.p1.10.m10.1.1.3.3.3.3" xref="S4.p1.10.m10.1.1.3.3.3.3.cmml">J</mi></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.10.m10.1b"><apply id="S4.p1.10.m10.1.1.cmml" xref="S4.p1.10.m10.1.1"><in id="S4.p1.10.m10.1.1.1.cmml" xref="S4.p1.10.m10.1.1.1"></in><apply id="S4.p1.10.m10.1.1.2.cmml" xref="S4.p1.10.m10.1.1.2"><csymbol cd="ambiguous" id="S4.p1.10.m10.1.1.2.1.cmml" xref="S4.p1.10.m10.1.1.2">subscript</csymbol><ci id="S4.p1.10.m10.1.1.2.2.cmml" xref="S4.p1.10.m10.1.1.2.2">𝒎</ci><ci id="S4.p1.10.m10.1.1.2.3.cmml" xref="S4.p1.10.m10.1.1.2.3">𝑘</ci></apply><apply id="S4.p1.10.m10.1.1.3.cmml" xref="S4.p1.10.m10.1.1.3"><csymbol cd="ambiguous" id="S4.p1.10.m10.1.1.3.1.cmml" xref="S4.p1.10.m10.1.1.3">superscript</csymbol><ci id="S4.p1.10.m10.1.1.3.2.cmml" xref="S4.p1.10.m10.1.1.3.2">ℝ</ci><apply id="S4.p1.10.m10.1.1.3.3.cmml" xref="S4.p1.10.m10.1.1.3.3"><plus id="S4.p1.10.m10.1.1.3.3.1.cmml" xref="S4.p1.10.m10.1.1.3.3.1"></plus><cn type="integer" id="S4.p1.10.m10.1.1.3.3.2.cmml" xref="S4.p1.10.m10.1.1.3.3.2">3</cn><apply id="S4.p1.10.m10.1.1.3.3.3.cmml" xref="S4.p1.10.m10.1.1.3.3.3"><times id="S4.p1.10.m10.1.1.3.3.3.1.cmml" xref="S4.p1.10.m10.1.1.3.3.3.1"></times><cn type="integer" id="S4.p1.10.m10.1.1.3.3.3.2.cmml" xref="S4.p1.10.m10.1.1.3.3.3.2">6</cn><ci id="S4.p1.10.m10.1.1.3.3.3.3.cmml" xref="S4.p1.10.m10.1.1.3.3.3.3">𝐽</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.10.m10.1c">\bm{m}_{k}\in\mathbb{R}^{3+6J}</annotation></semantics></math> consists of the translation of the avatar and the rotations of its <math id="S4.p1.11.m11.1" class="ltx_Math" alttext="J" display="inline"><semantics id="S4.p1.11.m11.1a"><mi id="S4.p1.11.m11.1.1" xref="S4.p1.11.m11.1.1.cmml">J</mi><annotation-xml encoding="MathML-Content" id="S4.p1.11.m11.1b"><ci id="S4.p1.11.m11.1.1.cmml" xref="S4.p1.11.m11.1.1">𝐽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.11.m11.1c">J</annotation></semantics></math> joints. The rotations are parameterized as the exponential map. Next, we will discuss the details of the gesture tokenizer and the generator <math id="S4.p1.12.m12.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S4.p1.12.m12.1a"><mi class="ltx_font_mathcaligraphic" id="S4.p1.12.m12.1.1" xref="S4.p1.12.m12.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S4.p1.12.m12.1b"><ci id="S4.p1.12.m12.1.1.cmml" xref="S4.p1.12.m12.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.12.m12.1c">\mathcal{G}</annotation></semantics></math>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Gesture Tokenizer</h3>

<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2405.09814/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="415" height="160" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.6.3.1" class="ltx_text" style="font-size:90%;">Figure 4</span>. </span><span id="S4.F4.4.2" class="ltx_text" style="font-size:90%;">Residual quantization module. The motion features <math id="S4.F4.3.1.m1.1" class="ltx_Math" alttext="\bm{Z}" display="inline"><semantics id="S4.F4.3.1.m1.1b"><mi id="S4.F4.3.1.m1.1.1" xref="S4.F4.3.1.m1.1.1.cmml">𝒁</mi><annotation-xml encoding="MathML-Content" id="S4.F4.3.1.m1.1c"><ci id="S4.F4.3.1.m1.1.1.cmml" xref="S4.F4.3.1.m1.1.1">𝒁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.3.1.m1.1d">\bm{Z}</annotation></semantics></math> are iteratively quantized by a series of residual quantization layers. The first RVQ layer is a special RVQ layer, where the preceding residue is <math id="S4.F4.4.2.m2.1" class="ltx_Math" alttext="\bm{r}_{l}^{1}=\bm{z}_{l}" display="inline"><semantics id="S4.F4.4.2.m2.1b"><mrow id="S4.F4.4.2.m2.1.1" xref="S4.F4.4.2.m2.1.1.cmml"><msubsup id="S4.F4.4.2.m2.1.1.2" xref="S4.F4.4.2.m2.1.1.2.cmml"><mi id="S4.F4.4.2.m2.1.1.2.2.2" xref="S4.F4.4.2.m2.1.1.2.2.2.cmml">𝒓</mi><mi id="S4.F4.4.2.m2.1.1.2.2.3" xref="S4.F4.4.2.m2.1.1.2.2.3.cmml">l</mi><mn id="S4.F4.4.2.m2.1.1.2.3" xref="S4.F4.4.2.m2.1.1.2.3.cmml">1</mn></msubsup><mo id="S4.F4.4.2.m2.1.1.1" xref="S4.F4.4.2.m2.1.1.1.cmml">=</mo><msub id="S4.F4.4.2.m2.1.1.3" xref="S4.F4.4.2.m2.1.1.3.cmml"><mi id="S4.F4.4.2.m2.1.1.3.2" xref="S4.F4.4.2.m2.1.1.3.2.cmml">𝒛</mi><mi id="S4.F4.4.2.m2.1.1.3.3" xref="S4.F4.4.2.m2.1.1.3.3.cmml">l</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.4.2.m2.1c"><apply id="S4.F4.4.2.m2.1.1.cmml" xref="S4.F4.4.2.m2.1.1"><eq id="S4.F4.4.2.m2.1.1.1.cmml" xref="S4.F4.4.2.m2.1.1.1"></eq><apply id="S4.F4.4.2.m2.1.1.2.cmml" xref="S4.F4.4.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.F4.4.2.m2.1.1.2.1.cmml" xref="S4.F4.4.2.m2.1.1.2">superscript</csymbol><apply id="S4.F4.4.2.m2.1.1.2.2.cmml" xref="S4.F4.4.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.F4.4.2.m2.1.1.2.2.1.cmml" xref="S4.F4.4.2.m2.1.1.2">subscript</csymbol><ci id="S4.F4.4.2.m2.1.1.2.2.2.cmml" xref="S4.F4.4.2.m2.1.1.2.2.2">𝒓</ci><ci id="S4.F4.4.2.m2.1.1.2.2.3.cmml" xref="S4.F4.4.2.m2.1.1.2.2.3">𝑙</ci></apply><cn type="integer" id="S4.F4.4.2.m2.1.1.2.3.cmml" xref="S4.F4.4.2.m2.1.1.2.3">1</cn></apply><apply id="S4.F4.4.2.m2.1.1.3.cmml" xref="S4.F4.4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.F4.4.2.m2.1.1.3.1.cmml" xref="S4.F4.4.2.m2.1.1.3">subscript</csymbol><ci id="S4.F4.4.2.m2.1.1.3.2.cmml" xref="S4.F4.4.2.m2.1.1.3.2">𝒛</ci><ci id="S4.F4.4.2.m2.1.1.3.3.cmml" xref="S4.F4.4.2.m2.1.1.3.3">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.4.2.m2.1d">\bm{r}_{l}^{1}=\bm{z}_{l}</annotation></semantics></math>.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F4.7" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.2" class="ltx_p">Vanilla VQ-VAE <cite class="ltx_cite ltx_citemacro_citep">(van den Oord et al<span class="ltx_text">.</span>, <a href="#bib.bib79" title="" class="ltx_ref">2017</a>)</cite> exhibits limited representational capacity, which hinders its ability to reconstruct complex motions, particularly in finger animation. To address this, as shown in Figure <a href="#S4.F3" title="Figure 3 ‣ 4. Co-Speech Gesture GPT Model ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a), we enhance the standard VQ-VAE across three dimensions: 1) by dividing the motion representation into two parts, namely, body and hands, and compressing them independently; 2) by designing a more powerful encoder <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{E}_{{\text{VQ}}}" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><msub id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">ℰ</mi><mtext id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3a.cmml">VQ</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">ℰ</ci><ci id="S4.SS1.p1.1.m1.1.1.3a.cmml" xref="S4.SS1.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">VQ</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\mathcal{E}_{{\text{VQ}}}</annotation></semantics></math> and decoder <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{D}_{{\text{VQ}}}" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><msub id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">𝒟</mi><mtext id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3a.cmml">VQ</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">𝒟</ci><ci id="S4.SS1.p1.2.m2.1.1.3a.cmml" xref="S4.SS1.p1.2.m2.1.1.3"><mtext mathsize="70%" id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">VQ</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\mathcal{D}_{{\text{VQ}}}</annotation></semantics></math> consisting of 1D convolutional layers and the Transformer layer <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2017</a>)</cite>; 3) by improving the capacity of the quantization module through the addition of multiple residual quantization layers.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.6" class="ltx_p">Specifically, we divide the gesture sequence <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\bm{M}" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mi id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">𝑴</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">𝑴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\bm{M}</annotation></semantics></math> into the body part <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="\bm{M}_{{\text{body}}}" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><msub id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mi id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml">𝑴</mi><mtext id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3a.cmml">body</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2">𝑴</ci><ci id="S4.SS1.p2.2.m2.1.1.3a.cmml" xref="S4.SS1.p2.2.m2.1.1.3"><mtext mathsize="70%" id="S4.SS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3">body</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">\bm{M}_{{\text{body}}}</annotation></semantics></math> and the hand part <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="\bm{M}_{{\text{hand}}}" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><msub id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><mi id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml">𝑴</mi><mtext id="S4.SS1.p2.3.m3.1.1.3" xref="S4.SS1.p2.3.m3.1.1.3a.cmml">hand</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2">𝑴</ci><ci id="S4.SS1.p2.3.m3.1.1.3a.cmml" xref="S4.SS1.p2.3.m3.1.1.3"><mtext mathsize="70%" id="S4.SS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3">hand</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">\bm{M}_{{\text{hand}}}</annotation></semantics></math>, utilizing two independent RVQ networks to model them respectively. The body part encompasses joints except for the fingers, while the hand part represents finger movements. Each network, acting as an expert, focuses on one specific part, thereby facilitating the handling of the complexity inherent in human movements. For simplicity, we next still use <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="\bm{M}" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><mi id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">𝑴</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><ci id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">𝑴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">\bm{M}</annotation></semantics></math> to stand in for the general motion sequence. Formally, the RVQ encoder <math id="S4.SS1.p2.5.m5.1" class="ltx_Math" alttext="\mathcal{E}_{{\text{VQ}}}" display="inline"><semantics id="S4.SS1.p2.5.m5.1a"><msub id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p2.5.m5.1.1.2" xref="S4.SS1.p2.5.m5.1.1.2.cmml">ℰ</mi><mtext id="S4.SS1.p2.5.m5.1.1.3" xref="S4.SS1.p2.5.m5.1.1.3a.cmml">VQ</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.1b"><apply id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.5.m5.1.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S4.SS1.p2.5.m5.1.1.2.cmml" xref="S4.SS1.p2.5.m5.1.1.2">ℰ</ci><ci id="S4.SS1.p2.5.m5.1.1.3a.cmml" xref="S4.SS1.p2.5.m5.1.1.3"><mtext mathsize="70%" id="S4.SS1.p2.5.m5.1.1.3.cmml" xref="S4.SS1.p2.5.m5.1.1.3">VQ</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.1c">\mathcal{E}_{{\text{VQ}}}</annotation></semantics></math> computes the gesture feature sequence <math id="S4.SS1.p2.6.m6.1" class="ltx_Math" alttext="\bm{Z}=[\bm{z}_{l}]_{l=1}^{L}" display="inline"><semantics id="S4.SS1.p2.6.m6.1a"><mrow id="S4.SS1.p2.6.m6.1.1" xref="S4.SS1.p2.6.m6.1.1.cmml"><mi id="S4.SS1.p2.6.m6.1.1.3" xref="S4.SS1.p2.6.m6.1.1.3.cmml">𝒁</mi><mo id="S4.SS1.p2.6.m6.1.1.2" xref="S4.SS1.p2.6.m6.1.1.2.cmml">=</mo><msubsup id="S4.SS1.p2.6.m6.1.1.1" xref="S4.SS1.p2.6.m6.1.1.1.cmml"><mrow id="S4.SS1.p2.6.m6.1.1.1.1.1.1" xref="S4.SS1.p2.6.m6.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS1.p2.6.m6.1.1.1.1.1.1.2" xref="S4.SS1.p2.6.m6.1.1.1.1.1.2.1.cmml">[</mo><msub id="S4.SS1.p2.6.m6.1.1.1.1.1.1.1" xref="S4.SS1.p2.6.m6.1.1.1.1.1.1.1.cmml"><mi id="S4.SS1.p2.6.m6.1.1.1.1.1.1.1.2" xref="S4.SS1.p2.6.m6.1.1.1.1.1.1.1.2.cmml">𝒛</mi><mi id="S4.SS1.p2.6.m6.1.1.1.1.1.1.1.3" xref="S4.SS1.p2.6.m6.1.1.1.1.1.1.1.3.cmml">l</mi></msub><mo stretchy="false" id="S4.SS1.p2.6.m6.1.1.1.1.1.1.3" xref="S4.SS1.p2.6.m6.1.1.1.1.1.2.1.cmml">]</mo></mrow><mrow id="S4.SS1.p2.6.m6.1.1.1.1.3" xref="S4.SS1.p2.6.m6.1.1.1.1.3.cmml"><mi id="S4.SS1.p2.6.m6.1.1.1.1.3.2" xref="S4.SS1.p2.6.m6.1.1.1.1.3.2.cmml">l</mi><mo id="S4.SS1.p2.6.m6.1.1.1.1.3.1" xref="S4.SS1.p2.6.m6.1.1.1.1.3.1.cmml">=</mo><mn id="S4.SS1.p2.6.m6.1.1.1.1.3.3" xref="S4.SS1.p2.6.m6.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S4.SS1.p2.6.m6.1.1.1.3" xref="S4.SS1.p2.6.m6.1.1.1.3.cmml">L</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.6.m6.1b"><apply id="S4.SS1.p2.6.m6.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1"><eq id="S4.SS1.p2.6.m6.1.1.2.cmml" xref="S4.SS1.p2.6.m6.1.1.2"></eq><ci id="S4.SS1.p2.6.m6.1.1.3.cmml" xref="S4.SS1.p2.6.m6.1.1.3">𝒁</ci><apply id="S4.SS1.p2.6.m6.1.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.6.m6.1.1.1.2.cmml" xref="S4.SS1.p2.6.m6.1.1.1">superscript</csymbol><apply id="S4.SS1.p2.6.m6.1.1.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.6.m6.1.1.1.1.2.cmml" xref="S4.SS1.p2.6.m6.1.1.1">subscript</csymbol><apply id="S4.SS1.p2.6.m6.1.1.1.1.1.2.cmml" xref="S4.SS1.p2.6.m6.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.SS1.p2.6.m6.1.1.1.1.1.2.1.cmml" xref="S4.SS1.p2.6.m6.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S4.SS1.p2.6.m6.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.6.m6.1.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p2.6.m6.1.1.1.1.1.1.1.2.cmml" xref="S4.SS1.p2.6.m6.1.1.1.1.1.1.1.2">𝒛</ci><ci id="S4.SS1.p2.6.m6.1.1.1.1.1.1.1.3.cmml" xref="S4.SS1.p2.6.m6.1.1.1.1.1.1.1.3">𝑙</ci></apply></apply><apply id="S4.SS1.p2.6.m6.1.1.1.1.3.cmml" xref="S4.SS1.p2.6.m6.1.1.1.1.3"><eq id="S4.SS1.p2.6.m6.1.1.1.1.3.1.cmml" xref="S4.SS1.p2.6.m6.1.1.1.1.3.1"></eq><ci id="S4.SS1.p2.6.m6.1.1.1.1.3.2.cmml" xref="S4.SS1.p2.6.m6.1.1.1.1.3.2">𝑙</ci><cn type="integer" id="S4.SS1.p2.6.m6.1.1.1.1.3.3.cmml" xref="S4.SS1.p2.6.m6.1.1.1.1.3.3">1</cn></apply></apply><ci id="S4.SS1.p2.6.m6.1.1.1.3.cmml" xref="S4.SS1.p2.6.m6.1.1.1.3">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.6.m6.1c">\bm{Z}=[\bm{z}_{l}]_{l=1}^{L}</annotation></semantics></math> as</p>
<table id="A2.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E1.m1.2" class="ltx_Math" alttext="\displaystyle\bm{Z}=\mathcal{E}_{{\text{VQ}}}(\bm{M})," display="inline"><semantics id="S4.E1.m1.2a"><mrow id="S4.E1.m1.2.2.1" xref="S4.E1.m1.2.2.1.1.cmml"><mrow id="S4.E1.m1.2.2.1.1" xref="S4.E1.m1.2.2.1.1.cmml"><mi id="S4.E1.m1.2.2.1.1.2" xref="S4.E1.m1.2.2.1.1.2.cmml">𝒁</mi><mo id="S4.E1.m1.2.2.1.1.1" xref="S4.E1.m1.2.2.1.1.1.cmml">=</mo><mrow id="S4.E1.m1.2.2.1.1.3" xref="S4.E1.m1.2.2.1.1.3.cmml"><msub id="S4.E1.m1.2.2.1.1.3.2" xref="S4.E1.m1.2.2.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.2.2.1.1.3.2.2" xref="S4.E1.m1.2.2.1.1.3.2.2.cmml">ℰ</mi><mtext id="S4.E1.m1.2.2.1.1.3.2.3" xref="S4.E1.m1.2.2.1.1.3.2.3a.cmml">VQ</mtext></msub><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.1.1.3.1" xref="S4.E1.m1.2.2.1.1.3.1.cmml">​</mo><mrow id="S4.E1.m1.2.2.1.1.3.3.2" xref="S4.E1.m1.2.2.1.1.3.cmml"><mo stretchy="false" id="S4.E1.m1.2.2.1.1.3.3.2.1" xref="S4.E1.m1.2.2.1.1.3.cmml">(</mo><mi id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml">𝑴</mi><mo stretchy="false" id="S4.E1.m1.2.2.1.1.3.3.2.2" xref="S4.E1.m1.2.2.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E1.m1.2.2.1.2" xref="S4.E1.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.2b"><apply id="S4.E1.m1.2.2.1.1.cmml" xref="S4.E1.m1.2.2.1"><eq id="S4.E1.m1.2.2.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1"></eq><ci id="S4.E1.m1.2.2.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.2">𝒁</ci><apply id="S4.E1.m1.2.2.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.3"><times id="S4.E1.m1.2.2.1.1.3.1.cmml" xref="S4.E1.m1.2.2.1.1.3.1"></times><apply id="S4.E1.m1.2.2.1.1.3.2.cmml" xref="S4.E1.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.3.2.1.cmml" xref="S4.E1.m1.2.2.1.1.3.2">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.3.2.2.cmml" xref="S4.E1.m1.2.2.1.1.3.2.2">ℰ</ci><ci id="S4.E1.m1.2.2.1.1.3.2.3a.cmml" xref="S4.E1.m1.2.2.1.1.3.2.3"><mtext mathsize="70%" id="S4.E1.m1.2.2.1.1.3.2.3.cmml" xref="S4.E1.m1.2.2.1.1.3.2.3">VQ</mtext></ci></apply><ci id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1">𝑴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.2c">\displaystyle\bm{Z}=\mathcal{E}_{{\text{VQ}}}(\bm{M}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS1.p2.8" class="ltx_p">where <math id="S4.SS1.p2.7.m1.1" class="ltx_Math" alttext="\bm{z}_{l}\in\mathbb{R}^{C}" display="inline"><semantics id="S4.SS1.p2.7.m1.1a"><mrow id="S4.SS1.p2.7.m1.1.1" xref="S4.SS1.p2.7.m1.1.1.cmml"><msub id="S4.SS1.p2.7.m1.1.1.2" xref="S4.SS1.p2.7.m1.1.1.2.cmml"><mi id="S4.SS1.p2.7.m1.1.1.2.2" xref="S4.SS1.p2.7.m1.1.1.2.2.cmml">𝒛</mi><mi id="S4.SS1.p2.7.m1.1.1.2.3" xref="S4.SS1.p2.7.m1.1.1.2.3.cmml">l</mi></msub><mo id="S4.SS1.p2.7.m1.1.1.1" xref="S4.SS1.p2.7.m1.1.1.1.cmml">∈</mo><msup id="S4.SS1.p2.7.m1.1.1.3" xref="S4.SS1.p2.7.m1.1.1.3.cmml"><mi id="S4.SS1.p2.7.m1.1.1.3.2" xref="S4.SS1.p2.7.m1.1.1.3.2.cmml">ℝ</mi><mi id="S4.SS1.p2.7.m1.1.1.3.3" xref="S4.SS1.p2.7.m1.1.1.3.3.cmml">C</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.7.m1.1b"><apply id="S4.SS1.p2.7.m1.1.1.cmml" xref="S4.SS1.p2.7.m1.1.1"><in id="S4.SS1.p2.7.m1.1.1.1.cmml" xref="S4.SS1.p2.7.m1.1.1.1"></in><apply id="S4.SS1.p2.7.m1.1.1.2.cmml" xref="S4.SS1.p2.7.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p2.7.m1.1.1.2.1.cmml" xref="S4.SS1.p2.7.m1.1.1.2">subscript</csymbol><ci id="S4.SS1.p2.7.m1.1.1.2.2.cmml" xref="S4.SS1.p2.7.m1.1.1.2.2">𝒛</ci><ci id="S4.SS1.p2.7.m1.1.1.2.3.cmml" xref="S4.SS1.p2.7.m1.1.1.2.3">𝑙</ci></apply><apply id="S4.SS1.p2.7.m1.1.1.3.cmml" xref="S4.SS1.p2.7.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.7.m1.1.1.3.1.cmml" xref="S4.SS1.p2.7.m1.1.1.3">superscript</csymbol><ci id="S4.SS1.p2.7.m1.1.1.3.2.cmml" xref="S4.SS1.p2.7.m1.1.1.3.2">ℝ</ci><ci id="S4.SS1.p2.7.m1.1.1.3.3.cmml" xref="S4.SS1.p2.7.m1.1.1.3.3">𝐶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.7.m1.1c">\bm{z}_{l}\in\mathbb{R}^{C}</annotation></semantics></math> and <math id="S4.SS1.p2.8.m2.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S4.SS1.p2.8.m2.1a"><mi id="S4.SS1.p2.8.m2.1.1" xref="S4.SS1.p2.8.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.8.m2.1b"><ci id="S4.SS1.p2.8.m2.1.1.cmml" xref="S4.SS1.p2.8.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.8.m2.1c">C</annotation></semantics></math> is the dimension of the latent space.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.9" class="ltx_p">Then, we should quantize <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="\bm{Z}" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mi id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">𝒁</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><ci id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">𝒁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">\bm{Z}</annotation></semantics></math> into <math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="\bm{\hat{Z}}" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mover accent="true" id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml"><mi id="S4.SS1.p3.2.m2.1.1.2" xref="S4.SS1.p3.2.m2.1.1.2.cmml">𝒁</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.SS1.p3.2.m2.1.1.1" xref="S4.SS1.p3.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><apply id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1"><ci id="S4.SS1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1.1">bold-^</ci><ci id="S4.SS1.p3.2.m2.1.1.2.cmml" xref="S4.SS1.p3.2.m2.1.1.2">𝒁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">\bm{\hat{Z}}</annotation></semantics></math>. Due to the high diversity of motion data, the representational capacity of the quantization module needs to be extended. But simply increasing the size of the codebook <math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><ci id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">\mathcal{C}</annotation></semantics></math> would lead to inefficient and unstable training, e.g., code collapse <cite class="ltx_cite ltx_citemacro_citep">(Dhariwal et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite>. To address it, inspired by <cite class="ltx_cite ltx_citemacro_citep">(Zeghidour et al<span class="ltx_text">.</span>, <a href="#bib.bib100" title="" class="ltx_ref">2022</a>)</cite>, we implement a hierarchical architecture with multiple residual vector quantization layers and corresponding independent codebooks <math id="S4.SS1.p3.4.m4.1" class="ltx_Math" alttext="\{\mathcal{C}_{i}\}_{i=1}^{R}" display="inline"><semantics id="S4.SS1.p3.4.m4.1a"><msubsup id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml"><mrow id="S4.SS1.p3.4.m4.1.1.1.1.1" xref="S4.SS1.p3.4.m4.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS1.p3.4.m4.1.1.1.1.1.2" xref="S4.SS1.p3.4.m4.1.1.1.1.2.cmml">{</mo><msub id="S4.SS1.p3.4.m4.1.1.1.1.1.1" xref="S4.SS1.p3.4.m4.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p3.4.m4.1.1.1.1.1.1.2" xref="S4.SS1.p3.4.m4.1.1.1.1.1.1.2.cmml">𝒞</mi><mi id="S4.SS1.p3.4.m4.1.1.1.1.1.1.3" xref="S4.SS1.p3.4.m4.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.SS1.p3.4.m4.1.1.1.1.1.3" xref="S4.SS1.p3.4.m4.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S4.SS1.p3.4.m4.1.1.1.3" xref="S4.SS1.p3.4.m4.1.1.1.3.cmml"><mi id="S4.SS1.p3.4.m4.1.1.1.3.2" xref="S4.SS1.p3.4.m4.1.1.1.3.2.cmml">i</mi><mo id="S4.SS1.p3.4.m4.1.1.1.3.1" xref="S4.SS1.p3.4.m4.1.1.1.3.1.cmml">=</mo><mn id="S4.SS1.p3.4.m4.1.1.1.3.3" xref="S4.SS1.p3.4.m4.1.1.1.3.3.cmml">1</mn></mrow><mi id="S4.SS1.p3.4.m4.1.1.3" xref="S4.SS1.p3.4.m4.1.1.3.cmml">R</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><apply id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.4.m4.1.1.2.cmml" xref="S4.SS1.p3.4.m4.1.1">superscript</csymbol><apply id="S4.SS1.p3.4.m4.1.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.4.m4.1.1.1.2.cmml" xref="S4.SS1.p3.4.m4.1.1">subscript</csymbol><set id="S4.SS1.p3.4.m4.1.1.1.1.2.cmml" xref="S4.SS1.p3.4.m4.1.1.1.1.1"><apply id="S4.SS1.p3.4.m4.1.1.1.1.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.4.m4.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p3.4.m4.1.1.1.1.1.1.2.cmml" xref="S4.SS1.p3.4.m4.1.1.1.1.1.1.2">𝒞</ci><ci id="S4.SS1.p3.4.m4.1.1.1.1.1.1.3.cmml" xref="S4.SS1.p3.4.m4.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S4.SS1.p3.4.m4.1.1.1.3.cmml" xref="S4.SS1.p3.4.m4.1.1.1.3"><eq id="S4.SS1.p3.4.m4.1.1.1.3.1.cmml" xref="S4.SS1.p3.4.m4.1.1.1.3.1"></eq><ci id="S4.SS1.p3.4.m4.1.1.1.3.2.cmml" xref="S4.SS1.p3.4.m4.1.1.1.3.2">𝑖</ci><cn type="integer" id="S4.SS1.p3.4.m4.1.1.1.3.3.cmml" xref="S4.SS1.p3.4.m4.1.1.1.3.3">1</cn></apply></apply><ci id="S4.SS1.p3.4.m4.1.1.3.cmml" xref="S4.SS1.p3.4.m4.1.1.3">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">\{\mathcal{C}_{i}\}_{i=1}^{R}</annotation></semantics></math> to iteratively model the motion features, where the size of each codebook is <math id="S4.SS1.p3.5.m5.1" class="ltx_Math" alttext="N_{{\text{VQ}}}" display="inline"><semantics id="S4.SS1.p3.5.m5.1a"><msub id="S4.SS1.p3.5.m5.1.1" xref="S4.SS1.p3.5.m5.1.1.cmml"><mi id="S4.SS1.p3.5.m5.1.1.2" xref="S4.SS1.p3.5.m5.1.1.2.cmml">N</mi><mtext id="S4.SS1.p3.5.m5.1.1.3" xref="S4.SS1.p3.5.m5.1.1.3a.cmml">VQ</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.5.m5.1b"><apply id="S4.SS1.p3.5.m5.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.5.m5.1.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1">subscript</csymbol><ci id="S4.SS1.p3.5.m5.1.1.2.cmml" xref="S4.SS1.p3.5.m5.1.1.2">𝑁</ci><ci id="S4.SS1.p3.5.m5.1.1.3a.cmml" xref="S4.SS1.p3.5.m5.1.1.3"><mtext mathsize="70%" id="S4.SS1.p3.5.m5.1.1.3.cmml" xref="S4.SS1.p3.5.m5.1.1.3">VQ</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.5.m5.1c">N_{{\text{VQ}}}</annotation></semantics></math>. As demonstrated in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.1. Gesture Tokenizer ‣ 4. Co-Speech Gesture GPT Model ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, a RVQ layer, e.g., RVQ-<math id="S4.SS1.p3.6.m6.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS1.p3.6.m6.1a"><mi id="S4.SS1.p3.6.m6.1.1" xref="S4.SS1.p3.6.m6.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.6.m6.1b"><ci id="S4.SS1.p3.6.m6.1.1.cmml" xref="S4.SS1.p3.6.m6.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.6.m6.1c">i</annotation></semantics></math>, processes the residue <math id="S4.SS1.p3.7.m7.1" class="ltx_Math" alttext="\bm{r}_{l}^{i}" display="inline"><semantics id="S4.SS1.p3.7.m7.1a"><msubsup id="S4.SS1.p3.7.m7.1.1" xref="S4.SS1.p3.7.m7.1.1.cmml"><mi id="S4.SS1.p3.7.m7.1.1.2.2" xref="S4.SS1.p3.7.m7.1.1.2.2.cmml">𝒓</mi><mi id="S4.SS1.p3.7.m7.1.1.2.3" xref="S4.SS1.p3.7.m7.1.1.2.3.cmml">l</mi><mi id="S4.SS1.p3.7.m7.1.1.3" xref="S4.SS1.p3.7.m7.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.7.m7.1b"><apply id="S4.SS1.p3.7.m7.1.1.cmml" xref="S4.SS1.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.7.m7.1.1.1.cmml" xref="S4.SS1.p3.7.m7.1.1">superscript</csymbol><apply id="S4.SS1.p3.7.m7.1.1.2.cmml" xref="S4.SS1.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.7.m7.1.1.2.1.cmml" xref="S4.SS1.p3.7.m7.1.1">subscript</csymbol><ci id="S4.SS1.p3.7.m7.1.1.2.2.cmml" xref="S4.SS1.p3.7.m7.1.1.2.2">𝒓</ci><ci id="S4.SS1.p3.7.m7.1.1.2.3.cmml" xref="S4.SS1.p3.7.m7.1.1.2.3">𝑙</ci></apply><ci id="S4.SS1.p3.7.m7.1.1.3.cmml" xref="S4.SS1.p3.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.7.m7.1c">\bm{r}_{l}^{i}</annotation></semantics></math> from the quantization of the preceding RVQ layer and quantizes it into a discrete feature sequence <math id="S4.SS1.p3.8.m8.1" class="ltx_Math" alttext="\bm{\hat{z}}_{l}^{i}" display="inline"><semantics id="S4.SS1.p3.8.m8.1a"><msubsup id="S4.SS1.p3.8.m8.1.1" xref="S4.SS1.p3.8.m8.1.1.cmml"><mover accent="true" id="S4.SS1.p3.8.m8.1.1.2.2" xref="S4.SS1.p3.8.m8.1.1.2.2.cmml"><mi id="S4.SS1.p3.8.m8.1.1.2.2.2" xref="S4.SS1.p3.8.m8.1.1.2.2.2.cmml">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.SS1.p3.8.m8.1.1.2.2.1" xref="S4.SS1.p3.8.m8.1.1.2.2.1.cmml">^</mo></mover><mi id="S4.SS1.p3.8.m8.1.1.2.3" xref="S4.SS1.p3.8.m8.1.1.2.3.cmml">l</mi><mi id="S4.SS1.p3.8.m8.1.1.3" xref="S4.SS1.p3.8.m8.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.8.m8.1b"><apply id="S4.SS1.p3.8.m8.1.1.cmml" xref="S4.SS1.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.8.m8.1.1.1.cmml" xref="S4.SS1.p3.8.m8.1.1">superscript</csymbol><apply id="S4.SS1.p3.8.m8.1.1.2.cmml" xref="S4.SS1.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.8.m8.1.1.2.1.cmml" xref="S4.SS1.p3.8.m8.1.1">subscript</csymbol><apply id="S4.SS1.p3.8.m8.1.1.2.2.cmml" xref="S4.SS1.p3.8.m8.1.1.2.2"><ci id="S4.SS1.p3.8.m8.1.1.2.2.1.cmml" xref="S4.SS1.p3.8.m8.1.1.2.2.1">bold-^</ci><ci id="S4.SS1.p3.8.m8.1.1.2.2.2.cmml" xref="S4.SS1.p3.8.m8.1.1.2.2.2">𝒛</ci></apply><ci id="S4.SS1.p3.8.m8.1.1.2.3.cmml" xref="S4.SS1.p3.8.m8.1.1.2.3">𝑙</ci></apply><ci id="S4.SS1.p3.8.m8.1.1.3.cmml" xref="S4.SS1.p3.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.8.m8.1c">\bm{\hat{z}}_{l}^{i}</annotation></semantics></math> by looking up the corresponding codebook <math id="S4.SS1.p3.9.m9.1" class="ltx_Math" alttext="\mathcal{C}_{i}" display="inline"><semantics id="S4.SS1.p3.9.m9.1a"><msub id="S4.SS1.p3.9.m9.1.1" xref="S4.SS1.p3.9.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p3.9.m9.1.1.2" xref="S4.SS1.p3.9.m9.1.1.2.cmml">𝒞</mi><mi id="S4.SS1.p3.9.m9.1.1.3" xref="S4.SS1.p3.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.9.m9.1b"><apply id="S4.SS1.p3.9.m9.1.1.cmml" xref="S4.SS1.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.9.m9.1.1.1.cmml" xref="S4.SS1.p3.9.m9.1.1">subscript</csymbol><ci id="S4.SS1.p3.9.m9.1.1.2.cmml" xref="S4.SS1.p3.9.m9.1.1.2">𝒞</ci><ci id="S4.SS1.p3.9.m9.1.1.3.cmml" xref="S4.SS1.p3.9.m9.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.9.m9.1c">\mathcal{C}_{i}</annotation></semantics></math> as</p>
<table id="A2.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E2.m1.1" class="ltx_math_unparsed" alttext="\displaystyle\bm{\hat{z}}_{l}^{i}=\operatorname*{arg\,min}_{\bm{\hat{z}}^{\prime}\in\mathcal{C}_{i}}\lVert\bm{\hat{z}}^{\prime}-\bm{r}_{l}^{i}\rVert_{2}." display="inline"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1b"><msubsup id="S4.E2.m1.1.1"><mover accent="true" id="S4.E2.m1.1.1.2.2"><mi id="S4.E2.m1.1.1.2.2.2">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.E2.m1.1.1.2.2.1">^</mo></mover><mi id="S4.E2.m1.1.1.2.3">l</mi><mi id="S4.E2.m1.1.1.3">i</mi></msubsup><mo id="S4.E2.m1.1.2">=</mo><munder id="S4.E2.m1.1.3"><mrow id="S4.E2.m1.1.3.2"><mi id="S4.E2.m1.1.3.2.2">arg</mi><mo lspace="0.170em" rspace="0em" id="S4.E2.m1.1.3.2.1">​</mo><mi id="S4.E2.m1.1.3.2.3">min</mi></mrow><mrow id="S4.E2.m1.1.3.3"><msup id="S4.E2.m1.1.3.3.2"><mover accent="true" id="S4.E2.m1.1.3.3.2.2"><mi id="S4.E2.m1.1.3.3.2.2.2">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.E2.m1.1.3.3.2.2.1">^</mo></mover><mo id="S4.E2.m1.1.3.3.2.3">′</mo></msup><mo id="S4.E2.m1.1.3.3.1">∈</mo><msub id="S4.E2.m1.1.3.3.3"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.1.3.3.3.2">𝒞</mi><mi id="S4.E2.m1.1.3.3.3.3">i</mi></msub></mrow></munder><msub id="S4.E2.m1.1.4"><mrow id="S4.E2.m1.1.4.2"><mo fence="true" lspace="0em" rspace="0em" id="S4.E2.m1.1.4.2.1">∥</mo><msup id="S4.E2.m1.1.4.2.2"><mover accent="true" id="S4.E2.m1.1.4.2.2.2"><mi id="S4.E2.m1.1.4.2.2.2.2">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.E2.m1.1.4.2.2.2.1">^</mo></mover><mo id="S4.E2.m1.1.4.2.2.3">′</mo></msup><mo id="S4.E2.m1.1.4.2.3">−</mo><msubsup id="S4.E2.m1.1.4.2.4"><mi id="S4.E2.m1.1.4.2.4.2.2">𝒓</mi><mi id="S4.E2.m1.1.4.2.4.2.3">l</mi><mi id="S4.E2.m1.1.4.2.4.3">i</mi></msubsup><mo fence="true" lspace="0em" rspace="0em" id="S4.E2.m1.1.4.2.5">∥</mo></mrow><mn id="S4.E2.m1.1.4.3">2</mn></msub><mo lspace="0em" id="S4.E2.m1.1.5">.</mo></mrow><annotation encoding="application/x-tex" id="S4.E2.m1.1c">\displaystyle\bm{\hat{z}}_{l}^{i}=\operatorname*{arg\,min}_{\bm{\hat{z}}^{\prime}\in\mathcal{C}_{i}}\lVert\bm{\hat{z}}^{\prime}-\bm{r}_{l}^{i}\rVert_{2}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS1.p3.11" class="ltx_p">When <math id="S4.SS1.p3.10.m1.1" class="ltx_Math" alttext="i&lt;R" display="inline"><semantics id="S4.SS1.p3.10.m1.1a"><mrow id="S4.SS1.p3.10.m1.1.1" xref="S4.SS1.p3.10.m1.1.1.cmml"><mi id="S4.SS1.p3.10.m1.1.1.2" xref="S4.SS1.p3.10.m1.1.1.2.cmml">i</mi><mo id="S4.SS1.p3.10.m1.1.1.1" xref="S4.SS1.p3.10.m1.1.1.1.cmml">&lt;</mo><mi id="S4.SS1.p3.10.m1.1.1.3" xref="S4.SS1.p3.10.m1.1.1.3.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.10.m1.1b"><apply id="S4.SS1.p3.10.m1.1.1.cmml" xref="S4.SS1.p3.10.m1.1.1"><lt id="S4.SS1.p3.10.m1.1.1.1.cmml" xref="S4.SS1.p3.10.m1.1.1.1"></lt><ci id="S4.SS1.p3.10.m1.1.1.2.cmml" xref="S4.SS1.p3.10.m1.1.1.2">𝑖</ci><ci id="S4.SS1.p3.10.m1.1.1.3.cmml" xref="S4.SS1.p3.10.m1.1.1.3">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.10.m1.1c">i&lt;R</annotation></semantics></math>, the new residue <math id="S4.SS1.p3.11.m2.1" class="ltx_Math" alttext="\bm{r}_{l}^{i+1}" display="inline"><semantics id="S4.SS1.p3.11.m2.1a"><msubsup id="S4.SS1.p3.11.m2.1.1" xref="S4.SS1.p3.11.m2.1.1.cmml"><mi id="S4.SS1.p3.11.m2.1.1.2.2" xref="S4.SS1.p3.11.m2.1.1.2.2.cmml">𝒓</mi><mi id="S4.SS1.p3.11.m2.1.1.2.3" xref="S4.SS1.p3.11.m2.1.1.2.3.cmml">l</mi><mrow id="S4.SS1.p3.11.m2.1.1.3" xref="S4.SS1.p3.11.m2.1.1.3.cmml"><mi id="S4.SS1.p3.11.m2.1.1.3.2" xref="S4.SS1.p3.11.m2.1.1.3.2.cmml">i</mi><mo id="S4.SS1.p3.11.m2.1.1.3.1" xref="S4.SS1.p3.11.m2.1.1.3.1.cmml">+</mo><mn id="S4.SS1.p3.11.m2.1.1.3.3" xref="S4.SS1.p3.11.m2.1.1.3.3.cmml">1</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.11.m2.1b"><apply id="S4.SS1.p3.11.m2.1.1.cmml" xref="S4.SS1.p3.11.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.11.m2.1.1.1.cmml" xref="S4.SS1.p3.11.m2.1.1">superscript</csymbol><apply id="S4.SS1.p3.11.m2.1.1.2.cmml" xref="S4.SS1.p3.11.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.11.m2.1.1.2.1.cmml" xref="S4.SS1.p3.11.m2.1.1">subscript</csymbol><ci id="S4.SS1.p3.11.m2.1.1.2.2.cmml" xref="S4.SS1.p3.11.m2.1.1.2.2">𝒓</ci><ci id="S4.SS1.p3.11.m2.1.1.2.3.cmml" xref="S4.SS1.p3.11.m2.1.1.2.3">𝑙</ci></apply><apply id="S4.SS1.p3.11.m2.1.1.3.cmml" xref="S4.SS1.p3.11.m2.1.1.3"><plus id="S4.SS1.p3.11.m2.1.1.3.1.cmml" xref="S4.SS1.p3.11.m2.1.1.3.1"></plus><ci id="S4.SS1.p3.11.m2.1.1.3.2.cmml" xref="S4.SS1.p3.11.m2.1.1.3.2">𝑖</ci><cn type="integer" id="S4.SS1.p3.11.m2.1.1.3.3.cmml" xref="S4.SS1.p3.11.m2.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.11.m2.1c">\bm{r}_{l}^{i+1}</annotation></semantics></math> is calculated as</p>
<table id="A2.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E3.m1.1" class="ltx_Math" alttext="\displaystyle\bm{r}_{l}^{i+1}=\bm{r}_{l}^{i}-\bm{\hat{z}}_{l}^{i}." display="inline"><semantics id="S4.E3.m1.1a"><mrow id="S4.E3.m1.1.1.1" xref="S4.E3.m1.1.1.1.1.cmml"><mrow id="S4.E3.m1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.cmml"><msubsup id="S4.E3.m1.1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.2.cmml"><mi id="S4.E3.m1.1.1.1.1.2.2.2" xref="S4.E3.m1.1.1.1.1.2.2.2.cmml">𝒓</mi><mi id="S4.E3.m1.1.1.1.1.2.2.3" xref="S4.E3.m1.1.1.1.1.2.2.3.cmml">l</mi><mrow id="S4.E3.m1.1.1.1.1.2.3" xref="S4.E3.m1.1.1.1.1.2.3.cmml"><mi id="S4.E3.m1.1.1.1.1.2.3.2" xref="S4.E3.m1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S4.E3.m1.1.1.1.1.2.3.1" xref="S4.E3.m1.1.1.1.1.2.3.1.cmml">+</mo><mn id="S4.E3.m1.1.1.1.1.2.3.3" xref="S4.E3.m1.1.1.1.1.2.3.3.cmml">1</mn></mrow></msubsup><mo id="S4.E3.m1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.cmml">=</mo><mrow id="S4.E3.m1.1.1.1.1.3" xref="S4.E3.m1.1.1.1.1.3.cmml"><msubsup id="S4.E3.m1.1.1.1.1.3.2" xref="S4.E3.m1.1.1.1.1.3.2.cmml"><mi id="S4.E3.m1.1.1.1.1.3.2.2.2" xref="S4.E3.m1.1.1.1.1.3.2.2.2.cmml">𝒓</mi><mi id="S4.E3.m1.1.1.1.1.3.2.2.3" xref="S4.E3.m1.1.1.1.1.3.2.2.3.cmml">l</mi><mi id="S4.E3.m1.1.1.1.1.3.2.3" xref="S4.E3.m1.1.1.1.1.3.2.3.cmml">i</mi></msubsup><mo id="S4.E3.m1.1.1.1.1.3.1" xref="S4.E3.m1.1.1.1.1.3.1.cmml">−</mo><msubsup id="S4.E3.m1.1.1.1.1.3.3" xref="S4.E3.m1.1.1.1.1.3.3.cmml"><mover accent="true" id="S4.E3.m1.1.1.1.1.3.3.2.2" xref="S4.E3.m1.1.1.1.1.3.3.2.2.cmml"><mi id="S4.E3.m1.1.1.1.1.3.3.2.2.2" xref="S4.E3.m1.1.1.1.1.3.3.2.2.2.cmml">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.E3.m1.1.1.1.1.3.3.2.2.1" xref="S4.E3.m1.1.1.1.1.3.3.2.2.1.cmml">^</mo></mover><mi id="S4.E3.m1.1.1.1.1.3.3.2.3" xref="S4.E3.m1.1.1.1.1.3.3.2.3.cmml">l</mi><mi id="S4.E3.m1.1.1.1.1.3.3.3" xref="S4.E3.m1.1.1.1.1.3.3.3.cmml">i</mi></msubsup></mrow></mrow><mo lspace="0em" id="S4.E3.m1.1.1.1.2" xref="S4.E3.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.1b"><apply id="S4.E3.m1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1"><eq id="S4.E3.m1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1"></eq><apply id="S4.E3.m1.1.1.1.1.2.cmml" xref="S4.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.2.1.cmml" xref="S4.E3.m1.1.1.1.1.2">superscript</csymbol><apply id="S4.E3.m1.1.1.1.1.2.2.cmml" xref="S4.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.2.2.1.cmml" xref="S4.E3.m1.1.1.1.1.2">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.2.2.2.cmml" xref="S4.E3.m1.1.1.1.1.2.2.2">𝒓</ci><ci id="S4.E3.m1.1.1.1.1.2.2.3.cmml" xref="S4.E3.m1.1.1.1.1.2.2.3">𝑙</ci></apply><apply id="S4.E3.m1.1.1.1.1.2.3.cmml" xref="S4.E3.m1.1.1.1.1.2.3"><plus id="S4.E3.m1.1.1.1.1.2.3.1.cmml" xref="S4.E3.m1.1.1.1.1.2.3.1"></plus><ci id="S4.E3.m1.1.1.1.1.2.3.2.cmml" xref="S4.E3.m1.1.1.1.1.2.3.2">𝑖</ci><cn type="integer" id="S4.E3.m1.1.1.1.1.2.3.3.cmml" xref="S4.E3.m1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S4.E3.m1.1.1.1.1.3.cmml" xref="S4.E3.m1.1.1.1.1.3"><minus id="S4.E3.m1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.1.1.1.1.3.1"></minus><apply id="S4.E3.m1.1.1.1.1.3.2.cmml" xref="S4.E3.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.3.2.1.cmml" xref="S4.E3.m1.1.1.1.1.3.2">superscript</csymbol><apply id="S4.E3.m1.1.1.1.1.3.2.2.cmml" xref="S4.E3.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.3.2.2.1.cmml" xref="S4.E3.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S4.E3.m1.1.1.1.1.3.2.2.2.cmml" xref="S4.E3.m1.1.1.1.1.3.2.2.2">𝒓</ci><ci id="S4.E3.m1.1.1.1.1.3.2.2.3.cmml" xref="S4.E3.m1.1.1.1.1.3.2.2.3">𝑙</ci></apply><ci id="S4.E3.m1.1.1.1.1.3.2.3.cmml" xref="S4.E3.m1.1.1.1.1.3.2.3">𝑖</ci></apply><apply id="S4.E3.m1.1.1.1.1.3.3.cmml" xref="S4.E3.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.3.3.1.cmml" xref="S4.E3.m1.1.1.1.1.3.3">superscript</csymbol><apply id="S4.E3.m1.1.1.1.1.3.3.2.cmml" xref="S4.E3.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.1.3.3.2.1.cmml" xref="S4.E3.m1.1.1.1.1.3.3">subscript</csymbol><apply id="S4.E3.m1.1.1.1.1.3.3.2.2.cmml" xref="S4.E3.m1.1.1.1.1.3.3.2.2"><ci id="S4.E3.m1.1.1.1.1.3.3.2.2.1.cmml" xref="S4.E3.m1.1.1.1.1.3.3.2.2.1">bold-^</ci><ci id="S4.E3.m1.1.1.1.1.3.3.2.2.2.cmml" xref="S4.E3.m1.1.1.1.1.3.3.2.2.2">𝒛</ci></apply><ci id="S4.E3.m1.1.1.1.1.3.3.2.3.cmml" xref="S4.E3.m1.1.1.1.1.3.3.2.3">𝑙</ci></apply><ci id="S4.E3.m1.1.1.1.1.3.3.3.cmml" xref="S4.E3.m1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.1c">\displaystyle\bm{r}_{l}^{i+1}=\bm{r}_{l}^{i}-\bm{\hat{z}}_{l}^{i}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS1.p3.15" class="ltx_p">This cycle continues sequentially for R times and finally the input motion feature <math id="S4.SS1.p3.12.m1.1" class="ltx_Math" alttext="\bm{z}_{l}" display="inline"><semantics id="S4.SS1.p3.12.m1.1a"><msub id="S4.SS1.p3.12.m1.1.1" xref="S4.SS1.p3.12.m1.1.1.cmml"><mi id="S4.SS1.p3.12.m1.1.1.2" xref="S4.SS1.p3.12.m1.1.1.2.cmml">𝒛</mi><mi id="S4.SS1.p3.12.m1.1.1.3" xref="S4.SS1.p3.12.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.12.m1.1b"><apply id="S4.SS1.p3.12.m1.1.1.cmml" xref="S4.SS1.p3.12.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.12.m1.1.1.1.cmml" xref="S4.SS1.p3.12.m1.1.1">subscript</csymbol><ci id="S4.SS1.p3.12.m1.1.1.2.cmml" xref="S4.SS1.p3.12.m1.1.1.2">𝒛</ci><ci id="S4.SS1.p3.12.m1.1.1.3.cmml" xref="S4.SS1.p3.12.m1.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.12.m1.1c">\bm{z}_{l}</annotation></semantics></math> is quantized into a hierarchical discrete feature sequence
<math id="S4.SS1.p3.13.m2.1" class="ltx_Math" alttext="[\bm{\hat{z}}_{l}^{i}]_{i=1}^{R}" display="inline"><semantics id="S4.SS1.p3.13.m2.1a"><msubsup id="S4.SS1.p3.13.m2.1.1" xref="S4.SS1.p3.13.m2.1.1.cmml"><mrow id="S4.SS1.p3.13.m2.1.1.1.1.1" xref="S4.SS1.p3.13.m2.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS1.p3.13.m2.1.1.1.1.1.2" xref="S4.SS1.p3.13.m2.1.1.1.1.2.1.cmml">[</mo><msubsup id="S4.SS1.p3.13.m2.1.1.1.1.1.1" xref="S4.SS1.p3.13.m2.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.SS1.p3.13.m2.1.1.1.1.1.1.2.2" xref="S4.SS1.p3.13.m2.1.1.1.1.1.1.2.2.cmml"><mi id="S4.SS1.p3.13.m2.1.1.1.1.1.1.2.2.2" xref="S4.SS1.p3.13.m2.1.1.1.1.1.1.2.2.2.cmml">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.SS1.p3.13.m2.1.1.1.1.1.1.2.2.1" xref="S4.SS1.p3.13.m2.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mi id="S4.SS1.p3.13.m2.1.1.1.1.1.1.2.3" xref="S4.SS1.p3.13.m2.1.1.1.1.1.1.2.3.cmml">l</mi><mi id="S4.SS1.p3.13.m2.1.1.1.1.1.1.3" xref="S4.SS1.p3.13.m2.1.1.1.1.1.1.3.cmml">i</mi></msubsup><mo stretchy="false" id="S4.SS1.p3.13.m2.1.1.1.1.1.3" xref="S4.SS1.p3.13.m2.1.1.1.1.2.1.cmml">]</mo></mrow><mrow id="S4.SS1.p3.13.m2.1.1.1.3" xref="S4.SS1.p3.13.m2.1.1.1.3.cmml"><mi id="S4.SS1.p3.13.m2.1.1.1.3.2" xref="S4.SS1.p3.13.m2.1.1.1.3.2.cmml">i</mi><mo id="S4.SS1.p3.13.m2.1.1.1.3.1" xref="S4.SS1.p3.13.m2.1.1.1.3.1.cmml">=</mo><mn id="S4.SS1.p3.13.m2.1.1.1.3.3" xref="S4.SS1.p3.13.m2.1.1.1.3.3.cmml">1</mn></mrow><mi id="S4.SS1.p3.13.m2.1.1.3" xref="S4.SS1.p3.13.m2.1.1.3.cmml">R</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.13.m2.1b"><apply id="S4.SS1.p3.13.m2.1.1.cmml" xref="S4.SS1.p3.13.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.13.m2.1.1.2.cmml" xref="S4.SS1.p3.13.m2.1.1">superscript</csymbol><apply id="S4.SS1.p3.13.m2.1.1.1.cmml" xref="S4.SS1.p3.13.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.13.m2.1.1.1.2.cmml" xref="S4.SS1.p3.13.m2.1.1">subscript</csymbol><apply id="S4.SS1.p3.13.m2.1.1.1.1.2.cmml" xref="S4.SS1.p3.13.m2.1.1.1.1.1"><csymbol cd="latexml" id="S4.SS1.p3.13.m2.1.1.1.1.2.1.cmml" xref="S4.SS1.p3.13.m2.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S4.SS1.p3.13.m2.1.1.1.1.1.1.cmml" xref="S4.SS1.p3.13.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.13.m2.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p3.13.m2.1.1.1.1.1.1">superscript</csymbol><apply id="S4.SS1.p3.13.m2.1.1.1.1.1.1.2.cmml" xref="S4.SS1.p3.13.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.13.m2.1.1.1.1.1.1.2.1.cmml" xref="S4.SS1.p3.13.m2.1.1.1.1.1.1">subscript</csymbol><apply id="S4.SS1.p3.13.m2.1.1.1.1.1.1.2.2.cmml" xref="S4.SS1.p3.13.m2.1.1.1.1.1.1.2.2"><ci id="S4.SS1.p3.13.m2.1.1.1.1.1.1.2.2.1.cmml" xref="S4.SS1.p3.13.m2.1.1.1.1.1.1.2.2.1">bold-^</ci><ci id="S4.SS1.p3.13.m2.1.1.1.1.1.1.2.2.2.cmml" xref="S4.SS1.p3.13.m2.1.1.1.1.1.1.2.2.2">𝒛</ci></apply><ci id="S4.SS1.p3.13.m2.1.1.1.1.1.1.2.3.cmml" xref="S4.SS1.p3.13.m2.1.1.1.1.1.1.2.3">𝑙</ci></apply><ci id="S4.SS1.p3.13.m2.1.1.1.1.1.1.3.cmml" xref="S4.SS1.p3.13.m2.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S4.SS1.p3.13.m2.1.1.1.3.cmml" xref="S4.SS1.p3.13.m2.1.1.1.3"><eq id="S4.SS1.p3.13.m2.1.1.1.3.1.cmml" xref="S4.SS1.p3.13.m2.1.1.1.3.1"></eq><ci id="S4.SS1.p3.13.m2.1.1.1.3.2.cmml" xref="S4.SS1.p3.13.m2.1.1.1.3.2">𝑖</ci><cn type="integer" id="S4.SS1.p3.13.m2.1.1.1.3.3.cmml" xref="S4.SS1.p3.13.m2.1.1.1.3.3">1</cn></apply></apply><ci id="S4.SS1.p3.13.m2.1.1.3.cmml" xref="S4.SS1.p3.13.m2.1.1.3">𝑅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.13.m2.1c">[\bm{\hat{z}}_{l}^{i}]_{i=1}^{R}</annotation></semantics></math>. Notably, the first layer (RVQ-<math id="S4.SS1.p3.14.m3.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS1.p3.14.m3.1a"><mn id="S4.SS1.p3.14.m3.1.1" xref="S4.SS1.p3.14.m3.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.14.m3.1b"><cn type="integer" id="S4.SS1.p3.14.m3.1.1.cmml" xref="S4.SS1.p3.14.m3.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.14.m3.1c">1</annotation></semantics></math>) actually functions as the standard VQ layer. We treat it as a special RVQ layer, where the preceding residue is <math id="S4.SS1.p3.15.m4.1" class="ltx_Math" alttext="\bm{r}_{l}^{1}=\bm{z}_{l}" display="inline"><semantics id="S4.SS1.p3.15.m4.1a"><mrow id="S4.SS1.p3.15.m4.1.1" xref="S4.SS1.p3.15.m4.1.1.cmml"><msubsup id="S4.SS1.p3.15.m4.1.1.2" xref="S4.SS1.p3.15.m4.1.1.2.cmml"><mi id="S4.SS1.p3.15.m4.1.1.2.2.2" xref="S4.SS1.p3.15.m4.1.1.2.2.2.cmml">𝒓</mi><mi id="S4.SS1.p3.15.m4.1.1.2.2.3" xref="S4.SS1.p3.15.m4.1.1.2.2.3.cmml">l</mi><mn id="S4.SS1.p3.15.m4.1.1.2.3" xref="S4.SS1.p3.15.m4.1.1.2.3.cmml">1</mn></msubsup><mo id="S4.SS1.p3.15.m4.1.1.1" xref="S4.SS1.p3.15.m4.1.1.1.cmml">=</mo><msub id="S4.SS1.p3.15.m4.1.1.3" xref="S4.SS1.p3.15.m4.1.1.3.cmml"><mi id="S4.SS1.p3.15.m4.1.1.3.2" xref="S4.SS1.p3.15.m4.1.1.3.2.cmml">𝒛</mi><mi id="S4.SS1.p3.15.m4.1.1.3.3" xref="S4.SS1.p3.15.m4.1.1.3.3.cmml">l</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.15.m4.1b"><apply id="S4.SS1.p3.15.m4.1.1.cmml" xref="S4.SS1.p3.15.m4.1.1"><eq id="S4.SS1.p3.15.m4.1.1.1.cmml" xref="S4.SS1.p3.15.m4.1.1.1"></eq><apply id="S4.SS1.p3.15.m4.1.1.2.cmml" xref="S4.SS1.p3.15.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p3.15.m4.1.1.2.1.cmml" xref="S4.SS1.p3.15.m4.1.1.2">superscript</csymbol><apply id="S4.SS1.p3.15.m4.1.1.2.2.cmml" xref="S4.SS1.p3.15.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p3.15.m4.1.1.2.2.1.cmml" xref="S4.SS1.p3.15.m4.1.1.2">subscript</csymbol><ci id="S4.SS1.p3.15.m4.1.1.2.2.2.cmml" xref="S4.SS1.p3.15.m4.1.1.2.2.2">𝒓</ci><ci id="S4.SS1.p3.15.m4.1.1.2.2.3.cmml" xref="S4.SS1.p3.15.m4.1.1.2.2.3">𝑙</ci></apply><cn type="integer" id="S4.SS1.p3.15.m4.1.1.2.3.cmml" xref="S4.SS1.p3.15.m4.1.1.2.3">1</cn></apply><apply id="S4.SS1.p3.15.m4.1.1.3.cmml" xref="S4.SS1.p3.15.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p3.15.m4.1.1.3.1.cmml" xref="S4.SS1.p3.15.m4.1.1.3">subscript</csymbol><ci id="S4.SS1.p3.15.m4.1.1.3.2.cmml" xref="S4.SS1.p3.15.m4.1.1.3.2">𝒛</ci><ci id="S4.SS1.p3.15.m4.1.1.3.3.cmml" xref="S4.SS1.p3.15.m4.1.1.3.3">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.15.m4.1c">\bm{r}_{l}^{1}=\bm{z}_{l}</annotation></semantics></math>. As the number of layers increases, the Residual VQ-VAE demonstrates an exponential expansion in its capacity <cite class="ltx_cite ltx_citemacro_citep">(Yao
et al<span class="ltx_text">.</span>, <a href="#bib.bib92" title="" class="ltx_ref">2023</a>)</cite>. This enhancement significantly boosts the model’s capability for expression.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Finally, we compute the reconstructed motion <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="\bm{M}^{*}" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><msup id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mi id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml">𝑴</mi><mo id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1">superscript</csymbol><ci id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2">𝑴</ci><times id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">\bm{M}^{*}</annotation></semantics></math> as</p>
<table id="A2.EGx4" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E4.m1.2" class="ltx_Math" alttext="\displaystyle\bm{M}^{*}=\mathcal{D}_{{\text{VQ}}}(\bm{\hat{Z}})." display="inline"><semantics id="S4.E4.m1.2a"><mrow id="S4.E4.m1.2.2.1" xref="S4.E4.m1.2.2.1.1.cmml"><mrow id="S4.E4.m1.2.2.1.1" xref="S4.E4.m1.2.2.1.1.cmml"><msup id="S4.E4.m1.2.2.1.1.2" xref="S4.E4.m1.2.2.1.1.2.cmml"><mi id="S4.E4.m1.2.2.1.1.2.2" xref="S4.E4.m1.2.2.1.1.2.2.cmml">𝑴</mi><mo id="S4.E4.m1.2.2.1.1.2.3" xref="S4.E4.m1.2.2.1.1.2.3.cmml">∗</mo></msup><mo id="S4.E4.m1.2.2.1.1.1" xref="S4.E4.m1.2.2.1.1.1.cmml">=</mo><mrow id="S4.E4.m1.2.2.1.1.3" xref="S4.E4.m1.2.2.1.1.3.cmml"><msub id="S4.E4.m1.2.2.1.1.3.2" xref="S4.E4.m1.2.2.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E4.m1.2.2.1.1.3.2.2" xref="S4.E4.m1.2.2.1.1.3.2.2.cmml">𝒟</mi><mtext id="S4.E4.m1.2.2.1.1.3.2.3" xref="S4.E4.m1.2.2.1.1.3.2.3a.cmml">VQ</mtext></msub><mo lspace="0em" rspace="0em" id="S4.E4.m1.2.2.1.1.3.1" xref="S4.E4.m1.2.2.1.1.3.1.cmml">​</mo><mrow id="S4.E4.m1.2.2.1.1.3.3.2" xref="S4.E4.m1.1.1.cmml"><mo stretchy="false" id="S4.E4.m1.2.2.1.1.3.3.2.1" xref="S4.E4.m1.1.1.cmml">(</mo><mover accent="true" id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml"><mi id="S4.E4.m1.1.1.2" xref="S4.E4.m1.1.1.2.cmml">𝒁</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.E4.m1.1.1.1" xref="S4.E4.m1.1.1.1.cmml">^</mo></mover><mo stretchy="false" id="S4.E4.m1.2.2.1.1.3.3.2.2" xref="S4.E4.m1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S4.E4.m1.2.2.1.2" xref="S4.E4.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.2b"><apply id="S4.E4.m1.2.2.1.1.cmml" xref="S4.E4.m1.2.2.1"><eq id="S4.E4.m1.2.2.1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.1"></eq><apply id="S4.E4.m1.2.2.1.1.2.cmml" xref="S4.E4.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S4.E4.m1.2.2.1.1.2.1.cmml" xref="S4.E4.m1.2.2.1.1.2">superscript</csymbol><ci id="S4.E4.m1.2.2.1.1.2.2.cmml" xref="S4.E4.m1.2.2.1.1.2.2">𝑴</ci><times id="S4.E4.m1.2.2.1.1.2.3.cmml" xref="S4.E4.m1.2.2.1.1.2.3"></times></apply><apply id="S4.E4.m1.2.2.1.1.3.cmml" xref="S4.E4.m1.2.2.1.1.3"><times id="S4.E4.m1.2.2.1.1.3.1.cmml" xref="S4.E4.m1.2.2.1.1.3.1"></times><apply id="S4.E4.m1.2.2.1.1.3.2.cmml" xref="S4.E4.m1.2.2.1.1.3.2"><csymbol cd="ambiguous" id="S4.E4.m1.2.2.1.1.3.2.1.cmml" xref="S4.E4.m1.2.2.1.1.3.2">subscript</csymbol><ci id="S4.E4.m1.2.2.1.1.3.2.2.cmml" xref="S4.E4.m1.2.2.1.1.3.2.2">𝒟</ci><ci id="S4.E4.m1.2.2.1.1.3.2.3a.cmml" xref="S4.E4.m1.2.2.1.1.3.2.3"><mtext mathsize="70%" id="S4.E4.m1.2.2.1.1.3.2.3.cmml" xref="S4.E4.m1.2.2.1.1.3.2.3">VQ</mtext></ci></apply><apply id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.2.2.1.1.3.3.2"><ci id="S4.E4.m1.1.1.1.cmml" xref="S4.E4.m1.1.1.1">bold-^</ci><ci id="S4.E4.m1.1.1.2.cmml" xref="S4.E4.m1.1.1.2">𝒁</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.2c">\displaystyle\bm{M}^{*}=\mathcal{D}_{{\text{VQ}}}(\bm{\hat{Z}}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS1.p4.7" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_citep">(van den Oord et al<span class="ltx_text">.</span>, <a href="#bib.bib79" title="" class="ltx_ref">2017</a>)</cite>, the loss function is defined as</p>
<table id="A2.EGx5" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.Ex1.m1.1" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{{\text{RVQ}}}" display="inline"><semantics id="S4.Ex1.m1.1a"><msub id="S4.Ex1.m1.1.1" xref="S4.Ex1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.Ex1.m1.1.1.2" xref="S4.Ex1.m1.1.1.2.cmml">ℒ</mi><mtext id="S4.Ex1.m1.1.1.3" xref="S4.Ex1.m1.1.1.3a.cmml">RVQ</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.Ex1.m1.1b"><apply id="S4.Ex1.m1.1.1.cmml" xref="S4.Ex1.m1.1.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.1.1.1.cmml" xref="S4.Ex1.m1.1.1">subscript</csymbol><ci id="S4.Ex1.m1.1.1.2.cmml" xref="S4.Ex1.m1.1.1.2">ℒ</ci><ci id="S4.Ex1.m1.1.1.3a.cmml" xref="S4.Ex1.m1.1.1.3"><mtext mathsize="70%" id="S4.Ex1.m1.1.1.3.cmml" xref="S4.Ex1.m1.1.1.3">RVQ</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m1.1c">\displaystyle\mathcal{L}_{{\text{RVQ}}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.Ex1.m2.3" class="ltx_Math" alttext="\displaystyle=w_{1}\lVert\bm{M}-\bm{M}^{*}\rVert_{1}+w_{2}\lVert\bm{\dot{M}}-\bm{\dot{M}}^{*}\rVert_{1}+w_{2}\lVert\bm{\ddot{M}}-\bm{\ddot{M}}^{*}\rVert_{1}" display="inline"><semantics id="S4.Ex1.m2.3a"><mrow id="S4.Ex1.m2.3.3" xref="S4.Ex1.m2.3.3.cmml"><mi id="S4.Ex1.m2.3.3.5" xref="S4.Ex1.m2.3.3.5.cmml"></mi><mo id="S4.Ex1.m2.3.3.4" xref="S4.Ex1.m2.3.3.4.cmml">=</mo><mrow id="S4.Ex1.m2.3.3.3" xref="S4.Ex1.m2.3.3.3.cmml"><mrow id="S4.Ex1.m2.1.1.1.1" xref="S4.Ex1.m2.1.1.1.1.cmml"><msub id="S4.Ex1.m2.1.1.1.1.3" xref="S4.Ex1.m2.1.1.1.1.3.cmml"><mi id="S4.Ex1.m2.1.1.1.1.3.2" xref="S4.Ex1.m2.1.1.1.1.3.2.cmml">w</mi><mn id="S4.Ex1.m2.1.1.1.1.3.3" xref="S4.Ex1.m2.1.1.1.1.3.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S4.Ex1.m2.1.1.1.1.2" xref="S4.Ex1.m2.1.1.1.1.2.cmml">​</mo><msub id="S4.Ex1.m2.1.1.1.1.1" xref="S4.Ex1.m2.1.1.1.1.1.cmml"><mrow id="S4.Ex1.m2.1.1.1.1.1.1.1" xref="S4.Ex1.m2.1.1.1.1.1.1.2.cmml"><mo fence="true" rspace="0em" id="S4.Ex1.m2.1.1.1.1.1.1.1.2" xref="S4.Ex1.m2.1.1.1.1.1.1.2.1.cmml">∥</mo><mrow id="S4.Ex1.m2.1.1.1.1.1.1.1.1" xref="S4.Ex1.m2.1.1.1.1.1.1.1.1.cmml"><mi id="S4.Ex1.m2.1.1.1.1.1.1.1.1.2" xref="S4.Ex1.m2.1.1.1.1.1.1.1.1.2.cmml">𝑴</mi><mo id="S4.Ex1.m2.1.1.1.1.1.1.1.1.1" xref="S4.Ex1.m2.1.1.1.1.1.1.1.1.1.cmml">−</mo><msup id="S4.Ex1.m2.1.1.1.1.1.1.1.1.3" xref="S4.Ex1.m2.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.Ex1.m2.1.1.1.1.1.1.1.1.3.2" xref="S4.Ex1.m2.1.1.1.1.1.1.1.1.3.2.cmml">𝑴</mi><mo id="S4.Ex1.m2.1.1.1.1.1.1.1.1.3.3" xref="S4.Ex1.m2.1.1.1.1.1.1.1.1.3.3.cmml">∗</mo></msup></mrow><mo fence="true" lspace="0em" rspace="0em" id="S4.Ex1.m2.1.1.1.1.1.1.1.3" xref="S4.Ex1.m2.1.1.1.1.1.1.2.1.cmml">∥</mo></mrow><mn id="S4.Ex1.m2.1.1.1.1.1.3" xref="S4.Ex1.m2.1.1.1.1.1.3.cmml">1</mn></msub></mrow><mo id="S4.Ex1.m2.3.3.3.4" xref="S4.Ex1.m2.3.3.3.4.cmml">+</mo><mrow id="S4.Ex1.m2.2.2.2.2" xref="S4.Ex1.m2.2.2.2.2.cmml"><msub id="S4.Ex1.m2.2.2.2.2.3" xref="S4.Ex1.m2.2.2.2.2.3.cmml"><mi id="S4.Ex1.m2.2.2.2.2.3.2" xref="S4.Ex1.m2.2.2.2.2.3.2.cmml">w</mi><mn id="S4.Ex1.m2.2.2.2.2.3.3" xref="S4.Ex1.m2.2.2.2.2.3.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S4.Ex1.m2.2.2.2.2.2" xref="S4.Ex1.m2.2.2.2.2.2.cmml">​</mo><msub id="S4.Ex1.m2.2.2.2.2.1" xref="S4.Ex1.m2.2.2.2.2.1.cmml"><mrow id="S4.Ex1.m2.2.2.2.2.1.1.1" xref="S4.Ex1.m2.2.2.2.2.1.1.2.cmml"><mo fence="true" rspace="0em" id="S4.Ex1.m2.2.2.2.2.1.1.1.2" xref="S4.Ex1.m2.2.2.2.2.1.1.2.1.cmml">∥</mo><mrow id="S4.Ex1.m2.2.2.2.2.1.1.1.1" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.cmml"><mover accent="true" id="S4.Ex1.m2.2.2.2.2.1.1.1.1.2" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.2.cmml"><mi id="S4.Ex1.m2.2.2.2.2.1.1.1.1.2.2" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.2.2.cmml">𝑴</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.Ex1.m2.2.2.2.2.1.1.1.1.2.1" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.2.1.cmml">˙</mo></mover><mo id="S4.Ex1.m2.2.2.2.2.1.1.1.1.1" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.1.cmml">−</mo><msup id="S4.Ex1.m2.2.2.2.2.1.1.1.1.3" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.3.cmml"><mover accent="true" id="S4.Ex1.m2.2.2.2.2.1.1.1.1.3.2" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.3.2.cmml"><mi id="S4.Ex1.m2.2.2.2.2.1.1.1.1.3.2.2" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.3.2.2.cmml">𝑴</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.Ex1.m2.2.2.2.2.1.1.1.1.3.2.1" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.3.2.1.cmml">˙</mo></mover><mo id="S4.Ex1.m2.2.2.2.2.1.1.1.1.3.3" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.3.3.cmml">∗</mo></msup></mrow><mo fence="true" lspace="0em" rspace="0em" id="S4.Ex1.m2.2.2.2.2.1.1.1.3" xref="S4.Ex1.m2.2.2.2.2.1.1.2.1.cmml">∥</mo></mrow><mn id="S4.Ex1.m2.2.2.2.2.1.3" xref="S4.Ex1.m2.2.2.2.2.1.3.cmml">1</mn></msub></mrow><mo id="S4.Ex1.m2.3.3.3.4a" xref="S4.Ex1.m2.3.3.3.4.cmml">+</mo><mrow id="S4.Ex1.m2.3.3.3.3" xref="S4.Ex1.m2.3.3.3.3.cmml"><msub id="S4.Ex1.m2.3.3.3.3.3" xref="S4.Ex1.m2.3.3.3.3.3.cmml"><mi id="S4.Ex1.m2.3.3.3.3.3.2" xref="S4.Ex1.m2.3.3.3.3.3.2.cmml">w</mi><mn id="S4.Ex1.m2.3.3.3.3.3.3" xref="S4.Ex1.m2.3.3.3.3.3.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S4.Ex1.m2.3.3.3.3.2" xref="S4.Ex1.m2.3.3.3.3.2.cmml">​</mo><msub id="S4.Ex1.m2.3.3.3.3.1" xref="S4.Ex1.m2.3.3.3.3.1.cmml"><mrow id="S4.Ex1.m2.3.3.3.3.1.1.1" xref="S4.Ex1.m2.3.3.3.3.1.1.2.cmml"><mo fence="true" rspace="0em" id="S4.Ex1.m2.3.3.3.3.1.1.1.2" xref="S4.Ex1.m2.3.3.3.3.1.1.2.1.cmml">∥</mo><mrow id="S4.Ex1.m2.3.3.3.3.1.1.1.1" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.cmml"><mover accent="true" id="S4.Ex1.m2.3.3.3.3.1.1.1.1.2" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.2.cmml"><mi id="S4.Ex1.m2.3.3.3.3.1.1.1.1.2.2" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.2.2.cmml">𝑴</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.Ex1.m2.3.3.3.3.1.1.1.1.2.1" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.2.1.cmml">¨</mo></mover><mo id="S4.Ex1.m2.3.3.3.3.1.1.1.1.1" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.1.cmml">−</mo><msup id="S4.Ex1.m2.3.3.3.3.1.1.1.1.3" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.3.cmml"><mover accent="true" id="S4.Ex1.m2.3.3.3.3.1.1.1.1.3.2" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.3.2.cmml"><mi id="S4.Ex1.m2.3.3.3.3.1.1.1.1.3.2.2" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.3.2.2.cmml">𝑴</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.Ex1.m2.3.3.3.3.1.1.1.1.3.2.1" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.3.2.1.cmml">¨</mo></mover><mo id="S4.Ex1.m2.3.3.3.3.1.1.1.1.3.3" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.3.3.cmml">∗</mo></msup></mrow><mo fence="true" lspace="0em" id="S4.Ex1.m2.3.3.3.3.1.1.1.3" xref="S4.Ex1.m2.3.3.3.3.1.1.2.1.cmml">∥</mo></mrow><mn id="S4.Ex1.m2.3.3.3.3.1.3" xref="S4.Ex1.m2.3.3.3.3.1.3.cmml">1</mn></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m2.3b"><apply id="S4.Ex1.m2.3.3.cmml" xref="S4.Ex1.m2.3.3"><eq id="S4.Ex1.m2.3.3.4.cmml" xref="S4.Ex1.m2.3.3.4"></eq><csymbol cd="latexml" id="S4.Ex1.m2.3.3.5.cmml" xref="S4.Ex1.m2.3.3.5">absent</csymbol><apply id="S4.Ex1.m2.3.3.3.cmml" xref="S4.Ex1.m2.3.3.3"><plus id="S4.Ex1.m2.3.3.3.4.cmml" xref="S4.Ex1.m2.3.3.3.4"></plus><apply id="S4.Ex1.m2.1.1.1.1.cmml" xref="S4.Ex1.m2.1.1.1.1"><times id="S4.Ex1.m2.1.1.1.1.2.cmml" xref="S4.Ex1.m2.1.1.1.1.2"></times><apply id="S4.Ex1.m2.1.1.1.1.3.cmml" xref="S4.Ex1.m2.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.Ex1.m2.1.1.1.1.3.1.cmml" xref="S4.Ex1.m2.1.1.1.1.3">subscript</csymbol><ci id="S4.Ex1.m2.1.1.1.1.3.2.cmml" xref="S4.Ex1.m2.1.1.1.1.3.2">𝑤</ci><cn type="integer" id="S4.Ex1.m2.1.1.1.1.3.3.cmml" xref="S4.Ex1.m2.1.1.1.1.3.3">1</cn></apply><apply id="S4.Ex1.m2.1.1.1.1.1.cmml" xref="S4.Ex1.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex1.m2.1.1.1.1.1.2.cmml" xref="S4.Ex1.m2.1.1.1.1.1">subscript</csymbol><apply id="S4.Ex1.m2.1.1.1.1.1.1.2.cmml" xref="S4.Ex1.m2.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.Ex1.m2.1.1.1.1.1.1.2.1.cmml" xref="S4.Ex1.m2.1.1.1.1.1.1.1.2">delimited-∥∥</csymbol><apply id="S4.Ex1.m2.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex1.m2.1.1.1.1.1.1.1.1"><minus id="S4.Ex1.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex1.m2.1.1.1.1.1.1.1.1.1"></minus><ci id="S4.Ex1.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex1.m2.1.1.1.1.1.1.1.1.2">𝑴</ci><apply id="S4.Ex1.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S4.Ex1.m2.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.Ex1.m2.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.Ex1.m2.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S4.Ex1.m2.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.Ex1.m2.1.1.1.1.1.1.1.1.3.2">𝑴</ci><times id="S4.Ex1.m2.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.Ex1.m2.1.1.1.1.1.1.1.1.3.3"></times></apply></apply></apply><cn type="integer" id="S4.Ex1.m2.1.1.1.1.1.3.cmml" xref="S4.Ex1.m2.1.1.1.1.1.3">1</cn></apply></apply><apply id="S4.Ex1.m2.2.2.2.2.cmml" xref="S4.Ex1.m2.2.2.2.2"><times id="S4.Ex1.m2.2.2.2.2.2.cmml" xref="S4.Ex1.m2.2.2.2.2.2"></times><apply id="S4.Ex1.m2.2.2.2.2.3.cmml" xref="S4.Ex1.m2.2.2.2.2.3"><csymbol cd="ambiguous" id="S4.Ex1.m2.2.2.2.2.3.1.cmml" xref="S4.Ex1.m2.2.2.2.2.3">subscript</csymbol><ci id="S4.Ex1.m2.2.2.2.2.3.2.cmml" xref="S4.Ex1.m2.2.2.2.2.3.2">𝑤</ci><cn type="integer" id="S4.Ex1.m2.2.2.2.2.3.3.cmml" xref="S4.Ex1.m2.2.2.2.2.3.3">2</cn></apply><apply id="S4.Ex1.m2.2.2.2.2.1.cmml" xref="S4.Ex1.m2.2.2.2.2.1"><csymbol cd="ambiguous" id="S4.Ex1.m2.2.2.2.2.1.2.cmml" xref="S4.Ex1.m2.2.2.2.2.1">subscript</csymbol><apply id="S4.Ex1.m2.2.2.2.2.1.1.2.cmml" xref="S4.Ex1.m2.2.2.2.2.1.1.1"><csymbol cd="latexml" id="S4.Ex1.m2.2.2.2.2.1.1.2.1.cmml" xref="S4.Ex1.m2.2.2.2.2.1.1.1.2">delimited-∥∥</csymbol><apply id="S4.Ex1.m2.2.2.2.2.1.1.1.1.cmml" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1"><minus id="S4.Ex1.m2.2.2.2.2.1.1.1.1.1.cmml" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.1"></minus><apply id="S4.Ex1.m2.2.2.2.2.1.1.1.1.2.cmml" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.2"><ci id="S4.Ex1.m2.2.2.2.2.1.1.1.1.2.1.cmml" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.2.1">bold-˙</ci><ci id="S4.Ex1.m2.2.2.2.2.1.1.1.1.2.2.cmml" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.2.2">𝑴</ci></apply><apply id="S4.Ex1.m2.2.2.2.2.1.1.1.1.3.cmml" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.Ex1.m2.2.2.2.2.1.1.1.1.3.1.cmml" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.3">superscript</csymbol><apply id="S4.Ex1.m2.2.2.2.2.1.1.1.1.3.2.cmml" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.3.2"><ci id="S4.Ex1.m2.2.2.2.2.1.1.1.1.3.2.1.cmml" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.3.2.1">bold-˙</ci><ci id="S4.Ex1.m2.2.2.2.2.1.1.1.1.3.2.2.cmml" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.3.2.2">𝑴</ci></apply><times id="S4.Ex1.m2.2.2.2.2.1.1.1.1.3.3.cmml" xref="S4.Ex1.m2.2.2.2.2.1.1.1.1.3.3"></times></apply></apply></apply><cn type="integer" id="S4.Ex1.m2.2.2.2.2.1.3.cmml" xref="S4.Ex1.m2.2.2.2.2.1.3">1</cn></apply></apply><apply id="S4.Ex1.m2.3.3.3.3.cmml" xref="S4.Ex1.m2.3.3.3.3"><times id="S4.Ex1.m2.3.3.3.3.2.cmml" xref="S4.Ex1.m2.3.3.3.3.2"></times><apply id="S4.Ex1.m2.3.3.3.3.3.cmml" xref="S4.Ex1.m2.3.3.3.3.3"><csymbol cd="ambiguous" id="S4.Ex1.m2.3.3.3.3.3.1.cmml" xref="S4.Ex1.m2.3.3.3.3.3">subscript</csymbol><ci id="S4.Ex1.m2.3.3.3.3.3.2.cmml" xref="S4.Ex1.m2.3.3.3.3.3.2">𝑤</ci><cn type="integer" id="S4.Ex1.m2.3.3.3.3.3.3.cmml" xref="S4.Ex1.m2.3.3.3.3.3.3">2</cn></apply><apply id="S4.Ex1.m2.3.3.3.3.1.cmml" xref="S4.Ex1.m2.3.3.3.3.1"><csymbol cd="ambiguous" id="S4.Ex1.m2.3.3.3.3.1.2.cmml" xref="S4.Ex1.m2.3.3.3.3.1">subscript</csymbol><apply id="S4.Ex1.m2.3.3.3.3.1.1.2.cmml" xref="S4.Ex1.m2.3.3.3.3.1.1.1"><csymbol cd="latexml" id="S4.Ex1.m2.3.3.3.3.1.1.2.1.cmml" xref="S4.Ex1.m2.3.3.3.3.1.1.1.2">delimited-∥∥</csymbol><apply id="S4.Ex1.m2.3.3.3.3.1.1.1.1.cmml" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1"><minus id="S4.Ex1.m2.3.3.3.3.1.1.1.1.1.cmml" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.1"></minus><apply id="S4.Ex1.m2.3.3.3.3.1.1.1.1.2.cmml" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.2"><ci id="S4.Ex1.m2.3.3.3.3.1.1.1.1.2.1.cmml" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.2.1">bold-¨</ci><ci id="S4.Ex1.m2.3.3.3.3.1.1.1.1.2.2.cmml" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.2.2">𝑴</ci></apply><apply id="S4.Ex1.m2.3.3.3.3.1.1.1.1.3.cmml" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.Ex1.m2.3.3.3.3.1.1.1.1.3.1.cmml" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.3">superscript</csymbol><apply id="S4.Ex1.m2.3.3.3.3.1.1.1.1.3.2.cmml" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.3.2"><ci id="S4.Ex1.m2.3.3.3.3.1.1.1.1.3.2.1.cmml" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.3.2.1">bold-¨</ci><ci id="S4.Ex1.m2.3.3.3.3.1.1.1.1.3.2.2.cmml" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.3.2.2">𝑴</ci></apply><times id="S4.Ex1.m2.3.3.3.3.1.1.1.1.3.3.cmml" xref="S4.Ex1.m2.3.3.3.3.1.1.1.1.3.3"></times></apply></apply></apply><cn type="integer" id="S4.Ex1.m2.3.3.3.3.1.3.cmml" xref="S4.Ex1.m2.3.3.3.3.1.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m2.3c">\displaystyle=w_{1}\lVert\bm{M}-\bm{M}^{*}\rVert_{1}+w_{2}\lVert\bm{\dot{M}}-\bm{\dot{M}}^{*}\rVert_{1}+w_{2}\lVert\bm{\ddot{M}}-\bm{\ddot{M}}^{*}\rVert_{1}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S4.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.Ex2.m1.3" class="ltx_Math" alttext="\displaystyle+w_{3}\lVert\mathcal{E}_{{\text{VQ}}}(\bm{M})-\operatorname*{sg}([\sum_{i=1}^{R}{\bm{\hat{z}}_{l}^{i}}]_{l=1}^{L})\rVert_{2}^{2}" display="inline"><semantics id="S4.Ex2.m1.3a"><mrow id="S4.Ex2.m1.3.3" xref="S4.Ex2.m1.3.3.cmml"><mo id="S4.Ex2.m1.3.3a" xref="S4.Ex2.m1.3.3.cmml">+</mo><mrow id="S4.Ex2.m1.3.3.1" xref="S4.Ex2.m1.3.3.1.cmml"><msub id="S4.Ex2.m1.3.3.1.3" xref="S4.Ex2.m1.3.3.1.3.cmml"><mi id="S4.Ex2.m1.3.3.1.3.2" xref="S4.Ex2.m1.3.3.1.3.2.cmml">w</mi><mn id="S4.Ex2.m1.3.3.1.3.3" xref="S4.Ex2.m1.3.3.1.3.3.cmml">3</mn></msub><mo lspace="0em" rspace="0em" id="S4.Ex2.m1.3.3.1.2" xref="S4.Ex2.m1.3.3.1.2.cmml">​</mo><msubsup id="S4.Ex2.m1.3.3.1.1" xref="S4.Ex2.m1.3.3.1.1.cmml"><mrow id="S4.Ex2.m1.3.3.1.1.1.1.1" xref="S4.Ex2.m1.3.3.1.1.1.1.2.cmml"><mo fence="true" rspace="0em" id="S4.Ex2.m1.3.3.1.1.1.1.1.2" xref="S4.Ex2.m1.3.3.1.1.1.1.2.1.cmml">∥</mo><mrow id="S4.Ex2.m1.3.3.1.1.1.1.1.1" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.cmml"><mrow id="S4.Ex2.m1.3.3.1.1.1.1.1.1.3" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.cmml"><msub id="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.2" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.2.2" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.2.2.cmml">ℰ</mi><mtext id="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.2.3" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.2.3a.cmml">VQ</mtext></msub><mo lspace="0em" rspace="0em" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.1" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.1.cmml">​</mo><mrow id="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.3.2" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.3.2.1" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.cmml">(</mo><mi id="S4.Ex2.m1.1.1" xref="S4.Ex2.m1.1.1.cmml">𝑴</mi><mo stretchy="false" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.3.2.2" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.cmml">)</mo></mrow></mrow><mo id="S4.Ex2.m1.3.3.1.1.1.1.1.1.2" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.2.cmml">−</mo><mrow id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.2.cmml"><mo lspace="0em" rspace="0em" id="S4.Ex2.m1.2.2" xref="S4.Ex2.m1.2.2.cmml">sg</mo><mrow id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.2.cmml">(</mo><msubsup id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">[</mo><mrow id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><munderover id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1a" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo movablelimits="false" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">∑</mo><mrow id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml">=</mo><mn id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow><mi id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">R</mi></munderover></mstyle><msubsup id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mover accent="true" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml">^</mo></mover><mi id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">l</mi><mi id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msubsup></mrow><mo stretchy="false" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">]</mo></mrow><mrow id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">l</mi><mo id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">=</mo><mn id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml">L</mi></msubsup><mo stretchy="false" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo fence="true" lspace="0em" id="S4.Ex2.m1.3.3.1.1.1.1.1.3" xref="S4.Ex2.m1.3.3.1.1.1.1.2.1.cmml">∥</mo></mrow><mn id="S4.Ex2.m1.3.3.1.1.1.3" xref="S4.Ex2.m1.3.3.1.1.1.3.cmml">2</mn><mn id="S4.Ex2.m1.3.3.1.1.3" xref="S4.Ex2.m1.3.3.1.1.3.cmml">2</mn></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex2.m1.3b"><apply id="S4.Ex2.m1.3.3.cmml" xref="S4.Ex2.m1.3.3"><plus id="S4.Ex2.m1.3.3.2.cmml" xref="S4.Ex2.m1.3.3"></plus><apply id="S4.Ex2.m1.3.3.1.cmml" xref="S4.Ex2.m1.3.3.1"><times id="S4.Ex2.m1.3.3.1.2.cmml" xref="S4.Ex2.m1.3.3.1.2"></times><apply id="S4.Ex2.m1.3.3.1.3.cmml" xref="S4.Ex2.m1.3.3.1.3"><csymbol cd="ambiguous" id="S4.Ex2.m1.3.3.1.3.1.cmml" xref="S4.Ex2.m1.3.3.1.3">subscript</csymbol><ci id="S4.Ex2.m1.3.3.1.3.2.cmml" xref="S4.Ex2.m1.3.3.1.3.2">𝑤</ci><cn type="integer" id="S4.Ex2.m1.3.3.1.3.3.cmml" xref="S4.Ex2.m1.3.3.1.3.3">3</cn></apply><apply id="S4.Ex2.m1.3.3.1.1.cmml" xref="S4.Ex2.m1.3.3.1.1"><csymbol cd="ambiguous" id="S4.Ex2.m1.3.3.1.1.2.cmml" xref="S4.Ex2.m1.3.3.1.1">superscript</csymbol><apply id="S4.Ex2.m1.3.3.1.1.1.cmml" xref="S4.Ex2.m1.3.3.1.1"><csymbol cd="ambiguous" id="S4.Ex2.m1.3.3.1.1.1.2.cmml" xref="S4.Ex2.m1.3.3.1.1">subscript</csymbol><apply id="S4.Ex2.m1.3.3.1.1.1.1.2.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S4.Ex2.m1.3.3.1.1.1.1.2.1.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.2">delimited-∥∥</csymbol><apply id="S4.Ex2.m1.3.3.1.1.1.1.1.1.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1"><minus id="S4.Ex2.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.2"></minus><apply id="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.3"><times id="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.1"></times><apply id="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.2.1.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.2.2.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.2.2">ℰ</ci><ci id="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.2.3a.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.2.3"><mtext mathsize="70%" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.2.3.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.3.2.3">VQ</mtext></ci></apply><ci id="S4.Ex2.m1.1.1.cmml" xref="S4.Ex2.m1.1.1">𝑴</ci></apply><apply id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1"><ci id="S4.Ex2.m1.2.2.cmml" xref="S4.Ex2.m1.2.2">sg</ci><apply id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><apply id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><sum id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2"></sum><apply id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3"><eq id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1"></eq><ci id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2">𝑖</ci><cn type="integer" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><ci id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑅</ci></apply><apply id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><apply id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2"><ci id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1">bold-^</ci><ci id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2">𝒛</ci></apply><ci id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3">𝑙</ci></apply><ci id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply></apply></apply><apply id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3"><eq id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1"></eq><ci id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑙</ci><cn type="integer" id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply><ci id="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.Ex2.m1.3.3.1.1.1.1.1.1.1.1.1.1.3">𝐿</ci></apply></apply></apply></apply><cn type="integer" id="S4.Ex2.m1.3.3.1.1.1.3.cmml" xref="S4.Ex2.m1.3.3.1.1.1.3">2</cn></apply><cn type="integer" id="S4.Ex2.m1.3.3.1.1.3.cmml" xref="S4.Ex2.m1.3.3.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex2.m1.3c">\displaystyle+w_{3}\lVert\mathcal{E}_{{\text{VQ}}}(\bm{M})-\operatorname*{sg}([\sum_{i=1}^{R}{\bm{\hat{z}}_{l}^{i}}]_{l=1}^{L})\rVert_{2}^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S4.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E5.m1.3" class="ltx_Math" alttext="\displaystyle+w_{4}\lVert\operatorname*{sg}(\mathcal{E}_{{\text{VQ}}}(\bm{M}))-{[\sum_{i=1}^{R}{\bm{\hat{z}}_{l}^{i}}]_{l=1}^{L}}\rVert_{2}^{2}," display="inline"><semantics id="S4.E5.m1.3a"><mrow id="S4.E5.m1.3.3.1" xref="S4.E5.m1.3.3.1.1.cmml"><mrow id="S4.E5.m1.3.3.1.1" xref="S4.E5.m1.3.3.1.1.cmml"><mo id="S4.E5.m1.3.3.1.1a" xref="S4.E5.m1.3.3.1.1.cmml">+</mo><mrow id="S4.E5.m1.3.3.1.1.1" xref="S4.E5.m1.3.3.1.1.1.cmml"><msub id="S4.E5.m1.3.3.1.1.1.3" xref="S4.E5.m1.3.3.1.1.1.3.cmml"><mi id="S4.E5.m1.3.3.1.1.1.3.2" xref="S4.E5.m1.3.3.1.1.1.3.2.cmml">w</mi><mn id="S4.E5.m1.3.3.1.1.1.3.3" xref="S4.E5.m1.3.3.1.1.1.3.3.cmml">4</mn></msub><mo lspace="0em" rspace="0em" id="S4.E5.m1.3.3.1.1.1.2" xref="S4.E5.m1.3.3.1.1.1.2.cmml">​</mo><msubsup id="S4.E5.m1.3.3.1.1.1.1" xref="S4.E5.m1.3.3.1.1.1.1.cmml"><mrow id="S4.E5.m1.3.3.1.1.1.1.1.1.1" xref="S4.E5.m1.3.3.1.1.1.1.1.1.2.cmml"><mo fence="true" rspace="0em" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.2" xref="S4.E5.m1.3.3.1.1.1.1.1.1.2.1.cmml">∥</mo><mrow id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mrow id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml"><mo lspace="0em" rspace="0em" id="S4.E5.m1.2.2" xref="S4.E5.m1.2.2.cmml">sg</mo><mrow id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><mrow id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">ℰ</mi><mtext id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.3a.cmml">VQ</mtext></msub><mo lspace="0em" rspace="0em" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.1" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S4.E5.m1.1.1" xref="S4.E5.m1.1.1.cmml">𝑴</mi><mo stretchy="false" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.3.cmml">−</mo><msubsup id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.cmml"><mrow id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.2.cmml"><mo stretchy="false" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.2" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.2.1.cmml">[</mo><mrow id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml"><mstyle displaystyle="true" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.cmml"><munderover id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1a" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.cmml"><mo movablelimits="false" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.2" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.2.cmml">∑</mo><mrow id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.3" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.3.cmml"><mi id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.3.2" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.3.1" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.3.1.cmml">=</mo><mn id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.3.3" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.3.3.cmml">1</mn></mrow><mi id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.3" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.3.cmml">R</mi></munderover></mstyle><msubsup id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.cmml"><mover accent="true" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.2" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.2.cmml"><mi id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.2.2" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.2.2.cmml">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.2.1" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.2.1.cmml">^</mo></mover><mi id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.3" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.3.cmml">l</mi><mi id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3.cmml">i</mi></msubsup></mrow><mo stretchy="false" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.3" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.2.1.cmml">]</mo></mrow><mrow id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.3" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.3.cmml"><mi id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.3.2" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.3.2.cmml">l</mi><mo id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.3.1" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.3.1.cmml">=</mo><mn id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.3.3" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.3.3.cmml">1</mn></mrow><mi id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.3" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.3.cmml">L</mi></msubsup></mrow><mo fence="true" lspace="0em" rspace="0em" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.3" xref="S4.E5.m1.3.3.1.1.1.1.1.1.2.1.cmml">∥</mo></mrow><mn id="S4.E5.m1.3.3.1.1.1.1.1.3" xref="S4.E5.m1.3.3.1.1.1.1.1.3.cmml">2</mn><mn id="S4.E5.m1.3.3.1.1.1.1.3" xref="S4.E5.m1.3.3.1.1.1.1.3.cmml">2</mn></msubsup></mrow></mrow><mo id="S4.E5.m1.3.3.1.2" xref="S4.E5.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E5.m1.3b"><apply id="S4.E5.m1.3.3.1.1.cmml" xref="S4.E5.m1.3.3.1"><plus id="S4.E5.m1.3.3.1.1.2.cmml" xref="S4.E5.m1.3.3.1"></plus><apply id="S4.E5.m1.3.3.1.1.1.cmml" xref="S4.E5.m1.3.3.1.1.1"><times id="S4.E5.m1.3.3.1.1.1.2.cmml" xref="S4.E5.m1.3.3.1.1.1.2"></times><apply id="S4.E5.m1.3.3.1.1.1.3.cmml" xref="S4.E5.m1.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S4.E5.m1.3.3.1.1.1.3.1.cmml" xref="S4.E5.m1.3.3.1.1.1.3">subscript</csymbol><ci id="S4.E5.m1.3.3.1.1.1.3.2.cmml" xref="S4.E5.m1.3.3.1.1.1.3.2">𝑤</ci><cn type="integer" id="S4.E5.m1.3.3.1.1.1.3.3.cmml" xref="S4.E5.m1.3.3.1.1.1.3.3">4</cn></apply><apply id="S4.E5.m1.3.3.1.1.1.1.cmml" xref="S4.E5.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S4.E5.m1.3.3.1.1.1.1.2.cmml" xref="S4.E5.m1.3.3.1.1.1.1">superscript</csymbol><apply id="S4.E5.m1.3.3.1.1.1.1.1.cmml" xref="S4.E5.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S4.E5.m1.3.3.1.1.1.1.1.2.cmml" xref="S4.E5.m1.3.3.1.1.1.1">subscript</csymbol><apply id="S4.E5.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E5.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.2">delimited-∥∥</csymbol><apply id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1"><minus id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.3"></minus><apply id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1"><ci id="S4.E5.m1.2.2.cmml" xref="S4.E5.m1.2.2">sg</ci><apply id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1"><times id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1"></times><apply id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.2">ℰ</ci><ci id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.3a.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.3"><mtext mathsize="70%" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.3">VQ</mtext></ci></apply><ci id="S4.E5.m1.1.1.cmml" xref="S4.E5.m1.1.1">𝑴</ci></apply></apply><apply id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.2.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2">subscript</csymbol><apply id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.2.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1"><csymbol cd="latexml" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.2.1.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.2">delimited-[]</csymbol><apply id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1"><apply id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.1.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1">superscript</csymbol><apply id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.1.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1">subscript</csymbol><sum id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.2.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.2"></sum><apply id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.3.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.3"><eq id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.3.1.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.3.1"></eq><ci id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.3.2.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.3.2">𝑖</ci><cn type="integer" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.3.3.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.2.3.3">1</cn></apply></apply><ci id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.3.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.1.3">𝑅</ci></apply><apply id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.1.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2">superscript</csymbol><apply id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.1.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2">subscript</csymbol><apply id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.2.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.2"><ci id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.2.1.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.2.1">bold-^</ci><ci id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.2.2.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.2.2">𝒛</ci></apply><ci id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.3.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.2.3">𝑙</ci></apply><ci id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.1.1.1.2.3">𝑖</ci></apply></apply></apply><apply id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.3.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.3"><eq id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.3.1.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.3.1"></eq><ci id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.3.2.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.3.2">𝑙</ci><cn type="integer" id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.3.3.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.1.3.3">1</cn></apply></apply><ci id="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.1.1.1.2.3">𝐿</ci></apply></apply></apply><cn type="integer" id="S4.E5.m1.3.3.1.1.1.1.1.3.cmml" xref="S4.E5.m1.3.3.1.1.1.1.1.3">2</cn></apply><cn type="integer" id="S4.E5.m1.3.3.1.1.1.1.3.cmml" xref="S4.E5.m1.3.3.1.1.1.1.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.3c">\displaystyle+w_{4}\lVert\operatorname*{sg}(\mathcal{E}_{{\text{VQ}}}(\bm{M}))-{[\sum_{i=1}^{R}{\bm{\hat{z}}_{l}^{i}}]_{l=1}^{L}}\rVert_{2}^{2},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS1.p4.6" class="ltx_p">where
<math id="S4.SS1.p4.2.m1.1" class="ltx_Math" alttext="\bm{\dot{M}}" display="inline"><semantics id="S4.SS1.p4.2.m1.1a"><mover accent="true" id="S4.SS1.p4.2.m1.1.1" xref="S4.SS1.p4.2.m1.1.1.cmml"><mi id="S4.SS1.p4.2.m1.1.1.2" xref="S4.SS1.p4.2.m1.1.1.2.cmml">𝑴</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.SS1.p4.2.m1.1.1.1" xref="S4.SS1.p4.2.m1.1.1.1.cmml">˙</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m1.1b"><apply id="S4.SS1.p4.2.m1.1.1.cmml" xref="S4.SS1.p4.2.m1.1.1"><ci id="S4.SS1.p4.2.m1.1.1.1.cmml" xref="S4.SS1.p4.2.m1.1.1.1">bold-˙</ci><ci id="S4.SS1.p4.2.m1.1.1.2.cmml" xref="S4.SS1.p4.2.m1.1.1.2">𝑴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m1.1c">\bm{\dot{M}}</annotation></semantics></math> and <math id="S4.SS1.p4.3.m2.1" class="ltx_Math" alttext="\bm{\ddot{M}}" display="inline"><semantics id="S4.SS1.p4.3.m2.1a"><mover accent="true" id="S4.SS1.p4.3.m2.1.1" xref="S4.SS1.p4.3.m2.1.1.cmml"><mi id="S4.SS1.p4.3.m2.1.1.2" xref="S4.SS1.p4.3.m2.1.1.2.cmml">𝑴</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.SS1.p4.3.m2.1.1.1" xref="S4.SS1.p4.3.m2.1.1.1.cmml">¨</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m2.1b"><apply id="S4.SS1.p4.3.m2.1.1.cmml" xref="S4.SS1.p4.3.m2.1.1"><ci id="S4.SS1.p4.3.m2.1.1.1.cmml" xref="S4.SS1.p4.3.m2.1.1.1">bold-¨</ci><ci id="S4.SS1.p4.3.m2.1.1.2.cmml" xref="S4.SS1.p4.3.m2.1.1.2">𝑴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m2.1c">\bm{\ddot{M}}</annotation></semantics></math> represent the first-order (velocity) and second-order (acceleration) derivatives of <math id="S4.SS1.p4.4.m3.1" class="ltx_Math" alttext="\bm{M}" display="inline"><semantics id="S4.SS1.p4.4.m3.1a"><mi id="S4.SS1.p4.4.m3.1.1" xref="S4.SS1.p4.4.m3.1.1.cmml">𝑴</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.4.m3.1b"><ci id="S4.SS1.p4.4.m3.1.1.cmml" xref="S4.SS1.p4.4.m3.1.1">𝑴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.4.m3.1c">\bm{M}</annotation></semantics></math> on time. <math id="S4.SS1.p4.5.m4.1" class="ltx_Math" alttext="\operatorname*{sg}" display="inline"><semantics id="S4.SS1.p4.5.m4.1a"><mo id="S4.SS1.p4.5.m4.1.1" xref="S4.SS1.p4.5.m4.1.1.cmml">sg</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.5.m4.1b"><ci id="S4.SS1.p4.5.m4.1.1.cmml" xref="S4.SS1.p4.5.m4.1.1">sg</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.5.m4.1c">\operatorname*{sg}</annotation></semantics></math> stands for the <em id="S4.SS1.p4.6.1" class="ltx_emph ltx_font_italic">stop gradient</em> operator that prevents the gradient from backpropagating through it. And <math id="S4.SS1.p4.6.m5.1" class="ltx_Math" alttext="[w_{i}]_{i=1}^{4}" display="inline"><semantics id="S4.SS1.p4.6.m5.1a"><msubsup id="S4.SS1.p4.6.m5.1.1" xref="S4.SS1.p4.6.m5.1.1.cmml"><mrow id="S4.SS1.p4.6.m5.1.1.1.1.1" xref="S4.SS1.p4.6.m5.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS1.p4.6.m5.1.1.1.1.1.2" xref="S4.SS1.p4.6.m5.1.1.1.1.2.1.cmml">[</mo><msub id="S4.SS1.p4.6.m5.1.1.1.1.1.1" xref="S4.SS1.p4.6.m5.1.1.1.1.1.1.cmml"><mi id="S4.SS1.p4.6.m5.1.1.1.1.1.1.2" xref="S4.SS1.p4.6.m5.1.1.1.1.1.1.2.cmml">w</mi><mi id="S4.SS1.p4.6.m5.1.1.1.1.1.1.3" xref="S4.SS1.p4.6.m5.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.SS1.p4.6.m5.1.1.1.1.1.3" xref="S4.SS1.p4.6.m5.1.1.1.1.2.1.cmml">]</mo></mrow><mrow id="S4.SS1.p4.6.m5.1.1.1.3" xref="S4.SS1.p4.6.m5.1.1.1.3.cmml"><mi id="S4.SS1.p4.6.m5.1.1.1.3.2" xref="S4.SS1.p4.6.m5.1.1.1.3.2.cmml">i</mi><mo id="S4.SS1.p4.6.m5.1.1.1.3.1" xref="S4.SS1.p4.6.m5.1.1.1.3.1.cmml">=</mo><mn id="S4.SS1.p4.6.m5.1.1.1.3.3" xref="S4.SS1.p4.6.m5.1.1.1.3.3.cmml">1</mn></mrow><mn id="S4.SS1.p4.6.m5.1.1.3" xref="S4.SS1.p4.6.m5.1.1.3.cmml">4</mn></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.6.m5.1b"><apply id="S4.SS1.p4.6.m5.1.1.cmml" xref="S4.SS1.p4.6.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.6.m5.1.1.2.cmml" xref="S4.SS1.p4.6.m5.1.1">superscript</csymbol><apply id="S4.SS1.p4.6.m5.1.1.1.cmml" xref="S4.SS1.p4.6.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.6.m5.1.1.1.2.cmml" xref="S4.SS1.p4.6.m5.1.1">subscript</csymbol><apply id="S4.SS1.p4.6.m5.1.1.1.1.2.cmml" xref="S4.SS1.p4.6.m5.1.1.1.1.1"><csymbol cd="latexml" id="S4.SS1.p4.6.m5.1.1.1.1.2.1.cmml" xref="S4.SS1.p4.6.m5.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S4.SS1.p4.6.m5.1.1.1.1.1.1.cmml" xref="S4.SS1.p4.6.m5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p4.6.m5.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p4.6.m5.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p4.6.m5.1.1.1.1.1.1.2.cmml" xref="S4.SS1.p4.6.m5.1.1.1.1.1.1.2">𝑤</ci><ci id="S4.SS1.p4.6.m5.1.1.1.1.1.1.3.cmml" xref="S4.SS1.p4.6.m5.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S4.SS1.p4.6.m5.1.1.1.3.cmml" xref="S4.SS1.p4.6.m5.1.1.1.3"><eq id="S4.SS1.p4.6.m5.1.1.1.3.1.cmml" xref="S4.SS1.p4.6.m5.1.1.1.3.1"></eq><ci id="S4.SS1.p4.6.m5.1.1.1.3.2.cmml" xref="S4.SS1.p4.6.m5.1.1.1.3.2">𝑖</ci><cn type="integer" id="S4.SS1.p4.6.m5.1.1.1.3.3.cmml" xref="S4.SS1.p4.6.m5.1.1.1.3.3">1</cn></apply></apply><cn type="integer" id="S4.SS1.p4.6.m5.1.1.3.cmml" xref="S4.SS1.p4.6.m5.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.6.m5.1c">[w_{i}]_{i=1}^{4}</annotation></semantics></math> corresponds to the weighted factors.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Gesture Generator</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.12" class="ltx_p">As shown in Figure <a href="#S4.F3" title="Figure 3 ‣ 4. Co-Speech Gesture GPT Model ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (b), the gesture generator <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\mathcal{G}</annotation></semantics></math> is based on the GPT-2 <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib72" title="" class="ltx_ref">2019</a>)</cite>, which predicts the future gesture tokens (<math id="S4.SS2.p1.2.m2.2" class="ltx_Math" alttext="\bm{\hat{z}}_{{\text{hand}},L+1}^{*}" display="inline"><semantics id="S4.SS2.p1.2.m2.2a"><msubsup id="S4.SS2.p1.2.m2.2.3" xref="S4.SS2.p1.2.m2.2.3.cmml"><mover accent="true" id="S4.SS2.p1.2.m2.2.3.2.2" xref="S4.SS2.p1.2.m2.2.3.2.2.cmml"><mi id="S4.SS2.p1.2.m2.2.3.2.2.2" xref="S4.SS2.p1.2.m2.2.3.2.2.2.cmml">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.SS2.p1.2.m2.2.3.2.2.1" xref="S4.SS2.p1.2.m2.2.3.2.2.1.cmml">^</mo></mover><mrow id="S4.SS2.p1.2.m2.2.2.2.2" xref="S4.SS2.p1.2.m2.2.2.2.3.cmml"><mtext id="S4.SS2.p1.2.m2.1.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.1a.cmml">hand</mtext><mo id="S4.SS2.p1.2.m2.2.2.2.2.2" xref="S4.SS2.p1.2.m2.2.2.2.3.cmml">,</mo><mrow id="S4.SS2.p1.2.m2.2.2.2.2.1" xref="S4.SS2.p1.2.m2.2.2.2.2.1.cmml"><mi id="S4.SS2.p1.2.m2.2.2.2.2.1.2" xref="S4.SS2.p1.2.m2.2.2.2.2.1.2.cmml">L</mi><mo id="S4.SS2.p1.2.m2.2.2.2.2.1.1" xref="S4.SS2.p1.2.m2.2.2.2.2.1.1.cmml">+</mo><mn id="S4.SS2.p1.2.m2.2.2.2.2.1.3" xref="S4.SS2.p1.2.m2.2.2.2.2.1.3.cmml">1</mn></mrow></mrow><mo id="S4.SS2.p1.2.m2.2.3.3" xref="S4.SS2.p1.2.m2.2.3.3.cmml">∗</mo></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.2b"><apply id="S4.SS2.p1.2.m2.2.3.cmml" xref="S4.SS2.p1.2.m2.2.3"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.2.3.1.cmml" xref="S4.SS2.p1.2.m2.2.3">superscript</csymbol><apply id="S4.SS2.p1.2.m2.2.3.2.cmml" xref="S4.SS2.p1.2.m2.2.3"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.2.3.2.1.cmml" xref="S4.SS2.p1.2.m2.2.3">subscript</csymbol><apply id="S4.SS2.p1.2.m2.2.3.2.2.cmml" xref="S4.SS2.p1.2.m2.2.3.2.2"><ci id="S4.SS2.p1.2.m2.2.3.2.2.1.cmml" xref="S4.SS2.p1.2.m2.2.3.2.2.1">bold-^</ci><ci id="S4.SS2.p1.2.m2.2.3.2.2.2.cmml" xref="S4.SS2.p1.2.m2.2.3.2.2.2">𝒛</ci></apply><list id="S4.SS2.p1.2.m2.2.2.2.3.cmml" xref="S4.SS2.p1.2.m2.2.2.2.2"><ci id="S4.SS2.p1.2.m2.1.1.1.1a.cmml" xref="S4.SS2.p1.2.m2.1.1.1.1"><mtext mathsize="70%" id="S4.SS2.p1.2.m2.1.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1.1">hand</mtext></ci><apply id="S4.SS2.p1.2.m2.2.2.2.2.1.cmml" xref="S4.SS2.p1.2.m2.2.2.2.2.1"><plus id="S4.SS2.p1.2.m2.2.2.2.2.1.1.cmml" xref="S4.SS2.p1.2.m2.2.2.2.2.1.1"></plus><ci id="S4.SS2.p1.2.m2.2.2.2.2.1.2.cmml" xref="S4.SS2.p1.2.m2.2.2.2.2.1.2">𝐿</ci><cn type="integer" id="S4.SS2.p1.2.m2.2.2.2.2.1.3.cmml" xref="S4.SS2.p1.2.m2.2.2.2.2.1.3">1</cn></apply></list></apply><times id="S4.SS2.p1.2.m2.2.3.3.cmml" xref="S4.SS2.p1.2.m2.2.3.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.2c">\bm{\hat{z}}_{{\text{hand}},L+1}^{*}</annotation></semantics></math>, <math id="S4.SS2.p1.3.m3.2" class="ltx_Math" alttext="\bm{\hat{z}}_{{\text{body}},L+1}^{*}" display="inline"><semantics id="S4.SS2.p1.3.m3.2a"><msubsup id="S4.SS2.p1.3.m3.2.3" xref="S4.SS2.p1.3.m3.2.3.cmml"><mover accent="true" id="S4.SS2.p1.3.m3.2.3.2.2" xref="S4.SS2.p1.3.m3.2.3.2.2.cmml"><mi id="S4.SS2.p1.3.m3.2.3.2.2.2" xref="S4.SS2.p1.3.m3.2.3.2.2.2.cmml">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.SS2.p1.3.m3.2.3.2.2.1" xref="S4.SS2.p1.3.m3.2.3.2.2.1.cmml">^</mo></mover><mrow id="S4.SS2.p1.3.m3.2.2.2.2" xref="S4.SS2.p1.3.m3.2.2.2.3.cmml"><mtext id="S4.SS2.p1.3.m3.1.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.1a.cmml">body</mtext><mo id="S4.SS2.p1.3.m3.2.2.2.2.2" xref="S4.SS2.p1.3.m3.2.2.2.3.cmml">,</mo><mrow id="S4.SS2.p1.3.m3.2.2.2.2.1" xref="S4.SS2.p1.3.m3.2.2.2.2.1.cmml"><mi id="S4.SS2.p1.3.m3.2.2.2.2.1.2" xref="S4.SS2.p1.3.m3.2.2.2.2.1.2.cmml">L</mi><mo id="S4.SS2.p1.3.m3.2.2.2.2.1.1" xref="S4.SS2.p1.3.m3.2.2.2.2.1.1.cmml">+</mo><mn id="S4.SS2.p1.3.m3.2.2.2.2.1.3" xref="S4.SS2.p1.3.m3.2.2.2.2.1.3.cmml">1</mn></mrow></mrow><mo id="S4.SS2.p1.3.m3.2.3.3" xref="S4.SS2.p1.3.m3.2.3.3.cmml">∗</mo></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.2b"><apply id="S4.SS2.p1.3.m3.2.3.cmml" xref="S4.SS2.p1.3.m3.2.3"><csymbol cd="ambiguous" id="S4.SS2.p1.3.m3.2.3.1.cmml" xref="S4.SS2.p1.3.m3.2.3">superscript</csymbol><apply id="S4.SS2.p1.3.m3.2.3.2.cmml" xref="S4.SS2.p1.3.m3.2.3"><csymbol cd="ambiguous" id="S4.SS2.p1.3.m3.2.3.2.1.cmml" xref="S4.SS2.p1.3.m3.2.3">subscript</csymbol><apply id="S4.SS2.p1.3.m3.2.3.2.2.cmml" xref="S4.SS2.p1.3.m3.2.3.2.2"><ci id="S4.SS2.p1.3.m3.2.3.2.2.1.cmml" xref="S4.SS2.p1.3.m3.2.3.2.2.1">bold-^</ci><ci id="S4.SS2.p1.3.m3.2.3.2.2.2.cmml" xref="S4.SS2.p1.3.m3.2.3.2.2.2">𝒛</ci></apply><list id="S4.SS2.p1.3.m3.2.2.2.3.cmml" xref="S4.SS2.p1.3.m3.2.2.2.2"><ci id="S4.SS2.p1.3.m3.1.1.1.1a.cmml" xref="S4.SS2.p1.3.m3.1.1.1.1"><mtext mathsize="70%" id="S4.SS2.p1.3.m3.1.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1.1">body</mtext></ci><apply id="S4.SS2.p1.3.m3.2.2.2.2.1.cmml" xref="S4.SS2.p1.3.m3.2.2.2.2.1"><plus id="S4.SS2.p1.3.m3.2.2.2.2.1.1.cmml" xref="S4.SS2.p1.3.m3.2.2.2.2.1.1"></plus><ci id="S4.SS2.p1.3.m3.2.2.2.2.1.2.cmml" xref="S4.SS2.p1.3.m3.2.2.2.2.1.2">𝐿</ci><cn type="integer" id="S4.SS2.p1.3.m3.2.2.2.2.1.3.cmml" xref="S4.SS2.p1.3.m3.2.2.2.2.1.3">1</cn></apply></list></apply><times id="S4.SS2.p1.3.m3.2.3.3.cmml" xref="S4.SS2.p1.3.m3.2.3.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.2c">\bm{\hat{z}}_{{\text{body}},L+1}^{*}</annotation></semantics></math>) in an autoregressive manner conditioned on preceding motion tokens
(<math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="\bm{\hat{Z}}_{{\text{hand}}}" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><msub id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mover accent="true" id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2.cmml"><mi id="S4.SS2.p1.4.m4.1.1.2.2" xref="S4.SS2.p1.4.m4.1.1.2.2.cmml">𝒁</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.SS2.p1.4.m4.1.1.2.1" xref="S4.SS2.p1.4.m4.1.1.2.1.cmml">^</mo></mover><mtext id="S4.SS2.p1.4.m4.1.1.3" xref="S4.SS2.p1.4.m4.1.1.3a.cmml">hand</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">subscript</csymbol><apply id="S4.SS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2"><ci id="S4.SS2.p1.4.m4.1.1.2.1.cmml" xref="S4.SS2.p1.4.m4.1.1.2.1">bold-^</ci><ci id="S4.SS2.p1.4.m4.1.1.2.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2.2">𝒁</ci></apply><ci id="S4.SS2.p1.4.m4.1.1.3a.cmml" xref="S4.SS2.p1.4.m4.1.1.3"><mtext mathsize="70%" id="S4.SS2.p1.4.m4.1.1.3.cmml" xref="S4.SS2.p1.4.m4.1.1.3">hand</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">\bm{\hat{Z}}_{{\text{hand}}}</annotation></semantics></math> <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="=" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><mo id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><eq id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">=</annotation></semantics></math> <math id="S4.SS2.p1.6.m6.3" class="ltx_Math" alttext="[\bm{\hat{z}}_{{\text{hand}},l}]_{l=1}^{L}" display="inline"><semantics id="S4.SS2.p1.6.m6.3a"><msubsup id="S4.SS2.p1.6.m6.3.3" xref="S4.SS2.p1.6.m6.3.3.cmml"><mrow id="S4.SS2.p1.6.m6.3.3.1.1.1" xref="S4.SS2.p1.6.m6.3.3.1.1.2.cmml"><mo stretchy="false" id="S4.SS2.p1.6.m6.3.3.1.1.1.2" xref="S4.SS2.p1.6.m6.3.3.1.1.2.1.cmml">[</mo><msub id="S4.SS2.p1.6.m6.3.3.1.1.1.1" xref="S4.SS2.p1.6.m6.3.3.1.1.1.1.cmml"><mover accent="true" id="S4.SS2.p1.6.m6.3.3.1.1.1.1.2" xref="S4.SS2.p1.6.m6.3.3.1.1.1.1.2.cmml"><mi id="S4.SS2.p1.6.m6.3.3.1.1.1.1.2.2" xref="S4.SS2.p1.6.m6.3.3.1.1.1.1.2.2.cmml">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.SS2.p1.6.m6.3.3.1.1.1.1.2.1" xref="S4.SS2.p1.6.m6.3.3.1.1.1.1.2.1.cmml">^</mo></mover><mrow id="S4.SS2.p1.6.m6.2.2.2.4" xref="S4.SS2.p1.6.m6.2.2.2.3.cmml"><mtext id="S4.SS2.p1.6.m6.1.1.1.1" xref="S4.SS2.p1.6.m6.1.1.1.1a.cmml">hand</mtext><mo id="S4.SS2.p1.6.m6.2.2.2.4.1" xref="S4.SS2.p1.6.m6.2.2.2.3.cmml">,</mo><mi id="S4.SS2.p1.6.m6.2.2.2.2" xref="S4.SS2.p1.6.m6.2.2.2.2.cmml">l</mi></mrow></msub><mo stretchy="false" id="S4.SS2.p1.6.m6.3.3.1.1.1.3" xref="S4.SS2.p1.6.m6.3.3.1.1.2.1.cmml">]</mo></mrow><mrow id="S4.SS2.p1.6.m6.3.3.1.3" xref="S4.SS2.p1.6.m6.3.3.1.3.cmml"><mi id="S4.SS2.p1.6.m6.3.3.1.3.2" xref="S4.SS2.p1.6.m6.3.3.1.3.2.cmml">l</mi><mo id="S4.SS2.p1.6.m6.3.3.1.3.1" xref="S4.SS2.p1.6.m6.3.3.1.3.1.cmml">=</mo><mn id="S4.SS2.p1.6.m6.3.3.1.3.3" xref="S4.SS2.p1.6.m6.3.3.1.3.3.cmml">1</mn></mrow><mi id="S4.SS2.p1.6.m6.3.3.3" xref="S4.SS2.p1.6.m6.3.3.3.cmml">L</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.3b"><apply id="S4.SS2.p1.6.m6.3.3.cmml" xref="S4.SS2.p1.6.m6.3.3"><csymbol cd="ambiguous" id="S4.SS2.p1.6.m6.3.3.2.cmml" xref="S4.SS2.p1.6.m6.3.3">superscript</csymbol><apply id="S4.SS2.p1.6.m6.3.3.1.cmml" xref="S4.SS2.p1.6.m6.3.3"><csymbol cd="ambiguous" id="S4.SS2.p1.6.m6.3.3.1.2.cmml" xref="S4.SS2.p1.6.m6.3.3">subscript</csymbol><apply id="S4.SS2.p1.6.m6.3.3.1.1.2.cmml" xref="S4.SS2.p1.6.m6.3.3.1.1.1"><csymbol cd="latexml" id="S4.SS2.p1.6.m6.3.3.1.1.2.1.cmml" xref="S4.SS2.p1.6.m6.3.3.1.1.1.2">delimited-[]</csymbol><apply id="S4.SS2.p1.6.m6.3.3.1.1.1.1.cmml" xref="S4.SS2.p1.6.m6.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.6.m6.3.3.1.1.1.1.1.cmml" xref="S4.SS2.p1.6.m6.3.3.1.1.1.1">subscript</csymbol><apply id="S4.SS2.p1.6.m6.3.3.1.1.1.1.2.cmml" xref="S4.SS2.p1.6.m6.3.3.1.1.1.1.2"><ci id="S4.SS2.p1.6.m6.3.3.1.1.1.1.2.1.cmml" xref="S4.SS2.p1.6.m6.3.3.1.1.1.1.2.1">bold-^</ci><ci id="S4.SS2.p1.6.m6.3.3.1.1.1.1.2.2.cmml" xref="S4.SS2.p1.6.m6.3.3.1.1.1.1.2.2">𝒛</ci></apply><list id="S4.SS2.p1.6.m6.2.2.2.3.cmml" xref="S4.SS2.p1.6.m6.2.2.2.4"><ci id="S4.SS2.p1.6.m6.1.1.1.1a.cmml" xref="S4.SS2.p1.6.m6.1.1.1.1"><mtext mathsize="70%" id="S4.SS2.p1.6.m6.1.1.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1.1.1">hand</mtext></ci><ci id="S4.SS2.p1.6.m6.2.2.2.2.cmml" xref="S4.SS2.p1.6.m6.2.2.2.2">𝑙</ci></list></apply></apply><apply id="S4.SS2.p1.6.m6.3.3.1.3.cmml" xref="S4.SS2.p1.6.m6.3.3.1.3"><eq id="S4.SS2.p1.6.m6.3.3.1.3.1.cmml" xref="S4.SS2.p1.6.m6.3.3.1.3.1"></eq><ci id="S4.SS2.p1.6.m6.3.3.1.3.2.cmml" xref="S4.SS2.p1.6.m6.3.3.1.3.2">𝑙</ci><cn type="integer" id="S4.SS2.p1.6.m6.3.3.1.3.3.cmml" xref="S4.SS2.p1.6.m6.3.3.1.3.3">1</cn></apply></apply><ci id="S4.SS2.p1.6.m6.3.3.3.cmml" xref="S4.SS2.p1.6.m6.3.3.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.3c">[\bm{\hat{z}}_{{\text{hand}},l}]_{l=1}^{L}</annotation></semantics></math>, <math id="S4.SS2.p1.7.m7.1" class="ltx_Math" alttext="\bm{\hat{Z}}_{{\text{body}}}" display="inline"><semantics id="S4.SS2.p1.7.m7.1a"><msub id="S4.SS2.p1.7.m7.1.1" xref="S4.SS2.p1.7.m7.1.1.cmml"><mover accent="true" id="S4.SS2.p1.7.m7.1.1.2" xref="S4.SS2.p1.7.m7.1.1.2.cmml"><mi id="S4.SS2.p1.7.m7.1.1.2.2" xref="S4.SS2.p1.7.m7.1.1.2.2.cmml">𝒁</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.SS2.p1.7.m7.1.1.2.1" xref="S4.SS2.p1.7.m7.1.1.2.1.cmml">^</mo></mover><mtext id="S4.SS2.p1.7.m7.1.1.3" xref="S4.SS2.p1.7.m7.1.1.3a.cmml">body</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.7.m7.1b"><apply id="S4.SS2.p1.7.m7.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.7.m7.1.1.1.cmml" xref="S4.SS2.p1.7.m7.1.1">subscript</csymbol><apply id="S4.SS2.p1.7.m7.1.1.2.cmml" xref="S4.SS2.p1.7.m7.1.1.2"><ci id="S4.SS2.p1.7.m7.1.1.2.1.cmml" xref="S4.SS2.p1.7.m7.1.1.2.1">bold-^</ci><ci id="S4.SS2.p1.7.m7.1.1.2.2.cmml" xref="S4.SS2.p1.7.m7.1.1.2.2">𝒁</ci></apply><ci id="S4.SS2.p1.7.m7.1.1.3a.cmml" xref="S4.SS2.p1.7.m7.1.1.3"><mtext mathsize="70%" id="S4.SS2.p1.7.m7.1.1.3.cmml" xref="S4.SS2.p1.7.m7.1.1.3">body</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.7.m7.1c">\bm{\hat{Z}}_{{\text{body}}}</annotation></semantics></math> <math id="S4.SS2.p1.8.m8.1" class="ltx_Math" alttext="=" display="inline"><semantics id="S4.SS2.p1.8.m8.1a"><mo id="S4.SS2.p1.8.m8.1.1" xref="S4.SS2.p1.8.m8.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.8.m8.1b"><eq id="S4.SS2.p1.8.m8.1.1.cmml" xref="S4.SS2.p1.8.m8.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.8.m8.1c">=</annotation></semantics></math> <math id="S4.SS2.p1.9.m9.3" class="ltx_Math" alttext="[\bm{\hat{z}}_{{\text{body}},l}]_{l=1}^{L}" display="inline"><semantics id="S4.SS2.p1.9.m9.3a"><msubsup id="S4.SS2.p1.9.m9.3.3" xref="S4.SS2.p1.9.m9.3.3.cmml"><mrow id="S4.SS2.p1.9.m9.3.3.1.1.1" xref="S4.SS2.p1.9.m9.3.3.1.1.2.cmml"><mo stretchy="false" id="S4.SS2.p1.9.m9.3.3.1.1.1.2" xref="S4.SS2.p1.9.m9.3.3.1.1.2.1.cmml">[</mo><msub id="S4.SS2.p1.9.m9.3.3.1.1.1.1" xref="S4.SS2.p1.9.m9.3.3.1.1.1.1.cmml"><mover accent="true" id="S4.SS2.p1.9.m9.3.3.1.1.1.1.2" xref="S4.SS2.p1.9.m9.3.3.1.1.1.1.2.cmml"><mi id="S4.SS2.p1.9.m9.3.3.1.1.1.1.2.2" xref="S4.SS2.p1.9.m9.3.3.1.1.1.1.2.2.cmml">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.SS2.p1.9.m9.3.3.1.1.1.1.2.1" xref="S4.SS2.p1.9.m9.3.3.1.1.1.1.2.1.cmml">^</mo></mover><mrow id="S4.SS2.p1.9.m9.2.2.2.4" xref="S4.SS2.p1.9.m9.2.2.2.3.cmml"><mtext id="S4.SS2.p1.9.m9.1.1.1.1" xref="S4.SS2.p1.9.m9.1.1.1.1a.cmml">body</mtext><mo id="S4.SS2.p1.9.m9.2.2.2.4.1" xref="S4.SS2.p1.9.m9.2.2.2.3.cmml">,</mo><mi id="S4.SS2.p1.9.m9.2.2.2.2" xref="S4.SS2.p1.9.m9.2.2.2.2.cmml">l</mi></mrow></msub><mo stretchy="false" id="S4.SS2.p1.9.m9.3.3.1.1.1.3" xref="S4.SS2.p1.9.m9.3.3.1.1.2.1.cmml">]</mo></mrow><mrow id="S4.SS2.p1.9.m9.3.3.1.3" xref="S4.SS2.p1.9.m9.3.3.1.3.cmml"><mi id="S4.SS2.p1.9.m9.3.3.1.3.2" xref="S4.SS2.p1.9.m9.3.3.1.3.2.cmml">l</mi><mo id="S4.SS2.p1.9.m9.3.3.1.3.1" xref="S4.SS2.p1.9.m9.3.3.1.3.1.cmml">=</mo><mn id="S4.SS2.p1.9.m9.3.3.1.3.3" xref="S4.SS2.p1.9.m9.3.3.1.3.3.cmml">1</mn></mrow><mi id="S4.SS2.p1.9.m9.3.3.3" xref="S4.SS2.p1.9.m9.3.3.3.cmml">L</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.9.m9.3b"><apply id="S4.SS2.p1.9.m9.3.3.cmml" xref="S4.SS2.p1.9.m9.3.3"><csymbol cd="ambiguous" id="S4.SS2.p1.9.m9.3.3.2.cmml" xref="S4.SS2.p1.9.m9.3.3">superscript</csymbol><apply id="S4.SS2.p1.9.m9.3.3.1.cmml" xref="S4.SS2.p1.9.m9.3.3"><csymbol cd="ambiguous" id="S4.SS2.p1.9.m9.3.3.1.2.cmml" xref="S4.SS2.p1.9.m9.3.3">subscript</csymbol><apply id="S4.SS2.p1.9.m9.3.3.1.1.2.cmml" xref="S4.SS2.p1.9.m9.3.3.1.1.1"><csymbol cd="latexml" id="S4.SS2.p1.9.m9.3.3.1.1.2.1.cmml" xref="S4.SS2.p1.9.m9.3.3.1.1.1.2">delimited-[]</csymbol><apply id="S4.SS2.p1.9.m9.3.3.1.1.1.1.cmml" xref="S4.SS2.p1.9.m9.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.9.m9.3.3.1.1.1.1.1.cmml" xref="S4.SS2.p1.9.m9.3.3.1.1.1.1">subscript</csymbol><apply id="S4.SS2.p1.9.m9.3.3.1.1.1.1.2.cmml" xref="S4.SS2.p1.9.m9.3.3.1.1.1.1.2"><ci id="S4.SS2.p1.9.m9.3.3.1.1.1.1.2.1.cmml" xref="S4.SS2.p1.9.m9.3.3.1.1.1.1.2.1">bold-^</ci><ci id="S4.SS2.p1.9.m9.3.3.1.1.1.1.2.2.cmml" xref="S4.SS2.p1.9.m9.3.3.1.1.1.1.2.2">𝒛</ci></apply><list id="S4.SS2.p1.9.m9.2.2.2.3.cmml" xref="S4.SS2.p1.9.m9.2.2.2.4"><ci id="S4.SS2.p1.9.m9.1.1.1.1a.cmml" xref="S4.SS2.p1.9.m9.1.1.1.1"><mtext mathsize="70%" id="S4.SS2.p1.9.m9.1.1.1.1.cmml" xref="S4.SS2.p1.9.m9.1.1.1.1">body</mtext></ci><ci id="S4.SS2.p1.9.m9.2.2.2.2.cmml" xref="S4.SS2.p1.9.m9.2.2.2.2">𝑙</ci></list></apply></apply><apply id="S4.SS2.p1.9.m9.3.3.1.3.cmml" xref="S4.SS2.p1.9.m9.3.3.1.3"><eq id="S4.SS2.p1.9.m9.3.3.1.3.1.cmml" xref="S4.SS2.p1.9.m9.3.3.1.3.1"></eq><ci id="S4.SS2.p1.9.m9.3.3.1.3.2.cmml" xref="S4.SS2.p1.9.m9.3.3.1.3.2">𝑙</ci><cn type="integer" id="S4.SS2.p1.9.m9.3.3.1.3.3.cmml" xref="S4.SS2.p1.9.m9.3.3.1.3.3">1</cn></apply></apply><ci id="S4.SS2.p1.9.m9.3.3.3.cmml" xref="S4.SS2.p1.9.m9.3.3.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.9.m9.3c">[\bm{\hat{z}}_{{\text{body}},l}]_{l=1}^{L}</annotation></semantics></math>) and the synchronized audio features <math id="S4.SS2.p1.10.m10.1" class="ltx_Math" alttext="\bm{A}" display="inline"><semantics id="S4.SS2.p1.10.m10.1a"><mi id="S4.SS2.p1.10.m10.1.1" xref="S4.SS2.p1.10.m10.1.1.cmml">𝑨</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.10.m10.1b"><ci id="S4.SS2.p1.10.m10.1.1.cmml" xref="S4.SS2.p1.10.m10.1.1">𝑨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.10.m10.1c">\bm{A}</annotation></semantics></math> <math id="S4.SS2.p1.11.m11.1" class="ltx_Math" alttext="=" display="inline"><semantics id="S4.SS2.p1.11.m11.1a"><mo id="S4.SS2.p1.11.m11.1.1" xref="S4.SS2.p1.11.m11.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.11.m11.1b"><eq id="S4.SS2.p1.11.m11.1.1.cmml" xref="S4.SS2.p1.11.m11.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.11.m11.1c">=</annotation></semantics></math> <math id="S4.SS2.p1.12.m12.1" class="ltx_Math" alttext="[a_{l}]_{l=1}^{L+1}" display="inline"><semantics id="S4.SS2.p1.12.m12.1a"><msubsup id="S4.SS2.p1.12.m12.1.1" xref="S4.SS2.p1.12.m12.1.1.cmml"><mrow id="S4.SS2.p1.12.m12.1.1.1.1.1" xref="S4.SS2.p1.12.m12.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS2.p1.12.m12.1.1.1.1.1.2" xref="S4.SS2.p1.12.m12.1.1.1.1.2.1.cmml">[</mo><msub id="S4.SS2.p1.12.m12.1.1.1.1.1.1" xref="S4.SS2.p1.12.m12.1.1.1.1.1.1.cmml"><mi id="S4.SS2.p1.12.m12.1.1.1.1.1.1.2" xref="S4.SS2.p1.12.m12.1.1.1.1.1.1.2.cmml">a</mi><mi id="S4.SS2.p1.12.m12.1.1.1.1.1.1.3" xref="S4.SS2.p1.12.m12.1.1.1.1.1.1.3.cmml">l</mi></msub><mo stretchy="false" id="S4.SS2.p1.12.m12.1.1.1.1.1.3" xref="S4.SS2.p1.12.m12.1.1.1.1.2.1.cmml">]</mo></mrow><mrow id="S4.SS2.p1.12.m12.1.1.1.3" xref="S4.SS2.p1.12.m12.1.1.1.3.cmml"><mi id="S4.SS2.p1.12.m12.1.1.1.3.2" xref="S4.SS2.p1.12.m12.1.1.1.3.2.cmml">l</mi><mo id="S4.SS2.p1.12.m12.1.1.1.3.1" xref="S4.SS2.p1.12.m12.1.1.1.3.1.cmml">=</mo><mn id="S4.SS2.p1.12.m12.1.1.1.3.3" xref="S4.SS2.p1.12.m12.1.1.1.3.3.cmml">1</mn></mrow><mrow id="S4.SS2.p1.12.m12.1.1.3" xref="S4.SS2.p1.12.m12.1.1.3.cmml"><mi id="S4.SS2.p1.12.m12.1.1.3.2" xref="S4.SS2.p1.12.m12.1.1.3.2.cmml">L</mi><mo id="S4.SS2.p1.12.m12.1.1.3.1" xref="S4.SS2.p1.12.m12.1.1.3.1.cmml">+</mo><mn id="S4.SS2.p1.12.m12.1.1.3.3" xref="S4.SS2.p1.12.m12.1.1.3.3.cmml">1</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.12.m12.1b"><apply id="S4.SS2.p1.12.m12.1.1.cmml" xref="S4.SS2.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.12.m12.1.1.2.cmml" xref="S4.SS2.p1.12.m12.1.1">superscript</csymbol><apply id="S4.SS2.p1.12.m12.1.1.1.cmml" xref="S4.SS2.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.12.m12.1.1.1.2.cmml" xref="S4.SS2.p1.12.m12.1.1">subscript</csymbol><apply id="S4.SS2.p1.12.m12.1.1.1.1.2.cmml" xref="S4.SS2.p1.12.m12.1.1.1.1.1"><csymbol cd="latexml" id="S4.SS2.p1.12.m12.1.1.1.1.2.1.cmml" xref="S4.SS2.p1.12.m12.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S4.SS2.p1.12.m12.1.1.1.1.1.1.cmml" xref="S4.SS2.p1.12.m12.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.12.m12.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p1.12.m12.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p1.12.m12.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p1.12.m12.1.1.1.1.1.1.2">𝑎</ci><ci id="S4.SS2.p1.12.m12.1.1.1.1.1.1.3.cmml" xref="S4.SS2.p1.12.m12.1.1.1.1.1.1.3">𝑙</ci></apply></apply><apply id="S4.SS2.p1.12.m12.1.1.1.3.cmml" xref="S4.SS2.p1.12.m12.1.1.1.3"><eq id="S4.SS2.p1.12.m12.1.1.1.3.1.cmml" xref="S4.SS2.p1.12.m12.1.1.1.3.1"></eq><ci id="S4.SS2.p1.12.m12.1.1.1.3.2.cmml" xref="S4.SS2.p1.12.m12.1.1.1.3.2">𝑙</ci><cn type="integer" id="S4.SS2.p1.12.m12.1.1.1.3.3.cmml" xref="S4.SS2.p1.12.m12.1.1.1.3.3">1</cn></apply></apply><apply id="S4.SS2.p1.12.m12.1.1.3.cmml" xref="S4.SS2.p1.12.m12.1.1.3"><plus id="S4.SS2.p1.12.m12.1.1.3.1.cmml" xref="S4.SS2.p1.12.m12.1.1.3.1"></plus><ci id="S4.SS2.p1.12.m12.1.1.3.2.cmml" xref="S4.SS2.p1.12.m12.1.1.3.2">𝐿</ci><cn type="integer" id="S4.SS2.p1.12.m12.1.1.3.3.cmml" xref="S4.SS2.p1.12.m12.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.12.m12.1c">[a_{l}]_{l=1}^{L+1}</annotation></semantics></math>. The process is formalized as</p>
<table id="A2.EGx6" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(6)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E6.m1.10" class="ltx_Math" alttext="\displaystyle\bm{\hat{z}}_{{\text{hand}},L+1}^{*},\bm{\hat{z}}_{{\text{body}},L+1}^{*}=\mathcal{G}(\bm{A},[\bm{\hat{z}}_{{\text{hand}},l}]_{l=1}^{L},[\bm{\hat{z}}_{{\text{body}},l}]_{l=1}^{L})," display="inline"><semantics id="S4.E6.m1.10a"><mrow id="S4.E6.m1.10.10.1" xref="S4.E6.m1.10.10.1.1.cmml"><mrow id="S4.E6.m1.10.10.1.1" xref="S4.E6.m1.10.10.1.1.cmml"><mrow id="S4.E6.m1.10.10.1.1.2.2" xref="S4.E6.m1.10.10.1.1.2.3.cmml"><msubsup id="S4.E6.m1.10.10.1.1.1.1.1" xref="S4.E6.m1.10.10.1.1.1.1.1.cmml"><mover accent="true" id="S4.E6.m1.10.10.1.1.1.1.1.2.2" xref="S4.E6.m1.10.10.1.1.1.1.1.2.2.cmml"><mi id="S4.E6.m1.10.10.1.1.1.1.1.2.2.2" xref="S4.E6.m1.10.10.1.1.1.1.1.2.2.2.cmml">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.E6.m1.10.10.1.1.1.1.1.2.2.1" xref="S4.E6.m1.10.10.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mrow id="S4.E6.m1.2.2.2.2" xref="S4.E6.m1.2.2.2.3.cmml"><mtext id="S4.E6.m1.1.1.1.1" xref="S4.E6.m1.1.1.1.1a.cmml">hand</mtext><mo id="S4.E6.m1.2.2.2.2.2" xref="S4.E6.m1.2.2.2.3.cmml">,</mo><mrow id="S4.E6.m1.2.2.2.2.1" xref="S4.E6.m1.2.2.2.2.1.cmml"><mi id="S4.E6.m1.2.2.2.2.1.2" xref="S4.E6.m1.2.2.2.2.1.2.cmml">L</mi><mo id="S4.E6.m1.2.2.2.2.1.1" xref="S4.E6.m1.2.2.2.2.1.1.cmml">+</mo><mn id="S4.E6.m1.2.2.2.2.1.3" xref="S4.E6.m1.2.2.2.2.1.3.cmml">1</mn></mrow></mrow><mo id="S4.E6.m1.10.10.1.1.1.1.1.3" xref="S4.E6.m1.10.10.1.1.1.1.1.3.cmml">∗</mo></msubsup><mo id="S4.E6.m1.10.10.1.1.2.2.3" xref="S4.E6.m1.10.10.1.1.2.3.cmml">,</mo><msubsup id="S4.E6.m1.10.10.1.1.2.2.2" xref="S4.E6.m1.10.10.1.1.2.2.2.cmml"><mover accent="true" id="S4.E6.m1.10.10.1.1.2.2.2.2.2" xref="S4.E6.m1.10.10.1.1.2.2.2.2.2.cmml"><mi id="S4.E6.m1.10.10.1.1.2.2.2.2.2.2" xref="S4.E6.m1.10.10.1.1.2.2.2.2.2.2.cmml">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.E6.m1.10.10.1.1.2.2.2.2.2.1" xref="S4.E6.m1.10.10.1.1.2.2.2.2.2.1.cmml">^</mo></mover><mrow id="S4.E6.m1.4.4.2.2" xref="S4.E6.m1.4.4.2.3.cmml"><mtext id="S4.E6.m1.3.3.1.1" xref="S4.E6.m1.3.3.1.1a.cmml">body</mtext><mo id="S4.E6.m1.4.4.2.2.2" xref="S4.E6.m1.4.4.2.3.cmml">,</mo><mrow id="S4.E6.m1.4.4.2.2.1" xref="S4.E6.m1.4.4.2.2.1.cmml"><mi id="S4.E6.m1.4.4.2.2.1.2" xref="S4.E6.m1.4.4.2.2.1.2.cmml">L</mi><mo id="S4.E6.m1.4.4.2.2.1.1" xref="S4.E6.m1.4.4.2.2.1.1.cmml">+</mo><mn id="S4.E6.m1.4.4.2.2.1.3" xref="S4.E6.m1.4.4.2.2.1.3.cmml">1</mn></mrow></mrow><mo id="S4.E6.m1.10.10.1.1.2.2.2.3" xref="S4.E6.m1.10.10.1.1.2.2.2.3.cmml">∗</mo></msubsup></mrow><mo id="S4.E6.m1.10.10.1.1.5" xref="S4.E6.m1.10.10.1.1.5.cmml">=</mo><mrow id="S4.E6.m1.10.10.1.1.4" xref="S4.E6.m1.10.10.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E6.m1.10.10.1.1.4.4" xref="S4.E6.m1.10.10.1.1.4.4.cmml">𝒢</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.10.10.1.1.4.3" xref="S4.E6.m1.10.10.1.1.4.3.cmml">​</mo><mrow id="S4.E6.m1.10.10.1.1.4.2.2" xref="S4.E6.m1.10.10.1.1.4.2.3.cmml"><mo stretchy="false" id="S4.E6.m1.10.10.1.1.4.2.2.3" xref="S4.E6.m1.10.10.1.1.4.2.3.cmml">(</mo><mi id="S4.E6.m1.9.9" xref="S4.E6.m1.9.9.cmml">𝑨</mi><mo id="S4.E6.m1.10.10.1.1.4.2.2.4" xref="S4.E6.m1.10.10.1.1.4.2.3.cmml">,</mo><msubsup id="S4.E6.m1.10.10.1.1.3.1.1.1" xref="S4.E6.m1.10.10.1.1.3.1.1.1.cmml"><mrow id="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.2" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.2.1.cmml">[</mo><msub id="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.1" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.1.2" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.1.2.2" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.1.2.2.cmml">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.1.2.1" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mrow id="S4.E6.m1.6.6.2.4" xref="S4.E6.m1.6.6.2.3.cmml"><mtext id="S4.E6.m1.5.5.1.1" xref="S4.E6.m1.5.5.1.1a.cmml">hand</mtext><mo id="S4.E6.m1.6.6.2.4.1" xref="S4.E6.m1.6.6.2.3.cmml">,</mo><mi id="S4.E6.m1.6.6.2.2" xref="S4.E6.m1.6.6.2.2.cmml">l</mi></mrow></msub><mo stretchy="false" id="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.3" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.2.1.cmml">]</mo></mrow><mrow id="S4.E6.m1.10.10.1.1.3.1.1.1.1.3" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.3.cmml"><mi id="S4.E6.m1.10.10.1.1.3.1.1.1.1.3.2" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.3.2.cmml">l</mi><mo id="S4.E6.m1.10.10.1.1.3.1.1.1.1.3.1" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.3.1.cmml">=</mo><mn id="S4.E6.m1.10.10.1.1.3.1.1.1.1.3.3" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S4.E6.m1.10.10.1.1.3.1.1.1.3" xref="S4.E6.m1.10.10.1.1.3.1.1.1.3.cmml">L</mi></msubsup><mo id="S4.E6.m1.10.10.1.1.4.2.2.5" xref="S4.E6.m1.10.10.1.1.4.2.3.cmml">,</mo><msubsup id="S4.E6.m1.10.10.1.1.4.2.2.2" xref="S4.E6.m1.10.10.1.1.4.2.2.2.cmml"><mrow id="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.2.cmml"><mo stretchy="false" id="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.2" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.2.1.cmml">[</mo><msub id="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.1" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.1.cmml"><mover accent="true" id="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.1.2" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.1.2.cmml"><mi id="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.1.2.2" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.1.2.2.cmml">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.1.2.1" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.1.2.1.cmml">^</mo></mover><mrow id="S4.E6.m1.8.8.2.4" xref="S4.E6.m1.8.8.2.3.cmml"><mtext id="S4.E6.m1.7.7.1.1" xref="S4.E6.m1.7.7.1.1a.cmml">body</mtext><mo id="S4.E6.m1.8.8.2.4.1" xref="S4.E6.m1.8.8.2.3.cmml">,</mo><mi id="S4.E6.m1.8.8.2.2" xref="S4.E6.m1.8.8.2.2.cmml">l</mi></mrow></msub><mo stretchy="false" id="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.3" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.2.1.cmml">]</mo></mrow><mrow id="S4.E6.m1.10.10.1.1.4.2.2.2.1.3" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.3.cmml"><mi id="S4.E6.m1.10.10.1.1.4.2.2.2.1.3.2" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.3.2.cmml">l</mi><mo id="S4.E6.m1.10.10.1.1.4.2.2.2.1.3.1" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.3.1.cmml">=</mo><mn id="S4.E6.m1.10.10.1.1.4.2.2.2.1.3.3" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.3.3.cmml">1</mn></mrow><mi id="S4.E6.m1.10.10.1.1.4.2.2.2.3" xref="S4.E6.m1.10.10.1.1.4.2.2.2.3.cmml">L</mi></msubsup><mo stretchy="false" id="S4.E6.m1.10.10.1.1.4.2.2.6" xref="S4.E6.m1.10.10.1.1.4.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E6.m1.10.10.1.2" xref="S4.E6.m1.10.10.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E6.m1.10b"><apply id="S4.E6.m1.10.10.1.1.cmml" xref="S4.E6.m1.10.10.1"><eq id="S4.E6.m1.10.10.1.1.5.cmml" xref="S4.E6.m1.10.10.1.1.5"></eq><list id="S4.E6.m1.10.10.1.1.2.3.cmml" xref="S4.E6.m1.10.10.1.1.2.2"><apply id="S4.E6.m1.10.10.1.1.1.1.1.cmml" xref="S4.E6.m1.10.10.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E6.m1.10.10.1.1.1.1.1.1.cmml" xref="S4.E6.m1.10.10.1.1.1.1.1">superscript</csymbol><apply id="S4.E6.m1.10.10.1.1.1.1.1.2.cmml" xref="S4.E6.m1.10.10.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E6.m1.10.10.1.1.1.1.1.2.1.cmml" xref="S4.E6.m1.10.10.1.1.1.1.1">subscript</csymbol><apply id="S4.E6.m1.10.10.1.1.1.1.1.2.2.cmml" xref="S4.E6.m1.10.10.1.1.1.1.1.2.2"><ci id="S4.E6.m1.10.10.1.1.1.1.1.2.2.1.cmml" xref="S4.E6.m1.10.10.1.1.1.1.1.2.2.1">bold-^</ci><ci id="S4.E6.m1.10.10.1.1.1.1.1.2.2.2.cmml" xref="S4.E6.m1.10.10.1.1.1.1.1.2.2.2">𝒛</ci></apply><list id="S4.E6.m1.2.2.2.3.cmml" xref="S4.E6.m1.2.2.2.2"><ci id="S4.E6.m1.1.1.1.1a.cmml" xref="S4.E6.m1.1.1.1.1"><mtext mathsize="70%" id="S4.E6.m1.1.1.1.1.cmml" xref="S4.E6.m1.1.1.1.1">hand</mtext></ci><apply id="S4.E6.m1.2.2.2.2.1.cmml" xref="S4.E6.m1.2.2.2.2.1"><plus id="S4.E6.m1.2.2.2.2.1.1.cmml" xref="S4.E6.m1.2.2.2.2.1.1"></plus><ci id="S4.E6.m1.2.2.2.2.1.2.cmml" xref="S4.E6.m1.2.2.2.2.1.2">𝐿</ci><cn type="integer" id="S4.E6.m1.2.2.2.2.1.3.cmml" xref="S4.E6.m1.2.2.2.2.1.3">1</cn></apply></list></apply><times id="S4.E6.m1.10.10.1.1.1.1.1.3.cmml" xref="S4.E6.m1.10.10.1.1.1.1.1.3"></times></apply><apply id="S4.E6.m1.10.10.1.1.2.2.2.cmml" xref="S4.E6.m1.10.10.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.E6.m1.10.10.1.1.2.2.2.1.cmml" xref="S4.E6.m1.10.10.1.1.2.2.2">superscript</csymbol><apply id="S4.E6.m1.10.10.1.1.2.2.2.2.cmml" xref="S4.E6.m1.10.10.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.E6.m1.10.10.1.1.2.2.2.2.1.cmml" xref="S4.E6.m1.10.10.1.1.2.2.2">subscript</csymbol><apply id="S4.E6.m1.10.10.1.1.2.2.2.2.2.cmml" xref="S4.E6.m1.10.10.1.1.2.2.2.2.2"><ci id="S4.E6.m1.10.10.1.1.2.2.2.2.2.1.cmml" xref="S4.E6.m1.10.10.1.1.2.2.2.2.2.1">bold-^</ci><ci id="S4.E6.m1.10.10.1.1.2.2.2.2.2.2.cmml" xref="S4.E6.m1.10.10.1.1.2.2.2.2.2.2">𝒛</ci></apply><list id="S4.E6.m1.4.4.2.3.cmml" xref="S4.E6.m1.4.4.2.2"><ci id="S4.E6.m1.3.3.1.1a.cmml" xref="S4.E6.m1.3.3.1.1"><mtext mathsize="70%" id="S4.E6.m1.3.3.1.1.cmml" xref="S4.E6.m1.3.3.1.1">body</mtext></ci><apply id="S4.E6.m1.4.4.2.2.1.cmml" xref="S4.E6.m1.4.4.2.2.1"><plus id="S4.E6.m1.4.4.2.2.1.1.cmml" xref="S4.E6.m1.4.4.2.2.1.1"></plus><ci id="S4.E6.m1.4.4.2.2.1.2.cmml" xref="S4.E6.m1.4.4.2.2.1.2">𝐿</ci><cn type="integer" id="S4.E6.m1.4.4.2.2.1.3.cmml" xref="S4.E6.m1.4.4.2.2.1.3">1</cn></apply></list></apply><times id="S4.E6.m1.10.10.1.1.2.2.2.3.cmml" xref="S4.E6.m1.10.10.1.1.2.2.2.3"></times></apply></list><apply id="S4.E6.m1.10.10.1.1.4.cmml" xref="S4.E6.m1.10.10.1.1.4"><times id="S4.E6.m1.10.10.1.1.4.3.cmml" xref="S4.E6.m1.10.10.1.1.4.3"></times><ci id="S4.E6.m1.10.10.1.1.4.4.cmml" xref="S4.E6.m1.10.10.1.1.4.4">𝒢</ci><vector id="S4.E6.m1.10.10.1.1.4.2.3.cmml" xref="S4.E6.m1.10.10.1.1.4.2.2"><ci id="S4.E6.m1.9.9.cmml" xref="S4.E6.m1.9.9">𝑨</ci><apply id="S4.E6.m1.10.10.1.1.3.1.1.1.cmml" xref="S4.E6.m1.10.10.1.1.3.1.1.1"><csymbol cd="ambiguous" id="S4.E6.m1.10.10.1.1.3.1.1.1.2.cmml" xref="S4.E6.m1.10.10.1.1.3.1.1.1">superscript</csymbol><apply id="S4.E6.m1.10.10.1.1.3.1.1.1.1.cmml" xref="S4.E6.m1.10.10.1.1.3.1.1.1"><csymbol cd="ambiguous" id="S4.E6.m1.10.10.1.1.3.1.1.1.1.2.cmml" xref="S4.E6.m1.10.10.1.1.3.1.1.1">subscript</csymbol><apply id="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.2.cmml" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.2.1.cmml" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.1.2.cmml" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.1.2"><ci id="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.1.2.1">bold-^</ci><ci id="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.1.1.1.2.2">𝒛</ci></apply><list id="S4.E6.m1.6.6.2.3.cmml" xref="S4.E6.m1.6.6.2.4"><ci id="S4.E6.m1.5.5.1.1a.cmml" xref="S4.E6.m1.5.5.1.1"><mtext mathsize="70%" id="S4.E6.m1.5.5.1.1.cmml" xref="S4.E6.m1.5.5.1.1">hand</mtext></ci><ci id="S4.E6.m1.6.6.2.2.cmml" xref="S4.E6.m1.6.6.2.2">𝑙</ci></list></apply></apply><apply id="S4.E6.m1.10.10.1.1.3.1.1.1.1.3.cmml" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.3"><eq id="S4.E6.m1.10.10.1.1.3.1.1.1.1.3.1.cmml" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.3.1"></eq><ci id="S4.E6.m1.10.10.1.1.3.1.1.1.1.3.2.cmml" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.3.2">𝑙</ci><cn type="integer" id="S4.E6.m1.10.10.1.1.3.1.1.1.1.3.3.cmml" xref="S4.E6.m1.10.10.1.1.3.1.1.1.1.3.3">1</cn></apply></apply><ci id="S4.E6.m1.10.10.1.1.3.1.1.1.3.cmml" xref="S4.E6.m1.10.10.1.1.3.1.1.1.3">𝐿</ci></apply><apply id="S4.E6.m1.10.10.1.1.4.2.2.2.cmml" xref="S4.E6.m1.10.10.1.1.4.2.2.2"><csymbol cd="ambiguous" id="S4.E6.m1.10.10.1.1.4.2.2.2.2.cmml" xref="S4.E6.m1.10.10.1.1.4.2.2.2">superscript</csymbol><apply id="S4.E6.m1.10.10.1.1.4.2.2.2.1.cmml" xref="S4.E6.m1.10.10.1.1.4.2.2.2"><csymbol cd="ambiguous" id="S4.E6.m1.10.10.1.1.4.2.2.2.1.2.cmml" xref="S4.E6.m1.10.10.1.1.4.2.2.2">subscript</csymbol><apply id="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.2.cmml" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1"><csymbol cd="latexml" id="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.2.1.cmml" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.2">delimited-[]</csymbol><apply id="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.1.cmml" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.1.1.cmml" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.1">subscript</csymbol><apply id="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.1.2.cmml" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.1.2"><ci id="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.1.2.1.cmml" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.1.2.1">bold-^</ci><ci id="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.1.2.2.cmml" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.1.1.1.2.2">𝒛</ci></apply><list id="S4.E6.m1.8.8.2.3.cmml" xref="S4.E6.m1.8.8.2.4"><ci id="S4.E6.m1.7.7.1.1a.cmml" xref="S4.E6.m1.7.7.1.1"><mtext mathsize="70%" id="S4.E6.m1.7.7.1.1.cmml" xref="S4.E6.m1.7.7.1.1">body</mtext></ci><ci id="S4.E6.m1.8.8.2.2.cmml" xref="S4.E6.m1.8.8.2.2">𝑙</ci></list></apply></apply><apply id="S4.E6.m1.10.10.1.1.4.2.2.2.1.3.cmml" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.3"><eq id="S4.E6.m1.10.10.1.1.4.2.2.2.1.3.1.cmml" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.3.1"></eq><ci id="S4.E6.m1.10.10.1.1.4.2.2.2.1.3.2.cmml" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.3.2">𝑙</ci><cn type="integer" id="S4.E6.m1.10.10.1.1.4.2.2.2.1.3.3.cmml" xref="S4.E6.m1.10.10.1.1.4.2.2.2.1.3.3">1</cn></apply></apply><ci id="S4.E6.m1.10.10.1.1.4.2.2.2.3.cmml" xref="S4.E6.m1.10.10.1.1.4.2.2.2.3">𝐿</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m1.10c">\displaystyle\bm{\hat{z}}_{{\text{hand}},L+1}^{*},\bm{\hat{z}}_{{\text{body}},L+1}^{*}=\mathcal{G}(\bm{A},[\bm{\hat{z}}_{{\text{hand}},l}]_{l=1}^{L},[\bm{\hat{z}}_{{\text{body}},l}]_{l=1}^{L}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS2.p1.13" class="ltx_p">where the audio features <math id="S4.SS2.p1.13.m1.1" class="ltx_Math" alttext="\bm{A}" display="inline"><semantics id="S4.SS2.p1.13.m1.1a"><mi id="S4.SS2.p1.13.m1.1.1" xref="S4.SS2.p1.13.m1.1.1.cmml">𝑨</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.13.m1.1b"><ci id="S4.SS2.p1.13.m1.1.1.cmml" xref="S4.SS2.p1.13.m1.1.1">𝑨</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.13.m1.1c">\bm{A}</annotation></semantics></math> consist of <em id="S4.SS2.p1.13.1" class="ltx_emph ltx_font_italic">mel frequency cepstral co-efficients (MFCC)</em>, <em id="S4.SS2.p1.13.2" class="ltx_emph ltx_font_italic">MFCC delta</em>, <em id="S4.SS2.p1.13.3" class="ltx_emph ltx_font_italic">constant-Q chromagram</em>, <em id="S4.SS2.p1.13.4" class="ltx_emph ltx_font_italic">onset</em>, and <em id="S4.SS2.p1.13.5" class="ltx_emph ltx_font_italic">tempogram</em>, which are extracted by the famous audio processing toolbox Librosa <cite class="ltx_cite ltx_citemacro_citep">(McFee et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2015</a>)</cite>. Our approach incorporates the causal attention layer, as introduced by <cite class="ltx_cite ltx_citemacro_citet">Vaswani et al<span class="ltx_text">.</span> (<a href="#bib.bib80" title="" class="ltx_ref">2017</a>)</cite>, which is designed to facilitate communication exclusively between current and preceding data, thereby maintaining causality. Following <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib72" title="" class="ltx_ref">2019</a>)</cite>, we minimize the standard categorical cross-entropy loss to train the generator.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Generative Semantic Gesture Retrieval</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we introduce Generative Semantic Gesture Retrieval. The traditional index-retrieval-rank paradigm <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a href="#bib.bib110" title="" class="ltx_ref">2023b</a>)</cite> retrieves candidate items from a database based on the context of the input query and then ranks these items to determine the final item. Although this method thoroughly considers the semantic distribution of the database, it only supports whole-sentence retrieval and cannot automatically determine the specific occurrence timestamp of the retrieved item within the sentence. To solve it, we choose the generative retrieval architecture inspired by GENRE <cite class="ltx_cite ltx_citemacro_citep">(Cao
et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>. Specifically, as shown in Figure <a href="#S5.F5" title="Figure 5 ‣ 5. Generative Semantic Gesture Retrieval ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the whole process is modeled as the standard prompt-based autoregressive generation. Given the input transcript as the prompt, the model will repeat the input and, based on the context, insert the retrieved semantic gesture, e.g., <span id="S5.p1.1.1" class="ltx_text ltx_font_typewriter">1 ARM FLEX</span>, at the appropriate position, where <span id="S5.p1.1.2" class="ltx_text ltx_font_typewriter">1</span> represents the index of the gesture. The retrieval model is initialized by the Large Language Model (LLM), ensuring its generalizability. Then, it is fine-tuned with a small amount of annotated data for task alignment and to perceive the semantic distribution of the semantic gesture set, with specific details to be discussed in Section <a href="#S5.SS2" title="5.2. LLM-Based Retrieval Model ‣ 5. Generative Semantic Gesture Retrieval ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2405.09814/assets/x5.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="322" height="260" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>. </span><span id="S5.F5.3.2" class="ltx_text" style="font-size:90%;">The overview of generative semantic gesture retrieval.</span></figcaption>
</figure>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2405.09814/assets/x6.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="216" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>. </span><span id="S5.F6.3.2" class="ltx_text" style="font-size:90%;">The meta-information of a semantic gesture in the SeG Dataset.</span></figcaption>
</figure>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2405.09814/assets/x7.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="220" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>. </span><span id="S5.F7.3.2" class="ltx_text" style="font-size:90%;">An example of the instruction dataset for (Large Language Model) LLM fine-tuning.</span></figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>SeG: Semantic Gesture Dataset</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.3" class="ltx_p">Gestures are intrinsically linked with semantic information and form a fundamental aspect of human communication. This connection dates back to the use of body postures for expressing intentions, predating the use of written symbols. Meanwhile, the semantic information conveyed by gestures is significantly influenced by cultural backgrounds, leading to substantial differences in gesture meanings across various cultures. To cover such diversity and avoid the ambiguity inherent in semantic gestures, we reference and synthesize findings in the fields of linguistic and human behavioral studies pertaining to semantic gestures <cite class="ltx_cite ltx_citemacro_citep">(Morris, <a href="#bib.bib59" title="" class="ltx_ref">1994</a>; Wagner and
Armstrong, <a href="#bib.bib82" title="" class="ltx_ref">2003</a>; Kipp, <a href="#bib.bib38" title="" class="ltx_ref">2005</a>; World
Federation of the Deaf, <a href="#bib.bib86" title="" class="ltx_ref">1975</a>)</cite>. This effort leads to the compilation of a comprehensive semantic gesture dataset (SeG), denoted as <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="\bm{G}=[\bm{g}_{i}]_{i=1}^{N_{{\text{sec}}}}" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mrow id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml">𝑮</mi><mo id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">=</mo><msubsup id="S5.SS1.p1.1.m1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.cmml"><mrow id="S5.SS1.p1.1.m1.1.1.1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S5.SS1.p1.1.m1.1.1.1.1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.1.1.1.2.1.cmml">[</mo><msub id="S5.SS1.p1.1.m1.1.1.1.1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S5.SS1.p1.1.m1.1.1.1.1.1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.1.1.1.1.1.2.cmml">𝒈</mi><mi id="S5.SS1.p1.1.m1.1.1.1.1.1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S5.SS1.p1.1.m1.1.1.1.1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.1.1.1.2.1.cmml">]</mo></mrow><mrow id="S5.SS1.p1.1.m1.1.1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.1.1.3.cmml"><mi id="S5.SS1.p1.1.m1.1.1.1.1.3.2" xref="S5.SS1.p1.1.m1.1.1.1.1.3.2.cmml">i</mi><mo id="S5.SS1.p1.1.m1.1.1.1.1.3.1" xref="S5.SS1.p1.1.m1.1.1.1.1.3.1.cmml">=</mo><mn id="S5.SS1.p1.1.m1.1.1.1.1.3.3" xref="S5.SS1.p1.1.m1.1.1.1.1.3.3.cmml">1</mn></mrow><msub id="S5.SS1.p1.1.m1.1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.1.3.cmml"><mi id="S5.SS1.p1.1.m1.1.1.1.3.2" xref="S5.SS1.p1.1.m1.1.1.1.3.2.cmml">N</mi><mtext id="S5.SS1.p1.1.m1.1.1.1.3.3" xref="S5.SS1.p1.1.m1.1.1.1.3.3a.cmml">sec</mtext></msub></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><eq id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2"></eq><ci id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3">𝑮</ci><apply id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.1">superscript</csymbol><apply id="S5.SS1.p1.1.m1.1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.1.1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.1">subscript</csymbol><apply id="S5.SS1.p1.1.m1.1.1.1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.SS1.p1.1.m1.1.1.1.1.1.2.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S5.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.SS1.p1.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1.1.1.1.2">𝒈</ci><ci id="S5.SS1.p1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S5.SS1.p1.1.m1.1.1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1.3"><eq id="S5.SS1.p1.1.m1.1.1.1.1.3.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1.3.1"></eq><ci id="S5.SS1.p1.1.m1.1.1.1.1.3.2.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1.3.2">𝑖</ci><cn type="integer" id="S5.SS1.p1.1.m1.1.1.1.1.3.3.cmml" xref="S5.SS1.p1.1.m1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S5.SS1.p1.1.m1.1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.1.1.1.3.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1.3">subscript</csymbol><ci id="S5.SS1.p1.1.m1.1.1.1.3.2.cmml" xref="S5.SS1.p1.1.m1.1.1.1.3.2">𝑁</ci><ci id="S5.SS1.p1.1.m1.1.1.1.3.3a.cmml" xref="S5.SS1.p1.1.m1.1.1.1.3.3"><mtext mathsize="50%" id="S5.SS1.p1.1.m1.1.1.1.3.3.cmml" xref="S5.SS1.p1.1.m1.1.1.1.3.3">sec</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\bm{G}=[\bm{g}_{i}]_{i=1}^{N_{{\text{sec}}}}</annotation></semantics></math>, which encompasses <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="N_{{\text{sec}}}" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><msub id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml"><mi id="S5.SS1.p1.2.m2.1.1.2" xref="S5.SS1.p1.2.m2.1.1.2.cmml">N</mi><mtext id="S5.SS1.p1.2.m2.1.1.3" xref="S5.SS1.p1.2.m2.1.1.3a.cmml">sec</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><apply id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.p1.2.m2.1.1.2">𝑁</ci><ci id="S5.SS1.p1.2.m2.1.1.3a.cmml" xref="S5.SS1.p1.2.m2.1.1.3"><mtext mathsize="70%" id="S5.SS1.p1.2.m2.1.1.3.cmml" xref="S5.SS1.p1.2.m2.1.1.3">sec</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">N_{{\text{sec}}}</annotation></semantics></math> types of semantic gestures commonly utilized globally. Each record <math id="S5.SS1.p1.3.m3.1" class="ltx_Math" alttext="\bm{g}" display="inline"><semantics id="S5.SS1.p1.3.m3.1a"><mi id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml">𝒈</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><ci id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1">𝒈</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">\bm{g}</annotation></semantics></math> in the dataset consists of a sequence of captured motion and its meta-information. Figure <a href="#S5.F6" title="Figure 6 ‣ 5. Generative Semantic Gesture Retrieval ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows an example of the meta-information of a semantic gesture in SeG, which includes an index, label, description, contextual meaning, and example sentence, facilitating a thorough organization and analysis of human gestures.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">As for motion, we employ professional motion capture equipment to gather high-quality, finger-contained animation. The collection involves two performers, a male and a female. To ensure the diversity of motion, performers deliver multiple interpretations and styles for each semantic gesture in the dataset. In total, we capture 1.5 hours of semantic gesture motion data, with each gesture being interpreted in an average of 5.7 in different ways.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>LLM-Based Retrieval Model</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Concurrently, we collect twenty 10<math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mo id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><csymbol cd="latexml" id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">\sim</annotation></semantics></math>20-minute speech transcripts from TED talks <cite class="ltx_cite ltx_citemacro_citep">(TED Talks, <a href="#bib.bib77" title="" class="ltx_ref">2023</a>)</cite> and enlist annotators for manual annotation, thereby creating an instruction dataset to fine-tune Large Language Models (LLMs) for semantic gesture retrieval. Figure <a href="#S5.F7" title="Figure 7 ‣ 5. Generative Semantic Gesture Retrieval ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows a case of the instruction dataset. Notably, we explicitly integrate the index information of each gesture into the <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_typewriter">Instruction</span> part of the dataset and require the model to predict the name of the gesture as well as its index simultaneously. In practice, this trick could significantly alleviate the model to produce hallucinations, including the generation of non-existent gestures and the combination of different body parts and other actions. Intuitively, incorporating such logical information can help enhance the reasoning ability of LLMs, similar to how some studies opt to add some code data when training LLMs <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a href="#bib.bib78" title="" class="ltx_ref">2023</a>; Anil
et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>. We choose to fine-tune OpenAI’s GPT-3.5-turbo <cite class="ltx_cite ltx_citemacro_citep">(Ouyang
et al<span class="ltx_text">.</span>, <a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite> on the instruction dataset.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Meanwhile, the current instruction data does not take into account the meta-information of semantic gestures, thereby overlooking the rich semantic information it contains. In addition to using <span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_typewriter">example sentence</span> from the meta-information of each gesture to enrich the dataset, we also replace the current indexing identifier for each gesture, a simple combination of increasing numbers and gesture names, with a new semantics-aware indexing identifier. The core idea is to cluster the semantic gestures according to their textual description and index them based on the clusters, making the index implicitly indicate the relationship between different gestures. To be specific, as illustrated in Figure <a href="#S5.F8" title="Figure 8 ‣ 5.2. LLM-Based Retrieval Model ‣ 5. Generative Semantic Gesture Retrieval ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we feed the gesture <span id="S5.SS2.p2.1.2" class="ltx_text ltx_font_typewriter">label</span>, <span id="S5.SS2.p2.1.3" class="ltx_text ltx_font_typewriter">description</span>, and <span id="S5.SS2.p2.1.4" class="ltx_text ltx_font_typewriter">contextual meaning</span> into a sentence-T5-base model <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al<span class="ltx_text">.</span>, <a href="#bib.bib74" title="" class="ltx_ref">2020</a>)</cite> to generate a representational embedding vector. Each vector encapsulates the semantic information of a specific gesture within the SeG, and collectively, these vectors form a big gesture embedding set. Then, we apply hierarchical clustering with constrained K-means <cite class="ltx_cite ltx_citemacro_citep">(Bennett
et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2000</a>)</cite> to these embedding vectors. The resultant index within each cluster is utilized as the final hierarchical semantic number identifiers. We conduct a related ablation study in Section <a href="#S7.SS5.SSS3" title="7.5.3. Semantics-Aware Indexing Identifier. ‣ 7.5. Ablation Study ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.5.3</span></a>.</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2405.09814/assets/x8.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="180" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>. </span><span id="S5.F8.3.2" class="ltx_text" style="font-size:90%;">The process of building a semantics-aware identifier for the semantic gesture.</span></figcaption>
</figure>
<figure id="S5.F9" class="ltx_figure"><img src="/html/2405.09814/assets/x9.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="311" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>. </span><span id="S5.F9.3.2" class="ltx_text" style="font-size:90%;">Comparison of the indexing results of two semantic gestures.</span></figcaption>
</figure>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">For example, as shown in Figure <a href="#S5.F9" title="Figure 9 ‣ 5.2. LLM-Based Retrieval Model ‣ 5. Generative Semantic Gesture Retrieval ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, the gestures <span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_typewriter">FINGERS RUB</span> and <span id="S5.SS2.p3.1.2" class="ltx_text ltx_font_typewriter">PALM UP</span> share the same prefix ”010” and differ in the <math id="S5.SS2.p3.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S5.SS2.p3.1.m1.1a"><mn id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><cn type="integer" id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">4</annotation></semantics></math>-th position, indicating similar description or contextual meanings. With these indexes, the final semantic identifiers, e.g., <span id="S5.SS2.p3.1.3" class="ltx_text ltx_font_typewriter">0100 FINGERS RUB</span>, can capture hierarchical semantics of gestures while ensuring interpretability.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Semantics Gesture Alignment</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We develop an alignment module to merge retrieved semantic gestures into rhythmic gestures synthesized by the generator <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S6.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><ci id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">\mathcal{G}</annotation></semantics></math> at the right timing. Specifically, we need to solve two problems: (a) determining <em id="S6.p1.1.1" class="ltx_emph ltx_font_italic">when</em> to merge each semantic gesture; and (b) <em id="S6.p1.1.2" class="ltx_emph ltx_font_italic">how</em> to merge these semantic gestures into the generated rhythmic gestures without affecting the naturalness of the original motion. Corresponding solutions are discussed in the following sections.</p>
</div>
<figure id="S6.F10" class="ltx_figure"><img src="/html/2405.09814/assets/x10.png" id="S6.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="341" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F10.4.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>. </span><span id="S6.F10.5.2" class="ltx_text" style="font-size:90%;">(a) Determining <em id="S6.F10.5.2.1" class="ltx_emph ltx_font_italic">when</em> to merge each semantic gesture based on audio beats; and (b) <em id="S6.F10.5.2.2" class="ltx_emph ltx_font_italic">how</em> to merge these semantic gestures into the generated rhythmic gestures without affecting the naturalness of the original motion.</span></figcaption>
</figure>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>When to Merge</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.2" class="ltx_p">To determine when to merge retrieved semantic gestures, a straightforward method is to directly mark the timestamp of the trigger word (such as ”opportunity” in Figure <a href="#S6.F10" title="Figure 10 ‣ 6. Semantics Gesture Alignment ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> (a)) identified by the retrieval model as the <em id="S6.SS1.p1.2.1" class="ltx_emph ltx_font_italic">merging timing</em> <math id="S6.SS1.p1.1.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S6.SS1.p1.1.m1.1a"><mi id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.1b"><ci id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.1.m1.1c">l</annotation></semantics></math>. The trigger word timestamp refers to the midpoint of the time interval in which the word occurs. But this approach would disrupt the ongoing beat gestures or create gestures with unexpected strokes during the subdued parts of the audio, leading to rhythm incongruity. To solve it, as shown in Figure <a href="#S6.F10" title="Figure 10 ‣ 6. Semantics Gesture Alignment ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> (a), our approach begins with detecting beats of the audio, which likely correspond to the stroke phase of gestures. This phase is recognized as the most expressive and energetically concentrated phase within a gesture’s progression  <cite class="ltx_cite ltx_citemacro_citep">(Ferstl
et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite>. Subsequently, we identify the beat closest to the timestamp of the trigger word as the final <math id="S6.SS1.p1.2.m2.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S6.SS1.p1.2.m2.1a"><mi id="S6.SS1.p1.2.m2.1.1" xref="S6.SS1.p1.2.m2.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.2.m2.1b"><ci id="S6.SS1.p1.2.m2.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.2.m2.1c">l</annotation></semantics></math>. The positive impact of this approach on rhythm integrity will be discussed in an ablation study.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>How to Merge</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.9" class="ltx_p">In this section, we aim at explicitly merging retrieved semantic gestures with generated rhythmic gestures at the merging timing <math id="S6.SS2.p1.1.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S6.SS2.p1.1.m1.1a"><mi id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><ci id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">l</annotation></semantics></math>. For preparation, we first extract the stroke part of each semantic gesture from the SeG Dataset and crop it into a <math id="S6.SS2.p1.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S6.SS2.p1.2.m2.1a"><mn id="S6.SS2.p1.2.m2.1.1" xref="S6.SS2.p1.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.2.m2.1b"><cn type="integer" id="S6.SS2.p1.2.m2.1.1.cmml" xref="S6.SS2.p1.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.2.m2.1c">1</annotation></semantics></math>-second segment. Then, these gesture segments are encoded into discrete tokens using <math id="S6.SS2.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{E}_{{\text{VQ}}}" display="inline"><semantics id="S6.SS2.p1.3.m3.1a"><msub id="S6.SS2.p1.3.m3.1.1" xref="S6.SS2.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS2.p1.3.m3.1.1.2" xref="S6.SS2.p1.3.m3.1.1.2.cmml">ℰ</mi><mtext id="S6.SS2.p1.3.m3.1.1.3" xref="S6.SS2.p1.3.m3.1.1.3a.cmml">VQ</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.3.m3.1b"><apply id="S6.SS2.p1.3.m3.1.1.cmml" xref="S6.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S6.SS2.p1.3.m3.1.1.1.cmml" xref="S6.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S6.SS2.p1.3.m3.1.1.2.cmml" xref="S6.SS2.p1.3.m3.1.1.2">ℰ</ci><ci id="S6.SS2.p1.3.m3.1.1.3a.cmml" xref="S6.SS2.p1.3.m3.1.1.3"><mtext mathsize="70%" id="S6.SS2.p1.3.m3.1.1.3.cmml" xref="S6.SS2.p1.3.m3.1.1.3">VQ</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.3.m3.1c">\mathcal{E}_{{\text{VQ}}}</annotation></semantics></math>, thereby constructing the SeG token library. Given the index and label of the retrieved semantic gesture, we could search the matched gesture tokens from the library. As illustrated in Figure <a href="#S6.F10" title="Figure 10 ‣ 6. Semantics Gesture Alignment ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> (b), the following merging process is executed in two phases: (a) we replace the original motion with matched semantic gesture tokens, while aligning the timing <math id="S6.SS2.p1.4.m4.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S6.SS2.p1.4.m4.1a"><mi id="S6.SS2.p1.4.m4.1.1" xref="S6.SS2.p1.4.m4.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.4.m4.1b"><ci id="S6.SS2.p1.4.m4.1.1.cmml" xref="S6.SS2.p1.4.m4.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.4.m4.1c">l</annotation></semantics></math> with the three-quarters of the way through the matched token sequences. It allows semantic gestures to manifest slightly ahead of the corresponding semantic content in the speech, reflecting the typical <math id="S6.SS2.p1.5.m5.1" class="ltx_Math" alttext="0.4" display="inline"><semantics id="S6.SS2.p1.5.m5.1a"><mn id="S6.SS2.p1.5.m5.1.1" xref="S6.SS2.p1.5.m5.1.1.cmml">0.4</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.5.m5.1b"><cn type="float" id="S6.SS2.p1.5.m5.1.1.cmml" xref="S6.SS2.p1.5.m5.1.1">0.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.5.m5.1c">0.4</annotation></semantics></math>-second planning phase observed in human gestures informed by the behavioral study findings by  <cite class="ltx_cite ltx_citemacro_citep">(Indefrey and
Levelt, <a href="#bib.bib34" title="" class="ltx_ref">2004</a>)</cite>. To mitigate abrupt and unnatural transitions often associated with hard replacements, inspired by TM2D  <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2023</a>)</cite>, we employ a strategy involving a weighted merge operation at token positions around the replacement site. Explicitly, we increase the merging weight of the matched semantic gesture tokens <math id="S6.SS2.p1.6.m6.1" class="ltx_Math" alttext="w_{s}" display="inline"><semantics id="S6.SS2.p1.6.m6.1a"><msub id="S6.SS2.p1.6.m6.1.1" xref="S6.SS2.p1.6.m6.1.1.cmml"><mi id="S6.SS2.p1.6.m6.1.1.2" xref="S6.SS2.p1.6.m6.1.1.2.cmml">w</mi><mi id="S6.SS2.p1.6.m6.1.1.3" xref="S6.SS2.p1.6.m6.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.6.m6.1b"><apply id="S6.SS2.p1.6.m6.1.1.cmml" xref="S6.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S6.SS2.p1.6.m6.1.1.1.cmml" xref="S6.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S6.SS2.p1.6.m6.1.1.2.cmml" xref="S6.SS2.p1.6.m6.1.1.2">𝑤</ci><ci id="S6.SS2.p1.6.m6.1.1.3.cmml" xref="S6.SS2.p1.6.m6.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.6.m6.1c">w_{s}</annotation></semantics></math> from 0.3 to 0.7 using a half cosine curve prior to replacement, and subsequently reduce it in the opposite manner after replacement. Meanwhile, the merging weight of the raw motion tokens <math id="S6.SS2.p1.7.m7.1" class="ltx_Math" alttext="w_{r}" display="inline"><semantics id="S6.SS2.p1.7.m7.1a"><msub id="S6.SS2.p1.7.m7.1.1" xref="S6.SS2.p1.7.m7.1.1.cmml"><mi id="S6.SS2.p1.7.m7.1.1.2" xref="S6.SS2.p1.7.m7.1.1.2.cmml">w</mi><mi id="S6.SS2.p1.7.m7.1.1.3" xref="S6.SS2.p1.7.m7.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.7.m7.1b"><apply id="S6.SS2.p1.7.m7.1.1.cmml" xref="S6.SS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S6.SS2.p1.7.m7.1.1.1.cmml" xref="S6.SS2.p1.7.m7.1.1">subscript</csymbol><ci id="S6.SS2.p1.7.m7.1.1.2.cmml" xref="S6.SS2.p1.7.m7.1.1.2">𝑤</ci><ci id="S6.SS2.p1.7.m7.1.1.3.cmml" xref="S6.SS2.p1.7.m7.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.7.m7.1c">w_{r}</annotation></semantics></math> is set to 1-<math id="S6.SS2.p1.8.m8.1" class="ltx_Math" alttext="w_{s}" display="inline"><semantics id="S6.SS2.p1.8.m8.1a"><msub id="S6.SS2.p1.8.m8.1.1" xref="S6.SS2.p1.8.m8.1.1.cmml"><mi id="S6.SS2.p1.8.m8.1.1.2" xref="S6.SS2.p1.8.m8.1.1.2.cmml">w</mi><mi id="S6.SS2.p1.8.m8.1.1.3" xref="S6.SS2.p1.8.m8.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.8.m8.1b"><apply id="S6.SS2.p1.8.m8.1.1.cmml" xref="S6.SS2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S6.SS2.p1.8.m8.1.1.1.cmml" xref="S6.SS2.p1.8.m8.1.1">subscript</csymbol><ci id="S6.SS2.p1.8.m8.1.1.2.cmml" xref="S6.SS2.p1.8.m8.1.1.2">𝑤</ci><ci id="S6.SS2.p1.8.m8.1.1.3.cmml" xref="S6.SS2.p1.8.m8.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.8.m8.1c">w_{s}</annotation></semantics></math> to maintain the scale of the features. This technique ensures the final animation decoded by <math id="S6.SS2.p1.9.m9.1" class="ltx_Math" alttext="\mathcal{D}_{{\text{VQ}}}" display="inline"><semantics id="S6.SS2.p1.9.m9.1a"><msub id="S6.SS2.p1.9.m9.1.1" xref="S6.SS2.p1.9.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS2.p1.9.m9.1.1.2" xref="S6.SS2.p1.9.m9.1.1.2.cmml">𝒟</mi><mtext id="S6.SS2.p1.9.m9.1.1.3" xref="S6.SS2.p1.9.m9.1.1.3a.cmml">VQ</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.9.m9.1b"><apply id="S6.SS2.p1.9.m9.1.1.cmml" xref="S6.SS2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S6.SS2.p1.9.m9.1.1.1.cmml" xref="S6.SS2.p1.9.m9.1.1">subscript</csymbol><ci id="S6.SS2.p1.9.m9.1.1.2.cmml" xref="S6.SS2.p1.9.m9.1.1.2">𝒟</ci><ci id="S6.SS2.p1.9.m9.1.1.3a.cmml" xref="S6.SS2.p1.9.m9.1.1.3"><mtext mathsize="70%" id="S6.SS2.p1.9.m9.1.1.3.cmml" xref="S6.SS2.p1.9.m9.1.1.3">VQ</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.9.m9.1c">\mathcal{D}_{{\text{VQ}}}</annotation></semantics></math> results is not only smooth but also retains the intended semantic richness and rhythmic precision.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.4" class="ltx_p">Additionally, the gesture generator <math id="S6.SS2.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S6.SS2.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><ci id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">\mathcal{G}</annotation></semantics></math>, during pre-training, only utilizes paired speech-gesture data and has not encountered the semantic gesture tokens from the SeG dataset. Before merging, it is necessary to ensure the output motion distribution of <math id="S6.SS2.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S6.SS2.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S6.SS2.p2.2.m2.1.1" xref="S6.SS2.p2.2.m2.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.2.m2.1b"><ci id="S6.SS2.p2.2.m2.1.1.cmml" xref="S6.SS2.p2.2.m2.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.2.m2.1c">\mathcal{G}</annotation></semantics></math> covering the SeG dataset. Inspired by the Supervised Fine-Tuning (SFT) strategy for LLMs <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib85" title="" class="ltx_ref">2023b</a>)</cite>, we utilize the speech transcripts with semantic annotation in Section <a href="#S5.SS2" title="5.2. LLM-Based Retrieval Model ‣ 5. Generative Semantic Gesture Retrieval ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a> and the alignment timestamps of each semantic gestures to construct a new instruction dataset to fine-tune the pre-trained generator <math id="S6.SS2.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S6.SS2.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S6.SS2.p2.3.m3.1.1" xref="S6.SS2.p2.3.m3.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.3.m3.1b"><ci id="S6.SS2.p2.3.m3.1.1.cmml" xref="S6.SS2.p2.3.m3.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.3.m3.1c">\mathcal{G}</annotation></semantics></math>. The instruction dataset consists of the speech audio synthesized by a Text-to-Speech (TTS) tool <cite class="ltx_cite ltx_citemacro_citep">(Murf.AI, <a href="#bib.bib60" title="" class="ltx_ref">2023</a>)</cite>, semantic gesture tokens encoded by <math id="S6.SS2.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{E}_{{\text{VQ}}}" display="inline"><semantics id="S6.SS2.p2.4.m4.1a"><msub id="S6.SS2.p2.4.m4.1.1" xref="S6.SS2.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S6.SS2.p2.4.m4.1.1.2" xref="S6.SS2.p2.4.m4.1.1.2.cmml">ℰ</mi><mtext id="S6.SS2.p2.4.m4.1.1.3" xref="S6.SS2.p2.4.m4.1.1.3a.cmml">VQ</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.4.m4.1b"><apply id="S6.SS2.p2.4.m4.1.1.cmml" xref="S6.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S6.SS2.p2.4.m4.1.1.1.cmml" xref="S6.SS2.p2.4.m4.1.1">subscript</csymbol><ci id="S6.SS2.p2.4.m4.1.1.2.cmml" xref="S6.SS2.p2.4.m4.1.1.2">ℰ</ci><ci id="S6.SS2.p2.4.m4.1.1.3a.cmml" xref="S6.SS2.p2.4.m4.1.1.3"><mtext mathsize="70%" id="S6.SS2.p2.4.m4.1.1.3.cmml" xref="S6.SS2.p2.4.m4.1.1.3">VQ</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.4.m4.1c">\mathcal{E}_{{\text{VQ}}}</annotation></semantics></math>, and their alignment timestamps. When fine-tuning, the generator takes the synthesized speech audio as input and generates the synchronized gestures, where we only optimize the cross-entropy loss at the merging interval for the target semantic gesture tokens. It could implicitly align the generator with the distribution of semantic gestures and make a preparation for the subsequent explicit merging.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Evaluation</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this section, we initially outline our system’s setup, followed by an evaluation of our results. We then compare these results with those from other systems, discuss potential applications, and validate our framework’s various design choices through an ablation study.</p>
</div>
<figure id="S7.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2405.09814/assets/x11.png" id="S7.F11.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="258" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F11.2.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>. </span><span id="S7.F11.3.2" class="ltx_text" style="font-size:90%;">Semantics-aware gestures synthesized by our system conditioned on results of the semantic gesture retrieval and the synchronized speech. The character performs a variety of semantic gestures, including significant body movements, reasonable arm swings, and delicate finger gesticulations.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S7.F11.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<figure id="S7.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2405.09814/assets/x12.png" id="S7.F12.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="217" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F12.2.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>. </span><span id="S7.F12.3.2" class="ltx_text" style="font-size:90%;">Qualitative comparison between synthesis without semantics-aware alignment (first row) and synthesis with alignment (second row).</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S7.F12.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>System Setup</h3>

<section id="S7.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.1. </span>Speech-Gesture Datasets</h4>

<div id="S7.SS1.SSS1.p1" class="ltx_para">
<p id="S7.SS1.SSS1.p1.1" class="ltx_p">As for the speech-gesture datasets, our system is trained and evaluated on two high-quality speech-gesture datasets: <em id="S7.SS1.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">ZEGGS</em> <cite class="ltx_cite ltx_citemacro_citep">(Ghorbani et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite> and <em id="S7.SS1.SSS1.p1.1.2" class="ltx_emph ltx_font_italic">BEAT</em> <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2022d</a>)</cite>. The ZEGGS Dataset comprises two hours of full-body motion capture and audio from monologues by an English-speaking female actor, performed in 19 distinct styles. Synchronized transcripts are obtained using an automatic speech recognition (ASR) tool <cite class="ltx_cite ltx_citemacro_citep">(Alibaba, <a href="#bib.bib4" title="" class="ltx_ref">2009</a>)</cite>. The BEAT dataset features about 76 hours of multimodal speech data, including audio, transcripts, and full-body motion capture from 30 speakers. These speakers performed in eight emotional styles across four languages. In alignment with the baseline setting in <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2022d</a>)</cite>, we selectively use the speech data of English speakers 2, 4, 6, and 8.</p>
</div>
</section>
<section id="S7.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.2. </span>Instruction Dataset for LLM Fine-Tuning</h4>

<div id="S7.SS1.SSS2.p1" class="ltx_para">
<p id="S7.SS1.SSS2.p1.1" class="ltx_p">To construct the annotation instruction dataset in Section <a href="#S5.SS2" title="5.2. LLM-Based Retrieval Model ‣ 5. Generative Semantic Gesture Retrieval ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>, we collect twenty 10<math id="S7.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S7.SS1.SSS2.p1.1.m1.1a"><mo id="S7.SS1.SSS2.p1.1.m1.1.1" xref="S7.SS1.SSS2.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS2.p1.1.m1.1b"><csymbol cd="latexml" id="S7.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S7.SS1.SSS2.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS2.p1.1.m1.1c">\sim</annotation></semantics></math>20-minute speech transcripts and annotate them manually. For gestures not annotated in this process, we use examples from SeG dataset and employ few-shot strategy to prompt GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib65" title="" class="ltx_ref">2023a</a>)</cite> in generating new annotated sentences containing them. This process mitigates the performance decline caused by the long-tail distribution of semantic gestures. When fine-tuning the LLM, there are a total of 5,410,940 training tokens for naive indexing identifier and 6,113,270 training tokens for semantics-aware indexing identifier, respectively.</p>
</div>
</section>
<section id="S7.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.3. </span>Settings</h4>

<div id="S7.SS1.SSS3.p1" class="ltx_para">
<p id="S7.SS1.SSS3.p1.20" class="ltx_p">Our system generates semantic-aware gestures at a rate of <math id="S7.SS1.SSS3.p1.1.m1.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S7.SS1.SSS3.p1.1.m1.1a"><mn id="S7.SS1.SSS3.p1.1.m1.1.1" xref="S7.SS1.SSS3.p1.1.m1.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.1.m1.1b"><cn type="integer" id="S7.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S7.SS1.SSS3.p1.1.m1.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.1.m1.1c">60</annotation></semantics></math> fps. In configuring the RVQ (Section <a href="#S4.SS1" title="4.1. Gesture Tokenizer ‣ 4. Co-Speech Gesture GPT Model ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>), we set the number of residual quantization layers to <math id="S7.SS1.SSS3.p1.2.m2.1" class="ltx_Math" alttext="R=4" display="inline"><semantics id="S7.SS1.SSS3.p1.2.m2.1a"><mrow id="S7.SS1.SSS3.p1.2.m2.1.1" xref="S7.SS1.SSS3.p1.2.m2.1.1.cmml"><mi id="S7.SS1.SSS3.p1.2.m2.1.1.2" xref="S7.SS1.SSS3.p1.2.m2.1.1.2.cmml">R</mi><mo id="S7.SS1.SSS3.p1.2.m2.1.1.1" xref="S7.SS1.SSS3.p1.2.m2.1.1.1.cmml">=</mo><mn id="S7.SS1.SSS3.p1.2.m2.1.1.3" xref="S7.SS1.SSS3.p1.2.m2.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.2.m2.1b"><apply id="S7.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S7.SS1.SSS3.p1.2.m2.1.1"><eq id="S7.SS1.SSS3.p1.2.m2.1.1.1.cmml" xref="S7.SS1.SSS3.p1.2.m2.1.1.1"></eq><ci id="S7.SS1.SSS3.p1.2.m2.1.1.2.cmml" xref="S7.SS1.SSS3.p1.2.m2.1.1.2">𝑅</ci><cn type="integer" id="S7.SS1.SSS3.p1.2.m2.1.1.3.cmml" xref="S7.SS1.SSS3.p1.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.2.m2.1c">R=4</annotation></semantics></math>, the downsampling rate to <math id="S7.SS1.SSS3.p1.3.m3.1" class="ltx_Math" alttext="d=8" display="inline"><semantics id="S7.SS1.SSS3.p1.3.m3.1a"><mrow id="S7.SS1.SSS3.p1.3.m3.1.1" xref="S7.SS1.SSS3.p1.3.m3.1.1.cmml"><mi id="S7.SS1.SSS3.p1.3.m3.1.1.2" xref="S7.SS1.SSS3.p1.3.m3.1.1.2.cmml">d</mi><mo id="S7.SS1.SSS3.p1.3.m3.1.1.1" xref="S7.SS1.SSS3.p1.3.m3.1.1.1.cmml">=</mo><mn id="S7.SS1.SSS3.p1.3.m3.1.1.3" xref="S7.SS1.SSS3.p1.3.m3.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.3.m3.1b"><apply id="S7.SS1.SSS3.p1.3.m3.1.1.cmml" xref="S7.SS1.SSS3.p1.3.m3.1.1"><eq id="S7.SS1.SSS3.p1.3.m3.1.1.1.cmml" xref="S7.SS1.SSS3.p1.3.m3.1.1.1"></eq><ci id="S7.SS1.SSS3.p1.3.m3.1.1.2.cmml" xref="S7.SS1.SSS3.p1.3.m3.1.1.2">𝑑</ci><cn type="integer" id="S7.SS1.SSS3.p1.3.m3.1.1.3.cmml" xref="S7.SS1.SSS3.p1.3.m3.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.3.m3.1c">d=8</annotation></semantics></math>, and the codebook size to <math id="S7.SS1.SSS3.p1.4.m4.1" class="ltx_Math" alttext="C=512" display="inline"><semantics id="S7.SS1.SSS3.p1.4.m4.1a"><mrow id="S7.SS1.SSS3.p1.4.m4.1.1" xref="S7.SS1.SSS3.p1.4.m4.1.1.cmml"><mi id="S7.SS1.SSS3.p1.4.m4.1.1.2" xref="S7.SS1.SSS3.p1.4.m4.1.1.2.cmml">C</mi><mo id="S7.SS1.SSS3.p1.4.m4.1.1.1" xref="S7.SS1.SSS3.p1.4.m4.1.1.1.cmml">=</mo><mn id="S7.SS1.SSS3.p1.4.m4.1.1.3" xref="S7.SS1.SSS3.p1.4.m4.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.4.m4.1b"><apply id="S7.SS1.SSS3.p1.4.m4.1.1.cmml" xref="S7.SS1.SSS3.p1.4.m4.1.1"><eq id="S7.SS1.SSS3.p1.4.m4.1.1.1.cmml" xref="S7.SS1.SSS3.p1.4.m4.1.1.1"></eq><ci id="S7.SS1.SSS3.p1.4.m4.1.1.2.cmml" xref="S7.SS1.SSS3.p1.4.m4.1.1.2">𝐶</ci><cn type="integer" id="S7.SS1.SSS3.p1.4.m4.1.1.3.cmml" xref="S7.SS1.SSS3.p1.4.m4.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.4.m4.1c">C=512</annotation></semantics></math>. This system is trained on both the SeG dataset and the speech-gesture datasets (Section <a href="#S7.SS1" title="7.1. System Setup ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.1</span></a>). The parameters <math id="S7.SS1.SSS3.p1.5.m5.1" class="ltx_Math" alttext="w_{1}" display="inline"><semantics id="S7.SS1.SSS3.p1.5.m5.1a"><msub id="S7.SS1.SSS3.p1.5.m5.1.1" xref="S7.SS1.SSS3.p1.5.m5.1.1.cmml"><mi id="S7.SS1.SSS3.p1.5.m5.1.1.2" xref="S7.SS1.SSS3.p1.5.m5.1.1.2.cmml">w</mi><mn id="S7.SS1.SSS3.p1.5.m5.1.1.3" xref="S7.SS1.SSS3.p1.5.m5.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.5.m5.1b"><apply id="S7.SS1.SSS3.p1.5.m5.1.1.cmml" xref="S7.SS1.SSS3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S7.SS1.SSS3.p1.5.m5.1.1.1.cmml" xref="S7.SS1.SSS3.p1.5.m5.1.1">subscript</csymbol><ci id="S7.SS1.SSS3.p1.5.m5.1.1.2.cmml" xref="S7.SS1.SSS3.p1.5.m5.1.1.2">𝑤</ci><cn type="integer" id="S7.SS1.SSS3.p1.5.m5.1.1.3.cmml" xref="S7.SS1.SSS3.p1.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.5.m5.1c">w_{1}</annotation></semantics></math>, <math id="S7.SS1.SSS3.p1.6.m6.1" class="ltx_Math" alttext="w_{2}" display="inline"><semantics id="S7.SS1.SSS3.p1.6.m6.1a"><msub id="S7.SS1.SSS3.p1.6.m6.1.1" xref="S7.SS1.SSS3.p1.6.m6.1.1.cmml"><mi id="S7.SS1.SSS3.p1.6.m6.1.1.2" xref="S7.SS1.SSS3.p1.6.m6.1.1.2.cmml">w</mi><mn id="S7.SS1.SSS3.p1.6.m6.1.1.3" xref="S7.SS1.SSS3.p1.6.m6.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.6.m6.1b"><apply id="S7.SS1.SSS3.p1.6.m6.1.1.cmml" xref="S7.SS1.SSS3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S7.SS1.SSS3.p1.6.m6.1.1.1.cmml" xref="S7.SS1.SSS3.p1.6.m6.1.1">subscript</csymbol><ci id="S7.SS1.SSS3.p1.6.m6.1.1.2.cmml" xref="S7.SS1.SSS3.p1.6.m6.1.1.2">𝑤</ci><cn type="integer" id="S7.SS1.SSS3.p1.6.m6.1.1.3.cmml" xref="S7.SS1.SSS3.p1.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.6.m6.1c">w_{2}</annotation></semantics></math>, <math id="S7.SS1.SSS3.p1.7.m7.1" class="ltx_Math" alttext="w_{3}" display="inline"><semantics id="S7.SS1.SSS3.p1.7.m7.1a"><msub id="S7.SS1.SSS3.p1.7.m7.1.1" xref="S7.SS1.SSS3.p1.7.m7.1.1.cmml"><mi id="S7.SS1.SSS3.p1.7.m7.1.1.2" xref="S7.SS1.SSS3.p1.7.m7.1.1.2.cmml">w</mi><mn id="S7.SS1.SSS3.p1.7.m7.1.1.3" xref="S7.SS1.SSS3.p1.7.m7.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.7.m7.1b"><apply id="S7.SS1.SSS3.p1.7.m7.1.1.cmml" xref="S7.SS1.SSS3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S7.SS1.SSS3.p1.7.m7.1.1.1.cmml" xref="S7.SS1.SSS3.p1.7.m7.1.1">subscript</csymbol><ci id="S7.SS1.SSS3.p1.7.m7.1.1.2.cmml" xref="S7.SS1.SSS3.p1.7.m7.1.1.2">𝑤</ci><cn type="integer" id="S7.SS1.SSS3.p1.7.m7.1.1.3.cmml" xref="S7.SS1.SSS3.p1.7.m7.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.7.m7.1c">w_{3}</annotation></semantics></math>, and <math id="S7.SS1.SSS3.p1.8.m8.1" class="ltx_Math" alttext="w_{4}" display="inline"><semantics id="S7.SS1.SSS3.p1.8.m8.1a"><msub id="S7.SS1.SSS3.p1.8.m8.1.1" xref="S7.SS1.SSS3.p1.8.m8.1.1.cmml"><mi id="S7.SS1.SSS3.p1.8.m8.1.1.2" xref="S7.SS1.SSS3.p1.8.m8.1.1.2.cmml">w</mi><mn id="S7.SS1.SSS3.p1.8.m8.1.1.3" xref="S7.SS1.SSS3.p1.8.m8.1.1.3.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.8.m8.1b"><apply id="S7.SS1.SSS3.p1.8.m8.1.1.cmml" xref="S7.SS1.SSS3.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S7.SS1.SSS3.p1.8.m8.1.1.1.cmml" xref="S7.SS1.SSS3.p1.8.m8.1.1">subscript</csymbol><ci id="S7.SS1.SSS3.p1.8.m8.1.1.2.cmml" xref="S7.SS1.SSS3.p1.8.m8.1.1.2">𝑤</ci><cn type="integer" id="S7.SS1.SSS3.p1.8.m8.1.1.3.cmml" xref="S7.SS1.SSS3.p1.8.m8.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.8.m8.1c">w_{4}</annotation></semantics></math> in the loss function are assigned values of <math id="S7.SS1.SSS3.p1.9.m9.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S7.SS1.SSS3.p1.9.m9.1a"><mn id="S7.SS1.SSS3.p1.9.m9.1.1" xref="S7.SS1.SSS3.p1.9.m9.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.9.m9.1b"><cn type="integer" id="S7.SS1.SSS3.p1.9.m9.1.1.cmml" xref="S7.SS1.SSS3.p1.9.m9.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.9.m9.1c">1</annotation></semantics></math>, <math id="S7.SS1.SSS3.p1.10.m10.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S7.SS1.SSS3.p1.10.m10.1a"><mn id="S7.SS1.SSS3.p1.10.m10.1.1" xref="S7.SS1.SSS3.p1.10.m10.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.10.m10.1b"><cn type="integer" id="S7.SS1.SSS3.p1.10.m10.1.1.cmml" xref="S7.SS1.SSS3.p1.10.m10.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.10.m10.1c">1</annotation></semantics></math>, <math id="S7.SS1.SSS3.p1.11.m11.1" class="ltx_Math" alttext="0.02" display="inline"><semantics id="S7.SS1.SSS3.p1.11.m11.1a"><mn id="S7.SS1.SSS3.p1.11.m11.1.1" xref="S7.SS1.SSS3.p1.11.m11.1.1.cmml">0.02</mn><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.11.m11.1b"><cn type="float" id="S7.SS1.SSS3.p1.11.m11.1.1.cmml" xref="S7.SS1.SSS3.p1.11.m11.1.1">0.02</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.11.m11.1c">0.02</annotation></semantics></math>, and <math id="S7.SS1.SSS3.p1.12.m12.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S7.SS1.SSS3.p1.12.m12.1a"><mn id="S7.SS1.SSS3.p1.12.m12.1.1" xref="S7.SS1.SSS3.p1.12.m12.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.12.m12.1b"><cn type="integer" id="S7.SS1.SSS3.p1.12.m12.1.1.cmml" xref="S7.SS1.SSS3.p1.12.m12.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.12.m12.1c">1</annotation></semantics></math>, respectively. Regarding the Gesture Generator <math id="S7.SS1.SSS3.p1.13.m13.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S7.SS1.SSS3.p1.13.m13.1a"><mi class="ltx_font_mathcaligraphic" id="S7.SS1.SSS3.p1.13.m13.1.1" xref="S7.SS1.SSS3.p1.13.m13.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.13.m13.1b"><ci id="S7.SS1.SSS3.p1.13.m13.1.1.cmml" xref="S7.SS1.SSS3.p1.13.m13.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.13.m13.1c">\mathcal{G}</annotation></semantics></math> (Section <a href="#S4.SS2" title="4.2. Gesture Generator ‣ 4. Co-Speech Gesture GPT Model ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>), it is composed of a <math id="S7.SS1.SSS3.p1.14.m14.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S7.SS1.SSS3.p1.14.m14.1a"><mn id="S7.SS1.SSS3.p1.14.m14.1.1" xref="S7.SS1.SSS3.p1.14.m14.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.14.m14.1b"><cn type="integer" id="S7.SS1.SSS3.p1.14.m14.1.1.cmml" xref="S7.SS1.SSS3.p1.14.m14.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.14.m14.1c">12</annotation></semantics></math>-layer transformer with a width of <math id="S7.SS1.SSS3.p1.15.m15.1" class="ltx_Math" alttext="768" display="inline"><semantics id="S7.SS1.SSS3.p1.15.m15.1a"><mn id="S7.SS1.SSS3.p1.15.m15.1.1" xref="S7.SS1.SSS3.p1.15.m15.1.1.cmml">768</mn><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.15.m15.1b"><cn type="integer" id="S7.SS1.SSS3.p1.15.m15.1.1.cmml" xref="S7.SS1.SSS3.p1.15.m15.1.1">768</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.15.m15.1c">768</annotation></semantics></math> features and is trained exclusively on speech-gesture datasets. During training, the RVQ is trained using standard motion clips of <math id="S7.SS1.SSS3.p1.16.m16.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S7.SS1.SSS3.p1.16.m16.1a"><mn id="S7.SS1.SSS3.p1.16.m16.1.1" xref="S7.SS1.SSS3.p1.16.m16.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.16.m16.1b"><cn type="integer" id="S7.SS1.SSS3.p1.16.m16.1.1.cmml" xref="S7.SS1.SSS3.p1.16.m16.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.16.m16.1c">2</annotation></semantics></math> seconds in length, whereas <math id="S7.SS1.SSS3.p1.17.m17.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S7.SS1.SSS3.p1.17.m17.1a"><mi class="ltx_font_mathcaligraphic" id="S7.SS1.SSS3.p1.17.m17.1.1" xref="S7.SS1.SSS3.p1.17.m17.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.17.m17.1b"><ci id="S7.SS1.SSS3.p1.17.m17.1.1.cmml" xref="S7.SS1.SSS3.p1.17.m17.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.17.m17.1c">\mathcal{G}</annotation></semantics></math> is trained with paired speech and motion clips of <math id="S7.SS1.SSS3.p1.18.m18.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S7.SS1.SSS3.p1.18.m18.1a"><mn id="S7.SS1.SSS3.p1.18.m18.1.1" xref="S7.SS1.SSS3.p1.18.m18.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.18.m18.1b"><cn type="integer" id="S7.SS1.SSS3.p1.18.m18.1.1.cmml" xref="S7.SS1.SSS3.p1.18.m18.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.18.m18.1c">4</annotation></semantics></math> seconds. All the models are trained using four NVIDIA 3090Ti GPUs, taking approximately two days for the ZEGGS dataset and five days for the BEAT dataset. For LLM fine-tuning, we train <math id="S7.SS1.SSS3.p1.19.m19.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S7.SS1.SSS3.p1.19.m19.1a"><mn id="S7.SS1.SSS3.p1.19.m19.1.1" xref="S7.SS1.SSS3.p1.19.m19.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.19.m19.1b"><cn type="integer" id="S7.SS1.SSS3.p1.19.m19.1.1.cmml" xref="S7.SS1.SSS3.p1.19.m19.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.19.m19.1c">5</annotation></semantics></math> epochs and set batch size to <math id="S7.SS1.SSS3.p1.20.m20.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S7.SS1.SSS3.p1.20.m20.1a"><mn id="S7.SS1.SSS3.p1.20.m20.1.1" xref="S7.SS1.SSS3.p1.20.m20.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S7.SS1.SSS3.p1.20.m20.1b"><cn type="integer" id="S7.SS1.SSS3.p1.20.m20.1.1.cmml" xref="S7.SS1.SSS3.p1.20.m20.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.SSS3.p1.20.m20.1c">2</annotation></semantics></math>, taking approximately one hour.</p>
</div>
</section>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2. </span>Results</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">Figure <a href="#S7.F11" title="Figure 11 ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> shows the visualization results of our system generating gestures conditioned on the semantic gesture retrieval outcomes and synchronized speech. The generator <math id="S7.SS2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S7.SS2.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S7.SS2.p1.1.m1.1.1" xref="S7.SS2.p1.1.m1.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S7.SS2.p1.1.m1.1b"><ci id="S7.SS2.p1.1.m1.1.1.cmml" xref="S7.SS2.p1.1.m1.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.p1.1.m1.1c">\mathcal{G}</annotation></semantics></math> employs top-5 sampling methods for token generation during inference and we utilize a physical tracking approach from <cite class="ltx_cite ltx_citemacro_citep">(Yao
et al<span class="ltx_text">.</span>, <a href="#bib.bib92" title="" class="ltx_ref">2023</a>)</cite> to alleviate the issue of sliding foot in our results. Our system successfully creates realistic gestures that accurately convey the intended meanings, aligning with the respective retrieval results. The character performs a range of semantic gestures, including natural body movements, reasonable arm swings, and delicate finger gesticulations. For example, the character performs suitable pointing gestures when “elephant” and “you” are mentioned. In the meanwhile, “uproot” and “book” are expressively portrayed. For abstract concepts like ”strength,” the character vividly illustrates them by gesturing towards the biceps. In the supplementary video, when the word “two” is mentioned, the character naturally makes a “V-sign” gesture. This demonstrates that the retrieval model can accurately capture the specific meanings of each semantic gesture in the SeG dataset.</p>
</div>
<div id="S7.SS2.p2" class="ltx_para">
<p id="S7.SS2.p2.1" class="ltx_p">To illustrate the significance of the semantics-aware alignment module, Figure <a href="#S7.F12" title="Figure 12 ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> presents the comparative visualization results of synthesis without semantics-aware alignment (the first row) against synthesis with such alignment (the second row). The communicative efficacy of the generated gestures is enhanced through the explicit integration of contextually appropriate, meaningful gestures based on the speech transcript.</p>
</div>
<figure id="S7.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S7.T1.38.5.1" class="ltx_text" style="font-size:90%;">Table 1</span>. </span><span id="S7.T1.8.4" class="ltx_text" style="font-size:90%;">Average scores of user study with <math id="S7.T1.5.1.m1.1" class="ltx_Math" alttext="95\%" display="inline"><semantics id="S7.T1.5.1.m1.1b"><mrow id="S7.T1.5.1.m1.1.1" xref="S7.T1.5.1.m1.1.1.cmml"><mn id="S7.T1.5.1.m1.1.1.2" xref="S7.T1.5.1.m1.1.1.2.cmml">95</mn><mo id="S7.T1.5.1.m1.1.1.1" xref="S7.T1.5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.5.1.m1.1c"><apply id="S7.T1.5.1.m1.1.1.cmml" xref="S7.T1.5.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.5.1.m1.1.1.1.cmml" xref="S7.T1.5.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S7.T1.5.1.m1.1.1.2.cmml" xref="S7.T1.5.1.m1.1.1.2">95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.5.1.m1.1d">95\%</annotation></semantics></math> confidence intervals. <em id="S7.T1.8.4.1" class="ltx_emph ltx_font_italic">Our system without semantic alignment (w/o semantic alignment)</em> excludes the semantics-aware alignment module for the pre-trained generator. Asterisks indicate the significant effects (<math id="S7.T1.6.2.m2.2" class="ltx_math_unparsed" alttext="*:p&lt;0.05" display="inline"><semantics id="S7.T1.6.2.m2.2b"><mrow id="S7.T1.6.2.m2.2c"><mo rspace="0em" id="S7.T1.6.2.m2.1.1">∗</mo><mo rspace="0.278em" id="S7.T1.6.2.m2.2.2">:</mo><mi id="S7.T1.6.2.m2.2.3">p</mi><mo id="S7.T1.6.2.m2.2.4">&lt;</mo><mn id="S7.T1.6.2.m2.2.5">0.05</mn></mrow><annotation encoding="application/x-tex" id="S7.T1.6.2.m2.2d">*:p&lt;0.05</annotation></semantics></math>, <math id="S7.T1.7.3.m3.3" class="ltx_math_unparsed" alttext="**:p&lt;0.01" display="inline"><semantics id="S7.T1.7.3.m3.3b"><mrow id="S7.T1.7.3.m3.3c"><mo rspace="0em" id="S7.T1.7.3.m3.1.1">∗</mo><mo lspace="0em" rspace="0em" id="S7.T1.7.3.m3.2.2">∗</mo><mo rspace="0.278em" id="S7.T1.7.3.m3.3.3">:</mo><mi id="S7.T1.7.3.m3.3.4">p</mi><mo id="S7.T1.7.3.m3.3.5">&lt;</mo><mn id="S7.T1.7.3.m3.3.6">0.01</mn></mrow><annotation encoding="application/x-tex" id="S7.T1.7.3.m3.3d">**:p&lt;0.01</annotation></semantics></math>, <math id="S7.T1.8.4.m4.4" class="ltx_math_unparsed" alttext="***:p&lt;0.001" display="inline"><semantics id="S7.T1.8.4.m4.4b"><mrow id="S7.T1.8.4.m4.4c"><mo rspace="0em" id="S7.T1.8.4.m4.1.1">∗</mo><mo lspace="0em" rspace="0em" id="S7.T1.8.4.m4.2.2">∗</mo><mo lspace="0em" rspace="0em" id="S7.T1.8.4.m4.3.3">∗</mo><mo rspace="0.278em" id="S7.T1.8.4.m4.4.4">:</mo><mi id="S7.T1.8.4.m4.4.5">p</mi><mo id="S7.T1.8.4.m4.4.6">&lt;</mo><mn id="S7.T1.8.4.m4.4.7">0.001</mn></mrow><annotation encoding="application/x-tex" id="S7.T1.8.4.m4.4d">***:p&lt;0.001</annotation></semantics></math>).</span></figcaption>
<table id="S7.T1.35" class="ltx_tabular ltx_align_middle">
<tr id="S7.T1.11.3" class="ltx_tr">
<td id="S7.T1.11.3.4" class="ltx_td ltx_align_left ltx_border_tt">Dataset</td>
<td id="S7.T1.11.3.5" class="ltx_td ltx_align_left ltx_border_tt">System</td>
<td id="S7.T1.9.1.1" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S7.T1.9.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.9.1.1.1.1" class="ltx_p">Human Likeness <math id="S7.T1.9.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T1.9.1.1.1.1.m1.1a"><mo stretchy="false" id="S7.T1.9.1.1.1.1.m1.1.1" xref="S7.T1.9.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T1.9.1.1.1.1.m1.1b"><ci id="S7.T1.9.1.1.1.1.m1.1.1.cmml" xref="S7.T1.9.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.9.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T1.10.2.2" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S7.T1.10.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.10.2.2.1.1" class="ltx_p">Beat Matching <math id="S7.T1.10.2.2.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T1.10.2.2.1.1.m1.1a"><mo stretchy="false" id="S7.T1.10.2.2.1.1.m1.1.1" xref="S7.T1.10.2.2.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T1.10.2.2.1.1.m1.1b"><ci id="S7.T1.10.2.2.1.1.m1.1.1.cmml" xref="S7.T1.10.2.2.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.10.2.2.1.1.m1.1c">\uparrow</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T1.11.3.3" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S7.T1.11.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.11.3.3.1.1" class="ltx_p">Semantic Accuracy <math id="S7.T1.11.3.3.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T1.11.3.3.1.1.m1.1a"><mo stretchy="false" id="S7.T1.11.3.3.1.1.m1.1.1" xref="S7.T1.11.3.3.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T1.11.3.3.1.1.m1.1b"><ci id="S7.T1.11.3.3.1.1.m1.1.1.cmml" xref="S7.T1.11.3.3.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.11.3.3.1.1.m1.1c">\uparrow</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S7.T1.14.6" class="ltx_tr">
<td id="S7.T1.14.6.4" class="ltx_td ltx_align_left ltx_border_tt" rowspan="4"><span id="S7.T1.14.6.4.1" class="ltx_text">ZEGGS</span></td>
<td id="S7.T1.14.6.5" class="ltx_td ltx_align_left ltx_border_tt">GT</td>
<td id="S7.T1.12.4.1" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S7.T1.12.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.12.4.1.1.1" class="ltx_p"><math id="S7.T1.12.4.1.1.1.m1.1" class="ltx_Math" alttext="0.07\pm 0.02^{*}" display="inline"><semantics id="S7.T1.12.4.1.1.1.m1.1a"><mrow id="S7.T1.12.4.1.1.1.m1.1.1" xref="S7.T1.12.4.1.1.1.m1.1.1.cmml"><mn id="S7.T1.12.4.1.1.1.m1.1.1.2" xref="S7.T1.12.4.1.1.1.m1.1.1.2.cmml">0.07</mn><mo id="S7.T1.12.4.1.1.1.m1.1.1.1" xref="S7.T1.12.4.1.1.1.m1.1.1.1.cmml">±</mo><msup id="S7.T1.12.4.1.1.1.m1.1.1.3" xref="S7.T1.12.4.1.1.1.m1.1.1.3.cmml"><mn id="S7.T1.12.4.1.1.1.m1.1.1.3.2" xref="S7.T1.12.4.1.1.1.m1.1.1.3.2.cmml">0.02</mn><mo id="S7.T1.12.4.1.1.1.m1.1.1.3.3" xref="S7.T1.12.4.1.1.1.m1.1.1.3.3.cmml">∗</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.12.4.1.1.1.m1.1b"><apply id="S7.T1.12.4.1.1.1.m1.1.1.cmml" xref="S7.T1.12.4.1.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.12.4.1.1.1.m1.1.1.1.cmml" xref="S7.T1.12.4.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T1.12.4.1.1.1.m1.1.1.2.cmml" xref="S7.T1.12.4.1.1.1.m1.1.1.2">0.07</cn><apply id="S7.T1.12.4.1.1.1.m1.1.1.3.cmml" xref="S7.T1.12.4.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.T1.12.4.1.1.1.m1.1.1.3.1.cmml" xref="S7.T1.12.4.1.1.1.m1.1.1.3">superscript</csymbol><cn type="float" id="S7.T1.12.4.1.1.1.m1.1.1.3.2.cmml" xref="S7.T1.12.4.1.1.1.m1.1.1.3.2">0.02</cn><times id="S7.T1.12.4.1.1.1.m1.1.1.3.3.cmml" xref="S7.T1.12.4.1.1.1.m1.1.1.3.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.12.4.1.1.1.m1.1c">0.07\pm 0.02^{*}</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T1.13.5.2" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S7.T1.13.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.13.5.2.1.1" class="ltx_p"><math id="S7.T1.13.5.2.1.1.m1.1" class="ltx_Math" alttext="0.15\pm 0.03^{*}" display="inline"><semantics id="S7.T1.13.5.2.1.1.m1.1a"><mrow id="S7.T1.13.5.2.1.1.m1.1.1" xref="S7.T1.13.5.2.1.1.m1.1.1.cmml"><mn id="S7.T1.13.5.2.1.1.m1.1.1.2" xref="S7.T1.13.5.2.1.1.m1.1.1.2.cmml">0.15</mn><mo id="S7.T1.13.5.2.1.1.m1.1.1.1" xref="S7.T1.13.5.2.1.1.m1.1.1.1.cmml">±</mo><msup id="S7.T1.13.5.2.1.1.m1.1.1.3" xref="S7.T1.13.5.2.1.1.m1.1.1.3.cmml"><mn id="S7.T1.13.5.2.1.1.m1.1.1.3.2" xref="S7.T1.13.5.2.1.1.m1.1.1.3.2.cmml">0.03</mn><mo id="S7.T1.13.5.2.1.1.m1.1.1.3.3" xref="S7.T1.13.5.2.1.1.m1.1.1.3.3.cmml">∗</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.13.5.2.1.1.m1.1b"><apply id="S7.T1.13.5.2.1.1.m1.1.1.cmml" xref="S7.T1.13.5.2.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.13.5.2.1.1.m1.1.1.1.cmml" xref="S7.T1.13.5.2.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T1.13.5.2.1.1.m1.1.1.2.cmml" xref="S7.T1.13.5.2.1.1.m1.1.1.2">0.15</cn><apply id="S7.T1.13.5.2.1.1.m1.1.1.3.cmml" xref="S7.T1.13.5.2.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.T1.13.5.2.1.1.m1.1.1.3.1.cmml" xref="S7.T1.13.5.2.1.1.m1.1.1.3">superscript</csymbol><cn type="float" id="S7.T1.13.5.2.1.1.m1.1.1.3.2.cmml" xref="S7.T1.13.5.2.1.1.m1.1.1.3.2">0.03</cn><times id="S7.T1.13.5.2.1.1.m1.1.1.3.3.cmml" xref="S7.T1.13.5.2.1.1.m1.1.1.3.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.13.5.2.1.1.m1.1c">0.15\pm 0.03^{*}</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T1.14.6.3" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S7.T1.14.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.14.6.3.1.1" class="ltx_p"><math id="S7.T1.14.6.3.1.1.m1.1" class="ltx_Math" alttext="0.51\pm 0.08" display="inline"><semantics id="S7.T1.14.6.3.1.1.m1.1a"><mrow id="S7.T1.14.6.3.1.1.m1.1.1" xref="S7.T1.14.6.3.1.1.m1.1.1.cmml"><mn id="S7.T1.14.6.3.1.1.m1.1.1.2" xref="S7.T1.14.6.3.1.1.m1.1.1.2.cmml">0.51</mn><mo id="S7.T1.14.6.3.1.1.m1.1.1.1" xref="S7.T1.14.6.3.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T1.14.6.3.1.1.m1.1.1.3" xref="S7.T1.14.6.3.1.1.m1.1.1.3.cmml">0.08</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.14.6.3.1.1.m1.1b"><apply id="S7.T1.14.6.3.1.1.m1.1.1.cmml" xref="S7.T1.14.6.3.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.14.6.3.1.1.m1.1.1.1.cmml" xref="S7.T1.14.6.3.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T1.14.6.3.1.1.m1.1.1.2.cmml" xref="S7.T1.14.6.3.1.1.m1.1.1.2">0.51</cn><cn type="float" id="S7.T1.14.6.3.1.1.m1.1.1.3.cmml" xref="S7.T1.14.6.3.1.1.m1.1.1.3">0.08</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.14.6.3.1.1.m1.1c">0.51\pm 0.08</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S7.T1.17.9" class="ltx_tr">
<td id="S7.T1.17.9.4" class="ltx_td ltx_align_left ltx_border_t">GestureDiffuCLIP</td>
<td id="S7.T1.15.7.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S7.T1.15.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.15.7.1.1.1" class="ltx_p"><math id="S7.T1.15.7.1.1.1.m1.1" class="ltx_Math" alttext="-0.02\pm 0.01^{*}" display="inline"><semantics id="S7.T1.15.7.1.1.1.m1.1a"><mrow id="S7.T1.15.7.1.1.1.m1.1.1" xref="S7.T1.15.7.1.1.1.m1.1.1.cmml"><mrow id="S7.T1.15.7.1.1.1.m1.1.1.2" xref="S7.T1.15.7.1.1.1.m1.1.1.2.cmml"><mo id="S7.T1.15.7.1.1.1.m1.1.1.2a" xref="S7.T1.15.7.1.1.1.m1.1.1.2.cmml">−</mo><mn id="S7.T1.15.7.1.1.1.m1.1.1.2.2" xref="S7.T1.15.7.1.1.1.m1.1.1.2.2.cmml">0.02</mn></mrow><mo id="S7.T1.15.7.1.1.1.m1.1.1.1" xref="S7.T1.15.7.1.1.1.m1.1.1.1.cmml">±</mo><msup id="S7.T1.15.7.1.1.1.m1.1.1.3" xref="S7.T1.15.7.1.1.1.m1.1.1.3.cmml"><mn id="S7.T1.15.7.1.1.1.m1.1.1.3.2" xref="S7.T1.15.7.1.1.1.m1.1.1.3.2.cmml">0.01</mn><mo id="S7.T1.15.7.1.1.1.m1.1.1.3.3" xref="S7.T1.15.7.1.1.1.m1.1.1.3.3.cmml">∗</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.15.7.1.1.1.m1.1b"><apply id="S7.T1.15.7.1.1.1.m1.1.1.cmml" xref="S7.T1.15.7.1.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.15.7.1.1.1.m1.1.1.1.cmml" xref="S7.T1.15.7.1.1.1.m1.1.1.1">plus-or-minus</csymbol><apply id="S7.T1.15.7.1.1.1.m1.1.1.2.cmml" xref="S7.T1.15.7.1.1.1.m1.1.1.2"><minus id="S7.T1.15.7.1.1.1.m1.1.1.2.1.cmml" xref="S7.T1.15.7.1.1.1.m1.1.1.2"></minus><cn type="float" id="S7.T1.15.7.1.1.1.m1.1.1.2.2.cmml" xref="S7.T1.15.7.1.1.1.m1.1.1.2.2">0.02</cn></apply><apply id="S7.T1.15.7.1.1.1.m1.1.1.3.cmml" xref="S7.T1.15.7.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.T1.15.7.1.1.1.m1.1.1.3.1.cmml" xref="S7.T1.15.7.1.1.1.m1.1.1.3">superscript</csymbol><cn type="float" id="S7.T1.15.7.1.1.1.m1.1.1.3.2.cmml" xref="S7.T1.15.7.1.1.1.m1.1.1.3.2">0.01</cn><times id="S7.T1.15.7.1.1.1.m1.1.1.3.3.cmml" xref="S7.T1.15.7.1.1.1.m1.1.1.3.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.15.7.1.1.1.m1.1c">-0.02\pm 0.01^{*}</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T1.16.8.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S7.T1.16.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.16.8.2.1.1" class="ltx_p"><math id="S7.T1.16.8.2.1.1.m1.1" class="ltx_Math" alttext="-0.05\pm 0.01" display="inline"><semantics id="S7.T1.16.8.2.1.1.m1.1a"><mrow id="S7.T1.16.8.2.1.1.m1.1.1" xref="S7.T1.16.8.2.1.1.m1.1.1.cmml"><mrow id="S7.T1.16.8.2.1.1.m1.1.1.2" xref="S7.T1.16.8.2.1.1.m1.1.1.2.cmml"><mo id="S7.T1.16.8.2.1.1.m1.1.1.2a" xref="S7.T1.16.8.2.1.1.m1.1.1.2.cmml">−</mo><mn id="S7.T1.16.8.2.1.1.m1.1.1.2.2" xref="S7.T1.16.8.2.1.1.m1.1.1.2.2.cmml">0.05</mn></mrow><mo id="S7.T1.16.8.2.1.1.m1.1.1.1" xref="S7.T1.16.8.2.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T1.16.8.2.1.1.m1.1.1.3" xref="S7.T1.16.8.2.1.1.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.16.8.2.1.1.m1.1b"><apply id="S7.T1.16.8.2.1.1.m1.1.1.cmml" xref="S7.T1.16.8.2.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.16.8.2.1.1.m1.1.1.1.cmml" xref="S7.T1.16.8.2.1.1.m1.1.1.1">plus-or-minus</csymbol><apply id="S7.T1.16.8.2.1.1.m1.1.1.2.cmml" xref="S7.T1.16.8.2.1.1.m1.1.1.2"><minus id="S7.T1.16.8.2.1.1.m1.1.1.2.1.cmml" xref="S7.T1.16.8.2.1.1.m1.1.1.2"></minus><cn type="float" id="S7.T1.16.8.2.1.1.m1.1.1.2.2.cmml" xref="S7.T1.16.8.2.1.1.m1.1.1.2.2">0.05</cn></apply><cn type="float" id="S7.T1.16.8.2.1.1.m1.1.1.3.cmml" xref="S7.T1.16.8.2.1.1.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.16.8.2.1.1.m1.1c">-0.05\pm 0.01</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T1.17.9.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S7.T1.17.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.17.9.3.1.1" class="ltx_p"><math id="S7.T1.17.9.3.1.1.m1.1" class="ltx_Math" alttext="-0.15\pm 0.10^{**}" display="inline"><semantics id="S7.T1.17.9.3.1.1.m1.1a"><mrow id="S7.T1.17.9.3.1.1.m1.1.1" xref="S7.T1.17.9.3.1.1.m1.1.1.cmml"><mrow id="S7.T1.17.9.3.1.1.m1.1.1.2" xref="S7.T1.17.9.3.1.1.m1.1.1.2.cmml"><mo id="S7.T1.17.9.3.1.1.m1.1.1.2a" xref="S7.T1.17.9.3.1.1.m1.1.1.2.cmml">−</mo><mn id="S7.T1.17.9.3.1.1.m1.1.1.2.2" xref="S7.T1.17.9.3.1.1.m1.1.1.2.2.cmml">0.15</mn></mrow><mo id="S7.T1.17.9.3.1.1.m1.1.1.1" xref="S7.T1.17.9.3.1.1.m1.1.1.1.cmml">±</mo><msup id="S7.T1.17.9.3.1.1.m1.1.1.3" xref="S7.T1.17.9.3.1.1.m1.1.1.3.cmml"><mn id="S7.T1.17.9.3.1.1.m1.1.1.3.2" xref="S7.T1.17.9.3.1.1.m1.1.1.3.2.cmml">0.10</mn><mrow id="S7.T1.17.9.3.1.1.m1.1.1.3.3" xref="S7.T1.17.9.3.1.1.m1.1.1.3.3.cmml"><mi id="S7.T1.17.9.3.1.1.m1.1.1.3.3.2" xref="S7.T1.17.9.3.1.1.m1.1.1.3.3.2.cmml"></mi><mo lspace="0.222em" rspace="0em" id="S7.T1.17.9.3.1.1.m1.1.1.3.3.1" xref="S7.T1.17.9.3.1.1.m1.1.1.3.3.1.cmml">∗</mo><mo lspace="0em" id="S7.T1.17.9.3.1.1.m1.1.1.3.3.3" xref="S7.T1.17.9.3.1.1.m1.1.1.3.3.3.cmml">∗</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.17.9.3.1.1.m1.1b"><apply id="S7.T1.17.9.3.1.1.m1.1.1.cmml" xref="S7.T1.17.9.3.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.17.9.3.1.1.m1.1.1.1.cmml" xref="S7.T1.17.9.3.1.1.m1.1.1.1">plus-or-minus</csymbol><apply id="S7.T1.17.9.3.1.1.m1.1.1.2.cmml" xref="S7.T1.17.9.3.1.1.m1.1.1.2"><minus id="S7.T1.17.9.3.1.1.m1.1.1.2.1.cmml" xref="S7.T1.17.9.3.1.1.m1.1.1.2"></minus><cn type="float" id="S7.T1.17.9.3.1.1.m1.1.1.2.2.cmml" xref="S7.T1.17.9.3.1.1.m1.1.1.2.2">0.15</cn></apply><apply id="S7.T1.17.9.3.1.1.m1.1.1.3.cmml" xref="S7.T1.17.9.3.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.T1.17.9.3.1.1.m1.1.1.3.1.cmml" xref="S7.T1.17.9.3.1.1.m1.1.1.3">superscript</csymbol><cn type="float" id="S7.T1.17.9.3.1.1.m1.1.1.3.2.cmml" xref="S7.T1.17.9.3.1.1.m1.1.1.3.2">0.10</cn><apply id="S7.T1.17.9.3.1.1.m1.1.1.3.3.cmml" xref="S7.T1.17.9.3.1.1.m1.1.1.3.3"><times id="S7.T1.17.9.3.1.1.m1.1.1.3.3.1.cmml" xref="S7.T1.17.9.3.1.1.m1.1.1.3.3.1"></times><csymbol cd="latexml" id="S7.T1.17.9.3.1.1.m1.1.1.3.3.2.cmml" xref="S7.T1.17.9.3.1.1.m1.1.1.3.3.2">absent</csymbol><times id="S7.T1.17.9.3.1.1.m1.1.1.3.3.3.cmml" xref="S7.T1.17.9.3.1.1.m1.1.1.3.3.3"></times></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.17.9.3.1.1.m1.1c">-0.15\pm 0.10^{**}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S7.T1.20.12" class="ltx_tr">
<td id="S7.T1.20.12.4" class="ltx_td ltx_align_left">Ours (w/o semantic alignment)</td>
<td id="S7.T1.18.10.1" class="ltx_td ltx_align_justify">
<span id="S7.T1.18.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.18.10.1.1.1" class="ltx_p"><math id="S7.T1.18.10.1.1.1.m1.1" class="ltx_Math" alttext="-0.08\pm 0.03^{*}" display="inline"><semantics id="S7.T1.18.10.1.1.1.m1.1a"><mrow id="S7.T1.18.10.1.1.1.m1.1.1" xref="S7.T1.18.10.1.1.1.m1.1.1.cmml"><mrow id="S7.T1.18.10.1.1.1.m1.1.1.2" xref="S7.T1.18.10.1.1.1.m1.1.1.2.cmml"><mo id="S7.T1.18.10.1.1.1.m1.1.1.2a" xref="S7.T1.18.10.1.1.1.m1.1.1.2.cmml">−</mo><mn id="S7.T1.18.10.1.1.1.m1.1.1.2.2" xref="S7.T1.18.10.1.1.1.m1.1.1.2.2.cmml">0.08</mn></mrow><mo id="S7.T1.18.10.1.1.1.m1.1.1.1" xref="S7.T1.18.10.1.1.1.m1.1.1.1.cmml">±</mo><msup id="S7.T1.18.10.1.1.1.m1.1.1.3" xref="S7.T1.18.10.1.1.1.m1.1.1.3.cmml"><mn id="S7.T1.18.10.1.1.1.m1.1.1.3.2" xref="S7.T1.18.10.1.1.1.m1.1.1.3.2.cmml">0.03</mn><mo id="S7.T1.18.10.1.1.1.m1.1.1.3.3" xref="S7.T1.18.10.1.1.1.m1.1.1.3.3.cmml">∗</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.18.10.1.1.1.m1.1b"><apply id="S7.T1.18.10.1.1.1.m1.1.1.cmml" xref="S7.T1.18.10.1.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.18.10.1.1.1.m1.1.1.1.cmml" xref="S7.T1.18.10.1.1.1.m1.1.1.1">plus-or-minus</csymbol><apply id="S7.T1.18.10.1.1.1.m1.1.1.2.cmml" xref="S7.T1.18.10.1.1.1.m1.1.1.2"><minus id="S7.T1.18.10.1.1.1.m1.1.1.2.1.cmml" xref="S7.T1.18.10.1.1.1.m1.1.1.2"></minus><cn type="float" id="S7.T1.18.10.1.1.1.m1.1.1.2.2.cmml" xref="S7.T1.18.10.1.1.1.m1.1.1.2.2">0.08</cn></apply><apply id="S7.T1.18.10.1.1.1.m1.1.1.3.cmml" xref="S7.T1.18.10.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.T1.18.10.1.1.1.m1.1.1.3.1.cmml" xref="S7.T1.18.10.1.1.1.m1.1.1.3">superscript</csymbol><cn type="float" id="S7.T1.18.10.1.1.1.m1.1.1.3.2.cmml" xref="S7.T1.18.10.1.1.1.m1.1.1.3.2">0.03</cn><times id="S7.T1.18.10.1.1.1.m1.1.1.3.3.cmml" xref="S7.T1.18.10.1.1.1.m1.1.1.3.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.18.10.1.1.1.m1.1c">-0.08\pm 0.03^{*}</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T1.19.11.2" class="ltx_td ltx_align_justify">
<span id="S7.T1.19.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.19.11.2.1.1" class="ltx_p"><math id="S7.T1.19.11.2.1.1.m1.1" class="ltx_Math" alttext="-0.07\pm 0.02" display="inline"><semantics id="S7.T1.19.11.2.1.1.m1.1a"><mrow id="S7.T1.19.11.2.1.1.m1.1.1" xref="S7.T1.19.11.2.1.1.m1.1.1.cmml"><mrow id="S7.T1.19.11.2.1.1.m1.1.1.2" xref="S7.T1.19.11.2.1.1.m1.1.1.2.cmml"><mo id="S7.T1.19.11.2.1.1.m1.1.1.2a" xref="S7.T1.19.11.2.1.1.m1.1.1.2.cmml">−</mo><mn id="S7.T1.19.11.2.1.1.m1.1.1.2.2" xref="S7.T1.19.11.2.1.1.m1.1.1.2.2.cmml">0.07</mn></mrow><mo id="S7.T1.19.11.2.1.1.m1.1.1.1" xref="S7.T1.19.11.2.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T1.19.11.2.1.1.m1.1.1.3" xref="S7.T1.19.11.2.1.1.m1.1.1.3.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.19.11.2.1.1.m1.1b"><apply id="S7.T1.19.11.2.1.1.m1.1.1.cmml" xref="S7.T1.19.11.2.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.19.11.2.1.1.m1.1.1.1.cmml" xref="S7.T1.19.11.2.1.1.m1.1.1.1">plus-or-minus</csymbol><apply id="S7.T1.19.11.2.1.1.m1.1.1.2.cmml" xref="S7.T1.19.11.2.1.1.m1.1.1.2"><minus id="S7.T1.19.11.2.1.1.m1.1.1.2.1.cmml" xref="S7.T1.19.11.2.1.1.m1.1.1.2"></minus><cn type="float" id="S7.T1.19.11.2.1.1.m1.1.1.2.2.cmml" xref="S7.T1.19.11.2.1.1.m1.1.1.2.2">0.07</cn></apply><cn type="float" id="S7.T1.19.11.2.1.1.m1.1.1.3.cmml" xref="S7.T1.19.11.2.1.1.m1.1.1.3">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.19.11.2.1.1.m1.1c">-0.07\pm 0.02</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T1.20.12.3" class="ltx_td ltx_align_justify">
<span id="S7.T1.20.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.20.12.3.1.1" class="ltx_p"><math id="S7.T1.20.12.3.1.1.m1.1" class="ltx_Math" alttext="-0.94\pm 0.12^{**}" display="inline"><semantics id="S7.T1.20.12.3.1.1.m1.1a"><mrow id="S7.T1.20.12.3.1.1.m1.1.1" xref="S7.T1.20.12.3.1.1.m1.1.1.cmml"><mrow id="S7.T1.20.12.3.1.1.m1.1.1.2" xref="S7.T1.20.12.3.1.1.m1.1.1.2.cmml"><mo id="S7.T1.20.12.3.1.1.m1.1.1.2a" xref="S7.T1.20.12.3.1.1.m1.1.1.2.cmml">−</mo><mn id="S7.T1.20.12.3.1.1.m1.1.1.2.2" xref="S7.T1.20.12.3.1.1.m1.1.1.2.2.cmml">0.94</mn></mrow><mo id="S7.T1.20.12.3.1.1.m1.1.1.1" xref="S7.T1.20.12.3.1.1.m1.1.1.1.cmml">±</mo><msup id="S7.T1.20.12.3.1.1.m1.1.1.3" xref="S7.T1.20.12.3.1.1.m1.1.1.3.cmml"><mn id="S7.T1.20.12.3.1.1.m1.1.1.3.2" xref="S7.T1.20.12.3.1.1.m1.1.1.3.2.cmml">0.12</mn><mrow id="S7.T1.20.12.3.1.1.m1.1.1.3.3" xref="S7.T1.20.12.3.1.1.m1.1.1.3.3.cmml"><mi id="S7.T1.20.12.3.1.1.m1.1.1.3.3.2" xref="S7.T1.20.12.3.1.1.m1.1.1.3.3.2.cmml"></mi><mo lspace="0.222em" rspace="0em" id="S7.T1.20.12.3.1.1.m1.1.1.3.3.1" xref="S7.T1.20.12.3.1.1.m1.1.1.3.3.1.cmml">∗</mo><mo lspace="0em" id="S7.T1.20.12.3.1.1.m1.1.1.3.3.3" xref="S7.T1.20.12.3.1.1.m1.1.1.3.3.3.cmml">∗</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.20.12.3.1.1.m1.1b"><apply id="S7.T1.20.12.3.1.1.m1.1.1.cmml" xref="S7.T1.20.12.3.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.20.12.3.1.1.m1.1.1.1.cmml" xref="S7.T1.20.12.3.1.1.m1.1.1.1">plus-or-minus</csymbol><apply id="S7.T1.20.12.3.1.1.m1.1.1.2.cmml" xref="S7.T1.20.12.3.1.1.m1.1.1.2"><minus id="S7.T1.20.12.3.1.1.m1.1.1.2.1.cmml" xref="S7.T1.20.12.3.1.1.m1.1.1.2"></minus><cn type="float" id="S7.T1.20.12.3.1.1.m1.1.1.2.2.cmml" xref="S7.T1.20.12.3.1.1.m1.1.1.2.2">0.94</cn></apply><apply id="S7.T1.20.12.3.1.1.m1.1.1.3.cmml" xref="S7.T1.20.12.3.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.T1.20.12.3.1.1.m1.1.1.3.1.cmml" xref="S7.T1.20.12.3.1.1.m1.1.1.3">superscript</csymbol><cn type="float" id="S7.T1.20.12.3.1.1.m1.1.1.3.2.cmml" xref="S7.T1.20.12.3.1.1.m1.1.1.3.2">0.12</cn><apply id="S7.T1.20.12.3.1.1.m1.1.1.3.3.cmml" xref="S7.T1.20.12.3.1.1.m1.1.1.3.3"><times id="S7.T1.20.12.3.1.1.m1.1.1.3.3.1.cmml" xref="S7.T1.20.12.3.1.1.m1.1.1.3.3.1"></times><csymbol cd="latexml" id="S7.T1.20.12.3.1.1.m1.1.1.3.3.2.cmml" xref="S7.T1.20.12.3.1.1.m1.1.1.3.3.2">absent</csymbol><times id="S7.T1.20.12.3.1.1.m1.1.1.3.3.3.cmml" xref="S7.T1.20.12.3.1.1.m1.1.1.3.3.3"></times></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.20.12.3.1.1.m1.1c">-0.94\pm 0.12^{**}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S7.T1.23.15" class="ltx_tr">
<td id="S7.T1.23.15.4" class="ltx_td ltx_align_left">Ours</td>
<td id="S7.T1.21.13.1" class="ltx_td ltx_align_justify">
<span id="S7.T1.21.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.21.13.1.1.1" class="ltx_p"><math id="S7.T1.21.13.1.1.1.m1.1" class="ltx_Math" alttext="0.02\pm 0.01" display="inline"><semantics id="S7.T1.21.13.1.1.1.m1.1a"><mrow id="S7.T1.21.13.1.1.1.m1.1.1" xref="S7.T1.21.13.1.1.1.m1.1.1.cmml"><mn id="S7.T1.21.13.1.1.1.m1.1.1.2" xref="S7.T1.21.13.1.1.1.m1.1.1.2.cmml">0.02</mn><mo id="S7.T1.21.13.1.1.1.m1.1.1.1" xref="S7.T1.21.13.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T1.21.13.1.1.1.m1.1.1.3" xref="S7.T1.21.13.1.1.1.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.21.13.1.1.1.m1.1b"><apply id="S7.T1.21.13.1.1.1.m1.1.1.cmml" xref="S7.T1.21.13.1.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.21.13.1.1.1.m1.1.1.1.cmml" xref="S7.T1.21.13.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T1.21.13.1.1.1.m1.1.1.2.cmml" xref="S7.T1.21.13.1.1.1.m1.1.1.2">0.02</cn><cn type="float" id="S7.T1.21.13.1.1.1.m1.1.1.3.cmml" xref="S7.T1.21.13.1.1.1.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.21.13.1.1.1.m1.1c">0.02\pm 0.01</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T1.22.14.2" class="ltx_td ltx_align_justify">
<span id="S7.T1.22.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.22.14.2.1.1" class="ltx_p"><math id="S7.T1.22.14.2.1.1.m1.1" class="ltx_Math" alttext="-0.05\pm 0.02" display="inline"><semantics id="S7.T1.22.14.2.1.1.m1.1a"><mrow id="S7.T1.22.14.2.1.1.m1.1.1" xref="S7.T1.22.14.2.1.1.m1.1.1.cmml"><mrow id="S7.T1.22.14.2.1.1.m1.1.1.2" xref="S7.T1.22.14.2.1.1.m1.1.1.2.cmml"><mo id="S7.T1.22.14.2.1.1.m1.1.1.2a" xref="S7.T1.22.14.2.1.1.m1.1.1.2.cmml">−</mo><mn id="S7.T1.22.14.2.1.1.m1.1.1.2.2" xref="S7.T1.22.14.2.1.1.m1.1.1.2.2.cmml">0.05</mn></mrow><mo id="S7.T1.22.14.2.1.1.m1.1.1.1" xref="S7.T1.22.14.2.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T1.22.14.2.1.1.m1.1.1.3" xref="S7.T1.22.14.2.1.1.m1.1.1.3.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.22.14.2.1.1.m1.1b"><apply id="S7.T1.22.14.2.1.1.m1.1.1.cmml" xref="S7.T1.22.14.2.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.22.14.2.1.1.m1.1.1.1.cmml" xref="S7.T1.22.14.2.1.1.m1.1.1.1">plus-or-minus</csymbol><apply id="S7.T1.22.14.2.1.1.m1.1.1.2.cmml" xref="S7.T1.22.14.2.1.1.m1.1.1.2"><minus id="S7.T1.22.14.2.1.1.m1.1.1.2.1.cmml" xref="S7.T1.22.14.2.1.1.m1.1.1.2"></minus><cn type="float" id="S7.T1.22.14.2.1.1.m1.1.1.2.2.cmml" xref="S7.T1.22.14.2.1.1.m1.1.1.2.2">0.05</cn></apply><cn type="float" id="S7.T1.22.14.2.1.1.m1.1.1.3.cmml" xref="S7.T1.22.14.2.1.1.m1.1.1.3">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.22.14.2.1.1.m1.1c">-0.05\pm 0.02</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T1.23.15.3" class="ltx_td ltx_align_justify">
<span id="S7.T1.23.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.23.15.3.1.1" class="ltx_p"><math id="S7.T1.23.15.3.1.1.m1.1" class="ltx_Math" alttext="\bm{0.48\pm 0.07}" display="inline"><semantics id="S7.T1.23.15.3.1.1.m1.1a"><mrow id="S7.T1.23.15.3.1.1.m1.1.1" xref="S7.T1.23.15.3.1.1.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T1.23.15.3.1.1.m1.1.1.2" xref="S7.T1.23.15.3.1.1.m1.1.1.2.cmml">0.48</mn><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T1.23.15.3.1.1.m1.1.1.1" xref="S7.T1.23.15.3.1.1.m1.1.1.1.cmml">±</mo><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T1.23.15.3.1.1.m1.1.1.3" xref="S7.T1.23.15.3.1.1.m1.1.1.3.cmml">0.07</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.23.15.3.1.1.m1.1b"><apply id="S7.T1.23.15.3.1.1.m1.1.1.cmml" xref="S7.T1.23.15.3.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.23.15.3.1.1.m1.1.1.1.cmml" xref="S7.T1.23.15.3.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T1.23.15.3.1.1.m1.1.1.2.cmml" xref="S7.T1.23.15.3.1.1.m1.1.1.2">0.48</cn><cn type="float" id="S7.T1.23.15.3.1.1.m1.1.1.3.cmml" xref="S7.T1.23.15.3.1.1.m1.1.1.3">0.07</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.23.15.3.1.1.m1.1c">\bm{0.48\pm 0.07}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S7.T1.26.18" class="ltx_tr">
<td id="S7.T1.26.18.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="4"><span id="S7.T1.26.18.4.1" class="ltx_text">BEAT</span></td>
<td id="S7.T1.26.18.5" class="ltx_td ltx_align_left ltx_border_t">GT</td>
<td id="S7.T1.24.16.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S7.T1.24.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.24.16.1.1.1" class="ltx_p"><math id="S7.T1.24.16.1.1.1.m1.1" class="ltx_Math" alttext="0.43\pm 0.06" display="inline"><semantics id="S7.T1.24.16.1.1.1.m1.1a"><mrow id="S7.T1.24.16.1.1.1.m1.1.1" xref="S7.T1.24.16.1.1.1.m1.1.1.cmml"><mn id="S7.T1.24.16.1.1.1.m1.1.1.2" xref="S7.T1.24.16.1.1.1.m1.1.1.2.cmml">0.43</mn><mo id="S7.T1.24.16.1.1.1.m1.1.1.1" xref="S7.T1.24.16.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T1.24.16.1.1.1.m1.1.1.3" xref="S7.T1.24.16.1.1.1.m1.1.1.3.cmml">0.06</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.24.16.1.1.1.m1.1b"><apply id="S7.T1.24.16.1.1.1.m1.1.1.cmml" xref="S7.T1.24.16.1.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.24.16.1.1.1.m1.1.1.1.cmml" xref="S7.T1.24.16.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T1.24.16.1.1.1.m1.1.1.2.cmml" xref="S7.T1.24.16.1.1.1.m1.1.1.2">0.43</cn><cn type="float" id="S7.T1.24.16.1.1.1.m1.1.1.3.cmml" xref="S7.T1.24.16.1.1.1.m1.1.1.3">0.06</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.24.16.1.1.1.m1.1c">0.43\pm 0.06</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T1.25.17.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S7.T1.25.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.25.17.2.1.1" class="ltx_p"><math id="S7.T1.25.17.2.1.1.m1.1" class="ltx_Math" alttext="0.39\pm 0.06" display="inline"><semantics id="S7.T1.25.17.2.1.1.m1.1a"><mrow id="S7.T1.25.17.2.1.1.m1.1.1" xref="S7.T1.25.17.2.1.1.m1.1.1.cmml"><mn id="S7.T1.25.17.2.1.1.m1.1.1.2" xref="S7.T1.25.17.2.1.1.m1.1.1.2.cmml">0.39</mn><mo id="S7.T1.25.17.2.1.1.m1.1.1.1" xref="S7.T1.25.17.2.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T1.25.17.2.1.1.m1.1.1.3" xref="S7.T1.25.17.2.1.1.m1.1.1.3.cmml">0.06</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.25.17.2.1.1.m1.1b"><apply id="S7.T1.25.17.2.1.1.m1.1.1.cmml" xref="S7.T1.25.17.2.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.25.17.2.1.1.m1.1.1.1.cmml" xref="S7.T1.25.17.2.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T1.25.17.2.1.1.m1.1.1.2.cmml" xref="S7.T1.25.17.2.1.1.m1.1.1.2">0.39</cn><cn type="float" id="S7.T1.25.17.2.1.1.m1.1.1.3.cmml" xref="S7.T1.25.17.2.1.1.m1.1.1.3">0.06</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.25.17.2.1.1.m1.1c">0.39\pm 0.06</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T1.26.18.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S7.T1.26.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.26.18.3.1.1" class="ltx_p"><math id="S7.T1.26.18.3.1.1.m1.1" class="ltx_Math" alttext="0.37\pm 0.05" display="inline"><semantics id="S7.T1.26.18.3.1.1.m1.1a"><mrow id="S7.T1.26.18.3.1.1.m1.1.1" xref="S7.T1.26.18.3.1.1.m1.1.1.cmml"><mn id="S7.T1.26.18.3.1.1.m1.1.1.2" xref="S7.T1.26.18.3.1.1.m1.1.1.2.cmml">0.37</mn><mo id="S7.T1.26.18.3.1.1.m1.1.1.1" xref="S7.T1.26.18.3.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T1.26.18.3.1.1.m1.1.1.3" xref="S7.T1.26.18.3.1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.26.18.3.1.1.m1.1b"><apply id="S7.T1.26.18.3.1.1.m1.1.1.cmml" xref="S7.T1.26.18.3.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.26.18.3.1.1.m1.1.1.1.cmml" xref="S7.T1.26.18.3.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T1.26.18.3.1.1.m1.1.1.2.cmml" xref="S7.T1.26.18.3.1.1.m1.1.1.2">0.37</cn><cn type="float" id="S7.T1.26.18.3.1.1.m1.1.1.3.cmml" xref="S7.T1.26.18.3.1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.26.18.3.1.1.m1.1c">0.37\pm 0.05</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S7.T1.29.21" class="ltx_tr">
<td id="S7.T1.29.21.4" class="ltx_td ltx_align_left ltx_border_t">CaMN</td>
<td id="S7.T1.27.19.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S7.T1.27.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.27.19.1.1.1" class="ltx_p"><math id="S7.T1.27.19.1.1.1.m1.1" class="ltx_Math" alttext="-1.03\pm 0.15^{**}" display="inline"><semantics id="S7.T1.27.19.1.1.1.m1.1a"><mrow id="S7.T1.27.19.1.1.1.m1.1.1" xref="S7.T1.27.19.1.1.1.m1.1.1.cmml"><mrow id="S7.T1.27.19.1.1.1.m1.1.1.2" xref="S7.T1.27.19.1.1.1.m1.1.1.2.cmml"><mo id="S7.T1.27.19.1.1.1.m1.1.1.2a" xref="S7.T1.27.19.1.1.1.m1.1.1.2.cmml">−</mo><mn id="S7.T1.27.19.1.1.1.m1.1.1.2.2" xref="S7.T1.27.19.1.1.1.m1.1.1.2.2.cmml">1.03</mn></mrow><mo id="S7.T1.27.19.1.1.1.m1.1.1.1" xref="S7.T1.27.19.1.1.1.m1.1.1.1.cmml">±</mo><msup id="S7.T1.27.19.1.1.1.m1.1.1.3" xref="S7.T1.27.19.1.1.1.m1.1.1.3.cmml"><mn id="S7.T1.27.19.1.1.1.m1.1.1.3.2" xref="S7.T1.27.19.1.1.1.m1.1.1.3.2.cmml">0.15</mn><mrow id="S7.T1.27.19.1.1.1.m1.1.1.3.3" xref="S7.T1.27.19.1.1.1.m1.1.1.3.3.cmml"><mi id="S7.T1.27.19.1.1.1.m1.1.1.3.3.2" xref="S7.T1.27.19.1.1.1.m1.1.1.3.3.2.cmml"></mi><mo lspace="0.222em" rspace="0em" id="S7.T1.27.19.1.1.1.m1.1.1.3.3.1" xref="S7.T1.27.19.1.1.1.m1.1.1.3.3.1.cmml">∗</mo><mo lspace="0em" id="S7.T1.27.19.1.1.1.m1.1.1.3.3.3" xref="S7.T1.27.19.1.1.1.m1.1.1.3.3.3.cmml">∗</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.27.19.1.1.1.m1.1b"><apply id="S7.T1.27.19.1.1.1.m1.1.1.cmml" xref="S7.T1.27.19.1.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.27.19.1.1.1.m1.1.1.1.cmml" xref="S7.T1.27.19.1.1.1.m1.1.1.1">plus-or-minus</csymbol><apply id="S7.T1.27.19.1.1.1.m1.1.1.2.cmml" xref="S7.T1.27.19.1.1.1.m1.1.1.2"><minus id="S7.T1.27.19.1.1.1.m1.1.1.2.1.cmml" xref="S7.T1.27.19.1.1.1.m1.1.1.2"></minus><cn type="float" id="S7.T1.27.19.1.1.1.m1.1.1.2.2.cmml" xref="S7.T1.27.19.1.1.1.m1.1.1.2.2">1.03</cn></apply><apply id="S7.T1.27.19.1.1.1.m1.1.1.3.cmml" xref="S7.T1.27.19.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.T1.27.19.1.1.1.m1.1.1.3.1.cmml" xref="S7.T1.27.19.1.1.1.m1.1.1.3">superscript</csymbol><cn type="float" id="S7.T1.27.19.1.1.1.m1.1.1.3.2.cmml" xref="S7.T1.27.19.1.1.1.m1.1.1.3.2">0.15</cn><apply id="S7.T1.27.19.1.1.1.m1.1.1.3.3.cmml" xref="S7.T1.27.19.1.1.1.m1.1.1.3.3"><times id="S7.T1.27.19.1.1.1.m1.1.1.3.3.1.cmml" xref="S7.T1.27.19.1.1.1.m1.1.1.3.3.1"></times><csymbol cd="latexml" id="S7.T1.27.19.1.1.1.m1.1.1.3.3.2.cmml" xref="S7.T1.27.19.1.1.1.m1.1.1.3.3.2">absent</csymbol><times id="S7.T1.27.19.1.1.1.m1.1.1.3.3.3.cmml" xref="S7.T1.27.19.1.1.1.m1.1.1.3.3.3"></times></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.27.19.1.1.1.m1.1c">-1.03\pm 0.15^{**}</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T1.28.20.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S7.T1.28.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.28.20.2.1.1" class="ltx_p"><math id="S7.T1.28.20.2.1.1.m1.1" class="ltx_Math" alttext="-0.91\pm 0.13^{**}" display="inline"><semantics id="S7.T1.28.20.2.1.1.m1.1a"><mrow id="S7.T1.28.20.2.1.1.m1.1.1" xref="S7.T1.28.20.2.1.1.m1.1.1.cmml"><mrow id="S7.T1.28.20.2.1.1.m1.1.1.2" xref="S7.T1.28.20.2.1.1.m1.1.1.2.cmml"><mo id="S7.T1.28.20.2.1.1.m1.1.1.2a" xref="S7.T1.28.20.2.1.1.m1.1.1.2.cmml">−</mo><mn id="S7.T1.28.20.2.1.1.m1.1.1.2.2" xref="S7.T1.28.20.2.1.1.m1.1.1.2.2.cmml">0.91</mn></mrow><mo id="S7.T1.28.20.2.1.1.m1.1.1.1" xref="S7.T1.28.20.2.1.1.m1.1.1.1.cmml">±</mo><msup id="S7.T1.28.20.2.1.1.m1.1.1.3" xref="S7.T1.28.20.2.1.1.m1.1.1.3.cmml"><mn id="S7.T1.28.20.2.1.1.m1.1.1.3.2" xref="S7.T1.28.20.2.1.1.m1.1.1.3.2.cmml">0.13</mn><mrow id="S7.T1.28.20.2.1.1.m1.1.1.3.3" xref="S7.T1.28.20.2.1.1.m1.1.1.3.3.cmml"><mi id="S7.T1.28.20.2.1.1.m1.1.1.3.3.2" xref="S7.T1.28.20.2.1.1.m1.1.1.3.3.2.cmml"></mi><mo lspace="0.222em" rspace="0em" id="S7.T1.28.20.2.1.1.m1.1.1.3.3.1" xref="S7.T1.28.20.2.1.1.m1.1.1.3.3.1.cmml">∗</mo><mo lspace="0em" id="S7.T1.28.20.2.1.1.m1.1.1.3.3.3" xref="S7.T1.28.20.2.1.1.m1.1.1.3.3.3.cmml">∗</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.28.20.2.1.1.m1.1b"><apply id="S7.T1.28.20.2.1.1.m1.1.1.cmml" xref="S7.T1.28.20.2.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.28.20.2.1.1.m1.1.1.1.cmml" xref="S7.T1.28.20.2.1.1.m1.1.1.1">plus-or-minus</csymbol><apply id="S7.T1.28.20.2.1.1.m1.1.1.2.cmml" xref="S7.T1.28.20.2.1.1.m1.1.1.2"><minus id="S7.T1.28.20.2.1.1.m1.1.1.2.1.cmml" xref="S7.T1.28.20.2.1.1.m1.1.1.2"></minus><cn type="float" id="S7.T1.28.20.2.1.1.m1.1.1.2.2.cmml" xref="S7.T1.28.20.2.1.1.m1.1.1.2.2">0.91</cn></apply><apply id="S7.T1.28.20.2.1.1.m1.1.1.3.cmml" xref="S7.T1.28.20.2.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.T1.28.20.2.1.1.m1.1.1.3.1.cmml" xref="S7.T1.28.20.2.1.1.m1.1.1.3">superscript</csymbol><cn type="float" id="S7.T1.28.20.2.1.1.m1.1.1.3.2.cmml" xref="S7.T1.28.20.2.1.1.m1.1.1.3.2">0.13</cn><apply id="S7.T1.28.20.2.1.1.m1.1.1.3.3.cmml" xref="S7.T1.28.20.2.1.1.m1.1.1.3.3"><times id="S7.T1.28.20.2.1.1.m1.1.1.3.3.1.cmml" xref="S7.T1.28.20.2.1.1.m1.1.1.3.3.1"></times><csymbol cd="latexml" id="S7.T1.28.20.2.1.1.m1.1.1.3.3.2.cmml" xref="S7.T1.28.20.2.1.1.m1.1.1.3.3.2">absent</csymbol><times id="S7.T1.28.20.2.1.1.m1.1.1.3.3.3.cmml" xref="S7.T1.28.20.2.1.1.m1.1.1.3.3.3"></times></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.28.20.2.1.1.m1.1c">-0.91\pm 0.13^{**}</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T1.29.21.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S7.T1.29.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.29.21.3.1.1" class="ltx_p"><math id="S7.T1.29.21.3.1.1.m1.1" class="ltx_Math" alttext="-0.22\pm 0.04^{**}" display="inline"><semantics id="S7.T1.29.21.3.1.1.m1.1a"><mrow id="S7.T1.29.21.3.1.1.m1.1.1" xref="S7.T1.29.21.3.1.1.m1.1.1.cmml"><mrow id="S7.T1.29.21.3.1.1.m1.1.1.2" xref="S7.T1.29.21.3.1.1.m1.1.1.2.cmml"><mo id="S7.T1.29.21.3.1.1.m1.1.1.2a" xref="S7.T1.29.21.3.1.1.m1.1.1.2.cmml">−</mo><mn id="S7.T1.29.21.3.1.1.m1.1.1.2.2" xref="S7.T1.29.21.3.1.1.m1.1.1.2.2.cmml">0.22</mn></mrow><mo id="S7.T1.29.21.3.1.1.m1.1.1.1" xref="S7.T1.29.21.3.1.1.m1.1.1.1.cmml">±</mo><msup id="S7.T1.29.21.3.1.1.m1.1.1.3" xref="S7.T1.29.21.3.1.1.m1.1.1.3.cmml"><mn id="S7.T1.29.21.3.1.1.m1.1.1.3.2" xref="S7.T1.29.21.3.1.1.m1.1.1.3.2.cmml">0.04</mn><mrow id="S7.T1.29.21.3.1.1.m1.1.1.3.3" xref="S7.T1.29.21.3.1.1.m1.1.1.3.3.cmml"><mi id="S7.T1.29.21.3.1.1.m1.1.1.3.3.2" xref="S7.T1.29.21.3.1.1.m1.1.1.3.3.2.cmml"></mi><mo lspace="0.222em" rspace="0em" id="S7.T1.29.21.3.1.1.m1.1.1.3.3.1" xref="S7.T1.29.21.3.1.1.m1.1.1.3.3.1.cmml">∗</mo><mo lspace="0em" id="S7.T1.29.21.3.1.1.m1.1.1.3.3.3" xref="S7.T1.29.21.3.1.1.m1.1.1.3.3.3.cmml">∗</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.29.21.3.1.1.m1.1b"><apply id="S7.T1.29.21.3.1.1.m1.1.1.cmml" xref="S7.T1.29.21.3.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.29.21.3.1.1.m1.1.1.1.cmml" xref="S7.T1.29.21.3.1.1.m1.1.1.1">plus-or-minus</csymbol><apply id="S7.T1.29.21.3.1.1.m1.1.1.2.cmml" xref="S7.T1.29.21.3.1.1.m1.1.1.2"><minus id="S7.T1.29.21.3.1.1.m1.1.1.2.1.cmml" xref="S7.T1.29.21.3.1.1.m1.1.1.2"></minus><cn type="float" id="S7.T1.29.21.3.1.1.m1.1.1.2.2.cmml" xref="S7.T1.29.21.3.1.1.m1.1.1.2.2">0.22</cn></apply><apply id="S7.T1.29.21.3.1.1.m1.1.1.3.cmml" xref="S7.T1.29.21.3.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.T1.29.21.3.1.1.m1.1.1.3.1.cmml" xref="S7.T1.29.21.3.1.1.m1.1.1.3">superscript</csymbol><cn type="float" id="S7.T1.29.21.3.1.1.m1.1.1.3.2.cmml" xref="S7.T1.29.21.3.1.1.m1.1.1.3.2">0.04</cn><apply id="S7.T1.29.21.3.1.1.m1.1.1.3.3.cmml" xref="S7.T1.29.21.3.1.1.m1.1.1.3.3"><times id="S7.T1.29.21.3.1.1.m1.1.1.3.3.1.cmml" xref="S7.T1.29.21.3.1.1.m1.1.1.3.3.1"></times><csymbol cd="latexml" id="S7.T1.29.21.3.1.1.m1.1.1.3.3.2.cmml" xref="S7.T1.29.21.3.1.1.m1.1.1.3.3.2">absent</csymbol><times id="S7.T1.29.21.3.1.1.m1.1.1.3.3.3.cmml" xref="S7.T1.29.21.3.1.1.m1.1.1.3.3.3"></times></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.29.21.3.1.1.m1.1c">-0.22\pm 0.04^{**}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S7.T1.32.24" class="ltx_tr">
<td id="S7.T1.32.24.4" class="ltx_td ltx_align_left">Ours (w/o semantic alignment)</td>
<td id="S7.T1.30.22.1" class="ltx_td ltx_align_justify">
<span id="S7.T1.30.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.30.22.1.1.1" class="ltx_p"><math id="S7.T1.30.22.1.1.1.m1.1" class="ltx_Math" alttext="0.29\pm 0.08" display="inline"><semantics id="S7.T1.30.22.1.1.1.m1.1a"><mrow id="S7.T1.30.22.1.1.1.m1.1.1" xref="S7.T1.30.22.1.1.1.m1.1.1.cmml"><mn id="S7.T1.30.22.1.1.1.m1.1.1.2" xref="S7.T1.30.22.1.1.1.m1.1.1.2.cmml">0.29</mn><mo id="S7.T1.30.22.1.1.1.m1.1.1.1" xref="S7.T1.30.22.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T1.30.22.1.1.1.m1.1.1.3" xref="S7.T1.30.22.1.1.1.m1.1.1.3.cmml">0.08</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.30.22.1.1.1.m1.1b"><apply id="S7.T1.30.22.1.1.1.m1.1.1.cmml" xref="S7.T1.30.22.1.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.30.22.1.1.1.m1.1.1.1.cmml" xref="S7.T1.30.22.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T1.30.22.1.1.1.m1.1.1.2.cmml" xref="S7.T1.30.22.1.1.1.m1.1.1.2">0.29</cn><cn type="float" id="S7.T1.30.22.1.1.1.m1.1.1.3.cmml" xref="S7.T1.30.22.1.1.1.m1.1.1.3">0.08</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.30.22.1.1.1.m1.1c">0.29\pm 0.08</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T1.31.23.2" class="ltx_td ltx_align_justify">
<span id="S7.T1.31.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.31.23.2.1.1" class="ltx_p"><math id="S7.T1.31.23.2.1.1.m1.1" class="ltx_Math" alttext="0.32\pm 0.07" display="inline"><semantics id="S7.T1.31.23.2.1.1.m1.1a"><mrow id="S7.T1.31.23.2.1.1.m1.1.1" xref="S7.T1.31.23.2.1.1.m1.1.1.cmml"><mn id="S7.T1.31.23.2.1.1.m1.1.1.2" xref="S7.T1.31.23.2.1.1.m1.1.1.2.cmml">0.32</mn><mo id="S7.T1.31.23.2.1.1.m1.1.1.1" xref="S7.T1.31.23.2.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T1.31.23.2.1.1.m1.1.1.3" xref="S7.T1.31.23.2.1.1.m1.1.1.3.cmml">0.07</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.31.23.2.1.1.m1.1b"><apply id="S7.T1.31.23.2.1.1.m1.1.1.cmml" xref="S7.T1.31.23.2.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.31.23.2.1.1.m1.1.1.1.cmml" xref="S7.T1.31.23.2.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T1.31.23.2.1.1.m1.1.1.2.cmml" xref="S7.T1.31.23.2.1.1.m1.1.1.2">0.32</cn><cn type="float" id="S7.T1.31.23.2.1.1.m1.1.1.3.cmml" xref="S7.T1.31.23.2.1.1.m1.1.1.3">0.07</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.31.23.2.1.1.m1.1c">0.32\pm 0.07</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T1.32.24.3" class="ltx_td ltx_align_justify">
<span id="S7.T1.32.24.3.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.32.24.3.1.1" class="ltx_p"><math id="S7.T1.32.24.3.1.1.m1.1" class="ltx_Math" alttext="-0.58\pm 0.07^{**}" display="inline"><semantics id="S7.T1.32.24.3.1.1.m1.1a"><mrow id="S7.T1.32.24.3.1.1.m1.1.1" xref="S7.T1.32.24.3.1.1.m1.1.1.cmml"><mrow id="S7.T1.32.24.3.1.1.m1.1.1.2" xref="S7.T1.32.24.3.1.1.m1.1.1.2.cmml"><mo id="S7.T1.32.24.3.1.1.m1.1.1.2a" xref="S7.T1.32.24.3.1.1.m1.1.1.2.cmml">−</mo><mn id="S7.T1.32.24.3.1.1.m1.1.1.2.2" xref="S7.T1.32.24.3.1.1.m1.1.1.2.2.cmml">0.58</mn></mrow><mo id="S7.T1.32.24.3.1.1.m1.1.1.1" xref="S7.T1.32.24.3.1.1.m1.1.1.1.cmml">±</mo><msup id="S7.T1.32.24.3.1.1.m1.1.1.3" xref="S7.T1.32.24.3.1.1.m1.1.1.3.cmml"><mn id="S7.T1.32.24.3.1.1.m1.1.1.3.2" xref="S7.T1.32.24.3.1.1.m1.1.1.3.2.cmml">0.07</mn><mrow id="S7.T1.32.24.3.1.1.m1.1.1.3.3" xref="S7.T1.32.24.3.1.1.m1.1.1.3.3.cmml"><mi id="S7.T1.32.24.3.1.1.m1.1.1.3.3.2" xref="S7.T1.32.24.3.1.1.m1.1.1.3.3.2.cmml"></mi><mo lspace="0.222em" rspace="0em" id="S7.T1.32.24.3.1.1.m1.1.1.3.3.1" xref="S7.T1.32.24.3.1.1.m1.1.1.3.3.1.cmml">∗</mo><mo lspace="0em" id="S7.T1.32.24.3.1.1.m1.1.1.3.3.3" xref="S7.T1.32.24.3.1.1.m1.1.1.3.3.3.cmml">∗</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.32.24.3.1.1.m1.1b"><apply id="S7.T1.32.24.3.1.1.m1.1.1.cmml" xref="S7.T1.32.24.3.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.32.24.3.1.1.m1.1.1.1.cmml" xref="S7.T1.32.24.3.1.1.m1.1.1.1">plus-or-minus</csymbol><apply id="S7.T1.32.24.3.1.1.m1.1.1.2.cmml" xref="S7.T1.32.24.3.1.1.m1.1.1.2"><minus id="S7.T1.32.24.3.1.1.m1.1.1.2.1.cmml" xref="S7.T1.32.24.3.1.1.m1.1.1.2"></minus><cn type="float" id="S7.T1.32.24.3.1.1.m1.1.1.2.2.cmml" xref="S7.T1.32.24.3.1.1.m1.1.1.2.2">0.58</cn></apply><apply id="S7.T1.32.24.3.1.1.m1.1.1.3.cmml" xref="S7.T1.32.24.3.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.T1.32.24.3.1.1.m1.1.1.3.1.cmml" xref="S7.T1.32.24.3.1.1.m1.1.1.3">superscript</csymbol><cn type="float" id="S7.T1.32.24.3.1.1.m1.1.1.3.2.cmml" xref="S7.T1.32.24.3.1.1.m1.1.1.3.2">0.07</cn><apply id="S7.T1.32.24.3.1.1.m1.1.1.3.3.cmml" xref="S7.T1.32.24.3.1.1.m1.1.1.3.3"><times id="S7.T1.32.24.3.1.1.m1.1.1.3.3.1.cmml" xref="S7.T1.32.24.3.1.1.m1.1.1.3.3.1"></times><csymbol cd="latexml" id="S7.T1.32.24.3.1.1.m1.1.1.3.3.2.cmml" xref="S7.T1.32.24.3.1.1.m1.1.1.3.3.2">absent</csymbol><times id="S7.T1.32.24.3.1.1.m1.1.1.3.3.3.cmml" xref="S7.T1.32.24.3.1.1.m1.1.1.3.3.3"></times></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.32.24.3.1.1.m1.1c">-0.58\pm 0.07^{**}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S7.T1.35.27" class="ltx_tr">
<td id="S7.T1.35.27.4" class="ltx_td ltx_align_left ltx_border_bb">Ours</td>
<td id="S7.T1.33.25.1" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S7.T1.33.25.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.33.25.1.1.1" class="ltx_p"><math id="S7.T1.33.25.1.1.1.m1.1" class="ltx_Math" alttext="\bm{0.35\pm 0.04}" display="inline"><semantics id="S7.T1.33.25.1.1.1.m1.1a"><mrow id="S7.T1.33.25.1.1.1.m1.1.1" xref="S7.T1.33.25.1.1.1.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T1.33.25.1.1.1.m1.1.1.2" xref="S7.T1.33.25.1.1.1.m1.1.1.2.cmml">0.35</mn><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T1.33.25.1.1.1.m1.1.1.1" xref="S7.T1.33.25.1.1.1.m1.1.1.1.cmml">±</mo><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T1.33.25.1.1.1.m1.1.1.3" xref="S7.T1.33.25.1.1.1.m1.1.1.3.cmml">0.04</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.33.25.1.1.1.m1.1b"><apply id="S7.T1.33.25.1.1.1.m1.1.1.cmml" xref="S7.T1.33.25.1.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.33.25.1.1.1.m1.1.1.1.cmml" xref="S7.T1.33.25.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T1.33.25.1.1.1.m1.1.1.2.cmml" xref="S7.T1.33.25.1.1.1.m1.1.1.2">0.35</cn><cn type="float" id="S7.T1.33.25.1.1.1.m1.1.1.3.cmml" xref="S7.T1.33.25.1.1.1.m1.1.1.3">0.04</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.33.25.1.1.1.m1.1c">\bm{0.35\pm 0.04}</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T1.34.26.2" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S7.T1.34.26.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.34.26.2.1.1" class="ltx_p"><math id="S7.T1.34.26.2.1.1.m1.1" class="ltx_Math" alttext="\bm{0.33\pm 0.05}" display="inline"><semantics id="S7.T1.34.26.2.1.1.m1.1a"><mrow id="S7.T1.34.26.2.1.1.m1.1.1" xref="S7.T1.34.26.2.1.1.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T1.34.26.2.1.1.m1.1.1.2" xref="S7.T1.34.26.2.1.1.m1.1.1.2.cmml">0.33</mn><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T1.34.26.2.1.1.m1.1.1.1" xref="S7.T1.34.26.2.1.1.m1.1.1.1.cmml">±</mo><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T1.34.26.2.1.1.m1.1.1.3" xref="S7.T1.34.26.2.1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.34.26.2.1.1.m1.1b"><apply id="S7.T1.34.26.2.1.1.m1.1.1.cmml" xref="S7.T1.34.26.2.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.34.26.2.1.1.m1.1.1.1.cmml" xref="S7.T1.34.26.2.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T1.34.26.2.1.1.m1.1.1.2.cmml" xref="S7.T1.34.26.2.1.1.m1.1.1.2">0.33</cn><cn type="float" id="S7.T1.34.26.2.1.1.m1.1.1.3.cmml" xref="S7.T1.34.26.2.1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.34.26.2.1.1.m1.1c">\bm{0.33\pm 0.05}</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T1.35.27.3" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S7.T1.35.27.3.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T1.35.27.3.1.1" class="ltx_p"><math id="S7.T1.35.27.3.1.1.m1.1" class="ltx_Math" alttext="\bm{0.41\pm 0.03}" display="inline"><semantics id="S7.T1.35.27.3.1.1.m1.1a"><mrow id="S7.T1.35.27.3.1.1.m1.1.1" xref="S7.T1.35.27.3.1.1.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T1.35.27.3.1.1.m1.1.1.2" xref="S7.T1.35.27.3.1.1.m1.1.1.2.cmml">0.41</mn><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T1.35.27.3.1.1.m1.1.1.1" xref="S7.T1.35.27.3.1.1.m1.1.1.1.cmml">±</mo><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T1.35.27.3.1.1.m1.1.1.3" xref="S7.T1.35.27.3.1.1.m1.1.1.3.cmml">0.03</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T1.35.27.3.1.1.m1.1b"><apply id="S7.T1.35.27.3.1.1.m1.1.1.cmml" xref="S7.T1.35.27.3.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T1.35.27.3.1.1.m1.1.1.1.cmml" xref="S7.T1.35.27.3.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T1.35.27.3.1.1.m1.1.1.2.cmml" xref="S7.T1.35.27.3.1.1.m1.1.1.2">0.41</cn><cn type="float" id="S7.T1.35.27.3.1.1.m1.1.1.3.cmml" xref="S7.T1.35.27.3.1.1.m1.1.1.3">0.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.35.27.3.1.1.m1.1c">\bm{0.41\pm 0.03}</annotation></semantics></math></span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="S7.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2405.09814/assets/x13.png" id="S7.F13.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="130" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F13.2.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>. </span><span id="S7.F13.3.2" class="ltx_text" style="font-size:90%;">Qualitative comparison between our system and baselines (GestureDiffuCLIP <cite class="ltx_cite ltx_citemacro_citep">(Ao et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite> and CaMN <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2022d</a>)</cite>) using two test speech excerpts.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S7.F13.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3. </span>Comparison</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">Evaluating gestures with objective metrics presents significant challenges. Many current objective metrics poorly correlate with subjective feedback outcomes <cite class="ltx_cite ltx_citemacro_citep">(Kucherenko et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2023</a>)</cite>. Echoing approaches in <cite class="ltx_cite ltx_citemacro_citep">(Alexanderson et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2023</a>; Ghorbani et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2023</a>; Ao et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>, this study emphasizes user study evaluations for generated results, with quantitative evaluations serving as supplementary references.</p>
</div>
<section id="S7.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.1. </span>Baselines</h4>

<div id="S7.SS3.SSS1.p1" class="ltx_para">
<p id="S7.SS3.SSS1.p1.1" class="ltx_p">We compare our system with GestureDiffuCLIP <cite class="ltx_cite ltx_citemacro_citep">(Ao et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite> on the ZEGGS dataset, using the code provided by its authors. This system learns an implicit shared space between transcripts and gestures, enhancing semantic perception. The BEAT dataset is released alongside a strong baseline, the Cascaded Motion Network (CaMN), which uses transcripts as inputs to generate semantic gestures based on a hierarchical architecture. The related codes for two semantics-aware systems <cite class="ltx_cite ltx_citemacro_citep">(Zhi
et al<span class="ltx_text">.</span>, <a href="#bib.bib105" title="" class="ltx_ref">2023</a>; Gao
et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2023</a>)</cite> are not available at the time of writing this work.</p>
</div>
</section>
<section id="S7.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.2. </span>User Study</h4>

<div id="S7.SS3.SSS2.p1" class="ltx_para">
<p id="S7.SS3.SSS2.p1.5" class="ltx_p">Following the method in <cite class="ltx_cite ltx_citemacro_citep">(Alexanderson et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2023</a>; Ao et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>, we conduct user studies using pairwise comparisons. For each test, participants view two 10-second videos, each synthesized by different models (including the ground truth) for the same speech segment, played one after the other. Participants are required to choose the video they prefer, following the instructions given below the videos, and rate their choice on a scale from <math id="S7.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S7.SS3.SSS2.p1.1.m1.1a"><mn id="S7.SS3.SSS2.p1.1.m1.1.1" xref="S7.SS3.SSS2.p1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p1.1.m1.1b"><cn type="integer" id="S7.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S7.SS3.SSS2.p1.1.m1.1.1">0</cn></annotation-xml></semantics></math> to <math id="S7.SS3.SSS2.p1.2.m2.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S7.SS3.SSS2.p1.2.m2.1a"><mn id="S7.SS3.SSS2.p1.2.m2.1.1" xref="S7.SS3.SSS2.p1.2.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p1.2.m2.1b"><cn type="integer" id="S7.SS3.SSS2.p1.2.m2.1.1.cmml" xref="S7.SS3.SSS2.p1.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p1.2.m2.1c">2</annotation></semantics></math>, where <math id="S7.SS3.SSS2.p1.3.m3.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S7.SS3.SSS2.p1.3.m3.1a"><mn id="S7.SS3.SSS2.p1.3.m3.1.1" xref="S7.SS3.SSS2.p1.3.m3.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p1.3.m3.1b"><cn type="integer" id="S7.SS3.SSS2.p1.3.m3.1.1.cmml" xref="S7.SS3.SSS2.p1.3.m3.1.1">0</cn></annotation-xml></semantics></math> signifies no preference. The unselected video in the pair is then assigned the inverse score (for instance, if the chosen video is rated <math id="S7.SS3.SSS2.p1.4.m4.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S7.SS3.SSS2.p1.4.m4.1a"><mn id="S7.SS3.SSS2.p1.4.m4.1.1" xref="S7.SS3.SSS2.p1.4.m4.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p1.4.m4.1b"><cn type="integer" id="S7.SS3.SSS2.p1.4.m4.1.1.cmml" xref="S7.SS3.SSS2.p1.4.m4.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p1.4.m4.1c">1</annotation></semantics></math>, the other video is assigned <math id="S7.SS3.SSS2.p1.5.m5.1" class="ltx_Math" alttext="-1" display="inline"><semantics id="S7.SS3.SSS2.p1.5.m5.1a"><mrow id="S7.SS3.SSS2.p1.5.m5.1.1" xref="S7.SS3.SSS2.p1.5.m5.1.1.cmml"><mo id="S7.SS3.SSS2.p1.5.m5.1.1a" xref="S7.SS3.SSS2.p1.5.m5.1.1.cmml">−</mo><mn id="S7.SS3.SSS2.p1.5.m5.1.1.2" xref="S7.SS3.SSS2.p1.5.m5.1.1.2.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p1.5.m5.1b"><apply id="S7.SS3.SSS2.p1.5.m5.1.1.cmml" xref="S7.SS3.SSS2.p1.5.m5.1.1"><minus id="S7.SS3.SSS2.p1.5.m5.1.1.1.cmml" xref="S7.SS3.SSS2.p1.5.m5.1.1"></minus><cn type="integer" id="S7.SS3.SSS2.p1.5.m5.1.1.2.cmml" xref="S7.SS3.SSS2.p1.5.m5.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p1.5.m5.1c">-1</annotation></semantics></math>). Participant recruitment is conducted via the Credamo platform <cite class="ltx_cite ltx_citemacro_citep">(Credamo, <a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite>. Details of the user study are described in Appendix <a href="#A1" title="Appendix A Details of User Study ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<div id="S7.SS3.SSS2.p2" class="ltx_para">
<p id="S7.SS3.SSS2.p2.1" class="ltx_p">We conduct three distinct preference tests: <em id="S7.SS3.SSS2.p2.1.1" class="ltx_emph ltx_font_italic">human likeness</em>, <em id="S7.SS3.SSS2.p2.1.2" class="ltx_emph ltx_font_italic">beat matching</em>, and <em id="S7.SS3.SSS2.p2.1.3" class="ltx_emph ltx_font_italic">semantic accuracy</em>, incorporating attention checks in each. During the <em id="S7.SS3.SSS2.p2.1.4" class="ltx_emph ltx_font_italic">human likeness</em> test, participants determine if the generated motion closely mimics that of a real human. To avoid any speech-induced bias, these video clips are presented without sound. In the <em id="S7.SS3.SSS2.p2.1.5" class="ltx_emph ltx_font_italic">beat matching</em> test, participants assess the synchronization of the generated motion with the speech’s rhythm. For the <em id="S7.SS3.SSS2.p2.1.6" class="ltx_emph ltx_font_italic">semantic accuracy</em> test, participants are required to evaluate whether the generated gestures based on the input speech accurately convey the appropriate semantics. The average scores from these tests are detailed in Table <a href="#S7.T1" title="Table 1 ‣ 7.2. Results ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We implement a one-way ANOVA and a post-hoc Tukey multiple comparison test for each user study. The assumptions of normality, homogeneity of variances, and independence for each ANOVA are all met for both the ZEGGS and BEAT datasets.</p>
</div>
<div id="S7.SS3.SSS2.p3" class="ltx_para">
<p id="S7.SS3.SSS2.p3.13" class="ltx_p">In the ZEGGS Dataset evaluation, we analyze four methods: the ground-truth gestures (GT), our system (Ours), our system without the semantic alignment module (w/o semantic alignment) for ablation, and GestureDiffuCLIP. After attention checks, we collect the valid answers of <math id="S7.SS3.SSS2.p3.1.m1.1" class="ltx_Math" alttext="98" display="inline"><semantics id="S7.SS3.SSS2.p3.1.m1.1a"><mn id="S7.SS3.SSS2.p3.1.m1.1.1" xref="S7.SS3.SSS2.p3.1.m1.1.1.cmml">98</mn><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p3.1.m1.1b"><cn type="integer" id="S7.SS3.SSS2.p3.1.m1.1.1.cmml" xref="S7.SS3.SSS2.p3.1.m1.1.1">98</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p3.1.m1.1c">98</annotation></semantics></math>, <math id="S7.SS3.SSS2.p3.2.m2.1" class="ltx_Math" alttext="99" display="inline"><semantics id="S7.SS3.SSS2.p3.2.m2.1a"><mn id="S7.SS3.SSS2.p3.2.m2.1.1" xref="S7.SS3.SSS2.p3.2.m2.1.1.cmml">99</mn><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p3.2.m2.1b"><cn type="integer" id="S7.SS3.SSS2.p3.2.m2.1.1.cmml" xref="S7.SS3.SSS2.p3.2.m2.1.1">99</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p3.2.m2.1c">99</annotation></semantics></math>, <math id="S7.SS3.SSS2.p3.3.m3.1" class="ltx_Math" alttext="101" display="inline"><semantics id="S7.SS3.SSS2.p3.3.m3.1a"><mn id="S7.SS3.SSS2.p3.3.m3.1.1" xref="S7.SS3.SSS2.p3.3.m3.1.1.cmml">101</mn><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p3.3.m3.1b"><cn type="integer" id="S7.SS3.SSS2.p3.3.m3.1.1.cmml" xref="S7.SS3.SSS2.p3.3.m3.1.1">101</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p3.3.m3.1c">101</annotation></semantics></math> participants for the human likeness, beat matching, and semantic accuracy tests, respectively. Multiple one-way ANOVAs are conducted, one for each questionnaire item. The results reveal that different baselines have statistically significant effects on
<em id="S7.SS3.SSS2.p3.13.1" class="ltx_emph ltx_font_italic">human likeness</em> (<math id="S7.SS3.SSS2.p3.4.m4.2" class="ltx_Math" alttext="F(3,4700)=142.91" display="inline"><semantics id="S7.SS3.SSS2.p3.4.m4.2a"><mrow id="S7.SS3.SSS2.p3.4.m4.2.3" xref="S7.SS3.SSS2.p3.4.m4.2.3.cmml"><mrow id="S7.SS3.SSS2.p3.4.m4.2.3.2" xref="S7.SS3.SSS2.p3.4.m4.2.3.2.cmml"><mi id="S7.SS3.SSS2.p3.4.m4.2.3.2.2" xref="S7.SS3.SSS2.p3.4.m4.2.3.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S7.SS3.SSS2.p3.4.m4.2.3.2.1" xref="S7.SS3.SSS2.p3.4.m4.2.3.2.1.cmml">​</mo><mrow id="S7.SS3.SSS2.p3.4.m4.2.3.2.3.2" xref="S7.SS3.SSS2.p3.4.m4.2.3.2.3.1.cmml"><mo stretchy="false" id="S7.SS3.SSS2.p3.4.m4.2.3.2.3.2.1" xref="S7.SS3.SSS2.p3.4.m4.2.3.2.3.1.cmml">(</mo><mn id="S7.SS3.SSS2.p3.4.m4.1.1" xref="S7.SS3.SSS2.p3.4.m4.1.1.cmml">3</mn><mo id="S7.SS3.SSS2.p3.4.m4.2.3.2.3.2.2" xref="S7.SS3.SSS2.p3.4.m4.2.3.2.3.1.cmml">,</mo><mn id="S7.SS3.SSS2.p3.4.m4.2.2" xref="S7.SS3.SSS2.p3.4.m4.2.2.cmml">4700</mn><mo stretchy="false" id="S7.SS3.SSS2.p3.4.m4.2.3.2.3.2.3" xref="S7.SS3.SSS2.p3.4.m4.2.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S7.SS3.SSS2.p3.4.m4.2.3.1" xref="S7.SS3.SSS2.p3.4.m4.2.3.1.cmml">=</mo><mn id="S7.SS3.SSS2.p3.4.m4.2.3.3" xref="S7.SS3.SSS2.p3.4.m4.2.3.3.cmml">142.91</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p3.4.m4.2b"><apply id="S7.SS3.SSS2.p3.4.m4.2.3.cmml" xref="S7.SS3.SSS2.p3.4.m4.2.3"><eq id="S7.SS3.SSS2.p3.4.m4.2.3.1.cmml" xref="S7.SS3.SSS2.p3.4.m4.2.3.1"></eq><apply id="S7.SS3.SSS2.p3.4.m4.2.3.2.cmml" xref="S7.SS3.SSS2.p3.4.m4.2.3.2"><times id="S7.SS3.SSS2.p3.4.m4.2.3.2.1.cmml" xref="S7.SS3.SSS2.p3.4.m4.2.3.2.1"></times><ci id="S7.SS3.SSS2.p3.4.m4.2.3.2.2.cmml" xref="S7.SS3.SSS2.p3.4.m4.2.3.2.2">𝐹</ci><interval closure="open" id="S7.SS3.SSS2.p3.4.m4.2.3.2.3.1.cmml" xref="S7.SS3.SSS2.p3.4.m4.2.3.2.3.2"><cn type="integer" id="S7.SS3.SSS2.p3.4.m4.1.1.cmml" xref="S7.SS3.SSS2.p3.4.m4.1.1">3</cn><cn type="integer" id="S7.SS3.SSS2.p3.4.m4.2.2.cmml" xref="S7.SS3.SSS2.p3.4.m4.2.2">4700</cn></interval></apply><cn type="float" id="S7.SS3.SSS2.p3.4.m4.2.3.3.cmml" xref="S7.SS3.SSS2.p3.4.m4.2.3.3">142.91</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p3.4.m4.2c">F(3,4700)=142.91</annotation></semantics></math>, <math id="S7.SS3.SSS2.p3.5.m5.1" class="ltx_Math" alttext="p&lt;.05" display="inline"><semantics id="S7.SS3.SSS2.p3.5.m5.1a"><mrow id="S7.SS3.SSS2.p3.5.m5.1.1" xref="S7.SS3.SSS2.p3.5.m5.1.1.cmml"><mi id="S7.SS3.SSS2.p3.5.m5.1.1.2" xref="S7.SS3.SSS2.p3.5.m5.1.1.2.cmml">p</mi><mo id="S7.SS3.SSS2.p3.5.m5.1.1.1" xref="S7.SS3.SSS2.p3.5.m5.1.1.1.cmml">&lt;</mo><mn id="S7.SS3.SSS2.p3.5.m5.1.1.3" xref="S7.SS3.SSS2.p3.5.m5.1.1.3.cmml">.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p3.5.m5.1b"><apply id="S7.SS3.SSS2.p3.5.m5.1.1.cmml" xref="S7.SS3.SSS2.p3.5.m5.1.1"><lt id="S7.SS3.SSS2.p3.5.m5.1.1.1.cmml" xref="S7.SS3.SSS2.p3.5.m5.1.1.1"></lt><ci id="S7.SS3.SSS2.p3.5.m5.1.1.2.cmml" xref="S7.SS3.SSS2.p3.5.m5.1.1.2">𝑝</ci><cn type="float" id="S7.SS3.SSS2.p3.5.m5.1.1.3.cmml" xref="S7.SS3.SSS2.p3.5.m5.1.1.3">.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p3.5.m5.1c">p&lt;.05</annotation></semantics></math>, <math id="S7.SS3.SSS2.p3.6.m6.1" class="ltx_Math" alttext="{\text{Partial Eta Squared}}=0.083" display="inline"><semantics id="S7.SS3.SSS2.p3.6.m6.1a"><mrow id="S7.SS3.SSS2.p3.6.m6.1.1" xref="S7.SS3.SSS2.p3.6.m6.1.1.cmml"><mtext id="S7.SS3.SSS2.p3.6.m6.1.1.2" xref="S7.SS3.SSS2.p3.6.m6.1.1.2a.cmml">Partial Eta Squared</mtext><mo id="S7.SS3.SSS2.p3.6.m6.1.1.1" xref="S7.SS3.SSS2.p3.6.m6.1.1.1.cmml">=</mo><mn id="S7.SS3.SSS2.p3.6.m6.1.1.3" xref="S7.SS3.SSS2.p3.6.m6.1.1.3.cmml">0.083</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p3.6.m6.1b"><apply id="S7.SS3.SSS2.p3.6.m6.1.1.cmml" xref="S7.SS3.SSS2.p3.6.m6.1.1"><eq id="S7.SS3.SSS2.p3.6.m6.1.1.1.cmml" xref="S7.SS3.SSS2.p3.6.m6.1.1.1"></eq><ci id="S7.SS3.SSS2.p3.6.m6.1.1.2a.cmml" xref="S7.SS3.SSS2.p3.6.m6.1.1.2"><mtext id="S7.SS3.SSS2.p3.6.m6.1.1.2.cmml" xref="S7.SS3.SSS2.p3.6.m6.1.1.2">Partial Eta Squared</mtext></ci><cn type="float" id="S7.SS3.SSS2.p3.6.m6.1.1.3.cmml" xref="S7.SS3.SSS2.p3.6.m6.1.1.3">0.083</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p3.6.m6.1c">{\text{Partial Eta Squared}}=0.083</annotation></semantics></math>),
<em id="S7.SS3.SSS2.p3.13.2" class="ltx_emph ltx_font_italic">beat matching</em> (<math id="S7.SS3.SSS2.p3.7.m7.2" class="ltx_Math" alttext="F(3,4748)=119.71" display="inline"><semantics id="S7.SS3.SSS2.p3.7.m7.2a"><mrow id="S7.SS3.SSS2.p3.7.m7.2.3" xref="S7.SS3.SSS2.p3.7.m7.2.3.cmml"><mrow id="S7.SS3.SSS2.p3.7.m7.2.3.2" xref="S7.SS3.SSS2.p3.7.m7.2.3.2.cmml"><mi id="S7.SS3.SSS2.p3.7.m7.2.3.2.2" xref="S7.SS3.SSS2.p3.7.m7.2.3.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S7.SS3.SSS2.p3.7.m7.2.3.2.1" xref="S7.SS3.SSS2.p3.7.m7.2.3.2.1.cmml">​</mo><mrow id="S7.SS3.SSS2.p3.7.m7.2.3.2.3.2" xref="S7.SS3.SSS2.p3.7.m7.2.3.2.3.1.cmml"><mo stretchy="false" id="S7.SS3.SSS2.p3.7.m7.2.3.2.3.2.1" xref="S7.SS3.SSS2.p3.7.m7.2.3.2.3.1.cmml">(</mo><mn id="S7.SS3.SSS2.p3.7.m7.1.1" xref="S7.SS3.SSS2.p3.7.m7.1.1.cmml">3</mn><mo id="S7.SS3.SSS2.p3.7.m7.2.3.2.3.2.2" xref="S7.SS3.SSS2.p3.7.m7.2.3.2.3.1.cmml">,</mo><mn id="S7.SS3.SSS2.p3.7.m7.2.2" xref="S7.SS3.SSS2.p3.7.m7.2.2.cmml">4748</mn><mo stretchy="false" id="S7.SS3.SSS2.p3.7.m7.2.3.2.3.2.3" xref="S7.SS3.SSS2.p3.7.m7.2.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S7.SS3.SSS2.p3.7.m7.2.3.1" xref="S7.SS3.SSS2.p3.7.m7.2.3.1.cmml">=</mo><mn id="S7.SS3.SSS2.p3.7.m7.2.3.3" xref="S7.SS3.SSS2.p3.7.m7.2.3.3.cmml">119.71</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p3.7.m7.2b"><apply id="S7.SS3.SSS2.p3.7.m7.2.3.cmml" xref="S7.SS3.SSS2.p3.7.m7.2.3"><eq id="S7.SS3.SSS2.p3.7.m7.2.3.1.cmml" xref="S7.SS3.SSS2.p3.7.m7.2.3.1"></eq><apply id="S7.SS3.SSS2.p3.7.m7.2.3.2.cmml" xref="S7.SS3.SSS2.p3.7.m7.2.3.2"><times id="S7.SS3.SSS2.p3.7.m7.2.3.2.1.cmml" xref="S7.SS3.SSS2.p3.7.m7.2.3.2.1"></times><ci id="S7.SS3.SSS2.p3.7.m7.2.3.2.2.cmml" xref="S7.SS3.SSS2.p3.7.m7.2.3.2.2">𝐹</ci><interval closure="open" id="S7.SS3.SSS2.p3.7.m7.2.3.2.3.1.cmml" xref="S7.SS3.SSS2.p3.7.m7.2.3.2.3.2"><cn type="integer" id="S7.SS3.SSS2.p3.7.m7.1.1.cmml" xref="S7.SS3.SSS2.p3.7.m7.1.1">3</cn><cn type="integer" id="S7.SS3.SSS2.p3.7.m7.2.2.cmml" xref="S7.SS3.SSS2.p3.7.m7.2.2">4748</cn></interval></apply><cn type="float" id="S7.SS3.SSS2.p3.7.m7.2.3.3.cmml" xref="S7.SS3.SSS2.p3.7.m7.2.3.3">119.71</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p3.7.m7.2c">F(3,4748)=119.71</annotation></semantics></math>, <math id="S7.SS3.SSS2.p3.8.m8.1" class="ltx_Math" alttext="p&lt;.05" display="inline"><semantics id="S7.SS3.SSS2.p3.8.m8.1a"><mrow id="S7.SS3.SSS2.p3.8.m8.1.1" xref="S7.SS3.SSS2.p3.8.m8.1.1.cmml"><mi id="S7.SS3.SSS2.p3.8.m8.1.1.2" xref="S7.SS3.SSS2.p3.8.m8.1.1.2.cmml">p</mi><mo id="S7.SS3.SSS2.p3.8.m8.1.1.1" xref="S7.SS3.SSS2.p3.8.m8.1.1.1.cmml">&lt;</mo><mn id="S7.SS3.SSS2.p3.8.m8.1.1.3" xref="S7.SS3.SSS2.p3.8.m8.1.1.3.cmml">.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p3.8.m8.1b"><apply id="S7.SS3.SSS2.p3.8.m8.1.1.cmml" xref="S7.SS3.SSS2.p3.8.m8.1.1"><lt id="S7.SS3.SSS2.p3.8.m8.1.1.1.cmml" xref="S7.SS3.SSS2.p3.8.m8.1.1.1"></lt><ci id="S7.SS3.SSS2.p3.8.m8.1.1.2.cmml" xref="S7.SS3.SSS2.p3.8.m8.1.1.2">𝑝</ci><cn type="float" id="S7.SS3.SSS2.p3.8.m8.1.1.3.cmml" xref="S7.SS3.SSS2.p3.8.m8.1.1.3">.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p3.8.m8.1c">p&lt;.05</annotation></semantics></math>, <math id="S7.SS3.SSS2.p3.9.m9.1" class="ltx_Math" alttext="{\text{Partial Eta Squared}}=0.071" display="inline"><semantics id="S7.SS3.SSS2.p3.9.m9.1a"><mrow id="S7.SS3.SSS2.p3.9.m9.1.1" xref="S7.SS3.SSS2.p3.9.m9.1.1.cmml"><mtext id="S7.SS3.SSS2.p3.9.m9.1.1.2" xref="S7.SS3.SSS2.p3.9.m9.1.1.2a.cmml">Partial Eta Squared</mtext><mo id="S7.SS3.SSS2.p3.9.m9.1.1.1" xref="S7.SS3.SSS2.p3.9.m9.1.1.1.cmml">=</mo><mn id="S7.SS3.SSS2.p3.9.m9.1.1.3" xref="S7.SS3.SSS2.p3.9.m9.1.1.3.cmml">0.071</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p3.9.m9.1b"><apply id="S7.SS3.SSS2.p3.9.m9.1.1.cmml" xref="S7.SS3.SSS2.p3.9.m9.1.1"><eq id="S7.SS3.SSS2.p3.9.m9.1.1.1.cmml" xref="S7.SS3.SSS2.p3.9.m9.1.1.1"></eq><ci id="S7.SS3.SSS2.p3.9.m9.1.1.2a.cmml" xref="S7.SS3.SSS2.p3.9.m9.1.1.2"><mtext id="S7.SS3.SSS2.p3.9.m9.1.1.2.cmml" xref="S7.SS3.SSS2.p3.9.m9.1.1.2">Partial Eta Squared</mtext></ci><cn type="float" id="S7.SS3.SSS2.p3.9.m9.1.1.3.cmml" xref="S7.SS3.SSS2.p3.9.m9.1.1.3">0.071</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p3.9.m9.1c">{\text{Partial Eta Squared}}=0.071</annotation></semantics></math>),
and <em id="S7.SS3.SSS2.p3.13.3" class="ltx_emph ltx_font_italic">semantic accuracy</em> (<math id="S7.SS3.SSS2.p3.10.m10.2" class="ltx_Math" alttext="F(3,4844)=250.87" display="inline"><semantics id="S7.SS3.SSS2.p3.10.m10.2a"><mrow id="S7.SS3.SSS2.p3.10.m10.2.3" xref="S7.SS3.SSS2.p3.10.m10.2.3.cmml"><mrow id="S7.SS3.SSS2.p3.10.m10.2.3.2" xref="S7.SS3.SSS2.p3.10.m10.2.3.2.cmml"><mi id="S7.SS3.SSS2.p3.10.m10.2.3.2.2" xref="S7.SS3.SSS2.p3.10.m10.2.3.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S7.SS3.SSS2.p3.10.m10.2.3.2.1" xref="S7.SS3.SSS2.p3.10.m10.2.3.2.1.cmml">​</mo><mrow id="S7.SS3.SSS2.p3.10.m10.2.3.2.3.2" xref="S7.SS3.SSS2.p3.10.m10.2.3.2.3.1.cmml"><mo stretchy="false" id="S7.SS3.SSS2.p3.10.m10.2.3.2.3.2.1" xref="S7.SS3.SSS2.p3.10.m10.2.3.2.3.1.cmml">(</mo><mn id="S7.SS3.SSS2.p3.10.m10.1.1" xref="S7.SS3.SSS2.p3.10.m10.1.1.cmml">3</mn><mo id="S7.SS3.SSS2.p3.10.m10.2.3.2.3.2.2" xref="S7.SS3.SSS2.p3.10.m10.2.3.2.3.1.cmml">,</mo><mn id="S7.SS3.SSS2.p3.10.m10.2.2" xref="S7.SS3.SSS2.p3.10.m10.2.2.cmml">4844</mn><mo stretchy="false" id="S7.SS3.SSS2.p3.10.m10.2.3.2.3.2.3" xref="S7.SS3.SSS2.p3.10.m10.2.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S7.SS3.SSS2.p3.10.m10.2.3.1" xref="S7.SS3.SSS2.p3.10.m10.2.3.1.cmml">=</mo><mn id="S7.SS3.SSS2.p3.10.m10.2.3.3" xref="S7.SS3.SSS2.p3.10.m10.2.3.3.cmml">250.87</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p3.10.m10.2b"><apply id="S7.SS3.SSS2.p3.10.m10.2.3.cmml" xref="S7.SS3.SSS2.p3.10.m10.2.3"><eq id="S7.SS3.SSS2.p3.10.m10.2.3.1.cmml" xref="S7.SS3.SSS2.p3.10.m10.2.3.1"></eq><apply id="S7.SS3.SSS2.p3.10.m10.2.3.2.cmml" xref="S7.SS3.SSS2.p3.10.m10.2.3.2"><times id="S7.SS3.SSS2.p3.10.m10.2.3.2.1.cmml" xref="S7.SS3.SSS2.p3.10.m10.2.3.2.1"></times><ci id="S7.SS3.SSS2.p3.10.m10.2.3.2.2.cmml" xref="S7.SS3.SSS2.p3.10.m10.2.3.2.2">𝐹</ci><interval closure="open" id="S7.SS3.SSS2.p3.10.m10.2.3.2.3.1.cmml" xref="S7.SS3.SSS2.p3.10.m10.2.3.2.3.2"><cn type="integer" id="S7.SS3.SSS2.p3.10.m10.1.1.cmml" xref="S7.SS3.SSS2.p3.10.m10.1.1">3</cn><cn type="integer" id="S7.SS3.SSS2.p3.10.m10.2.2.cmml" xref="S7.SS3.SSS2.p3.10.m10.2.2">4844</cn></interval></apply><cn type="float" id="S7.SS3.SSS2.p3.10.m10.2.3.3.cmml" xref="S7.SS3.SSS2.p3.10.m10.2.3.3">250.87</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p3.10.m10.2c">F(3,4844)=250.87</annotation></semantics></math>, <math id="S7.SS3.SSS2.p3.11.m11.1" class="ltx_Math" alttext="p&lt;.01" display="inline"><semantics id="S7.SS3.SSS2.p3.11.m11.1a"><mrow id="S7.SS3.SSS2.p3.11.m11.1.1" xref="S7.SS3.SSS2.p3.11.m11.1.1.cmml"><mi id="S7.SS3.SSS2.p3.11.m11.1.1.2" xref="S7.SS3.SSS2.p3.11.m11.1.1.2.cmml">p</mi><mo id="S7.SS3.SSS2.p3.11.m11.1.1.1" xref="S7.SS3.SSS2.p3.11.m11.1.1.1.cmml">&lt;</mo><mn id="S7.SS3.SSS2.p3.11.m11.1.1.3" xref="S7.SS3.SSS2.p3.11.m11.1.1.3.cmml">.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p3.11.m11.1b"><apply id="S7.SS3.SSS2.p3.11.m11.1.1.cmml" xref="S7.SS3.SSS2.p3.11.m11.1.1"><lt id="S7.SS3.SSS2.p3.11.m11.1.1.1.cmml" xref="S7.SS3.SSS2.p3.11.m11.1.1.1"></lt><ci id="S7.SS3.SSS2.p3.11.m11.1.1.2.cmml" xref="S7.SS3.SSS2.p3.11.m11.1.1.2">𝑝</ci><cn type="float" id="S7.SS3.SSS2.p3.11.m11.1.1.3.cmml" xref="S7.SS3.SSS2.p3.11.m11.1.1.3">.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p3.11.m11.1c">p&lt;.01</annotation></semantics></math>, <math id="S7.SS3.SSS2.p3.12.m12.1" class="ltx_Math" alttext="{\text{Partial Eta Squared}}=0.134" display="inline"><semantics id="S7.SS3.SSS2.p3.12.m12.1a"><mrow id="S7.SS3.SSS2.p3.12.m12.1.1" xref="S7.SS3.SSS2.p3.12.m12.1.1.cmml"><mtext id="S7.SS3.SSS2.p3.12.m12.1.1.2" xref="S7.SS3.SSS2.p3.12.m12.1.1.2a.cmml">Partial Eta Squared</mtext><mo id="S7.SS3.SSS2.p3.12.m12.1.1.1" xref="S7.SS3.SSS2.p3.12.m12.1.1.1.cmml">=</mo><mn id="S7.SS3.SSS2.p3.12.m12.1.1.3" xref="S7.SS3.SSS2.p3.12.m12.1.1.3.cmml">0.134</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p3.12.m12.1b"><apply id="S7.SS3.SSS2.p3.12.m12.1.1.cmml" xref="S7.SS3.SSS2.p3.12.m12.1.1"><eq id="S7.SS3.SSS2.p3.12.m12.1.1.1.cmml" xref="S7.SS3.SSS2.p3.12.m12.1.1.1"></eq><ci id="S7.SS3.SSS2.p3.12.m12.1.1.2a.cmml" xref="S7.SS3.SSS2.p3.12.m12.1.1.2"><mtext id="S7.SS3.SSS2.p3.12.m12.1.1.2.cmml" xref="S7.SS3.SSS2.p3.12.m12.1.1.2">Partial Eta Squared</mtext></ci><cn type="float" id="S7.SS3.SSS2.p3.12.m12.1.1.3.cmml" xref="S7.SS3.SSS2.p3.12.m12.1.1.3">0.134</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p3.12.m12.1c">{\text{Partial Eta Squared}}=0.134</annotation></semantics></math>).
Table <a href="#S7.T1" title="Table 1 ‣ 7.2. Results ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> indicates that, in the human likeness and beat matching tests, the performance differences between Ours and the other methods are not significant. But in the semantic accuracy test, Ours notably excels over both GestureDiffuCLIP and Ours w/o semantic alignment, with a considerable margin (<math id="S7.SS3.SSS2.p3.13.m13.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="S7.SS3.SSS2.p3.13.m13.1a"><mrow id="S7.SS3.SSS2.p3.13.m13.1.1" xref="S7.SS3.SSS2.p3.13.m13.1.1.cmml"><mi id="S7.SS3.SSS2.p3.13.m13.1.1.2" xref="S7.SS3.SSS2.p3.13.m13.1.1.2.cmml">p</mi><mo id="S7.SS3.SSS2.p3.13.m13.1.1.1" xref="S7.SS3.SSS2.p3.13.m13.1.1.1.cmml">&lt;</mo><mn id="S7.SS3.SSS2.p3.13.m13.1.1.3" xref="S7.SS3.SSS2.p3.13.m13.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p3.13.m13.1b"><apply id="S7.SS3.SSS2.p3.13.m13.1.1.cmml" xref="S7.SS3.SSS2.p3.13.m13.1.1"><lt id="S7.SS3.SSS2.p3.13.m13.1.1.1.cmml" xref="S7.SS3.SSS2.p3.13.m13.1.1.1"></lt><ci id="S7.SS3.SSS2.p3.13.m13.1.1.2.cmml" xref="S7.SS3.SSS2.p3.13.m13.1.1.2">𝑝</ci><cn type="float" id="S7.SS3.SSS2.p3.13.m13.1.1.3.cmml" xref="S7.SS3.SSS2.p3.13.m13.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p3.13.m13.1c">p&lt;0.001</annotation></semantics></math>), emphasizing the vital role of the semantic alignment module in enhancing semantic perception. The left part of Figure <a href="#S7.F13" title="Figure 13 ‣ 7.2. Results ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> provides a visual demonstration, showing that the motions generated by GestureDiffuCLIP are less meaningful compared to those from Ours. These results confirm the efficiency of our system in semantic gesture synthesis.</p>
</div>
<div id="S7.SS3.SSS2.p4" class="ltx_para">
<p id="S7.SS3.SSS2.p4.16" class="ltx_p">On the BEAT Dataset, our evaluation encompasses four methods: the ground-truth gestures (GT), our system (Ours), our system without the semantic alignment module (w/o semantic alignment) for ablation, and CaMN, with its speaker ID input matched to the ground truth. In the user study, <math id="S7.SS3.SSS2.p4.1.m1.1" class="ltx_Math" alttext="99" display="inline"><semantics id="S7.SS3.SSS2.p4.1.m1.1a"><mn id="S7.SS3.SSS2.p4.1.m1.1.1" xref="S7.SS3.SSS2.p4.1.m1.1.1.cmml">99</mn><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p4.1.m1.1b"><cn type="integer" id="S7.SS3.SSS2.p4.1.m1.1.1.cmml" xref="S7.SS3.SSS2.p4.1.m1.1.1">99</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p4.1.m1.1c">99</annotation></semantics></math>, <math id="S7.SS3.SSS2.p4.2.m2.1" class="ltx_Math" alttext="103" display="inline"><semantics id="S7.SS3.SSS2.p4.2.m2.1a"><mn id="S7.SS3.SSS2.p4.2.m2.1.1" xref="S7.SS3.SSS2.p4.2.m2.1.1.cmml">103</mn><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p4.2.m2.1b"><cn type="integer" id="S7.SS3.SSS2.p4.2.m2.1.1.cmml" xref="S7.SS3.SSS2.p4.2.m2.1.1">103</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p4.2.m2.1c">103</annotation></semantics></math>, and <math id="S7.SS3.SSS2.p4.3.m3.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S7.SS3.SSS2.p4.3.m3.1a"><mn id="S7.SS3.SSS2.p4.3.m3.1.1" xref="S7.SS3.SSS2.p4.3.m3.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p4.3.m3.1b"><cn type="integer" id="S7.SS3.SSS2.p4.3.m3.1.1.cmml" xref="S7.SS3.SSS2.p4.3.m3.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p4.3.m3.1c">100</annotation></semantics></math> subjects pass the attention checks for the human likeness, beat matching, and semantic accuracy tests, respectively. Multiple one-way ANOVAs indicate that different generation methods have main effects on
<em id="S7.SS3.SSS2.p4.16.1" class="ltx_emph ltx_font_italic">human likeness</em> (<math id="S7.SS3.SSS2.p4.4.m4.2" class="ltx_Math" alttext="F(3,4748)=179.92" display="inline"><semantics id="S7.SS3.SSS2.p4.4.m4.2a"><mrow id="S7.SS3.SSS2.p4.4.m4.2.3" xref="S7.SS3.SSS2.p4.4.m4.2.3.cmml"><mrow id="S7.SS3.SSS2.p4.4.m4.2.3.2" xref="S7.SS3.SSS2.p4.4.m4.2.3.2.cmml"><mi id="S7.SS3.SSS2.p4.4.m4.2.3.2.2" xref="S7.SS3.SSS2.p4.4.m4.2.3.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S7.SS3.SSS2.p4.4.m4.2.3.2.1" xref="S7.SS3.SSS2.p4.4.m4.2.3.2.1.cmml">​</mo><mrow id="S7.SS3.SSS2.p4.4.m4.2.3.2.3.2" xref="S7.SS3.SSS2.p4.4.m4.2.3.2.3.1.cmml"><mo stretchy="false" id="S7.SS3.SSS2.p4.4.m4.2.3.2.3.2.1" xref="S7.SS3.SSS2.p4.4.m4.2.3.2.3.1.cmml">(</mo><mn id="S7.SS3.SSS2.p4.4.m4.1.1" xref="S7.SS3.SSS2.p4.4.m4.1.1.cmml">3</mn><mo id="S7.SS3.SSS2.p4.4.m4.2.3.2.3.2.2" xref="S7.SS3.SSS2.p4.4.m4.2.3.2.3.1.cmml">,</mo><mn id="S7.SS3.SSS2.p4.4.m4.2.2" xref="S7.SS3.SSS2.p4.4.m4.2.2.cmml">4748</mn><mo stretchy="false" id="S7.SS3.SSS2.p4.4.m4.2.3.2.3.2.3" xref="S7.SS3.SSS2.p4.4.m4.2.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S7.SS3.SSS2.p4.4.m4.2.3.1" xref="S7.SS3.SSS2.p4.4.m4.2.3.1.cmml">=</mo><mn id="S7.SS3.SSS2.p4.4.m4.2.3.3" xref="S7.SS3.SSS2.p4.4.m4.2.3.3.cmml">179.92</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p4.4.m4.2b"><apply id="S7.SS3.SSS2.p4.4.m4.2.3.cmml" xref="S7.SS3.SSS2.p4.4.m4.2.3"><eq id="S7.SS3.SSS2.p4.4.m4.2.3.1.cmml" xref="S7.SS3.SSS2.p4.4.m4.2.3.1"></eq><apply id="S7.SS3.SSS2.p4.4.m4.2.3.2.cmml" xref="S7.SS3.SSS2.p4.4.m4.2.3.2"><times id="S7.SS3.SSS2.p4.4.m4.2.3.2.1.cmml" xref="S7.SS3.SSS2.p4.4.m4.2.3.2.1"></times><ci id="S7.SS3.SSS2.p4.4.m4.2.3.2.2.cmml" xref="S7.SS3.SSS2.p4.4.m4.2.3.2.2">𝐹</ci><interval closure="open" id="S7.SS3.SSS2.p4.4.m4.2.3.2.3.1.cmml" xref="S7.SS3.SSS2.p4.4.m4.2.3.2.3.2"><cn type="integer" id="S7.SS3.SSS2.p4.4.m4.1.1.cmml" xref="S7.SS3.SSS2.p4.4.m4.1.1">3</cn><cn type="integer" id="S7.SS3.SSS2.p4.4.m4.2.2.cmml" xref="S7.SS3.SSS2.p4.4.m4.2.2">4748</cn></interval></apply><cn type="float" id="S7.SS3.SSS2.p4.4.m4.2.3.3.cmml" xref="S7.SS3.SSS2.p4.4.m4.2.3.3">179.92</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p4.4.m4.2c">F(3,4748)=179.92</annotation></semantics></math>, <math id="S7.SS3.SSS2.p4.5.m5.1" class="ltx_Math" alttext="p&lt;.01" display="inline"><semantics id="S7.SS3.SSS2.p4.5.m5.1a"><mrow id="S7.SS3.SSS2.p4.5.m5.1.1" xref="S7.SS3.SSS2.p4.5.m5.1.1.cmml"><mi id="S7.SS3.SSS2.p4.5.m5.1.1.2" xref="S7.SS3.SSS2.p4.5.m5.1.1.2.cmml">p</mi><mo id="S7.SS3.SSS2.p4.5.m5.1.1.1" xref="S7.SS3.SSS2.p4.5.m5.1.1.1.cmml">&lt;</mo><mn id="S7.SS3.SSS2.p4.5.m5.1.1.3" xref="S7.SS3.SSS2.p4.5.m5.1.1.3.cmml">.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p4.5.m5.1b"><apply id="S7.SS3.SSS2.p4.5.m5.1.1.cmml" xref="S7.SS3.SSS2.p4.5.m5.1.1"><lt id="S7.SS3.SSS2.p4.5.m5.1.1.1.cmml" xref="S7.SS3.SSS2.p4.5.m5.1.1.1"></lt><ci id="S7.SS3.SSS2.p4.5.m5.1.1.2.cmml" xref="S7.SS3.SSS2.p4.5.m5.1.1.2">𝑝</ci><cn type="float" id="S7.SS3.SSS2.p4.5.m5.1.1.3.cmml" xref="S7.SS3.SSS2.p4.5.m5.1.1.3">.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p4.5.m5.1c">p&lt;.01</annotation></semantics></math>, <math id="S7.SS3.SSS2.p4.6.m6.1" class="ltx_Math" alttext="{\text{Partial Eta Squared}}=0.102" display="inline"><semantics id="S7.SS3.SSS2.p4.6.m6.1a"><mrow id="S7.SS3.SSS2.p4.6.m6.1.1" xref="S7.SS3.SSS2.p4.6.m6.1.1.cmml"><mtext id="S7.SS3.SSS2.p4.6.m6.1.1.2" xref="S7.SS3.SSS2.p4.6.m6.1.1.2a.cmml">Partial Eta Squared</mtext><mo id="S7.SS3.SSS2.p4.6.m6.1.1.1" xref="S7.SS3.SSS2.p4.6.m6.1.1.1.cmml">=</mo><mn id="S7.SS3.SSS2.p4.6.m6.1.1.3" xref="S7.SS3.SSS2.p4.6.m6.1.1.3.cmml">0.102</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p4.6.m6.1b"><apply id="S7.SS3.SSS2.p4.6.m6.1.1.cmml" xref="S7.SS3.SSS2.p4.6.m6.1.1"><eq id="S7.SS3.SSS2.p4.6.m6.1.1.1.cmml" xref="S7.SS3.SSS2.p4.6.m6.1.1.1"></eq><ci id="S7.SS3.SSS2.p4.6.m6.1.1.2a.cmml" xref="S7.SS3.SSS2.p4.6.m6.1.1.2"><mtext id="S7.SS3.SSS2.p4.6.m6.1.1.2.cmml" xref="S7.SS3.SSS2.p4.6.m6.1.1.2">Partial Eta Squared</mtext></ci><cn type="float" id="S7.SS3.SSS2.p4.6.m6.1.1.3.cmml" xref="S7.SS3.SSS2.p4.6.m6.1.1.3">0.102</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p4.6.m6.1c">{\text{Partial Eta Squared}}=0.102</annotation></semantics></math>),
<em id="S7.SS3.SSS2.p4.16.2" class="ltx_emph ltx_font_italic">beat matching</em> (<math id="S7.SS3.SSS2.p4.7.m7.2" class="ltx_Math" alttext="F(3,4940)=271.35" display="inline"><semantics id="S7.SS3.SSS2.p4.7.m7.2a"><mrow id="S7.SS3.SSS2.p4.7.m7.2.3" xref="S7.SS3.SSS2.p4.7.m7.2.3.cmml"><mrow id="S7.SS3.SSS2.p4.7.m7.2.3.2" xref="S7.SS3.SSS2.p4.7.m7.2.3.2.cmml"><mi id="S7.SS3.SSS2.p4.7.m7.2.3.2.2" xref="S7.SS3.SSS2.p4.7.m7.2.3.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S7.SS3.SSS2.p4.7.m7.2.3.2.1" xref="S7.SS3.SSS2.p4.7.m7.2.3.2.1.cmml">​</mo><mrow id="S7.SS3.SSS2.p4.7.m7.2.3.2.3.2" xref="S7.SS3.SSS2.p4.7.m7.2.3.2.3.1.cmml"><mo stretchy="false" id="S7.SS3.SSS2.p4.7.m7.2.3.2.3.2.1" xref="S7.SS3.SSS2.p4.7.m7.2.3.2.3.1.cmml">(</mo><mn id="S7.SS3.SSS2.p4.7.m7.1.1" xref="S7.SS3.SSS2.p4.7.m7.1.1.cmml">3</mn><mo id="S7.SS3.SSS2.p4.7.m7.2.3.2.3.2.2" xref="S7.SS3.SSS2.p4.7.m7.2.3.2.3.1.cmml">,</mo><mn id="S7.SS3.SSS2.p4.7.m7.2.2" xref="S7.SS3.SSS2.p4.7.m7.2.2.cmml">4940</mn><mo stretchy="false" id="S7.SS3.SSS2.p4.7.m7.2.3.2.3.2.3" xref="S7.SS3.SSS2.p4.7.m7.2.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S7.SS3.SSS2.p4.7.m7.2.3.1" xref="S7.SS3.SSS2.p4.7.m7.2.3.1.cmml">=</mo><mn id="S7.SS3.SSS2.p4.7.m7.2.3.3" xref="S7.SS3.SSS2.p4.7.m7.2.3.3.cmml">271.35</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p4.7.m7.2b"><apply id="S7.SS3.SSS2.p4.7.m7.2.3.cmml" xref="S7.SS3.SSS2.p4.7.m7.2.3"><eq id="S7.SS3.SSS2.p4.7.m7.2.3.1.cmml" xref="S7.SS3.SSS2.p4.7.m7.2.3.1"></eq><apply id="S7.SS3.SSS2.p4.7.m7.2.3.2.cmml" xref="S7.SS3.SSS2.p4.7.m7.2.3.2"><times id="S7.SS3.SSS2.p4.7.m7.2.3.2.1.cmml" xref="S7.SS3.SSS2.p4.7.m7.2.3.2.1"></times><ci id="S7.SS3.SSS2.p4.7.m7.2.3.2.2.cmml" xref="S7.SS3.SSS2.p4.7.m7.2.3.2.2">𝐹</ci><interval closure="open" id="S7.SS3.SSS2.p4.7.m7.2.3.2.3.1.cmml" xref="S7.SS3.SSS2.p4.7.m7.2.3.2.3.2"><cn type="integer" id="S7.SS3.SSS2.p4.7.m7.1.1.cmml" xref="S7.SS3.SSS2.p4.7.m7.1.1">3</cn><cn type="integer" id="S7.SS3.SSS2.p4.7.m7.2.2.cmml" xref="S7.SS3.SSS2.p4.7.m7.2.2">4940</cn></interval></apply><cn type="float" id="S7.SS3.SSS2.p4.7.m7.2.3.3.cmml" xref="S7.SS3.SSS2.p4.7.m7.2.3.3">271.35</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p4.7.m7.2c">F(3,4940)=271.35</annotation></semantics></math>, <math id="S7.SS3.SSS2.p4.8.m8.1" class="ltx_Math" alttext="p&lt;.01" display="inline"><semantics id="S7.SS3.SSS2.p4.8.m8.1a"><mrow id="S7.SS3.SSS2.p4.8.m8.1.1" xref="S7.SS3.SSS2.p4.8.m8.1.1.cmml"><mi id="S7.SS3.SSS2.p4.8.m8.1.1.2" xref="S7.SS3.SSS2.p4.8.m8.1.1.2.cmml">p</mi><mo id="S7.SS3.SSS2.p4.8.m8.1.1.1" xref="S7.SS3.SSS2.p4.8.m8.1.1.1.cmml">&lt;</mo><mn id="S7.SS3.SSS2.p4.8.m8.1.1.3" xref="S7.SS3.SSS2.p4.8.m8.1.1.3.cmml">.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p4.8.m8.1b"><apply id="S7.SS3.SSS2.p4.8.m8.1.1.cmml" xref="S7.SS3.SSS2.p4.8.m8.1.1"><lt id="S7.SS3.SSS2.p4.8.m8.1.1.1.cmml" xref="S7.SS3.SSS2.p4.8.m8.1.1.1"></lt><ci id="S7.SS3.SSS2.p4.8.m8.1.1.2.cmml" xref="S7.SS3.SSS2.p4.8.m8.1.1.2">𝑝</ci><cn type="float" id="S7.SS3.SSS2.p4.8.m8.1.1.3.cmml" xref="S7.SS3.SSS2.p4.8.m8.1.1.3">.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p4.8.m8.1c">p&lt;.01</annotation></semantics></math>, <math id="S7.SS3.SSS2.p4.9.m9.1" class="ltx_Math" alttext="{\text{Partial Eta Squared}}=0.141" display="inline"><semantics id="S7.SS3.SSS2.p4.9.m9.1a"><mrow id="S7.SS3.SSS2.p4.9.m9.1.1" xref="S7.SS3.SSS2.p4.9.m9.1.1.cmml"><mtext id="S7.SS3.SSS2.p4.9.m9.1.1.2" xref="S7.SS3.SSS2.p4.9.m9.1.1.2a.cmml">Partial Eta Squared</mtext><mo id="S7.SS3.SSS2.p4.9.m9.1.1.1" xref="S7.SS3.SSS2.p4.9.m9.1.1.1.cmml">=</mo><mn id="S7.SS3.SSS2.p4.9.m9.1.1.3" xref="S7.SS3.SSS2.p4.9.m9.1.1.3.cmml">0.141</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p4.9.m9.1b"><apply id="S7.SS3.SSS2.p4.9.m9.1.1.cmml" xref="S7.SS3.SSS2.p4.9.m9.1.1"><eq id="S7.SS3.SSS2.p4.9.m9.1.1.1.cmml" xref="S7.SS3.SSS2.p4.9.m9.1.1.1"></eq><ci id="S7.SS3.SSS2.p4.9.m9.1.1.2a.cmml" xref="S7.SS3.SSS2.p4.9.m9.1.1.2"><mtext id="S7.SS3.SSS2.p4.9.m9.1.1.2.cmml" xref="S7.SS3.SSS2.p4.9.m9.1.1.2">Partial Eta Squared</mtext></ci><cn type="float" id="S7.SS3.SSS2.p4.9.m9.1.1.3.cmml" xref="S7.SS3.SSS2.p4.9.m9.1.1.3">0.141</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p4.9.m9.1c">{\text{Partial Eta Squared}}=0.141</annotation></semantics></math>),
and <em id="S7.SS3.SSS2.p4.16.3" class="ltx_emph ltx_font_italic">semantic accuracy</em> (<math id="S7.SS3.SSS2.p4.10.m10.2" class="ltx_Math" alttext="F(3,4796)=151.52" display="inline"><semantics id="S7.SS3.SSS2.p4.10.m10.2a"><mrow id="S7.SS3.SSS2.p4.10.m10.2.3" xref="S7.SS3.SSS2.p4.10.m10.2.3.cmml"><mrow id="S7.SS3.SSS2.p4.10.m10.2.3.2" xref="S7.SS3.SSS2.p4.10.m10.2.3.2.cmml"><mi id="S7.SS3.SSS2.p4.10.m10.2.3.2.2" xref="S7.SS3.SSS2.p4.10.m10.2.3.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S7.SS3.SSS2.p4.10.m10.2.3.2.1" xref="S7.SS3.SSS2.p4.10.m10.2.3.2.1.cmml">​</mo><mrow id="S7.SS3.SSS2.p4.10.m10.2.3.2.3.2" xref="S7.SS3.SSS2.p4.10.m10.2.3.2.3.1.cmml"><mo stretchy="false" id="S7.SS3.SSS2.p4.10.m10.2.3.2.3.2.1" xref="S7.SS3.SSS2.p4.10.m10.2.3.2.3.1.cmml">(</mo><mn id="S7.SS3.SSS2.p4.10.m10.1.1" xref="S7.SS3.SSS2.p4.10.m10.1.1.cmml">3</mn><mo id="S7.SS3.SSS2.p4.10.m10.2.3.2.3.2.2" xref="S7.SS3.SSS2.p4.10.m10.2.3.2.3.1.cmml">,</mo><mn id="S7.SS3.SSS2.p4.10.m10.2.2" xref="S7.SS3.SSS2.p4.10.m10.2.2.cmml">4796</mn><mo stretchy="false" id="S7.SS3.SSS2.p4.10.m10.2.3.2.3.2.3" xref="S7.SS3.SSS2.p4.10.m10.2.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S7.SS3.SSS2.p4.10.m10.2.3.1" xref="S7.SS3.SSS2.p4.10.m10.2.3.1.cmml">=</mo><mn id="S7.SS3.SSS2.p4.10.m10.2.3.3" xref="S7.SS3.SSS2.p4.10.m10.2.3.3.cmml">151.52</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p4.10.m10.2b"><apply id="S7.SS3.SSS2.p4.10.m10.2.3.cmml" xref="S7.SS3.SSS2.p4.10.m10.2.3"><eq id="S7.SS3.SSS2.p4.10.m10.2.3.1.cmml" xref="S7.SS3.SSS2.p4.10.m10.2.3.1"></eq><apply id="S7.SS3.SSS2.p4.10.m10.2.3.2.cmml" xref="S7.SS3.SSS2.p4.10.m10.2.3.2"><times id="S7.SS3.SSS2.p4.10.m10.2.3.2.1.cmml" xref="S7.SS3.SSS2.p4.10.m10.2.3.2.1"></times><ci id="S7.SS3.SSS2.p4.10.m10.2.3.2.2.cmml" xref="S7.SS3.SSS2.p4.10.m10.2.3.2.2">𝐹</ci><interval closure="open" id="S7.SS3.SSS2.p4.10.m10.2.3.2.3.1.cmml" xref="S7.SS3.SSS2.p4.10.m10.2.3.2.3.2"><cn type="integer" id="S7.SS3.SSS2.p4.10.m10.1.1.cmml" xref="S7.SS3.SSS2.p4.10.m10.1.1">3</cn><cn type="integer" id="S7.SS3.SSS2.p4.10.m10.2.2.cmml" xref="S7.SS3.SSS2.p4.10.m10.2.2">4796</cn></interval></apply><cn type="float" id="S7.SS3.SSS2.p4.10.m10.2.3.3.cmml" xref="S7.SS3.SSS2.p4.10.m10.2.3.3">151.52</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p4.10.m10.2c">F(3,4796)=151.52</annotation></semantics></math>, <math id="S7.SS3.SSS2.p4.11.m11.1" class="ltx_Math" alttext="p&lt;.01" display="inline"><semantics id="S7.SS3.SSS2.p4.11.m11.1a"><mrow id="S7.SS3.SSS2.p4.11.m11.1.1" xref="S7.SS3.SSS2.p4.11.m11.1.1.cmml"><mi id="S7.SS3.SSS2.p4.11.m11.1.1.2" xref="S7.SS3.SSS2.p4.11.m11.1.1.2.cmml">p</mi><mo id="S7.SS3.SSS2.p4.11.m11.1.1.1" xref="S7.SS3.SSS2.p4.11.m11.1.1.1.cmml">&lt;</mo><mn id="S7.SS3.SSS2.p4.11.m11.1.1.3" xref="S7.SS3.SSS2.p4.11.m11.1.1.3.cmml">.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p4.11.m11.1b"><apply id="S7.SS3.SSS2.p4.11.m11.1.1.cmml" xref="S7.SS3.SSS2.p4.11.m11.1.1"><lt id="S7.SS3.SSS2.p4.11.m11.1.1.1.cmml" xref="S7.SS3.SSS2.p4.11.m11.1.1.1"></lt><ci id="S7.SS3.SSS2.p4.11.m11.1.1.2.cmml" xref="S7.SS3.SSS2.p4.11.m11.1.1.2">𝑝</ci><cn type="float" id="S7.SS3.SSS2.p4.11.m11.1.1.3.cmml" xref="S7.SS3.SSS2.p4.11.m11.1.1.3">.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p4.11.m11.1c">p&lt;.01</annotation></semantics></math>, <span id="S7.SS3.SSS2.p4.16.4" class="ltx_text ltx_markedasmath">Partial</span> <span id="S7.SS3.SSS2.p4.16.5" class="ltx_text ltx_markedasmath">Eta</span> <math id="S7.SS3.SSS2.p4.14.m14.1" class="ltx_Math" alttext="{\text{Squared}}=0.087" display="inline"><semantics id="S7.SS3.SSS2.p4.14.m14.1a"><mrow id="S7.SS3.SSS2.p4.14.m14.1.1" xref="S7.SS3.SSS2.p4.14.m14.1.1.cmml"><mtext id="S7.SS3.SSS2.p4.14.m14.1.1.2" xref="S7.SS3.SSS2.p4.14.m14.1.1.2a.cmml">Squared</mtext><mo id="S7.SS3.SSS2.p4.14.m14.1.1.1" xref="S7.SS3.SSS2.p4.14.m14.1.1.1.cmml">=</mo><mn id="S7.SS3.SSS2.p4.14.m14.1.1.3" xref="S7.SS3.SSS2.p4.14.m14.1.1.3.cmml">0.087</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p4.14.m14.1b"><apply id="S7.SS3.SSS2.p4.14.m14.1.1.cmml" xref="S7.SS3.SSS2.p4.14.m14.1.1"><eq id="S7.SS3.SSS2.p4.14.m14.1.1.1.cmml" xref="S7.SS3.SSS2.p4.14.m14.1.1.1"></eq><ci id="S7.SS3.SSS2.p4.14.m14.1.1.2a.cmml" xref="S7.SS3.SSS2.p4.14.m14.1.1.2"><mtext id="S7.SS3.SSS2.p4.14.m14.1.1.2.cmml" xref="S7.SS3.SSS2.p4.14.m14.1.1.2">Squared</mtext></ci><cn type="float" id="S7.SS3.SSS2.p4.14.m14.1.1.3.cmml" xref="S7.SS3.SSS2.p4.14.m14.1.1.3">0.087</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p4.14.m14.1c">{\text{Squared}}=0.087</annotation></semantics></math>).
As shown in Table <a href="#S7.T1" title="Table 1 ‣ 7.2. Results ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, in the human likeness and beat matching tests, GT, Ours, and Ours w/o semantic alignment perform comparably and surpass CaMN (<math id="S7.SS3.SSS2.p4.15.m15.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="S7.SS3.SSS2.p4.15.m15.1a"><mrow id="S7.SS3.SSS2.p4.15.m15.1.1" xref="S7.SS3.SSS2.p4.15.m15.1.1.cmml"><mi id="S7.SS3.SSS2.p4.15.m15.1.1.2" xref="S7.SS3.SSS2.p4.15.m15.1.1.2.cmml">p</mi><mo id="S7.SS3.SSS2.p4.15.m15.1.1.1" xref="S7.SS3.SSS2.p4.15.m15.1.1.1.cmml">&lt;</mo><mn id="S7.SS3.SSS2.p4.15.m15.1.1.3" xref="S7.SS3.SSS2.p4.15.m15.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p4.15.m15.1b"><apply id="S7.SS3.SSS2.p4.15.m15.1.1.cmml" xref="S7.SS3.SSS2.p4.15.m15.1.1"><lt id="S7.SS3.SSS2.p4.15.m15.1.1.1.cmml" xref="S7.SS3.SSS2.p4.15.m15.1.1.1"></lt><ci id="S7.SS3.SSS2.p4.15.m15.1.1.2.cmml" xref="S7.SS3.SSS2.p4.15.m15.1.1.2">𝑝</ci><cn type="float" id="S7.SS3.SSS2.p4.15.m15.1.1.3.cmml" xref="S7.SS3.SSS2.p4.15.m15.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p4.15.m15.1c">p&lt;0.001</annotation></semantics></math>). For the semantic accuracy test, Ours outscore other baselines (<math id="S7.SS3.SSS2.p4.16.m16.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="S7.SS3.SSS2.p4.16.m16.1a"><mrow id="S7.SS3.SSS2.p4.16.m16.1.1" xref="S7.SS3.SSS2.p4.16.m16.1.1.cmml"><mi id="S7.SS3.SSS2.p4.16.m16.1.1.2" xref="S7.SS3.SSS2.p4.16.m16.1.1.2.cmml">p</mi><mo id="S7.SS3.SSS2.p4.16.m16.1.1.1" xref="S7.SS3.SSS2.p4.16.m16.1.1.1.cmml">&lt;</mo><mn id="S7.SS3.SSS2.p4.16.m16.1.1.3" xref="S7.SS3.SSS2.p4.16.m16.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS2.p4.16.m16.1b"><apply id="S7.SS3.SSS2.p4.16.m16.1.1.cmml" xref="S7.SS3.SSS2.p4.16.m16.1.1"><lt id="S7.SS3.SSS2.p4.16.m16.1.1.1.cmml" xref="S7.SS3.SSS2.p4.16.m16.1.1.1"></lt><ci id="S7.SS3.SSS2.p4.16.m16.1.1.2.cmml" xref="S7.SS3.SSS2.p4.16.m16.1.1.2">𝑝</ci><cn type="float" id="S7.SS3.SSS2.p4.16.m16.1.1.3.cmml" xref="S7.SS3.SSS2.p4.16.m16.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS2.p4.16.m16.1c">p&lt;0.001</annotation></semantics></math>), but the score of Ours (w/o semantic alignment) decreases significantly due to the lack of semantic alignment. It highlights the essential role of the semantic alignment module in maintaining semantic consistency between speech and gestures. The right part of Figure <a href="#S7.F13" title="Figure 13 ‣ 7.2. Results ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> illustrates that gestures generated by our system exhibit greater communicative efficacy compared to those produced by CaMN.</p>
</div>
<figure id="S7.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S7.T2.26.3.1" class="ltx_text" style="font-size:90%;">Table 2</span>. </span><span id="S7.T2.4.2" class="ltx_text" style="font-size:90%;">Quantitative evaluation on the ZEGGS and BEAT Datasets. This table reports the mean (<math id="S7.T2.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.T2.3.1.m1.1b"><mo id="S7.T2.3.1.m1.1.1" xref="S7.T2.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.T2.3.1.m1.1c"><csymbol cd="latexml" id="S7.T2.3.1.m1.1.1.cmml" xref="S7.T2.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.3.1.m1.1d">\pm</annotation></semantics></math> standard deviation) values for each metric by synthesizing on the test data <math id="S7.T2.4.2.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S7.T2.4.2.m2.1b"><mn id="S7.T2.4.2.m2.1.1" xref="S7.T2.4.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S7.T2.4.2.m2.1c"><cn type="integer" id="S7.T2.4.2.m2.1.1.cmml" xref="S7.T2.4.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.4.2.m2.1d">10</annotation></semantics></math> times.</span></figcaption>
<table id="S7.T2.24" class="ltx_tabular ltx_align_middle">
<tr id="S7.T2.6.2" class="ltx_tr">
<td id="S7.T2.6.2.3" class="ltx_td ltx_align_left ltx_border_tt">Dataset</td>
<td id="S7.T2.6.2.4" class="ltx_td ltx_align_left ltx_border_tt">System</td>
<td id="S7.T2.5.1.1" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S7.T2.5.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.5.1.1.1.1" class="ltx_p">FGD <math id="S7.T2.5.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S7.T2.5.1.1.1.1.m1.1a"><mo stretchy="false" id="S7.T2.5.1.1.1.1.m1.1.1" xref="S7.T2.5.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S7.T2.5.1.1.1.1.m1.1b"><ci id="S7.T2.5.1.1.1.1.m1.1.1.cmml" xref="S7.T2.5.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.5.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T2.6.2.2" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S7.T2.6.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.6.2.2.1.1" class="ltx_p">SC <math id="S7.T2.6.2.2.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S7.T2.6.2.2.1.1.m1.1a"><mo stretchy="false" id="S7.T2.6.2.2.1.1.m1.1.1" xref="S7.T2.6.2.2.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S7.T2.6.2.2.1.1.m1.1b"><ci id="S7.T2.6.2.2.1.1.m1.1.1.cmml" xref="S7.T2.6.2.2.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.6.2.2.1.1.m1.1c">\uparrow</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S7.T2.7.3" class="ltx_tr">
<td id="S7.T2.7.3.2" class="ltx_td ltx_align_left ltx_border_tt" rowspan="4"><span id="S7.T2.7.3.2.1" class="ltx_text">ZEGGS</span></td>
<td id="S7.T2.7.3.3" class="ltx_td ltx_align_left ltx_border_tt">GT</td>
<td id="S7.T2.7.3.4" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S7.T2.7.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.7.3.4.1.1" class="ltx_p">-</span>
</span>
</td>
<td id="S7.T2.7.3.1" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="S7.T2.7.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.7.3.1.1.1" class="ltx_p"><math id="S7.T2.7.3.1.1.1.m1.1" class="ltx_Math" alttext="0.55" display="inline"><semantics id="S7.T2.7.3.1.1.1.m1.1a"><mn id="S7.T2.7.3.1.1.1.m1.1.1" xref="S7.T2.7.3.1.1.1.m1.1.1.cmml">0.55</mn><annotation-xml encoding="MathML-Content" id="S7.T2.7.3.1.1.1.m1.1b"><cn type="float" id="S7.T2.7.3.1.1.1.m1.1.1.cmml" xref="S7.T2.7.3.1.1.1.m1.1.1">0.55</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.7.3.1.1.1.m1.1c">0.55</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S7.T2.9.5" class="ltx_tr">
<td id="S7.T2.9.5.3" class="ltx_td ltx_align_left ltx_border_t">GestureDiffuCLIP</td>
<td id="S7.T2.8.4.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S7.T2.8.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.8.4.1.1.1" class="ltx_p"><math id="S7.T2.8.4.1.1.1.m1.1" class="ltx_Math" alttext="81.73\pm 3.27" display="inline"><semantics id="S7.T2.8.4.1.1.1.m1.1a"><mrow id="S7.T2.8.4.1.1.1.m1.1.1" xref="S7.T2.8.4.1.1.1.m1.1.1.cmml"><mn id="S7.T2.8.4.1.1.1.m1.1.1.2" xref="S7.T2.8.4.1.1.1.m1.1.1.2.cmml">81.73</mn><mo id="S7.T2.8.4.1.1.1.m1.1.1.1" xref="S7.T2.8.4.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T2.8.4.1.1.1.m1.1.1.3" xref="S7.T2.8.4.1.1.1.m1.1.1.3.cmml">3.27</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T2.8.4.1.1.1.m1.1b"><apply id="S7.T2.8.4.1.1.1.m1.1.1.cmml" xref="S7.T2.8.4.1.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T2.8.4.1.1.1.m1.1.1.1.cmml" xref="S7.T2.8.4.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T2.8.4.1.1.1.m1.1.1.2.cmml" xref="S7.T2.8.4.1.1.1.m1.1.1.2">81.73</cn><cn type="float" id="S7.T2.8.4.1.1.1.m1.1.1.3.cmml" xref="S7.T2.8.4.1.1.1.m1.1.1.3">3.27</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.8.4.1.1.1.m1.1c">81.73\pm 3.27</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T2.9.5.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S7.T2.9.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.9.5.2.1.1" class="ltx_p"><math id="S7.T2.9.5.2.1.1.m1.1" class="ltx_Math" alttext="0.21\pm 0.07" display="inline"><semantics id="S7.T2.9.5.2.1.1.m1.1a"><mrow id="S7.T2.9.5.2.1.1.m1.1.1" xref="S7.T2.9.5.2.1.1.m1.1.1.cmml"><mn id="S7.T2.9.5.2.1.1.m1.1.1.2" xref="S7.T2.9.5.2.1.1.m1.1.1.2.cmml">0.21</mn><mo id="S7.T2.9.5.2.1.1.m1.1.1.1" xref="S7.T2.9.5.2.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T2.9.5.2.1.1.m1.1.1.3" xref="S7.T2.9.5.2.1.1.m1.1.1.3.cmml">0.07</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T2.9.5.2.1.1.m1.1b"><apply id="S7.T2.9.5.2.1.1.m1.1.1.cmml" xref="S7.T2.9.5.2.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T2.9.5.2.1.1.m1.1.1.1.cmml" xref="S7.T2.9.5.2.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T2.9.5.2.1.1.m1.1.1.2.cmml" xref="S7.T2.9.5.2.1.1.m1.1.1.2">0.21</cn><cn type="float" id="S7.T2.9.5.2.1.1.m1.1.1.3.cmml" xref="S7.T2.9.5.2.1.1.m1.1.1.3">0.07</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.9.5.2.1.1.m1.1c">0.21\pm 0.07</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S7.T2.11.7" class="ltx_tr">
<td id="S7.T2.11.7.3" class="ltx_td ltx_align_left">Ours (w/o semantic alignment)</td>
<td id="S7.T2.10.6.1" class="ltx_td ltx_align_justify">
<span id="S7.T2.10.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.10.6.1.1.1" class="ltx_p"><math id="S7.T2.10.6.1.1.1.m1.1" class="ltx_Math" alttext="82.02\pm 2.49" display="inline"><semantics id="S7.T2.10.6.1.1.1.m1.1a"><mrow id="S7.T2.10.6.1.1.1.m1.1.1" xref="S7.T2.10.6.1.1.1.m1.1.1.cmml"><mn id="S7.T2.10.6.1.1.1.m1.1.1.2" xref="S7.T2.10.6.1.1.1.m1.1.1.2.cmml">82.02</mn><mo id="S7.T2.10.6.1.1.1.m1.1.1.1" xref="S7.T2.10.6.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T2.10.6.1.1.1.m1.1.1.3" xref="S7.T2.10.6.1.1.1.m1.1.1.3.cmml">2.49</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T2.10.6.1.1.1.m1.1b"><apply id="S7.T2.10.6.1.1.1.m1.1.1.cmml" xref="S7.T2.10.6.1.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T2.10.6.1.1.1.m1.1.1.1.cmml" xref="S7.T2.10.6.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T2.10.6.1.1.1.m1.1.1.2.cmml" xref="S7.T2.10.6.1.1.1.m1.1.1.2">82.02</cn><cn type="float" id="S7.T2.10.6.1.1.1.m1.1.1.3.cmml" xref="S7.T2.10.6.1.1.1.m1.1.1.3">2.49</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.10.6.1.1.1.m1.1c">82.02\pm 2.49</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T2.11.7.2" class="ltx_td ltx_align_justify">
<span id="S7.T2.11.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.11.7.2.1.1" class="ltx_p"><math id="S7.T2.11.7.2.1.1.m1.1" class="ltx_Math" alttext="0.09\pm 0.02" display="inline"><semantics id="S7.T2.11.7.2.1.1.m1.1a"><mrow id="S7.T2.11.7.2.1.1.m1.1.1" xref="S7.T2.11.7.2.1.1.m1.1.1.cmml"><mn id="S7.T2.11.7.2.1.1.m1.1.1.2" xref="S7.T2.11.7.2.1.1.m1.1.1.2.cmml">0.09</mn><mo id="S7.T2.11.7.2.1.1.m1.1.1.1" xref="S7.T2.11.7.2.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T2.11.7.2.1.1.m1.1.1.3" xref="S7.T2.11.7.2.1.1.m1.1.1.3.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T2.11.7.2.1.1.m1.1b"><apply id="S7.T2.11.7.2.1.1.m1.1.1.cmml" xref="S7.T2.11.7.2.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T2.11.7.2.1.1.m1.1.1.1.cmml" xref="S7.T2.11.7.2.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T2.11.7.2.1.1.m1.1.1.2.cmml" xref="S7.T2.11.7.2.1.1.m1.1.1.2">0.09</cn><cn type="float" id="S7.T2.11.7.2.1.1.m1.1.1.3.cmml" xref="S7.T2.11.7.2.1.1.m1.1.1.3">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.11.7.2.1.1.m1.1c">0.09\pm 0.02</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S7.T2.13.9" class="ltx_tr">
<td id="S7.T2.13.9.3" class="ltx_td ltx_align_left">Ours (w/ naive indexer)</td>
<td id="S7.T2.12.8.1" class="ltx_td ltx_align_justify">
<span id="S7.T2.12.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.12.8.1.1.1" class="ltx_p"><math id="S7.T2.12.8.1.1.1.m1.1" class="ltx_Math" alttext="81.88\pm 2.23" display="inline"><semantics id="S7.T2.12.8.1.1.1.m1.1a"><mrow id="S7.T2.12.8.1.1.1.m1.1.1" xref="S7.T2.12.8.1.1.1.m1.1.1.cmml"><mn id="S7.T2.12.8.1.1.1.m1.1.1.2" xref="S7.T2.12.8.1.1.1.m1.1.1.2.cmml">81.88</mn><mo id="S7.T2.12.8.1.1.1.m1.1.1.1" xref="S7.T2.12.8.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T2.12.8.1.1.1.m1.1.1.3" xref="S7.T2.12.8.1.1.1.m1.1.1.3.cmml">2.23</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T2.12.8.1.1.1.m1.1b"><apply id="S7.T2.12.8.1.1.1.m1.1.1.cmml" xref="S7.T2.12.8.1.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T2.12.8.1.1.1.m1.1.1.1.cmml" xref="S7.T2.12.8.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T2.12.8.1.1.1.m1.1.1.2.cmml" xref="S7.T2.12.8.1.1.1.m1.1.1.2">81.88</cn><cn type="float" id="S7.T2.12.8.1.1.1.m1.1.1.3.cmml" xref="S7.T2.12.8.1.1.1.m1.1.1.3">2.23</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.12.8.1.1.1.m1.1c">81.88\pm 2.23</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T2.13.9.2" class="ltx_td ltx_align_justify">
<span id="S7.T2.13.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.13.9.2.1.1" class="ltx_p"><math id="S7.T2.13.9.2.1.1.m1.1" class="ltx_Math" alttext="0.30\pm 0.03" display="inline"><semantics id="S7.T2.13.9.2.1.1.m1.1a"><mrow id="S7.T2.13.9.2.1.1.m1.1.1" xref="S7.T2.13.9.2.1.1.m1.1.1.cmml"><mn id="S7.T2.13.9.2.1.1.m1.1.1.2" xref="S7.T2.13.9.2.1.1.m1.1.1.2.cmml">0.30</mn><mo id="S7.T2.13.9.2.1.1.m1.1.1.1" xref="S7.T2.13.9.2.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T2.13.9.2.1.1.m1.1.1.3" xref="S7.T2.13.9.2.1.1.m1.1.1.3.cmml">0.03</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T2.13.9.2.1.1.m1.1b"><apply id="S7.T2.13.9.2.1.1.m1.1.1.cmml" xref="S7.T2.13.9.2.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T2.13.9.2.1.1.m1.1.1.1.cmml" xref="S7.T2.13.9.2.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T2.13.9.2.1.1.m1.1.1.2.cmml" xref="S7.T2.13.9.2.1.1.m1.1.1.2">0.30</cn><cn type="float" id="S7.T2.13.9.2.1.1.m1.1.1.3.cmml" xref="S7.T2.13.9.2.1.1.m1.1.1.3">0.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.13.9.2.1.1.m1.1c">0.30\pm 0.03</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S7.T2.15.11" class="ltx_tr">
<td id="S7.T2.15.11.3" class="ltx_td"></td>
<td id="S7.T2.15.11.4" class="ltx_td ltx_align_left">Ours</td>
<td id="S7.T2.14.10.1" class="ltx_td ltx_align_justify">
<span id="S7.T2.14.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.14.10.1.1.1" class="ltx_p"><math id="S7.T2.14.10.1.1.1.m1.1" class="ltx_Math" alttext="\bm{81.22\pm 2.53}" display="inline"><semantics id="S7.T2.14.10.1.1.1.m1.1a"><mrow id="S7.T2.14.10.1.1.1.m1.1.1" xref="S7.T2.14.10.1.1.1.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T2.14.10.1.1.1.m1.1.1.2" xref="S7.T2.14.10.1.1.1.m1.1.1.2.cmml">81.22</mn><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T2.14.10.1.1.1.m1.1.1.1" xref="S7.T2.14.10.1.1.1.m1.1.1.1.cmml">±</mo><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T2.14.10.1.1.1.m1.1.1.3" xref="S7.T2.14.10.1.1.1.m1.1.1.3.cmml">2.53</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T2.14.10.1.1.1.m1.1b"><apply id="S7.T2.14.10.1.1.1.m1.1.1.cmml" xref="S7.T2.14.10.1.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T2.14.10.1.1.1.m1.1.1.1.cmml" xref="S7.T2.14.10.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T2.14.10.1.1.1.m1.1.1.2.cmml" xref="S7.T2.14.10.1.1.1.m1.1.1.2">81.22</cn><cn type="float" id="S7.T2.14.10.1.1.1.m1.1.1.3.cmml" xref="S7.T2.14.10.1.1.1.m1.1.1.3">2.53</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.14.10.1.1.1.m1.1c">\bm{81.22\pm 2.53}</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T2.15.11.2" class="ltx_td ltx_align_justify">
<span id="S7.T2.15.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.15.11.2.1.1" class="ltx_p"><math id="S7.T2.15.11.2.1.1.m1.1" class="ltx_Math" alttext="\bm{0.38\pm 0.05}" display="inline"><semantics id="S7.T2.15.11.2.1.1.m1.1a"><mrow id="S7.T2.15.11.2.1.1.m1.1.1" xref="S7.T2.15.11.2.1.1.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T2.15.11.2.1.1.m1.1.1.2" xref="S7.T2.15.11.2.1.1.m1.1.1.2.cmml">0.38</mn><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T2.15.11.2.1.1.m1.1.1.1" xref="S7.T2.15.11.2.1.1.m1.1.1.1.cmml">±</mo><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T2.15.11.2.1.1.m1.1.1.3" xref="S7.T2.15.11.2.1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T2.15.11.2.1.1.m1.1b"><apply id="S7.T2.15.11.2.1.1.m1.1.1.cmml" xref="S7.T2.15.11.2.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T2.15.11.2.1.1.m1.1.1.1.cmml" xref="S7.T2.15.11.2.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T2.15.11.2.1.1.m1.1.1.2.cmml" xref="S7.T2.15.11.2.1.1.m1.1.1.2">0.38</cn><cn type="float" id="S7.T2.15.11.2.1.1.m1.1.1.3.cmml" xref="S7.T2.15.11.2.1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.15.11.2.1.1.m1.1c">\bm{0.38\pm 0.05}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S7.T2.16.12" class="ltx_tr">
<td id="S7.T2.16.12.2" class="ltx_td ltx_align_left ltx_border_t" rowspan="4"><span id="S7.T2.16.12.2.1" class="ltx_text">BEAT</span></td>
<td id="S7.T2.16.12.3" class="ltx_td ltx_align_left ltx_border_t">GT</td>
<td id="S7.T2.16.12.4" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S7.T2.16.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.16.12.4.1.1" class="ltx_p">-</span>
</span>
</td>
<td id="S7.T2.16.12.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S7.T2.16.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.16.12.1.1.1" class="ltx_p"><math id="S7.T2.16.12.1.1.1.m1.1" class="ltx_Math" alttext="0.65" display="inline"><semantics id="S7.T2.16.12.1.1.1.m1.1a"><mn id="S7.T2.16.12.1.1.1.m1.1.1" xref="S7.T2.16.12.1.1.1.m1.1.1.cmml">0.65</mn><annotation-xml encoding="MathML-Content" id="S7.T2.16.12.1.1.1.m1.1b"><cn type="float" id="S7.T2.16.12.1.1.1.m1.1.1.cmml" xref="S7.T2.16.12.1.1.1.m1.1.1">0.65</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.16.12.1.1.1.m1.1c">0.65</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S7.T2.18.14" class="ltx_tr">
<td id="S7.T2.18.14.3" class="ltx_td ltx_align_left">CaMN</td>
<td id="S7.T2.17.13.1" class="ltx_td ltx_align_justify">
<span id="S7.T2.17.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.17.13.1.1.1" class="ltx_p"><math id="S7.T2.17.13.1.1.1.m1.1" class="ltx_Math" alttext="105.42\pm 0.00" display="inline"><semantics id="S7.T2.17.13.1.1.1.m1.1a"><mrow id="S7.T2.17.13.1.1.1.m1.1.1" xref="S7.T2.17.13.1.1.1.m1.1.1.cmml"><mn id="S7.T2.17.13.1.1.1.m1.1.1.2" xref="S7.T2.17.13.1.1.1.m1.1.1.2.cmml">105.42</mn><mo id="S7.T2.17.13.1.1.1.m1.1.1.1" xref="S7.T2.17.13.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T2.17.13.1.1.1.m1.1.1.3" xref="S7.T2.17.13.1.1.1.m1.1.1.3.cmml">0.00</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T2.17.13.1.1.1.m1.1b"><apply id="S7.T2.17.13.1.1.1.m1.1.1.cmml" xref="S7.T2.17.13.1.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T2.17.13.1.1.1.m1.1.1.1.cmml" xref="S7.T2.17.13.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T2.17.13.1.1.1.m1.1.1.2.cmml" xref="S7.T2.17.13.1.1.1.m1.1.1.2">105.42</cn><cn type="float" id="S7.T2.17.13.1.1.1.m1.1.1.3.cmml" xref="S7.T2.17.13.1.1.1.m1.1.1.3">0.00</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.17.13.1.1.1.m1.1c">105.42\pm 0.00</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T2.18.14.2" class="ltx_td ltx_align_justify">
<span id="S7.T2.18.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.18.14.2.1.1" class="ltx_p"><math id="S7.T2.18.14.2.1.1.m1.1" class="ltx_Math" alttext="0.21\pm 0.00" display="inline"><semantics id="S7.T2.18.14.2.1.1.m1.1a"><mrow id="S7.T2.18.14.2.1.1.m1.1.1" xref="S7.T2.18.14.2.1.1.m1.1.1.cmml"><mn id="S7.T2.18.14.2.1.1.m1.1.1.2" xref="S7.T2.18.14.2.1.1.m1.1.1.2.cmml">0.21</mn><mo id="S7.T2.18.14.2.1.1.m1.1.1.1" xref="S7.T2.18.14.2.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T2.18.14.2.1.1.m1.1.1.3" xref="S7.T2.18.14.2.1.1.m1.1.1.3.cmml">0.00</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T2.18.14.2.1.1.m1.1b"><apply id="S7.T2.18.14.2.1.1.m1.1.1.cmml" xref="S7.T2.18.14.2.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T2.18.14.2.1.1.m1.1.1.1.cmml" xref="S7.T2.18.14.2.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T2.18.14.2.1.1.m1.1.1.2.cmml" xref="S7.T2.18.14.2.1.1.m1.1.1.2">0.21</cn><cn type="float" id="S7.T2.18.14.2.1.1.m1.1.1.3.cmml" xref="S7.T2.18.14.2.1.1.m1.1.1.3">0.00</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.18.14.2.1.1.m1.1c">0.21\pm 0.00</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S7.T2.20.16" class="ltx_tr">
<td id="S7.T2.20.16.3" class="ltx_td ltx_align_left">Ours (w/o semantic alignment)</td>
<td id="S7.T2.19.15.1" class="ltx_td ltx_align_justify">
<span id="S7.T2.19.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.19.15.1.1.1" class="ltx_p"><math id="S7.T2.19.15.1.1.1.m1.1" class="ltx_Math" alttext="89.73\pm 2.11" display="inline"><semantics id="S7.T2.19.15.1.1.1.m1.1a"><mrow id="S7.T2.19.15.1.1.1.m1.1.1" xref="S7.T2.19.15.1.1.1.m1.1.1.cmml"><mn id="S7.T2.19.15.1.1.1.m1.1.1.2" xref="S7.T2.19.15.1.1.1.m1.1.1.2.cmml">89.73</mn><mo id="S7.T2.19.15.1.1.1.m1.1.1.1" xref="S7.T2.19.15.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T2.19.15.1.1.1.m1.1.1.3" xref="S7.T2.19.15.1.1.1.m1.1.1.3.cmml">2.11</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T2.19.15.1.1.1.m1.1b"><apply id="S7.T2.19.15.1.1.1.m1.1.1.cmml" xref="S7.T2.19.15.1.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T2.19.15.1.1.1.m1.1.1.1.cmml" xref="S7.T2.19.15.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T2.19.15.1.1.1.m1.1.1.2.cmml" xref="S7.T2.19.15.1.1.1.m1.1.1.2">89.73</cn><cn type="float" id="S7.T2.19.15.1.1.1.m1.1.1.3.cmml" xref="S7.T2.19.15.1.1.1.m1.1.1.3">2.11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.19.15.1.1.1.m1.1c">89.73\pm 2.11</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T2.20.16.2" class="ltx_td ltx_align_justify">
<span id="S7.T2.20.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.20.16.2.1.1" class="ltx_p"><math id="S7.T2.20.16.2.1.1.m1.1" class="ltx_Math" alttext="0.08\pm 0.02" display="inline"><semantics id="S7.T2.20.16.2.1.1.m1.1a"><mrow id="S7.T2.20.16.2.1.1.m1.1.1" xref="S7.T2.20.16.2.1.1.m1.1.1.cmml"><mn id="S7.T2.20.16.2.1.1.m1.1.1.2" xref="S7.T2.20.16.2.1.1.m1.1.1.2.cmml">0.08</mn><mo id="S7.T2.20.16.2.1.1.m1.1.1.1" xref="S7.T2.20.16.2.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T2.20.16.2.1.1.m1.1.1.3" xref="S7.T2.20.16.2.1.1.m1.1.1.3.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T2.20.16.2.1.1.m1.1b"><apply id="S7.T2.20.16.2.1.1.m1.1.1.cmml" xref="S7.T2.20.16.2.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T2.20.16.2.1.1.m1.1.1.1.cmml" xref="S7.T2.20.16.2.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T2.20.16.2.1.1.m1.1.1.2.cmml" xref="S7.T2.20.16.2.1.1.m1.1.1.2">0.08</cn><cn type="float" id="S7.T2.20.16.2.1.1.m1.1.1.3.cmml" xref="S7.T2.20.16.2.1.1.m1.1.1.3">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.20.16.2.1.1.m1.1c">0.08\pm 0.02</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S7.T2.22.18" class="ltx_tr">
<td id="S7.T2.22.18.3" class="ltx_td ltx_align_left">Ours (w/ naive indexer)</td>
<td id="S7.T2.21.17.1" class="ltx_td ltx_align_justify">
<span id="S7.T2.21.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.21.17.1.1.1" class="ltx_p"><math id="S7.T2.21.17.1.1.1.m1.1" class="ltx_Math" alttext="89.58\pm 2.23" display="inline"><semantics id="S7.T2.21.17.1.1.1.m1.1a"><mrow id="S7.T2.21.17.1.1.1.m1.1.1" xref="S7.T2.21.17.1.1.1.m1.1.1.cmml"><mn id="S7.T2.21.17.1.1.1.m1.1.1.2" xref="S7.T2.21.17.1.1.1.m1.1.1.2.cmml">89.58</mn><mo id="S7.T2.21.17.1.1.1.m1.1.1.1" xref="S7.T2.21.17.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T2.21.17.1.1.1.m1.1.1.3" xref="S7.T2.21.17.1.1.1.m1.1.1.3.cmml">2.23</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T2.21.17.1.1.1.m1.1b"><apply id="S7.T2.21.17.1.1.1.m1.1.1.cmml" xref="S7.T2.21.17.1.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T2.21.17.1.1.1.m1.1.1.1.cmml" xref="S7.T2.21.17.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T2.21.17.1.1.1.m1.1.1.2.cmml" xref="S7.T2.21.17.1.1.1.m1.1.1.2">89.58</cn><cn type="float" id="S7.T2.21.17.1.1.1.m1.1.1.3.cmml" xref="S7.T2.21.17.1.1.1.m1.1.1.3">2.23</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.21.17.1.1.1.m1.1c">89.58\pm 2.23</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T2.22.18.2" class="ltx_td ltx_align_justify">
<span id="S7.T2.22.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.22.18.2.1.1" class="ltx_p"><math id="S7.T2.22.18.2.1.1.m1.1" class="ltx_Math" alttext="0.38\pm 0.01" display="inline"><semantics id="S7.T2.22.18.2.1.1.m1.1a"><mrow id="S7.T2.22.18.2.1.1.m1.1.1" xref="S7.T2.22.18.2.1.1.m1.1.1.cmml"><mn id="S7.T2.22.18.2.1.1.m1.1.1.2" xref="S7.T2.22.18.2.1.1.m1.1.1.2.cmml">0.38</mn><mo id="S7.T2.22.18.2.1.1.m1.1.1.1" xref="S7.T2.22.18.2.1.1.m1.1.1.1.cmml">±</mo><mn id="S7.T2.22.18.2.1.1.m1.1.1.3" xref="S7.T2.22.18.2.1.1.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T2.22.18.2.1.1.m1.1b"><apply id="S7.T2.22.18.2.1.1.m1.1.1.cmml" xref="S7.T2.22.18.2.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T2.22.18.2.1.1.m1.1.1.1.cmml" xref="S7.T2.22.18.2.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T2.22.18.2.1.1.m1.1.1.2.cmml" xref="S7.T2.22.18.2.1.1.m1.1.1.2">0.38</cn><cn type="float" id="S7.T2.22.18.2.1.1.m1.1.1.3.cmml" xref="S7.T2.22.18.2.1.1.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.22.18.2.1.1.m1.1c">0.38\pm 0.01</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S7.T2.24.20" class="ltx_tr">
<td id="S7.T2.24.20.3" class="ltx_td ltx_border_bb"></td>
<td id="S7.T2.24.20.4" class="ltx_td ltx_align_left ltx_border_bb">Ours</td>
<td id="S7.T2.23.19.1" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S7.T2.23.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.23.19.1.1.1" class="ltx_p"><math id="S7.T2.23.19.1.1.1.m1.1" class="ltx_Math" alttext="\bm{89.15\pm 2.06}" display="inline"><semantics id="S7.T2.23.19.1.1.1.m1.1a"><mrow id="S7.T2.23.19.1.1.1.m1.1.1" xref="S7.T2.23.19.1.1.1.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T2.23.19.1.1.1.m1.1.1.2" xref="S7.T2.23.19.1.1.1.m1.1.1.2.cmml">89.15</mn><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T2.23.19.1.1.1.m1.1.1.1" xref="S7.T2.23.19.1.1.1.m1.1.1.1.cmml">±</mo><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T2.23.19.1.1.1.m1.1.1.3" xref="S7.T2.23.19.1.1.1.m1.1.1.3.cmml">2.06</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T2.23.19.1.1.1.m1.1b"><apply id="S7.T2.23.19.1.1.1.m1.1.1.cmml" xref="S7.T2.23.19.1.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T2.23.19.1.1.1.m1.1.1.1.cmml" xref="S7.T2.23.19.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T2.23.19.1.1.1.m1.1.1.2.cmml" xref="S7.T2.23.19.1.1.1.m1.1.1.2">89.15</cn><cn type="float" id="S7.T2.23.19.1.1.1.m1.1.1.3.cmml" xref="S7.T2.23.19.1.1.1.m1.1.1.3">2.06</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.23.19.1.1.1.m1.1c">\bm{89.15\pm 2.06}</annotation></semantics></math></span>
</span>
</td>
<td id="S7.T2.24.20.2" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S7.T2.24.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S7.T2.24.20.2.1.1" class="ltx_p"><math id="S7.T2.24.20.2.1.1.m1.1" class="ltx_Math" alttext="\bm{0.45\pm 0.09}" display="inline"><semantics id="S7.T2.24.20.2.1.1.m1.1a"><mrow id="S7.T2.24.20.2.1.1.m1.1.1" xref="S7.T2.24.20.2.1.1.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T2.24.20.2.1.1.m1.1.1.2" xref="S7.T2.24.20.2.1.1.m1.1.1.2.cmml">0.45</mn><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T2.24.20.2.1.1.m1.1.1.1" xref="S7.T2.24.20.2.1.1.m1.1.1.1.cmml">±</mo><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S7.T2.24.20.2.1.1.m1.1.1.3" xref="S7.T2.24.20.2.1.1.m1.1.1.3.cmml">0.09</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.T2.24.20.2.1.1.m1.1b"><apply id="S7.T2.24.20.2.1.1.m1.1.1.cmml" xref="S7.T2.24.20.2.1.1.m1.1.1"><csymbol cd="latexml" id="S7.T2.24.20.2.1.1.m1.1.1.1.cmml" xref="S7.T2.24.20.2.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S7.T2.24.20.2.1.1.m1.1.1.2.cmml" xref="S7.T2.24.20.2.1.1.m1.1.1.2">0.45</cn><cn type="float" id="S7.T2.24.20.2.1.1.m1.1.1.3.cmml" xref="S7.T2.24.20.2.1.1.m1.1.1.3">0.09</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.T2.24.20.2.1.1.m1.1c">\bm{0.45\pm 0.09}</annotation></semantics></math></span>
</span>
</td>
</tr>
</table>
</figure>
</section>
<section id="S7.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.3. </span>Quantitative Evaluation</h4>

<div id="S7.SS3.SSS3.p1" class="ltx_para">
<p id="S7.SS3.SSS3.p1.1" class="ltx_p">We quantitatively evaluate the human likeness of generated motion and speech-gesture semantic matching using two metrics, i.e., Fréchet Gesture Distance (FGD) <cite class="ltx_cite ltx_citemacro_citep">(Yoon
et al<span class="ltx_text">.</span>, <a href="#bib.bib97" title="" class="ltx_ref">2020</a>)</cite> and Semantic Score (SC) <cite class="ltx_cite ltx_citemacro_citep">(Ao et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>, respectively. The Fréchet Gesture Distance (FGD) quantifies the disparity between the latent feature distributions of generated and real gestures. Commonly employed to evaluate gesture perceptual quality, a lower FGD indicates superior motion quality. The Semantic Score (SC) assesses the semantic coherence between speech and generated gestures. It computes the cosine similarity in the semantic space between generated motion and ground-truth transcripts, as defined in the gesture-transcript embedding framework by <cite class="ltx_cite ltx_citemacro_citet">Ao et al<span class="ltx_text">.</span> (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>. SC ranges from -1 to 1, with a higher SC indicating more effective speech-gesture content alignment. Besides, the FGD and SC are computed using sentence-level motion segments. We compute the mean (<math id="S7.SS3.SSS3.p1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S7.SS3.SSS3.p1.1.m1.1a"><mo id="S7.SS3.SSS3.p1.1.m1.1.1" xref="S7.SS3.SSS3.p1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S7.SS3.SSS3.p1.1.m1.1b"><csymbol cd="latexml" id="S7.SS3.SSS3.p1.1.m1.1.1.cmml" xref="S7.SS3.SSS3.p1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.SSS3.p1.1.m1.1c">\pm</annotation></semantics></math> standard deviation) values for each metric by generating on the test data 10 times.</p>
</div>
<div id="S7.SS3.SSS3.p2" class="ltx_para">
<p id="S7.SS3.SSS3.p2.1" class="ltx_p">As shown in Table <a href="#S7.T2" title="Table 2 ‣ 7.3.2. User Study ‣ 7.3. Comparison ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, our system surpasses all baseline comparisons in both metrics, FGD and SC. Notably, discarding the semantic alignment module leads to a substantial decrease in the SC value of our system, underscoring the module’s critical role. Meanwhile, the FGD value remains relatively unchanged with the addition of the semantic alignment module. This suggests that the fusion operation within this module does not degrade the quality of the motion. The SC metric exhibits a decline in our system when semantic gestures are indexed with increasing numbers (w/ naive indexer). This trend suggests that the semantics-aware indexing identifier, as detailed in Section <a href="#S5.SS2" title="5.2. LLM-Based Retrieval Model ‣ 5. Generative Semantic Gesture Retrieval ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>, can effectively enhance the performance of the gesture retrieval model.</p>
</div>
</section>
</section>
<section id="S7.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4. </span>Gesture Editing</h3>

<figure id="S7.F14" class="ltx_figure"><img src="/html/2405.09814/assets/x14.png" id="S7.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="325" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F14.3.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>. </span><span id="S7.F14.4.2" class="ltx_text" style="font-size:90%;">The data augmentation consists of two stages: (a) a high-level filtering utilizes the GPT-4V <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib66" title="" class="ltx_ref">2023b</a>)</cite> to filter out candidate gesture segments from the motion base according to the meta-information of the semantic gesture anchor; and (b) an example of finding joint-level similar motion in the latent space of RVQ. The red point represents the embedding of semantic gesture <span id="S7.F14.4.2.1" class="ltx_text ltx_font_typewriter">3030 ARMS RAISE V-SHAPE</span>. We can find similar motion within an appropriate threshold.</span></figcaption>
</figure>
<figure id="S7.F15" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2405.09814/assets/x15.png" id="S7.F15.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="110" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F15.2.1.1" class="ltx_text" style="font-size:90%;">Figure 15</span>. </span><span id="S7.F15.3.2" class="ltx_text" style="font-size:90%;">An example of gesture editing (Section <a href="#S7.SS4" title="7.4. Gesture Editing ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.4</span></a>). We use GPT-4V to filter appropriate semantic gestures from in-the-wild 2D videos to edit the original motion segment.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S7.F15.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<div id="S7.SS4.p1" class="ltx_para">
<p id="S7.SS4.p1.8" class="ltx_p">Our SeG dataset (Section <a href="#S5.SS1" title="5.1. SeG: Semantic Gesture Dataset ‣ 5. Generative Semantic Gesture Retrieval ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>) faces two primary issues: a) the variations of each semantic gesture is restricted; and b) there is a discrepancy between the captured motion and the real, spontaneous speech gestures. To enrich the diversity and bridge the gap, we propose a data augmentation framework to retrieve the semantically similar gesture segment <math id="S7.SS4.p1.1.m1.1" class="ltx_Math" alttext="\bm{M}^{\prime}" display="inline"><semantics id="S7.SS4.p1.1.m1.1a"><msup id="S7.SS4.p1.1.m1.1.1" xref="S7.SS4.p1.1.m1.1.1.cmml"><mi id="S7.SS4.p1.1.m1.1.1.2" xref="S7.SS4.p1.1.m1.1.1.2.cmml">𝑴</mi><mo id="S7.SS4.p1.1.m1.1.1.3" xref="S7.SS4.p1.1.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.1.m1.1b"><apply id="S7.SS4.p1.1.m1.1.1.cmml" xref="S7.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S7.SS4.p1.1.m1.1.1.1.cmml" xref="S7.SS4.p1.1.m1.1.1">superscript</csymbol><ci id="S7.SS4.p1.1.m1.1.1.2.cmml" xref="S7.SS4.p1.1.m1.1.1.2">𝑴</ci><ci id="S7.SS4.p1.1.m1.1.1.3.cmml" xref="S7.SS4.p1.1.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.1.m1.1c">\bm{M}^{\prime}</annotation></semantics></math> corresponding to the given semantic gesture <math id="S7.SS4.p1.2.m2.1" class="ltx_Math" alttext="\bm{g}" display="inline"><semantics id="S7.SS4.p1.2.m2.1a"><mi id="S7.SS4.p1.2.m2.1.1" xref="S7.SS4.p1.2.m2.1.1.cmml">𝒈</mi><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.2.m2.1b"><ci id="S7.SS4.p1.2.m2.1.1.cmml" xref="S7.SS4.p1.2.m2.1.1">𝒈</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.2.m2.1c">\bm{g}</annotation></semantics></math> from other existing datasets, e.g., <math id="S7.SS4.p1.3.m3.1" class="ltx_Math" alttext="\bm{G}^{\prime}" display="inline"><semantics id="S7.SS4.p1.3.m3.1a"><msup id="S7.SS4.p1.3.m3.1.1" xref="S7.SS4.p1.3.m3.1.1.cmml"><mi id="S7.SS4.p1.3.m3.1.1.2" xref="S7.SS4.p1.3.m3.1.1.2.cmml">𝑮</mi><mo id="S7.SS4.p1.3.m3.1.1.3" xref="S7.SS4.p1.3.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.3.m3.1b"><apply id="S7.SS4.p1.3.m3.1.1.cmml" xref="S7.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S7.SS4.p1.3.m3.1.1.1.cmml" xref="S7.SS4.p1.3.m3.1.1">superscript</csymbol><ci id="S7.SS4.p1.3.m3.1.1.2.cmml" xref="S7.SS4.p1.3.m3.1.1.2">𝑮</ci><ci id="S7.SS4.p1.3.m3.1.1.3.cmml" xref="S7.SS4.p1.3.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.3.m3.1c">\bm{G}^{\prime}</annotation></semantics></math>. <math id="S7.SS4.p1.4.m4.1" class="ltx_Math" alttext="\bm{G}^{\prime}" display="inline"><semantics id="S7.SS4.p1.4.m4.1a"><msup id="S7.SS4.p1.4.m4.1.1" xref="S7.SS4.p1.4.m4.1.1.cmml"><mi id="S7.SS4.p1.4.m4.1.1.2" xref="S7.SS4.p1.4.m4.1.1.2.cmml">𝑮</mi><mo id="S7.SS4.p1.4.m4.1.1.3" xref="S7.SS4.p1.4.m4.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.4.m4.1b"><apply id="S7.SS4.p1.4.m4.1.1.cmml" xref="S7.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S7.SS4.p1.4.m4.1.1.1.cmml" xref="S7.SS4.p1.4.m4.1.1">superscript</csymbol><ci id="S7.SS4.p1.4.m4.1.1.2.cmml" xref="S7.SS4.p1.4.m4.1.1.2">𝑮</ci><ci id="S7.SS4.p1.4.m4.1.1.3.cmml" xref="S7.SS4.p1.4.m4.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.4.m4.1c">\bm{G}^{\prime}</annotation></semantics></math> is derived from 2D videos. We measure the semantic relevance between gesture sequences in two dimensions: (a) a high-level filtering based on the meta-information of <math id="S7.SS4.p1.5.m5.1" class="ltx_Math" alttext="\bm{g}" display="inline"><semantics id="S7.SS4.p1.5.m5.1a"><mi id="S7.SS4.p1.5.m5.1.1" xref="S7.SS4.p1.5.m5.1.1.cmml">𝒈</mi><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.5.m5.1b"><ci id="S7.SS4.p1.5.m5.1.1.cmml" xref="S7.SS4.p1.5.m5.1.1">𝒈</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.5.m5.1c">\bm{g}</annotation></semantics></math>; and (b) a low-level movement matching between <math id="S7.SS4.p1.6.m6.1" class="ltx_Math" alttext="\bm{M}^{\prime}" display="inline"><semantics id="S7.SS4.p1.6.m6.1a"><msup id="S7.SS4.p1.6.m6.1.1" xref="S7.SS4.p1.6.m6.1.1.cmml"><mi id="S7.SS4.p1.6.m6.1.1.2" xref="S7.SS4.p1.6.m6.1.1.2.cmml">𝑴</mi><mo id="S7.SS4.p1.6.m6.1.1.3" xref="S7.SS4.p1.6.m6.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.6.m6.1b"><apply id="S7.SS4.p1.6.m6.1.1.cmml" xref="S7.SS4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S7.SS4.p1.6.m6.1.1.1.cmml" xref="S7.SS4.p1.6.m6.1.1">superscript</csymbol><ci id="S7.SS4.p1.6.m6.1.1.2.cmml" xref="S7.SS4.p1.6.m6.1.1.2">𝑴</ci><ci id="S7.SS4.p1.6.m6.1.1.3.cmml" xref="S7.SS4.p1.6.m6.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.6.m6.1c">\bm{M}^{\prime}</annotation></semantics></math> and motion part <math id="S7.SS4.p1.7.m7.1" class="ltx_Math" alttext="\bm{M}" display="inline"><semantics id="S7.SS4.p1.7.m7.1a"><mi id="S7.SS4.p1.7.m7.1.1" xref="S7.SS4.p1.7.m7.1.1.cmml">𝑴</mi><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.7.m7.1b"><ci id="S7.SS4.p1.7.m7.1.1.cmml" xref="S7.SS4.p1.7.m7.1.1">𝑴</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.7.m7.1c">\bm{M}</annotation></semantics></math> of <math id="S7.SS4.p1.8.m8.1" class="ltx_Math" alttext="\bm{g}" display="inline"><semantics id="S7.SS4.p1.8.m8.1a"><mi id="S7.SS4.p1.8.m8.1.1" xref="S7.SS4.p1.8.m8.1.1.cmml">𝒈</mi><annotation-xml encoding="MathML-Content" id="S7.SS4.p1.8.m8.1b"><ci id="S7.SS4.p1.8.m8.1.1.cmml" xref="S7.SS4.p1.8.m8.1.1">𝒈</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.p1.8.m8.1c">\bm{g}</annotation></semantics></math>. Details of these methods are introduced in the following sections.</p>
</div>
<div id="S7.SS4.p2" class="ltx_para">
<p id="S7.SS4.p2.1" class="ltx_p">Based on the augmentation method, we are able to achieve flexible editing of the generated semantic gestures. For instance, as illustrated in Figure <a href="#S7.F15" title="Figure 15 ‣ 7.4. Gesture Editing ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>, we employ a large multi-modality model to select appropriate
semantic gestures from in-the-wild 2D videos conditioned on the meta-information of the retrieved semantic gesture, and align the new semantic gestures with original motion sequence. Furthermore, users can flexibly control the style and appearance of the final generated gestures by customizing a 2D video library. Please refer to the supplementary video for more visualization results.</p>
</div>
<section id="S7.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.4.1. </span>High-Level Filtering.</h4>

<div id="S7.SS4.SSS1.p1" class="ltx_para">
<p id="S7.SS4.SSS1.p1.6" class="ltx_p">We first utilize the GPT-4 with vision (GPT-4V) <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib66" title="" class="ltx_ref">2023b</a>)</cite>, which is a powerful multimodal model and supports both image and text inputs, to filter out candidate gesture segments from <math id="S7.SS4.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\bm{G}^{\prime}" display="inline"><semantics id="S7.SS4.SSS1.p1.1.m1.1a"><msup id="S7.SS4.SSS1.p1.1.m1.1.1" xref="S7.SS4.SSS1.p1.1.m1.1.1.cmml"><mi id="S7.SS4.SSS1.p1.1.m1.1.1.2" xref="S7.SS4.SSS1.p1.1.m1.1.1.2.cmml">𝑮</mi><mo id="S7.SS4.SSS1.p1.1.m1.1.1.3" xref="S7.SS4.SSS1.p1.1.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS1.p1.1.m1.1b"><apply id="S7.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S7.SS4.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S7.SS4.SSS1.p1.1.m1.1.1.1.cmml" xref="S7.SS4.SSS1.p1.1.m1.1.1">superscript</csymbol><ci id="S7.SS4.SSS1.p1.1.m1.1.1.2.cmml" xref="S7.SS4.SSS1.p1.1.m1.1.1.2">𝑮</ci><ci id="S7.SS4.SSS1.p1.1.m1.1.1.3.cmml" xref="S7.SS4.SSS1.p1.1.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS1.p1.1.m1.1c">\bm{G}^{\prime}</annotation></semantics></math> according to the meta-information of the anchor <math id="S7.SS4.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\bm{g}" display="inline"><semantics id="S7.SS4.SSS1.p1.2.m2.1a"><mi id="S7.SS4.SSS1.p1.2.m2.1.1" xref="S7.SS4.SSS1.p1.2.m2.1.1.cmml">𝒈</mi><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS1.p1.2.m2.1b"><ci id="S7.SS4.SSS1.p1.2.m2.1.1.cmml" xref="S7.SS4.SSS1.p1.2.m2.1.1">𝒈</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS1.p1.2.m2.1c">\bm{g}</annotation></semantics></math>. Specifically, as shown in Figure <a href="#S7.F14" title="Figure 14 ‣ 7.4. Gesture Editing ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> (a), each segment in the motion set <math id="S7.SS4.SSS1.p1.3.m3.1" class="ltx_Math" alttext="\bm{G}^{\prime}" display="inline"><semantics id="S7.SS4.SSS1.p1.3.m3.1a"><msup id="S7.SS4.SSS1.p1.3.m3.1.1" xref="S7.SS4.SSS1.p1.3.m3.1.1.cmml"><mi id="S7.SS4.SSS1.p1.3.m3.1.1.2" xref="S7.SS4.SSS1.p1.3.m3.1.1.2.cmml">𝑮</mi><mo id="S7.SS4.SSS1.p1.3.m3.1.1.3" xref="S7.SS4.SSS1.p1.3.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS1.p1.3.m3.1b"><apply id="S7.SS4.SSS1.p1.3.m3.1.1.cmml" xref="S7.SS4.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S7.SS4.SSS1.p1.3.m3.1.1.1.cmml" xref="S7.SS4.SSS1.p1.3.m3.1.1">superscript</csymbol><ci id="S7.SS4.SSS1.p1.3.m3.1.1.2.cmml" xref="S7.SS4.SSS1.p1.3.m3.1.1.2">𝑮</ci><ci id="S7.SS4.SSS1.p1.3.m3.1.1.3.cmml" xref="S7.SS4.SSS1.p1.3.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS1.p1.3.m3.1c">\bm{G}^{\prime}</annotation></semantics></math> corresponds to a sequence of image frames. These image frames are about <math id="S7.SS4.SSS1.p1.4.m4.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S7.SS4.SSS1.p1.4.m4.1a"><mn id="S7.SS4.SSS1.p1.4.m4.1.1" xref="S7.SS4.SSS1.p1.4.m4.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS1.p1.4.m4.1b"><cn type="integer" id="S7.SS4.SSS1.p1.4.m4.1.1.cmml" xref="S7.SS4.SSS1.p1.4.m4.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS1.p1.4.m4.1c">1</annotation></semantics></math> second in duration, and we uniformly sample <math id="S7.SS4.SSS1.p1.5.m5.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S7.SS4.SSS1.p1.5.m5.1a"><mn id="S7.SS4.SSS1.p1.5.m5.1.1" xref="S7.SS4.SSS1.p1.5.m5.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS1.p1.5.m5.1b"><cn type="integer" id="S7.SS4.SSS1.p1.5.m5.1.1.cmml" xref="S7.SS4.SSS1.p1.5.m5.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS1.p1.5.m5.1c">5</annotation></semantics></math> images as the image prompt for GPT-4V. Meanwhile, the meta-information of the semantic gesture anchor is added as the text prompt. GPT-4V analyzes these prompts and selects the final candidate segments, matching them with the textual description of the anchor <math id="S7.SS4.SSS1.p1.6.m6.1" class="ltx_Math" alttext="\bm{g}" display="inline"><semantics id="S7.SS4.SSS1.p1.6.m6.1a"><mi id="S7.SS4.SSS1.p1.6.m6.1.1" xref="S7.SS4.SSS1.p1.6.m6.1.1.cmml">𝒈</mi><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS1.p1.6.m6.1b"><ci id="S7.SS4.SSS1.p1.6.m6.1.1.cmml" xref="S7.SS4.SSS1.p1.6.m6.1.1">𝒈</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS1.p1.6.m6.1c">\bm{g}</annotation></semantics></math>. These selected segments are converted into 3D motion sequences using a pose estimation tool <cite class="ltx_cite ltx_citemacro_citep">(DeepMotion, <a href="#bib.bib22" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
</section>
<section id="S7.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.4.2. </span>Low-Level Matching.</h4>

<div id="S7.SS4.SSS2.p1" class="ltx_para">
<p id="S7.SS4.SSS2.p1.9" class="ltx_p">To refine the results of GPT-4V and ensure motion plausibility, we further select gestures from candidate motion segments that are similar to the semantic gesture anchor at the movement level. Specifically, we use the RVQ encoder <math id="S7.SS4.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{E}_{{\text{VQ}}}" display="inline"><semantics id="S7.SS4.SSS2.p1.1.m1.1a"><msub id="S7.SS4.SSS2.p1.1.m1.1.1" xref="S7.SS4.SSS2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S7.SS4.SSS2.p1.1.m1.1.1.2" xref="S7.SS4.SSS2.p1.1.m1.1.1.2.cmml">ℰ</mi><mtext id="S7.SS4.SSS2.p1.1.m1.1.1.3" xref="S7.SS4.SSS2.p1.1.m1.1.1.3a.cmml">VQ</mtext></msub><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS2.p1.1.m1.1b"><apply id="S7.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S7.SS4.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S7.SS4.SSS2.p1.1.m1.1.1.1.cmml" xref="S7.SS4.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S7.SS4.SSS2.p1.1.m1.1.1.2.cmml" xref="S7.SS4.SSS2.p1.1.m1.1.1.2">ℰ</ci><ci id="S7.SS4.SSS2.p1.1.m1.1.1.3a.cmml" xref="S7.SS4.SSS2.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S7.SS4.SSS2.p1.1.m1.1.1.3.cmml" xref="S7.SS4.SSS2.p1.1.m1.1.1.3">VQ</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS2.p1.1.m1.1c">\mathcal{E}_{{\text{VQ}}}</annotation></semantics></math> to encode the motion part <math id="S7.SS4.SSS2.p1.2.m2.1" class="ltx_Math" alttext="\bm{M}" display="inline"><semantics id="S7.SS4.SSS2.p1.2.m2.1a"><mi id="S7.SS4.SSS2.p1.2.m2.1.1" xref="S7.SS4.SSS2.p1.2.m2.1.1.cmml">𝑴</mi><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS2.p1.2.m2.1b"><ci id="S7.SS4.SSS2.p1.2.m2.1.1.cmml" xref="S7.SS4.SSS2.p1.2.m2.1.1">𝑴</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS2.p1.2.m2.1c">\bm{M}</annotation></semantics></math> of the anchor and one of the selected motion segment <math id="S7.SS4.SSS2.p1.3.m3.1" class="ltx_Math" alttext="\bm{M}^{\prime}" display="inline"><semantics id="S7.SS4.SSS2.p1.3.m3.1a"><msup id="S7.SS4.SSS2.p1.3.m3.1.1" xref="S7.SS4.SSS2.p1.3.m3.1.1.cmml"><mi id="S7.SS4.SSS2.p1.3.m3.1.1.2" xref="S7.SS4.SSS2.p1.3.m3.1.1.2.cmml">𝑴</mi><mo id="S7.SS4.SSS2.p1.3.m3.1.1.3" xref="S7.SS4.SSS2.p1.3.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS2.p1.3.m3.1b"><apply id="S7.SS4.SSS2.p1.3.m3.1.1.cmml" xref="S7.SS4.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S7.SS4.SSS2.p1.3.m3.1.1.1.cmml" xref="S7.SS4.SSS2.p1.3.m3.1.1">superscript</csymbol><ci id="S7.SS4.SSS2.p1.3.m3.1.1.2.cmml" xref="S7.SS4.SSS2.p1.3.m3.1.1.2">𝑴</ci><ci id="S7.SS4.SSS2.p1.3.m3.1.1.3.cmml" xref="S7.SS4.SSS2.p1.3.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS2.p1.3.m3.1c">\bm{M}^{\prime}</annotation></semantics></math> into corresponding latent sequences <math id="S7.SS4.SSS2.p1.4.m4.1" class="ltx_Math" alttext="\bm{Z}" display="inline"><semantics id="S7.SS4.SSS2.p1.4.m4.1a"><mi id="S7.SS4.SSS2.p1.4.m4.1.1" xref="S7.SS4.SSS2.p1.4.m4.1.1.cmml">𝒁</mi><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS2.p1.4.m4.1b"><ci id="S7.SS4.SSS2.p1.4.m4.1.1.cmml" xref="S7.SS4.SSS2.p1.4.m4.1.1">𝒁</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS2.p1.4.m4.1c">\bm{Z}</annotation></semantics></math> and <math id="S7.SS4.SSS2.p1.5.m5.1" class="ltx_Math" alttext="\bm{Z}^{\prime}" display="inline"><semantics id="S7.SS4.SSS2.p1.5.m5.1a"><msup id="S7.SS4.SSS2.p1.5.m5.1.1" xref="S7.SS4.SSS2.p1.5.m5.1.1.cmml"><mi id="S7.SS4.SSS2.p1.5.m5.1.1.2" xref="S7.SS4.SSS2.p1.5.m5.1.1.2.cmml">𝒁</mi><mo id="S7.SS4.SSS2.p1.5.m5.1.1.3" xref="S7.SS4.SSS2.p1.5.m5.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS2.p1.5.m5.1b"><apply id="S7.SS4.SSS2.p1.5.m5.1.1.cmml" xref="S7.SS4.SSS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S7.SS4.SSS2.p1.5.m5.1.1.1.cmml" xref="S7.SS4.SSS2.p1.5.m5.1.1">superscript</csymbol><ci id="S7.SS4.SSS2.p1.5.m5.1.1.2.cmml" xref="S7.SS4.SSS2.p1.5.m5.1.1.2">𝒁</ci><ci id="S7.SS4.SSS2.p1.5.m5.1.1.3.cmml" xref="S7.SS4.SSS2.p1.5.m5.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS2.p1.5.m5.1c">\bm{Z}^{\prime}</annotation></semantics></math>, respectively. These sequences are then averaged along the temporal dimension into <math id="S7.SS4.SSS2.p1.6.m6.1" class="ltx_Math" alttext="\bm{\overline{z}}" display="inline"><semantics id="S7.SS4.SSS2.p1.6.m6.1a"><mover accent="true" id="S7.SS4.SSS2.p1.6.m6.1.1" xref="S7.SS4.SSS2.p1.6.m6.1.1.cmml"><mi id="S7.SS4.SSS2.p1.6.m6.1.1.2" xref="S7.SS4.SSS2.p1.6.m6.1.1.2.cmml">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S7.SS4.SSS2.p1.6.m6.1.1.1" xref="S7.SS4.SSS2.p1.6.m6.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS2.p1.6.m6.1b"><apply id="S7.SS4.SSS2.p1.6.m6.1.1.cmml" xref="S7.SS4.SSS2.p1.6.m6.1.1"><ci id="S7.SS4.SSS2.p1.6.m6.1.1.1.cmml" xref="S7.SS4.SSS2.p1.6.m6.1.1.1">bold-¯</ci><ci id="S7.SS4.SSS2.p1.6.m6.1.1.2.cmml" xref="S7.SS4.SSS2.p1.6.m6.1.1.2">𝒛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS2.p1.6.m6.1c">\bm{\overline{z}}</annotation></semantics></math> and <math id="S7.SS4.SSS2.p1.7.m7.1" class="ltx_Math" alttext="\bm{\overline{z}}^{\prime}" display="inline"><semantics id="S7.SS4.SSS2.p1.7.m7.1a"><msup id="S7.SS4.SSS2.p1.7.m7.1.1" xref="S7.SS4.SSS2.p1.7.m7.1.1.cmml"><mover accent="true" id="S7.SS4.SSS2.p1.7.m7.1.1.2" xref="S7.SS4.SSS2.p1.7.m7.1.1.2.cmml"><mi id="S7.SS4.SSS2.p1.7.m7.1.1.2.2" xref="S7.SS4.SSS2.p1.7.m7.1.1.2.2.cmml">𝒛</mi><mo class="ltx_mathvariant_bold" mathvariant="bold" id="S7.SS4.SSS2.p1.7.m7.1.1.2.1" xref="S7.SS4.SSS2.p1.7.m7.1.1.2.1.cmml">¯</mo></mover><mo id="S7.SS4.SSS2.p1.7.m7.1.1.3" xref="S7.SS4.SSS2.p1.7.m7.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS2.p1.7.m7.1b"><apply id="S7.SS4.SSS2.p1.7.m7.1.1.cmml" xref="S7.SS4.SSS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S7.SS4.SSS2.p1.7.m7.1.1.1.cmml" xref="S7.SS4.SSS2.p1.7.m7.1.1">superscript</csymbol><apply id="S7.SS4.SSS2.p1.7.m7.1.1.2.cmml" xref="S7.SS4.SSS2.p1.7.m7.1.1.2"><ci id="S7.SS4.SSS2.p1.7.m7.1.1.2.1.cmml" xref="S7.SS4.SSS2.p1.7.m7.1.1.2.1">bold-¯</ci><ci id="S7.SS4.SSS2.p1.7.m7.1.1.2.2.cmml" xref="S7.SS4.SSS2.p1.7.m7.1.1.2.2">𝒛</ci></apply><ci id="S7.SS4.SSS2.p1.7.m7.1.1.3.cmml" xref="S7.SS4.SSS2.p1.7.m7.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS2.p1.7.m7.1c">\bm{\overline{z}}^{\prime}</annotation></semantics></math>, respectively. We compute the Euclidean distance between these two embeddings as the similarity score. If the score falls below the predefined threshold, <math id="S7.SS4.SSS2.p1.8.m8.1" class="ltx_Math" alttext="\bm{M}^{\prime}" display="inline"><semantics id="S7.SS4.SSS2.p1.8.m8.1a"><msup id="S7.SS4.SSS2.p1.8.m8.1.1" xref="S7.SS4.SSS2.p1.8.m8.1.1.cmml"><mi id="S7.SS4.SSS2.p1.8.m8.1.1.2" xref="S7.SS4.SSS2.p1.8.m8.1.1.2.cmml">𝑴</mi><mo id="S7.SS4.SSS2.p1.8.m8.1.1.3" xref="S7.SS4.SSS2.p1.8.m8.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS2.p1.8.m8.1b"><apply id="S7.SS4.SSS2.p1.8.m8.1.1.cmml" xref="S7.SS4.SSS2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S7.SS4.SSS2.p1.8.m8.1.1.1.cmml" xref="S7.SS4.SSS2.p1.8.m8.1.1">superscript</csymbol><ci id="S7.SS4.SSS2.p1.8.m8.1.1.2.cmml" xref="S7.SS4.SSS2.p1.8.m8.1.1.2">𝑴</ci><ci id="S7.SS4.SSS2.p1.8.m8.1.1.3.cmml" xref="S7.SS4.SSS2.p1.8.m8.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS2.p1.8.m8.1c">\bm{M}^{\prime}</annotation></semantics></math> is selected. Finally, we obtain semantically relevant gestures from <math id="S7.SS4.SSS2.p1.9.m9.1" class="ltx_Math" alttext="\bm{G}^{\prime}" display="inline"><semantics id="S7.SS4.SSS2.p1.9.m9.1a"><msup id="S7.SS4.SSS2.p1.9.m9.1.1" xref="S7.SS4.SSS2.p1.9.m9.1.1.cmml"><mi id="S7.SS4.SSS2.p1.9.m9.1.1.2" xref="S7.SS4.SSS2.p1.9.m9.1.1.2.cmml">𝑮</mi><mo id="S7.SS4.SSS2.p1.9.m9.1.1.3" xref="S7.SS4.SSS2.p1.9.m9.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S7.SS4.SSS2.p1.9.m9.1b"><apply id="S7.SS4.SSS2.p1.9.m9.1.1.cmml" xref="S7.SS4.SSS2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S7.SS4.SSS2.p1.9.m9.1.1.1.cmml" xref="S7.SS4.SSS2.p1.9.m9.1.1">superscript</csymbol><ci id="S7.SS4.SSS2.p1.9.m9.1.1.2.cmml" xref="S7.SS4.SSS2.p1.9.m9.1.1.2">𝑮</ci><ci id="S7.SS4.SSS2.p1.9.m9.1.1.3.cmml" xref="S7.SS4.SSS2.p1.9.m9.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS4.SSS2.p1.9.m9.1c">\bm{G}^{\prime}</annotation></semantics></math>, thereby enriching the diversity of the gesture library. Figure <a href="#S7.F14" title="Figure 14 ‣ 7.4. Gesture Editing ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> (b) illustrates an example of this process.</p>
</div>
</section>
</section>
<section id="S7.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5. </span>Ablation Study</h3>

<div id="S7.SS5.p1" class="ltx_para">
<p id="S7.SS5.p1.1" class="ltx_p">Our analysis focuses on the effects of various architectures of the gesture tokenizer, the semantics-aware indexing identifier, and differing configurations of the semantics-aware alignment module on our system’s performance. These findings are detailed in Figure <a href="#S7.F12" title="Figure 12 ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>, Table <a href="#S7.T1" title="Table 1 ‣ 7.2. Results ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, Table <a href="#S7.T2" title="Table 2 ‣ 7.3.2. User Study ‣ 7.3. Comparison ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, Figure <a href="#S7.F16" title="Figure 16 ‣ 7.5.1. LLM-Based Retrieval Model. ‣ 7.5. Ablation Study ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>, and the supplementary video.</p>
</div>
<section id="S7.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.5.1. </span>LLM-Based Retrieval Model.</h4>

<div id="S7.SS5.SSS1.p1" class="ltx_para">
<p id="S7.SS5.SSS1.p1.1" class="ltx_p">In this experiment, we compare three different strategies, zero-shot, few-shot and finetuned LLM, which have exactly the same instruction settings and simultaneously contain the index information of all semantic gestures. Zero-shot strategy only has instruction settings, few-shot strategy has some examples of output formats, and the finetuned LLM is trained on the instruction dataset. We select GPT-3.5-turbo-1106 as the base model. There are two commonly used evaluation methods for LLM: automatic evaluation as quantitative analysis and human evaluation as qualitative analysis  <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>. We conduct a qualitative analysis and present several examples in Figure <a href="#S7.F16" title="Figure 16 ‣ 7.5.1. LLM-Based Retrieval Model. ‣ 7.5. Ablation Study ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>. With the same instruction, the zero-shot LLM struggles to perform adequately, failing to generate valid gestures or comprehend semantic information effectively. For example, from Figure <a href="#S7.F16" title="Figure 16 ‣ 7.5.1. LLM-Based Retrieval Model. ‣ 7.5. Ablation Study ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> we can see that zero-shot LLM can not always produce appropriate gesture labels. The few-shot LLM effectively performs the annotation task and produces valid gesture labels but falls short in fully understanding semantic information. The fine-tuned model addresses all these requirements robustly, and can understand the semantics of gestures.</p>
</div>
<figure id="S7.F16" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2405.09814/assets/x16.png" id="S7.F16.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="228" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F16.2.1.1" class="ltx_text" style="font-size:90%;">Figure 16</span>. </span><span id="S7.F16.3.2" class="ltx_text" style="font-size:90%;">The comparison of three strategies.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S7.F16.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<div id="S7.SS5.SSS1.p2" class="ltx_para">
<p id="S7.SS5.SSS1.p2.1" class="ltx_p">It is challenging to design an effective quantitative pipeline to evaluate the outcomes of LLM because there is a lack of credible metrics to judge whether the response is aligned with human performance. As explored in the NLP field  <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib104" title="" class="ltx_ref">2023c</a>)</cite>, a potential approach is to utilize the LLM itself for evaluation and stabilize the results through multiple independent evaluations. Details and results of the quantitative evaluation are described in Appendix <a href="#A2" title="Appendix B Quantitative Evaluation of LLM-Based Retrieval Model ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
<section id="S7.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.5.2. </span>Architecture of Gesture Tokenizer.</h4>

<div id="S7.SS5.SSS2.p1" class="ltx_para">
<p id="S7.SS5.SSS2.p1.1" class="ltx_p">In this experiment, we train the gesture tokenizer (Section <a href="#S4.SS1" title="4.1. Gesture Tokenizer ‣ 4. Co-Speech Gesture GPT Model ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>) with three different architectures, i.e., (1D-Conv+Transformer, RVQ), (1D-Conv, RVQ), and (1D-Conv+Transformer, VQ) to investigate the impact of the Transformer layer and the residual quantization layer on motion encoding, respectively. “1D-Conv+Transformer” refers to a tokenizer whose encoder and decoder consist of 1D convolutional layers and the Transformer layer. We conduct comparisons by observing the quality of the reconstructed motion. As demonstrated in the supplementary video, motion reconstructed using RVQ based solely on 1D convolutional layers tend to exhibit jitter, while those using a vanilla quantization layer are prone to losing fine details, such as finger movements.</p>
</div>
</section>
<section id="S7.SS5.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.5.3. </span>Semantics-Aware Indexing Identifier.</h4>

<div id="S7.SS5.SSS3.p1" class="ltx_para">
<p id="S7.SS5.SSS3.p1.1" class="ltx_p">In this experiment, we train and compare two generative retrieval models (Section <a href="#S5.SS2" title="5.2. LLM-Based Retrieval Model ‣ 5. Generative Semantic Gesture Retrieval ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>) using the naive and semantics-aware indexing identifiers, respectively. The naive approach involves indexing semantic gestures with sequentially increasing numbers. Table <a href="#S7.T2" title="Table 2 ‣ 7.3.2. User Study ‣ 7.3. Comparison ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> reveals that the semantics-aware metric, SC, experiences a decrease with the naive indexing approach. This observation confirms that the semantics-aware indexing identifier effectively improves the semantic consistency between speech and gestures.</p>
</div>
</section>
<section id="S7.SS5.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.5.4. </span>Semantics-Aware Alignment.</h4>

<div id="S7.SS5.SSS4.p1" class="ltx_para">
<p id="S7.SS5.SSS4.p1.1" class="ltx_p">We conduct three experiments to study the semantics-aware alignment module (Section <a href="#S6" title="6. Semantics Gesture Alignment ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>) from a holistic to a localized perspective. First, we discard the whole semantic alignment module, resulting in a significant drop in relevant semantic metrics, such as semantic accuracy (Table <a href="#S7.T1" title="Table 1 ‣ 7.2. Results ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) and SC (Table <a href="#S7.T2" title="Table 2 ‣ 7.3.2. User Study ‣ 7.3. Comparison ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Figure <a href="#S7.F12" title="Figure 12 ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> also demonstrates that gestures synthesized by Ours (w/o semantic alignment) perform low communicative efficacy. Then, we do not consider audio beats when determining the merging timing (Section <a href="#S6.SS1" title="6.1. When to Merge ‣ 6. Semantics Gesture Alignment ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>) for each retrieved semantic gesture. As shown in the supplementary video, the rhythmic harmony of the generated gestures is disrupted. Lastly, we discard the weighted merge operation (Section <a href="#S6.SS2" title="6.2. How to Merge ‣ 6. Semantics Gesture Alignment ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2</span></a>) when replacing the original motion with retrieved semantic gestures. As demonstrated in the supplementary video, the generated motion exhibits unnatural transitions.</p>
</div>
</section>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">In this paper, we present Semantic Gesticulator, a semantics-aware co-speech gesture synthesis system that generates
realistic and meaningful gestures while maintaining rhythmic coherence with speech. Initially, we develop a GPT-based gesture generative model, trained using hierarchical discrete tokens extracted by a scalable, body part-aware residual VQ-VAE. This architecture effectively produces gestures that are rhythmically coherent and exhibit robust generalization across a wide range of audio inputs. Subsequently, we employ a powerful large language model (LLM) to establish a generative semantic gesture retrieval framework. This framework analyzes the context of speech transcripts and efficiently retrieves suitable semantic gestures from our self-collected, high-quality gesture library, namely the SeG Dataset. This dataset encompasses a comprehensive array of semantic gestures frequently utilized in human communication. Finally, we introduce a semantic alignment mechanism. This mechanism merges retrieved semantic gestures with generated rhythmic gestures at the latent space level, ensuring that the resulting gestures are both meaningful and rhythmically synchronized. We carry out a comprehensive series of experiments to assess our framework. Our system surpasses all baseline comparisons in terms of both qualitative and quantitative measures, demonstrated by the results of FGD and SC metrics, as well as user study outcomes. For applications, we devise an augmentation framework for identifying semantically similar gestures from an extensive collection of 2D videos, thereby enhancing the diversity of gestures. Moreover, through customizing a 2D video library, users can flexibly edit the style of final outcomes.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">There is still room for improvement in our current research.
First, the semantic gesture retrieval model only considers the textual information of each gesture and the input speech transcript, neglecting the inherent rhythm of speech audio. This may result in retrieving redundant gestures in areas lacking prominent prosody (moments when the speaker typically does not perform semantic gestures), or missing the corresponding semantic gestures in instances where prosody is significant. Employing a model to integrate multiple modalities, such as textual descriptions, gesture example images, and speech audio, can further enhance the accuracy of semantic gesture mining. Moreover, our retrieval model occasionally retrieves an excess of semantic gestures than necessary, failing to align with user preferences. We have observed that our annotators tend to label as many potential gestures as possible, which contributes to this issue. Our system currently supports users in manually revising the retrieval results. A potential way to achieve this automatically is to employ a LLM as a discriminator to refine the results of semantic gesture retrieval.A user may specify their preferences via prompting and let the LLM adjust the frequency of retrieved semantic gestures to a suitable degree.</p>
</div>
<div id="S8.p3" class="ltx_para">
<p id="S8.p3.1" class="ltx_p">Second, we employ a straightforward but effective merging strategy to align the retrieved semantic gestures with the generator’s outcomes. However, we only align the stroke phase of semantic gestures to the audio beats to maintain the existing gesture sequence’s phases. A more phase-informed strategy may better preserve motion details. Concurrently, augmenting the pool of semantic gesture samples with additional references and training a specialized generator could pave the way for the creation of a more varied range of semantic gestures. To further align the pre-trained generator <math id="S8.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{G}" display="inline"><semantics id="S8.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S8.p3.1.m1.1.1" xref="S8.p3.1.m1.1.1.cmml">𝒢</mi><annotation-xml encoding="MathML-Content" id="S8.p3.1.m1.1b"><ci id="S8.p3.1.m1.1.1.cmml" xref="S8.p3.1.m1.1.1">𝒢</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.p3.1.m1.1c">\mathcal{G}</annotation></semantics></math> with semantic preferences or even human values, methods like reinforcement learning with human feedback <cite class="ltx_cite ltx_citemacro_citep">(Christiano et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2023</a>; Ouyang
et al<span class="ltx_text">.</span>, <a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite> and Direct Preference Optimization (DPO) <cite class="ltx_cite ltx_citemacro_citep">(Rafailov et al<span class="ltx_text">.</span>, <a href="#bib.bib73" title="" class="ltx_ref">2023</a>)</cite> can be explored to enhance the model’s capabilities.</p>
</div>
<div id="S8.p4" class="ltx_para">
<p id="S8.p4.1" class="ltx_p">Third, the motion quality exhibits certain deficiencies, such as foot sliding and excessive upper-body movements. The former issue can be alleviated by employing additional constraints in training and by using IK-based post-processing. The latter is partially caused by exaggerated gesture performance in motion data. Constructing a more curated dataset may address this problem.</p>
</div>
<div id="S8.p5" class="ltx_para">
<p id="S8.p5.1" class="ltx_p">Finally, our system enhances the communicative efficacy of generated gestures by explicitly specifying appropriate semantic gestures. Expanding this system to two-party, and even multi-party conversational scenarios, is a direction worth exploring in the future.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
We thank the anonymous reviewers for their constructive comments. This work was supported in part by National Key R&amp;D Program of China 2022ZD0160803.

</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alexanderson et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Simon Alexanderson,
Gustav Eje Henter, Taras Kucherenko,
and Jonas Beskow. 2020.

</span>
<span class="ltx_bibblock">Style-Controllable Speech-Driven Gesture Synthesis
Using Normalising Flows.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">Computer Graphics Forum</em>
39, 2 (2020),
487–496.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1111/cgf.13946" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1111/cgf.13946</a>

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alexanderson et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Simon Alexanderson,
Rajmund Nagy, Jonas Beskow, and
Gustav Eje Henter. 2023.

</span>
<span class="ltx_bibblock">Listen, denoise, action! audio-driven motion
synthesis with diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">ACM Transactions on Graphics (TOG)</em>
42, 4 (2023),
1–20.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alibaba (2009)</span>
<span class="ltx_bibblock">
Alibaba. 2009.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Alibaba Cloud Automatic Speech
Recognition</em>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2023-12-15.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anil
et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Rohan Anil, Andrew M.
Dai, Orhan Firat, Melvin Johnson,
Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa,
Paige Bailey, Zhifeng Chen,
Eric Chu, Jonathan H. Clark,
Laurent El Shafey, Yanping Huang,
Kathy Meier-Hellstern, Gaurav Mishra,
Erica Moreira, Mark Omernick,
Kevin Robinson, Sebastian Ruder,
Yi Tay, Kefan Xiao,
Yuanzhong Xu, Yujing Zhang,
Gustavo Hernandez Abrego, Junwhan Ahn,
Jacob Austin, Paul Barham,
Jan Botha, James Bradbury,
Siddhartha Brahma, Kevin Brooks,
Michele Catasta, Yong Cheng,
Colin Cherry, Christopher A.
Choquette-Choo, Aakanksha Chowdhery,
Clément Crepy, Shachi Dave,
Mostafa Dehghani, Sunipa Dev,
Jacob Devlin, Mark Díaz,
Nan Du, Ethan Dyer, Vlad
Feinberg, Fangxiaoyu Feng, Vlad Fienber,
Markus Freitag, Xavier Garcia,
Sebastian Gehrmann, Lucas Gonzalez,
Guy Gur-Ari, Steven Hand,
Hadi Hashemi, Le Hou,
Joshua Howland, Andrea Hu,
Jeffrey Hui, Jeremy Hurwitz,
Michael Isard, Abe Ittycheriah,
Matthew Jagielski, Wenhao Jia,
Kathleen Kenealy, Maxim Krikun,
Sneha Kudugunta, Chang Lan,
Katherine Lee, Benjamin Lee,
Eric Li, Music Li, Wei
Li, YaGuang Li, Jian Li,
Hyeontaek Lim, Hanzhao Lin,
Zhongtao Liu, Frederick Liu,
Marcello Maggioni, Aroma Mahendru,
Joshua Maynez, Vedant Misra,
Maysam Moussalem, Zachary Nado,
John Nham, Eric Ni,
Andrew Nystrom, Alicia Parrish,
Marie Pellat, Martin Polacek,
Alex Polozov, Reiner Pope,
Siyuan Qiao, Emily Reif,
Bryan Richter, Parker Riley,
Alex Castro Ros, Aurko Roy,
Brennan Saeta, Rajkumar Samuel,
Renee Shelby, Ambrose Slone,
Daniel Smilkov, David R. So,
Daniel Sohn, Simon Tokumine,
Dasha Valter, Vijay Vasudevan,
Kiran Vodrahalli, Xuezhi Wang,
Pidong Wang, Zirui Wang,
Tao Wang, John Wieting,
Yuhuai Wu, Kelvin Xu,
Yunhan Xu, Linting Xue,
Pengcheng Yin, Jiahui Yu,
Qiao Zhang, Steven Zheng,
Ce Zheng, Weikang Zhou,
Denny Zhou, Slav Petrov, and
Yonghui Wu. 2023.

</span>
<span class="ltx_bibblock">PaLM 2 Technical Report.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.10403 [cs.CL]

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ao
et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Tenglong Ao, Qingzhe Gao,
Yuke Lou, Baoquan Chen, and
Libin Liu. 2022.

</span>
<span class="ltx_bibblock">Rhythmic Gesticulator: Rhythm-Aware Co-Speech
Gesture Synthesis with Hierarchical Neural Embeddings.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">ACM Trans. Graph.</em> 41,
6, Article 209 (nov
2022), 19 pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3550454.3555435" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3550454.3555435</a>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ao et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Tenglong Ao, Zeyi Zhang,
and Libin Liu. 2023.

</span>
<span class="ltx_bibblock">GestureDiffuCLIP: Gesture Diffusion Model with CLIP
Latents.

</span>
<span class="ltx_bibblock">42, 4, Article
42 (jul 2023),
18 pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3592097" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3592097</a>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bennett
et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2000)</span>
<span class="ltx_bibblock">
K.P. Bennett, P.S.
Bradley, and A. Demiriz.
2000.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">Constrained K-Means Clustering</em>.

</span>
<span class="ltx_bibblock">Technical Report MSR-TR-2000-65.
8 pages.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.microsoft.com/en-us/research/publication/constrained-k-means-clustering/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.microsoft.com/en-us/research/publication/constrained-k-means-clustering/</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bevilacqua et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Michele Bevilacqua,
Giuseppe Ottaviano, Patrick Lewis,
Wen tau Yih, Sebastian Riedel, and
Fabio Petroni. 2022.

</span>
<span class="ltx_bibblock">Autoregressive Search Engines: Generating Substrings
as Document Identifiers.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2204.10628 [cs.CL]

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhattacharya et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Uttaran Bhattacharya,
Elizabeth Childs, Nicholas Rewkowski,
and Dinesh Manocha. 2021a.

</span>
<span class="ltx_bibblock">Speech2AffectiveGestures: Synthesizing Co-Speech
Gestures with Generative Adversarial Affective Expression Learning. In
<em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th ACM International
Conference on Multimedia</em> (Virtual Event, China) <em id="bib.bib10.4.2" class="ltx_emph ltx_font_italic">(MM
’21)</em>. Association for Computing Machinery,
New York, NY, USA, 2027–2036.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3474085.3475223" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3474085.3475223</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhattacharya et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Uttaran Bhattacharya,
Nicholas Rewkowski, Abhishek Banerjee,
Pooja Guhan, Aniket Bera, and
Dinesh Manocha. 2021b.

</span>
<span class="ltx_bibblock">Text2Gestures: A Transformer-Based Network for
Generating Emotive Body Gestures for Virtual Agents. In
<em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">2021 IEEE Conference on Virtual Reality and 3D
User Interfaces (IEEE VR)</em>. IEEE.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao
et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Nicola De Cao, Gautier
Izacard, Sebastian Riedel, and Fabio
Petroni. 2021.

</span>
<span class="ltx_bibblock">Autoregressive Entity Retrieval.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2010.00904 [cs.CL]

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao
et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Zhe Cao, Tomas Simon,
Shih-En Wei, and Yaser Sheikh.
2017.

</span>
<span class="ltx_bibblock">Realtime multi-person 2d pose estimation using part
affinity fields. In <em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>.
7291–7299.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cassell et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (1994)</span>
<span class="ltx_bibblock">
Justine Cassell, Catherine
Pelachaud, Norman Badler, Mark Steedman,
Brett Achorn, Tripp Becket,
Brett Douville, Scott Prevost, and
Matthew Stone. 1994.

</span>
<span class="ltx_bibblock">Animated Conversation: Rule-Based Generation of
Facial Expression, Gesture &amp; Spoken Intonation for Multiple Conversational
Agents. In <em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 21st Annual
Conference on Computer Graphics and Interactive Techniques</em>
<em id="bib.bib14.4.2" class="ltx_emph ltx_font_italic">(SIGGRAPH ’94)</em>. Association for
Computing Machinery, New York, NY, USA,
413–420.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/192161.192272" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/192161.192272</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cassell et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2001)</span>
<span class="ltx_bibblock">
Justine Cassell,
Hannes Högni Vilhjálmsson, and
Timothy Bickmore. 2001.

</span>
<span class="ltx_bibblock">BEAT: The Behavior Expression Animation Toolkit.
In <em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th Annual Conference on
Computer Graphics and Interactive Techniques</em>
<em id="bib.bib15.4.2" class="ltx_emph ltx_font_italic">(SIGGRAPH ’01)</em>. Association for
Computing Machinery, New York, NY, USA,
477–486.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/383259.383315" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/383259.383315</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Yupeng Chang, Xu Wang,
Jindong Wang, Yuan Wu,
Linyi Yang, Kaijie Zhu,
Hao Chen, Xiaoyuan Yi,
Cunxiang Wang, Yidong Wang,
Wei Ye, Yue Zhang, Yi
Chang, Philip S. Yu, Qiang Yang, and
Xing Xie. 2023.

</span>
<span class="ltx_bibblock">A Survey on Evaluation of Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2307.03109 [cs.CL]

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Junming Chen, Yunfei Liu,
Jianan Wang, Ailing Zeng,
Yu Li, and Qifeng Chen.
2023.

</span>
<span class="ltx_bibblock">DiffSHEG: A Diffusion-Based Approach for Real-Time
Speech-driven Holistic 3D Expression and Gesture Generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">Preprint</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Jiangui Chen, Ruqing
Zhang, Jiafeng Guo, Yiqun Liu,
Yixing Fan, and Xueqi Cheng.
2022.

</span>
<span class="ltx_bibblock">CorpusBrain: Pre-train a Generative Retrieval Model
for Knowledge-Intensive Language Tasks. In
<em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st ACM International
Conference on Information &amp;amp; Knowledge Management</em>
<em id="bib.bib18.4.2" class="ltx_emph ltx_font_italic">(CIKM ’22)</em>. ACM.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1145/3511808.3557271" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3511808.3557271</a>

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chhatre et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Kiran Chhatre, Radek
Daněček, Nikos Athanasiou,
Giorgio Becherini, Christopher Peters,
Michael J Black, and Timo Bolkart.
2023.

</span>
<span class="ltx_bibblock">Emotional Speech-driven 3D Body Animation via
Disentangled Latent Diffusion.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.04466</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christiano et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Paul Christiano, Jan
Leike, Tom B. Brown, Miljan Martic,
Shane Legg, and Dario Amodei.
2023.

</span>
<span class="ltx_bibblock">Deep reinforcement learning from human preferences.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1706.03741 [stat.ML]

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Credamo (2017)</span>
<span class="ltx_bibblock">
Credamo. 2017.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Credamo: an online data survey platform</em>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2023-12-15.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeepMotion (2024)</span>
<span class="ltx_bibblock">
DeepMotion.
2024.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">DeepMotion - AI Motion Capture &amp; Body
Tracking</em>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2024-1-2.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deichler et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Anna Deichler, Shivam
Mehta, Simon Alexanderson, and Jonas
Beskow. 2023.

</span>
<span class="ltx_bibblock">Diffusion-based co-speech gesture generation using
joint text and audio representation. In
<em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 25th International Conference on
Multimodal Interaction</em>. 755–762.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhariwal et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Prafulla Dhariwal, Heewoo
Jun, Christine Payne, Jong Wook Kim,
Alec Radford, and Ilya Sutskever.
2020.

</span>
<span class="ltx_bibblock">Jukebox: A generative model for music.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2005.00341</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ferstl and
McDonnell (2018)</span>
<span class="ltx_bibblock">
Ylva Ferstl and Rachel
McDonnell. 2018.

</span>
<span class="ltx_bibblock">IVA: Investigating the use of recurrent motion
modelling for speech gesture generation. In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">IVA
’18 Proceedings of the 18th International Conference on Intelligent Virtual
Agents</em>.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://trinityspeechgesture.scss.tcd.ie" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://trinityspeechgesture.scss.tcd.ie</a>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ferstl
et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Ylva Ferstl, Michael
Neff, and Rachel McDonnell.
2020.

</span>
<span class="ltx_bibblock">Adversarial gesture generation with realistic
gesture phasing.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">Computers &amp; Graphics</em> 89
(2020), 117–130.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.cag.2020.04.007" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.cag.2020.04.007</a>

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ferstl
et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Ylva Ferstl, Michael
Neff, and Rachel McDonnell.
2021.

</span>
<span class="ltx_bibblock">ExpressGesture: Expressive gesture generation from
speech through database matching.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">Computer Animation and Virtual Worlds</em>
32 (05 2021).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1002/cav.2016" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1002/cav.2016</a>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao
et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Nan Gao, Zeyu Zhao,
Zhi Zeng, Shuwu Zhang, and
Dongdong Weng. 2023.

</span>
<span class="ltx_bibblock">GesGPT: Speech Gesture Synthesis With Text Parsing
from GPT.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.13013</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghorbani et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Saeed Ghorbani, Ylva
Ferstl, Daniel Holden, Nikolaus F.
Troje, and Marc-André Carbonneau.
2023.

</span>
<span class="ltx_bibblock">ZeroEGGS: Zero-shot Example-based Gesture
Generation from Speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">Computer Graphics Forum</em>
42, 1 (2023),
206–216.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1111/cgf.14734" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1111/cgf.14734</a>
arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14734

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ginosar et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Shiry Ginosar, Amir Bar,
Gefen Kohavi, Caroline Chan,
Andrew Owens, and Jitendra Malik.
2019.

</span>
<span class="ltx_bibblock">Learning Individual Styles of Conversational
Gesture. In <em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Kehong Gong, Dongze Lian,
Heng Chang, Chuan Guo,
Zihang Jiang, Xinxin Zuo,
Michael Bi Mi, and Xinchao Wang.
2023.

</span>
<span class="ltx_bibblock">TM2D: Bimodality Driven 3D Dance Generation via
Music-Text Integration.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2304.02419 [cs.CV]

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Habibie et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Ikhsanul Habibie, Mohamed
Elgharib, Kripasindhu Sarkar, Ahsan
Abdullah, Simbarashe Nyatsanga, Michael
Neff, and Christian Theobalt.
2022.

</span>
<span class="ltx_bibblock">A Motion Matching-Based Framework for Controllable
Gesture Synthesis from Speech. In <em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">ACM SIGGRAPH
2022 Conference Proceedings</em> (Vancouver, BC, Canada)
<em id="bib.bib32.4.2" class="ltx_emph ltx_font_italic">(SIGGRAPH ’22)</em>. Association for
Computing Machinery, New York, NY, USA, Article
46, 9 pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3528233.3530750" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3528233.3530750</a>

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Habibie et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Ikhsanul Habibie, Weipeng
Xu, Dushyant Mehta, Lingjie Liu,
Hans-Peter Seidel, Gerard Pons-Moll,
Mohamed Elgharib, and Christian
Theobalt. 2021.

</span>
<span class="ltx_bibblock">Learning Speech-Driven 3D Conversational Gestures
from Video. In <em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 21st ACM
International Conference on Intelligent Virtual Agents</em> (Virtual Event,
Japan) <em id="bib.bib33.4.2" class="ltx_emph ltx_font_italic">(IVA ’21)</em>. Association
for Computing Machinery, New York, NY, USA,
101–108.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3472306.3478335" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3472306.3478335</a>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Indefrey and
Levelt (2004)</span>
<span class="ltx_bibblock">
P Indefrey and W.J.M
Levelt. 2004.

</span>
<span class="ltx_bibblock">The spatial and temporal signatures of word
production components.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Cognition</em> 92,
1 (2004), 101–144.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.cognition.2002.06.001" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.cognition.2002.06.001</a>

</span>
<span class="ltx_bibblock">Towards a New Functional Anatomy of Language.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Gautier Izacard, Patrick
Lewis, Maria Lomeli, Lucas Hosseini,
Fabio Petroni, Timo Schick,
Jane Dwivedi-Yu, Armand Joulin,
Sebastian Riedel, and Edouard Grave.
2022.

</span>
<span class="ltx_bibblock">Few-shot learning with retrieval augmented language
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.03299</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji
et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Longbin Ji, Pengfei Wei,
Yi Ren, Jinglin Liu,
Chen Zhang, and Xiang Yin.
2023.

</span>
<span class="ltx_bibblock">C2G2: Controllable Co-speech Gesture Generation
with Latent Diffusion Model.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.15016</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kipp (2004)</span>
<span class="ltx_bibblock">
Michael Kipp.
2004.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Gesture Generation by Imitation: From Human
Behavior to Computer Character Animation</em>.

</span>
<span class="ltx_bibblock">Dissertation.com, Boca
Raton.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kipp (2005)</span>
<span class="ltx_bibblock">
Michael Kipp.
2005.

</span>
<span class="ltx_bibblock">Gesture generation by imitation: from human
behavior to computer character animation.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:26271318" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:26271318</a>

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kopp et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2006)</span>
<span class="ltx_bibblock">
Stefan Kopp, Brigitte
Krenn, Stacy Marsella, Andrew N.
Marshall, Catherine Pelachaud, Hannes
Pirker, Kristinn R. Thórisson, and
Hannes Vilhjálmsson. 2006.

</span>
<span class="ltx_bibblock">Towards a Common Framework for Multimodal
Generation: The Behavior Markup Language. In
<em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 6th International Conference on
Intelligent Virtual Agents</em> (Marina Del Rey, CA)
<em id="bib.bib39.4.2" class="ltx_emph ltx_font_italic">(IVA’06)</em>. Springer-Verlag,
Berlin, Heidelberg, 205–217.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/11821830_17" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1007/11821830_17</a>

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kucherenko et al<span id="bib.bib40.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Taras Kucherenko, Patrik
Jonell, Sanne van Waveren, Gustav Eje
Henter, Simon Alexandersson, Iolanda
Leite, and Hedvig Kjellström.
2020.

</span>
<span class="ltx_bibblock">Gesticulator: A Framework for Semantically-Aware
Speech-Driven Gesture Generation. In <em id="bib.bib40.3.1" class="ltx_emph ltx_font_italic">Proceedings
of the 2020 International Conference on Multimodal Interaction</em> (Virtual
Event, Netherlands) <em id="bib.bib40.4.2" class="ltx_emph ltx_font_italic">(ICMI ’20)</em>.
Association for Computing Machinery,
New York, NY, USA, 242–250.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3382507.3418815" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3382507.3418815</a>

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kucherenko et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Taras Kucherenko, Rajmund
Nagy, Patrik Jonell, Michael Neff,
Hedvig Kjellström, and Gustav Eje
Henter. 2021.

</span>
<span class="ltx_bibblock">Speech2Properties2Gestures: Gesture-Property
Prediction as a Tool for Generating Representational Gestures from Speech.
In <em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 21th ACM International
Conference on Intelligent Virtual Agents</em> (Virtual Event, Japan)
<em id="bib.bib41.4.2" class="ltx_emph ltx_font_italic">(IVA ’21)</em>. Association for
Computing Machinery, New York, NY, USA.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3472306.347833" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3472306.347833</a>

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kucherenko et al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Taras Kucherenko, Pieter
Wolfert, Youngwoo Yoon, Carla Viegas,
Teodor Nikolov, Mihail Tsakov, and
Gustav Eje Henter. 2023.

</span>
<span class="ltx_bibblock">Evaluating gesture-generation in a large-scale open
challenge: The GENEA Challenge 2022.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08737</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Gilwoo Lee, Zhiwei Deng,
Shugao Ma, Takaaki Shiratori,
Siddhartha S Srinivasa, and Yaser
Sheikh. 2019.

</span>
<span class="ltx_bibblock">Talking with hands 16.2 m: A large-scale dataset of
synchronized body-finger motion and audio for conversational motion analysis
and synthesis. In <em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
International Conference on Computer Vision</em>. 763–772.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levine et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2010)</span>
<span class="ltx_bibblock">
Sergey Levine, Philipp
Krähenbühl, Sebastian Thrun, and
Vladlen Koltun. 2010.

</span>
<span class="ltx_bibblock">Gesture Controllers.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">ACM Trans. Graph.</em> 29,
4, Article 124 (jul
2010), 11 pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/1778765.1778861" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/1778765.1778861</a>

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levine
et al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2009)</span>
<span class="ltx_bibblock">
Sergey Levine, Christian
Theobalt, and Vladlen Koltun.
2009.

</span>
<span class="ltx_bibblock">Real-Time Prosody-Driven Synthesis of Body
Language.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">ACM Trans. Graph.</em> 28,
5 (dec 2009),
1–10.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/1618452.1618518" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/1618452.1618518</a>

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis
et al<span id="bib.bib46.3.3.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan
Perez, Aleksandra Piktus, Fabio Petroni,
Vladimir Karpukhin, Naman Goyal,
Heinrich Küttler, Mike Lewis,
Wen-tau Yih, Tim Rocktäschel,
et al<span id="bib.bib46.4.1" class="ltx_text">.</span> 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for
knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.5.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 33 (2020),
9459–9474.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jing Li, Di Kang,
Wenjie Pei, Xuefei Zhe,
Ying Zhang, Zhenyu He, and
Linchao Bao. 2021.

</span>
<span class="ltx_bibblock">Audio2Gestures: Generating Diverse Gestures From
Speech Audio With Conditional Variational Autoencoders. In
<em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV)</em>. 11293–11302.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li
et al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Yongqi Li, Nan Yang,
Liang Wang, Furu Wei, and
Wenjie Li. 2023.

</span>
<span class="ltx_bibblock">Multiview Identifiers Enhanced Generative Retrieval.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.16675 [cs.CL]

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang
et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Yuanzhi Liang, Qianyu
Feng, Linchao Zhu, Li Hu,
Pan Pan, and Yi Yang.
2022.

</span>
<span class="ltx_bibblock">SEEG: Semantic Energized Co-Speech Gesture
Generation. In <em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR)</em>.
10473–10482.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Haiyang Liu, Naoya
Iwamoto, Zihao Zhu, Zhengqing Li,
You Zhou, Elif Bozkurt, and
Bo Zheng. 2022a.

</span>
<span class="ltx_bibblock">DisCo: Disentangled Implicit Content and Rhythm
Learning for Diverse Co-Speech Gestures Synthesis. In
<em id="bib.bib50.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 30th ACM International
Conference on Multimedia</em> (Lisboa, Portugal) <em id="bib.bib50.4.2" class="ltx_emph ltx_font_italic">(MM
’22)</em>. Association for Computing Machinery,
New York, NY, USA, 3764–3773.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3503161.3548400" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3503161.3548400</a>

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Haiyang Liu, Zihao Zhu,
Giorgio Becherini, Yichen Peng,
Mingyang Su, You Zhou,
Xuefei Zhe, Naoya Iwamoto,
Bo Zheng, and Michael J. Black.
2023.

</span>
<span class="ltx_bibblock">EMAGE: Towards Unified Holistic Co-Speech Gesture
Generation via Masked Audio Gesture Modeling.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2401.00374 [cs.CV]

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> (2022d)</span>
<span class="ltx_bibblock">
Haiyang Liu, Zihao Zhu,
Naoya Iwamoto, Yichen Peng,
Zhengqing Li, You Zhou,
Elif Bozkurt, and Bo Zheng.
2022d.

</span>
<span class="ltx_bibblock">BEAT: A Large-Scale Semantic and Emotional
Multi-Modal Dataset for Conversational Gestures Synthesis. In
<em id="bib.bib52.3.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu
et al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Xian Liu, Qianyi Wu,
Hang Zhou, Yuanqi Du,
Wayne Wu, Dahua Lin, and
Ziwei Liu. 2022b.

</span>
<span class="ltx_bibblock">Audio-Driven Co-Speech Gesture Video Generation.
In <em id="bib.bib53.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em>, Alice H. Oh,
Alekh Agarwal, Danielle Belgrave, and
Kyunghyun Cho (Eds.).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2022c)</span>
<span class="ltx_bibblock">
Xian Liu, Qianyi Wu,
Hang Zhou, Yinghao Xu,
Rui Qian, Xinyi Lin,
Xiaowei Zhou, Wayne Wu,
Bo Dai, and Bolei Zhou.
2022c.

</span>
<span class="ltx_bibblock">Learning Hierarchical Cross-Modal Association for
Co-Speech Gesture Generation. In <em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.
10462–10472.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Shuhong Lu, Youngwoo
Yoon, and Andrew Feng. 2023.

</span>
<span class="ltx_bibblock">Co-Speech Gesture Synthesis using Discrete Gesture
Token Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.12822</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McFee et al<span id="bib.bib56.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Brian McFee, Colin
Raffel, Dawen Liang, Daniel P Ellis,
Matt McVicar, Eric Battenberg, and
Oriol Nieto. 2015.

</span>
<span class="ltx_bibblock">librosa: Audio and music signal analysis in
python. In <em id="bib.bib56.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 14th python in
science conference</em>, Vol. 8. 18–25.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McNeill (1992)</span>
<span class="ltx_bibblock">
David McNeill.
1992.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Hand and Mind: What Gestures Reveal about
Thought</em>.

</span>
<span class="ltx_bibblock">University of Chicago Press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta et al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Shivam Mehta, Siyang
Wang, Simon Alexanderson, Jonas Beskow,
Éva Székely, and Gustav Eje
Henter. 2023.

</span>
<span class="ltx_bibblock">Diff-TTSG: Denoising probabilistic integrated
speech and gesture synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.09417</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morris (1994)</span>
<span class="ltx_bibblock">
Desmond Morris.
1994.

</span>
<span class="ltx_bibblock">Bodytalk: A World Guide to Gestures.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:193353377" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:193353377</a>

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Murf.AI (2023)</span>
<span class="ltx_bibblock">
Murf.AI. 2023.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Murf.AI: An Online Text-to-Speech Tool</em>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2023-12-15.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neff
et al<span id="bib.bib61.2.2.1" class="ltx_text">.</span> (2008)</span>
<span class="ltx_bibblock">
Michael Neff, Michael
Kipp, Irene Albrecht, and Hans-Peter
Seidel. 2008.

</span>
<span class="ltx_bibblock">Gesture Modeling and Animation Based on a
Probabilistic Re-Creation of Speaker Style.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.3.1" class="ltx_emph ltx_font_italic">ACM Trans. Graph.</em> 27,
1, Article 5 (mar
2008), 24 pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/1330511.1330516" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/1330511.1330516</a>

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ng et al<span id="bib.bib62.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Evonne Ng, Javier Romero,
Timur Bagautdinov, Shaojie Bai,
Trevor Darrell, Angjoo Kanazawa, and
Alexander Richard. 2024.

</span>
<span class="ltx_bibblock">From Audio to Photoreal Embodiment: Synthesizing
Humans in Conversations. In <em id="bib.bib62.3.1" class="ltx_emph ltx_font_italic">ArXiv</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nyatsanga et al<span id="bib.bib63.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Simbarashe Nyatsanga,
Taras Kucherenko, Chaitanya Ahuja,
Gustav Eje Henter, and Michael Neff.
2023.

</span>
<span class="ltx_bibblock">A Comprehensive Review of Data-Driven Co-Speech
Gesture Generation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/ARXIV.2301.05339" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/ARXIV.2301.05339</a>

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI. 2022.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">ChatGPT: Optimizing Language Models for
Dialogue</em>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2023-05-03.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023a)</span>
<span class="ltx_bibblock">
OpenAI.
2023a.

</span>
<span class="ltx_bibblock">GPT-4 System Card.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://openai.com/research/gpt-4" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/research/gpt-4</a>

</span>
<span class="ltx_bibblock">Accessed: 2023-12-15.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023b)</span>
<span class="ltx_bibblock">
OpenAI.
2023b.

</span>
<span class="ltx_bibblock">GPT-4 Vision System Card.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://openai.com/research/gpt-4v-system-card" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/research/gpt-4v-system-card</a>

</span>
<span class="ltx_bibblock">Accessed: 2023-12-15.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang
et al<span id="bib.bib67.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeff Wu,
Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal,
Katarina Slama, Alex Ray,
John Schulman, Jacob Hilton,
Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell,
Peter Welinder, Paul Christiano,
Jan Leike, and Ryan Lowe.
2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with
human feedback.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2203.02155 [cs.CL]

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pang et al<span id="bib.bib68.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Kunkun Pang, Dafei Qin,
Yingruo Fan, Julian Habekost,
Takaaki Shiratori, Junichi Yamagishi,
and Taku Komura. 2023.

</span>
<span class="ltx_bibblock">BodyFormer: Semantics-Guided 3D Body Gesture
Synthesis with Transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.3.1" class="ltx_emph ltx_font_italic">ACM Trans. Graph.</em> 42,
4, Article 43 (jul
2023), 12 pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3592456" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3592456</a>

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi
et al<span id="bib.bib69.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Xingqun Qi, Chen Liu,
Lincheng Li, Jie Hou,
Haoran Xin, and Xin Yu.
2023a.

</span>
<span class="ltx_bibblock">EmotionGesture: Audio-Driven Diverse Emotional
Co-Speech 3D Gesture Generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib69.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.18891</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et al<span id="bib.bib70.3.3.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Xingqun Qi, Jiahao Pan,
Peng Li, Ruibin Yuan,
Xiaowei Chi, Mengfei Li,
Wenhan Luo, Wei Xue,
Shanghang Zhang, Qifeng Liu,
et al<span id="bib.bib70.4.1" class="ltx_text">.</span> 2023b.

</span>
<span class="ltx_bibblock">Weakly-Supervised Emotion Transition Learning for
Diverse 3D Co-speech Gesture Generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib70.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.17532</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span id="bib.bib71.3.3.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Alec Radford, Karthik
Narasimhan, Tim Salimans, Ilya
Sutskever, et al<span id="bib.bib71.4.1" class="ltx_text">.</span> 2018.

</span>
<span class="ltx_bibblock">Improving language understanding by generative
pre-training.

</span>
<span class="ltx_bibblock">(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span id="bib.bib72.3.3.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu,
Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever,
et al<span id="bib.bib72.4.1" class="ltx_text">.</span> 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask
learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.5.1" class="ltx_emph ltx_font_italic">OpenAI blog</em> 1,
8 (2019), 9.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et al<span id="bib.bib73.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit
Sharma, Eric Mitchell, Stefano Ermon,
Christopher D. Manning, and Chelsea
Finn. 2023.

</span>
<span class="ltx_bibblock">Direct Preference Optimization: Your Language Model
is Secretly a Reward Model.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.18290 [cs.LG]

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al<span id="bib.bib74.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam
Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and
Peter J. Liu. 2020.

</span>
<span class="ltx_bibblock">Exploring the Limits of Transfer Learning with a
Unified Text-to-Text Transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib74.3.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>
21, 140 (2020),
1–67.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://jmlr.org/papers/v21/20-074.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://jmlr.org/papers/v21/20-074.html</a>

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram et al<span id="bib.bib75.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Ori Ram, Yoav Levine,
Itay Dalmedigos, Dor Muhlgay,
Amnon Shashua, Kevin Leyton-Brown, and
Yoav Shoham. 2023.

</span>
<span class="ltx_bibblock">In-context retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.00083</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et al<span id="bib.bib76.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Yi Tay, Vinh Q. Tran,
Mostafa Dehghani, Jianmo Ni,
Dara Bahri, Harsh Mehta,
Zhen Qin, Kai Hui, Zhe
Zhao, Jai Gupta, Tal Schuster,
William W. Cohen, and Donald Metzler.
2022.

</span>
<span class="ltx_bibblock">Transformer Memory as a Differentiable Search Index.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2202.06991 [cs.CL]

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">TED Talks (2023)</span>
<span class="ltx_bibblock">
TED Talks.
2023.

</span>
<span class="ltx_bibblock">TED: Ideas change everything.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Accessed: 2023-12-15.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span id="bib.bib78.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut
Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal,
Eric Hambro, Faisal Azhar,
Aurelien Rodriguez, Armand Joulin,
Edouard Grave, and Guillaume Lample.
2023.

</span>
<span class="ltx_bibblock">LLaMA: Open and Efficient Foundation Language
Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2302.13971 [cs.CL]

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van den Oord et al<span id="bib.bib79.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Aaron van den Oord, Oriol
Vinyals, and Koray Kavukcuoglu.
2017.

</span>
<span class="ltx_bibblock">Neural Discrete Representation Learning. In
<em id="bib.bib79.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st International Conference on
Neural Information Processing Systems</em> (Long Beach, California, USA)
<em id="bib.bib79.4.2" class="ltx_emph ltx_font_italic">(NIPS’17)</em>. Curran Associates
Inc., Red Hook, NY, USA, 6309–6318.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span id="bib.bib80.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam
Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin.
2017.

</span>
<span class="ltx_bibblock">Attention is All you Need. In
<em id="bib.bib80.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em>, I. Guyon,
U. Von Luxburg, S. Bengio,
H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett
(Eds.), Vol. 30. Curran Associates,
Inc.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voß and Kopp (2023)</span>
<span class="ltx_bibblock">
Hendric Voß and
Stefan Kopp. 2023.

</span>
<span class="ltx_bibblock">Augmented Co-Speech Gesture Generation: Including
Form and Meaning Features to Guide Learning-Based Gesture Synthesis. In
<em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd ACM International
Conference on Intelligent Virtual Agents</em>. 1–8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wagner and
Armstrong (2003)</span>
<span class="ltx_bibblock">
Melissa Wagner and
Nancy Leonard Armstrong. 2003.

</span>
<span class="ltx_bibblock">Field Guide to Gestures: How to Identify and
Interpret Virtually Every Gesture Known to Man.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://api.semanticscholar.org/CorpusID:141961447" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:141961447</a>

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wagner
et al<span id="bib.bib83.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Petra Wagner, Zofia
Malisz, and Stefan Kopp.
2014.

</span>
<span class="ltx_bibblock">Gesture and speech in interaction: An overview.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.3.1" class="ltx_emph ltx_font_italic">Speech Communication</em> 57
(2014), 209–232.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.specom.2013.09.008" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016/j.specom.2013.09.008</a>

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib84.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yujing Wang, Yingyan Hou,
Haonan Wang, Ziming Miao,
Shibin Wu, Hao Sun, Qi
Chen, Yuqing Xia, Chengmin Chi,
Guoshuai Zhao, Zheng Liu,
Xing Xie, Hao Allen Sun,
Weiwei Deng, Qi Zhang, and
Mao Yang. 2023a.

</span>
<span class="ltx_bibblock">A Neural Corpus Indexer for Document Retrieval.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2206.02743 [cs.IR]

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib85.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yizhong Wang, Yeganeh
Kordi, Swaroop Mishra, Alisa Liu,
Noah A. Smith, Daniel Khashabi, and
Hannaneh Hajishirzi. 2023b.

</span>
<span class="ltx_bibblock">Self-Instruct: Aligning Language Models with
Self-Generated Instructions.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2212.10560 [cs.CL]

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">World
Federation of the Deaf (1975)</span>
<span class="ltx_bibblock">
World Federation of the Deaf.
1975.

</span>
<span class="ltx_bibblock"><em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">GESTUNO: International sign language of
the deaf, langage gestuel international Des sourds</em>.

</span>
<span class="ltx_bibblock">British Deaf Association [for] the World
Federation of the Deaf, Carlise.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu
et al<span id="bib.bib87.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Zunnan Xu, Yachao Zhang,
Sicheng Yang, Ronghui Li, and
Xiu Li. 2023.

</span>
<span class="ltx_bibblock">Chain of Generation: Multi-Modal Gesture Synthesis
via Cascaded Conditional Control.

</span>
<span class="ltx_bibblock"><em id="bib.bib87.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.15900</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue
et al<span id="bib.bib88.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Haiwei Xue, Sicheng Yang,
Zhensong Zhang, Zhiyong Wu,
Minglei Li, Zonghong Dai, and
Helen Meng. 2023.

</span>
<span class="ltx_bibblock">Conversational Co-Speech Gesture Generation via
Modeling Dialog Intention, Emotion, and Context with Diffusion Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib88.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.15567</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib89.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Sicheng Yang, Zhiyong Wu,
Minglei Li, Zhensong Zhang,
Lei Hao, Weihong Bao,
Ming Cheng, and Long Xiao.
2023b.

</span>
<span class="ltx_bibblock">DiffuseStyleGesture: Stylized Audio-Driven
Co-Speech Gesture Generation with Diffusion Models. In
<em id="bib.bib89.3.1" class="ltx_emph ltx_font_italic">Proceedings of the Thirty-Second International
Joint Conference on Artificial Intelligence, IJCAI-23</em>.
International Joint Conferences on Artificial
Intelligence Organization, 5860–5868.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.24963/ijcai.2023/650" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.24963/ijcai.2023/650</a>

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib90.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Sicheng Yang, Zhiyong Wu,
Minglei Li, Zhensong Zhang,
Lei Hao, Weihong Bao, and
Haolin Zhuang. 2023a.

</span>
<span class="ltx_bibblock">QPGesture: Quantization-Based and Phase-Guided
Motion Matching for Natural Speech-Driven Gesture Generation. In
<em id="bib.bib90.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition</em>. 2321–2330.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib91.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Sicheng Yang, Zunnan Xu,
Haiwei Xue, Yongkang Cheng,
Shaoli Huang, Mingming Gong, and
Zhiyong Wu. 2024.

</span>
<span class="ltx_bibblock">Freetalker: Controllable Speech and Text-Driven
Gesture Generation Based on Diffusion Models for Enhanced Speaker
Naturalness. In <em id="bib.bib91.3.1" class="ltx_emph ltx_font_italic">ICASSP 2024 - 2024 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao
et al<span id="bib.bib92.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Heyuan Yao, Zhenhua Song,
Yuyang Zhou, Tenglong Ao,
Baoquan Chen, and Libin Liu.
2023.

</span>
<span class="ltx_bibblock">MoConVQ: Unified Physics-Based Motion Control via
Scalable Discrete Representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib92.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.10198</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yazdian
et al<span id="bib.bib93.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Payam Jome Yazdian, Mo
Chen, and Angelica Lim.
2022.

</span>
<span class="ltx_bibblock">Gesture2Vec: Clustering Gestures using
Representation Learning Methods for Co-speech Gesture Generation. In
<em id="bib.bib93.3.1" class="ltx_emph ltx_font_italic">2022 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS)</em>. 3100–3107.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/IROS47612.2022.9981117" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/IROS47612.2022.9981117</a>

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye
et al<span id="bib.bib94.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Sheng Ye, Yu-Hui Wen,
Yanan Sun, Ying He,
Ziyang Zhang, Yaoyuan Wang,
Weihua He, and Yong-Jin Liu.
2022.

</span>
<span class="ltx_bibblock">Audio-Driven Stylized Gesture Generation
with Flow-Based Model. In <em id="bib.bib94.3.1" class="ltx_emph ltx_font_italic">Computer Vision – ECCV
2022</em>, Shai Avidan,
Gabriel Brostow, Moustapha Cissé,
Giovanni Maria Farinella, and Tal
Hassner (Eds.). Springer Nature Switzerland,
Cham, 712–728.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yi et al<span id="bib.bib95.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Hongwei Yi, Hualin Liang,
Yifei Liu, Qiong Cao,
Yandong Wen, Timo Bolkart,
Dacheng Tao, and Michael J. Black.
2022.

</span>
<span class="ltx_bibblock">Generating Holistic 3D Human Motion from Speech.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al<span id="bib.bib96.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Lianying Yin, Yijun Wang,
Tianyu He, Jinming Liu,
Wei Zhao, Bohan Li, Xin
Jin, and Jianxin Lin. 2023.

</span>
<span class="ltx_bibblock">EMoG: Synthesizing Emotive Co-speech 3D Gesture
with Diffusion Model.

</span>
<span class="ltx_bibblock"><em id="bib.bib96.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.11496</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoon
et al<span id="bib.bib97.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Youngwoo Yoon, Bok Cha,
Joo-Haeng Lee, Minsu Jang,
Jaeyeon Lee, Jaehong Kim, and
Geehyuk Lee. 2020.

</span>
<span class="ltx_bibblock">Speech Gesture Generation from the Trimodal Context
of Text, Audio, and Speaker Identity.

</span>
<span class="ltx_bibblock"><em id="bib.bib97.3.1" class="ltx_emph ltx_font_italic">ACM Trans. Graph.</em> 39,
6, Article 222 (nov
2020), 16 pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3414685.3417838" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3414685.3417838</a>

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoon
et al<span id="bib.bib98.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Youngwoo Yoon, Woo-Ri Ko,
Minsu Jang, Jaeyeon Lee,
Jaehong Kim, and Geehyuk Lee.
2019.

</span>
<span class="ltx_bibblock">Robots Learn Social Skills: End-to-End Learning of
Co-Speech Gesture Generation for Humanoid Robots. In
<em id="bib.bib98.3.1" class="ltx_emph ltx_font_italic">2019 International Conference on Robotics and
Automation (ICRA)</em>. 4303–4309.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ICRA.2019.8793720" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICRA.2019.8793720</a>

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoon et al<span id="bib.bib99.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Youngwoo Yoon, Pieter
Wolfert, Taras Kucherenko, Carla Viegas,
Teodor Nikolov, Mihail Tsakov, and
Gustav Eje Henter. 2022.

</span>
<span class="ltx_bibblock">The GENEA Challenge 2022: A Large Evaluation of
Data-Driven Co-Speech Gesture Generation. In
<em id="bib.bib99.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 International Conference on
Multimodal Interaction</em> (Bengaluru, India) <em id="bib.bib99.4.2" class="ltx_emph ltx_font_italic">(ICMI
’22)</em>. Association for Computing Machinery,
New York, NY, USA, 736–747.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3536221.3558058" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3536221.3558058</a>

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeghidour et al<span id="bib.bib100.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Neil Zeghidour, Alejandro
Luebs, Ahmed Omran, Jan Skoglund, and
Marco Tagliasacchi. 2022.

</span>
<span class="ltx_bibblock">SoundStream: An End-to-End Neural Audio Codec.

</span>
<span class="ltx_bibblock"><em id="bib.bib100.3.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and
Language Processing</em> 30 (2022),
495–507.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/TASLP.2021.3129994" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TASLP.2021.3129994</a>

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
et al<span id="bib.bib101.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Fan Zhang, Naye Ji,
Fuxing Gao, and Yongping Li.
2023a.

</span>
<span class="ltx_bibblock">DiffMotion: Speech-Driven Gesture Synthesis Using
Denoising Diffusion Model. In <em id="bib.bib101.3.1" class="ltx_emph ltx_font_italic">MultiMedia Modeling:
29th International Conference, MMM 2023, Bergen, Norway, January 9–12, 2023,
Proceedings, Part I</em>. Springer, 231–242.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
et al<span id="bib.bib102.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Fan Zhang, Naye Ji,
Fuxing Gao, Siyuan Zhao,
Zhaohan Wang, and Shunman Li.
2023b.

</span>
<span class="ltx_bibblock">Audio is all in one: speech-driven gesture synthetics
using WavLM pre-trained model.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2308.05995 [cs.SD]

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
et al<span id="bib.bib103.2.2.1" class="ltx_text">.</span> (2023d)</span>
<span class="ltx_bibblock">
Jinsong Zhang, Minjie
Zhu, Yuxiang Zhang, Yebin Liu, and
Kun Li. 2023d.

</span>
<span class="ltx_bibblock">SpeechAct: Towards Generating Whole-body Motion
from Speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib103.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.17425</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib104.2.2.1" class="ltx_text">.</span> (2023c)</span>
<span class="ltx_bibblock">
Xinghua Zhang, Bowen Yu,
Haiyang Yu, Yangyu Lv,
Tingwen Liu, Fei Huang,
Hongbo Xu, and Yongbin Li.
2023c.

</span>
<span class="ltx_bibblock">Wider and Deeper LLM Networks are Fairer LLM
Evaluators.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2308.01862 [cs.CL]

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhi
et al<span id="bib.bib105.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Yihao Zhi, Xiaodong Cun,
Xuelin Chen, Xi Shen,
Wen Guo, Shaoli Huang, and
Shenghua Gao. 2023.

</span>
<span class="ltx_bibblock">LivelySpeaker: Towards Semantic-Aware Co-Speech
Gesture Generation. In <em id="bib.bib105.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV)</em>.
20807–20817.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou
et al<span id="bib.bib106.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Chi Zhou, Tengyue Bian,
and Kang Chen. 2022a.

</span>
<span class="ltx_bibblock">GestureMaster: Graph-Based Speech-Driven Gesture
Generation. In <em id="bib.bib106.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022
International Conference on Multimodal Interaction</em> (Bengaluru, India)
<em id="bib.bib106.4.2" class="ltx_emph ltx_font_italic">(ICMI ’22)</em>. Association for
Computing Machinery, New York, NY, USA,
764–770.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3536221.3558063" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3536221.3558063</a>

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou
et al<span id="bib.bib107.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Yujia Zhou, Jing Yao,
Zhicheng Dou, Ledell Wu, and
Ji-Rong Wen. 2022b.

</span>
<span class="ltx_bibblock">DynamicRetriever: A Pre-training Model-based IR
System with Neither Sparse nor Dense Index.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2203.00537 [cs.IR]

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou
et al<span id="bib.bib108.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Zixiang Zhou, Yu Wan,
and Baoyuan Wang. 2023.

</span>
<span class="ltx_bibblock">A Unified Framework for Multimodal, Multi-Part
Human Motion Synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib108.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.16471</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu
et al<span id="bib.bib109.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Lingting Zhu, Xian Liu,
Xuanyu Liu, Rui Qian,
Ziwei Liu, and Lequan Yu.
2023a.

</span>
<span class="ltx_bibblock">Taming Diffusion Models for Audio-Driven Co-Speech
Gesture Generation. In <em id="bib.bib109.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>.
10544–10553.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span id="bib.bib110.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yutao Zhu, Huaying Yuan,
Shuting Wang, Jiongnan Liu,
Wenhan Liu, Chenlong Deng,
Zhicheng Dou, and Ji-Rong Wen.
2023b.

</span>
<span class="ltx_bibblock">Large Language Models for Information Retrieval: A
Survey.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2308.07107 [cs.CL]

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ziems
et al<span id="bib.bib111.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Noah Ziems, Wenhao Yu,
Zhihan Zhang, and Meng Jiang.
2023.

</span>
<span class="ltx_bibblock">Large Language Models are Built-in Autoregressive
Search Engines.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.09612 [cs.CL]

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Details of User Study</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">A comparison set comprises two videos, each lasting 10 seconds, displayed sequentially from left to right. These pairs are generated using the same speech and character model. Questionnaires of the user study are built via the Human Behavior Online (HBO) tool provided by the Credamo platform <cite class="ltx_cite ltx_citemacro_citep">(Credamo, <a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite>. Each test and questionnaire consist of 24 such video pairs. On average, an experiment is completed in 12 minutes. We have recruited 220 participants through Credamo, of which 117 are male and 103 are female. 77 participants are 18 - 25 years of age, 121 are between 25 - 35, and 22 are above 35. They are sourced from the United States (30%) and China (70%) , and those involved in tests with audio must speak English fluently. In order to ensure that participants can clearly understand the detailed meaning of each metric and can distinguish between them effectively, We provide the participants with detailed instructions. For <em id="A1.p1.1.1" class="ltx_emph ltx_font_italic">human likeness</em>, participants are requested to assess the naturalness and fluidity of the movements, as well as their resemblance to actual human movements. For <em id="A1.p1.1.2" class="ltx_emph ltx_font_italic">beat matching</em>, we instruct the participants to focus on the rhythmic coherence between gestures and speech audio. For <em id="A1.p1.1.3" class="ltx_emph ltx_font_italic">semantic accuracy</em>, the participants first learn about ”what are semantic gestures” by watching several examples of semantic gestures. They are asked to read the transcript of each speech before rating, ensuring a more informed judgment on the semantic performance of the results. Meanwhile, to ensure the validity of the responses, following <cite class="ltx_cite ltx_citemacro_citep">(Ao et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>, an attention check is incorporated randomly within the experiment. This check involves a text message, <em id="A1.p1.1.4" class="ltx_emph ltx_font_italic">attention: please select the leftmost option</em>, displayed continuously at the video pair’s bottom throughout the question and embedded in the video during the transition between the two clips. Responses failing this attention check are excluded from the final results.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p">The settings for the <em id="A1.p2.1.1" class="ltx_emph ltx_font_italic">human likeness</em>, <em id="A1.p2.1.2" class="ltx_emph ltx_font_italic">beat matching</em>, and <em id="A1.p2.1.3" class="ltx_emph ltx_font_italic">semantic accuracy</em> tests on both the ZEGGS dataset and the BEAT dataset are identical. Specifically, We evaluate four methods for each dataset: GT, Ours, Ours (without semantic alignment), and either GestureDiffuCLIP for the ZEGGS dataset or CaMN for the BEAT dataset. This leads to 12 distinct side-by-side comparison combinations. We randomly select 24 speech segments from the test sets of the ZEGGS and BEAT datasets, respectively, and extract a 10-second clip from each segment starting at an arbitrary position. They are employed in the generation of gestures, yielding 24 video clips for each method. Consequently, there are 288 video pairs in total (24 speech segments <math id="A1.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.p2.1.m1.1a"><mo id="A1.p2.1.m1.1.1" xref="A1.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.p2.1.m1.1b"><times id="A1.p2.1.m1.1.1.cmml" xref="A1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.1.m1.1c">\times</annotation></semantics></math> 12 combinations). Participants are tasked with evaluating 24 video pairs, covering all 24 speech segments. Each of the 12 comparison combinations is presented twice. The orders of both speech samples and the pairing of comparisons are randomized for each subject.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Quantitative Evaluation of LLM-Based Retrieval Model</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">We evaluate the annotation results quantitatively using three methods by two metrics: a) <em id="A2.p1.1.1" class="ltx_emph ltx_font_italic">Accuracy</em>, which assesses whether the annotated gestures are in the SeG dataset, and b) <em id="A2.p1.1.2" class="ltx_emph ltx_font_italic">Semantic Matching</em>, which examines whether the annotated gestures at a specific point match the context and semantics of the situation.</p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.3" class="ltx_p">In particular, we randomly select <math id="A2.p2.1.m1.1" class="ltx_Math" alttext="49" display="inline"><semantics id="A2.p2.1.m1.1a"><mn id="A2.p2.1.m1.1.1" xref="A2.p2.1.m1.1.1.cmml">49</mn><annotation-xml encoding="MathML-Content" id="A2.p2.1.m1.1b"><cn type="integer" id="A2.p2.1.m1.1.1.cmml" xref="A2.p2.1.m1.1.1">49</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.1.m1.1c">49</annotation></semantics></math> different speech fragments from the BEAT dataset at first, extracting complete sentences from these fragments’ first 20 seconds of transcript. Subsequently, we apply zero-shot, few-shot, and finetuned LLM strategies to perform semantic gesture retrieval on these sentences. For <em id="A2.p2.3.1" class="ltx_emph ltx_font_italic">Accuracy</em>, we employ word matching to detect whether each annotated semantic gesture appears in the SeG dataset we are given. We calculated this metric as the ratio of correctly annotated semantic gestures to the total number of annotated gestures. For <em id="A2.p2.3.2" class="ltx_emph ltx_font_italic">Semantic Matching</em>, we utilize the most powerful language model to date, GPT-4 Turbo <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib65" title="" class="ltx_ref">2023a</a>)</cite>. By employing a prompt-based approach, we make it to play the role of scorer, with each metric rated on a scale from <math id="A2.p2.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="A2.p2.2.m2.1a"><mn id="A2.p2.2.m2.1.1" xref="A2.p2.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A2.p2.2.m2.1b"><cn type="integer" id="A2.p2.2.m2.1.1.cmml" xref="A2.p2.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.2.m2.1c">1</annotation></semantics></math> to <math id="A2.p2.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="A2.p2.3.m3.1a"><mn id="A2.p2.3.m3.1.1" xref="A2.p2.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="A2.p2.3.m3.1b"><cn type="integer" id="A2.p2.3.m3.1.1.cmml" xref="A2.p2.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.3.m3.1c">10</annotation></semantics></math>. The corresponding prompt is shown in fig <a href="#A2.F17" title="Figure 17 ‣ Appendix B Quantitative Evaluation of LLM-Based Retrieval Model ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>. Finally, we calculate the mean values of the three different methods across the two metrics. The average scores are detailed in table <a href="#A2.T3" title="Table 3 ‣ Appendix B Quantitative Evaluation of LLM-Based Retrieval Model ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>:</p>
</div>
<figure id="A2.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A2.T3.8.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>. </span><span id="A2.T3.9.2" class="ltx_text" style="font-size:90%;">Quantitative evaluation on annotation results. This table reports the mean (± standard deviation) values for each metric by synthesizing on the test data 10 times.</span></figcaption>
<table id="A2.T3.6" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A2.T3.6.7" class="ltx_tr">
<td id="A2.T3.6.7.1" class="ltx_td ltx_align_left ltx_border_tt">Metric</td>
<td id="A2.T3.6.7.2" class="ltx_td ltx_align_center ltx_border_tt">zero-shot</td>
<td id="A2.T3.6.7.3" class="ltx_td ltx_align_center ltx_border_tt">few-shot</td>
<td id="A2.T3.6.7.4" class="ltx_td ltx_align_center ltx_border_tt">finetune</td>
</tr>
<tr id="A2.T3.3.3" class="ltx_tr">
<td id="A2.T3.3.3.4" class="ltx_td ltx_align_left ltx_border_t">Accuracy</td>
<td id="A2.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="A2.T3.1.1.1.m1.1" class="ltx_Math" alttext="56.7\%\pm 8.1\%" display="inline"><semantics id="A2.T3.1.1.1.m1.1a"><mrow id="A2.T3.1.1.1.m1.1.1" xref="A2.T3.1.1.1.m1.1.1.cmml"><mrow id="A2.T3.1.1.1.m1.1.1.2" xref="A2.T3.1.1.1.m1.1.1.2.cmml"><mn id="A2.T3.1.1.1.m1.1.1.2.2" xref="A2.T3.1.1.1.m1.1.1.2.2.cmml">56.7</mn><mo id="A2.T3.1.1.1.m1.1.1.2.1" xref="A2.T3.1.1.1.m1.1.1.2.1.cmml">%</mo></mrow><mo id="A2.T3.1.1.1.m1.1.1.1" xref="A2.T3.1.1.1.m1.1.1.1.cmml">±</mo><mrow id="A2.T3.1.1.1.m1.1.1.3" xref="A2.T3.1.1.1.m1.1.1.3.cmml"><mn id="A2.T3.1.1.1.m1.1.1.3.2" xref="A2.T3.1.1.1.m1.1.1.3.2.cmml">8.1</mn><mo id="A2.T3.1.1.1.m1.1.1.3.1" xref="A2.T3.1.1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.1.1.1.m1.1b"><apply id="A2.T3.1.1.1.m1.1.1.cmml" xref="A2.T3.1.1.1.m1.1.1"><csymbol cd="latexml" id="A2.T3.1.1.1.m1.1.1.1.cmml" xref="A2.T3.1.1.1.m1.1.1.1">plus-or-minus</csymbol><apply id="A2.T3.1.1.1.m1.1.1.2.cmml" xref="A2.T3.1.1.1.m1.1.1.2"><csymbol cd="latexml" id="A2.T3.1.1.1.m1.1.1.2.1.cmml" xref="A2.T3.1.1.1.m1.1.1.2.1">percent</csymbol><cn type="float" id="A2.T3.1.1.1.m1.1.1.2.2.cmml" xref="A2.T3.1.1.1.m1.1.1.2.2">56.7</cn></apply><apply id="A2.T3.1.1.1.m1.1.1.3.cmml" xref="A2.T3.1.1.1.m1.1.1.3"><csymbol cd="latexml" id="A2.T3.1.1.1.m1.1.1.3.1.cmml" xref="A2.T3.1.1.1.m1.1.1.3.1">percent</csymbol><cn type="float" id="A2.T3.1.1.1.m1.1.1.3.2.cmml" xref="A2.T3.1.1.1.m1.1.1.3.2">8.1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.1.1.1.m1.1c">56.7\%\pm 8.1\%</annotation></semantics></math></td>
<td id="A2.T3.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><math id="A2.T3.2.2.2.m1.1" class="ltx_Math" alttext="92.8\%\pm 3.8\%" display="inline"><semantics id="A2.T3.2.2.2.m1.1a"><mrow id="A2.T3.2.2.2.m1.1.1" xref="A2.T3.2.2.2.m1.1.1.cmml"><mrow id="A2.T3.2.2.2.m1.1.1.2" xref="A2.T3.2.2.2.m1.1.1.2.cmml"><mn id="A2.T3.2.2.2.m1.1.1.2.2" xref="A2.T3.2.2.2.m1.1.1.2.2.cmml">92.8</mn><mo id="A2.T3.2.2.2.m1.1.1.2.1" xref="A2.T3.2.2.2.m1.1.1.2.1.cmml">%</mo></mrow><mo id="A2.T3.2.2.2.m1.1.1.1" xref="A2.T3.2.2.2.m1.1.1.1.cmml">±</mo><mrow id="A2.T3.2.2.2.m1.1.1.3" xref="A2.T3.2.2.2.m1.1.1.3.cmml"><mn id="A2.T3.2.2.2.m1.1.1.3.2" xref="A2.T3.2.2.2.m1.1.1.3.2.cmml">3.8</mn><mo id="A2.T3.2.2.2.m1.1.1.3.1" xref="A2.T3.2.2.2.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.2.2.2.m1.1b"><apply id="A2.T3.2.2.2.m1.1.1.cmml" xref="A2.T3.2.2.2.m1.1.1"><csymbol cd="latexml" id="A2.T3.2.2.2.m1.1.1.1.cmml" xref="A2.T3.2.2.2.m1.1.1.1">plus-or-minus</csymbol><apply id="A2.T3.2.2.2.m1.1.1.2.cmml" xref="A2.T3.2.2.2.m1.1.1.2"><csymbol cd="latexml" id="A2.T3.2.2.2.m1.1.1.2.1.cmml" xref="A2.T3.2.2.2.m1.1.1.2.1">percent</csymbol><cn type="float" id="A2.T3.2.2.2.m1.1.1.2.2.cmml" xref="A2.T3.2.2.2.m1.1.1.2.2">92.8</cn></apply><apply id="A2.T3.2.2.2.m1.1.1.3.cmml" xref="A2.T3.2.2.2.m1.1.1.3"><csymbol cd="latexml" id="A2.T3.2.2.2.m1.1.1.3.1.cmml" xref="A2.T3.2.2.2.m1.1.1.3.1">percent</csymbol><cn type="float" id="A2.T3.2.2.2.m1.1.1.3.2.cmml" xref="A2.T3.2.2.2.m1.1.1.3.2">3.8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.2.2.2.m1.1c">92.8\%\pm 3.8\%</annotation></semantics></math></td>
<td id="A2.T3.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><math id="A2.T3.3.3.3.m1.1" class="ltx_Math" alttext="\bm{97.8\%\pm 1.4\%}" display="inline"><semantics id="A2.T3.3.3.3.m1.1a"><mrow id="A2.T3.3.3.3.m1.1.1" xref="A2.T3.3.3.3.m1.1.1.cmml"><mrow id="A2.T3.3.3.3.m1.1.1.2" xref="A2.T3.3.3.3.m1.1.1.2.cmml"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="A2.T3.3.3.3.m1.1.1.2.2" xref="A2.T3.3.3.3.m1.1.1.2.2.cmml">97.8</mn><mo class="ltx_mathvariant_bold" mathvariant="bold" id="A2.T3.3.3.3.m1.1.1.2.1" xref="A2.T3.3.3.3.m1.1.1.2.1.cmml">%</mo></mrow><mo class="ltx_mathvariant_bold" mathvariant="bold" id="A2.T3.3.3.3.m1.1.1.1" xref="A2.T3.3.3.3.m1.1.1.1.cmml">±</mo><mrow id="A2.T3.3.3.3.m1.1.1.3" xref="A2.T3.3.3.3.m1.1.1.3.cmml"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="A2.T3.3.3.3.m1.1.1.3.2" xref="A2.T3.3.3.3.m1.1.1.3.2.cmml">1.4</mn><mo class="ltx_mathvariant_bold" mathvariant="bold" id="A2.T3.3.3.3.m1.1.1.3.1" xref="A2.T3.3.3.3.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.3.3.3.m1.1b"><apply id="A2.T3.3.3.3.m1.1.1.cmml" xref="A2.T3.3.3.3.m1.1.1"><csymbol cd="latexml" id="A2.T3.3.3.3.m1.1.1.1.cmml" xref="A2.T3.3.3.3.m1.1.1.1">plus-or-minus</csymbol><apply id="A2.T3.3.3.3.m1.1.1.2.cmml" xref="A2.T3.3.3.3.m1.1.1.2"><csymbol cd="latexml" id="A2.T3.3.3.3.m1.1.1.2.1.cmml" xref="A2.T3.3.3.3.m1.1.1.2.1">percent</csymbol><cn type="float" id="A2.T3.3.3.3.m1.1.1.2.2.cmml" xref="A2.T3.3.3.3.m1.1.1.2.2">97.8</cn></apply><apply id="A2.T3.3.3.3.m1.1.1.3.cmml" xref="A2.T3.3.3.3.m1.1.1.3"><csymbol cd="latexml" id="A2.T3.3.3.3.m1.1.1.3.1.cmml" xref="A2.T3.3.3.3.m1.1.1.3.1">percent</csymbol><cn type="float" id="A2.T3.3.3.3.m1.1.1.3.2.cmml" xref="A2.T3.3.3.3.m1.1.1.3.2">1.4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.3.3.3.m1.1c">\bm{97.8\%\pm 1.4\%}</annotation></semantics></math></td>
</tr>
<tr id="A2.T3.6.6" class="ltx_tr">
<td id="A2.T3.6.6.4" class="ltx_td ltx_align_left ltx_border_bb">Semantic Matching</td>
<td id="A2.T3.4.4.1" class="ltx_td ltx_align_center ltx_border_bb"><math id="A2.T3.4.4.1.m1.1" class="ltx_Math" alttext="4.35\pm 0.21" display="inline"><semantics id="A2.T3.4.4.1.m1.1a"><mrow id="A2.T3.4.4.1.m1.1.1" xref="A2.T3.4.4.1.m1.1.1.cmml"><mn id="A2.T3.4.4.1.m1.1.1.2" xref="A2.T3.4.4.1.m1.1.1.2.cmml">4.35</mn><mo id="A2.T3.4.4.1.m1.1.1.1" xref="A2.T3.4.4.1.m1.1.1.1.cmml">±</mo><mn id="A2.T3.4.4.1.m1.1.1.3" xref="A2.T3.4.4.1.m1.1.1.3.cmml">0.21</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.4.4.1.m1.1b"><apply id="A2.T3.4.4.1.m1.1.1.cmml" xref="A2.T3.4.4.1.m1.1.1"><csymbol cd="latexml" id="A2.T3.4.4.1.m1.1.1.1.cmml" xref="A2.T3.4.4.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="A2.T3.4.4.1.m1.1.1.2.cmml" xref="A2.T3.4.4.1.m1.1.1.2">4.35</cn><cn type="float" id="A2.T3.4.4.1.m1.1.1.3.cmml" xref="A2.T3.4.4.1.m1.1.1.3">0.21</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.4.4.1.m1.1c">4.35\pm 0.21</annotation></semantics></math></td>
<td id="A2.T3.5.5.2" class="ltx_td ltx_align_center ltx_border_bb"><math id="A2.T3.5.5.2.m1.1" class="ltx_Math" alttext="6.00\pm 0.30" display="inline"><semantics id="A2.T3.5.5.2.m1.1a"><mrow id="A2.T3.5.5.2.m1.1.1" xref="A2.T3.5.5.2.m1.1.1.cmml"><mn id="A2.T3.5.5.2.m1.1.1.2" xref="A2.T3.5.5.2.m1.1.1.2.cmml">6.00</mn><mo id="A2.T3.5.5.2.m1.1.1.1" xref="A2.T3.5.5.2.m1.1.1.1.cmml">±</mo><mn id="A2.T3.5.5.2.m1.1.1.3" xref="A2.T3.5.5.2.m1.1.1.3.cmml">0.30</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.5.5.2.m1.1b"><apply id="A2.T3.5.5.2.m1.1.1.cmml" xref="A2.T3.5.5.2.m1.1.1"><csymbol cd="latexml" id="A2.T3.5.5.2.m1.1.1.1.cmml" xref="A2.T3.5.5.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="A2.T3.5.5.2.m1.1.1.2.cmml" xref="A2.T3.5.5.2.m1.1.1.2">6.00</cn><cn type="float" id="A2.T3.5.5.2.m1.1.1.3.cmml" xref="A2.T3.5.5.2.m1.1.1.3">0.30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.5.5.2.m1.1c">6.00\pm 0.30</annotation></semantics></math></td>
<td id="A2.T3.6.6.3" class="ltx_td ltx_align_center ltx_border_bb"><math id="A2.T3.6.6.3.m1.1" class="ltx_Math" alttext="\bm{7.38\pm 0.29}" display="inline"><semantics id="A2.T3.6.6.3.m1.1a"><mrow id="A2.T3.6.6.3.m1.1.1" xref="A2.T3.6.6.3.m1.1.1.cmml"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="A2.T3.6.6.3.m1.1.1.2" xref="A2.T3.6.6.3.m1.1.1.2.cmml">7.38</mn><mo class="ltx_mathvariant_bold" mathvariant="bold" id="A2.T3.6.6.3.m1.1.1.1" xref="A2.T3.6.6.3.m1.1.1.1.cmml">±</mo><mn class="ltx_mathvariant_bold" mathvariant="bold" id="A2.T3.6.6.3.m1.1.1.3" xref="A2.T3.6.6.3.m1.1.1.3.cmml">0.29</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.6.6.3.m1.1b"><apply id="A2.T3.6.6.3.m1.1.1.cmml" xref="A2.T3.6.6.3.m1.1.1"><csymbol cd="latexml" id="A2.T3.6.6.3.m1.1.1.1.cmml" xref="A2.T3.6.6.3.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="A2.T3.6.6.3.m1.1.1.2.cmml" xref="A2.T3.6.6.3.m1.1.1.2">7.38</cn><cn type="float" id="A2.T3.6.6.3.m1.1.1.3.cmml" xref="A2.T3.6.6.3.m1.1.1.3">0.29</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.6.6.3.m1.1c">\bm{7.38\pm 0.29}</annotation></semantics></math></td>
</tr>
</table>
</figure>
<div id="A2.p3" class="ltx_para">
<p id="A2.p3.1" class="ltx_p">As indicated by the qualitative analysis in Section <a href="#S7.SS5.SSS1" title="7.5.1. LLM-Based Retrieval Model. ‣ 7.5. Ablation Study ‣ 7. Evaluation ‣ Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.5.1</span></a>, the zero-shot strategy performs significantly worse, while the few-shot strategy shows marked improvement in both two metrics, particularly in <em id="A2.p3.1.1" class="ltx_emph ltx_font_italic">Accurancy</em>. The fine-tuned LLM exhibits a 23.0% enhancement in <em id="A2.p3.1.2" class="ltx_emph ltx_font_italic">Semantic Matching</em> beyond the few-shot strategy, reflecting significant advances in semantic comprehension. These results demonstrate the superiority of our fine-tuned LLM.</p>
</div>
<figure id="A2.F17" class="ltx_figure"><img src="/html/2405.09814/assets/x17.png" id="A2.F17.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="369" height="230" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F17.3.1.1" class="ltx_text" style="font-size:90%;">Figure 17</span>. </span><span id="A2.F17.4.2" class="ltx_text" style="font-size:90%;">The prompt of GPT4-based Scorer for <em id="A2.F17.4.2.1" class="ltx_emph ltx_font_italic">Semantic Matching</em>.</span></figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.09813" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.09814" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.09814">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.09814" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.09815" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 13:36:33 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
