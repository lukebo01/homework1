<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.18635] Fusion approaches for emotion recognition from speech using acoustic and text-based features</title><meta property="og:description" content="In this paper, we study different approaches for classifying emotions from speech using acoustic and text-based features. We propose to obtain contextualized word embeddings with BERT to represent the information contaâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Fusion approaches for emotion recognition from speech using acoustic and text-based features">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Fusion approaches for emotion recognition from speech using acoustic and text-based features">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.18635">

<!--Generated on Fri Apr  5 15:57:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Fusion approaches for emotion recognition from speech using acoustic and text-based features</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text" style="font-size:90%;">In this paper, we study different approaches for classifying emotions from speech using acoustic and text-based features. We propose to obtain contextualized word embeddings with BERT to represent the information contained in speech transcriptions and show that
this results in better performance than using Glove embeddings. We also propose and compare different strategies to combine the audio and text modalities, evaluating them on IEMOCAP and MSP-PODCAST datasets. We find that fusing acoustic and text-based systems is beneficial on both datasets, though only subtle differences are observed across the evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect that the criteria used to define the cross-validation folds have on results. In particular, the standard way of creating folds for this dataset results in a highly optimistic estimation of performance for the text-based system, suggesting that some previous works may overestimate the advantage of incorporating transcriptions.</span></p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">â€”â€‰<span id="p1.1.1.1.1" class="ltx_text ltx_font_medium">
speech emotion recognition, fusion, deep learning, BERT</span></span></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text" style="font-size:90%;">Speech emotion recognition (SER) is an active research area with important applications in the field of human-computer interaction. SER is a complex task even for humans </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p1.1.4" class="ltx_text" style="font-size:90%;">. In fact, in spite of recent advances enabled by deep learning models and the release of larger emotion datasets, the performance of SER systems is still relatively poor, with average recall rates usually well below 70% on the most realistic datasets, indicating that it remains an open problem.</span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p"><span id="S1.p2.1.1" class="ltx_text" style="font-size:90%;">Most SER systems use low-level descriptors (LLD) extracted from the audio signal such as MFCCs, pitch and voice quality features </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S1.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.4" class="ltx_text" style="font-size:90%;">, or features learned automatically from spectrograms using deep neural networks </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S1.p2.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.7" class="ltx_text" style="font-size:90%;">. The excellent performance of current automatic speech recognition systems (ASR) also allows us to extract reliable text transcriptions from the speech without the need for human annotators. A few works have incorporated this information into SER systems. In some of these studies, emotional word-based vectors were computed from word occurrences in each emotion class </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S1.p2.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.10" class="ltx_text" style="font-size:90%;">, or using external lexicons </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S1.p2.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.13" class="ltx_text" style="font-size:90%;">. Similarly, emotional vectors can be extracted from phonemes </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.14.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S1.p2.1.15.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.16" class="ltx_text" style="font-size:90%;">. In some works </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.17.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S1.p2.1.18.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.19" class="ltx_text" style="font-size:90%;">, the word-based vectors are used as input to SVM classifiers together with high-level statistics of the acoustic LLDs. Another approach is to train text- and audio-based classifiers separately and combine their outputs to make a final prediction </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.20.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S1.p2.1.21.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.22" class="ltx_text" style="font-size:90%;">. Recently, deep neural networks have been used to learn audio-linguistic embeddings </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.23.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S1.p2.1.24.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.25" class="ltx_text" style="font-size:90%;"> and to train emotion classifiers in an end-to-end framework combining text and audio modalities </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.26.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S1.p2.1.27.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.28" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p"><span id="S1.p3.1.1" class="ltx_text" style="font-size:90%;">In this paper, we study different ways of fusing audio and linguistic information, using early and late fusion techniques and comparing different training approaches, including (1) initializing two individual branches with models trained separately for audio and text and further fine-tuning the last few layers, (2) fixing the text and audio branches and training only the fusion parameters, and (3) training the whole combined neural network from scratch. For the audio branch, we use a standard approach based on MFCC, pitch, loudness, jitter, shimmer and logHNR features. For the text branch, we use contextualized word embeddings </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S1.p3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.4" class="ltx_text" style="font-size:90%;"> instead of the standard word embeddings like Glove </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S1.p3.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.7" class="ltx_text" style="font-size:90%;"> used in most of the previous works </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S1.p3.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.10" class="ltx_text" style="font-size:90%;">. Standard word embeddings like those obtained with Glove are extracted independently of the context in which the words are found. For example, the word â€œsadâ€ would be assigned the same embedding whether the phrase was â€œI am very sadâ€ or â€œI am not sad at allâ€. On the other hand, contextualized word embeddings like those extracted by BERT take into account the whole phrase in which the word is found. As a consequence, the embedding corresponding to the word â€œsadâ€ in those two phrases would most likely be different. We hypothesized that this characteristic should positively impact SER performance. To our knowledge, </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S1.p3.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.13" class="ltx_text" style="font-size:90%;"> is the only work in which word embeddings extracted with BERT have been used for SER. In that paper, authors propose a shared representation of audio, text and video modalities through deep canonical correlation analysis. A comparison with other types of embeddings is not shown in that work.</span></p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><span id="S1.p4.1.1" class="ltx_text" style="font-size:90%;">The proposed models are tested on the well-studied IEMOCAP dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S1.p4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p4.1.4" class="ltx_text" style="font-size:90%;">, as well as on the more challenging MSP-PODCAST dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p4.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S1.p4.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p4.1.7" class="ltx_text" style="font-size:90%;">. Our first contribution is to show that linguistic information gives significant improvements in performance when combined with acoustic information on the MSP-PODCAST dataset. As far as we know, this is the first time that linguistic information has been used on this dataset. Second, we show that the use of contextualized word embeddings obtained with BERT results in significant improvements with respect to using standard word embeddings obtained with Glove. Third, we propose a novel way to fuse audio and text information by pretraining the neural network in audio and text modalities and then fine-tuning the fused model. Finally, we show that creating folds by speaker is not sufficient to obtain fair performance predictions on IEMOCAP, since the data contains scripted dialogues which greatly affect the performance of text-based systems when the same script is observed in training and testing. This being such a widely used dataset, we believe this observation is of great importance to the research community.</span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">2 </span>Models</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text" style="font-size:90%;">This section describes the models used in the experiments. First, the individual models for each modality are introduced. Then, models that combine the text and audio information are described. All models are trained to optimize cross-entropy loss for four emotion classes: happy, sad, angry and neutral.</span></p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Text-based model</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p"><span id="S2.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">Recently, a language model called BERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S2.SS1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS1.p1.1.4" class="ltx_text" style="font-size:90%;">, trained with large amounts of data has been released to the community. This model can be fine-tuned or used as a feature extractor for downstream tasks, achieving state-of-the-art results on many of them. BERT is based on the Transformer </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS1.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S2.SS1.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS1.p1.1.7" class="ltx_text" style="font-size:90%;"> â€“ a network capable of modeling long contextual information, generating word embeddings that are conditioned on the phrase in which the word is found.
In this study, a sequence of word embeddings is extracted from speech transcriptions using the pretrained BERT base uncased model, which consists of 12 layers, 12 attention heads and 110M parameters. The word embeddings are formed by adding the activations of the last 4 layers of the pretrained BERT model without fine-tuning. The resulting features are used as input to the text model shown on the left of FigureÂ </span><a href="#S2.F1" title="Figure 1 â€£ 2.2 Audio-based model â€£ 2 Models â€£ Fusion approaches for emotion recognition from speech using acoustic and text-based features" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S2.SS1.p1.1.8" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.4" class="ltx_p"><span id="S2.SS1.p2.4.1" class="ltx_text" style="font-size:90%;">The first layer (</span><math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="L_{T1}" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><msub id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mi mathsize="90%" id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">L</mi><mrow id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml"><mi mathsize="90%" id="S2.SS1.p2.1.m1.1.1.3.2" xref="S2.SS1.p2.1.m1.1.1.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.1.m1.1.1.3.1" xref="S2.SS1.p2.1.m1.1.1.3.1.cmml">â€‹</mo><mn mathsize="90%" id="S2.SS1.p2.1.m1.1.1.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">ğ¿</ci><apply id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3"><times id="S2.SS1.p2.1.m1.1.1.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3.1"></times><ci id="S2.SS1.p2.1.m1.1.1.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.2">ğ‘‡</ci><cn type="integer" id="S2.SS1.p2.1.m1.1.1.3.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">L_{T1}</annotation></semantics></math><span id="S2.SS1.p2.4.2" class="ltx_text" style="font-size:90%;">) in our text-based model operates on each embedding in the sequence reducing its dimensionality from 768 to 128. Then, 2 convolutional layers (</span><math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="L_{T2}" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><msub id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml"><mi mathsize="90%" id="S2.SS1.p2.2.m2.1.1.2" xref="S2.SS1.p2.2.m2.1.1.2.cmml">L</mi><mrow id="S2.SS1.p2.2.m2.1.1.3" xref="S2.SS1.p2.2.m2.1.1.3.cmml"><mi mathsize="90%" id="S2.SS1.p2.2.m2.1.1.3.2" xref="S2.SS1.p2.2.m2.1.1.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.2.m2.1.1.3.1" xref="S2.SS1.p2.2.m2.1.1.3.1.cmml">â€‹</mo><mn mathsize="90%" id="S2.SS1.p2.2.m2.1.1.3.3" xref="S2.SS1.p2.2.m2.1.1.3.3.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p2.2.m2.1.1.2.cmml" xref="S2.SS1.p2.2.m2.1.1.2">ğ¿</ci><apply id="S2.SS1.p2.2.m2.1.1.3.cmml" xref="S2.SS1.p2.2.m2.1.1.3"><times id="S2.SS1.p2.2.m2.1.1.3.1.cmml" xref="S2.SS1.p2.2.m2.1.1.3.1"></times><ci id="S2.SS1.p2.2.m2.1.1.3.2.cmml" xref="S2.SS1.p2.2.m2.1.1.3.2">ğ‘‡</ci><cn type="integer" id="S2.SS1.p2.2.m2.1.1.3.3.cmml" xref="S2.SS1.p2.2.m2.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">L_{T2}</annotation></semantics></math><span id="S2.SS1.p2.4.3" class="ltx_text" style="font-size:90%;"> and </span><math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="L_{T3}" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><msub id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml"><mi mathsize="90%" id="S2.SS1.p2.3.m3.1.1.2" xref="S2.SS1.p2.3.m3.1.1.2.cmml">L</mi><mrow id="S2.SS1.p2.3.m3.1.1.3" xref="S2.SS1.p2.3.m3.1.1.3.cmml"><mi mathsize="90%" id="S2.SS1.p2.3.m3.1.1.3.2" xref="S2.SS1.p2.3.m3.1.1.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.3.m3.1.1.3.1" xref="S2.SS1.p2.3.m3.1.1.3.1.cmml">â€‹</mo><mn mathsize="90%" id="S2.SS1.p2.3.m3.1.1.3.3" xref="S2.SS1.p2.3.m3.1.1.3.3.cmml">3</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><apply id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.3.m3.1.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.p2.3.m3.1.1.2.cmml" xref="S2.SS1.p2.3.m3.1.1.2">ğ¿</ci><apply id="S2.SS1.p2.3.m3.1.1.3.cmml" xref="S2.SS1.p2.3.m3.1.1.3"><times id="S2.SS1.p2.3.m3.1.1.3.1.cmml" xref="S2.SS1.p2.3.m3.1.1.3.1"></times><ci id="S2.SS1.p2.3.m3.1.1.3.2.cmml" xref="S2.SS1.p2.3.m3.1.1.3.2">ğ‘‡</ci><cn type="integer" id="S2.SS1.p2.3.m3.1.1.3.3.cmml" xref="S2.SS1.p2.3.m3.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">L_{T3}</annotation></semantics></math><span id="S2.SS1.p2.4.4" class="ltx_text" style="font-size:90%;">) model relationships across neighboring elements of the sequence. Finally, an average over time is taken, resulting in an embedding that summarizes all the information in the sample. A final dense layer with softmax activation predicts emotion probabilities </span><math id="S2.SS1.p2.4.m4.1" class="ltx_Math" alttext="P(C_{k})" display="inline"><semantics id="S2.SS1.p2.4.m4.1a"><mrow id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml"><mi mathsize="90%" id="S2.SS1.p2.4.m4.1.1.3" xref="S2.SS1.p2.4.m4.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.4.m4.1.1.2" xref="S2.SS1.p2.4.m4.1.1.2.cmml">â€‹</mo><mrow id="S2.SS1.p2.4.m4.1.1.1.1" xref="S2.SS1.p2.4.m4.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S2.SS1.p2.4.m4.1.1.1.1.2" xref="S2.SS1.p2.4.m4.1.1.1.1.1.cmml">(</mo><msub id="S2.SS1.p2.4.m4.1.1.1.1.1" xref="S2.SS1.p2.4.m4.1.1.1.1.1.cmml"><mi mathsize="90%" id="S2.SS1.p2.4.m4.1.1.1.1.1.2" xref="S2.SS1.p2.4.m4.1.1.1.1.1.2.cmml">C</mi><mi mathsize="90%" id="S2.SS1.p2.4.m4.1.1.1.1.1.3" xref="S2.SS1.p2.4.m4.1.1.1.1.1.3.cmml">k</mi></msub><mo maxsize="90%" minsize="90%" id="S2.SS1.p2.4.m4.1.1.1.1.3" xref="S2.SS1.p2.4.m4.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><apply id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1"><times id="S2.SS1.p2.4.m4.1.1.2.cmml" xref="S2.SS1.p2.4.m4.1.1.2"></times><ci id="S2.SS1.p2.4.m4.1.1.3.cmml" xref="S2.SS1.p2.4.m4.1.1.3">ğ‘ƒ</ci><apply id="S2.SS1.p2.4.m4.1.1.1.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.4.m4.1.1.1.1.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p2.4.m4.1.1.1.1.1.2.cmml" xref="S2.SS1.p2.4.m4.1.1.1.1.1.2">ğ¶</ci><ci id="S2.SS1.p2.4.m4.1.1.1.1.1.3.cmml" xref="S2.SS1.p2.4.m4.1.1.1.1.1.3">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">P(C_{k})</annotation></semantics></math><span id="S2.SS1.p2.4.5" class="ltx_text" style="font-size:90%;">. We applied batch normalization in all layers.</span></p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p"><span id="S2.SS1.p3.1.1" class="ltx_text" style="font-size:90%;">To make a comparison with non-contextualized word embeddings, we trained the same model using 300-dimensional Glove embeddings </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS1.p3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S2.SS1.p3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS1.p3.1.4" class="ltx_text" style="font-size:90%;">.</span><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The pretrained Glove model we used can be downloaded from http://nlp.stanford.edu/data/glove.42B.300d.zip.</span></span></span><span id="S2.SS1.p3.1.5" class="ltx_text" style="font-size:90%;"></span></p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Audio-based model</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p"><span id="S2.SS2.p1.1.1" class="ltx_text" style="font-size:90%;">Each speaker utterance was divided into 32ms segments, using a hop length of 10ms. The following acoustic features were extracted from each window using openSMILE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS2.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S2.SS2.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS2.p1.1.4" class="ltx_text" style="font-size:90%;">: pitch, jitter, shimmer, logHNR, loudness, and the first 13 MFCCs. These features were normalized to have a mean of 0 and standard deviation of 1, using the global statistics. Finally, first-order differences were added for all features to form a sequence of 36-dimensional feature vectors that are the input to the neural network shown on the right of FigureÂ </span><a href="#S2.F1" title="Figure 1 â€£ 2.2 Audio-based model â€£ 2 Models â€£ Fusion approaches for emotion recognition from speech using acoustic and text-based features" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S2.SS2.p1.1.5" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.3" class="ltx_p"><span id="S2.SS2.p2.3.1" class="ltx_text" style="font-size:90%;">The audio model consists of two convolutional layers </span><math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="L_{A1}" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><msub id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml"><mi mathsize="90%" id="S2.SS2.p2.1.m1.1.1.2" xref="S2.SS2.p2.1.m1.1.1.2.cmml">L</mi><mrow id="S2.SS2.p2.1.m1.1.1.3" xref="S2.SS2.p2.1.m1.1.1.3.cmml"><mi mathsize="90%" id="S2.SS2.p2.1.m1.1.1.3.2" xref="S2.SS2.p2.1.m1.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p2.1.m1.1.1.3.1" xref="S2.SS2.p2.1.m1.1.1.3.1.cmml">â€‹</mo><mn mathsize="90%" id="S2.SS2.p2.1.m1.1.1.3.3" xref="S2.SS2.p2.1.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><apply id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.1.m1.1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p2.1.m1.1.1.2.cmml" xref="S2.SS2.p2.1.m1.1.1.2">ğ¿</ci><apply id="S2.SS2.p2.1.m1.1.1.3.cmml" xref="S2.SS2.p2.1.m1.1.1.3"><times id="S2.SS2.p2.1.m1.1.1.3.1.cmml" xref="S2.SS2.p2.1.m1.1.1.3.1"></times><ci id="S2.SS2.p2.1.m1.1.1.3.2.cmml" xref="S2.SS2.p2.1.m1.1.1.3.2">ğ´</ci><cn type="integer" id="S2.SS2.p2.1.m1.1.1.3.3.cmml" xref="S2.SS2.p2.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">L_{A1}</annotation></semantics></math><span id="S2.SS2.p2.3.2" class="ltx_text" style="font-size:90%;"> and </span><math id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="L_{A2}" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><msub id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml"><mi mathsize="90%" id="S2.SS2.p2.2.m2.1.1.2" xref="S2.SS2.p2.2.m2.1.1.2.cmml">L</mi><mrow id="S2.SS2.p2.2.m2.1.1.3" xref="S2.SS2.p2.2.m2.1.1.3.cmml"><mi mathsize="90%" id="S2.SS2.p2.2.m2.1.1.3.2" xref="S2.SS2.p2.2.m2.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p2.2.m2.1.1.3.1" xref="S2.SS2.p2.2.m2.1.1.3.1.cmml">â€‹</mo><mn mathsize="90%" id="S2.SS2.p2.2.m2.1.1.3.3" xref="S2.SS2.p2.2.m2.1.1.3.3.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><apply id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.2.m2.1.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p2.2.m2.1.1.2.cmml" xref="S2.SS2.p2.2.m2.1.1.2">ğ¿</ci><apply id="S2.SS2.p2.2.m2.1.1.3.cmml" xref="S2.SS2.p2.2.m2.1.1.3"><times id="S2.SS2.p2.2.m2.1.1.3.1.cmml" xref="S2.SS2.p2.2.m2.1.1.3.1"></times><ci id="S2.SS2.p2.2.m2.1.1.3.2.cmml" xref="S2.SS2.p2.2.m2.1.1.3.2">ğ´</ci><cn type="integer" id="S2.SS2.p2.2.m2.1.1.3.3.cmml" xref="S2.SS2.p2.2.m2.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">L_{A2}</annotation></semantics></math><span id="S2.SS2.p2.3.3" class="ltx_text" style="font-size:90%;"> that model the temporal evolution of the input sequence followed by mean-pooling over time. A final dense layer with softmax activation returns the emotion probabilities </span><math id="S2.SS2.p2.3.m3.1" class="ltx_Math" alttext="P(C_{k})" display="inline"><semantics id="S2.SS2.p2.3.m3.1a"><mrow id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml"><mi mathsize="90%" id="S2.SS2.p2.3.m3.1.1.3" xref="S2.SS2.p2.3.m3.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p2.3.m3.1.1.2" xref="S2.SS2.p2.3.m3.1.1.2.cmml">â€‹</mo><mrow id="S2.SS2.p2.3.m3.1.1.1.1" xref="S2.SS2.p2.3.m3.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S2.SS2.p2.3.m3.1.1.1.1.2" xref="S2.SS2.p2.3.m3.1.1.1.1.1.cmml">(</mo><msub id="S2.SS2.p2.3.m3.1.1.1.1.1" xref="S2.SS2.p2.3.m3.1.1.1.1.1.cmml"><mi mathsize="90%" id="S2.SS2.p2.3.m3.1.1.1.1.1.2" xref="S2.SS2.p2.3.m3.1.1.1.1.1.2.cmml">C</mi><mi mathsize="90%" id="S2.SS2.p2.3.m3.1.1.1.1.1.3" xref="S2.SS2.p2.3.m3.1.1.1.1.1.3.cmml">k</mi></msub><mo maxsize="90%" minsize="90%" id="S2.SS2.p2.3.m3.1.1.1.1.3" xref="S2.SS2.p2.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.1b"><apply id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1"><times id="S2.SS2.p2.3.m3.1.1.2.cmml" xref="S2.SS2.p2.3.m3.1.1.2"></times><ci id="S2.SS2.p2.3.m3.1.1.3.cmml" xref="S2.SS2.p2.3.m3.1.1.3">ğ‘ƒ</ci><apply id="S2.SS2.p2.3.m3.1.1.1.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.3.m3.1.1.1.1.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1.1.1">subscript</csymbol><ci id="S2.SS2.p2.3.m3.1.1.1.1.1.2.cmml" xref="S2.SS2.p2.3.m3.1.1.1.1.1.2">ğ¶</ci><ci id="S2.SS2.p2.3.m3.1.1.1.1.1.3.cmml" xref="S2.SS2.p2.3.m3.1.1.1.1.1.3">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.1c">P(C_{k})</annotation></semantics></math><span id="S2.SS2.p2.3.4" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2403.18635/assets/figures/branches-4.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="535" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.25.1.1" class="ltx_text ltx_font_bold">Fig.Â 1</span>: </span>Text-based and audio-based architectures. <math id="S2.F1.8.m1.1" class="ltx_Math" alttext="T_{text}" display="inline"><semantics id="S2.F1.8.m1.1b"><msub id="S2.F1.8.m1.1.1" xref="S2.F1.8.m1.1.1.cmml"><mi id="S2.F1.8.m1.1.1.2" xref="S2.F1.8.m1.1.1.2.cmml">T</mi><mrow id="S2.F1.8.m1.1.1.3" xref="S2.F1.8.m1.1.1.3.cmml"><mi id="S2.F1.8.m1.1.1.3.2" xref="S2.F1.8.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.F1.8.m1.1.1.3.1" xref="S2.F1.8.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.F1.8.m1.1.1.3.3" xref="S2.F1.8.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.F1.8.m1.1.1.3.1b" xref="S2.F1.8.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.F1.8.m1.1.1.3.4" xref="S2.F1.8.m1.1.1.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.F1.8.m1.1.1.3.1c" xref="S2.F1.8.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.F1.8.m1.1.1.3.5" xref="S2.F1.8.m1.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.F1.8.m1.1c"><apply id="S2.F1.8.m1.1.1.cmml" xref="S2.F1.8.m1.1.1"><csymbol cd="ambiguous" id="S2.F1.8.m1.1.1.1.cmml" xref="S2.F1.8.m1.1.1">subscript</csymbol><ci id="S2.F1.8.m1.1.1.2.cmml" xref="S2.F1.8.m1.1.1.2">ğ‘‡</ci><apply id="S2.F1.8.m1.1.1.3.cmml" xref="S2.F1.8.m1.1.1.3"><times id="S2.F1.8.m1.1.1.3.1.cmml" xref="S2.F1.8.m1.1.1.3.1"></times><ci id="S2.F1.8.m1.1.1.3.2.cmml" xref="S2.F1.8.m1.1.1.3.2">ğ‘¡</ci><ci id="S2.F1.8.m1.1.1.3.3.cmml" xref="S2.F1.8.m1.1.1.3.3">ğ‘’</ci><ci id="S2.F1.8.m1.1.1.3.4.cmml" xref="S2.F1.8.m1.1.1.3.4">ğ‘¥</ci><ci id="S2.F1.8.m1.1.1.3.5.cmml" xref="S2.F1.8.m1.1.1.3.5">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.8.m1.1d">T_{text}</annotation></semantics></math> and <math id="S2.F1.9.m2.1" class="ltx_Math" alttext="T_{audio}" display="inline"><semantics id="S2.F1.9.m2.1b"><msub id="S2.F1.9.m2.1.1" xref="S2.F1.9.m2.1.1.cmml"><mi id="S2.F1.9.m2.1.1.2" xref="S2.F1.9.m2.1.1.2.cmml">T</mi><mrow id="S2.F1.9.m2.1.1.3" xref="S2.F1.9.m2.1.1.3.cmml"><mi id="S2.F1.9.m2.1.1.3.2" xref="S2.F1.9.m2.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.F1.9.m2.1.1.3.1" xref="S2.F1.9.m2.1.1.3.1.cmml">â€‹</mo><mi id="S2.F1.9.m2.1.1.3.3" xref="S2.F1.9.m2.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.F1.9.m2.1.1.3.1b" xref="S2.F1.9.m2.1.1.3.1.cmml">â€‹</mo><mi id="S2.F1.9.m2.1.1.3.4" xref="S2.F1.9.m2.1.1.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S2.F1.9.m2.1.1.3.1c" xref="S2.F1.9.m2.1.1.3.1.cmml">â€‹</mo><mi id="S2.F1.9.m2.1.1.3.5" xref="S2.F1.9.m2.1.1.3.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.F1.9.m2.1.1.3.1d" xref="S2.F1.9.m2.1.1.3.1.cmml">â€‹</mo><mi id="S2.F1.9.m2.1.1.3.6" xref="S2.F1.9.m2.1.1.3.6.cmml">o</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.F1.9.m2.1c"><apply id="S2.F1.9.m2.1.1.cmml" xref="S2.F1.9.m2.1.1"><csymbol cd="ambiguous" id="S2.F1.9.m2.1.1.1.cmml" xref="S2.F1.9.m2.1.1">subscript</csymbol><ci id="S2.F1.9.m2.1.1.2.cmml" xref="S2.F1.9.m2.1.1.2">ğ‘‡</ci><apply id="S2.F1.9.m2.1.1.3.cmml" xref="S2.F1.9.m2.1.1.3"><times id="S2.F1.9.m2.1.1.3.1.cmml" xref="S2.F1.9.m2.1.1.3.1"></times><ci id="S2.F1.9.m2.1.1.3.2.cmml" xref="S2.F1.9.m2.1.1.3.2">ğ‘</ci><ci id="S2.F1.9.m2.1.1.3.3.cmml" xref="S2.F1.9.m2.1.1.3.3">ğ‘¢</ci><ci id="S2.F1.9.m2.1.1.3.4.cmml" xref="S2.F1.9.m2.1.1.3.4">ğ‘‘</ci><ci id="S2.F1.9.m2.1.1.3.5.cmml" xref="S2.F1.9.m2.1.1.3.5">ğ‘–</ci><ci id="S2.F1.9.m2.1.1.3.6.cmml" xref="S2.F1.9.m2.1.1.3.6">ğ‘œ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.9.m2.1d">T_{audio}</annotation></semantics></math> are the sequence lengths of the model inputs and <math id="S2.F1.10.m3.1" class="ltx_Math" alttext="D_{text}" display="inline"><semantics id="S2.F1.10.m3.1b"><msub id="S2.F1.10.m3.1.1" xref="S2.F1.10.m3.1.1.cmml"><mi id="S2.F1.10.m3.1.1.2" xref="S2.F1.10.m3.1.1.2.cmml">D</mi><mrow id="S2.F1.10.m3.1.1.3" xref="S2.F1.10.m3.1.1.3.cmml"><mi id="S2.F1.10.m3.1.1.3.2" xref="S2.F1.10.m3.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.F1.10.m3.1.1.3.1" xref="S2.F1.10.m3.1.1.3.1.cmml">â€‹</mo><mi id="S2.F1.10.m3.1.1.3.3" xref="S2.F1.10.m3.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.F1.10.m3.1.1.3.1b" xref="S2.F1.10.m3.1.1.3.1.cmml">â€‹</mo><mi id="S2.F1.10.m3.1.1.3.4" xref="S2.F1.10.m3.1.1.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.F1.10.m3.1.1.3.1c" xref="S2.F1.10.m3.1.1.3.1.cmml">â€‹</mo><mi id="S2.F1.10.m3.1.1.3.5" xref="S2.F1.10.m3.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.F1.10.m3.1c"><apply id="S2.F1.10.m3.1.1.cmml" xref="S2.F1.10.m3.1.1"><csymbol cd="ambiguous" id="S2.F1.10.m3.1.1.1.cmml" xref="S2.F1.10.m3.1.1">subscript</csymbol><ci id="S2.F1.10.m3.1.1.2.cmml" xref="S2.F1.10.m3.1.1.2">ğ·</ci><apply id="S2.F1.10.m3.1.1.3.cmml" xref="S2.F1.10.m3.1.1.3"><times id="S2.F1.10.m3.1.1.3.1.cmml" xref="S2.F1.10.m3.1.1.3.1"></times><ci id="S2.F1.10.m3.1.1.3.2.cmml" xref="S2.F1.10.m3.1.1.3.2">ğ‘¡</ci><ci id="S2.F1.10.m3.1.1.3.3.cmml" xref="S2.F1.10.m3.1.1.3.3">ğ‘’</ci><ci id="S2.F1.10.m3.1.1.3.4.cmml" xref="S2.F1.10.m3.1.1.3.4">ğ‘¥</ci><ci id="S2.F1.10.m3.1.1.3.5.cmml" xref="S2.F1.10.m3.1.1.3.5">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.10.m3.1d">D_{text}</annotation></semantics></math> and <math id="S2.F1.11.m4.1" class="ltx_Math" alttext="D_{audio}" display="inline"><semantics id="S2.F1.11.m4.1b"><msub id="S2.F1.11.m4.1.1" xref="S2.F1.11.m4.1.1.cmml"><mi id="S2.F1.11.m4.1.1.2" xref="S2.F1.11.m4.1.1.2.cmml">D</mi><mrow id="S2.F1.11.m4.1.1.3" xref="S2.F1.11.m4.1.1.3.cmml"><mi id="S2.F1.11.m4.1.1.3.2" xref="S2.F1.11.m4.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.F1.11.m4.1.1.3.1" xref="S2.F1.11.m4.1.1.3.1.cmml">â€‹</mo><mi id="S2.F1.11.m4.1.1.3.3" xref="S2.F1.11.m4.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.F1.11.m4.1.1.3.1b" xref="S2.F1.11.m4.1.1.3.1.cmml">â€‹</mo><mi id="S2.F1.11.m4.1.1.3.4" xref="S2.F1.11.m4.1.1.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S2.F1.11.m4.1.1.3.1c" xref="S2.F1.11.m4.1.1.3.1.cmml">â€‹</mo><mi id="S2.F1.11.m4.1.1.3.5" xref="S2.F1.11.m4.1.1.3.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.F1.11.m4.1.1.3.1d" xref="S2.F1.11.m4.1.1.3.1.cmml">â€‹</mo><mi id="S2.F1.11.m4.1.1.3.6" xref="S2.F1.11.m4.1.1.3.6.cmml">o</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.F1.11.m4.1c"><apply id="S2.F1.11.m4.1.1.cmml" xref="S2.F1.11.m4.1.1"><csymbol cd="ambiguous" id="S2.F1.11.m4.1.1.1.cmml" xref="S2.F1.11.m4.1.1">subscript</csymbol><ci id="S2.F1.11.m4.1.1.2.cmml" xref="S2.F1.11.m4.1.1.2">ğ·</ci><apply id="S2.F1.11.m4.1.1.3.cmml" xref="S2.F1.11.m4.1.1.3"><times id="S2.F1.11.m4.1.1.3.1.cmml" xref="S2.F1.11.m4.1.1.3.1"></times><ci id="S2.F1.11.m4.1.1.3.2.cmml" xref="S2.F1.11.m4.1.1.3.2">ğ‘</ci><ci id="S2.F1.11.m4.1.1.3.3.cmml" xref="S2.F1.11.m4.1.1.3.3">ğ‘¢</ci><ci id="S2.F1.11.m4.1.1.3.4.cmml" xref="S2.F1.11.m4.1.1.3.4">ğ‘‘</ci><ci id="S2.F1.11.m4.1.1.3.5.cmml" xref="S2.F1.11.m4.1.1.3.5">ğ‘–</ci><ci id="S2.F1.11.m4.1.1.3.6.cmml" xref="S2.F1.11.m4.1.1.3.6">ğ‘œ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.11.m4.1d">D_{audio}</annotation></semantics></math> are the number of features for each input. <math id="S2.F1.12.m5.1" class="ltx_Math" alttext="N_{F}" display="inline"><semantics id="S2.F1.12.m5.1b"><msub id="S2.F1.12.m5.1.1" xref="S2.F1.12.m5.1.1.cmml"><mi id="S2.F1.12.m5.1.1.2" xref="S2.F1.12.m5.1.1.2.cmml">N</mi><mi id="S2.F1.12.m5.1.1.3" xref="S2.F1.12.m5.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F1.12.m5.1c"><apply id="S2.F1.12.m5.1.1.cmml" xref="S2.F1.12.m5.1.1"><csymbol cd="ambiguous" id="S2.F1.12.m5.1.1.1.cmml" xref="S2.F1.12.m5.1.1">subscript</csymbol><ci id="S2.F1.12.m5.1.1.2.cmml" xref="S2.F1.12.m5.1.1.2">ğ‘</ci><ci id="S2.F1.12.m5.1.1.3.cmml" xref="S2.F1.12.m5.1.1.3">ğ¹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.12.m5.1d">N_{F}</annotation></semantics></math> is the number of convolutional filters, <math id="S2.F1.13.m6.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.F1.13.m6.1b"><mi id="S2.F1.13.m6.1.1" xref="S2.F1.13.m6.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.F1.13.m6.1c"><ci id="S2.F1.13.m6.1.1.cmml" xref="S2.F1.13.m6.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.13.m6.1d">S</annotation></semantics></math> is the kernel size and <math id="S2.F1.14.m7.1" class="ltx_Math" alttext="N_{U}" display="inline"><semantics id="S2.F1.14.m7.1b"><msub id="S2.F1.14.m7.1.1" xref="S2.F1.14.m7.1.1.cmml"><mi id="S2.F1.14.m7.1.1.2" xref="S2.F1.14.m7.1.1.2.cmml">N</mi><mi id="S2.F1.14.m7.1.1.3" xref="S2.F1.14.m7.1.1.3.cmml">U</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F1.14.m7.1c"><apply id="S2.F1.14.m7.1.1.cmml" xref="S2.F1.14.m7.1.1"><csymbol cd="ambiguous" id="S2.F1.14.m7.1.1.1.cmml" xref="S2.F1.14.m7.1.1">subscript</csymbol><ci id="S2.F1.14.m7.1.1.2.cmml" xref="S2.F1.14.m7.1.1.2">ğ‘</ci><ci id="S2.F1.14.m7.1.1.3.cmml" xref="S2.F1.14.m7.1.1.3">ğ‘ˆ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.14.m7.1d">N_{U}</annotation></semantics></math> is the number of neurons in dense layers. 1D-Convolutional layers operate on the time axis.</figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Fusion models</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p"><span id="S2.SS3.p1.1.1" class="ltx_text" style="font-size:90%;">In this section, we describe the strategies we implemented to combine audio and text information. In all cases, the fusion model consists of two parallel branches processing audio and text separately up to a layer where the information from the two branches is merged. The models differ on the location of the merging layer, on the network appended after merging, and on the training approach.</span></p>
</div>
<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Early Fusion</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para">
<p id="S2.SS3.SSS1.p1.2" class="ltx_p"><span id="S2.SS3.SSS1.p1.2.1" class="ltx_text" style="font-size:90%;">In the early fusion (EF) approach, the fixed-size embeddings obtained after mean pooling in the audio and text models (FigureÂ </span><a href="#S2.F1" title="Figure 1 â€£ 2.2 Audio-based model â€£ 2 Models â€£ Fusion approaches for emotion recognition from speech using acoustic and text-based features" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S2.SS3.SSS1.p1.2.2" class="ltx_text" style="font-size:90%;">, layers </span><math id="S2.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="L_{T4}" display="inline"><semantics id="S2.SS3.SSS1.p1.1.m1.1a"><msub id="S2.SS3.SSS1.p1.1.m1.1.1" xref="S2.SS3.SSS1.p1.1.m1.1.1.cmml"><mi mathsize="90%" id="S2.SS3.SSS1.p1.1.m1.1.1.2" xref="S2.SS3.SSS1.p1.1.m1.1.1.2.cmml">L</mi><mrow id="S2.SS3.SSS1.p1.1.m1.1.1.3" xref="S2.SS3.SSS1.p1.1.m1.1.1.3.cmml"><mi mathsize="90%" id="S2.SS3.SSS1.p1.1.m1.1.1.3.2" xref="S2.SS3.SSS1.p1.1.m1.1.1.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.SS3.SSS1.p1.1.m1.1.1.3.1" xref="S2.SS3.SSS1.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mn mathsize="90%" id="S2.SS3.SSS1.p1.1.m1.1.1.3.3" xref="S2.SS3.SSS1.p1.1.m1.1.1.3.3.cmml">4</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.1.m1.1b"><apply id="S2.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="S2.SS3.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.p1.1.m1.1.1.2.cmml" xref="S2.SS3.SSS1.p1.1.m1.1.1.2">ğ¿</ci><apply id="S2.SS3.SSS1.p1.1.m1.1.1.3.cmml" xref="S2.SS3.SSS1.p1.1.m1.1.1.3"><times id="S2.SS3.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S2.SS3.SSS1.p1.1.m1.1.1.3.1"></times><ci id="S2.SS3.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S2.SS3.SSS1.p1.1.m1.1.1.3.2">ğ‘‡</ci><cn type="integer" id="S2.SS3.SSS1.p1.1.m1.1.1.3.3.cmml" xref="S2.SS3.SSS1.p1.1.m1.1.1.3.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.1.m1.1c">L_{T4}</annotation></semantics></math><span id="S2.SS3.SSS1.p1.2.3" class="ltx_text" style="font-size:90%;"> and </span><math id="S2.SS3.SSS1.p1.2.m2.1" class="ltx_Math" alttext="L_{A3}" display="inline"><semantics id="S2.SS3.SSS1.p1.2.m2.1a"><msub id="S2.SS3.SSS1.p1.2.m2.1.1" xref="S2.SS3.SSS1.p1.2.m2.1.1.cmml"><mi mathsize="90%" id="S2.SS3.SSS1.p1.2.m2.1.1.2" xref="S2.SS3.SSS1.p1.2.m2.1.1.2.cmml">L</mi><mrow id="S2.SS3.SSS1.p1.2.m2.1.1.3" xref="S2.SS3.SSS1.p1.2.m2.1.1.3.cmml"><mi mathsize="90%" id="S2.SS3.SSS1.p1.2.m2.1.1.3.2" xref="S2.SS3.SSS1.p1.2.m2.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.SS3.SSS1.p1.2.m2.1.1.3.1" xref="S2.SS3.SSS1.p1.2.m2.1.1.3.1.cmml">â€‹</mo><mn mathsize="90%" id="S2.SS3.SSS1.p1.2.m2.1.1.3.3" xref="S2.SS3.SSS1.p1.2.m2.1.1.3.3.cmml">3</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.2.m2.1b"><apply id="S2.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS3.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS1.p1.2.m2.1.1.1.cmml" xref="S2.SS3.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS3.SSS1.p1.2.m2.1.1.2.cmml" xref="S2.SS3.SSS1.p1.2.m2.1.1.2">ğ¿</ci><apply id="S2.SS3.SSS1.p1.2.m2.1.1.3.cmml" xref="S2.SS3.SSS1.p1.2.m2.1.1.3"><times id="S2.SS3.SSS1.p1.2.m2.1.1.3.1.cmml" xref="S2.SS3.SSS1.p1.2.m2.1.1.3.1"></times><ci id="S2.SS3.SSS1.p1.2.m2.1.1.3.2.cmml" xref="S2.SS3.SSS1.p1.2.m2.1.1.3.2">ğ´</ci><cn type="integer" id="S2.SS3.SSS1.p1.2.m2.1.1.3.3.cmml" xref="S2.SS3.SSS1.p1.2.m2.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.2.m2.1c">L_{A3}</annotation></semantics></math><span id="S2.SS3.SSS1.p1.2.4" class="ltx_text" style="font-size:90%;">) are concatenated resulting in a multi-modal embedding of 232 dimensions. This embedding is input to a feed-forward neural network with a hidden dense layer of 128 units with ReLU activation and an output layer with 4 units and softmax activation.</span></p>
</div>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Late Fusion</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para">
<p id="S2.SS3.SSS2.p1.3" class="ltx_p"><span id="S2.SS3.SSS2.p1.3.1" class="ltx_text" style="font-size:90%;">In the late fusion model (LF), the logits (pre-softmax) of the audio and text models are concatenated (FigureÂ </span><a href="#S2.F1" title="Figure 1 â€£ 2.2 Audio-based model â€£ 2 Models â€£ Fusion approaches for emotion recognition from speech using acoustic and text-based features" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S2.SS3.SSS2.p1.3.2" class="ltx_text" style="font-size:90%;">, layers </span><math id="S2.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="L_{T5}" display="inline"><semantics id="S2.SS3.SSS2.p1.1.m1.1a"><msub id="S2.SS3.SSS2.p1.1.m1.1.1" xref="S2.SS3.SSS2.p1.1.m1.1.1.cmml"><mi mathsize="90%" id="S2.SS3.SSS2.p1.1.m1.1.1.2" xref="S2.SS3.SSS2.p1.1.m1.1.1.2.cmml">L</mi><mrow id="S2.SS3.SSS2.p1.1.m1.1.1.3" xref="S2.SS3.SSS2.p1.1.m1.1.1.3.cmml"><mi mathsize="90%" id="S2.SS3.SSS2.p1.1.m1.1.1.3.2" xref="S2.SS3.SSS2.p1.1.m1.1.1.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.SS3.SSS2.p1.1.m1.1.1.3.1" xref="S2.SS3.SSS2.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mn mathsize="90%" id="S2.SS3.SSS2.p1.1.m1.1.1.3.3" xref="S2.SS3.SSS2.p1.1.m1.1.1.3.3.cmml">5</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p1.1.m1.1b"><apply id="S2.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p1.1.m1.1.1.1.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.p1.1.m1.1.1.2.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1.2">ğ¿</ci><apply id="S2.SS3.SSS2.p1.1.m1.1.1.3.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1.3"><times id="S2.SS3.SSS2.p1.1.m1.1.1.3.1.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1.3.1"></times><ci id="S2.SS3.SSS2.p1.1.m1.1.1.3.2.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1.3.2">ğ‘‡</ci><cn type="integer" id="S2.SS3.SSS2.p1.1.m1.1.1.3.3.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1.3.3">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p1.1.m1.1c">L_{T5}</annotation></semantics></math><span id="S2.SS3.SSS2.p1.3.3" class="ltx_text" style="font-size:90%;"> and </span><math id="S2.SS3.SSS2.p1.2.m2.1" class="ltx_Math" alttext="L_{A4}" display="inline"><semantics id="S2.SS3.SSS2.p1.2.m2.1a"><msub id="S2.SS3.SSS2.p1.2.m2.1.1" xref="S2.SS3.SSS2.p1.2.m2.1.1.cmml"><mi mathsize="90%" id="S2.SS3.SSS2.p1.2.m2.1.1.2" xref="S2.SS3.SSS2.p1.2.m2.1.1.2.cmml">L</mi><mrow id="S2.SS3.SSS2.p1.2.m2.1.1.3" xref="S2.SS3.SSS2.p1.2.m2.1.1.3.cmml"><mi mathsize="90%" id="S2.SS3.SSS2.p1.2.m2.1.1.3.2" xref="S2.SS3.SSS2.p1.2.m2.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.SS3.SSS2.p1.2.m2.1.1.3.1" xref="S2.SS3.SSS2.p1.2.m2.1.1.3.1.cmml">â€‹</mo><mn mathsize="90%" id="S2.SS3.SSS2.p1.2.m2.1.1.3.3" xref="S2.SS3.SSS2.p1.2.m2.1.1.3.3.cmml">4</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p1.2.m2.1b"><apply id="S2.SS3.SSS2.p1.2.m2.1.1.cmml" xref="S2.SS3.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p1.2.m2.1.1.1.cmml" xref="S2.SS3.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.p1.2.m2.1.1.2.cmml" xref="S2.SS3.SSS2.p1.2.m2.1.1.2">ğ¿</ci><apply id="S2.SS3.SSS2.p1.2.m2.1.1.3.cmml" xref="S2.SS3.SSS2.p1.2.m2.1.1.3"><times id="S2.SS3.SSS2.p1.2.m2.1.1.3.1.cmml" xref="S2.SS3.SSS2.p1.2.m2.1.1.3.1"></times><ci id="S2.SS3.SSS2.p1.2.m2.1.1.3.2.cmml" xref="S2.SS3.SSS2.p1.2.m2.1.1.3.2">ğ´</ci><cn type="integer" id="S2.SS3.SSS2.p1.2.m2.1.1.3.3.cmml" xref="S2.SS3.SSS2.p1.2.m2.1.1.3.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p1.2.m2.1c">L_{A4}</annotation></semantics></math><span id="S2.SS3.SSS2.p1.3.4" class="ltx_text" style="font-size:90%;">) resulting in an 8-dimensional vector that is used as input to a dense layer of 4 units with softmax activation. This dense layer learns to combine the logits of audio and text modalities to generate the final output probabilities </span><math id="S2.SS3.SSS2.p1.3.m3.1" class="ltx_Math" alttext="P(C_{k})" display="inline"><semantics id="S2.SS3.SSS2.p1.3.m3.1a"><mrow id="S2.SS3.SSS2.p1.3.m3.1.1" xref="S2.SS3.SSS2.p1.3.m3.1.1.cmml"><mi mathsize="90%" id="S2.SS3.SSS2.p1.3.m3.1.1.3" xref="S2.SS3.SSS2.p1.3.m3.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.SS3.SSS2.p1.3.m3.1.1.2" xref="S2.SS3.SSS2.p1.3.m3.1.1.2.cmml">â€‹</mo><mrow id="S2.SS3.SSS2.p1.3.m3.1.1.1.1" xref="S2.SS3.SSS2.p1.3.m3.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S2.SS3.SSS2.p1.3.m3.1.1.1.1.2" xref="S2.SS3.SSS2.p1.3.m3.1.1.1.1.1.cmml">(</mo><msub id="S2.SS3.SSS2.p1.3.m3.1.1.1.1.1" xref="S2.SS3.SSS2.p1.3.m3.1.1.1.1.1.cmml"><mi mathsize="90%" id="S2.SS3.SSS2.p1.3.m3.1.1.1.1.1.2" xref="S2.SS3.SSS2.p1.3.m3.1.1.1.1.1.2.cmml">C</mi><mi mathsize="90%" id="S2.SS3.SSS2.p1.3.m3.1.1.1.1.1.3" xref="S2.SS3.SSS2.p1.3.m3.1.1.1.1.1.3.cmml">k</mi></msub><mo maxsize="90%" minsize="90%" id="S2.SS3.SSS2.p1.3.m3.1.1.1.1.3" xref="S2.SS3.SSS2.p1.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p1.3.m3.1b"><apply id="S2.SS3.SSS2.p1.3.m3.1.1.cmml" xref="S2.SS3.SSS2.p1.3.m3.1.1"><times id="S2.SS3.SSS2.p1.3.m3.1.1.2.cmml" xref="S2.SS3.SSS2.p1.3.m3.1.1.2"></times><ci id="S2.SS3.SSS2.p1.3.m3.1.1.3.cmml" xref="S2.SS3.SSS2.p1.3.m3.1.1.3">ğ‘ƒ</ci><apply id="S2.SS3.SSS2.p1.3.m3.1.1.1.1.1.cmml" xref="S2.SS3.SSS2.p1.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p1.3.m3.1.1.1.1.1.1.cmml" xref="S2.SS3.SSS2.p1.3.m3.1.1.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.p1.3.m3.1.1.1.1.1.2.cmml" xref="S2.SS3.SSS2.p1.3.m3.1.1.1.1.1.2">ğ¶</ci><ci id="S2.SS3.SSS2.p1.3.m3.1.1.1.1.1.3.cmml" xref="S2.SS3.SSS2.p1.3.m3.1.1.1.1.1.3">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p1.3.m3.1c">P(C_{k})</annotation></semantics></math><span id="S2.SS3.SSS2.p1.3.5" class="ltx_text" style="font-size:90%;">. We have also explored learning a scalar weight for each system instead of a full dense layer but the resulting performance was slightly worse.</span></p>
</div>
</section>
<section id="S2.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">2.3.3 </span>Training strategies</h4>

<div id="S2.SS3.SSS3.p1" class="ltx_para">
<p id="S2.SS3.SSS3.p1.1" class="ltx_p"><span id="S2.SS3.SSS3.p1.1.1" class="ltx_text" style="font-size:90%;">We trained our fusion models in 3 different ways:</span></p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text" style="font-size:90%;">Cold-start (CS): Use Xavier uniform initialization </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.I1.i1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S2.I1.i1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.I1.i1.p1.1.4" class="ltx_text" style="font-size:90%;"> for all layers of the fusion model and train the model jointly from scratch.</span></p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text" style="font-size:90%;">Pre-trained (PT): Train the audio and text models separately and use the trained weights to initialize the corresponding layers of the audio and text branches in the fusion model. The layers after merging are initialized with Xavier uniform initialization. Only these layers are trained, keeping the layers up to the merging point frozen.</span></p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.5" class="ltx_p"><span id="S2.I1.i3.p1.5.1" class="ltx_text" style="font-size:90%;">Warm-start (WS): Initialize all layers as in the PT approach but instead of training only the layers after merging, train also the layers right before pooling for each branch (</span><math id="S2.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="L_{T3}" display="inline"><semantics id="S2.I1.i3.p1.1.m1.1a"><msub id="S2.I1.i3.p1.1.m1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.cmml"><mi mathsize="90%" id="S2.I1.i3.p1.1.m1.1.1.2" xref="S2.I1.i3.p1.1.m1.1.1.2.cmml">L</mi><mrow id="S2.I1.i3.p1.1.m1.1.1.3" xref="S2.I1.i3.p1.1.m1.1.1.3.cmml"><mi mathsize="90%" id="S2.I1.i3.p1.1.m1.1.1.3.2" xref="S2.I1.i3.p1.1.m1.1.1.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.I1.i3.p1.1.m1.1.1.3.1" xref="S2.I1.i3.p1.1.m1.1.1.3.1.cmml">â€‹</mo><mn mathsize="90%" id="S2.I1.i3.p1.1.m1.1.1.3.3" xref="S2.I1.i3.p1.1.m1.1.1.3.3.cmml">3</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.1.m1.1b"><apply id="S2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i3.p1.1.m1.1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i3.p1.1.m1.1.1.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2">ğ¿</ci><apply id="S2.I1.i3.p1.1.m1.1.1.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3"><times id="S2.I1.i3.p1.1.m1.1.1.3.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.1"></times><ci id="S2.I1.i3.p1.1.m1.1.1.3.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.2">ğ‘‡</ci><cn type="integer" id="S2.I1.i3.p1.1.m1.1.1.3.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.1.m1.1c">L_{T3}</annotation></semantics></math><span id="S2.I1.i3.p1.5.2" class="ltx_text" style="font-size:90%;"> and </span><math id="S2.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="L_{A2}" display="inline"><semantics id="S2.I1.i3.p1.2.m2.1a"><msub id="S2.I1.i3.p1.2.m2.1.1" xref="S2.I1.i3.p1.2.m2.1.1.cmml"><mi mathsize="90%" id="S2.I1.i3.p1.2.m2.1.1.2" xref="S2.I1.i3.p1.2.m2.1.1.2.cmml">L</mi><mrow id="S2.I1.i3.p1.2.m2.1.1.3" xref="S2.I1.i3.p1.2.m2.1.1.3.cmml"><mi mathsize="90%" id="S2.I1.i3.p1.2.m2.1.1.3.2" xref="S2.I1.i3.p1.2.m2.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.I1.i3.p1.2.m2.1.1.3.1" xref="S2.I1.i3.p1.2.m2.1.1.3.1.cmml">â€‹</mo><mn mathsize="90%" id="S2.I1.i3.p1.2.m2.1.1.3.3" xref="S2.I1.i3.p1.2.m2.1.1.3.3.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.2.m2.1b"><apply id="S2.I1.i3.p1.2.m2.1.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.I1.i3.p1.2.m2.1.1.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1">subscript</csymbol><ci id="S2.I1.i3.p1.2.m2.1.1.2.cmml" xref="S2.I1.i3.p1.2.m2.1.1.2">ğ¿</ci><apply id="S2.I1.i3.p1.2.m2.1.1.3.cmml" xref="S2.I1.i3.p1.2.m2.1.1.3"><times id="S2.I1.i3.p1.2.m2.1.1.3.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1.3.1"></times><ci id="S2.I1.i3.p1.2.m2.1.1.3.2.cmml" xref="S2.I1.i3.p1.2.m2.1.1.3.2">ğ´</ci><cn type="integer" id="S2.I1.i3.p1.2.m2.1.1.3.3.cmml" xref="S2.I1.i3.p1.2.m2.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.2.m2.1c">L_{A2}</annotation></semantics></math><span id="S2.I1.i3.p1.5.3" class="ltx_text" style="font-size:90%;">), keeping the first layers (</span><math id="S2.I1.i3.p1.3.m3.1" class="ltx_Math" alttext="L_{T1}" display="inline"><semantics id="S2.I1.i3.p1.3.m3.1a"><msub id="S2.I1.i3.p1.3.m3.1.1" xref="S2.I1.i3.p1.3.m3.1.1.cmml"><mi mathsize="90%" id="S2.I1.i3.p1.3.m3.1.1.2" xref="S2.I1.i3.p1.3.m3.1.1.2.cmml">L</mi><mrow id="S2.I1.i3.p1.3.m3.1.1.3" xref="S2.I1.i3.p1.3.m3.1.1.3.cmml"><mi mathsize="90%" id="S2.I1.i3.p1.3.m3.1.1.3.2" xref="S2.I1.i3.p1.3.m3.1.1.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.I1.i3.p1.3.m3.1.1.3.1" xref="S2.I1.i3.p1.3.m3.1.1.3.1.cmml">â€‹</mo><mn mathsize="90%" id="S2.I1.i3.p1.3.m3.1.1.3.3" xref="S2.I1.i3.p1.3.m3.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.3.m3.1b"><apply id="S2.I1.i3.p1.3.m3.1.1.cmml" xref="S2.I1.i3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.I1.i3.p1.3.m3.1.1.1.cmml" xref="S2.I1.i3.p1.3.m3.1.1">subscript</csymbol><ci id="S2.I1.i3.p1.3.m3.1.1.2.cmml" xref="S2.I1.i3.p1.3.m3.1.1.2">ğ¿</ci><apply id="S2.I1.i3.p1.3.m3.1.1.3.cmml" xref="S2.I1.i3.p1.3.m3.1.1.3"><times id="S2.I1.i3.p1.3.m3.1.1.3.1.cmml" xref="S2.I1.i3.p1.3.m3.1.1.3.1"></times><ci id="S2.I1.i3.p1.3.m3.1.1.3.2.cmml" xref="S2.I1.i3.p1.3.m3.1.1.3.2">ğ‘‡</ci><cn type="integer" id="S2.I1.i3.p1.3.m3.1.1.3.3.cmml" xref="S2.I1.i3.p1.3.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.3.m3.1c">L_{T1}</annotation></semantics></math><span id="S2.I1.i3.p1.5.4" class="ltx_text" style="font-size:90%;">, </span><math id="S2.I1.i3.p1.4.m4.1" class="ltx_Math" alttext="L_{T2}" display="inline"><semantics id="S2.I1.i3.p1.4.m4.1a"><msub id="S2.I1.i3.p1.4.m4.1.1" xref="S2.I1.i3.p1.4.m4.1.1.cmml"><mi mathsize="90%" id="S2.I1.i3.p1.4.m4.1.1.2" xref="S2.I1.i3.p1.4.m4.1.1.2.cmml">L</mi><mrow id="S2.I1.i3.p1.4.m4.1.1.3" xref="S2.I1.i3.p1.4.m4.1.1.3.cmml"><mi mathsize="90%" id="S2.I1.i3.p1.4.m4.1.1.3.2" xref="S2.I1.i3.p1.4.m4.1.1.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.I1.i3.p1.4.m4.1.1.3.1" xref="S2.I1.i3.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mn mathsize="90%" id="S2.I1.i3.p1.4.m4.1.1.3.3" xref="S2.I1.i3.p1.4.m4.1.1.3.3.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.4.m4.1b"><apply id="S2.I1.i3.p1.4.m4.1.1.cmml" xref="S2.I1.i3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.I1.i3.p1.4.m4.1.1.1.cmml" xref="S2.I1.i3.p1.4.m4.1.1">subscript</csymbol><ci id="S2.I1.i3.p1.4.m4.1.1.2.cmml" xref="S2.I1.i3.p1.4.m4.1.1.2">ğ¿</ci><apply id="S2.I1.i3.p1.4.m4.1.1.3.cmml" xref="S2.I1.i3.p1.4.m4.1.1.3"><times id="S2.I1.i3.p1.4.m4.1.1.3.1.cmml" xref="S2.I1.i3.p1.4.m4.1.1.3.1"></times><ci id="S2.I1.i3.p1.4.m4.1.1.3.2.cmml" xref="S2.I1.i3.p1.4.m4.1.1.3.2">ğ‘‡</ci><cn type="integer" id="S2.I1.i3.p1.4.m4.1.1.3.3.cmml" xref="S2.I1.i3.p1.4.m4.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.4.m4.1c">L_{T2}</annotation></semantics></math><span id="S2.I1.i3.p1.5.5" class="ltx_text" style="font-size:90%;"> and </span><math id="S2.I1.i3.p1.5.m5.1" class="ltx_Math" alttext="L_{A1}" display="inline"><semantics id="S2.I1.i3.p1.5.m5.1a"><msub id="S2.I1.i3.p1.5.m5.1.1" xref="S2.I1.i3.p1.5.m5.1.1.cmml"><mi mathsize="90%" id="S2.I1.i3.p1.5.m5.1.1.2" xref="S2.I1.i3.p1.5.m5.1.1.2.cmml">L</mi><mrow id="S2.I1.i3.p1.5.m5.1.1.3" xref="S2.I1.i3.p1.5.m5.1.1.3.cmml"><mi mathsize="90%" id="S2.I1.i3.p1.5.m5.1.1.3.2" xref="S2.I1.i3.p1.5.m5.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.I1.i3.p1.5.m5.1.1.3.1" xref="S2.I1.i3.p1.5.m5.1.1.3.1.cmml">â€‹</mo><mn mathsize="90%" id="S2.I1.i3.p1.5.m5.1.1.3.3" xref="S2.I1.i3.p1.5.m5.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.5.m5.1b"><apply id="S2.I1.i3.p1.5.m5.1.1.cmml" xref="S2.I1.i3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.I1.i3.p1.5.m5.1.1.1.cmml" xref="S2.I1.i3.p1.5.m5.1.1">subscript</csymbol><ci id="S2.I1.i3.p1.5.m5.1.1.2.cmml" xref="S2.I1.i3.p1.5.m5.1.1.2">ğ¿</ci><apply id="S2.I1.i3.p1.5.m5.1.1.3.cmml" xref="S2.I1.i3.p1.5.m5.1.1.3"><times id="S2.I1.i3.p1.5.m5.1.1.3.1.cmml" xref="S2.I1.i3.p1.5.m5.1.1.3.1"></times><ci id="S2.I1.i3.p1.5.m5.1.1.3.2.cmml" xref="S2.I1.i3.p1.5.m5.1.1.3.2">ğ´</ci><cn type="integer" id="S2.I1.i3.p1.5.m5.1.1.3.3.cmml" xref="S2.I1.i3.p1.5.m5.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.5.m5.1c">L_{A1}</annotation></semantics></math><span id="S2.I1.i3.p1.5.6" class="ltx_text" style="font-size:90%;">) frozen, as in the PT approach. This procedure, in contrast to PT, allows the layers immediately before the pooling to change their weights.</span></p>
</div>
</li>
</ul>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental setup and Datasets</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p"><span id="S3.p1.1.1" class="ltx_text" style="font-size:90%;">Our experiments were performed on the IEMOCAP and MSP-PODCAST datasets.
The Interactive Emotional Dyadic Motion Capture
(IEMOCAP) dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S3.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p1.1.4" class="ltx_text" style="font-size:90%;"> has a length of approximately 12 hours and consists of scripted and improvised dialogues by 10 speakers. It is composed of 5 sessions, each including speech from an actor and an actress. Annotators were asked to label each sample choosing one or more labels from a pool of emotions. In this work, we used 4 emotional classes: anger, happiness, sadness and neutral, and following </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S3.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p1.1.7" class="ltx_text" style="font-size:90%;">, we relabeled excitement samples as happiness. Instances from other classes and with no annotator agreement were discarded.</span><span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Note that discarding no-agreement samples and samples from non-target emotions is not an ideal practice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. Here, we decided to do this since it is standard practice in SER literature, facilitating comparisons across papers.</span></span></span><span id="S3.p1.1.8" class="ltx_text" style="font-size:90%;"> For this dataset, human transcriptions are used for the text-based system.</span></p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text" style="font-size:90%;">To test our models we used 5-fold cross-validation, organizing the folds so that training and test sets do not share actors or scripts.
This last point is very important for the text-based model, as has been noted in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S3.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p2.1.4" class="ltx_text" style="font-size:90%;">, because dialogues from the same script are very similar. We show the effect of the criteria used for making the folds on both text and audio models in Section </span><a href="#S4.SS1" title="4.1 Effect of partition criteria for IEMOCAP folds â€£ 4 Results and discussion â€£ Fusion approaches for emotion recognition from speech using acoustic and text-based features" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4.1</span></a><span id="S3.p2.1.5" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text" style="font-size:90%;">The MSP-PODCAST dataset v1.4 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S3.p3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p3.1.4" class="ltx_text" style="font-size:90%;"> contains speech segments from podcast recordings, annotated using crowdsourcing. After discarding the instances not belonging to any of the 4 emotional classes under study, the training set contains 12078 speech segments from 601 speakers, while the test set contains 5557 utterances from 50 speakers not present in the training set. The training and test set definitions used in this paper are the ones provided with the dataset. The test set is gender balanced. Speech transcriptions were extracted using the Google Cloud Speech-to-Text API.</span><span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://cloud.google.com/speech-to-text/</span></span></span><span id="S3.p3.1.5" class="ltx_text" style="font-size:90%;"></span></p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text" style="font-size:90%;">To counteract the effect of class imbalance present in both datasets, a cost-sensitive training strategy was applied by multiplying the loss of each instance with the inverse of the frequency of the class it belongs to. The models were optimized using Adam </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S3.p4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p4.1.4" class="ltx_text" style="font-size:90%;"> with a learning rate of 0.0007, except for the fine-tuning case in which the learning rate was decreased to 0.0001, and the case of late fusion using pre-trained branches where learning rate was increased to 0.01. During the first 40 steps, the learning rate was linearly increased from 0 to the final value, except for the late fusion system with pretraining (LF-PT). We applied dropout with 0.5 probability at the input of layer </span><math id="S3.p4.1.m1.1" class="ltx_Math" alttext="L_{A2}" display="inline"><semantics id="S3.p4.1.m1.1a"><msub id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml"><mi mathsize="90%" id="S3.p4.1.m1.1.1.2" xref="S3.p4.1.m1.1.1.2.cmml">L</mi><mrow id="S3.p4.1.m1.1.1.3" xref="S3.p4.1.m1.1.1.3.cmml"><mi mathsize="90%" id="S3.p4.1.m1.1.1.3.2" xref="S3.p4.1.m1.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.p4.1.m1.1.1.3.1" xref="S3.p4.1.m1.1.1.3.1.cmml">â€‹</mo><mn mathsize="90%" id="S3.p4.1.m1.1.1.3.3" xref="S3.p4.1.m1.1.1.3.3.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><apply id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p4.1.m1.1.1.1.cmml" xref="S3.p4.1.m1.1.1">subscript</csymbol><ci id="S3.p4.1.m1.1.1.2.cmml" xref="S3.p4.1.m1.1.1.2">ğ¿</ci><apply id="S3.p4.1.m1.1.1.3.cmml" xref="S3.p4.1.m1.1.1.3"><times id="S3.p4.1.m1.1.1.3.1.cmml" xref="S3.p4.1.m1.1.1.3.1"></times><ci id="S3.p4.1.m1.1.1.3.2.cmml" xref="S3.p4.1.m1.1.1.3.2">ğ´</ci><cn type="integer" id="S3.p4.1.m1.1.1.3.3.cmml" xref="S3.p4.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">L_{A2}</annotation></semantics></math><span id="S3.p4.1.5" class="ltx_text" style="font-size:90%;"> only for the audio branch. As the input sequences have variable length, we padded them with zeros up to a maximum sequence length and then masked the padded values.</span></p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p"><span id="S3.p5.1.1" class="ltx_text" style="font-size:90%;">We report two different metrics: average recall (AvRec), and the average area under the ROC (AvAUC). Average recall is used instead of accuracy since both datasets have significant imbalance across classes. Both averages are computed over the four target emotions, considering a one-vs-all problem in order to compute individual recall and AUC values.</span></p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p"><span id="S3.p6.1.1" class="ltx_text" style="font-size:90%;">We observed that using early stopping in IEMOCAP led to inconsistent results as the data are scarce to generate a validation fold large enough. For this reason, the number of epochs for training each model was selected by optimizing the median AvAUC value on IEMOCAP over 5 seeds. The architectures and hyperparameters were also selected based on IEMOCAP results (sometimes using a single seed). The final results on IEMOCAP were obtained over 10 seeds, including the 5 used for the optimization of the epoch and the hyperparameters tuning. This leads to possibly optimistic results on this dataset. On the other hand, the results on MSP-PODCAST were obtained using the same number of epochs and hyperparameters chosen for IEMOCAP, also averaging over 10 seeds. All of our models were trained using Keras </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="S3.p6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p6.1.4" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">4 </span>Results and discussion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text" style="font-size:90%;">In this section we report results for individual and fused systems. We start by showing the effect that the criteria used to define the folds for cross-validation on IEMOCAP has on the two individual systems.</span></p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Effect of partition criteria for IEMOCAP folds</h3>

<figure id="S4.F2" class="ltx_figure"><img src="/html/2403.18635/assets/x1.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="218" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.4.1.1" class="ltx_text ltx_font_bold">Fig.Â 2</span>: </span>Effect of different criteria for defining the folds in IEMOCAP on audio- and text-based systems for two different model sizes (small and large). RAND: random folds, SP: by-speaker folds, SP&amp;SC: by-speaker and by-script folds.</figcaption>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">FigureÂ </span><a href="#S4.F2" title="Figure 2 â€£ 4.1 Effect of partition criteria for IEMOCAP folds â€£ 4 Results and discussion â€£ Fusion approaches for emotion recognition from speech using acoustic and text-based features" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S4.SS1.p1.1.2" class="ltx_text" style="font-size:90%;"> shows the AvAUC for three different criteria used to define the folds on IEMOCAP. Results are computed on the merged test scores for all folds. In all cases, 5 folds are used. We compare: (1) random folds (RAND), where no information about speakers or scripts is used to define the folds; (2) folds by speaker (SP) where each fold contains the two speakers from one of the sessions; and (3) fold by speaker and script (SP&amp;SC) where the folds are defined as in the previous case, but only script 3 is used for testing while all other scripts are used in training. Note that this last option includes less data for each fold. Finally, we compare two different sizes of models: one using half of the nodes in the models from FigureÂ </span><a href="#S2.F1" title="Figure 1 â€£ 2.2 Audio-based model â€£ 2 Models â€£ Fusion approaches for emotion recognition from speech using acoustic and text-based features" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.SS1.p1.1.3" class="ltx_text" style="font-size:90%;"> (small), and one using twice the number of nodes (large). We note that most papers use by-speaker folds </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S4.SS1.p1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p1.1.6" class="ltx_text" style="font-size:90%;">, while some use random folds </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p1.1.7.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S4.SS1.p1.1.8.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p1.1.9" class="ltx_text" style="font-size:90%;">. We are not aware of any work that splits by script, though some works discard the scripts altogether using only improvisation instances for testing </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p1.1.10.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S4.SS1.p1.1.11.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p1.1.12" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2403.18635/assets/x2.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="116" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.4.1.1" class="ltx_text ltx_font_bold">Fig.Â 3</span>: </span>Results for IEMOCAP and MSP-PODCAST dataset. Average AUC distributions for 10 different initialization seeds for different systems: audio model, Glove and BERT based text models, early fusion with cold-start (EF-CS), pretraining (EF-PT) and warm-start (EF-WS) and late fusion with pretraining (LF-PT) models.</figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text" style="font-size:90%;">FigureÂ </span><a href="#S4.F2" title="Figure 2 â€£ 4.1 Effect of partition criteria for IEMOCAP folds â€£ 4 Results and discussion â€£ Fusion approaches for emotion recognition from speech using acoustic and text-based features" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S4.SS1.p2.1.2" class="ltx_text" style="font-size:90%;"> clearly shows that both random and by-speaker folds result in optimistic performance for the text-based systems. For the audio-based system, both by-speaker and by-speaker-and-script options lead to similar performance (indicating that the effect of by-speaker-and-script folds having less data is limited), while the random splits result in an optimistic estimation of performance. Furthermore, the conclusion of which model size is optimal for BERT features changes depending on the fold criteria, as a large model is more likely to overfit, but this effect can only be observed when using folds that do not repeat speakers or scripts between training and test sets. Given these results, we believe it is essential to define the folds for IEMOCAP carefully, not allowing speakers and scripts seen in training to be repeated in testing. In the remaining experiments, we use folds by speaker and script.</span></p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.35.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Median of evaluation metrics <math id="S4.T1.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.2.m1.1b"><mo id="S4.T1.2.m1.1.1" xref="S4.T1.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.m1.1c"><csymbol cd="latexml" id="S4.T1.2.m1.1.1.cmml" xref="S4.T1.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.m1.1d">\pm</annotation></semantics></math> interquartile range obtained using 10 seeds for all the tested models in both datasets.</figcaption>
<table id="S4.T1.30" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.30.29.1" class="ltx_tr">
<th id="S4.T1.30.29.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"></th>
<td id="S4.T1.30.29.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2"><span id="S4.T1.30.29.1.2.1" class="ltx_text" style="font-size:90%;">IEMOCAP</span></td>
<td id="S4.T1.30.29.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T1.30.29.1.3.1" class="ltx_text" style="font-size:90%;">MSP-PODCAST</span></td>
</tr>
<tr id="S4.T1.30.30.2" class="ltx_tr">
<td id="S4.T1.30.30.2.1" class="ltx_td ltx_align_center"><span id="S4.T1.30.30.2.1.1" class="ltx_text" style="font-size:90%;">AvRec (%)</span></td>
<td id="S4.T1.30.30.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.30.30.2.2.1" class="ltx_text" style="font-size:90%;">AvAUC</span></td>
<td id="S4.T1.30.30.2.3" class="ltx_td ltx_align_center"><span id="S4.T1.30.30.2.3.1" class="ltx_text" style="font-size:90%;">AvRec (%)</span></td>
<td id="S4.T1.30.30.2.4" class="ltx_td ltx_align_center"><span id="S4.T1.30.30.2.4.1" class="ltx_text" style="font-size:90%;">AvAUC</span></td>
</tr>
<tr id="S4.T1.6.4" class="ltx_tr">
<th id="S4.T1.6.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.6.4.5.1" class="ltx_text" style="font-size:90%;">Audio</span></th>
<td id="S4.T1.3.1.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.3.1.1.1" class="ltx_text" style="font-size:90%;">56,0</span><math id="S4.T1.3.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.3.1.1.m1.1a"><mo mathsize="90%" id="S4.T1.3.1.1.m1.1.1" xref="S4.T1.3.1.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.1.1.m1.1b"><csymbol cd="latexml" id="S4.T1.3.1.1.m1.1.1.cmml" xref="S4.T1.3.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.1.1.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.3.1.1.2" class="ltx_text" style="font-size:90%;">1,9</span>
</td>
<td id="S4.T1.4.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S4.T1.4.2.2.1" class="ltx_text" style="font-size:90%;">.782</span><math id="S4.T1.4.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.4.2.2.m1.1a"><mo mathsize="90%" id="S4.T1.4.2.2.m1.1.1" xref="S4.T1.4.2.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.2.2.m1.1b"><csymbol cd="latexml" id="S4.T1.4.2.2.m1.1.1.cmml" xref="S4.T1.4.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.2.2.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.4.2.2.2" class="ltx_text" style="font-size:90%;">.006</span>
</td>
<td id="S4.T1.5.3.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.5.3.3.1" class="ltx_text" style="font-size:90%;">45,7</span><math id="S4.T1.5.3.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.5.3.3.m1.1a"><mo mathsize="90%" id="S4.T1.5.3.3.m1.1.1" xref="S4.T1.5.3.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.3.3.m1.1b"><csymbol cd="latexml" id="S4.T1.5.3.3.m1.1.1.cmml" xref="S4.T1.5.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.3.3.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.5.3.3.2" class="ltx_text" style="font-size:90%;">1,4</span>
</td>
<td id="S4.T1.6.4.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.6.4.4.1" class="ltx_text" style="font-size:90%;">.726</span><math id="S4.T1.6.4.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.6.4.4.m1.1a"><mo mathsize="90%" id="S4.T1.6.4.4.m1.1.1" xref="S4.T1.6.4.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.4.4.m1.1b"><csymbol cd="latexml" id="S4.T1.6.4.4.m1.1.1.cmml" xref="S4.T1.6.4.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.4.4.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.6.4.4.2" class="ltx_text" style="font-size:90%;">.010</span>
</td>
</tr>
<tr id="S4.T1.10.8" class="ltx_tr">
<th id="S4.T1.10.8.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.10.8.5.1" class="ltx_text" style="font-size:90%;">Glove</span></th>
<td id="S4.T1.7.5.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.7.5.1.1" class="ltx_text" style="font-size:90%;">47,8</span><math id="S4.T1.7.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.7.5.1.m1.1a"><mo mathsize="90%" id="S4.T1.7.5.1.m1.1.1" xref="S4.T1.7.5.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.5.1.m1.1b"><csymbol cd="latexml" id="S4.T1.7.5.1.m1.1.1.cmml" xref="S4.T1.7.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.5.1.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.7.5.1.2" class="ltx_text" style="font-size:90%;">0.1</span>
</td>
<td id="S4.T1.8.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S4.T1.8.6.2.1" class="ltx_text" style="font-size:90%;">.736</span><math id="S4.T1.8.6.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.8.6.2.m1.1a"><mo mathsize="90%" id="S4.T1.8.6.2.m1.1.1" xref="S4.T1.8.6.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.6.2.m1.1b"><csymbol cd="latexml" id="S4.T1.8.6.2.m1.1.1.cmml" xref="S4.T1.8.6.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.6.2.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.8.6.2.2" class="ltx_text" style="font-size:90%;">.007</span>
</td>
<td id="S4.T1.9.7.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.9.7.3.1" class="ltx_text" style="font-size:90%;">49,8</span><math id="S4.T1.9.7.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.9.7.3.m1.1a"><mo mathsize="90%" id="S4.T1.9.7.3.m1.1.1" xref="S4.T1.9.7.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.9.7.3.m1.1b"><csymbol cd="latexml" id="S4.T1.9.7.3.m1.1.1.cmml" xref="S4.T1.9.7.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.7.3.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.9.7.3.2" class="ltx_text" style="font-size:90%;">0.4</span>
</td>
<td id="S4.T1.10.8.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.10.8.4.1" class="ltx_text" style="font-size:90%;">.736</span><math id="S4.T1.10.8.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.10.8.4.m1.1a"><mo mathsize="90%" id="S4.T1.10.8.4.m1.1.1" xref="S4.T1.10.8.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.10.8.4.m1.1b"><csymbol cd="latexml" id="S4.T1.10.8.4.m1.1.1.cmml" xref="S4.T1.10.8.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.8.4.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.10.8.4.2" class="ltx_text" style="font-size:90%;">.003</span>
</td>
</tr>
<tr id="S4.T1.14.12" class="ltx_tr">
<th id="S4.T1.14.12.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.14.12.5.1" class="ltx_text" style="font-size:90%;">BERT</span></th>
<td id="S4.T1.11.9.1" class="ltx_td ltx_align_center">
<span id="S4.T1.11.9.1.1" class="ltx_text" style="font-size:90%;">55,2</span><math id="S4.T1.11.9.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.11.9.1.m1.1a"><mo mathsize="90%" id="S4.T1.11.9.1.m1.1.1" xref="S4.T1.11.9.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.11.9.1.m1.1b"><csymbol cd="latexml" id="S4.T1.11.9.1.m1.1.1.cmml" xref="S4.T1.11.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.9.1.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.11.9.1.2" class="ltx_text" style="font-size:90%;">1.0</span>
</td>
<td id="S4.T1.12.10.2" class="ltx_td ltx_align_center ltx_border_r">
<span id="S4.T1.12.10.2.1" class="ltx_text" style="font-size:90%;">.792</span><math id="S4.T1.12.10.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.12.10.2.m1.1a"><mo mathsize="90%" id="S4.T1.12.10.2.m1.1.1" xref="S4.T1.12.10.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.12.10.2.m1.1b"><csymbol cd="latexml" id="S4.T1.12.10.2.m1.1.1.cmml" xref="S4.T1.12.10.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.10.2.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.12.10.2.2" class="ltx_text" style="font-size:90%;">.003</span>
</td>
<td id="S4.T1.13.11.3" class="ltx_td ltx_align_center">
<span id="S4.T1.13.11.3.1" class="ltx_text" style="font-size:90%;">51,0</span><math id="S4.T1.13.11.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.13.11.3.m1.1a"><mo mathsize="90%" id="S4.T1.13.11.3.m1.1.1" xref="S4.T1.13.11.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.13.11.3.m1.1b"><csymbol cd="latexml" id="S4.T1.13.11.3.m1.1.1.cmml" xref="S4.T1.13.11.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.11.3.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.13.11.3.2" class="ltx_text" style="font-size:90%;">0.9</span>
</td>
<td id="S4.T1.14.12.4" class="ltx_td ltx_align_center">
<span id="S4.T1.14.12.4.1" class="ltx_text" style="font-size:90%;">.749</span><math id="S4.T1.14.12.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.14.12.4.m1.1a"><mo mathsize="90%" id="S4.T1.14.12.4.m1.1.1" xref="S4.T1.14.12.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.14.12.4.m1.1b"><csymbol cd="latexml" id="S4.T1.14.12.4.m1.1.1.cmml" xref="S4.T1.14.12.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.12.4.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.14.12.4.2" class="ltx_text" style="font-size:90%;">.007</span>
</td>
</tr>
<tr id="S4.T1.18.16" class="ltx_tr">
<th id="S4.T1.18.16.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.18.16.5.1" class="ltx_text" style="font-size:90%;">EF-CS</span></th>
<td id="S4.T1.15.13.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.15.13.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">65,1<math id="S4.T1.15.13.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.15.13.1.1.m1.1a"><mo id="S4.T1.15.13.1.1.m1.1.1" xref="S4.T1.15.13.1.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.15.13.1.1.m1.1b"><csymbol cd="latexml" id="S4.T1.15.13.1.1.m1.1.1.cmml" xref="S4.T1.15.13.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.13.1.1.m1.1c">\pm</annotation></semantics></math>0.5</span></td>
<td id="S4.T1.16.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="S4.T1.16.14.2.1" class="ltx_text" style="font-size:90%;">.857</span><math id="S4.T1.16.14.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.16.14.2.m1.1a"><mo mathsize="90%" id="S4.T1.16.14.2.m1.1.1" xref="S4.T1.16.14.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.16.14.2.m1.1b"><csymbol cd="latexml" id="S4.T1.16.14.2.m1.1.1.cmml" xref="S4.T1.16.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.16.14.2.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.16.14.2.2" class="ltx_text" style="font-size:90%;">.002</span>
</td>
<td id="S4.T1.17.15.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.17.15.3.1" class="ltx_text" style="font-size:90%;">58.2</span><math id="S4.T1.17.15.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.17.15.3.m1.1a"><mo mathsize="90%" id="S4.T1.17.15.3.m1.1.1" xref="S4.T1.17.15.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.17.15.3.m1.1b"><csymbol cd="latexml" id="S4.T1.17.15.3.m1.1.1.cmml" xref="S4.T1.17.15.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.17.15.3.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.17.15.3.2" class="ltx_text" style="font-size:90%;">2.4</span>
</td>
<td id="S4.T1.18.16.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T1.18.16.4.1" class="ltx_text" style="font-size:90%;">.817</span><math id="S4.T1.18.16.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.18.16.4.m1.1a"><mo mathsize="90%" id="S4.T1.18.16.4.m1.1.1" xref="S4.T1.18.16.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.18.16.4.m1.1b"><csymbol cd="latexml" id="S4.T1.18.16.4.m1.1.1.cmml" xref="S4.T1.18.16.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.18.16.4.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.18.16.4.2" class="ltx_text" style="font-size:90%;">.009</span>
</td>
</tr>
<tr id="S4.T1.22.20" class="ltx_tr">
<th id="S4.T1.22.20.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.22.20.5.1" class="ltx_text" style="font-size:90%;">EF-WS</span></th>
<td id="S4.T1.19.17.1" class="ltx_td ltx_align_center">
<span id="S4.T1.19.17.1.1" class="ltx_text" style="font-size:90%;">64.7</span><math id="S4.T1.19.17.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.19.17.1.m1.1a"><mo mathsize="90%" id="S4.T1.19.17.1.m1.1.1" xref="S4.T1.19.17.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.19.17.1.m1.1b"><csymbol cd="latexml" id="S4.T1.19.17.1.m1.1.1.cmml" xref="S4.T1.19.17.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.19.17.1.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.19.17.1.2" class="ltx_text" style="font-size:90%;">1.6</span>
</td>
<td id="S4.T1.20.18.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.20.18.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">.863<math id="S4.T1.20.18.2.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.20.18.2.1.m1.1a"><mo id="S4.T1.20.18.2.1.m1.1.1" xref="S4.T1.20.18.2.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.20.18.2.1.m1.1b"><csymbol cd="latexml" id="S4.T1.20.18.2.1.m1.1.1.cmml" xref="S4.T1.20.18.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.20.18.2.1.m1.1c">\pm</annotation></semantics></math>.002</span></td>
<td id="S4.T1.21.19.3" class="ltx_td ltx_align_center"><span id="S4.T1.21.19.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">59.1<math id="S4.T1.21.19.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.21.19.3.1.m1.1a"><mo id="S4.T1.21.19.3.1.m1.1.1" xref="S4.T1.21.19.3.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.21.19.3.1.m1.1b"><csymbol cd="latexml" id="S4.T1.21.19.3.1.m1.1.1.cmml" xref="S4.T1.21.19.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.21.19.3.1.m1.1c">\pm</annotation></semantics></math>1.8</span></td>
<td id="S4.T1.22.20.4" class="ltx_td ltx_align_center"><span id="S4.T1.22.20.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">.823<math id="S4.T1.22.20.4.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.22.20.4.1.m1.1a"><mo id="S4.T1.22.20.4.1.m1.1.1" xref="S4.T1.22.20.4.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.22.20.4.1.m1.1b"><csymbol cd="latexml" id="S4.T1.22.20.4.1.m1.1.1.cmml" xref="S4.T1.22.20.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.22.20.4.1.m1.1c">\pm</annotation></semantics></math>.003</span></td>
</tr>
<tr id="S4.T1.26.24" class="ltx_tr">
<th id="S4.T1.26.24.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.26.24.5.1" class="ltx_text" style="font-size:90%;">EF-PT</span></th>
<td id="S4.T1.23.21.1" class="ltx_td ltx_align_center">
<span id="S4.T1.23.21.1.1" class="ltx_text" style="font-size:90%;">64.9</span><math id="S4.T1.23.21.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.23.21.1.m1.1a"><mo mathsize="90%" id="S4.T1.23.21.1.m1.1.1" xref="S4.T1.23.21.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.23.21.1.m1.1b"><csymbol cd="latexml" id="S4.T1.23.21.1.m1.1.1.cmml" xref="S4.T1.23.21.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.23.21.1.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.23.21.1.2" class="ltx_text" style="font-size:90%;">1.0</span>
</td>
<td id="S4.T1.24.22.2" class="ltx_td ltx_align_center ltx_border_r">
<span id="S4.T1.24.22.2.1" class="ltx_text" style="font-size:90%;">.859</span><math id="S4.T1.24.22.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.24.22.2.m1.1a"><mo mathsize="90%" id="S4.T1.24.22.2.m1.1.1" xref="S4.T1.24.22.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.24.22.2.m1.1b"><csymbol cd="latexml" id="S4.T1.24.22.2.m1.1.1.cmml" xref="S4.T1.24.22.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.24.22.2.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.24.22.2.2" class="ltx_text" style="font-size:90%;">.004</span>
</td>
<td id="S4.T1.25.23.3" class="ltx_td ltx_align_center">
<span id="S4.T1.25.23.3.1" class="ltx_text" style="font-size:90%;">56.5</span><math id="S4.T1.25.23.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.25.23.3.m1.1a"><mo mathsize="90%" id="S4.T1.25.23.3.m1.1.1" xref="S4.T1.25.23.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.25.23.3.m1.1b"><csymbol cd="latexml" id="S4.T1.25.23.3.m1.1.1.cmml" xref="S4.T1.25.23.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.25.23.3.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.25.23.3.2" class="ltx_text" style="font-size:90%;">0.3</span>
</td>
<td id="S4.T1.26.24.4" class="ltx_td ltx_align_center">
<span id="S4.T1.26.24.4.1" class="ltx_text" style="font-size:90%;">.817</span><math id="S4.T1.26.24.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.26.24.4.m1.1a"><mo mathsize="90%" id="S4.T1.26.24.4.m1.1.1" xref="S4.T1.26.24.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.26.24.4.m1.1b"><csymbol cd="latexml" id="S4.T1.26.24.4.m1.1.1.cmml" xref="S4.T1.26.24.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.26.24.4.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.26.24.4.2" class="ltx_text" style="font-size:90%;">.002</span>
</td>
</tr>
<tr id="S4.T1.30.28" class="ltx_tr">
<th id="S4.T1.30.28.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T1.30.28.5.1" class="ltx_text" style="font-size:90%;">LF-PT</span></th>
<td id="S4.T1.27.25.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S4.T1.27.25.1.1" class="ltx_text" style="font-size:90%;">63.9</span><math id="S4.T1.27.25.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.27.25.1.m1.1a"><mo mathsize="90%" id="S4.T1.27.25.1.m1.1.1" xref="S4.T1.27.25.1.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.27.25.1.m1.1b"><csymbol cd="latexml" id="S4.T1.27.25.1.m1.1.1.cmml" xref="S4.T1.27.25.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.27.25.1.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.27.25.1.2" class="ltx_text" style="font-size:90%;">0.5</span>
</td>
<td id="S4.T1.28.26.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">
<span id="S4.T1.28.26.2.1" class="ltx_text" style="font-size:90%;">.857</span><math id="S4.T1.28.26.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.28.26.2.m1.1a"><mo mathsize="90%" id="S4.T1.28.26.2.m1.1.1" xref="S4.T1.28.26.2.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.28.26.2.m1.1b"><csymbol cd="latexml" id="S4.T1.28.26.2.m1.1.1.cmml" xref="S4.T1.28.26.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.28.26.2.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.28.26.2.2" class="ltx_text" style="font-size:90%;">.006</span>
</td>
<td id="S4.T1.29.27.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S4.T1.29.27.3.1" class="ltx_text" style="font-size:90%;">58.0</span><math id="S4.T1.29.27.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.29.27.3.m1.1a"><mo mathsize="90%" id="S4.T1.29.27.3.m1.1.1" xref="S4.T1.29.27.3.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.29.27.3.m1.1b"><csymbol cd="latexml" id="S4.T1.29.27.3.m1.1.1.cmml" xref="S4.T1.29.27.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.29.27.3.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.29.27.3.2" class="ltx_text" style="font-size:90%;">0.7</span>
</td>
<td id="S4.T1.30.28.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S4.T1.30.28.4.1" class="ltx_text" style="font-size:90%;">.819</span><math id="S4.T1.30.28.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.30.28.4.m1.1a"><mo mathsize="90%" id="S4.T1.30.28.4.m1.1.1" xref="S4.T1.30.28.4.m1.1.1.cmml">Â±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.30.28.4.m1.1b"><csymbol cd="latexml" id="S4.T1.30.28.4.m1.1.1.cmml" xref="S4.T1.30.28.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.30.28.4.m1.1c">\pm</annotation></semantics></math><span id="S4.T1.30.28.4.2" class="ltx_text" style="font-size:90%;">.004</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Audio- and text-based models</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text" style="font-size:90%;">FigureÂ </span><a href="#S4.F3" title="Figure 3 â€£ 4.1 Effect of partition criteria for IEMOCAP folds â€£ 4 Results and discussion â€£ Fusion approaches for emotion recognition from speech using acoustic and text-based features" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S4.SS2.p1.1.2" class="ltx_text" style="font-size:90%;"> shows the performance obtained with the different systems on both datasets. Average AUC is reported using box and whiskers plots to show the variation in performance for 10 different seeds used to initialize the DNN weights. Table </span><a href="#S4.T1" title="Table 1 â€£ 4.1 Effect of partition criteria for IEMOCAP folds â€£ 4 Results and discussion â€£ Fusion approaches for emotion recognition from speech using acoustic and text-based features" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.SS2.p1.1.3" class="ltx_text" style="font-size:90%;"> shows both AvAUC and AvRec values. We can see that our proposed text model based on BERT embeddings shows slightly better performance than the audio model on IEMOCAP, while Glove embeddings give significantly worse performance. This contradicts previous results on IEMOCAP, where the text-based models significantly outperform audio models </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.SS2.p1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS2.p1.1.6" class="ltx_text" style="font-size:90%;">. As we showed in Section </span><a href="#S4.SS1" title="4.1 Effect of partition criteria for IEMOCAP folds â€£ 4 Results and discussion â€£ Fusion approaches for emotion recognition from speech using acoustic and text-based features" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4.1</span></a><span id="S4.SS2.p1.1.7" class="ltx_text" style="font-size:90%;"> this is explained by the way we have defined the folds, preventing the text model from being trained in dialogues very similar or identical to the ones present in the test set and avoiding unrealistically good performance estimates for these systems.</span></p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text" style="font-size:90%;">The effect of using BERT versus Glove to represent word information can be seen in FigureÂ </span><a href="#S4.F3" title="Figure 3 â€£ 4.1 Effect of partition criteria for IEMOCAP folds â€£ 4 Results and discussion â€£ Fusion approaches for emotion recognition from speech using acoustic and text-based features" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S4.SS2.p2.1.2" class="ltx_text" style="font-size:90%;"> and Table </span><a href="#S4.T1" title="Table 1 â€£ 4.1 Effect of partition criteria for IEMOCAP folds â€£ 4 Results and discussion â€£ Fusion approaches for emotion recognition from speech using acoustic and text-based features" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.SS2.p2.1.3" class="ltx_text" style="font-size:90%;">. BERT embeddings outperform Glove ones in both datasets with relative UAR improvements of 15.5% and 2.4% in IEMOCAP and MSP-PODCAST datasets, respectively. We attribute this performance gain to the contextual information imbued in the pretrained BERT model. While our text model could potentially learn contextual information from standard word embeddings like Glove, learning to represent negations or modification values would require a significant amount of data. We hypothesize that this is the reason why Glove performance is closer to BERT in MSP-PODCAST than in IEMOCAP, since the size and variability of dialogues in MSP-PODCAST may be allowing the text model to learn contextual information even from standard word embeddings.</span></p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text" style="font-size:90%;">Finally, we note the large effect that the seed has on our systems. In many cases, the ranking of systems changes significantly depending on the seed (results not shown due to lack of space), which thus highlights the critical importance of using several seeds in order to reach more solid conclusions.</span></p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Fusion models</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text" style="font-size:90%;">As it has been noted in previous works, adding text information to audio-based SER systems gives significant performance improvements </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S4.SS3.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS3.p1.1.4" class="ltx_text" style="font-size:90%;">. This is also observed in our fusion experiments where for both MSP-PODCAST and IEMOCAP datasets, the AvRec improves 16% relative to the best performing single model. All fusion approaches perform similarly, in agreement with previous results in the literature </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.SS3.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS3.p1.1.7" class="ltx_text" style="font-size:90%;">. Only the late fusion approach with pre-training is shown here, due to space considerations. The other two training approaches gave similar results.</span></p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text" style="font-size:90%;">A small advantage of the warm-start approach can be observed for both datasets with the early fusion architecture, indicating that this direction may be worth further exploration. In the future, we plan to explore approaches where the fusion is made before or at the pooling layer. We believe this has the potential to give additional benefits since the interaction between both modalities is most likely happening at short time intervals rather than at phrase level.</span></p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text" style="font-size:90%;">We presented different approaches for emotion recognition from speech using audio features and transcriptions. We showed results on two publicly available datasets: IEMOCAP and MSP-PODCAST.
We demonstrated the positive effect of representing linguistic information using contextualized word embeddings extracted with BERT compared to using standard word embeddings like those extracted with Glove. We also showed, in agreement with previous works, that the fusion of audio- and text-based information leads to significant improvements of approximately 16% on both datasets relative to using the best single modality. To our knowledge, these are the first published results using linguistic information on MSP-PODCAST, a very large, naturalistic and challenging emotion dataset.</span></p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text" style="font-size:90%;">Several fusion strategies were tested, including early and late fusion using different training procedures. Results were not significantly different for the different methods, which again agrees with previous observations in the literature.</span></p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text" style="font-size:90%;">As an additional contribution, we highlighted the importance and impact of how folds are defined for the IEMOCAP dataset, showing how the standard procedure of splitting by session leads to highly optimistic results on our text-based system. We hope that our proposed criteria, which avoids repeating scripted dialogues between training and test sets, or the alternative of discarding scripted dialogues, will be adopted in future works on the IEMOCAP dataset, specially for text-based systems.</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Laurence Devillers, Laurence Vidrascu, and Lori Lamel,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">â€œChallenges in real-life emotion annotation and machine learning based detection,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural Networks</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, vol. 18, no. 4, pp. 407â€“422, May 2005.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Christos-Nikolaos Anagnostopoulos, Theodoros Iliou, and Ioannis Giannoukos,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">â€œFeatures and classifiers for emotion recognition from speech: a survey from 2000 to 2011,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Artificial Intelligence Review</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, vol. 43, no. 2, pp. 155â€“177, Feb. 2015.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
W.Â Q. Zheng, J.Â S. Yu, and Y.Â X. Zou,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">â€œAn experimental study of speech emotion recognition based on deep convolutional neural networks,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2015 International Conference on Affective Computing and Intelligent Interaction (ACII)</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, Sept. 2015, pp. 827â€“831.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Aharon Satt, Shai Rozenberg, and Ron Hoory,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">â€œEfficient Emotion Recognition from Speech Using Deep Learning on Spectrograms,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Interspeech</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, Aug. 2017, pp. 1089â€“1093.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Q.Â Jin, C.Â Li, S.Â Chen, and H.Â Wu,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">â€œSpeech emotion recognition with acoustic and lexical features,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, April 2015, pp. 4749â€“4753.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Ze-Jing Chuang and Chung-Hsien Wu,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">â€œMulti-modal emotion recognition from speech and text,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computational Linguistics &amp; Chinese Language Processing: Special Issue on New Trends of Speech and Language Processing</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, August 2004, vol.Â 9, pp. 45â€“62.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
KalaniÂ Wataraka Gamage, Vidhyasaharan Sethu, and Eliathamby Ambikairajah,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">â€œSalience based lexical features for emotion recognition,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2017, pp. 5830â€“5834.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
B.Â Schuller, G.Â Rigoll, and M.Â Lang,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">â€œSpeech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, May 2004, vol.Â 1, pp. Iâ€“577.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Albert Haque, Michelle Guo, Prateek Verma, and LiÂ Fei-Fei,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">â€œAudio-linguistic embeddings for spoken sentences,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2019, pp. 7355â€“7359.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Seunghyun Yoon, Seokhyun Byun, and Kyomin Jung,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">â€œMultimodal speech emotion recognition using audio and text,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 IEEE Spoken Language Technology Workshop (SLT)</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 112â€“118.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Jilt Sebastian and Piero Pierucci,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">â€œFusion Techniques for Utterance-Level Emotion Recognition Combining Speech and Transcripts,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Interspeech 2019</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, Sept. 2019, pp. 51â€“55.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Saurabh Sahu, Vikramjit Mitra, Nadee Seneviratne, and Carol Espy-Wilson,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">â€œMulti-Modal Learning for Speech Emotion Recognition: An Analysis and Comparison of ASR Outputs with Ground Truth Transcription,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Interspeech</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, Sept. 2019, pp. 3302â€“3306.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Biqiao Zhang, Soheil Khorram, and EmilyÂ Mower Provost,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">â€œExploiting Acoustic and Lexical Properties of Phonemes to Recognize Valence from Speech,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, May 2019, pp. 5871â€“5875.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">â€œBert: Pre-training of deep bidirectional transformers for language understanding,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of NAACL-HLT, Minneapolis, USA</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2019, pp. 4171â€“4186.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Jeffrey Pennington, Richard Socher, and ChristopherÂ D. Manning,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">â€œGlove: Global vectors for word representation,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Zhongkai Sun, PrathushaÂ K. Sarma, William Sethares, and ErikÂ P. Bucy,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">â€œMulti-Modal Sentiment Analysis Using Deep Canonical Correlation Analysis,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Interspeech</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, Sept. 2019, pp. 1323â€“1327.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, JeannetteÂ N. Chang, Sungbok Lee, and ShrikanthÂ S. Narayanan,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">â€œIemocap: interactive emotional dyadic motion capture database,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Language Resources and Evaluation</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, vol. 42, no. 4, pp. 335, Nov 2008.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Reza Lotfian and Carlos Busso,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">â€œBuilding naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Affective Computing</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, vol. PP, pp. 1â€“1, 08 2017.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, AidanÂ N Gomez, </span><span id="bib.bib19.2.2" class="ltx_text ltx_font_caligraphic" style="font-size:90%;">L</span><span id="bib.bib19.3.3" class="ltx_text" style="font-size:90%;">ukasz Kaiser, and Illia Polosukhin,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.4.1" class="ltx_text" style="font-size:90%;">â€œAttention is all you need,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.5.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib19.6.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib19.7.3" class="ltx_text" style="font-size:90%;">, 2017, pp. 5998â€“6008.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Florian Eyben, Martin WÃ¶llmer, and BjÃ¶rn Schuller,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">â€œOpensmile: The munich versatile and fast open-source audio feature extractor,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 18th ACM International Conference on Multimedia</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, New York, NY, USA, 2010, pp. 1459â€“1462.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Xavier Glorot and Yoshua Bengio,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">â€œUnderstanding the difficulty of training deep feedforward neural networks,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATSâ€™10)</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
HaythamÂ M. Fayek, Margaret Lech, and Lawrence Cavedon,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">â€œEvaluating deep learning architectures for Speech Emotion Recognition,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural Networks</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, vol. 92, pp. 60â€“68, 2017.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Pablo Riera, Luciana Ferrer, AgustÃ­n Gravano, and Lara Gauder,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">â€œNo sample left behind: Towards a comprehensive evaluation of speech emotion recognition system,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Workshop on Speech, Music and Mind 2019</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
DiederikÂ P. Kingma and Jimmy Ba,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">â€œAdam: A method for stochastic optimization,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">3rd International Conference for Learning Representations</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
FranÃ§ois Chollet etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">â€œKeras,â€ https://keras.io, 2015.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Haiyang Xu, Hui Zhang, Kun Han, Yun Wang, Yiping Peng, and Xiangang Li,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">â€œLearning alignment for multimodal emotion recognition from speech,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Interspeech</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2019, pp. 3569â€“3573.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Efthymios Georgiou, Charilaos Papaioannou, and Alexandros Potamianos,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">â€œDeep Hierarchical Fusion with Application in Sentiment Analysis,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Interspeech 2019</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, Sept. 2019, pp. 1646â€“1650.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.18634" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.18635" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.18635">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.18635" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.18636" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 15:57:45 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
