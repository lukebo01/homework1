<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.15754] Multimodal Segmentation for Vocal Tract Modeling</title><meta property="og:description" content="Accurate modeling of the vocal tract is necessary to construct articulatory representations for interpretable speech processing and linguistics. However, vocal tract modeling is challenging because many internal articu…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multimodal Segmentation for Vocal Tract Modeling">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multimodal Segmentation for Vocal Tract Modeling">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.15754">

<!--Generated on Fri Jul  5 22:18:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.3" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.4" class="ltx_ERROR undefined">\name</span>
<p id="p1.2" class="ltx_p">[]RishiJain<sup id="p1.2.1" class="ltx_sup">∗</sup>
<span id="p1.2.2" class="ltx_ERROR undefined">\name</span>[]BohanYu<sup id="p1.2.3" class="ltx_sup">∗</sup>
<span id="p1.2.4" class="ltx_ERROR undefined">\name</span>[]PeterWu
<span id="p1.2.5" class="ltx_ERROR undefined">\name</span>[]TejasPrabhune
<span id="p1.2.6" class="ltx_ERROR undefined">\name</span>[]GopalaAnumanchipalli</p>
</div>
<h1 class="ltx_title ltx_title_document">Multimodal Segmentation for Vocal Tract Modeling</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Accurate modeling of the vocal tract is necessary to construct articulatory representations for interpretable speech processing and linguistics. However, vocal tract modeling is challenging because many internal articulators are occluded from external motion capture technologies. Real-time magnetic resonance imaging (RT-MRI) allows measuring precise movements of internal articulators during speech, but annotated datasets of MRI are limited in size due to time-consuming and computationally expensive labeling methods. We first present a deep labeling strategy for the RT-MRI video using a vision-only segmentation approach. We then introduce a multimodal algorithm using audio to improve segmentation of vocal articulators. Together, we set a new benchmark for vocal tract modeling in MRI video segmentation and use this to release labels for a 75-speaker RT-MRI dataset, increasing the amount of labeled public RT-MRI data of the vocal tract by over a factor of 9. The code and dataset labels can be found at <a href="rishiraij.github.io/multimodal-mri-avatar/" title="" class="ltx_ref ltx_url ltx_font_typewriter">rishiraij.github.io/multimodal-mri-avatar/</a>.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>articulatory speech, audio-visual perception
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_note_type">footnotetext: </span>These authors contributed equally to this work</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Vocal tract modeling is an essential technology in many applications including facial animation, naturalistic speaking avatars, speaker modeling, and second language
pronunciation learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
In fact, popular self-supervised speech representations inherently learn features correlated with articulators <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
Modeling is also necessary in healthcare applications such as
brain-computer interfaces for communication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and treating speech disfluencies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Methods of external motion capture cannot
record precise and accurate vocal tract movements for occluded
articulators. Thus, the inner mouth is often poorly represented
or neglected in multimedia approaches to motion capture-based
facial animation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Popular approaches to solving the issue of inner mouth occlusion
include electromagnetic articulography (EMA) and electromyography (EMG)
as models for the vocal tract. However, these methods only contain
a small subset of articulatory features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">A more comprehensive approach uses Real-Time Magnetic Resonance Imaging (RT-MRI) of the vocal tract <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
This technology offers audio-aligned videos of internal
and external articulators that are not measurable by other articulatory
representations. When tested on downstream speech-related tasks,
RT-MRI has been shown to more reliably and completely model the vocal
tract in comparison to EMA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. For example, MRI representations distinguish
between oral vowels (lowered velum) and nasal vowels (raised
velum), while EMA does not track the velum at all.
However, current state-of-the-art
labeling methods for extracting interpretable features from these videos
are time-consuming, computationally expensive, and prone to errors
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Therefore, only a small
amount of vocal tract RT-MRI data is labeled <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. As a result, current work using real-time articulatory MRI falls into two broad categories: (1) methods which rely on the previously extracted articulator segmentations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, or (2) models which directly work with RT-MRI videos but do not contain an interpretable intermediate representation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. To address the scarcity of publicly-available articulatory segmentations for RT-MRI, we propose:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A vision-based fully-convolutional neural network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> for speaker-independent vocal tract boundary segmentation.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">A multimodal Transformer model which additionally includes the speech waveform to set a new benchmark for vocal tract RT-MRI segmentation.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Labels for the 75-speaker Speech MRI Open Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> containing
over 20 hours of vocal tract RT-MRI data for 75 speakers diverse in
age, gender, and accent.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Datasets</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>USC-TIMIT Dataset</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.3" class="ltx_p">We use the labeled 8-speaker RT-MRI
USC-TIMIT dataset of the vocal tract described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> for training.
Subjects were instructed to read phonetically-diverse sentences out loud at a natural speaking
rate while laying supine in an MRI scanner. A four-channel upper airway
receiver coil array was used for signal reception, which was
processed to reproduce <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="84\times 84" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mrow id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mn id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">84</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p1.1.m1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml">84</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><times id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">84</cn><cn type="integer" id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3">84</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">84\times 84</annotation></semantics></math> pixel midsaggital MRI videos capturing
lingual, labial, and jaw motion, and velum, pharynx, and larynx articulations.
These videos are collected at 83.33 Hz. We start with the 170 representative points from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> to represent vocal tract air-tissue boundary segmentations. Of these 170 points, we
take the subset of 95 points (190 <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">x</annotation></semantics></math> and <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">y</annotation></semantics></math> coordinates) that has been
determined to be most vital for speech tasks
in Wu <span id="S2.SS1.p1.3.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. All RT-MRI video in the USC-TIMIT dataset is accompanied by existing
articulator points extracted using the baseline algorithm described further in Section <a href="#S3.SS1" title="3.1 Frequency-domain Gradient Descent Baseline ‣ 3 Models and Training ‣ Multimodal Segmentation for Vocal Tract Modeling" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. We use these point labels as training targets for the other segmentation methods described in Section <a href="#S3" title="3 Models and Training ‣ Multimodal Segmentation for Vocal Tract Modeling" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Paired with these trajectories is the 16kHz
speech data (resampled from original 20kHz)
corresponding to the spoken audio during the RT-MRI scan.
Following previous articulatory MRI work, we further enhance this audio using Adobe Podcast to reduce
reverbation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. For training, we use 7 of the 8 speakers (roughly 66 minutes of RT-MRI video) and leave out the remaining speaker as ``unseen''.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Speech MRI Open Dataset</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The Speech MRI Open Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> is a diverse 75-speaker dataset
that provides 20 hours of raw multi-coil RT-MRI videos of the vocal
tract during articulation, aligned with corresponding speech. Such a large, rich dataset
can help solve many open problems in fields
related to phonetics, spoken language, and vocal
articulation. However, unlike the USC-TIMIT dataset, the data does not include labeled MRI
feature points tracked over time.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Models and Training</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Frequency-domain Gradient Descent Baseline</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The existing algorithm for articulatory RT-MRI segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> relies on hand-traced
air-tissue boundaries for the first frame of every video. This is followed by nonlinear
optimization in the frequency space of subsequent frames, requiring 20 minutes to converge for a
single frame using gradient descent. This procedure is also prone to mislabeling and requires
human supervision, making it expensive to run. Because each frame is optimized independently, it often results in jitter, or high-frequency perturbations, for individual articulator points across consecutive frames. As this is the only existing algorithm for articulatory RT-MRI labeling, the outputs of this model are used as the ``ground truth'' training targets for the following models, and the algorithm will be referred to as the ``baseline'' algorithm.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Heatmap U-Net</h3>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2406.15754/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="185" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The attention U-Net model. Dotted lines represent the paths of attention gating in contracting/expanding layers.</figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, a residual fully-convolutional neural network, has historically performed well on low resolution medical images, especially when training data is limited. Because labeled data was only originally available from eight speakers, this architecture was a natural fit. Input MRI frames were padded to a spatial dimension of 96 by 96 and subsequently reduced in the spatial dimension by a factor of two in each layer of the contracting path before expanding. Of the spatial features, the key articulators only occupy a subset of the space. For this reason, we apply attention gating following the Attention U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> with the modification of using additive attention as opposed to multiplicative, visualized in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.2 Heatmap U-Net ‣ 3 Models and Training ‣ Multimodal Segmentation for Vocal Tract Modeling" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. While minimally increasing complexity, the model learns to suppress the components of the signal which are not important for the labeling task.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.2" class="ltx_p">We trained this model on approximately 90 minutes of labeled midsaggital RT-MRI video from 7 speakers for a total of 6 epochs. The model outputs a 96 by 96 grid for each of the 95 articulatory points. Each of the target keypoints were modeled as 2-dimensional isotropic Gaussian distributions over the 96 by 96 spatial grid with a standard deviation of 2 pixels. For generating keypoint locations from the output heatmaps, we took a weighted average of the <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">k</annotation></semantics></math> pixels with the highest output values, where the best <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">k</annotation></semantics></math> was found experimentally to be 25. During training, we also applied random affine transformations to frames and the corresponding annotations to promote generalization to unseen speakers.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Typically, the pixelwise mean squared error loss, also known as L2 loss, is used for heatmap regression tasks, but we also introduce using the Kullback–Leibler (KL) divergence between the output and target grids in which each output grid is restricted to a 2-dimensional probability distribution using a softmax nonlinearity. To our knowledge, this training objective has not been used for heatmap regression in medical imaging in the past, but guides the model into producing an output that also appears Gaussian in nature and is intuitively well suited for measuring the difference in the two probability distributions.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">In addition, articulator points have varying degrees of movement (standard deviation) and importance in speech production. In this context, articulator ``importance'' is determined by the effect that dropping the articulator has on downstream speech synthesis. Both the importance and standard deviation were calculated using the 7 training speakers by previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. We multiply the standard deviation and importance of each point to determine its weighting in the combined loss. This articulatory weighting emphasizes the importance of points that show significant movement and are important to speech production over those which show minimal movement or have been found to be less essential.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Multimodal Transformer</h3>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2406.15754/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="190" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Architecture of the multimodal segmentation model.</figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Using the U-Net model as a pretrained convolutional input, we
further explored joint point tracking methods. To ensure tracks remain smooth, we applied a temporal Gaussian low-pass filter independently for each point of the U-Net output.
We also tried using a convolutional
LSTM as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> (CLSTM) and
a Transformer. The CLSTM, previously used in MRI video
segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, applies a 2-layer LSTM to the
predicted U-Net outputs, trained on
speech from the same 7 USC-TIMIT speakers. The Transformer
similarly used the U-Net points from each timestep, with an
additional positional encoding. Additionally, we experimented with adding optical flow,
Kalman filtering, and Lucas Kanade to improve temporal point tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Both the CRNN and the Transformer methods did not achieve equal
or better performance than smoothed U-Net tracks on MRI videos
of unseen speakers, reinforcing the fact that articulatory MRI tracking
is fundamentally different than other traditional video-only
tracking problems.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">We subsequently experimented with multimodal models
for feature extraction, using representations from
video frames and speech waveforms. For video frames, we used the output of the frozen U-Net model described in Section <a href="#S3.SS2" title="3.2 Heatmap U-Net ‣ 3 Models and Training ‣ Multimodal Segmentation for Vocal Tract Modeling" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> and also experimented with other image representation models including ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and ConvNeXt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. To represent audio, we used the 10th layer of WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> to derive speech representations. The two representations were then concatenated as input to a Transformer prepended with three residual convolutional blocks as seen in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.3 Multimodal Transformer ‣ 3 Models and Training ‣ Multimodal Segmentation for Vocal Tract Modeling" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
Additionally, we experimented with an audio-only segmentation model (articulatory inversion) using the same WavLM and Transformer methods. The Transformer models were trained on the
speech data from the same 7 of 8 USC-TIMIT speakers as in Section <a href="#S3.SS2" title="3.2 Heatmap U-Net ‣ 3 Models and Training ‣ Multimodal Segmentation for Vocal Tract Modeling" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>. Using multi-task
learning, the Transformer experiments output MRI trajectories and pitch simultaneously, optimized using weighted L1 loss.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We performed quantitative evaluations of both our vision-based and
multimodal vocal tract segmentation approaches. The segmentations were then used to add articulatory
labels to RT-MRI from 75 previously-unlabeled speakers.
Using this data as a multimodal pretraining approach, the different segmentations
were further used for a downstream speech task to measure how well speech features were captured
by different segmentation methods. Finally, we conducted a qualitative hypothesis test using our best method.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Vision-only U-Net</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The first experiment compared L2 (mean squared error) loss against our new pixel-wise KL-divergence loss with and without articulatory weighting for the U-Net model. This was evaluated using the root mean squared error (RMSE) of the predicted x-y points for the 95 articulator points on an unseen speaker. The results in Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Vision-only U-Net ‣ 4 Results ‣ Multimodal Segmentation for Vocal Tract Modeling" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> demonstrate that the KL-divergence loss is better suited for low-resolution point recognition for air-tissue boundary segmentation. As RMSE and MSE have the same convergence point, articulatory weighting predictably appears worse using this metric. However, manual inspection reveals that most of this error can be attributed to shifts in less phonologically important articulators such as the hard palate, with significant improvement on the more important articulators.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of the root mean squared error of the U-Net models trained using L2 loss, KL-divergence loss, and KL-divergence loss with articulatory weighting. More details are available in Section <a href="#S4.SS1" title="4.1 Vision-only U-Net ‣ 4 Results ‣ Multimodal Segmentation for Vocal Tract Modeling" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.</figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Loss</span></th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">RMSE</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<td id="S4.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">MSE (L2)</td>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">7.33</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<td id="S4.T1.1.3.2.1" class="ltx_td ltx_align_center">KL-div</td>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center">3.74</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<td id="S4.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_bb">KL-div + Weighting</td>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">3.92</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Labeling the Speech MRI Open Dataset</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The vision-based U-Net above was used to provide labels to RT-MRI video for the 75 speakers in the Speech MRI Open Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Outputs from this model were subsequently run through a temporal Gaussian low-pass filter, which was applied independently for each articulator x-y point and used to provide video and audio aligned MRI trajectories.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2406.15754/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="369" height="388" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Two representative examples of predicted MRI points (<span id="S4.F3.5.1" class="ltx_text ltx_font_italic">right</span>) compared to
expert hand labels (<span id="S4.F3.6.2" class="ltx_text ltx_font_italic">left</span>). The examples are spoken by unseen Female (<span id="S4.F3.7.3" class="ltx_text ltx_font_italic">bottom</span>) and Male (<span id="S4.F3.8.4" class="ltx_text ltx_font_italic">top</span>) speakers in the Speech MRI Open Dataset.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">In Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Labeling the Speech MRI Open Dataset ‣ 4 Results ‣ Multimodal Segmentation for Vocal Tract Modeling" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we highlight the generalization
of the U-Net model on unseen speakers,
allowing us to expand the amount of labeled RT-MRI video to over 20 hours
across 83 total speakers. Qualitatively, the predicted segmentations
closely follow the MRI segments,
achieving high quality labeling for unseen speakers. As part of
this paper, we also present this labeling for use in future
downstream speech tasks, increasing the amount of
publically-available labeled articulatory RT-MRI data by over a factor of <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">9</span>. The labels are available at <a href="rishiraij.github.io/multimodal-mri-avatar/" title="" class="ltx_ref ltx_url ltx_font_typewriter">rishiraij.github.io/multimodal-mri-avatar/</a>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparison with Multimodal Transformer</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">When analyzing our various feature extraction methods, we
first evaluate performance within the context of seen speakers but
unseen examples. Figure <a href="#S4.F4" title="Figure 4 ‣ 4.3 Comparison with Multimodal Transformer ‣ 4 Results ‣ Multimodal Segmentation for Vocal Tract Modeling" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> highlights quantitative results in L1 losses and Pearson
Correlation Coefficients (PCCs) when evaluating models on unseen examples from
seen speakers. We observe that multimodal models perform
consistently better than the purely video-based U-Net. In fact, the best model
in terms of both metrics includes the outputs of the U-Net as one of the input
modalities alongside WavLM vectors. These results suggest the inclusion of speech
within segmentation provides additional speaker-specific information related to the
anatomy of the vocal tract. Since the shape of different parts of the vocal tract can
greatly vary from speaker to speaker, this inclusion is crucial for better in-domain
modeling of speech production.
With only a single modality, the pixel value-based U-Net generalizes better
to unseen speakers than the WavLM-based speech inversion model since contour pixel values capture speaker-specific anatomy better than speech waveforms alone.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.15754/assets/x4.png" id="S4.F4.1.g1" class="ltx_graphics ltx_img_square" width="185" height="158" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2406.15754/assets/x5.png" id="S4.F4.2.g1" class="ltx_graphics ltx_img_square" width="184" height="156" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>L1 losses [↓] (<span id="S4.F4.5.1" class="ltx_text ltx_font_italic">left</span>) and Pearson
Correlation Coefficients (PCCs) [↑] (<span id="S4.F4.6.2" class="ltx_text ltx_font_italic">right</span>) comparing
MRI trajectories of unseen examples from seen
speakers of a given model with the USC-TIMIT ground truth. Varying
through a subset of six representative models.</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Similarly, we evaluate our segmentation methods on downstream speech tasks using speech synthesis
within seen and unseen speaker contexts. Using the state-of-the-art MRI synthesis model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> pretrained on
the newly-labeled 75-speaker dataset, we finetune on the projected MRI trajectories of a USC-TIMIT speaker provided by the different feature extraction models (i.e. baseline, U-Net, and multimodal). To evaluate the intelligibility of synthesized speech, we compute the word error
rate (WER) on test unseen examples using Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, a
state-of-the-art automatic speech recognition (ASR) model. For seen speakers, speech synthesized using the multimodal U-Net + WavLM based segmentations is more intelligible than speech synthesized from either
the ground truth baseline or the U-Net outputs, suggesting that the addition
of the speech modality helps preserves more speech-related information within the
predicted MRI point trajectories compared to a purely image-based approach. Table <a href="#S4.T2" title="Table 2 ‣ 4.3 Comparison with Multimodal Transformer ‣ 4 Results ‣ Multimodal Segmentation for Vocal Tract Modeling" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>
summarizes these results. The results in Table <a href="#S4.T3" title="Table 3 ‣ 4.3 Comparison with Multimodal Transformer ‣ 4 Results ‣ Multimodal Segmentation for Vocal Tract Modeling" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> highlight that
the U-Net + WavLM based model has the lowest WER when testing on an
unseen USC-TIMIT speaker, documenting that the segmentations from the multimodal model on unseen speakers still capture representative articulatory kinematics for naturalistic speech. Pretraining the synthesis model on the 75 speakers also results in much better unseen speaker
generalization, demonstrating that the new labels for the Speech MRI Open Dataset are beneficial for
future work in articulatory speech.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Speech synthesis ASR WER finetuning on segmentations from a seen speaker during segmentation model training, but unseen utterances.
(S) denotes synthesis model pretrained using single
MRI speaker.
All other models are pretrained with 75-speaker MRI.</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">WER [%]</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">U-Net + WavLM</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.1.2.1.2.1" class="ltx_text ltx_font_bold">31.3% (16.4%-49.3%)</span></td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_left">U-Net</td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_left">36.4% (20.9%-55.1%)</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_left">Ground Truth</td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_left">34.7% (18.6%-53.2%)</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<td id="S4.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_border_bb">U-Net + WavLM (S)</td>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_left ltx_border_bb">34.9% (20.3%-52.8%)</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Speech synthesis ASR WER finetuning on segmentations from an unseen speaker during segmentation model training.
(S) denotes synthesis model pretrained using single
MRI speaker.
All other models are pretrained with 75-speaker MRI.</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">WER [%]</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<td id="S4.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">U-Net + WavLM</td>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T3.1.2.1.2.1" class="ltx_text ltx_font_bold">33.3% (20.2%-49.8%)</span></td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<td id="S4.T3.1.3.2.1" class="ltx_td ltx_align_left">U-Net</td>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_left">35.2% (17.2%-56.8%)</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<td id="S4.T3.1.4.3.1" class="ltx_td ltx_align_left">Ground Truth</td>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_left">49.7% (34.8%-66.6%)</td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<td id="S4.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_border_bb">U-Net + WavLM (S)</td>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_align_left ltx_border_bb">50.1% (28.0%-72.8%)</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Qualitative Evaluation</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Despite relying on the output of the baseline segmentation algorithm as the training targets,
our segmentation methods performed better than the baseline algorithm when
evaluated on downstream speech synthesis. We hypothesize that this is because the baseline
segmentations have high amounts of jitter and inconsistencies across frames, and are sometimes even physiologically implausible. In comparison, the estimates of the deep learning approaches do not have the same level of frame-dependent noise, possibly explaining why they are better suited for
building downstream methods. To validate this hypothesis with a subjective evaluation, we ran a
one-tailed perceptual test for statistical significance where
participants looked at two video animations of vocal tract movements in
side-by-side panels (one with the baseline labels, and the other with outputs
of our segmentation method). The participants then selected which rendering
is a more accurate representation of the associated audio. Each participant repeated this process for five test examples. Our
results reveal the participants (<span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_italic">n=</span>21)
prefer the outputs of our algorithm over the baseline
segmentations (p <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mo id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><lt id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">&lt;</annotation></semantics></math> 0.001). For visualization of these results, we invite you to
watch our demo video at <a href="rishiraij.github.io/multimodal-mri-avatar" title="" class="ltx_ref ltx_url ltx_font_typewriter">rishiraij.github.io/multimodal-mri-avatar</a>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we looked at developing a generalizable articulatory segmentation algorithm from RT-MRI videos of the vocal tract. We used the limited existing articulatory labeling to train vision-based and multimodal models which efficiently and accurately extract physiological features from MRI videos of unseen speakers. Through speech synthesis, we demonstrate that our approach results in higher quality segmentations for downstream speech tasks than existing baselines, while also being more accurate representations of speech audio. While MRI-based articulatory modeling is less studied than other approaches such as EMA, we hope that our released labeling of 75 speakers will allow future work in speech modeling and linguistics to take advantage of the more-complete physiological representation that RT-MRI provides.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Suemitsu and J. Dang, ``A real-time articulatory visual feedback approach with target presentation for second language pronunciation learning,'' <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">The Journal of the Acoustical Society of America</em>, 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. S. Levitt and W. F. Katz, ``The effects of EMA-based augmented visual feedback on the English speakers' acquisition of the Japanese flap: a perceptual study,'' in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2010</em>, 2010, pp. 1862–1865.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
B. Gick, B. M. Bernhardt, P. Bacsfalvi, and I. Wilson, ``11. ultrasound imaging applications in second language acquisition,'' in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Phonology and Second Language Acquisition</em>, 2008. [Online]. Available: <a target="_blank" href="https://api.semanticscholar.org/CorpusID:63438867" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:63438867</a>

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. L. Metzger, K. T. Littlejohn, A. B. Silva, D. A. Moses, M. P. Seaton, R. Wang, M. E. Dougherty, J. R. Liu, P. Wu, M. A. Berger, I. Zhuravleva, A. Tu-Chan, K. Ganguly, G. K. Anumanchipalli, and E. F. Chang, ``A high-performance neuroprosthesis for speech decoding and avatar control.'' <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Nature</em>, vol. 620, pp. 1037–1046, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
U. Desai, C. Yarra, and P. Ghosh, ``Concatenative articulatory video synthesis using real-time mri data for spoken language training,'' in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ICASSP</em>, 04 2018, pp. 4999–5003.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. Chandana, C. Yarra, R. Aggarwal, S. K. Mittal, N. Kausthubha, K. Raseena, A. Singh, and P. K. Ghosh, ``Automatic visual augmentation for concatenation based synthesized articulatory videos from real-time mri data for spoken language training,'' in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
C. J. Cho, P. Wu, A. Mohamed, and G. K. Anumanchipalli, ``Evidence of vocal tract articulation in self-supervised learning of speech,'' in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
G. K. Anumanchipalli, J. Chartier, and E. F. Chang, ``Speech synthesis from neural decoding of spoken sentences,'' <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Nature</em>, vol. 568, no. 7753, p. 493–498, Apr. 2019. [Online]. Available: <a target="_blank" href="http://dx.doi.org/10.1038/s41586-019-1119-1" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dx.doi.org/10.1038/s41586-019-1119-1</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Y. Lu, C. E. Wiltshire, K. E. Watkins, M. Chiew, and L. Goldstein, ``Characteristics of articulatory gestures in stuttered speech: A case study using real-time magnetic resonance imaging,'' <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Journal of Communication Disorders</em>, vol. 97, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Richard, C. Lea, S. Ma, J. Gall, F. de la Torre, and Y. Sheikh, ``Audio- and gaze-driven facial animation of codec avatars,'' in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, January 2021, pp. 41–50.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
S. Medina, D. Tome, C. Stoll, M. Tiede, K. Munhall, A. Hauptmann, and I. Matthews, ``Speech driven tongue animation,'' in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.   IEEE, Jun. 2022. [Online]. Available: <a target="_blank" href="https://doi.org/10.1109/cvpr52688.2022.01976" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/cvpr52688.2022.01976</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. S. Perkell, M. H. Cohen, M. A. Svirsky, M. L. Matthies, I. Garabieta, and M. T. Jackson, ``Electromagnetic midsagittal articulometer systems for transducing speech articulatory movements,'' <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">The Journal of the Acoustical Society of America</em>, vol. 92, no. 6, pp. 3078–3096, 1992.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
D. Gaddy and D. Klein, ``An improved model for voicing silent speech,'' in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</em>, C. Zong, F. Xia, W. Li, and R. Navigli, Eds.   Online: Association for Computational Linguistics, Aug. 2021, pp. 175–181. [Online]. Available: <a target="_blank" href="https://aclanthology.org/2021.acl-short.23" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.acl-short.23</a>

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
V. Ramanarayanan, L. Goldstein, D. Byrd, and S. S. Narayanan, ``An investigation of articulatory setting using real-time magnetic resonance imaging,'' <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">The Journal of the Acoustical Society of America</em>, vol. 134, no. 1, 2013.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
P. Wu, T. Li, Y. Lu, Y. Zhang, J. Lian, A. W. Black, L. Goldstein, S. Watanabe, and G. K. Anumanchipalli, ``Deep speech synthesis from mri-based articulatory representations,'' 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
E. Bresch and S. Narayanan, ``Region segmentation in the frequency domain applied to upper airway real-time magnetic resonance images,'' <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Medical Imaging</em>, vol. 28, no. 3, pp. 323–338, 2009.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
S. Narayanan, A. Toutios, V. Ramanarayanan, A. Lammert, J. Kim, S. Lee, K. Nayak, Y.-C. Kim, Y. Zhu, L. Goldstein, D. Byrd, E. Bresch, P. Ghosh, A. Katsamanis, and M. Proctor, ``Real-time magnetic resonance imaging and electromagnetic articulography database for speech production research (tc),'' <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">The Journal of the Acoustical Society of America</em>, vol. 136, no. 3, p. 1307–1311, Sep. 2014. [Online]. Available: <a target="_blank" href="http://dx.doi.org/10.1121/1.4890284" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dx.doi.org/10.1121/1.4890284</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Y. Yu, A. H. Shandiz, and L. Tóth, ``Reconstructing speech from real-time articulatory mri using neural vocoders,'' 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Y. Otani, S. Sawada, H. Ohmura, and K. Katsurada, ``Speech Synthesis from Articulatory Movements Recorded by Real-time MRI,'' in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH 2023</em>, 2023, pp. 127–131.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
O. Ronneberger, P. Fischer, and T. Brox, ``U-net: Convolutional networks for biomedical image segmentation,'' 2015.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Y. Lim, A. Toutios, Y. Bliesener, Y. Tian, S. G. Lingala, C. Vaz, T. Sorensen, M. Oh, S. Harper, W. Chen, Y. Lee, J. Töger, M. L. Monteserin, C. Smith, B. Godinez, L. Goldstein, D. Byrd, K. S. Nayak, and S. S. Narayanan, ``A multispeaker dataset of raw and reconstructed speech production real-time mri video and 3d volumetric images,'' <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Scientific Data</em>, vol. 8, no. 1, jul 2021. [Online]. Available: <a target="_blank" href="http://dx.doi.org/10.1038/s41597-021-00976-x" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dx.doi.org/10.1038/s41597-021-00976-x</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz, B. Glocker, and D. Rueckert, ``Attention u-net: Learning where to look for the pancreas,'' 2018.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S. A. Hebbar, R. Sharma, K. Somandepalli, A. Toutios, and S. Narayanan, ``Vocal tract articulatory contour detection in real-time magnetic resonance images using spatio-temporal context,'' in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2020, pp. 7354–7358.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
F. Loewenich and F. Maire, ``A head-tracker based on the lucas-kanade optical flow algorithm,'' in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2006 Conference on Advances in Intelligent IT: Active Media Technology 2006</em>.   NLD: IOS Press, 2006, p. 25–30.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Y. Chen, D. Zhao, and H. Li, ``Deep kalman filter with optical flow for multiple object tracking,'' in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)</em>, 2019, pp. 3036–3041.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, ``Deep residual learning for image recognition,'' 2015.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, ``A convnet for the 2020s,'' 2022.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao, J. Wu, L. Zhou, S. Ren, Y. Qian, Y. Qian, J. Wu, M. Zeng, X. Yu, and F. Wei, ``Wavlm: Large-scale self-supervised pre-training for full stack speech processing,'' <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, vol. 16, no. 6, p. 1505–1518, Oct. 2022. [Online]. Available: <a target="_blank" href="http://dx.doi.org/10.1109/JSTSP.2022.3188113" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dx.doi.org/10.1109/JSTSP.2022.3188113</a>

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Anonymous, ``Transfer learning for articulatory synthesis,'' 2024, preprint. [Online]. Available: <a target="_blank" href="https://openreview.net/pdf?id=pEW9cXrY4O" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/pdf?id=pEW9cXrY4O</a>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, ``Robust speech recognition via large-scale weak supervision,'' 2022.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.15753" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.15754" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.15754">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.15754" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.15755" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 22:18:09 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
