<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.14083] emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition</title><meta property="og:description" content="Speech Emotion Recognition (SER) is crucial for enabling computers to understand the emotions conveyed in human communication. With recent advancements in Deep Learning (DL), the performance of SER models has significa…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.14083">

<!--Generated on Fri Apr  5 13:35:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
speech emotion recognition,  neural architecture search,  deep learning,  DARTS
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">emoDARTS: Joint Optimisation of CNN &amp;
Sequential Neural Network Architectures for Superior Speech Emotion Recognition</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Thejan Rajapakshe<sup id="id3.3.id1" class="ltx_sup"><span id="id3.3.id1.1" class="ltx_text ltx_font_italic">∗</span></sup>, ,
Rajib Rana, , 
<br class="ltx_break">Sara Khalifa, ,
Berrak Sisman, ,
<br class="ltx_break">Björn W. Schuller, ,
and Carlos Busso, 
<br class="ltx_break">
</span><span class="ltx_author_notes">T. Rajapakshe and R. Rana are affiliated with the University of Southern Queensland, Australia.S. Khalifa is affiliated with Queensland University of Technology, Australia.B. Sisman is affiliated with The University of Texas at Dallas, USA.B. Schuller is with CHI – Chair of Health Informatics, MRI, Technical University of Munich, Germany, Munich Data Science Institute (MDSI), Germany, Munich Center for Machine Learning (MCML), Germany and GLAM – Group on Language, Audio, &amp; Music, Imperial College London, UKC. Busso is affiliated with The University of Texas at Dallas, USA.<sup id="id4.4.id1" class="ltx_sup"><span id="id4.4.id1.1" class="ltx_text ltx_font_italic">∗</span></sup>Thejan.Rajapakshe@unisq.edu.auManuscript submitted to IEEE Transactions on Affective Computing on February 19, 2024</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Speech Emotion Recognition (SER) is crucial for enabling computers to understand the emotions conveyed in human communication. With recent advancements in Deep Learning (DL), the performance of SER models has significantly improved. However, designing an optimal DL architecture requires specialised knowledge and experimental assessments. Fortunately, Neural Architecture Search (NAS) provides a potential solution for automatically determining the best DL model. The Differentiable Architecture Search (DARTS) is a particularly efficient method for discovering optimal models. This study presents emoDARTS, a DARTS-optimised joint CNN and Sequential Neural Network (SeqNN: LSTM, RNN) architecture that enhances SER performance. The literature supports the selection of CNN and LSTM coupling to improve performance.</p>
<p id="id6.id2" class="ltx_p">While DARTS has previously been used to choose CNN and LSTM operations independently, our technique adds a novel mechanism for selecting CNN and SeqNN operations in conjunction using DARTS. Unlike earlier work, we do not impose limits on the layer order of the CNN. Instead, we let DARTS choose the best layer order inside the DARTS cell. We demonstrate that emoDARTS outperforms conventionally designed CNN-LSTM models and surpasses the best-reported SER results achieved through DARTS on CNN-LSTM by evaluating our approach on the IEMOCAP, MSP-IMPROV, and MSP-Podcast datasets.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
speech emotion recognition, neural architecture search, deep learning, DARTS

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recognising the emotional nuances embedded in speech is a fundamental, yet complex challenge. Over the last decade, the field of Speech Emotion Recognition (SER) has experienced significant strides, predominantly driven by the exponential growth of deep learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. A key breakthrough facilitated by deep learning is its capability to automatically learn features, departing from the traditional reliance on manually crafted features shaped by human perceptions of speech signals. Nevertheless, determining the optimal deep-learning architecture for SER remains a challenging task that warrants attention. Conventional approaches involve iterative modifications and recursive training of models until an optimal configuration is found. However, this approach becomes prohibitively time-consuming due to the extensive training and testing required for numerous configurations.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">An alternative to the conventional approach is the “Neural Architecture Search” (NAS), which can help discover optimal neural networks for a given task. The idea is to find the models’ architecture to minimise the loss. In NAS, search is done over a discrete set of candidate operations, which requires the model to be trained on a specific configuration before moving on to the next configuration. Nevertheless, this approach demands considerable time and resources.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The Differentiable Architecture Search (DARTS) is a method that has been developed to optimise the search for a neural network architecture. It allows for the relaxation of the discrete set of candidate operations, making the space continuous and reducing the computation time significantly, from 2,000 GPU days to just 2-3 GPU days. This is a major improvement from the previous methods of reinforcement learning or evolution algorithm, which required 2,000 and 3,150 GPU days, respectively. Additionally, through network optimisation DARTS has the potential to offer significantly high SER accuracy, which is currently quite low and needs improvement. These two points serve as motivation to use DARTS for SER.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Additionally, previous studies have shown that a multi-temporal Convolutional Neural Network (CNN) stacked on a Long Short-Term Memory Network (LSTM)
can capture contextual information at multiple temporal resolutions, complementing LSTM for modelling long-term contextual information, thus offering improved performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Sequential Neural Networks (SeqNN) like Recurrent Neural Networks (RNN)
or LSTM can easily identify the patterns of a sequential stream of data.
This paper takes a pioneering step by leveraging DARTS for a novel joint CNN–SeqNN configuration, named “emoDARTS”, as depicted in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, with an attention network seamlessly integrated into the SeqNN component to further elevate its performance.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2403.14083/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="177" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The proposed architecture of emoDARTS passes the input features to the CNN component through the SeqNN component and finally to a dense layer. The optimum CNN and SeqNN operations are selected by DARTS jointly.</figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The investigation of DARTS within the SER domain is minimal and invites further inquiry to uncover the potential for improving SER performance.
DARTS has only recently been employed in SER tasks to improve models, as recently as 2022 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, wherein the researchers have mostly applied DARTS separately on CNNs and RNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Hence, the viability of utilising DARTS jointly for CNN and SeqNN requires exploration. While there is a lone study that explores the joint optimisation of CNN and LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, it imposes constraints on the layer order for the CNN within the DARTS component, thereby limiting the full potential of DARTS. In response to this limitation, our paper takes on the challenge of optimising this joint configuration without such constraints. The contributions of this paper are summarised as follows.</p>
</div>
<div id="S1.p6" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">This paper proposes a novel DARTS-optimised joint CNN and SeqNN architecture, emoDARTS, achieving greater autonomy to DARTS in selecting optimal network configurations.
</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We demonstrate the robust generalisation capabilities of the proposed emoDARTS model by testing it on three widely used datasets: IEMOCAP, MSP-IMPROV, and MSP-Podcast.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Experimental results demonstrate that emoDARTS
achieves
considerably higher SER accuracy than humans designing the CNN-LSTM configuration. It also outperforms the best-reported SER results achieved using DARTS on CNN-LSTM.
</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This section delves into the existing literature on using DARTS and NAS for SER. Notably, our exploration reveals a limited number of papers in this space. We therefore extend our review to encompass relevant papers in related fields to provide a comprehensive perspective. For completeness, we also include studies employing CNNs, LSTM networks, and their joint utilisation for SER.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span><span id="S2.SS1.1.1" class="ltx_text ltx_font_italic">Speech Emotion Recognition using CNN and LSTM</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">One of the earliest uses of CNN networks in SER is reported by Zheng et al. in 2015 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
The authors introduced a spectrum generated from an audio signal to a CNN network and output the recognised emotion.
The authors report that they can surpass the SVM-based classification performance and reach 40% classification accuracy for a five-class classification using the IEMOCAP dataset.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The earliest work combining CNN and LSTM for SER is by Trigeorgis et al. in 2016 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. The authors show an impressive improvement by a fully self-learnt representation over traditional expert-crafted features on dimensional emotion recognition.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Zhaoa et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> show that using CNN and LSTM networks combined in the same SER model produces better results than using only CNN. Using the IEMOCAP dataset, they obtained a speaker-independent accuracy of 52% by using a log-Mel spectrogram as the input feature. Their SER approach utilises an LSTM layer to learn contextual dependencies in local features, while a CNN-based layer learns the local features.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span><span id="S2.SS2.1.1" class="ltx_text ltx_font_italic">Application of NAS and DARTS in SER and Related Fields</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The first paper suggesting NAS in SER was by Zhang et al. in 2018 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. The authors employ a controller network that shapes the architecture by the number of layers and nodes per layer and the hyperparameter activation function of a child network by reinforcement learning. They show an improvement over human-designed architectures and random searches of these.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.2" class="ltx_p">Zoph and Le <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> use reinforcement learning to optimise an RNN network that develops model architectures to maximise the resulting accuracy of the generated model. As a result, they develop outstanding models for the CIFAR-10 and Penn Treebank datasets. They were able to develop a convolutional network architecture for the CIFAR-10 dataset which has a <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="3.65" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mn id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">3.65</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><cn type="float" id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">3.65</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">3.65</annotation></semantics></math> error rate and a recurrent network architecture for Penn Treebank with <math id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="62.4" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><mn id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">62.4</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><cn type="float" id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">62.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">62.4</annotation></semantics></math> perplexity.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.2" class="ltx_p">Even though NAS is primarily used to find optimised architecture for complex and large models, researchers have also studied the possibility of using NAS to design smaller deep neural network models. Liberis et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> develop a NAS system called <math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><mi id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><ci id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">\mu</annotation></semantics></math>NAS to design smaller neural architectures that can run on microcontroller units. They improve the top-1 accuracy by <math id="S2.SS2.p3.2.m2.1" class="ltx_Math" alttext="4.8\%" display="inline"><semantics id="S2.SS2.p3.2.m2.1a"><mrow id="S2.SS2.p3.2.m2.1.1" xref="S2.SS2.p3.2.m2.1.1.cmml"><mn id="S2.SS2.p3.2.m2.1.1.2" xref="S2.SS2.p3.2.m2.1.1.2.cmml">4.8</mn><mo id="S2.SS2.p3.2.m2.1.1.1" xref="S2.SS2.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m2.1b"><apply id="S2.SS2.p3.2.m2.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1"><csymbol cd="latexml" id="S2.SS2.p3.2.m2.1.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1.1">percent</csymbol><cn type="float" id="S2.SS2.p3.2.m2.1.1.2.cmml" xref="S2.SS2.p3.2.m2.1.1.2">4.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m2.1c">4.8\%</annotation></semantics></math> in image classification problems while reducing the memory footprint up to 13 times. Similarly, Gong et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> study the feasibility of using NAS for reducing deep learning models to deploy on resource-constrained edge devices.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Traditional NAS consumes much computational power and time to achieve the optimal model for a given problem. In 2018, Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> came up with a differentiable approach to solving the optimisation by continuous relaxation of the architecture representation. This approach is more compute efficient and high performing as the search space is not discrete and non-differentiable. They produce high-performing CNN and RNN architectures for tasks such as image recognition and language modelling within a fraction of the search cost of traditional NAS algorithms. DARTS has been popular in the past three years with many studies carried on for extending and improving the algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> proposed a uniform path dropout strategy to optimise candidate architecture. They use SER as their DARTS application and the IEMOCAP dataset to develop an SER model with an accuracy of <math id="S2.SS2.p5.1.m1.1" class="ltx_Math" alttext="56.28\%" display="inline"><semantics id="S2.SS2.p5.1.m1.1a"><mrow id="S2.SS2.p5.1.m1.1.1" xref="S2.SS2.p5.1.m1.1.1.cmml"><mn id="S2.SS2.p5.1.m1.1.1.2" xref="S2.SS2.p5.1.m1.1.1.2.cmml">56.28</mn><mo id="S2.SS2.p5.1.m1.1.1.1" xref="S2.SS2.p5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.1.m1.1b"><apply id="S2.SS2.p5.1.m1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1"><csymbol cd="latexml" id="S2.SS2.p5.1.m1.1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1.1">percent</csymbol><cn type="float" id="S2.SS2.p5.1.m1.1.1.2.cmml" xref="S2.SS2.p5.1.m1.1.1.2">56.28</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.1.m1.1c">56.28\%</annotation></semantics></math> for a four-class classification problem using discrete Fourier transform spectrograms extracted from audio as input. In their work, the authors specify layer order as two convolution layers at first, followed by a max-pooling layer, a convolution layer. They use DARTS to select the optimum parameters for each layer. We, on the other hand, do not specify the layer sequence and instead enable DARTS to select the ideal design with minimal interference.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Summary and focus on the literature on NAS, DARTS, and speech emotion recognition.</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" rowspan="2"><span id="S2.T1.1.1.1.1.1" class="ltx_text">Paper</span></th>
<td id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="4">Focus</td>
<td id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="3">Dataset</td>
</tr>
<tr id="S2.T1.1.2.2" class="ltx_tr">
<td id="S2.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">SER</td>
<td id="S2.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">NAS</td>
<td id="S2.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">DARTS</td>
<td id="S2.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Joint Opt. of CNN &amp; SeqNN.</td>
<td id="S2.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">IEMOCAP</td>
<td id="S2.T1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">MSP-IMPROV</td>
<td id="S2.T1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">MSP-Podcast</td>
</tr>
<tr id="S2.T1.1.3.3" class="ltx_tr">
<th id="S2.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Zoph and Le 2016 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</th>
<td id="S2.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td id="S2.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
</tr>
<tr id="S2.T1.1.4.4" class="ltx_tr">
<th id="S2.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Zhang et al. 2018 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</th>
<td id="S2.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td id="S2.T1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td id="S2.T1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td id="S2.T1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
</tr>
<tr id="S2.T1.1.5.5" class="ltx_tr">
<th id="S2.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Liu et al. 2018 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</th>
<td id="S2.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td id="S2.T1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td id="S2.T1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
</tr>
<tr id="S2.T1.1.6.6" class="ltx_tr">
<th id="S2.T1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Gong et al. 2019 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</th>
<td id="S2.T1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td id="S2.T1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
</tr>
<tr id="S2.T1.1.7.7" class="ltx_tr">
<th id="S2.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Liberis et al. 2020 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</th>
<td id="S2.T1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td id="S2.T1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
</tr>
<tr id="S2.T1.1.8.8" class="ltx_tr">
<th id="S2.T1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Wu et al. 2022 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</th>
<td id="S2.T1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td id="S2.T1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td id="S2.T1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td id="S2.T1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td id="S2.T1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td id="S2.T1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.8.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
</tr>
<tr id="S2.T1.1.9.9" class="ltx_tr">
<th id="S2.T1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Sun et al. 2023 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</th>
<td id="S2.T1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td id="S2.T1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td id="S2.T1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td id="S2.T1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✓</td>
<td id="S2.T1.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
<td id="S2.T1.1.9.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✗</td>
</tr>
<tr id="S2.T1.1.10.10" class="ltx_tr">
<th id="S2.T1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S2.T1.1.10.10.1.1" class="ltx_text ltx_font_bold">emoDARTS</span></th>
<td id="S2.T1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✔</td>
<td id="S2.T1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✔</td>
<td id="S2.T1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✔</td>
<td id="S2.T1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✔</td>
<td id="S2.T1.1.10.10.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✔</td>
<td id="S2.T1.1.10.10.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✔</td>
<td id="S2.T1.1.10.10.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">✔</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.2" class="ltx_p">EmotionNAS is a two-branch NAS strategy introduced by Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> in 2023. The authors use DARTS to optimise their two models in two branches, the CNN model and RNN model, which use a spectrogram and a waveform as inputs, respectively. They obtained an unweighted accuracy of <math id="S2.SS2.p6.1.m1.1" class="ltx_Math" alttext="72.1\%" display="inline"><semantics id="S2.SS2.p6.1.m1.1a"><mrow id="S2.SS2.p6.1.m1.1.1" xref="S2.SS2.p6.1.m1.1.1.cmml"><mn id="S2.SS2.p6.1.m1.1.1.2" xref="S2.SS2.p6.1.m1.1.1.2.cmml">72.1</mn><mo id="S2.SS2.p6.1.m1.1.1.1" xref="S2.SS2.p6.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.1.m1.1b"><apply id="S2.SS2.p6.1.m1.1.1.cmml" xref="S2.SS2.p6.1.m1.1.1"><csymbol cd="latexml" id="S2.SS2.p6.1.m1.1.1.1.cmml" xref="S2.SS2.p6.1.m1.1.1.1">percent</csymbol><cn type="float" id="S2.SS2.p6.1.m1.1.1.2.cmml" xref="S2.SS2.p6.1.m1.1.1.2">72.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.1.m1.1c">72.1\%</annotation></semantics></math> from the combined model for the IEMOCAP dataset. They also report the performance of <math id="S2.SS2.p6.2.m2.1" class="ltx_Math" alttext="63.2\%" display="inline"><semantics id="S2.SS2.p6.2.m2.1a"><mrow id="S2.SS2.p6.2.m2.1.1" xref="S2.SS2.p6.2.m2.1.1.cmml"><mn id="S2.SS2.p6.2.m2.1.1.2" xref="S2.SS2.p6.2.m2.1.1.2.cmml">63.2</mn><mo id="S2.SS2.p6.2.m2.1.1.1" xref="S2.SS2.p6.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.2.m2.1b"><apply id="S2.SS2.p6.2.m2.1.1.cmml" xref="S2.SS2.p6.2.m2.1.1"><csymbol cd="latexml" id="S2.SS2.p6.2.m2.1.1.1.cmml" xref="S2.SS2.p6.2.m2.1.1.1">percent</csymbol><cn type="float" id="S2.SS2.p6.2.m2.1.1.2.cmml" xref="S2.SS2.p6.2.m2.1.1.2">63.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.2.m2.1c">63.2\%</annotation></semantics></math> in the spectrogram branch, which only uses a CNN component.
The main difference between our approach and the study by Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> is that we use a SeqNN component coupled in series with the CNN layer as in Figure <a href="#S3.F2" title="Figure 2 ‣ 3 emoDARTS Framework ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> while Sun et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> use an RNN layer in parallel to the CNN layer in a different branch.</p>
</div>
<div id="S2.SS2.p7" class="ltx_para">
<p id="S2.SS2.p7.1" class="ltx_p">We conducted preliminary research to determine the feasibility of utilising DARTS for SER in a CNN-LSTM architecture, where we only optimised the CNN network using DARTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
This paper extends the idea of using DARTS in SER but with more relaxation in the SeqNN component by jointly optimising the whole architecture.</p>
</div>
<div id="S2.SS2.p8" class="ltx_para">
<p id="S2.SS2.p8.1" class="ltx_p">In recent years, the literature has highlighted the use of attention networks in SER, which has provided superior outcomes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. We added an attention network component to the DARTS search scope to discover whether it improves performance.
Zou et. al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> have introduced a concept called ‘co-attention’ where many separate inputs from multimodal inputs are fused by co-attention. They used three sets of features MFCC, spectrogram, and Wav2Vec2 features from the IEMOCAP dataset and obtained 72.70% accuracy.
Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> have utilised an attention-based bi-directional LSTM followed by a CNN layer for a SER problem. They have achieved a significant performance of <math id="S2.SS2.p8.1.m1.1" class="ltx_Math" alttext="66.27\%" display="inline"><semantics id="S2.SS2.p8.1.m1.1a"><mrow id="S2.SS2.p8.1.m1.1.1" xref="S2.SS2.p8.1.m1.1.1.cmml"><mn id="S2.SS2.p8.1.m1.1.1.2" xref="S2.SS2.p8.1.m1.1.1.2.cmml">66.27</mn><mo id="S2.SS2.p8.1.m1.1.1.1" xref="S2.SS2.p8.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p8.1.m1.1b"><apply id="S2.SS2.p8.1.m1.1.1.cmml" xref="S2.SS2.p8.1.m1.1.1"><csymbol cd="latexml" id="S2.SS2.p8.1.m1.1.1.1.cmml" xref="S2.SS2.p8.1.m1.1.1.1">percent</csymbol><cn type="float" id="S2.SS2.p8.1.m1.1.1.2.cmml" xref="S2.SS2.p8.1.m1.1.1.2">66.27</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p8.1.m1.1c">66.27\%</annotation></semantics></math> for the IEMOCAP Dataset. Their idea of ‘CNN - LSTM attention’ paved the foundation for our model architecture.</p>
</div>
<div id="S2.SS2.p9" class="ltx_para">
<p id="S2.SS2.p9.1" class="ltx_p">In Table <a href="#S2.T1" title="TABLE I ‣ 2.2 Application of NAS and DARTS in SER and Related Fields ‣ 2 Related Work ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, we briefly compare the existing studies with emoDARTS. The comparison clearly shows that,</p>
</div>
<div id="S2.SS2.p10" class="ltx_para">
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">While some studies employ NAS for SER, the utilisation of DARTS in SER is notably limited.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Singularly, one study has explored the concept of jointly optimising CNN and SeqNN using DARTS for SER, in which the researchers specified the layer order. However, in our study, we let DARTS determine the optimal network from a relaxed search scope which enables it to select any operation in search space at the optimum layer.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">Most existing studies primarily focus on the IEMOCAP dataset. In contrast, our study uniquely incorporates three widely recognised SER datasets: IEMOCAP, MSP-IMPROV, and MSP-Podcast to demonstrate the generalisation power of emoDARTS.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">emoDARTS Framework</span>
</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2403.14083/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="266" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The emoDARTS architecture comprises input features processed through CNN, SeqNN, and Dense layers and it utilises DARTS for jointly optimising the CNN and SeqNN components.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The proposed ‘emoDARTS’ uses DARTS to improve SER using a CNN-SeqNN network, which was motivated by studies that showed increased SER performance when CNN and LSTM layers were combined <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. We represent our network as a multi-component DARTS network, with the input fed into a CNN component and the output from the CNN component fed into a SeqNN component, but all components are optimised jointly during the architecture search phase, delivering an optimal architecture (Figure <a href="#S3.F2" title="Figure 2 ‣ 3 emoDARTS Framework ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.7" class="ltx_p">DARTS uses a differentiable approach to network optimisation. A computation cell is the DARTS algorithm’s fundamental unit. It aims to optimise the cell so that the architecture can function to its maximum performance. A DARTS cell is described as a directed graph, with each node representing a feature (representation) and each edge representing an operation that can be performed to a representation.
One unique feature of this network is that each node is connected to all of its previous nodes by an edge, as seen in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 emoDARTS Framework ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a). If the output of the node <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">j</annotation></semantics></math> is <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="x^{(j)}" display="inline"><semantics id="S3.p2.2.m2.1a"><msup id="S3.p2.2.m2.1.2" xref="S3.p2.2.m2.1.2.cmml"><mi id="S3.p2.2.m2.1.2.2" xref="S3.p2.2.m2.1.2.2.cmml">x</mi><mrow id="S3.p2.2.m2.1.1.1.3" xref="S3.p2.2.m2.1.2.cmml"><mo stretchy="false" id="S3.p2.2.m2.1.1.1.3.1" xref="S3.p2.2.m2.1.2.cmml">(</mo><mi id="S3.p2.2.m2.1.1.1.1" xref="S3.p2.2.m2.1.1.1.1.cmml">j</mi><mo stretchy="false" id="S3.p2.2.m2.1.1.1.3.2" xref="S3.p2.2.m2.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.2.cmml" xref="S3.p2.2.m2.1.2"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.2.1.cmml" xref="S3.p2.2.m2.1.2">superscript</csymbol><ci id="S3.p2.2.m2.1.2.2.cmml" xref="S3.p2.2.m2.1.2.2">𝑥</ci><ci id="S3.p2.2.m2.1.1.1.1.cmml" xref="S3.p2.2.m2.1.1.1.1">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">x^{(j)}</annotation></semantics></math> and the operation ‘<math id="S3.p2.3.m3.1" class="ltx_Math" alttext="o" display="inline"><semantics id="S3.p2.3.m3.1a"><mi id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><ci id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">𝑜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">o</annotation></semantics></math>’ on the edge connecting the nodes <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.p2.4.m4.1a"><mi id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><ci id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">i</annotation></semantics></math> and <math id="S3.p2.5.m5.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.p2.5.m5.1a"><mi id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><ci id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">j</annotation></semantics></math> is <math id="S3.p2.6.m6.2" class="ltx_Math" alttext="o^{(i,j)}" display="inline"><semantics id="S3.p2.6.m6.2a"><msup id="S3.p2.6.m6.2.3" xref="S3.p2.6.m6.2.3.cmml"><mi id="S3.p2.6.m6.2.3.2" xref="S3.p2.6.m6.2.3.2.cmml">o</mi><mrow id="S3.p2.6.m6.2.2.2.4" xref="S3.p2.6.m6.2.2.2.3.cmml"><mo stretchy="false" id="S3.p2.6.m6.2.2.2.4.1" xref="S3.p2.6.m6.2.2.2.3.cmml">(</mo><mi id="S3.p2.6.m6.1.1.1.1" xref="S3.p2.6.m6.1.1.1.1.cmml">i</mi><mo id="S3.p2.6.m6.2.2.2.4.2" xref="S3.p2.6.m6.2.2.2.3.cmml">,</mo><mi id="S3.p2.6.m6.2.2.2.2" xref="S3.p2.6.m6.2.2.2.2.cmml">j</mi><mo stretchy="false" id="S3.p2.6.m6.2.2.2.4.3" xref="S3.p2.6.m6.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.2b"><apply id="S3.p2.6.m6.2.3.cmml" xref="S3.p2.6.m6.2.3"><csymbol cd="ambiguous" id="S3.p2.6.m6.2.3.1.cmml" xref="S3.p2.6.m6.2.3">superscript</csymbol><ci id="S3.p2.6.m6.2.3.2.cmml" xref="S3.p2.6.m6.2.3.2">𝑜</ci><interval closure="open" id="S3.p2.6.m6.2.2.2.3.cmml" xref="S3.p2.6.m6.2.2.2.4"><ci id="S3.p2.6.m6.1.1.1.1.cmml" xref="S3.p2.6.m6.1.1.1.1">𝑖</ci><ci id="S3.p2.6.m6.2.2.2.2.cmml" xref="S3.p2.6.m6.2.2.2.2">𝑗</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.2c">o^{(i,j)}</annotation></semantics></math>, <math id="S3.p2.7.m7.1" class="ltx_Math" alttext="x^{(j)}" display="inline"><semantics id="S3.p2.7.m7.1a"><msup id="S3.p2.7.m7.1.2" xref="S3.p2.7.m7.1.2.cmml"><mi id="S3.p2.7.m7.1.2.2" xref="S3.p2.7.m7.1.2.2.cmml">x</mi><mrow id="S3.p2.7.m7.1.1.1.3" xref="S3.p2.7.m7.1.2.cmml"><mo stretchy="false" id="S3.p2.7.m7.1.1.1.3.1" xref="S3.p2.7.m7.1.2.cmml">(</mo><mi id="S3.p2.7.m7.1.1.1.1" xref="S3.p2.7.m7.1.1.1.1.cmml">j</mi><mo stretchy="false" id="S3.p2.7.m7.1.1.1.3.2" xref="S3.p2.7.m7.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p2.7.m7.1b"><apply id="S3.p2.7.m7.1.2.cmml" xref="S3.p2.7.m7.1.2"><csymbol cd="ambiguous" id="S3.p2.7.m7.1.2.1.cmml" xref="S3.p2.7.m7.1.2">superscript</csymbol><ci id="S3.p2.7.m7.1.2.2.cmml" xref="S3.p2.7.m7.1.2.2">𝑥</ci><ci id="S3.p2.7.m7.1.1.1.1.cmml" xref="S3.p2.7.m7.1.1.1.1">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m7.1c">x^{(j)}</annotation></semantics></math> can be obtained by the Equation <a href="#S3.E1" title="In 3 emoDARTS Framework ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.5" class="ltx_Math" alttext="\centering x^{(j)}=\sum_{i&lt;j}o^{(i,j)}(x^{(i)})\@add@centering" display="block"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml"><msup id="S3.E1.m1.5.5.3" xref="S3.E1.m1.5.5.3.cmml"><mi id="S3.E1.m1.5.5.3.2" xref="S3.E1.m1.5.5.3.2.cmml">x</mi><mrow id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.5.5.3.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.3.1" xref="S3.E1.m1.5.5.3.cmml">(</mo><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">j</mi><mo stretchy="false" id="S3.E1.m1.1.1.1.3.2" xref="S3.E1.m1.5.5.3.cmml">)</mo></mrow></msup><mo rspace="0.111em" id="S3.E1.m1.5.5.2" xref="S3.E1.m1.5.5.2.cmml">=</mo><mrow id="S3.E1.m1.5.5.1" xref="S3.E1.m1.5.5.1.cmml"><munder id="S3.E1.m1.5.5.1.2" xref="S3.E1.m1.5.5.1.2.cmml"><mo movablelimits="false" id="S3.E1.m1.5.5.1.2.2" xref="S3.E1.m1.5.5.1.2.2.cmml">∑</mo><mrow id="S3.E1.m1.5.5.1.2.3" xref="S3.E1.m1.5.5.1.2.3.cmml"><mi id="S3.E1.m1.5.5.1.2.3.2" xref="S3.E1.m1.5.5.1.2.3.2.cmml">i</mi><mo id="S3.E1.m1.5.5.1.2.3.1" xref="S3.E1.m1.5.5.1.2.3.1.cmml">&lt;</mo><mi id="S3.E1.m1.5.5.1.2.3.3" xref="S3.E1.m1.5.5.1.2.3.3.cmml">j</mi></mrow></munder><mrow id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.cmml"><msup id="S3.E1.m1.5.5.1.1.3" xref="S3.E1.m1.5.5.1.1.3.cmml"><mi id="S3.E1.m1.5.5.1.1.3.2" xref="S3.E1.m1.5.5.1.1.3.2.cmml">o</mi><mrow id="S3.E1.m1.3.3.2.4" xref="S3.E1.m1.3.3.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.2.4.1" xref="S3.E1.m1.3.3.2.3.cmml">(</mo><mi id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml">i</mi><mo id="S3.E1.m1.3.3.2.4.2" xref="S3.E1.m1.3.3.2.3.cmml">,</mo><mi id="S3.E1.m1.3.3.2.2" xref="S3.E1.m1.3.3.2.2.cmml">j</mi><mo stretchy="false" id="S3.E1.m1.3.3.2.4.3" xref="S3.E1.m1.3.3.2.3.cmml">)</mo></mrow></msup><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.1.1.2" xref="S3.E1.m1.5.5.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.5.5.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.5.5.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.1.cmml">(</mo><msup id="S3.E1.m1.5.5.1.1.1.1.1" xref="S3.E1.m1.5.5.1.1.1.1.1.cmml"><mi id="S3.E1.m1.5.5.1.1.1.1.1.2" xref="S3.E1.m1.5.5.1.1.1.1.1.2.cmml">x</mi><mrow id="S3.E1.m1.4.4.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.1.3.1" xref="S3.E1.m1.5.5.1.1.1.1.1.cmml">(</mo><mi id="S3.E1.m1.4.4.1.1" xref="S3.E1.m1.4.4.1.1.cmml">i</mi><mo stretchy="false" id="S3.E1.m1.4.4.1.3.2" xref="S3.E1.m1.5.5.1.1.1.1.1.cmml">)</mo></mrow></msup><mo stretchy="false" id="S3.E1.m1.5.5.1.1.1.1.3" xref="S3.E1.m1.5.5.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.5b"><apply id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5"><eq id="S3.E1.m1.5.5.2.cmml" xref="S3.E1.m1.5.5.2"></eq><apply id="S3.E1.m1.5.5.3.cmml" xref="S3.E1.m1.5.5.3"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.3.1.cmml" xref="S3.E1.m1.5.5.3">superscript</csymbol><ci id="S3.E1.m1.5.5.3.2.cmml" xref="S3.E1.m1.5.5.3.2">𝑥</ci><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">𝑗</ci></apply><apply id="S3.E1.m1.5.5.1.cmml" xref="S3.E1.m1.5.5.1"><apply id="S3.E1.m1.5.5.1.2.cmml" xref="S3.E1.m1.5.5.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.2.1.cmml" xref="S3.E1.m1.5.5.1.2">subscript</csymbol><sum id="S3.E1.m1.5.5.1.2.2.cmml" xref="S3.E1.m1.5.5.1.2.2"></sum><apply id="S3.E1.m1.5.5.1.2.3.cmml" xref="S3.E1.m1.5.5.1.2.3"><lt id="S3.E1.m1.5.5.1.2.3.1.cmml" xref="S3.E1.m1.5.5.1.2.3.1"></lt><ci id="S3.E1.m1.5.5.1.2.3.2.cmml" xref="S3.E1.m1.5.5.1.2.3.2">𝑖</ci><ci id="S3.E1.m1.5.5.1.2.3.3.cmml" xref="S3.E1.m1.5.5.1.2.3.3">𝑗</ci></apply></apply><apply id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1.1"><times id="S3.E1.m1.5.5.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.2"></times><apply id="S3.E1.m1.5.5.1.1.3.cmml" xref="S3.E1.m1.5.5.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.3.1.cmml" xref="S3.E1.m1.5.5.1.1.3">superscript</csymbol><ci id="S3.E1.m1.5.5.1.1.3.2.cmml" xref="S3.E1.m1.5.5.1.1.3.2">𝑜</ci><interval closure="open" id="S3.E1.m1.3.3.2.3.cmml" xref="S3.E1.m1.3.3.2.4"><ci id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1">𝑖</ci><ci id="S3.E1.m1.3.3.2.2.cmml" xref="S3.E1.m1.3.3.2.2">𝑗</ci></interval></apply><apply id="S3.E1.m1.5.5.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.1.1.1.1">superscript</csymbol><ci id="S3.E1.m1.5.5.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.1.1.1.1.1.2">𝑥</ci><ci id="S3.E1.m1.4.4.1.1.cmml" xref="S3.E1.m1.4.4.1.1">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.5c">\centering x^{(j)}=\sum_{i&lt;j}o^{(i,j)}(x^{(i)})\@add@centering</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.3" class="ltx_p">In the beginning, the candidate search space is generated by combining each node of the DARTS cell with all the candidate operations (with multiple links between nodes), as illustrated in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 emoDARTS Framework ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (b). Equation <a href="#S3.E1" title="In 3 emoDARTS Framework ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> incorporates a weight parameter <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">\alpha</annotation></semantics></math> to identify the optimal edge (operation) connecting two nodes, <math id="S3.p3.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.p3.2.m2.1a"><mi id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">i</annotation></semantics></math> and <math id="S3.p3.3.m3.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.p3.3.m3.1a"><mi id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><ci id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">j</annotation></semantics></math>, from the candidate search space of all operations. Equation <a href="#S3.E2" title="In 3 emoDARTS Framework ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> describes how the node’s output be represented.</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.7" class="ltx_Math" alttext="\centering x^{(j)}=\sum_{i&lt;j}\alpha^{(i,j)}o^{(i,j)}(x^{(i)})\@add@centering" display="block"><semantics id="S3.E2.m1.7a"><mrow id="S3.E2.m1.7.7" xref="S3.E2.m1.7.7.cmml"><msup id="S3.E2.m1.7.7.3" xref="S3.E2.m1.7.7.3.cmml"><mi id="S3.E2.m1.7.7.3.2" xref="S3.E2.m1.7.7.3.2.cmml">x</mi><mrow id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.7.7.3.cmml"><mo stretchy="false" id="S3.E2.m1.1.1.1.3.1" xref="S3.E2.m1.7.7.3.cmml">(</mo><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">j</mi><mo stretchy="false" id="S3.E2.m1.1.1.1.3.2" xref="S3.E2.m1.7.7.3.cmml">)</mo></mrow></msup><mo rspace="0.111em" id="S3.E2.m1.7.7.2" xref="S3.E2.m1.7.7.2.cmml">=</mo><mrow id="S3.E2.m1.7.7.1" xref="S3.E2.m1.7.7.1.cmml"><munder id="S3.E2.m1.7.7.1.2" xref="S3.E2.m1.7.7.1.2.cmml"><mo movablelimits="false" id="S3.E2.m1.7.7.1.2.2" xref="S3.E2.m1.7.7.1.2.2.cmml">∑</mo><mrow id="S3.E2.m1.7.7.1.2.3" xref="S3.E2.m1.7.7.1.2.3.cmml"><mi id="S3.E2.m1.7.7.1.2.3.2" xref="S3.E2.m1.7.7.1.2.3.2.cmml">i</mi><mo id="S3.E2.m1.7.7.1.2.3.1" xref="S3.E2.m1.7.7.1.2.3.1.cmml">&lt;</mo><mi id="S3.E2.m1.7.7.1.2.3.3" xref="S3.E2.m1.7.7.1.2.3.3.cmml">j</mi></mrow></munder><mrow id="S3.E2.m1.7.7.1.1" xref="S3.E2.m1.7.7.1.1.cmml"><msup id="S3.E2.m1.7.7.1.1.3" xref="S3.E2.m1.7.7.1.1.3.cmml"><mi id="S3.E2.m1.7.7.1.1.3.2" xref="S3.E2.m1.7.7.1.1.3.2.cmml">α</mi><mrow id="S3.E2.m1.3.3.2.4" xref="S3.E2.m1.3.3.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.2.4.1" xref="S3.E2.m1.3.3.2.3.cmml">(</mo><mi id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml">i</mi><mo id="S3.E2.m1.3.3.2.4.2" xref="S3.E2.m1.3.3.2.3.cmml">,</mo><mi id="S3.E2.m1.3.3.2.2" xref="S3.E2.m1.3.3.2.2.cmml">j</mi><mo stretchy="false" id="S3.E2.m1.3.3.2.4.3" xref="S3.E2.m1.3.3.2.3.cmml">)</mo></mrow></msup><mo lspace="0em" rspace="0em" id="S3.E2.m1.7.7.1.1.2" xref="S3.E2.m1.7.7.1.1.2.cmml">​</mo><msup id="S3.E2.m1.7.7.1.1.4" xref="S3.E2.m1.7.7.1.1.4.cmml"><mi id="S3.E2.m1.7.7.1.1.4.2" xref="S3.E2.m1.7.7.1.1.4.2.cmml">o</mi><mrow id="S3.E2.m1.5.5.2.4" xref="S3.E2.m1.5.5.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.5.5.2.4.1" xref="S3.E2.m1.5.5.2.3.cmml">(</mo><mi id="S3.E2.m1.4.4.1.1" xref="S3.E2.m1.4.4.1.1.cmml">i</mi><mo id="S3.E2.m1.5.5.2.4.2" xref="S3.E2.m1.5.5.2.3.cmml">,</mo><mi id="S3.E2.m1.5.5.2.2" xref="S3.E2.m1.5.5.2.2.cmml">j</mi><mo stretchy="false" id="S3.E2.m1.5.5.2.4.3" xref="S3.E2.m1.5.5.2.3.cmml">)</mo></mrow></msup><mo lspace="0em" rspace="0em" id="S3.E2.m1.7.7.1.1.2a" xref="S3.E2.m1.7.7.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.7.7.1.1.1.1" xref="S3.E2.m1.7.7.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.7.7.1.1.1.1.2" xref="S3.E2.m1.7.7.1.1.1.1.1.cmml">(</mo><msup id="S3.E2.m1.7.7.1.1.1.1.1" xref="S3.E2.m1.7.7.1.1.1.1.1.cmml"><mi id="S3.E2.m1.7.7.1.1.1.1.1.2" xref="S3.E2.m1.7.7.1.1.1.1.1.2.cmml">x</mi><mrow id="S3.E2.m1.6.6.1.3" xref="S3.E2.m1.7.7.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.6.6.1.3.1" xref="S3.E2.m1.7.7.1.1.1.1.1.cmml">(</mo><mi id="S3.E2.m1.6.6.1.1" xref="S3.E2.m1.6.6.1.1.cmml">i</mi><mo stretchy="false" id="S3.E2.m1.6.6.1.3.2" xref="S3.E2.m1.7.7.1.1.1.1.1.cmml">)</mo></mrow></msup><mo stretchy="false" id="S3.E2.m1.7.7.1.1.1.1.3" xref="S3.E2.m1.7.7.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.7b"><apply id="S3.E2.m1.7.7.cmml" xref="S3.E2.m1.7.7"><eq id="S3.E2.m1.7.7.2.cmml" xref="S3.E2.m1.7.7.2"></eq><apply id="S3.E2.m1.7.7.3.cmml" xref="S3.E2.m1.7.7.3"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.3.1.cmml" xref="S3.E2.m1.7.7.3">superscript</csymbol><ci id="S3.E2.m1.7.7.3.2.cmml" xref="S3.E2.m1.7.7.3.2">𝑥</ci><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝑗</ci></apply><apply id="S3.E2.m1.7.7.1.cmml" xref="S3.E2.m1.7.7.1"><apply id="S3.E2.m1.7.7.1.2.cmml" xref="S3.E2.m1.7.7.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.1.2.1.cmml" xref="S3.E2.m1.7.7.1.2">subscript</csymbol><sum id="S3.E2.m1.7.7.1.2.2.cmml" xref="S3.E2.m1.7.7.1.2.2"></sum><apply id="S3.E2.m1.7.7.1.2.3.cmml" xref="S3.E2.m1.7.7.1.2.3"><lt id="S3.E2.m1.7.7.1.2.3.1.cmml" xref="S3.E2.m1.7.7.1.2.3.1"></lt><ci id="S3.E2.m1.7.7.1.2.3.2.cmml" xref="S3.E2.m1.7.7.1.2.3.2">𝑖</ci><ci id="S3.E2.m1.7.7.1.2.3.3.cmml" xref="S3.E2.m1.7.7.1.2.3.3">𝑗</ci></apply></apply><apply id="S3.E2.m1.7.7.1.1.cmml" xref="S3.E2.m1.7.7.1.1"><times id="S3.E2.m1.7.7.1.1.2.cmml" xref="S3.E2.m1.7.7.1.1.2"></times><apply id="S3.E2.m1.7.7.1.1.3.cmml" xref="S3.E2.m1.7.7.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.1.1.3.1.cmml" xref="S3.E2.m1.7.7.1.1.3">superscript</csymbol><ci id="S3.E2.m1.7.7.1.1.3.2.cmml" xref="S3.E2.m1.7.7.1.1.3.2">𝛼</ci><interval closure="open" id="S3.E2.m1.3.3.2.3.cmml" xref="S3.E2.m1.3.3.2.4"><ci id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1.1">𝑖</ci><ci id="S3.E2.m1.3.3.2.2.cmml" xref="S3.E2.m1.3.3.2.2">𝑗</ci></interval></apply><apply id="S3.E2.m1.7.7.1.1.4.cmml" xref="S3.E2.m1.7.7.1.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.1.1.4.1.cmml" xref="S3.E2.m1.7.7.1.1.4">superscript</csymbol><ci id="S3.E2.m1.7.7.1.1.4.2.cmml" xref="S3.E2.m1.7.7.1.1.4.2">𝑜</ci><interval closure="open" id="S3.E2.m1.5.5.2.3.cmml" xref="S3.E2.m1.5.5.2.4"><ci id="S3.E2.m1.4.4.1.1.cmml" xref="S3.E2.m1.4.4.1.1">𝑖</ci><ci id="S3.E2.m1.5.5.2.2.cmml" xref="S3.E2.m1.5.5.2.2">𝑗</ci></interval></apply><apply id="S3.E2.m1.7.7.1.1.1.1.1.cmml" xref="S3.E2.m1.7.7.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.7.7.1.1.1.1.1.1.cmml" xref="S3.E2.m1.7.7.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.7.7.1.1.1.1.1.2.cmml" xref="S3.E2.m1.7.7.1.1.1.1.1.2">𝑥</ci><ci id="S3.E2.m1.6.6.1.1.cmml" xref="S3.E2.m1.6.6.1.1">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.7c">\centering x^{(j)}=\sum_{i&lt;j}\alpha^{(i,j)}o^{(i,j)}(x^{(i)})\@add@centering</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.p3.5" class="ltx_p">Then, the continuous relaxation of the search space updates the weights (<math id="S3.p3.4.m1.2" class="ltx_Math" alttext="\alpha^{i,j}" display="inline"><semantics id="S3.p3.4.m1.2a"><msup id="S3.p3.4.m1.2.3" xref="S3.p3.4.m1.2.3.cmml"><mi id="S3.p3.4.m1.2.3.2" xref="S3.p3.4.m1.2.3.2.cmml">α</mi><mrow id="S3.p3.4.m1.2.2.2.4" xref="S3.p3.4.m1.2.2.2.3.cmml"><mi id="S3.p3.4.m1.1.1.1.1" xref="S3.p3.4.m1.1.1.1.1.cmml">i</mi><mo id="S3.p3.4.m1.2.2.2.4.1" xref="S3.p3.4.m1.2.2.2.3.cmml">,</mo><mi id="S3.p3.4.m1.2.2.2.2" xref="S3.p3.4.m1.2.2.2.2.cmml">j</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p3.4.m1.2b"><apply id="S3.p3.4.m1.2.3.cmml" xref="S3.p3.4.m1.2.3"><csymbol cd="ambiguous" id="S3.p3.4.m1.2.3.1.cmml" xref="S3.p3.4.m1.2.3">superscript</csymbol><ci id="S3.p3.4.m1.2.3.2.cmml" xref="S3.p3.4.m1.2.3.2">𝛼</ci><list id="S3.p3.4.m1.2.2.2.3.cmml" xref="S3.p3.4.m1.2.2.2.4"><ci id="S3.p3.4.m1.1.1.1.1.cmml" xref="S3.p3.4.m1.1.1.1.1">𝑖</ci><ci id="S3.p3.4.m1.2.2.2.2.cmml" xref="S3.p3.4.m1.2.2.2.2">𝑗</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m1.2c">\alpha^{i,j}</annotation></semantics></math>) of the edges. The final architecture can be obtained by selecting the operation between two nodes with the highest weight <math id="S3.p3.5.m2.5" class="ltx_Math" alttext="(o^{(i,j)*})" display="inline"><semantics id="S3.p3.5.m2.5a"><mrow id="S3.p3.5.m2.5.5.1" xref="S3.p3.5.m2.5.5.1.1.cmml"><mo stretchy="false" id="S3.p3.5.m2.5.5.1.2" xref="S3.p3.5.m2.5.5.1.1.cmml">(</mo><msup id="S3.p3.5.m2.5.5.1.1" xref="S3.p3.5.m2.5.5.1.1.cmml"><mi id="S3.p3.5.m2.5.5.1.1.2" xref="S3.p3.5.m2.5.5.1.1.2.cmml">o</mi><mrow id="S3.p3.5.m2.4.4.4.4" xref="S3.p3.5.m2.4.4.4.5.cmml"><mrow id="S3.p3.5.m2.4.4.4.4.1.2" xref="S3.p3.5.m2.4.4.4.4.1.1.cmml"><mo stretchy="false" id="S3.p3.5.m2.4.4.4.4.1.2.1" xref="S3.p3.5.m2.4.4.4.4.1.1.cmml">(</mo><mi id="S3.p3.5.m2.1.1.1.1" xref="S3.p3.5.m2.1.1.1.1.cmml">i</mi><mo id="S3.p3.5.m2.4.4.4.4.1.2.2" xref="S3.p3.5.m2.4.4.4.4.1.1.cmml">,</mo><mi id="S3.p3.5.m2.2.2.2.2" xref="S3.p3.5.m2.2.2.2.2.cmml">j</mi><mo stretchy="false" id="S3.p3.5.m2.4.4.4.4.1.2.3" xref="S3.p3.5.m2.4.4.4.4.1.1.cmml">)</mo></mrow><mo lspace="0.222em" id="S3.p3.5.m2.4.4.4.4.2" xref="S3.p3.5.m2.4.4.4.5.cmml">⁣</mo><mo id="S3.p3.5.m2.3.3.3.3" xref="S3.p3.5.m2.3.3.3.3.cmml">∗</mo></mrow></msup><mo stretchy="false" id="S3.p3.5.m2.5.5.1.3" xref="S3.p3.5.m2.5.5.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.5.m2.5b"><apply id="S3.p3.5.m2.5.5.1.1.cmml" xref="S3.p3.5.m2.5.5.1"><csymbol cd="ambiguous" id="S3.p3.5.m2.5.5.1.1.1.cmml" xref="S3.p3.5.m2.5.5.1">superscript</csymbol><ci id="S3.p3.5.m2.5.5.1.1.2.cmml" xref="S3.p3.5.m2.5.5.1.1.2">𝑜</ci><list id="S3.p3.5.m2.4.4.4.5.cmml" xref="S3.p3.5.m2.4.4.4.4"><interval closure="open" id="S3.p3.5.m2.4.4.4.4.1.1.cmml" xref="S3.p3.5.m2.4.4.4.4.1.2"><ci id="S3.p3.5.m2.1.1.1.1.cmml" xref="S3.p3.5.m2.1.1.1.1">𝑖</ci><ci id="S3.p3.5.m2.2.2.2.2.cmml" xref="S3.p3.5.m2.2.2.2.2">𝑗</ci></interval><times id="S3.p3.5.m2.3.3.3.3.cmml" xref="S3.p3.5.m2.3.3.3.3"></times></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.5.m2.5c">(o^{(i,j)*})</annotation></semantics></math> by using Equation <a href="#S3.E3" title="In 3 emoDARTS Framework ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.7" class="ltx_Math" alttext="\centering o^{(i,j)*}=argmax_{o}(\alpha^{(i,j)})\@add@centering" display="block"><semantics id="S3.E3.m1.7a"><mrow id="S3.E3.m1.7.7" xref="S3.E3.m1.7.7.cmml"><msup id="S3.E3.m1.7.7.3" xref="S3.E3.m1.7.7.3.cmml"><mi id="S3.E3.m1.7.7.3.2" xref="S3.E3.m1.7.7.3.2.cmml">o</mi><mrow id="S3.E3.m1.4.4.4.4" xref="S3.E3.m1.4.4.4.5.cmml"><mrow id="S3.E3.m1.4.4.4.4.1.2" xref="S3.E3.m1.4.4.4.4.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.4.4.4.4.1.2.1" xref="S3.E3.m1.4.4.4.4.1.1.cmml">(</mo><mi id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">i</mi><mo id="S3.E3.m1.4.4.4.4.1.2.2" xref="S3.E3.m1.4.4.4.4.1.1.cmml">,</mo><mi id="S3.E3.m1.2.2.2.2" xref="S3.E3.m1.2.2.2.2.cmml">j</mi><mo stretchy="false" id="S3.E3.m1.4.4.4.4.1.2.3" xref="S3.E3.m1.4.4.4.4.1.1.cmml">)</mo></mrow><mo lspace="0.222em" id="S3.E3.m1.4.4.4.4.2" xref="S3.E3.m1.4.4.4.5.cmml">⁣</mo><mo id="S3.E3.m1.3.3.3.3" xref="S3.E3.m1.3.3.3.3.cmml">∗</mo></mrow></msup><mo id="S3.E3.m1.7.7.2" xref="S3.E3.m1.7.7.2.cmml">=</mo><mrow id="S3.E3.m1.7.7.1" xref="S3.E3.m1.7.7.1.cmml"><mi id="S3.E3.m1.7.7.1.3" xref="S3.E3.m1.7.7.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.1.2" xref="S3.E3.m1.7.7.1.2.cmml">​</mo><mi id="S3.E3.m1.7.7.1.4" xref="S3.E3.m1.7.7.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.1.2a" xref="S3.E3.m1.7.7.1.2.cmml">​</mo><mi id="S3.E3.m1.7.7.1.5" xref="S3.E3.m1.7.7.1.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.1.2b" xref="S3.E3.m1.7.7.1.2.cmml">​</mo><mi id="S3.E3.m1.7.7.1.6" xref="S3.E3.m1.7.7.1.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.1.2c" xref="S3.E3.m1.7.7.1.2.cmml">​</mo><mi id="S3.E3.m1.7.7.1.7" xref="S3.E3.m1.7.7.1.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.1.2d" xref="S3.E3.m1.7.7.1.2.cmml">​</mo><msub id="S3.E3.m1.7.7.1.8" xref="S3.E3.m1.7.7.1.8.cmml"><mi id="S3.E3.m1.7.7.1.8.2" xref="S3.E3.m1.7.7.1.8.2.cmml">x</mi><mi id="S3.E3.m1.7.7.1.8.3" xref="S3.E3.m1.7.7.1.8.3.cmml">o</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.7.7.1.2e" xref="S3.E3.m1.7.7.1.2.cmml">​</mo><mrow id="S3.E3.m1.7.7.1.1.1" xref="S3.E3.m1.7.7.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.7.7.1.1.1.2" xref="S3.E3.m1.7.7.1.1.1.1.cmml">(</mo><msup id="S3.E3.m1.7.7.1.1.1.1" xref="S3.E3.m1.7.7.1.1.1.1.cmml"><mi id="S3.E3.m1.7.7.1.1.1.1.2" xref="S3.E3.m1.7.7.1.1.1.1.2.cmml">α</mi><mrow id="S3.E3.m1.6.6.2.4" xref="S3.E3.m1.6.6.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.6.6.2.4.1" xref="S3.E3.m1.6.6.2.3.cmml">(</mo><mi id="S3.E3.m1.5.5.1.1" xref="S3.E3.m1.5.5.1.1.cmml">i</mi><mo id="S3.E3.m1.6.6.2.4.2" xref="S3.E3.m1.6.6.2.3.cmml">,</mo><mi id="S3.E3.m1.6.6.2.2" xref="S3.E3.m1.6.6.2.2.cmml">j</mi><mo stretchy="false" id="S3.E3.m1.6.6.2.4.3" xref="S3.E3.m1.6.6.2.3.cmml">)</mo></mrow></msup><mo stretchy="false" id="S3.E3.m1.7.7.1.1.1.3" xref="S3.E3.m1.7.7.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.7b"><apply id="S3.E3.m1.7.7.cmml" xref="S3.E3.m1.7.7"><eq id="S3.E3.m1.7.7.2.cmml" xref="S3.E3.m1.7.7.2"></eq><apply id="S3.E3.m1.7.7.3.cmml" xref="S3.E3.m1.7.7.3"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.3.1.cmml" xref="S3.E3.m1.7.7.3">superscript</csymbol><ci id="S3.E3.m1.7.7.3.2.cmml" xref="S3.E3.m1.7.7.3.2">𝑜</ci><list id="S3.E3.m1.4.4.4.5.cmml" xref="S3.E3.m1.4.4.4.4"><interval closure="open" id="S3.E3.m1.4.4.4.4.1.1.cmml" xref="S3.E3.m1.4.4.4.4.1.2"><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">𝑖</ci><ci id="S3.E3.m1.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2">𝑗</ci></interval><times id="S3.E3.m1.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3"></times></list></apply><apply id="S3.E3.m1.7.7.1.cmml" xref="S3.E3.m1.7.7.1"><times id="S3.E3.m1.7.7.1.2.cmml" xref="S3.E3.m1.7.7.1.2"></times><ci id="S3.E3.m1.7.7.1.3.cmml" xref="S3.E3.m1.7.7.1.3">𝑎</ci><ci id="S3.E3.m1.7.7.1.4.cmml" xref="S3.E3.m1.7.7.1.4">𝑟</ci><ci id="S3.E3.m1.7.7.1.5.cmml" xref="S3.E3.m1.7.7.1.5">𝑔</ci><ci id="S3.E3.m1.7.7.1.6.cmml" xref="S3.E3.m1.7.7.1.6">𝑚</ci><ci id="S3.E3.m1.7.7.1.7.cmml" xref="S3.E3.m1.7.7.1.7">𝑎</ci><apply id="S3.E3.m1.7.7.1.8.cmml" xref="S3.E3.m1.7.7.1.8"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.8.1.cmml" xref="S3.E3.m1.7.7.1.8">subscript</csymbol><ci id="S3.E3.m1.7.7.1.8.2.cmml" xref="S3.E3.m1.7.7.1.8.2">𝑥</ci><ci id="S3.E3.m1.7.7.1.8.3.cmml" xref="S3.E3.m1.7.7.1.8.3">𝑜</ci></apply><apply id="S3.E3.m1.7.7.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.1.1.1.1.1.cmml" xref="S3.E3.m1.7.7.1.1.1">superscript</csymbol><ci id="S3.E3.m1.7.7.1.1.1.1.2.cmml" xref="S3.E3.m1.7.7.1.1.1.1.2">𝛼</ci><interval closure="open" id="S3.E3.m1.6.6.2.3.cmml" xref="S3.E3.m1.6.6.2.4"><ci id="S3.E3.m1.5.5.1.1.cmml" xref="S3.E3.m1.5.5.1.1">𝑖</ci><ci id="S3.E3.m1.6.6.2.2.cmml" xref="S3.E3.m1.6.6.2.2">𝑗</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.7c">\centering o^{(i,j)*}=argmax_{o}(\alpha^{(i,j)})\@add@centering</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S3.p3.6" class="ltx_p">The searched discrete cell architecture is shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 emoDARTS Framework ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (d).</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2403.14083/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="392" height="188" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>DARTS employs steps (a) to (d) to search cell architectures: (a) initialises the graph, (b) forms a search space, (c) updates edge weights, and (d) determines the final cell structure. Nodes signify representations, edges represent operations, with light-coloured edges indicating weaker and dark-coloured edges representing stronger operations.</figcaption>
</figure>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.6" class="ltx_p">The number of cells (<math id="S3.p4.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.p4.1.m1.1a"><mi id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><ci id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">C</annotation></semantics></math> or <math id="S3.p4.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.p4.2.m2.1a"><mi id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><ci id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">N</annotation></semantics></math>) in a component is a parameter for the DARTS algorithm that specifies how many DARTS cells are stacked to form a component in the model. Each cell takes the last two cells’ output as input. If the output from each cell <math id="S3.p4.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.p4.3.m3.1a"><mi id="S3.p4.3.m3.1.1" xref="S3.p4.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.p4.3.m3.1b"><ci id="S3.p4.3.m3.1.1.cmml" xref="S3.p4.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.3.m3.1c">t</annotation></semantics></math> is <math id="S3.p4.4.m4.1" class="ltx_Math" alttext="y_{t}" display="inline"><semantics id="S3.p4.4.m4.1a"><msub id="S3.p4.4.m4.1.1" xref="S3.p4.4.m4.1.1.cmml"><mi id="S3.p4.4.m4.1.1.2" xref="S3.p4.4.m4.1.1.2.cmml">y</mi><mi id="S3.p4.4.m4.1.1.3" xref="S3.p4.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p4.4.m4.1b"><apply id="S3.p4.4.m4.1.1.cmml" xref="S3.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p4.4.m4.1.1.1.cmml" xref="S3.p4.4.m4.1.1">subscript</csymbol><ci id="S3.p4.4.m4.1.1.2.cmml" xref="S3.p4.4.m4.1.1.2">𝑦</ci><ci id="S3.p4.4.m4.1.1.3.cmml" xref="S3.p4.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.4.m4.1c">y_{t}</annotation></semantics></math> and the function within the cell is <math id="S3.p4.5.m5.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.p4.5.m5.1a"><mi id="S3.p4.5.m5.1.1" xref="S3.p4.5.m5.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.p4.5.m5.1b"><ci id="S3.p4.5.m5.1.1.cmml" xref="S3.p4.5.m5.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.5.m5.1c">f</annotation></semantics></math>, then <math id="S3.p4.6.m6.1" class="ltx_Math" alttext="y_{t}" display="inline"><semantics id="S3.p4.6.m6.1a"><msub id="S3.p4.6.m6.1.1" xref="S3.p4.6.m6.1.1.cmml"><mi id="S3.p4.6.m6.1.1.2" xref="S3.p4.6.m6.1.1.2.cmml">y</mi><mi id="S3.p4.6.m6.1.1.3" xref="S3.p4.6.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p4.6.m6.1b"><apply id="S3.p4.6.m6.1.1.cmml" xref="S3.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p4.6.m6.1.1.1.cmml" xref="S3.p4.6.m6.1.1">subscript</csymbol><ci id="S3.p4.6.m6.1.1.2.cmml" xref="S3.p4.6.m6.1.1.2">𝑦</ci><ci id="S3.p4.6.m6.1.1.3.cmml" xref="S3.p4.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.6.m6.1c">y_{t}</annotation></semantics></math> can be represented as;</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.2" class="ltx_Math" alttext="\centering y_{t}=f(y_{t-1},y_{t-2})\@add@centering" display="block"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml"><msub id="S3.E4.m1.2.2.4" xref="S3.E4.m1.2.2.4.cmml"><mi id="S3.E4.m1.2.2.4.2" xref="S3.E4.m1.2.2.4.2.cmml">y</mi><mi id="S3.E4.m1.2.2.4.3" xref="S3.E4.m1.2.2.4.3.cmml">t</mi></msub><mo id="S3.E4.m1.2.2.3" xref="S3.E4.m1.2.2.3.cmml">=</mo><mrow id="S3.E4.m1.2.2.2" xref="S3.E4.m1.2.2.2.cmml"><mi id="S3.E4.m1.2.2.2.4" xref="S3.E4.m1.2.2.2.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.2.3" xref="S3.E4.m1.2.2.2.3.cmml">​</mo><mrow id="S3.E4.m1.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.2.2.2.3" xref="S3.E4.m1.2.2.2.2.3.cmml">(</mo><msub id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.2.cmml">y</mi><mrow id="S3.E4.m1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.1.3.2.cmml">t</mi><mo id="S3.E4.m1.1.1.1.1.1.1.3.1" xref="S3.E4.m1.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="S3.E4.m1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S3.E4.m1.2.2.2.2.2.4" xref="S3.E4.m1.2.2.2.2.3.cmml">,</mo><msub id="S3.E4.m1.2.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.2.2.cmml"><mi id="S3.E4.m1.2.2.2.2.2.2.2" xref="S3.E4.m1.2.2.2.2.2.2.2.cmml">y</mi><mrow id="S3.E4.m1.2.2.2.2.2.2.3" xref="S3.E4.m1.2.2.2.2.2.2.3.cmml"><mi id="S3.E4.m1.2.2.2.2.2.2.3.2" xref="S3.E4.m1.2.2.2.2.2.2.3.2.cmml">t</mi><mo id="S3.E4.m1.2.2.2.2.2.2.3.1" xref="S3.E4.m1.2.2.2.2.2.2.3.1.cmml">−</mo><mn id="S3.E4.m1.2.2.2.2.2.2.3.3" xref="S3.E4.m1.2.2.2.2.2.2.3.3.cmml">2</mn></mrow></msub><mo stretchy="false" id="S3.E4.m1.2.2.2.2.2.5" xref="S3.E4.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2"><eq id="S3.E4.m1.2.2.3.cmml" xref="S3.E4.m1.2.2.3"></eq><apply id="S3.E4.m1.2.2.4.cmml" xref="S3.E4.m1.2.2.4"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.4.1.cmml" xref="S3.E4.m1.2.2.4">subscript</csymbol><ci id="S3.E4.m1.2.2.4.2.cmml" xref="S3.E4.m1.2.2.4.2">𝑦</ci><ci id="S3.E4.m1.2.2.4.3.cmml" xref="S3.E4.m1.2.2.4.3">𝑡</ci></apply><apply id="S3.E4.m1.2.2.2.cmml" xref="S3.E4.m1.2.2.2"><times id="S3.E4.m1.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.3"></times><ci id="S3.E4.m1.2.2.2.4.cmml" xref="S3.E4.m1.2.2.2.4">𝑓</ci><interval closure="open" id="S3.E4.m1.2.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.2.2"><apply id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2">𝑦</ci><apply id="S3.E4.m1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3"><minus id="S3.E4.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.1"></minus><ci id="S3.E4.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.2">𝑡</ci><cn type="integer" id="S3.E4.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S3.E4.m1.2.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.2.2.1.cmml" xref="S3.E4.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.2.2.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2.2">𝑦</ci><apply id="S3.E4.m1.2.2.2.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3"><minus id="S3.E4.m1.2.2.2.2.2.2.3.1.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.1"></minus><ci id="S3.E4.m1.2.2.2.2.2.2.3.2.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.2">𝑡</ci><cn type="integer" id="S3.E4.m1.2.2.2.2.2.2.3.3.cmml" xref="S3.E4.m1.2.2.2.2.2.2.3.3">2</cn></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">\centering y_{t}=f(y_{t-1},y_{t-2})\@add@centering</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">DARTS’ CNN component has two types of CNN cells: ‘normal’ and ‘reduction’ cells. It sets the stride to one in normal cells and two in reduction cells, resulting in a downsampled output in the reduction cells. This downsampling allows the model to eliminate the duplication of intermediate characteristics, reducing complexity.
</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">We decided to jointly optimise the CNN and SeqNN components rather than individually since it is important for downstream components (in this case, the CNN component) to understand the behaviour of upstream components (SeqNN). Joint optimisation in a multiple-component network improves architecture search in various ways, including: 1. the back-propagation of loss minimisation flows through all the components in a single compute graph; and 2. it reduces the time required in the search phase by searching the architecture of the whole network at once.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experimental Setup</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span><span id="S4.SS1.1.1" class="ltx_text ltx_font_italic">Dataset and Feature Selection</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.2" class="ltx_p">We use the widely used IEMOCAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, MSP-IMPROV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, and MSP-Podcast <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> datasets for our experiments.
Our study takes the improvised subset of IEMOCAP and the four categorical labels, happiness, sadness, anger, and neutral as classes from the datasets. We employ five-fold cross-validation with at least one speaker out in our training and evaluations. At each fold, the training dataset is divided into two subsets, ‘search’, and ‘training’, by a <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="70/30" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">70</mn><mo id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">/</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">30</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><divide id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></divide><cn type="integer" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">70</cn><cn type="integer" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">70/30</annotation></semantics></math> fraction. The ‘search’ set is used in the architecture search; the ‘training’ set is used in optimising the searched architecture, and the remaining testing dataset is used to infer and obtain the testing performance of the searched and optimised model.
This way, we manage to split the dataset into three sets in each cross-validation session. The IEMOCAP dataset has five sessions with ten actors and two unique speakers in each. We use one session for the testing dataset and four sessions for the search and training datasets. Similarly, MSP-Improv comprises six sessions including twelve actors. We take one session in the testing dataset and the remaining five sessions in the search and training dataset. MSP-Podcast includes a speaker ID with each audio utterance, and we group the entire dataset by the speaker and divide it by the <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="70/30" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mn id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">70</mn><mo id="S4.SS1.p1.2.m2.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.cmml">/</mo><mn id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">30</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><divide id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1"></divide><cn type="integer" id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">70</cn><cn type="integer" id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">70/30</annotation></semantics></math> rule.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.5" class="ltx_p">In this research, we use Mel Frequency Cepstral Coefficients (MFCC) as input features to the model. MFCC has been used as the input feature in many SER studies in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and has proven to obtain promising results.
Some machine learning research uses the Mel Filter bank as an input feature when the algorithm is not vulnerable to strongly correlated data. We picked the MFCC for this study since the deep learning model is produced automatically and we do not want to infer the model’s sensitivity to correlated input.
We extract <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="128" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><cn type="integer" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">128</annotation></semantics></math> MFCCs from each <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mn id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><cn type="integer" id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">8</annotation></semantics></math>-second audio utterance from the dataset. If the audio utterance length is less than <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mn id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><cn type="integer" id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">8</annotation></semantics></math> seconds, we added padding with zeros while the lengthier utterances are truncated. The MFCC extraction from the Librosa python library  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>
outputs a shape <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="128\times 512" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><mrow id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml"><mn id="S4.SS1.p2.4.m4.1.1.2" xref="S4.SS1.p2.4.m4.1.1.2.cmml">128</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p2.4.m4.1.1.1" xref="S4.SS1.p2.4.m4.1.1.1.cmml">×</mo><mn id="S4.SS1.p2.4.m4.1.1.3" xref="S4.SS1.p2.4.m4.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><apply id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1"><times id="S4.SS1.p2.4.m4.1.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1.1"></times><cn type="integer" id="S4.SS1.p2.4.m4.1.1.2.cmml" xref="S4.SS1.p2.4.m4.1.1.2">128</cn><cn type="integer" id="S4.SS1.p2.4.m4.1.1.3.cmml" xref="S4.SS1.p2.4.m4.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">128\times 512</annotation></semantics></math>, downsampled with max pooling, to create a spectrogram of the shape <math id="S4.SS1.p2.5.m5.1" class="ltx_Math" alttext="128\times 128" display="inline"><semantics id="S4.SS1.p2.5.m5.1a"><mrow id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml"><mn id="S4.SS1.p2.5.m5.1.1.2" xref="S4.SS1.p2.5.m5.1.1.2.cmml">128</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p2.5.m5.1.1.1" xref="S4.SS1.p2.5.m5.1.1.1.cmml">×</mo><mn id="S4.SS1.p2.5.m5.1.1.3" xref="S4.SS1.p2.5.m5.1.1.3.cmml">128</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.1b"><apply id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1"><times id="S4.SS1.p2.5.m5.1.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1.1"></times><cn type="integer" id="S4.SS1.p2.5.m5.1.1.2.cmml" xref="S4.SS1.p2.5.m5.1.1.2">128</cn><cn type="integer" id="S4.SS1.p2.5.m5.1.1.3.cmml" xref="S4.SS1.p2.5.m5.1.1.3">128</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.1c">128\times 128</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span><span id="S4.SS2.1.1" class="ltx_text ltx_font_italic">Baseline Models</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We compare the performance of emoDARTS for SER with three models developed without DARTS (w/o DARTS)
: 1) CNN, 2) CNN+ LSTM, and 3) CNN+LSTM with attention
as baseline models. The CNN baseline model consists of a CNN layer (kernel size=2, stride=2, and padding=2) followed by a Max-Pooling layer (kernel size=2 and stride=2). Two dense layers then processes the output from the Max-Pooling layer after applying a dropout of <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="0.3" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">0.3</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn type="float" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">0.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">0.3</annotation></semantics></math>. Finally, the last dense layer has four output units resembling the four emotion classes, and the model outputs the probability estimation of each emotion for a given input by a Softmax function.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The CNN+LSTM baseline model is built, including an additional bi-directional LSTM layer of 128 units after the Max-Pooling layer. An attention layer is added to the LSTM layer in the ‘CNN+LSTM attention’ baseline model. Figure <a href="#S4.F4" title="Figure 4 ‣ 4.2 Baseline Models ‣ 4 Experimental Setup ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the architecture of the CNN+LSTM attention baseline model</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2403.14083/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="188" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Visualisation of the CNN+LSTM attention baseline model. The parameters of the CNN layer are: kernel size (k)=2, stride (s)=2 and, padding (p)=2 and the parameters of the Max-pooling layer are: kernel size (k)=2 and stride (s)=2 and the LSTM layer has 128 units.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span><span id="S4.SS3.1.1" class="ltx_text ltx_font_italic">DARTS Configuration</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.16" class="ltx_p">We divide the cell search space operations into the two separate parts CNN and SeqNN based on the components they apply.
Table <a href="#S4.T2" title="TABLE II ‣ 4.3 DARTS Configuration ‣ 4 Experimental Setup ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> lists the type of operations used in each component.
The cell search space of the CNN component consists of pooling operations such as <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><times id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">3</cn><cn type="integer" id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">3\times 3</annotation></semantics></math> max pooling (<math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="i=3" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mi id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">i</mi><mo id="S4.SS3.p1.2.m2.1.1.1" xref="S4.SS3.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><eq id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1.1"></eq><ci id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">𝑖</ci><cn type="integer" id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">i=3</annotation></semantics></math>) and <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mrow id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml"><mn id="S4.SS3.p1.3.m3.1.1.2" xref="S4.SS3.p1.3.m3.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.3.m3.1.1.1" xref="S4.SS3.p1.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS3.p1.3.m3.1.1.3" xref="S4.SS3.p1.3.m3.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><apply id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"><times id="S4.SS3.p1.3.m3.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1"></times><cn type="integer" id="S4.SS3.p1.3.m3.1.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1.2">3</cn><cn type="integer" id="S4.SS3.p1.3.m3.1.1.3.cmml" xref="S4.SS3.p1.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">3\times 3</annotation></semantics></math> average pooling (<math id="S4.SS3.p1.4.m4.1" class="ltx_Math" alttext="i=3" display="inline"><semantics id="S4.SS3.p1.4.m4.1a"><mrow id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml"><mi id="S4.SS3.p1.4.m4.1.1.2" xref="S4.SS3.p1.4.m4.1.1.2.cmml">i</mi><mo id="S4.SS3.p1.4.m4.1.1.1" xref="S4.SS3.p1.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS3.p1.4.m4.1.1.3" xref="S4.SS3.p1.4.m4.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><apply id="S4.SS3.p1.4.m4.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1"><eq id="S4.SS3.p1.4.m4.1.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1.1"></eq><ci id="S4.SS3.p1.4.m4.1.1.2.cmml" xref="S4.SS3.p1.4.m4.1.1.2">𝑖</ci><cn type="integer" id="S4.SS3.p1.4.m4.1.1.3.cmml" xref="S4.SS3.p1.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">i=3</annotation></semantics></math>), convolutional operations such as <math id="S4.SS3.p1.5.m5.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S4.SS3.p1.5.m5.1a"><mrow id="S4.SS3.p1.5.m5.1.1" xref="S4.SS3.p1.5.m5.1.1.cmml"><mn id="S4.SS3.p1.5.m5.1.1.2" xref="S4.SS3.p1.5.m5.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.5.m5.1.1.1" xref="S4.SS3.p1.5.m5.1.1.1.cmml">×</mo><mn id="S4.SS3.p1.5.m5.1.1.3" xref="S4.SS3.p1.5.m5.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.5.m5.1b"><apply id="S4.SS3.p1.5.m5.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1"><times id="S4.SS3.p1.5.m5.1.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1.1"></times><cn type="integer" id="S4.SS3.p1.5.m5.1.1.2.cmml" xref="S4.SS3.p1.5.m5.1.1.2">3</cn><cn type="integer" id="S4.SS3.p1.5.m5.1.1.3.cmml" xref="S4.SS3.p1.5.m5.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.5.m5.1c">3\times 3</annotation></semantics></math> and <math id="S4.SS3.p1.6.m6.1" class="ltx_Math" alttext="5\times 5" display="inline"><semantics id="S4.SS3.p1.6.m6.1a"><mrow id="S4.SS3.p1.6.m6.1.1" xref="S4.SS3.p1.6.m6.1.1.cmml"><mn id="S4.SS3.p1.6.m6.1.1.2" xref="S4.SS3.p1.6.m6.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.6.m6.1.1.1" xref="S4.SS3.p1.6.m6.1.1.1.cmml">×</mo><mn id="S4.SS3.p1.6.m6.1.1.3" xref="S4.SS3.p1.6.m6.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.6.m6.1b"><apply id="S4.SS3.p1.6.m6.1.1.cmml" xref="S4.SS3.p1.6.m6.1.1"><times id="S4.SS3.p1.6.m6.1.1.1.cmml" xref="S4.SS3.p1.6.m6.1.1.1"></times><cn type="integer" id="S4.SS3.p1.6.m6.1.1.2.cmml" xref="S4.SS3.p1.6.m6.1.1.2">5</cn><cn type="integer" id="S4.SS3.p1.6.m6.1.1.3.cmml" xref="S4.SS3.p1.6.m6.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.6.m6.1c">5\times 5</annotation></semantics></math> separable convolutions (<math id="S4.SS3.p1.7.m7.2" class="ltx_Math" alttext="i=3,5" display="inline"><semantics id="S4.SS3.p1.7.m7.2a"><mrow id="S4.SS3.p1.7.m7.2.3" xref="S4.SS3.p1.7.m7.2.3.cmml"><mi id="S4.SS3.p1.7.m7.2.3.2" xref="S4.SS3.p1.7.m7.2.3.2.cmml">i</mi><mo id="S4.SS3.p1.7.m7.2.3.1" xref="S4.SS3.p1.7.m7.2.3.1.cmml">=</mo><mrow id="S4.SS3.p1.7.m7.2.3.3.2" xref="S4.SS3.p1.7.m7.2.3.3.1.cmml"><mn id="S4.SS3.p1.7.m7.1.1" xref="S4.SS3.p1.7.m7.1.1.cmml">3</mn><mo id="S4.SS3.p1.7.m7.2.3.3.2.1" xref="S4.SS3.p1.7.m7.2.3.3.1.cmml">,</mo><mn id="S4.SS3.p1.7.m7.2.2" xref="S4.SS3.p1.7.m7.2.2.cmml">5</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.7.m7.2b"><apply id="S4.SS3.p1.7.m7.2.3.cmml" xref="S4.SS3.p1.7.m7.2.3"><eq id="S4.SS3.p1.7.m7.2.3.1.cmml" xref="S4.SS3.p1.7.m7.2.3.1"></eq><ci id="S4.SS3.p1.7.m7.2.3.2.cmml" xref="S4.SS3.p1.7.m7.2.3.2">𝑖</ci><list id="S4.SS3.p1.7.m7.2.3.3.1.cmml" xref="S4.SS3.p1.7.m7.2.3.3.2"><cn type="integer" id="S4.SS3.p1.7.m7.1.1.cmml" xref="S4.SS3.p1.7.m7.1.1">3</cn><cn type="integer" id="S4.SS3.p1.7.m7.2.2.cmml" xref="S4.SS3.p1.7.m7.2.2">5</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.7.m7.2c">i=3,5</annotation></semantics></math>), <math id="S4.SS3.p1.8.m8.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S4.SS3.p1.8.m8.1a"><mrow id="S4.SS3.p1.8.m8.1.1" xref="S4.SS3.p1.8.m8.1.1.cmml"><mn id="S4.SS3.p1.8.m8.1.1.2" xref="S4.SS3.p1.8.m8.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.8.m8.1.1.1" xref="S4.SS3.p1.8.m8.1.1.1.cmml">×</mo><mn id="S4.SS3.p1.8.m8.1.1.3" xref="S4.SS3.p1.8.m8.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.8.m8.1b"><apply id="S4.SS3.p1.8.m8.1.1.cmml" xref="S4.SS3.p1.8.m8.1.1"><times id="S4.SS3.p1.8.m8.1.1.1.cmml" xref="S4.SS3.p1.8.m8.1.1.1"></times><cn type="integer" id="S4.SS3.p1.8.m8.1.1.2.cmml" xref="S4.SS3.p1.8.m8.1.1.2">3</cn><cn type="integer" id="S4.SS3.p1.8.m8.1.1.3.cmml" xref="S4.SS3.p1.8.m8.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.8.m8.1c">3\times 3</annotation></semantics></math> and <math id="S4.SS3.p1.9.m9.1" class="ltx_Math" alttext="5\times 5" display="inline"><semantics id="S4.SS3.p1.9.m9.1a"><mrow id="S4.SS3.p1.9.m9.1.1" xref="S4.SS3.p1.9.m9.1.1.cmml"><mn id="S4.SS3.p1.9.m9.1.1.2" xref="S4.SS3.p1.9.m9.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.9.m9.1.1.1" xref="S4.SS3.p1.9.m9.1.1.1.cmml">×</mo><mn id="S4.SS3.p1.9.m9.1.1.3" xref="S4.SS3.p1.9.m9.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.9.m9.1b"><apply id="S4.SS3.p1.9.m9.1.1.cmml" xref="S4.SS3.p1.9.m9.1.1"><times id="S4.SS3.p1.9.m9.1.1.1.cmml" xref="S4.SS3.p1.9.m9.1.1.1"></times><cn type="integer" id="S4.SS3.p1.9.m9.1.1.2.cmml" xref="S4.SS3.p1.9.m9.1.1.2">5</cn><cn type="integer" id="S4.SS3.p1.9.m9.1.1.3.cmml" xref="S4.SS3.p1.9.m9.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.9.m9.1c">5\times 5</annotation></semantics></math> dilated convolution (<math id="S4.SS3.p1.10.m10.2" class="ltx_Math" alttext="i=3,5" display="inline"><semantics id="S4.SS3.p1.10.m10.2a"><mrow id="S4.SS3.p1.10.m10.2.3" xref="S4.SS3.p1.10.m10.2.3.cmml"><mi id="S4.SS3.p1.10.m10.2.3.2" xref="S4.SS3.p1.10.m10.2.3.2.cmml">i</mi><mo id="S4.SS3.p1.10.m10.2.3.1" xref="S4.SS3.p1.10.m10.2.3.1.cmml">=</mo><mrow id="S4.SS3.p1.10.m10.2.3.3.2" xref="S4.SS3.p1.10.m10.2.3.3.1.cmml"><mn id="S4.SS3.p1.10.m10.1.1" xref="S4.SS3.p1.10.m10.1.1.cmml">3</mn><mo id="S4.SS3.p1.10.m10.2.3.3.2.1" xref="S4.SS3.p1.10.m10.2.3.3.1.cmml">,</mo><mn id="S4.SS3.p1.10.m10.2.2" xref="S4.SS3.p1.10.m10.2.2.cmml">5</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.10.m10.2b"><apply id="S4.SS3.p1.10.m10.2.3.cmml" xref="S4.SS3.p1.10.m10.2.3"><eq id="S4.SS3.p1.10.m10.2.3.1.cmml" xref="S4.SS3.p1.10.m10.2.3.1"></eq><ci id="S4.SS3.p1.10.m10.2.3.2.cmml" xref="S4.SS3.p1.10.m10.2.3.2">𝑖</ci><list id="S4.SS3.p1.10.m10.2.3.3.1.cmml" xref="S4.SS3.p1.10.m10.2.3.3.2"><cn type="integer" id="S4.SS3.p1.10.m10.1.1.cmml" xref="S4.SS3.p1.10.m10.1.1">3</cn><cn type="integer" id="S4.SS3.p1.10.m10.2.2.cmml" xref="S4.SS3.p1.10.m10.2.2">5</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.10.m10.2c">i=3,5</annotation></semantics></math>), <math id="S4.SS3.p1.11.m11.1" class="ltx_Math" alttext="7\times 1-1\times 7" display="inline"><semantics id="S4.SS3.p1.11.m11.1a"><mrow id="S4.SS3.p1.11.m11.1.1" xref="S4.SS3.p1.11.m11.1.1.cmml"><mrow id="S4.SS3.p1.11.m11.1.1.2" xref="S4.SS3.p1.11.m11.1.1.2.cmml"><mn id="S4.SS3.p1.11.m11.1.1.2.2" xref="S4.SS3.p1.11.m11.1.1.2.2.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.11.m11.1.1.2.1" xref="S4.SS3.p1.11.m11.1.1.2.1.cmml">×</mo><mn id="S4.SS3.p1.11.m11.1.1.2.3" xref="S4.SS3.p1.11.m11.1.1.2.3.cmml">1</mn></mrow><mo id="S4.SS3.p1.11.m11.1.1.1" xref="S4.SS3.p1.11.m11.1.1.1.cmml">−</mo><mrow id="S4.SS3.p1.11.m11.1.1.3" xref="S4.SS3.p1.11.m11.1.1.3.cmml"><mn id="S4.SS3.p1.11.m11.1.1.3.2" xref="S4.SS3.p1.11.m11.1.1.3.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.11.m11.1.1.3.1" xref="S4.SS3.p1.11.m11.1.1.3.1.cmml">×</mo><mn id="S4.SS3.p1.11.m11.1.1.3.3" xref="S4.SS3.p1.11.m11.1.1.3.3.cmml">7</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.11.m11.1b"><apply id="S4.SS3.p1.11.m11.1.1.cmml" xref="S4.SS3.p1.11.m11.1.1"><minus id="S4.SS3.p1.11.m11.1.1.1.cmml" xref="S4.SS3.p1.11.m11.1.1.1"></minus><apply id="S4.SS3.p1.11.m11.1.1.2.cmml" xref="S4.SS3.p1.11.m11.1.1.2"><times id="S4.SS3.p1.11.m11.1.1.2.1.cmml" xref="S4.SS3.p1.11.m11.1.1.2.1"></times><cn type="integer" id="S4.SS3.p1.11.m11.1.1.2.2.cmml" xref="S4.SS3.p1.11.m11.1.1.2.2">7</cn><cn type="integer" id="S4.SS3.p1.11.m11.1.1.2.3.cmml" xref="S4.SS3.p1.11.m11.1.1.2.3">1</cn></apply><apply id="S4.SS3.p1.11.m11.1.1.3.cmml" xref="S4.SS3.p1.11.m11.1.1.3"><times id="S4.SS3.p1.11.m11.1.1.3.1.cmml" xref="S4.SS3.p1.11.m11.1.1.3.1"></times><cn type="integer" id="S4.SS3.p1.11.m11.1.1.3.2.cmml" xref="S4.SS3.p1.11.m11.1.1.3.2">1</cn><cn type="integer" id="S4.SS3.p1.11.m11.1.1.3.3.cmml" xref="S4.SS3.p1.11.m11.1.1.3.3">7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.11.m11.1c">7\times 1-1\times 7</annotation></semantics></math> factorised convolution (<math id="S4.SS3.p1.12.m12.1" class="ltx_Math" alttext="i=7" display="inline"><semantics id="S4.SS3.p1.12.m12.1a"><mrow id="S4.SS3.p1.12.m12.1.1" xref="S4.SS3.p1.12.m12.1.1.cmml"><mi id="S4.SS3.p1.12.m12.1.1.2" xref="S4.SS3.p1.12.m12.1.1.2.cmml">i</mi><mo id="S4.SS3.p1.12.m12.1.1.1" xref="S4.SS3.p1.12.m12.1.1.1.cmml">=</mo><mn id="S4.SS3.p1.12.m12.1.1.3" xref="S4.SS3.p1.12.m12.1.1.3.cmml">7</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.12.m12.1b"><apply id="S4.SS3.p1.12.m12.1.1.cmml" xref="S4.SS3.p1.12.m12.1.1"><eq id="S4.SS3.p1.12.m12.1.1.1.cmml" xref="S4.SS3.p1.12.m12.1.1.1"></eq><ci id="S4.SS3.p1.12.m12.1.1.2.cmml" xref="S4.SS3.p1.12.m12.1.1.2">𝑖</ci><cn type="integer" id="S4.SS3.p1.12.m12.1.1.3.cmml" xref="S4.SS3.p1.12.m12.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.12.m12.1c">i=7</annotation></semantics></math>), identity connections, and no connections while the SeqNN component consists of operations such as RNN of layers 1 through 4 (<math id="S4.SS3.p1.13.m13.4" class="ltx_Math" alttext="j=1,2,3,4" display="inline"><semantics id="S4.SS3.p1.13.m13.4a"><mrow id="S4.SS3.p1.13.m13.4.5" xref="S4.SS3.p1.13.m13.4.5.cmml"><mi id="S4.SS3.p1.13.m13.4.5.2" xref="S4.SS3.p1.13.m13.4.5.2.cmml">j</mi><mo id="S4.SS3.p1.13.m13.4.5.1" xref="S4.SS3.p1.13.m13.4.5.1.cmml">=</mo><mrow id="S4.SS3.p1.13.m13.4.5.3.2" xref="S4.SS3.p1.13.m13.4.5.3.1.cmml"><mn id="S4.SS3.p1.13.m13.1.1" xref="S4.SS3.p1.13.m13.1.1.cmml">1</mn><mo id="S4.SS3.p1.13.m13.4.5.3.2.1" xref="S4.SS3.p1.13.m13.4.5.3.1.cmml">,</mo><mn id="S4.SS3.p1.13.m13.2.2" xref="S4.SS3.p1.13.m13.2.2.cmml">2</mn><mo id="S4.SS3.p1.13.m13.4.5.3.2.2" xref="S4.SS3.p1.13.m13.4.5.3.1.cmml">,</mo><mn id="S4.SS3.p1.13.m13.3.3" xref="S4.SS3.p1.13.m13.3.3.cmml">3</mn><mo id="S4.SS3.p1.13.m13.4.5.3.2.3" xref="S4.SS3.p1.13.m13.4.5.3.1.cmml">,</mo><mn id="S4.SS3.p1.13.m13.4.4" xref="S4.SS3.p1.13.m13.4.4.cmml">4</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.13.m13.4b"><apply id="S4.SS3.p1.13.m13.4.5.cmml" xref="S4.SS3.p1.13.m13.4.5"><eq id="S4.SS3.p1.13.m13.4.5.1.cmml" xref="S4.SS3.p1.13.m13.4.5.1"></eq><ci id="S4.SS3.p1.13.m13.4.5.2.cmml" xref="S4.SS3.p1.13.m13.4.5.2">𝑗</ci><list id="S4.SS3.p1.13.m13.4.5.3.1.cmml" xref="S4.SS3.p1.13.m13.4.5.3.2"><cn type="integer" id="S4.SS3.p1.13.m13.1.1.cmml" xref="S4.SS3.p1.13.m13.1.1">1</cn><cn type="integer" id="S4.SS3.p1.13.m13.2.2.cmml" xref="S4.SS3.p1.13.m13.2.2">2</cn><cn type="integer" id="S4.SS3.p1.13.m13.3.3.cmml" xref="S4.SS3.p1.13.m13.3.3">3</cn><cn type="integer" id="S4.SS3.p1.13.m13.4.4.cmml" xref="S4.SS3.p1.13.m13.4.4">4</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.13.m13.4c">j=1,2,3,4</annotation></semantics></math>), RNN of layers 1 and 2 with attention (<math id="S4.SS3.p1.14.m14.2" class="ltx_Math" alttext="j=1,2" display="inline"><semantics id="S4.SS3.p1.14.m14.2a"><mrow id="S4.SS3.p1.14.m14.2.3" xref="S4.SS3.p1.14.m14.2.3.cmml"><mi id="S4.SS3.p1.14.m14.2.3.2" xref="S4.SS3.p1.14.m14.2.3.2.cmml">j</mi><mo id="S4.SS3.p1.14.m14.2.3.1" xref="S4.SS3.p1.14.m14.2.3.1.cmml">=</mo><mrow id="S4.SS3.p1.14.m14.2.3.3.2" xref="S4.SS3.p1.14.m14.2.3.3.1.cmml"><mn id="S4.SS3.p1.14.m14.1.1" xref="S4.SS3.p1.14.m14.1.1.cmml">1</mn><mo id="S4.SS3.p1.14.m14.2.3.3.2.1" xref="S4.SS3.p1.14.m14.2.3.3.1.cmml">,</mo><mn id="S4.SS3.p1.14.m14.2.2" xref="S4.SS3.p1.14.m14.2.2.cmml">2</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.14.m14.2b"><apply id="S4.SS3.p1.14.m14.2.3.cmml" xref="S4.SS3.p1.14.m14.2.3"><eq id="S4.SS3.p1.14.m14.2.3.1.cmml" xref="S4.SS3.p1.14.m14.2.3.1"></eq><ci id="S4.SS3.p1.14.m14.2.3.2.cmml" xref="S4.SS3.p1.14.m14.2.3.2">𝑗</ci><list id="S4.SS3.p1.14.m14.2.3.3.1.cmml" xref="S4.SS3.p1.14.m14.2.3.3.2"><cn type="integer" id="S4.SS3.p1.14.m14.1.1.cmml" xref="S4.SS3.p1.14.m14.1.1">1</cn><cn type="integer" id="S4.SS3.p1.14.m14.2.2.cmml" xref="S4.SS3.p1.14.m14.2.2">2</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.14.m14.2c">j=1,2</annotation></semantics></math>), LSTM of layers 1 through 4 (<math id="S4.SS3.p1.15.m15.4" class="ltx_Math" alttext="j=1,2,3,4" display="inline"><semantics id="S4.SS3.p1.15.m15.4a"><mrow id="S4.SS3.p1.15.m15.4.5" xref="S4.SS3.p1.15.m15.4.5.cmml"><mi id="S4.SS3.p1.15.m15.4.5.2" xref="S4.SS3.p1.15.m15.4.5.2.cmml">j</mi><mo id="S4.SS3.p1.15.m15.4.5.1" xref="S4.SS3.p1.15.m15.4.5.1.cmml">=</mo><mrow id="S4.SS3.p1.15.m15.4.5.3.2" xref="S4.SS3.p1.15.m15.4.5.3.1.cmml"><mn id="S4.SS3.p1.15.m15.1.1" xref="S4.SS3.p1.15.m15.1.1.cmml">1</mn><mo id="S4.SS3.p1.15.m15.4.5.3.2.1" xref="S4.SS3.p1.15.m15.4.5.3.1.cmml">,</mo><mn id="S4.SS3.p1.15.m15.2.2" xref="S4.SS3.p1.15.m15.2.2.cmml">2</mn><mo id="S4.SS3.p1.15.m15.4.5.3.2.2" xref="S4.SS3.p1.15.m15.4.5.3.1.cmml">,</mo><mn id="S4.SS3.p1.15.m15.3.3" xref="S4.SS3.p1.15.m15.3.3.cmml">3</mn><mo id="S4.SS3.p1.15.m15.4.5.3.2.3" xref="S4.SS3.p1.15.m15.4.5.3.1.cmml">,</mo><mn id="S4.SS3.p1.15.m15.4.4" xref="S4.SS3.p1.15.m15.4.4.cmml">4</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.15.m15.4b"><apply id="S4.SS3.p1.15.m15.4.5.cmml" xref="S4.SS3.p1.15.m15.4.5"><eq id="S4.SS3.p1.15.m15.4.5.1.cmml" xref="S4.SS3.p1.15.m15.4.5.1"></eq><ci id="S4.SS3.p1.15.m15.4.5.2.cmml" xref="S4.SS3.p1.15.m15.4.5.2">𝑗</ci><list id="S4.SS3.p1.15.m15.4.5.3.1.cmml" xref="S4.SS3.p1.15.m15.4.5.3.2"><cn type="integer" id="S4.SS3.p1.15.m15.1.1.cmml" xref="S4.SS3.p1.15.m15.1.1">1</cn><cn type="integer" id="S4.SS3.p1.15.m15.2.2.cmml" xref="S4.SS3.p1.15.m15.2.2">2</cn><cn type="integer" id="S4.SS3.p1.15.m15.3.3.cmml" xref="S4.SS3.p1.15.m15.3.3">3</cn><cn type="integer" id="S4.SS3.p1.15.m15.4.4.cmml" xref="S4.SS3.p1.15.m15.4.4">4</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.15.m15.4c">j=1,2,3,4</annotation></semantics></math>), LSTM of layers 1 and 2 with attention (<math id="S4.SS3.p1.16.m16.2" class="ltx_Math" alttext="j=1,2" display="inline"><semantics id="S4.SS3.p1.16.m16.2a"><mrow id="S4.SS3.p1.16.m16.2.3" xref="S4.SS3.p1.16.m16.2.3.cmml"><mi id="S4.SS3.p1.16.m16.2.3.2" xref="S4.SS3.p1.16.m16.2.3.2.cmml">j</mi><mo id="S4.SS3.p1.16.m16.2.3.1" xref="S4.SS3.p1.16.m16.2.3.1.cmml">=</mo><mrow id="S4.SS3.p1.16.m16.2.3.3.2" xref="S4.SS3.p1.16.m16.2.3.3.1.cmml"><mn id="S4.SS3.p1.16.m16.1.1" xref="S4.SS3.p1.16.m16.1.1.cmml">1</mn><mo id="S4.SS3.p1.16.m16.2.3.3.2.1" xref="S4.SS3.p1.16.m16.2.3.3.1.cmml">,</mo><mn id="S4.SS3.p1.16.m16.2.2" xref="S4.SS3.p1.16.m16.2.2.cmml">2</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.16.m16.2b"><apply id="S4.SS3.p1.16.m16.2.3.cmml" xref="S4.SS3.p1.16.m16.2.3"><eq id="S4.SS3.p1.16.m16.2.3.1.cmml" xref="S4.SS3.p1.16.m16.2.3.1"></eq><ci id="S4.SS3.p1.16.m16.2.3.2.cmml" xref="S4.SS3.p1.16.m16.2.3.2">𝑗</ci><list id="S4.SS3.p1.16.m16.2.3.3.1.cmml" xref="S4.SS3.p1.16.m16.2.3.3.2"><cn type="integer" id="S4.SS3.p1.16.m16.1.1.cmml" xref="S4.SS3.p1.16.m16.1.1">1</cn><cn type="integer" id="S4.SS3.p1.16.m16.2.2.cmml" xref="S4.SS3.p1.16.m16.2.2">2</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.16.m16.2c">j=1,2</annotation></semantics></math>), identity connections and no connections.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Type of DARTS operations used in each component. “<math id="S4.T2.3.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.T2.3.m1.1b"><mi id="S4.T2.3.m1.1.1" xref="S4.T2.3.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.T2.3.m1.1c"><ci id="S4.T2.3.m1.1.1.cmml" xref="S4.T2.3.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.m1.1d">i</annotation></semantics></math>” represents the kernel size in CNN while “<math id="S4.T2.4.m2.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.T2.4.m2.1b"><mi id="S4.T2.4.m2.1.1" xref="S4.T2.4.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.T2.4.m2.1c"><ci id="S4.T2.4.m2.1.1.cmml" xref="S4.T2.4.m2.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.m2.1d">j</annotation></semantics></math>” represents the number of layers in the SeqNN component.</figcaption>
<table id="S4.T2.28" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.28.25.1" class="ltx_tr">
<th id="S4.T2.28.25.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.28.25.1.1.1" class="ltx_text ltx_font_bold">Component</span></th>
<th id="S4.T2.28.25.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S4.T2.28.25.1.2.1" class="ltx_text ltx_font_bold">Operation</span></th>
<th id="S4.T2.28.25.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T2.28.25.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.28.25.1.3.1.1" class="ltx_p" style="width:195.1pt;"><span id="S4.T2.28.25.1.3.1.1.1" class="ltx_text ltx_font_bold">Description</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.7.3" class="ltx_tr">
<td id="S4.T2.7.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">CNN</td>
<td id="S4.T2.6.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">max_pool_<math id="S4.T2.5.1.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.T2.5.1.1.m1.1a"><mi id="S4.T2.5.1.1.m1.1.1" xref="S4.T2.5.1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.T2.5.1.1.m1.1b"><ci id="S4.T2.5.1.1.m1.1.1.cmml" xref="S4.T2.5.1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.1.1.m1.1c">i</annotation></semantics></math>x<math id="S4.T2.6.2.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.T2.6.2.2.m2.1a"><mi id="S4.T2.6.2.2.m2.1.1" xref="S4.T2.6.2.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.T2.6.2.2.m2.1b"><ci id="S4.T2.6.2.2.m2.1.1.cmml" xref="S4.T2.6.2.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.2.2.m2.1c">i</annotation></semantics></math>
</td>
<td id="S4.T2.7.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T2.7.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.7.3.3.1.1" class="ltx_p" style="width:195.1pt;">Max Pooling layer with kernel <math id="S4.T2.7.3.3.1.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.T2.7.3.3.1.1.m1.1a"><mi id="S4.T2.7.3.3.1.1.m1.1.1" xref="S4.T2.7.3.3.1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.T2.7.3.3.1.1.m1.1b"><ci id="S4.T2.7.3.3.1.1.m1.1.1.cmml" xref="S4.T2.7.3.3.1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.3.3.1.1.m1.1c">i</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T2.10.6" class="ltx_tr">
<td id="S4.T2.10.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">CNN</td>
<td id="S4.T2.9.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">avg_pool_<math id="S4.T2.8.4.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.T2.8.4.1.m1.1a"><mi id="S4.T2.8.4.1.m1.1.1" xref="S4.T2.8.4.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.T2.8.4.1.m1.1b"><ci id="S4.T2.8.4.1.m1.1.1.cmml" xref="S4.T2.8.4.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.4.1.m1.1c">i</annotation></semantics></math>x<math id="S4.T2.9.5.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.T2.9.5.2.m2.1a"><mi id="S4.T2.9.5.2.m2.1.1" xref="S4.T2.9.5.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.T2.9.5.2.m2.1b"><ci id="S4.T2.9.5.2.m2.1.1.cmml" xref="S4.T2.9.5.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.5.2.m2.1c">i</annotation></semantics></math>
</td>
<td id="S4.T2.10.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T2.10.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.10.6.3.1.1" class="ltx_p" style="width:195.1pt;">Average Pooling layer with kernel <math id="S4.T2.10.6.3.1.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.T2.10.6.3.1.1.m1.1a"><mi id="S4.T2.10.6.3.1.1.m1.1.1" xref="S4.T2.10.6.3.1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.T2.10.6.3.1.1.m1.1b"><ci id="S4.T2.10.6.3.1.1.m1.1.1.cmml" xref="S4.T2.10.6.3.1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.6.3.1.1.m1.1c">i</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T2.13.9" class="ltx_tr">
<td id="S4.T2.13.9.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">CNN</td>
<td id="S4.T2.12.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">dil_conv_<math id="S4.T2.11.7.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.T2.11.7.1.m1.1a"><mi id="S4.T2.11.7.1.m1.1.1" xref="S4.T2.11.7.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.T2.11.7.1.m1.1b"><ci id="S4.T2.11.7.1.m1.1.1.cmml" xref="S4.T2.11.7.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.7.1.m1.1c">i</annotation></semantics></math>x<math id="S4.T2.12.8.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.T2.12.8.2.m2.1a"><mi id="S4.T2.12.8.2.m2.1.1" xref="S4.T2.12.8.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.T2.12.8.2.m2.1b"><ci id="S4.T2.12.8.2.m2.1.1.cmml" xref="S4.T2.12.8.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.8.2.m2.1c">i</annotation></semantics></math>
</td>
<td id="S4.T2.13.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T2.13.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.13.9.3.1.1" class="ltx_p" style="width:195.1pt;">Dilated Convolution layer with kernel <math id="S4.T2.13.9.3.1.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.T2.13.9.3.1.1.m1.1a"><mi id="S4.T2.13.9.3.1.1.m1.1.1" xref="S4.T2.13.9.3.1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.T2.13.9.3.1.1.m1.1b"><ci id="S4.T2.13.9.3.1.1.m1.1.1.cmml" xref="S4.T2.13.9.3.1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.13.9.3.1.1.m1.1c">i</annotation></semantics></math>, and dilation 2</span>
</span>
</td>
</tr>
<tr id="S4.T2.16.12" class="ltx_tr">
<td id="S4.T2.16.12.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">CNN</td>
<td id="S4.T2.15.11.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">sep_conv_<math id="S4.T2.14.10.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.T2.14.10.1.m1.1a"><mi id="S4.T2.14.10.1.m1.1.1" xref="S4.T2.14.10.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.T2.14.10.1.m1.1b"><ci id="S4.T2.14.10.1.m1.1.1.cmml" xref="S4.T2.14.10.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.14.10.1.m1.1c">i</annotation></semantics></math>x<math id="S4.T2.15.11.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.T2.15.11.2.m2.1a"><mi id="S4.T2.15.11.2.m2.1.1" xref="S4.T2.15.11.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.T2.15.11.2.m2.1b"><ci id="S4.T2.15.11.2.m2.1.1.cmml" xref="S4.T2.15.11.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.15.11.2.m2.1c">i</annotation></semantics></math>
</td>
<td id="S4.T2.16.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T2.16.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.16.12.3.1.1" class="ltx_p" style="width:195.1pt;">Two Convolution layers with kernel <math id="S4.T2.16.12.3.1.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.T2.16.12.3.1.1.m1.1a"><mi id="S4.T2.16.12.3.1.1.m1.1.1" xref="S4.T2.16.12.3.1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.T2.16.12.3.1.1.m1.1b"><ci id="S4.T2.16.12.3.1.1.m1.1.1.cmml" xref="S4.T2.16.12.3.1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.16.12.3.1.1.m1.1c">i</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S4.T2.20.16" class="ltx_tr">
<td id="S4.T2.20.16.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">CNN</td>
<td id="S4.T2.18.14.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">conv_<math id="S4.T2.17.13.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.T2.17.13.1.m1.1a"><mi id="S4.T2.17.13.1.m1.1.1" xref="S4.T2.17.13.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.T2.17.13.1.m1.1b"><ci id="S4.T2.17.13.1.m1.1.1.cmml" xref="S4.T2.17.13.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.17.13.1.m1.1c">i</annotation></semantics></math>x1_1x<math id="S4.T2.18.14.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.T2.18.14.2.m2.1a"><mi id="S4.T2.18.14.2.m2.1.1" xref="S4.T2.18.14.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.T2.18.14.2.m2.1b"><ci id="S4.T2.18.14.2.m2.1.1.cmml" xref="S4.T2.18.14.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.18.14.2.m2.1c">i</annotation></semantics></math>
</td>
<td id="S4.T2.20.16.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T2.20.16.4.2" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.20.16.4.2.2" class="ltx_p" style="width:195.1pt;">Two Convolution layers with first kernel (<math id="S4.T2.19.15.3.1.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.T2.19.15.3.1.1.m1.1a"><mi id="S4.T2.19.15.3.1.1.m1.1.1" xref="S4.T2.19.15.3.1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.T2.19.15.3.1.1.m1.1b"><ci id="S4.T2.19.15.3.1.1.m1.1.1.cmml" xref="S4.T2.19.15.3.1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.19.15.3.1.1.m1.1c">i</annotation></semantics></math>x1) and second (1x<math id="S4.T2.20.16.4.2.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.T2.20.16.4.2.2.m2.1a"><mi id="S4.T2.20.16.4.2.2.m2.1.1" xref="S4.T2.20.16.4.2.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.T2.20.16.4.2.2.m2.1b"><ci id="S4.T2.20.16.4.2.2.m2.1.1.cmml" xref="S4.T2.20.16.4.2.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.20.16.4.2.2.m2.1c">i</annotation></semantics></math>)</span>
</span>
</td>
</tr>
<tr id="S4.T2.22.18" class="ltx_tr">
<td id="S4.T2.22.18.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">SeqNN</td>
<td id="S4.T2.21.17.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">lstm_<math id="S4.T2.21.17.1.m1.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.T2.21.17.1.m1.1a"><mi id="S4.T2.21.17.1.m1.1.1" xref="S4.T2.21.17.1.m1.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.T2.21.17.1.m1.1b"><ci id="S4.T2.21.17.1.m1.1.1.cmml" xref="S4.T2.21.17.1.m1.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.21.17.1.m1.1c">j</annotation></semantics></math>
</td>
<td id="S4.T2.22.18.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T2.22.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.22.18.2.1.1" class="ltx_p" style="width:195.1pt;">LSTM with <math id="S4.T2.22.18.2.1.1.m1.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.T2.22.18.2.1.1.m1.1a"><mi id="S4.T2.22.18.2.1.1.m1.1.1" xref="S4.T2.22.18.2.1.1.m1.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.T2.22.18.2.1.1.m1.1b"><ci id="S4.T2.22.18.2.1.1.m1.1.1.cmml" xref="S4.T2.22.18.2.1.1.m1.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.22.18.2.1.1.m1.1c">j</annotation></semantics></math> layers</span>
</span>
</td>
</tr>
<tr id="S4.T2.24.20" class="ltx_tr">
<td id="S4.T2.24.20.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">SeqNN</td>
<td id="S4.T2.23.19.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">lstm_att_<math id="S4.T2.23.19.1.m1.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.T2.23.19.1.m1.1a"><mi id="S4.T2.23.19.1.m1.1.1" xref="S4.T2.23.19.1.m1.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.T2.23.19.1.m1.1b"><ci id="S4.T2.23.19.1.m1.1.1.cmml" xref="S4.T2.23.19.1.m1.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.23.19.1.m1.1c">j</annotation></semantics></math>
</td>
<td id="S4.T2.24.20.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T2.24.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.24.20.2.1.1" class="ltx_p" style="width:195.1pt;">LSTM of <math id="S4.T2.24.20.2.1.1.m1.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.T2.24.20.2.1.1.m1.1a"><mi id="S4.T2.24.20.2.1.1.m1.1.1" xref="S4.T2.24.20.2.1.1.m1.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.T2.24.20.2.1.1.m1.1b"><ci id="S4.T2.24.20.2.1.1.m1.1.1.cmml" xref="S4.T2.24.20.2.1.1.m1.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.24.20.2.1.1.m1.1c">j</annotation></semantics></math> layers with Attention</span>
</span>
</td>
</tr>
<tr id="S4.T2.26.22" class="ltx_tr">
<td id="S4.T2.26.22.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">SeqNN</td>
<td id="S4.T2.25.21.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">rnn_<math id="S4.T2.25.21.1.m1.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.T2.25.21.1.m1.1a"><mi id="S4.T2.25.21.1.m1.1.1" xref="S4.T2.25.21.1.m1.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.T2.25.21.1.m1.1b"><ci id="S4.T2.25.21.1.m1.1.1.cmml" xref="S4.T2.25.21.1.m1.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.25.21.1.m1.1c">j</annotation></semantics></math>
</td>
<td id="S4.T2.26.22.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T2.26.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.26.22.2.1.1" class="ltx_p" style="width:195.1pt;">RNN with <math id="S4.T2.26.22.2.1.1.m1.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.T2.26.22.2.1.1.m1.1a"><mi id="S4.T2.26.22.2.1.1.m1.1.1" xref="S4.T2.26.22.2.1.1.m1.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.T2.26.22.2.1.1.m1.1b"><ci id="S4.T2.26.22.2.1.1.m1.1.1.cmml" xref="S4.T2.26.22.2.1.1.m1.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.26.22.2.1.1.m1.1c">j</annotation></semantics></math> layers</span>
</span>
</td>
</tr>
<tr id="S4.T2.28.24" class="ltx_tr">
<td id="S4.T2.28.24.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">SeqNN</td>
<td id="S4.T2.27.23.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">rnn_att_<math id="S4.T2.27.23.1.m1.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.T2.27.23.1.m1.1a"><mi id="S4.T2.27.23.1.m1.1.1" xref="S4.T2.27.23.1.m1.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.T2.27.23.1.m1.1b"><ci id="S4.T2.27.23.1.m1.1.1.cmml" xref="S4.T2.27.23.1.m1.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.27.23.1.m1.1c">j</annotation></semantics></math>
</td>
<td id="S4.T2.28.24.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S4.T2.28.24.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.28.24.2.1.1" class="ltx_p" style="width:195.1pt;">RNN of <math id="S4.T2.28.24.2.1.1.m1.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.T2.28.24.2.1.1.m1.1a"><mi id="S4.T2.28.24.2.1.1.m1.1.1" xref="S4.T2.28.24.2.1.1.m1.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.T2.28.24.2.1.1.m1.1b"><ci id="S4.T2.28.24.2.1.1.m1.1.1.cmml" xref="S4.T2.28.24.2.1.1.m1.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.28.24.2.1.1.m1.1c">j</annotation></semantics></math> layers with Attention</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.3" class="ltx_p">We use stochastic gradient descent with a learning rate from <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="0.025" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mn id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">0.025</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><cn type="float" id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">0.025</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">0.025</annotation></semantics></math> to <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mn id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><cn type="float" id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">0.001</annotation></semantics></math>
using a cosine annealing schedule as the optimiser to optimise the weights of the operations. The search is run for <math id="S4.SS3.p2.3.m3.1" class="ltx_Math" alttext="300" display="inline"><semantics id="S4.SS3.p2.3.m3.1a"><mn id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml">300</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><cn type="integer" id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1">300</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">300</annotation></semantics></math> epochs.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.7" class="ltx_p">In our experiments, we use four DARTS cells (<math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="C=4" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mrow id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mi id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml">C</mi><mo id="S4.SS3.p3.1.m1.1.1.1" xref="S4.SS3.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><eq id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1.1"></eq><ci id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">𝐶</ci><cn type="integer" id="S4.SS3.p3.1.m1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">C=4</annotation></semantics></math>) for the CNN component following the work of Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and two DARTS cells (<math id="S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="N=2" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><mrow id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml"><mi id="S4.SS3.p3.2.m2.1.1.2" xref="S4.SS3.p3.2.m2.1.1.2.cmml">N</mi><mo id="S4.SS3.p3.2.m2.1.1.1" xref="S4.SS3.p3.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS3.p3.2.m2.1.1.3" xref="S4.SS3.p3.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><apply id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1"><eq id="S4.SS3.p3.2.m2.1.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1.1"></eq><ci id="S4.SS3.p3.2.m2.1.1.2.cmml" xref="S4.SS3.p3.2.m2.1.1.2">𝑁</ci><cn type="integer" id="S4.SS3.p3.2.m2.1.1.3.cmml" xref="S4.SS3.p3.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">N=2</annotation></semantics></math>) for the SeqNN component. The intuition of using <math id="S4.SS3.p3.3.m3.1" class="ltx_Math" alttext="N=2" display="inline"><semantics id="S4.SS3.p3.3.m3.1a"><mrow id="S4.SS3.p3.3.m3.1.1" xref="S4.SS3.p3.3.m3.1.1.cmml"><mi id="S4.SS3.p3.3.m3.1.1.2" xref="S4.SS3.p3.3.m3.1.1.2.cmml">N</mi><mo id="S4.SS3.p3.3.m3.1.1.1" xref="S4.SS3.p3.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS3.p3.3.m3.1.1.3" xref="S4.SS3.p3.3.m3.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m3.1b"><apply id="S4.SS3.p3.3.m3.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1"><eq id="S4.SS3.p3.3.m3.1.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1.1"></eq><ci id="S4.SS3.p3.3.m3.1.1.2.cmml" xref="S4.SS3.p3.3.m3.1.1.2">𝑁</ci><cn type="integer" id="S4.SS3.p3.3.m3.1.1.3.cmml" xref="S4.SS3.p3.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m3.1c">N=2</annotation></semantics></math> for the SeqNN component is discussed in section <a href="#S6.SS2" title="6.2 Converging to a local minima ‣ 6 Discussion ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2</span></a>.
As defined in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, we apply reduction cells at every <math id="S4.SS3.p3.4.m4.1" class="ltx_Math" alttext="\frac{1}{3}C^{th}" display="inline"><semantics id="S4.SS3.p3.4.m4.1a"><mrow id="S4.SS3.p3.4.m4.1.1" xref="S4.SS3.p3.4.m4.1.1.cmml"><mfrac id="S4.SS3.p3.4.m4.1.1.2" xref="S4.SS3.p3.4.m4.1.1.2.cmml"><mn id="S4.SS3.p3.4.m4.1.1.2.2" xref="S4.SS3.p3.4.m4.1.1.2.2.cmml">1</mn><mn id="S4.SS3.p3.4.m4.1.1.2.3" xref="S4.SS3.p3.4.m4.1.1.2.3.cmml">3</mn></mfrac><mo lspace="0em" rspace="0em" id="S4.SS3.p3.4.m4.1.1.1" xref="S4.SS3.p3.4.m4.1.1.1.cmml">​</mo><msup id="S4.SS3.p3.4.m4.1.1.3" xref="S4.SS3.p3.4.m4.1.1.3.cmml"><mi id="S4.SS3.p3.4.m4.1.1.3.2" xref="S4.SS3.p3.4.m4.1.1.3.2.cmml">C</mi><mrow id="S4.SS3.p3.4.m4.1.1.3.3" xref="S4.SS3.p3.4.m4.1.1.3.3.cmml"><mi id="S4.SS3.p3.4.m4.1.1.3.3.2" xref="S4.SS3.p3.4.m4.1.1.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p3.4.m4.1.1.3.3.1" xref="S4.SS3.p3.4.m4.1.1.3.3.1.cmml">​</mo><mi id="S4.SS3.p3.4.m4.1.1.3.3.3" xref="S4.SS3.p3.4.m4.1.1.3.3.3.cmml">h</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.4.m4.1b"><apply id="S4.SS3.p3.4.m4.1.1.cmml" xref="S4.SS3.p3.4.m4.1.1"><times id="S4.SS3.p3.4.m4.1.1.1.cmml" xref="S4.SS3.p3.4.m4.1.1.1"></times><apply id="S4.SS3.p3.4.m4.1.1.2.cmml" xref="S4.SS3.p3.4.m4.1.1.2"><divide id="S4.SS3.p3.4.m4.1.1.2.1.cmml" xref="S4.SS3.p3.4.m4.1.1.2"></divide><cn type="integer" id="S4.SS3.p3.4.m4.1.1.2.2.cmml" xref="S4.SS3.p3.4.m4.1.1.2.2">1</cn><cn type="integer" id="S4.SS3.p3.4.m4.1.1.2.3.cmml" xref="S4.SS3.p3.4.m4.1.1.2.3">3</cn></apply><apply id="S4.SS3.p3.4.m4.1.1.3.cmml" xref="S4.SS3.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p3.4.m4.1.1.3.1.cmml" xref="S4.SS3.p3.4.m4.1.1.3">superscript</csymbol><ci id="S4.SS3.p3.4.m4.1.1.3.2.cmml" xref="S4.SS3.p3.4.m4.1.1.3.2">𝐶</ci><apply id="S4.SS3.p3.4.m4.1.1.3.3.cmml" xref="S4.SS3.p3.4.m4.1.1.3.3"><times id="S4.SS3.p3.4.m4.1.1.3.3.1.cmml" xref="S4.SS3.p3.4.m4.1.1.3.3.1"></times><ci id="S4.SS3.p3.4.m4.1.1.3.3.2.cmml" xref="S4.SS3.p3.4.m4.1.1.3.3.2">𝑡</ci><ci id="S4.SS3.p3.4.m4.1.1.3.3.3.cmml" xref="S4.SS3.p3.4.m4.1.1.3.3.3">ℎ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.4.m4.1c">\frac{1}{3}C^{th}</annotation></semantics></math> and <math id="S4.SS3.p3.5.m5.1" class="ltx_Math" alttext="\frac{2}{3}C^{th}" display="inline"><semantics id="S4.SS3.p3.5.m5.1a"><mrow id="S4.SS3.p3.5.m5.1.1" xref="S4.SS3.p3.5.m5.1.1.cmml"><mfrac id="S4.SS3.p3.5.m5.1.1.2" xref="S4.SS3.p3.5.m5.1.1.2.cmml"><mn id="S4.SS3.p3.5.m5.1.1.2.2" xref="S4.SS3.p3.5.m5.1.1.2.2.cmml">2</mn><mn id="S4.SS3.p3.5.m5.1.1.2.3" xref="S4.SS3.p3.5.m5.1.1.2.3.cmml">3</mn></mfrac><mo lspace="0em" rspace="0em" id="S4.SS3.p3.5.m5.1.1.1" xref="S4.SS3.p3.5.m5.1.1.1.cmml">​</mo><msup id="S4.SS3.p3.5.m5.1.1.3" xref="S4.SS3.p3.5.m5.1.1.3.cmml"><mi id="S4.SS3.p3.5.m5.1.1.3.2" xref="S4.SS3.p3.5.m5.1.1.3.2.cmml">C</mi><mrow id="S4.SS3.p3.5.m5.1.1.3.3" xref="S4.SS3.p3.5.m5.1.1.3.3.cmml"><mi id="S4.SS3.p3.5.m5.1.1.3.3.2" xref="S4.SS3.p3.5.m5.1.1.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p3.5.m5.1.1.3.3.1" xref="S4.SS3.p3.5.m5.1.1.3.3.1.cmml">​</mo><mi id="S4.SS3.p3.5.m5.1.1.3.3.3" xref="S4.SS3.p3.5.m5.1.1.3.3.3.cmml">h</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.5.m5.1b"><apply id="S4.SS3.p3.5.m5.1.1.cmml" xref="S4.SS3.p3.5.m5.1.1"><times id="S4.SS3.p3.5.m5.1.1.1.cmml" xref="S4.SS3.p3.5.m5.1.1.1"></times><apply id="S4.SS3.p3.5.m5.1.1.2.cmml" xref="S4.SS3.p3.5.m5.1.1.2"><divide id="S4.SS3.p3.5.m5.1.1.2.1.cmml" xref="S4.SS3.p3.5.m5.1.1.2"></divide><cn type="integer" id="S4.SS3.p3.5.m5.1.1.2.2.cmml" xref="S4.SS3.p3.5.m5.1.1.2.2">2</cn><cn type="integer" id="S4.SS3.p3.5.m5.1.1.2.3.cmml" xref="S4.SS3.p3.5.m5.1.1.2.3">3</cn></apply><apply id="S4.SS3.p3.5.m5.1.1.3.cmml" xref="S4.SS3.p3.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p3.5.m5.1.1.3.1.cmml" xref="S4.SS3.p3.5.m5.1.1.3">superscript</csymbol><ci id="S4.SS3.p3.5.m5.1.1.3.2.cmml" xref="S4.SS3.p3.5.m5.1.1.3.2">𝐶</ci><apply id="S4.SS3.p3.5.m5.1.1.3.3.cmml" xref="S4.SS3.p3.5.m5.1.1.3.3"><times id="S4.SS3.p3.5.m5.1.1.3.3.1.cmml" xref="S4.SS3.p3.5.m5.1.1.3.3.1"></times><ci id="S4.SS3.p3.5.m5.1.1.3.3.2.cmml" xref="S4.SS3.p3.5.m5.1.1.3.3.2">𝑡</ci><ci id="S4.SS3.p3.5.m5.1.1.3.3.3.cmml" xref="S4.SS3.p3.5.m5.1.1.3.3.3">ℎ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.5.m5.1c">\frac{2}{3}C^{th}</annotation></semantics></math> position of the layers in CNN component. We randomly initialise <math id="S4.SS3.p3.6.m6.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS3.p3.6.m6.1a"><mi id="S4.SS3.p3.6.m6.1.1" xref="S4.SS3.p3.6.m6.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.6.m6.1b"><ci id="S4.SS3.p3.6.m6.1.1.cmml" xref="S4.SS3.p3.6.m6.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.6.m6.1c">\alpha</annotation></semantics></math> values and the DARTS search algorithm optimises <math id="S4.SS3.p3.7.m7.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS3.p3.7.m7.1a"><mi id="S4.SS3.p3.7.m7.1.1" xref="S4.SS3.p3.7.m7.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.7.m7.1b"><ci id="S4.SS3.p3.7.m7.1.1.cmml" xref="S4.SS3.p3.7.m7.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.7.m7.1c">\alpha</annotation></semantics></math> values related to each operation.
The output from the CNN component is flattened to a vector before passing to the SeqNN component to adjust the input dimension of the RNN and LSTM layers.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">Once the search operation completes, it outputs the architecture of DARTS cells, which is called “genome”. We create a deep learning model with the CNN component having four CNN cells and the SeqNN component having two SeqNN cells. This model is trained for 300 epochs with the training set of the datasets to minimise the loss.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.1" class="ltx_p">We use the popular deep learning library PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> for model development and training. The experiments are run on an NVIDIA A100 GPU with 40 GB of VRAM. We published the source code related to our research in a dedicated GitHub repository, allowing for smooth replication of our research findings<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/iot-health/emoDARTS</span></span></span></span>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Evaluation</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We report the results using the Unweighted Accuracy (UA%), calculated by dividing the total of all classes’ recall by their number. This is recognised to depict unbalanced data workloads intrinsic to SER accurately. We additionally provide the Weighted Accuracy (WA%) mainly to compare our results with relevant studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Last but not least, we also report the number of parameters of the model as an indication of the model’s complexity, calculated by adding all trainable parameters in the created model.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span><span id="S5.SS1.1.1" class="ltx_text ltx_font_italic">CNN only model</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.2" class="ltx_p">We initially assess the performance of the CNN-only model generated by DARTS (CNN – DARTS) compared to our benchmark model, specifically CNN – w/o DARTS, using the IEMOCAP dataset. The results, detailed in Table <a href="#S5.T3" title="TABLE III ‣ 5.1 CNN only model ‣ 5 Evaluation ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, reveal that the DARTS-generated CNN model outperforms the performance of the baseline SER model. Additionally, Table <a href="#S5.T3" title="TABLE III ‣ 5.1 CNN only model ‣ 5 Evaluation ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> illustrates the performance of the DARTS-generated model with eight cells (<math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="C=8" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mrow id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">C</mi><mo id="S5.SS1.p1.1.m1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><eq id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1"></eq><ci id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2">𝐶</ci><cn type="integer" id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">C=8</annotation></semantics></math>), showing a lower performance compared to its counterpart with <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="C=4" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mrow id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml"><mi id="S5.SS1.p1.2.m2.1.1.2" xref="S5.SS1.p1.2.m2.1.1.2.cmml">C</mi><mo id="S5.SS1.p1.2.m2.1.1.1" xref="S5.SS1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS1.p1.2.m2.1.1.3" xref="S5.SS1.p1.2.m2.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><apply id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"><eq id="S5.SS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1.1"></eq><ci id="S5.SS1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.p1.2.m2.1.1.2">𝐶</ci><cn type="integer" id="S5.SS1.p1.2.m2.1.1.3.cmml" xref="S5.SS1.p1.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">C=4</annotation></semantics></math>. This decline in performance with an increased number of cells indicates a rise in the model’s complexity, leading to overfitting and subsequent accuracy reduction.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">We further examine the results from the CNN branch of Sun et al.’s EmotionNAS model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> to highlight performance enhancements. For a direct and clear comparison of performance, we specifically utilise the ‘Spectrogram Branch’ of EmotionNAS, contrasting it with our ‘CNN – DARTS’ model. This focused comparison is chosen to ensure a fair evaluation since both models share a similar architecture.
We see that the performance of our CNN – DARTS models surpasses the performance of the CNN branch of EmotionNAS by at least 5%.
It is worth noting that the ‘whole model’ of EmotionNAS has a different architecture, employing a branched structure, while emoDARTS utilises a stacked architecture.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Performance comparison between the DARTS generated CNN model (CNN – DARTS) and a CNN SER model developed without DARTS (CNN – w/o DARTS) for the IEMOCAP dataset. 
<br class="ltx_break"><span id="S5.T3.8.1" class="ltx_text ltx_font_italic">The number of parameters is in thousands</span></figcaption>
<table id="S5.T3.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.6.7.1" class="ltx_tr">
<th id="S5.T3.6.7.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S5.T3.6.7.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.6.7.1.1.1.1" class="ltx_p" style="width:99.7pt;"><span id="S5.T3.6.7.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></span>
</span>
</th>
<th id="S5.T3.6.7.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T3.6.7.1.2.1" class="ltx_text ltx_font_bold">Param.</span></th>
<th id="S5.T3.6.7.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T3.6.7.1.3.1" class="ltx_text ltx_font_bold">Cell</span></th>
<th id="S5.T3.6.7.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T3.6.7.1.4.1" class="ltx_text ltx_font_bold">UA (%)</span></th>
<th id="S5.T3.6.7.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T3.6.7.1.5.1" class="ltx_text ltx_font_bold">WA (%)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.2.2" class="ltx_tr">
<td id="S5.T3.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S5.T3.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.2.2.3.1.1" class="ltx_p" style="width:99.7pt;">CNN – DARTS</span>
</span>
</td>
<td id="S5.T3.2.2.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T3.2.2.4.1" class="ltx_text ltx_font_bold">417K</span></td>
<td id="S5.T3.2.2.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">4</td>
<td id="S5.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T3.1.1.1.1" class="ltx_text ltx_font_bold">69.36 <math id="S5.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T3.1.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T3.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.m1.1c">\pm</annotation></semantics></math> 3.00</span></td>
<td id="S5.T3.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><math id="S5.T3.2.2.2.m1.1" class="ltx_Math" alttext="72.55\pm 3.70" display="inline"><semantics id="S5.T3.2.2.2.m1.1a"><mrow id="S5.T3.2.2.2.m1.1.1" xref="S5.T3.2.2.2.m1.1.1.cmml"><mn id="S5.T3.2.2.2.m1.1.1.2" xref="S5.T3.2.2.2.m1.1.1.2.cmml">72.55</mn><mo id="S5.T3.2.2.2.m1.1.1.1" xref="S5.T3.2.2.2.m1.1.1.1.cmml">±</mo><mn id="S5.T3.2.2.2.m1.1.1.3" xref="S5.T3.2.2.2.m1.1.1.3.cmml">3.70</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.m1.1b"><apply id="S5.T3.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.m1.1.1"><csymbol cd="latexml" id="S5.T3.2.2.2.m1.1.1.1.cmml" xref="S5.T3.2.2.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T3.2.2.2.m1.1.1.2.cmml" xref="S5.T3.2.2.2.m1.1.1.2">72.55</cn><cn type="float" id="S5.T3.2.2.2.m1.1.1.3.cmml" xref="S5.T3.2.2.2.m1.1.1.3">3.70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.m1.1c">72.55\pm 3.70</annotation></semantics></math></td>
</tr>
<tr id="S5.T3.4.4" class="ltx_tr">
<td id="S5.T3.4.4.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S5.T3.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.4.4.3.1.1" class="ltx_p" style="width:99.7pt;">CNN – DARTS</span>
</span>
</td>
<td id="S5.T3.4.4.4" class="ltx_td ltx_align_right" style="padding-top:1.5pt;padding-bottom:1.5pt;">428K</td>
<td id="S5.T3.4.4.5" class="ltx_td ltx_align_right" style="padding-top:1.5pt;padding-bottom:1.5pt;">8</td>
<td id="S5.T3.3.3.1" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><math id="S5.T3.3.3.1.m1.1" class="ltx_Math" alttext="62.25\pm 6.74" display="inline"><semantics id="S5.T3.3.3.1.m1.1a"><mrow id="S5.T3.3.3.1.m1.1.1" xref="S5.T3.3.3.1.m1.1.1.cmml"><mn id="S5.T3.3.3.1.m1.1.1.2" xref="S5.T3.3.3.1.m1.1.1.2.cmml">62.25</mn><mo id="S5.T3.3.3.1.m1.1.1.1" xref="S5.T3.3.3.1.m1.1.1.1.cmml">±</mo><mn id="S5.T3.3.3.1.m1.1.1.3" xref="S5.T3.3.3.1.m1.1.1.3.cmml">6.74</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.1.m1.1b"><apply id="S5.T3.3.3.1.m1.1.1.cmml" xref="S5.T3.3.3.1.m1.1.1"><csymbol cd="latexml" id="S5.T3.3.3.1.m1.1.1.1.cmml" xref="S5.T3.3.3.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T3.3.3.1.m1.1.1.2.cmml" xref="S5.T3.3.3.1.m1.1.1.2">62.25</cn><cn type="float" id="S5.T3.3.3.1.m1.1.1.3.cmml" xref="S5.T3.3.3.1.m1.1.1.3">6.74</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.1.m1.1c">62.25\pm 6.74</annotation></semantics></math></td>
<td id="S5.T3.4.4.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><math id="S5.T3.4.4.2.m1.1" class="ltx_Math" alttext="63.78\pm 6.83" display="inline"><semantics id="S5.T3.4.4.2.m1.1a"><mrow id="S5.T3.4.4.2.m1.1.1" xref="S5.T3.4.4.2.m1.1.1.cmml"><mn id="S5.T3.4.4.2.m1.1.1.2" xref="S5.T3.4.4.2.m1.1.1.2.cmml">63.78</mn><mo id="S5.T3.4.4.2.m1.1.1.1" xref="S5.T3.4.4.2.m1.1.1.1.cmml">±</mo><mn id="S5.T3.4.4.2.m1.1.1.3" xref="S5.T3.4.4.2.m1.1.1.3.cmml">6.83</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.2.m1.1b"><apply id="S5.T3.4.4.2.m1.1.1.cmml" xref="S5.T3.4.4.2.m1.1.1"><csymbol cd="latexml" id="S5.T3.4.4.2.m1.1.1.1.cmml" xref="S5.T3.4.4.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T3.4.4.2.m1.1.1.2.cmml" xref="S5.T3.4.4.2.m1.1.1.2">63.78</cn><cn type="float" id="S5.T3.4.4.2.m1.1.1.3.cmml" xref="S5.T3.4.4.2.m1.1.1.3">6.83</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.2.m1.1c">63.78\pm 6.83</annotation></semantics></math></td>
</tr>
<tr id="S5.T3.6.6" class="ltx_tr">
<td id="S5.T3.6.6.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S5.T3.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.6.6.3.1.1" class="ltx_p" style="width:99.7pt;">CNN – w/o DARTS</span>
</span>
</td>
<td id="S5.T3.6.6.4" class="ltx_td ltx_align_right" style="padding-top:1.5pt;padding-bottom:1.5pt;">35K</td>
<td id="S5.T3.6.6.5" class="ltx_td ltx_align_right" style="padding-top:1.5pt;padding-bottom:1.5pt;">-</td>
<td id="S5.T3.5.5.1" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><math id="S5.T3.5.5.1.m1.1" class="ltx_Math" alttext="50.04\pm 2.69" display="inline"><semantics id="S5.T3.5.5.1.m1.1a"><mrow id="S5.T3.5.5.1.m1.1.1" xref="S5.T3.5.5.1.m1.1.1.cmml"><mn id="S5.T3.5.5.1.m1.1.1.2" xref="S5.T3.5.5.1.m1.1.1.2.cmml">50.04</mn><mo id="S5.T3.5.5.1.m1.1.1.1" xref="S5.T3.5.5.1.m1.1.1.1.cmml">±</mo><mn id="S5.T3.5.5.1.m1.1.1.3" xref="S5.T3.5.5.1.m1.1.1.3.cmml">2.69</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.1.m1.1b"><apply id="S5.T3.5.5.1.m1.1.1.cmml" xref="S5.T3.5.5.1.m1.1.1"><csymbol cd="latexml" id="S5.T3.5.5.1.m1.1.1.1.cmml" xref="S5.T3.5.5.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T3.5.5.1.m1.1.1.2.cmml" xref="S5.T3.5.5.1.m1.1.1.2">50.04</cn><cn type="float" id="S5.T3.5.5.1.m1.1.1.3.cmml" xref="S5.T3.5.5.1.m1.1.1.3">2.69</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.1.m1.1c">50.04\pm 2.69</annotation></semantics></math></td>
<td id="S5.T3.6.6.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><math id="S5.T3.6.6.2.m1.1" class="ltx_Math" alttext="51.01\pm 2.23" display="inline"><semantics id="S5.T3.6.6.2.m1.1a"><mrow id="S5.T3.6.6.2.m1.1.1" xref="S5.T3.6.6.2.m1.1.1.cmml"><mn id="S5.T3.6.6.2.m1.1.1.2" xref="S5.T3.6.6.2.m1.1.1.2.cmml">51.01</mn><mo id="S5.T3.6.6.2.m1.1.1.1" xref="S5.T3.6.6.2.m1.1.1.1.cmml">±</mo><mn id="S5.T3.6.6.2.m1.1.1.3" xref="S5.T3.6.6.2.m1.1.1.3.cmml">2.23</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.2.m1.1b"><apply id="S5.T3.6.6.2.m1.1.1.cmml" xref="S5.T3.6.6.2.m1.1.1"><csymbol cd="latexml" id="S5.T3.6.6.2.m1.1.1.1.cmml" xref="S5.T3.6.6.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.T3.6.6.2.m1.1.1.2.cmml" xref="S5.T3.6.6.2.m1.1.1.2">51.01</cn><cn type="float" id="S5.T3.6.6.2.m1.1.1.3.cmml" xref="S5.T3.6.6.2.m1.1.1.3">2.23</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.2.m1.1c">51.01\pm 2.23</annotation></semantics></math></td>
</tr>
<tr id="S5.T3.6.8.1" class="ltx_tr">
<td id="S5.T3.6.8.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S5.T3.6.8.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T3.6.8.1.1.1.1" class="ltx_p" style="width:99.7pt;">EmotionNAS [CNN] <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite></span>
</span>
</td>
<td id="S5.T3.6.8.1.2" class="ltx_td ltx_align_right ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">130K</td>
<td id="S5.T3.6.8.1.3" class="ltx_td ltx_align_right ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">3</td>
<td id="S5.T3.6.8.1.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">57.3</td>
<td id="S5.T3.6.8.1.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">63.2</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span><span id="S5.SS2.1.1" class="ltx_text ltx_font_italic">emoDARTS model</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We analyse the performance of the CNN-SeqNN model generated by DARTS (emoDARTS) in contrast to the SER models optimised without DARTS (w/o DARTS) and visualise this in Figure <a href="#S5.F5" title="Figure 5 ‣ 5.2 emoDARTS model ‣ 5 Evaluation ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and Table <a href="#S5.T4" title="TABLE IV ‣ 5.2 emoDARTS model ‣ 5 Evaluation ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.
The graph shows that the NAS-generated SER model performs better than the
baseline SER model developed without DARTS for the three datasets.
</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2403.14083/assets/x5.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="368" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparison of UA% between the datasets the NAS generated (emoDARTS) and CNN+LSTM attention models developed without DARTS (w/o DARTS)</figcaption>
</figure>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Unweighted Accuracy (UA%) of the CNN+LSTM attention model developed without DARTS (w/o DARTS), Unweighted Accuracy (UA%), and Weighted Accuracy (WA%) of the emoDARTS model for each Dataset</figcaption>
<table id="S5.T4.9" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.9.10.1" class="ltx_tr">
<th id="S5.T4.9.10.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;" rowspan="2"><span id="S5.T4.9.10.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S5.T4.9.10.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T4.9.10.1.2.1" class="ltx_text ltx_font_bold">w/o DARTS</span></th>
<th id="S5.T4.9.10.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="2"><span id="S5.T4.9.10.1.3.1" class="ltx_text ltx_font_bold">emoDARTS</span></th>
</tr>
<tr id="S5.T4.9.11.2" class="ltx_tr">
<th id="S5.T4.9.11.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">UA%</th>
<th id="S5.T4.9.11.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">UA%</th>
<th id="S5.T4.9.11.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:1.5pt;padding-bottom:1.5pt;">WA%</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.3.3" class="ltx_tr">
<td id="S5.T4.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">IEMOCAP</td>
<td id="S5.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">53.55 <math id="S5.T4.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T4.1.1.1.m1.1a"><mo id="S5.T4.1.1.1.m1.1.1" xref="S5.T4.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T4.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.m1.1c">\pm</annotation></semantics></math> 2.53</td>
<td id="S5.T4.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">76.56 <math id="S5.T4.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T4.2.2.2.m1.1a"><mo id="S5.T4.2.2.2.m1.1.1" xref="S5.T4.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.m1.1b"><csymbol cd="latexml" id="S5.T4.2.2.2.m1.1.1.cmml" xref="S5.T4.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.m1.1c">\pm</annotation></semantics></math> 4.03</td>
<td id="S5.T4.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">78.03 <math id="S5.T4.3.3.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T4.3.3.3.m1.1a"><mo id="S5.T4.3.3.3.m1.1.1" xref="S5.T4.3.3.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.m1.1b"><csymbol cd="latexml" id="S5.T4.3.3.3.m1.1.1.cmml" xref="S5.T4.3.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.m1.1c">\pm</annotation></semantics></math> 3.51</td>
</tr>
<tr id="S5.T4.6.6" class="ltx_tr">
<td id="S5.T4.6.6.4" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">MSP-IMPROV</td>
<td id="S5.T4.4.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">42.31 <math id="S5.T4.4.4.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T4.4.4.1.m1.1a"><mo id="S5.T4.4.4.1.m1.1.1" xref="S5.T4.4.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T4.4.4.1.m1.1b"><csymbol cd="latexml" id="S5.T4.4.4.1.m1.1.1.cmml" xref="S5.T4.4.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.4.1.m1.1c">\pm</annotation></semantics></math> 1.34</td>
<td id="S5.T4.5.5.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">65.63 <math id="S5.T4.5.5.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T4.5.5.2.m1.1a"><mo id="S5.T4.5.5.2.m1.1.1" xref="S5.T4.5.5.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T4.5.5.2.m1.1b"><csymbol cd="latexml" id="S5.T4.5.5.2.m1.1.1.cmml" xref="S5.T4.5.5.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.5.5.2.m1.1c">\pm</annotation></semantics></math> 8.85</td>
<td id="S5.T4.6.6.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">65.32 <math id="S5.T4.6.6.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T4.6.6.3.m1.1a"><mo id="S5.T4.6.6.3.m1.1.1" xref="S5.T4.6.6.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T4.6.6.3.m1.1b"><csymbol cd="latexml" id="S5.T4.6.6.3.m1.1.1.cmml" xref="S5.T4.6.6.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.6.6.3.m1.1c">\pm</annotation></semantics></math> 8.73</td>
</tr>
<tr id="S5.T4.9.9" class="ltx_tr">
<td id="S5.T4.9.9.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">MSP-Podcast</td>
<td id="S5.T4.7.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">54.48 <math id="S5.T4.7.7.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T4.7.7.1.m1.1a"><mo id="S5.T4.7.7.1.m1.1.1" xref="S5.T4.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T4.7.7.1.m1.1b"><csymbol cd="latexml" id="S5.T4.7.7.1.m1.1.1.cmml" xref="S5.T4.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.7.7.1.m1.1c">\pm</annotation></semantics></math> 3.25</td>
<td id="S5.T4.8.8.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">61.15 <math id="S5.T4.8.8.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T4.8.8.2.m1.1a"><mo id="S5.T4.8.8.2.m1.1.1" xref="S5.T4.8.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T4.8.8.2.m1.1b"><csymbol cd="latexml" id="S5.T4.8.8.2.m1.1.1.cmml" xref="S5.T4.8.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.8.8.2.m1.1c">\pm</annotation></semantics></math> 2.41</td>
<td id="S5.T4.9.9.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">62.33 <math id="S5.T4.9.9.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T4.9.9.3.m1.1a"><mo id="S5.T4.9.9.3.m1.1.1" xref="S5.T4.9.9.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T4.9.9.3.m1.1b"><csymbol cd="latexml" id="S5.T4.9.9.3.m1.1.1.cmml" xref="S5.T4.9.9.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.9.9.3.m1.1c">\pm</annotation></semantics></math> 1.80</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">We also compare the performance of the SER model generated by our approach with the most related studies, ‘EmotionNAS’ of Sun H. et al., and the ‘CNN_RNN_att’ system of Wu X. et al. in Table <a href="#S5.T5" title="TABLE V ‣ 5.2 emoDARTS model ‣ 5 Evaluation ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Accuracy of SER models published by related studies compared with our study for the improvised subset in the IEMOCAP dataset.</figcaption>
<table id="S5.T5.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.2.3.1" class="ltx_tr">
<th id="S5.T5.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T5.2.3.1.1.1" class="ltx_text ltx_font_bold">Study</span></th>
<th id="S5.T5.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T5.2.3.1.2.1" class="ltx_text ltx_font_bold">UA%</span></th>
<th id="S5.T5.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T5.2.3.1.3.1" class="ltx_text ltx_font_bold">WA%</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.2.4.1" class="ltx_tr">
<th id="S5.T5.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Sun H. et al. 2022 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</th>
<td id="S5.T5.2.4.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">69.1</td>
<td id="S5.T5.2.4.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">72.1</td>
</tr>
<tr id="S5.T5.2.5.2" class="ltx_tr">
<th id="S5.T5.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">Wu X. et al. 2022 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</th>
<td id="S5.T5.2.5.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">56.28</td>
<td id="S5.T5.2.5.2.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">68.87</td>
</tr>
<tr id="S5.T5.2.2" class="ltx_tr">
<th id="S5.T5.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">emoDARTS (Our Study)</th>
<td id="S5.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S5.T5.1.1.1.1" class="ltx_text ltx_font_bold">76.56 <math id="S5.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T5.1.1.1.1.m1.1a"><mo id="S5.T5.1.1.1.1.m1.1.1" xref="S5.T5.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T5.1.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.1.m1.1c">\pm</annotation></semantics></math> 4.03</span></td>
<td id="S5.T5.2.2.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:1.5pt;padding-bottom:1.5pt;">78.03 <math id="S5.T5.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T5.2.2.2.m1.1a"><mo id="S5.T5.2.2.2.m1.1.1" xref="S5.T5.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.2.m1.1b"><csymbol cd="latexml" id="S5.T5.2.2.2.m1.1.1.cmml" xref="S5.T5.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.2.m1.1c">\pm</annotation></semantics></math> 3.51</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">It is visible that the SER model generated by our methodology for the IEMOCAP dataset outperforms the SER models generated by DARTS in the related literature.
</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">It is further worthwhile to investigate the rationale for increased performance when compared to the results of Wu et al.’s ‘CNN RNN att’ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> system. We suggest the improved performance is due to the relaxed candidate operations order rather than the pre-defined layer order. In Wu et al.’s study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, for example, the initial layers are pre-defined to be convolutional layers. The DARTS algorithm must select the best convolutional layer from a pool of just CNN layers. In contrast, our technique allows DARTS to choose among many operations such as convolutions, pooling, and skip connections. Figure <a href="#S5.F6" title="Figure 6 ‣ 5.2 emoDARTS model ‣ 5 Evaluation ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows one such use scenario, in which the DARTS searched architecture consists of pooling layers in the initial segments.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">Figure <a href="#S5.F6" title="Figure 6 ‣ 5.2 emoDARTS model ‣ 5 Evaluation ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows a visualisation of the architecture for each type of cell (normal and reduction cell of the CNN component, and cell in the SeqNN component) searched by DARTS for the emoDARTS model. It is visible that DARTS has selected three LSTM based operations for the SeqNN component and only one of them contains attention. This shows that jointly optimising the emoDARTS model has enabled the DARTS framework to choose optimum operations rather than blindly choosing layers with ‘attention’ for all the operations.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2403.14083/assets/x6.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="185" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>DARTS searched <math id="S5.F6.2.m1.1" class="ltx_Math" alttext="t^{th}" display="inline"><semantics id="S5.F6.2.m1.1b"><msup id="S5.F6.2.m1.1.1" xref="S5.F6.2.m1.1.1.cmml"><mi id="S5.F6.2.m1.1.1.2" xref="S5.F6.2.m1.1.1.2.cmml">t</mi><mrow id="S5.F6.2.m1.1.1.3" xref="S5.F6.2.m1.1.1.3.cmml"><mi id="S5.F6.2.m1.1.1.3.2" xref="S5.F6.2.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.F6.2.m1.1.1.3.1" xref="S5.F6.2.m1.1.1.3.1.cmml">​</mo><mi id="S5.F6.2.m1.1.1.3.3" xref="S5.F6.2.m1.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.F6.2.m1.1c"><apply id="S5.F6.2.m1.1.1.cmml" xref="S5.F6.2.m1.1.1"><csymbol cd="ambiguous" id="S5.F6.2.m1.1.1.1.cmml" xref="S5.F6.2.m1.1.1">superscript</csymbol><ci id="S5.F6.2.m1.1.1.2.cmml" xref="S5.F6.2.m1.1.1.2">𝑡</ci><apply id="S5.F6.2.m1.1.1.3.cmml" xref="S5.F6.2.m1.1.1.3"><times id="S5.F6.2.m1.1.1.3.1.cmml" xref="S5.F6.2.m1.1.1.3.1"></times><ci id="S5.F6.2.m1.1.1.3.2.cmml" xref="S5.F6.2.m1.1.1.3.2">𝑡</ci><ci id="S5.F6.2.m1.1.1.3.3.cmml" xref="S5.F6.2.m1.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F6.2.m1.1d">t^{th}</annotation></semantics></math> cell structure for the CNN Normal Cell (a), CNN Reduction Cell (b), and SeqNN cell (c) for the emoDARTS model.</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span><span id="S5.SS3.1.1" class="ltx_text ltx_font_italic">Restricting the search scope</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We study the impact on the performance of the searched model by restricting the search scope for the SeqNN component. We divide the search scope into five segments namely ‘LSTM Only’, ‘LSTM-Att. Only’, ‘RNN Only’, and ‘RNN-Att. Only’. Table <a href="#S5.T6" title="TABLE VI ‣ 5.3 Restricting the search scope ‣ 5 Evaluation ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> shows the DARTS operations allowed as the candidate operations in the SeqNN component during the search phase.</p>
</div>
<figure id="S5.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>DARTS operations allowed as the candidate operations in the SeqNN component during the search phase</figcaption>
<table id="S5.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T6.1.1.1" class="ltx_tr">
<th id="S5.T6.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S5.T6.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.1.1.1.1.1" class="ltx_p" style="width:101.9pt;"><span id="S5.T6.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Scope</span></span>
</span>
</th>
<th id="S5.T6.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S5.T6.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.1.1.2.1.1" class="ltx_p" style="width:260.2pt;"><span id="S5.T6.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Candidate Operations</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T6.1.2.1" class="ltx_tr">
<td id="S5.T6.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S5.T6.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.2.1.1.1.1" class="ltx_p" style="width:101.9pt;">emoDARTS</span>
</span>
</td>
<td id="S5.T6.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S5.T6.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.2.1.2.1.1" class="ltx_p" style="width:260.2pt;">lstm_1, lstm_2, lstm_3, lstm_4, lstm_att_1, lstm_att_2, rnn_1, rnn_2, rnn_3, rnn_4, rnn_att_1, rnn_att_2</span>
</span>
</td>
</tr>
<tr id="S5.T6.1.3.2" class="ltx_tr">
<td id="S5.T6.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S5.T6.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.3.2.1.1.1" class="ltx_p" style="width:101.9pt;">LSTM Only</span>
</span>
</td>
<td id="S5.T6.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S5.T6.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.3.2.2.1.1" class="ltx_p" style="width:260.2pt;">lstm_1, lstm_2, lstm_3, lstm_4</span>
</span>
</td>
</tr>
<tr id="S5.T6.1.4.3" class="ltx_tr">
<td id="S5.T6.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S5.T6.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.4.3.1.1.1" class="ltx_p" style="width:101.9pt;">LSTM-Att. Only</span>
</span>
</td>
<td id="S5.T6.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S5.T6.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.4.3.2.1.1" class="ltx_p" style="width:260.2pt;">lstm_att_1, lstm_att_2</span>
</span>
</td>
</tr>
<tr id="S5.T6.1.5.4" class="ltx_tr">
<td id="S5.T6.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S5.T6.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.5.4.1.1.1" class="ltx_p" style="width:101.9pt;">RNN Only</span>
</span>
</td>
<td id="S5.T6.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S5.T6.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.5.4.2.1.1" class="ltx_p" style="width:260.2pt;">rnn_1, rnn_2, rnn_3, rnn_4</span>
</span>
</td>
</tr>
<tr id="S5.T6.1.6.5" class="ltx_tr">
<td id="S5.T6.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S5.T6.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.6.5.1.1.1" class="ltx_p" style="width:101.9pt;">RNN-Att. Only</span>
</span>
</td>
<td id="S5.T6.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S5.T6.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T6.1.6.5.2.1.1" class="ltx_p" style="width:260.2pt;">rnn_att_1, rnn_att_2</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Table <a href="#S5.T7" title="TABLE VII ‣ 5.3 Restricting the search scope ‣ 5 Evaluation ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> shows the performance and number of parameters of the searched model when the candidate search operations are restricted. Here, we study the effect on the performance of the
searched architecture when the search algorithm was only given a restricted set of operations. For example, the ‘LSTM Only’ study only allowed to use operations from lstm_1, lstm_2, lstm_3, and lstm_4. We try to identify the most important types of genome operations that we can use in the search algorithm. This approach allows to use of only the important operations in the search scope and optimises the memory utilisation in the search phase.</p>
</div>
<figure id="S5.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VII: </span>Performance (UA%, WA%) and number of parameters (Param.) of each generated model when the candidate search operations are restricted</figcaption>
<table id="S5.T7.30" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T7.30.31.1" class="ltx_tr">
<th id="S5.T7.30.31.1.1" class="ltx_td ltx_th ltx_th_column" style="padding:1.5pt 4.5pt;"></th>
<th id="S5.T7.30.31.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding:1.5pt 4.5pt;" colspan="3"><span id="S5.T7.30.31.1.2.1" class="ltx_text ltx_font_bold">IEMOCAP</span></th>
<th id="S5.T7.30.31.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding:1.5pt 4.5pt;" colspan="3"><span id="S5.T7.30.31.1.3.1" class="ltx_text ltx_font_bold">MSP-Improv</span></th>
<th id="S5.T7.30.31.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.5pt 4.5pt;" colspan="3"><span id="S5.T7.30.31.1.4.1" class="ltx_text ltx_font_bold">MSP-Podcast</span></th>
</tr>
<tr id="S5.T7.30.32.2" class="ltx_tr">
<th id="S5.T7.30.32.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column" style="padding:1.5pt 4.5pt;"><span id="S5.T7.30.32.2.1.1" class="ltx_text ltx_font_bold">Genome Ops.</span></th>
<th id="S5.T7.30.32.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.5pt 4.5pt;"><span id="S5.T7.30.32.2.2.1" class="ltx_text ltx_font_bold">UA%</span></th>
<th id="S5.T7.30.32.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.5pt 4.5pt;"><span id="S5.T7.30.32.2.3.1" class="ltx_text ltx_font_bold">WA%</span></th>
<th id="S5.T7.30.32.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding:1.5pt 4.5pt;"><span id="S5.T7.30.32.2.4.1" class="ltx_text ltx_font_bold">Param.</span></th>
<th id="S5.T7.30.32.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.5pt 4.5pt;"><span id="S5.T7.30.32.2.5.1" class="ltx_text ltx_font_bold">UA%</span></th>
<th id="S5.T7.30.32.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.5pt 4.5pt;"><span id="S5.T7.30.32.2.6.1" class="ltx_text ltx_font_bold">WA%</span></th>
<th id="S5.T7.30.32.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding:1.5pt 4.5pt;"><span id="S5.T7.30.32.2.7.1" class="ltx_text ltx_font_bold">Param.</span></th>
<th id="S5.T7.30.32.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.5pt 4.5pt;"><span id="S5.T7.30.32.2.8.1" class="ltx_text ltx_font_bold">UA%</span></th>
<th id="S5.T7.30.32.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.5pt 4.5pt;"><span id="S5.T7.30.32.2.9.1" class="ltx_text ltx_font_bold">WA%</span></th>
<th id="S5.T7.30.32.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding:1.5pt 4.5pt;"><span id="S5.T7.30.32.2.10.1" class="ltx_text ltx_font_bold">Param.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T7.6.6" class="ltx_tr">
<td id="S5.T7.6.6.7" class="ltx_td ltx_align_left ltx_border_t" style="padding:1.5pt 4.5pt;">emoDARTS</td>
<td id="S5.T7.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 4.5pt;"><span id="S5.T7.1.1.1.1" class="ltx_text ltx_font_bold">76.55 <math id="S5.T7.1.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.1.1.1.1.m1.1a"><mo id="S5.T7.1.1.1.1.m1.1.1" xref="S5.T7.1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T7.1.1.1.1.m1.1.1.cmml" xref="S5.T7.1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.1.1.1.1.m1.1c">\pm</annotation></semantics></math> 4.03</span></td>
<td id="S5.T7.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 4.5pt;">78.03 <math id="S5.T7.2.2.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.2.2.2.m1.1a"><mo id="S5.T7.2.2.2.m1.1.1" xref="S5.T7.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.2.2.2.m1.1b"><csymbol cd="latexml" id="S5.T7.2.2.2.m1.1.1.cmml" xref="S5.T7.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.2.2.2.m1.1c">\pm</annotation></semantics></math> 3.51</td>
<td id="S5.T7.6.6.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding:1.5pt 4.5pt;">1 014 556</td>
<td id="S5.T7.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 4.5pt;"><span id="S5.T7.3.3.3.1" class="ltx_text ltx_font_bold">65.63 <math id="S5.T7.3.3.3.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.3.3.3.1.m1.1a"><mo id="S5.T7.3.3.3.1.m1.1.1" xref="S5.T7.3.3.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.3.3.3.1.m1.1b"><csymbol cd="latexml" id="S5.T7.3.3.3.1.m1.1.1.cmml" xref="S5.T7.3.3.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.3.3.3.1.m1.1c">\pm</annotation></semantics></math> 8.85</span></td>
<td id="S5.T7.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 4.5pt;">65.32 <math id="S5.T7.4.4.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.4.4.4.m1.1a"><mo id="S5.T7.4.4.4.m1.1.1" xref="S5.T7.4.4.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.4.4.4.m1.1b"><csymbol cd="latexml" id="S5.T7.4.4.4.m1.1.1.cmml" xref="S5.T7.4.4.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.4.4.4.m1.1c">\pm</annotation></semantics></math> 8.73</td>
<td id="S5.T7.6.6.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding:1.5pt 4.5pt;">1 008 812</td>
<td id="S5.T7.5.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 4.5pt;"><span id="S5.T7.5.5.5.1" class="ltx_text ltx_font_bold">61.15 <math id="S5.T7.5.5.5.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.5.5.5.1.m1.1a"><mo id="S5.T7.5.5.5.1.m1.1.1" xref="S5.T7.5.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.5.5.5.1.m1.1b"><csymbol cd="latexml" id="S5.T7.5.5.5.1.m1.1.1.cmml" xref="S5.T7.5.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.5.5.5.1.m1.1c">\pm</annotation></semantics></math> 2.41</span></td>
<td id="S5.T7.6.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 4.5pt;">62.33 <math id="S5.T7.6.6.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.6.6.6.m1.1a"><mo id="S5.T7.6.6.6.m1.1.1" xref="S5.T7.6.6.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.6.6.6.m1.1b"><csymbol cd="latexml" id="S5.T7.6.6.6.m1.1.1.cmml" xref="S5.T7.6.6.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.6.6.6.m1.1c">\pm</annotation></semantics></math> 1.79</td>
<td id="S5.T7.6.6.10" class="ltx_td ltx_align_right ltx_border_t" style="padding:1.5pt 4.5pt;">1 835 604</td>
</tr>
<tr id="S5.T7.12.12" class="ltx_tr">
<td id="S5.T7.12.12.7" class="ltx_td ltx_align_left" style="padding:1.5pt 4.5pt;">LSTM Only</td>
<td id="S5.T7.7.7.1" class="ltx_td ltx_align_center" style="padding:1.5pt 4.5pt;">67.42 <math id="S5.T7.7.7.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.7.7.1.m1.1a"><mo id="S5.T7.7.7.1.m1.1.1" xref="S5.T7.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.7.7.1.m1.1b"><csymbol cd="latexml" id="S5.T7.7.7.1.m1.1.1.cmml" xref="S5.T7.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.7.7.1.m1.1c">\pm</annotation></semantics></math> 5.76</td>
<td id="S5.T7.8.8.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.5pt;">67.78 <math id="S5.T7.8.8.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.8.8.2.m1.1a"><mo id="S5.T7.8.8.2.m1.1.1" xref="S5.T7.8.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.8.8.2.m1.1b"><csymbol cd="latexml" id="S5.T7.8.8.2.m1.1.1.cmml" xref="S5.T7.8.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.8.8.2.m1.1c">\pm</annotation></semantics></math> 5.97</td>
<td id="S5.T7.12.12.8" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.5pt 4.5pt;">2 939 372</td>
<td id="S5.T7.9.9.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.5pt;">32.35 <math id="S5.T7.9.9.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.9.9.3.m1.1a"><mo id="S5.T7.9.9.3.m1.1.1" xref="S5.T7.9.9.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.9.9.3.m1.1b"><csymbol cd="latexml" id="S5.T7.9.9.3.m1.1.1.cmml" xref="S5.T7.9.9.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.9.9.3.m1.1c">\pm</annotation></semantics></math> 1.93</td>
<td id="S5.T7.10.10.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.5pt;">31.85 <math id="S5.T7.10.10.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.10.10.4.m1.1a"><mo id="S5.T7.10.10.4.m1.1.1" xref="S5.T7.10.10.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.10.10.4.m1.1b"><csymbol cd="latexml" id="S5.T7.10.10.4.m1.1.1.cmml" xref="S5.T7.10.10.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.10.10.4.m1.1c">\pm</annotation></semantics></math> 0.98</td>
<td id="S5.T7.12.12.9" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.5pt 4.5pt;">2 931 748</td>
<td id="S5.T7.11.11.5" class="ltx_td ltx_align_center" style="padding:1.5pt 4.5pt;">53.12 <math id="S5.T7.11.11.5.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.11.11.5.m1.1a"><mo id="S5.T7.11.11.5.m1.1.1" xref="S5.T7.11.11.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.11.11.5.m1.1b"><csymbol cd="latexml" id="S5.T7.11.11.5.m1.1.1.cmml" xref="S5.T7.11.11.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.11.11.5.m1.1c">\pm</annotation></semantics></math> 5.82</td>
<td id="S5.T7.12.12.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.5pt;">54.48 <math id="S5.T7.12.12.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.12.12.6.m1.1a"><mo id="S5.T7.12.12.6.m1.1.1" xref="S5.T7.12.12.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.12.12.6.m1.1b"><csymbol cd="latexml" id="S5.T7.12.12.6.m1.1.1.cmml" xref="S5.T7.12.12.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.12.12.6.m1.1c">\pm</annotation></semantics></math> 5.43</td>
<td id="S5.T7.12.12.10" class="ltx_td ltx_align_right" style="padding:1.5pt 4.5pt;">2 943 852</td>
</tr>
<tr id="S5.T7.18.18" class="ltx_tr">
<td id="S5.T7.18.18.7" class="ltx_td ltx_align_left" style="padding:1.5pt 4.5pt;">LSTM-Att. Only</td>
<td id="S5.T7.13.13.1" class="ltx_td ltx_align_center" style="padding:1.5pt 4.5pt;">68.12 <math id="S5.T7.13.13.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.13.13.1.m1.1a"><mo id="S5.T7.13.13.1.m1.1.1" xref="S5.T7.13.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.13.13.1.m1.1b"><csymbol cd="latexml" id="S5.T7.13.13.1.m1.1.1.cmml" xref="S5.T7.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.13.13.1.m1.1c">\pm</annotation></semantics></math> 7.12</td>
<td id="S5.T7.14.14.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.5pt;">68.59 <math id="S5.T7.14.14.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.14.14.2.m1.1a"><mo id="S5.T7.14.14.2.m1.1.1" xref="S5.T7.14.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.14.14.2.m1.1b"><csymbol cd="latexml" id="S5.T7.14.14.2.m1.1.1.cmml" xref="S5.T7.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.14.14.2.m1.1c">\pm</annotation></semantics></math> 6.62</td>
<td id="S5.T7.18.18.8" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.5pt 4.5pt;">7 284 412</td>
<td id="S5.T7.15.15.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.5pt;">49.74 <math id="S5.T7.15.15.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.15.15.3.m1.1a"><mo id="S5.T7.15.15.3.m1.1.1" xref="S5.T7.15.15.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.15.15.3.m1.1b"><csymbol cd="latexml" id="S5.T7.15.15.3.m1.1.1.cmml" xref="S5.T7.15.15.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.15.15.3.m1.1c">\pm</annotation></semantics></math> 10.08</td>
<td id="S5.T7.16.16.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.5pt;">49.8 <math id="S5.T7.16.16.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.16.16.4.m1.1a"><mo id="S5.T7.16.16.4.m1.1.1" xref="S5.T7.16.16.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.16.16.4.m1.1b"><csymbol cd="latexml" id="S5.T7.16.16.4.m1.1.1.cmml" xref="S5.T7.16.16.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.16.16.4.m1.1c">\pm</annotation></semantics></math> 10.15</td>
<td id="S5.T7.18.18.9" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.5pt 4.5pt;">6 474 676</td>
<td id="S5.T7.17.17.5" class="ltx_td ltx_align_center" style="padding:1.5pt 4.5pt;">53.11 <math id="S5.T7.17.17.5.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.17.17.5.m1.1a"><mo id="S5.T7.17.17.5.m1.1.1" xref="S5.T7.17.17.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.17.17.5.m1.1b"><csymbol cd="latexml" id="S5.T7.17.17.5.m1.1.1.cmml" xref="S5.T7.17.17.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.17.17.5.m1.1c">\pm</annotation></semantics></math> 5.83</td>
<td id="S5.T7.18.18.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.5pt;">55.07 <math id="S5.T7.18.18.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.18.18.6.m1.1a"><mo id="S5.T7.18.18.6.m1.1.1" xref="S5.T7.18.18.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.18.18.6.m1.1b"><csymbol cd="latexml" id="S5.T7.18.18.6.m1.1.1.cmml" xref="S5.T7.18.18.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.18.18.6.m1.1c">\pm</annotation></semantics></math> 5.14</td>
<td id="S5.T7.18.18.10" class="ltx_td ltx_align_right" style="padding:1.5pt 4.5pt;">7 284 932</td>
</tr>
<tr id="S5.T7.24.24" class="ltx_tr">
<td id="S5.T7.24.24.7" class="ltx_td ltx_align_left" style="padding:1.5pt 4.5pt;">RNN Only</td>
<td id="S5.T7.19.19.1" class="ltx_td ltx_align_center" style="padding:1.5pt 4.5pt;">70.42 <math id="S5.T7.19.19.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.19.19.1.m1.1a"><mo id="S5.T7.19.19.1.m1.1.1" xref="S5.T7.19.19.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.19.19.1.m1.1b"><csymbol cd="latexml" id="S5.T7.19.19.1.m1.1.1.cmml" xref="S5.T7.19.19.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.19.19.1.m1.1c">\pm</annotation></semantics></math> 8.32</td>
<td id="S5.T7.20.20.2" class="ltx_td ltx_align_center" style="padding:1.5pt 4.5pt;">72.07 <math id="S5.T7.20.20.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.20.20.2.m1.1a"><mo id="S5.T7.20.20.2.m1.1.1" xref="S5.T7.20.20.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.20.20.2.m1.1b"><csymbol cd="latexml" id="S5.T7.20.20.2.m1.1.1.cmml" xref="S5.T7.20.20.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.20.20.2.m1.1c">\pm</annotation></semantics></math> 8.23</td>
<td id="S5.T7.24.24.8" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.5pt 4.5pt;">356 492</td>
<td id="S5.T7.21.21.3" class="ltx_td ltx_align_center" style="padding:1.5pt 4.5pt;">63.59 <math id="S5.T7.21.21.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.21.21.3.m1.1a"><mo id="S5.T7.21.21.3.m1.1.1" xref="S5.T7.21.21.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.21.21.3.m1.1b"><csymbol cd="latexml" id="S5.T7.21.21.3.m1.1.1.cmml" xref="S5.T7.21.21.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.21.21.3.m1.1c">\pm</annotation></semantics></math> 7.88</td>
<td id="S5.T7.22.22.4" class="ltx_td ltx_align_center" style="padding:1.5pt 4.5pt;">63.49 <math id="S5.T7.22.22.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.22.22.4.m1.1a"><mo id="S5.T7.22.22.4.m1.1.1" xref="S5.T7.22.22.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.22.22.4.m1.1b"><csymbol cd="latexml" id="S5.T7.22.22.4.m1.1.1.cmml" xref="S5.T7.22.22.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.22.22.4.m1.1c">\pm</annotation></semantics></math> 7.95</td>
<td id="S5.T7.24.24.9" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.5pt 4.5pt;">1 016 284</td>
<td id="S5.T7.23.23.5" class="ltx_td ltx_align_center" style="padding:1.5pt 4.5pt;">59.16 <math id="S5.T7.23.23.5.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.23.23.5.m1.1a"><mo id="S5.T7.23.23.5.m1.1.1" xref="S5.T7.23.23.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.23.23.5.m1.1b"><csymbol cd="latexml" id="S5.T7.23.23.5.m1.1.1.cmml" xref="S5.T7.23.23.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.23.23.5.m1.1c">\pm</annotation></semantics></math> 6.67</td>
<td id="S5.T7.24.24.6" class="ltx_td ltx_align_center" style="padding:1.5pt 4.5pt;">60.72 <math id="S5.T7.24.24.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.24.24.6.m1.1a"><mo id="S5.T7.24.24.6.m1.1.1" xref="S5.T7.24.24.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.24.24.6.m1.1b"><csymbol cd="latexml" id="S5.T7.24.24.6.m1.1.1.cmml" xref="S5.T7.24.24.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.24.24.6.m1.1c">\pm</annotation></semantics></math> 7.65</td>
<td id="S5.T7.24.24.10" class="ltx_td ltx_align_right" style="padding:1.5pt 4.5pt;">706 068</td>
</tr>
<tr id="S5.T7.30.30" class="ltx_tr">
<td id="S5.T7.30.30.7" class="ltx_td ltx_align_left ltx_border_b" style="padding:1.5pt 4.5pt;">RNN-Att. Only</td>
<td id="S5.T7.25.25.1" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.5pt 4.5pt;">36.14 <math id="S5.T7.25.25.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.25.25.1.m1.1a"><mo id="S5.T7.25.25.1.m1.1.1" xref="S5.T7.25.25.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.25.25.1.m1.1b"><csymbol cd="latexml" id="S5.T7.25.25.1.m1.1.1.cmml" xref="S5.T7.25.25.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.25.25.1.m1.1c">\pm</annotation></semantics></math> 10.97</td>
<td id="S5.T7.26.26.2" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.5pt 4.5pt;">48.63 <math id="S5.T7.26.26.2.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.26.26.2.m1.1a"><mo id="S5.T7.26.26.2.m1.1.1" xref="S5.T7.26.26.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.26.26.2.m1.1b"><csymbol cd="latexml" id="S5.T7.26.26.2.m1.1.1.cmml" xref="S5.T7.26.26.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.26.26.2.m1.1c">\pm</annotation></semantics></math> 6.56</td>
<td id="S5.T7.30.30.8" class="ltx_td ltx_align_right ltx_border_b ltx_border_r" style="padding:1.5pt 4.5pt;">29 324</td>
<td id="S5.T7.27.27.3" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.5pt 4.5pt;">57.71 <math id="S5.T7.27.27.3.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.27.27.3.m1.1a"><mo id="S5.T7.27.27.3.m1.1.1" xref="S5.T7.27.27.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.27.27.3.m1.1b"><csymbol cd="latexml" id="S5.T7.27.27.3.m1.1.1.cmml" xref="S5.T7.27.27.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.27.27.3.m1.1c">\pm</annotation></semantics></math> 12.61</td>
<td id="S5.T7.28.28.4" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.5pt 4.5pt;">57.04 <math id="S5.T7.28.28.4.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.28.28.4.m1.1a"><mo id="S5.T7.28.28.4.m1.1.1" xref="S5.T7.28.28.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.28.28.4.m1.1b"><csymbol cd="latexml" id="S5.T7.28.28.4.m1.1.1.cmml" xref="S5.T7.28.28.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.28.28.4.m1.1c">\pm</annotation></semantics></math> 12.14</td>
<td id="S5.T7.30.30.9" class="ltx_td ltx_align_right ltx_border_b ltx_border_r" style="padding:1.5pt 4.5pt;">22 932</td>
<td id="S5.T7.29.29.5" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.5pt 4.5pt;">59.29 <math id="S5.T7.29.29.5.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.29.29.5.m1.1a"><mo id="S5.T7.29.29.5.m1.1.1" xref="S5.T7.29.29.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.29.29.5.m1.1b"><csymbol cd="latexml" id="S5.T7.29.29.5.m1.1.1.cmml" xref="S5.T7.29.29.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.29.29.5.m1.1c">\pm</annotation></semantics></math> 6.38</td>
<td id="S5.T7.30.30.6" class="ltx_td ltx_align_center ltx_border_b" style="padding:1.5pt 4.5pt;">60.25 <math id="S5.T7.30.30.6.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.T7.30.30.6.m1.1a"><mo id="S5.T7.30.30.6.m1.1.1" xref="S5.T7.30.30.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T7.30.30.6.m1.1b"><csymbol cd="latexml" id="S5.T7.30.30.6.m1.1.1.cmml" xref="S5.T7.30.30.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.30.30.6.m1.1c">\pm</annotation></semantics></math> 6.09</td>
<td id="S5.T7.30.30.10" class="ltx_td ltx_align_right ltx_border_b" style="padding:1.5pt 4.5pt;">2 844 420</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2403.14083/assets/x7.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Visualisation of results for the studies restricting the search space for the three datasets: IEMOCAP, MSP-IMPROV, and MSP-Podcast. The vertical axis is the mean UA% and the horizontal axis is the standard deviation of UA%. The size of the marker depicts the size (number of parameters) of the generated model. The best performing model can be found at the top-left most position of the figure which has the highest mean UA% and lowest standard deviation of UA%.</figcaption>
</figure>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">Comparing the trials ‘emoDARTS’ and ‘LSTM Only’ in the IEMOCAP dataset, we can observe that even though the number of parameters has tripled in the ‘LSTM Only’ scenario, the performance (UA%) has not increased. This indicates that increasing the number of parameters just by increasing the complexity of the model does not tend to give better performance, but the model components should be compatible with each other.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p">Notably, models using ’RNN Only’ genomic operations achieve the second-highest accuracy despite having much fewer trainable parameters. Figure <a href="#S5.F8" title="Figure 8 ‣ 5.3 Restricting the search scope ‣ 5 Evaluation ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> depicts the cell architecture, which consists mostly of pooling layers and skip connection operations that do not have any training parameters and hence do not contribute to the total number of trainable parameters.</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2403.14083/assets/x8.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="369" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>DARTS searched cell structure for CNN and SeqNN cells when the SeqNN search space has only RNN operations (RNN Only) for the IEMOCAP dataset</figcaption>
</figure>
<div id="S5.SS3.p5" class="ltx_para">
<p id="S5.SS3.p5.1" class="ltx_p">We provide in Table <a href="#S5.T7" title="TABLE VII ‣ 5.3 Restricting the search scope ‣ 5 Evaluation ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> results as well as a scatter plot for better visualisation in Figure <a href="#S5.F7" title="Figure 7 ‣ 5.3 Restricting the search scope ‣ 5 Evaluation ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, where the mean UA% in the vertical axis, standard deviation of UA% in the horizontal axis and size of the markers indicates the number of parameters. The polts indicate which model gives better performance in terms of the mean accuracy and its standard deviation. The best performing model can be found in the top left corner of the plot where the highest UA% and lowest Standard deviation of UA% are present. According to the figure, the best performing model for all three datasets is given by ‘emoDARTS’ where all the candidate operations are available in the search scope.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Discussion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Throughout this study, we encountered various challenges. In this section, we report the key challenges and our strategies for overcoming them. The three primary challenges we faced were:</p>
<ol id="S6.I1" class="ltx_enumerate">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p">Optimising the GPU Memory utilisation</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p">Converging to a local minima</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p">High Standard Deviation of the results</p>
</div>
</li>
</ol>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span><span id="S6.SS1.1.1" class="ltx_text ltx_font_italic">Optimising the GPU Memory utilisation</span>
</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">The DARTS algorithm conceptualises the search problem as a network graph, establishing multiple edges between each node. The quantity of edges corresponds to the defined candidate operations. These operations encompass various possibilities, ranging from simple CNN, pooling, and RNN layers to intricate modules like an LSTM-attention module. In the search phase, a super-neural network is constructed, resulting in multiple instances of neural network layers or modules within this overarching structure.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">Figure <a href="#S6.F9" title="Figure 9 ‣ 6.1 Optimising the GPU Memory utilisation ‣ 6 Discussion ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows an example of a graph inside a DARTS cell that has four nodes and the candidate operations are “lstm_1”, “lstm_2”, and “lstm_att_1”, where “lstm_1” is a single layer LSTM component, “lstm_2” is a double layer LSTM component, and “lstm_att_1” is an attention induced single layer LSTM component.</p>
</div>
<figure id="S6.F9" class="ltx_figure"><img src="/html/2403.14083/assets/x9.png" id="S6.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="163" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Example graph of a DARTS cell which has four nodes and candidate operations are “lstm_1”, “lstm_2”, and “lstm_att_1”. The same edge colour denotes the same type of operation.</figcaption>
</figure>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.5" class="ltx_p">According to the example, a single DARTS cell should initiate 6<math id="S6.SS1.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p3.1.m1.1a"><mo id="S6.SS1.p3.1.m1.1.1" xref="S6.SS1.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.1.m1.1b"><times id="S6.SS1.p3.1.m1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.1.m1.1c">\times</annotation></semantics></math>lstm_1 layers, 6<math id="S6.SS1.p3.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p3.2.m2.1a"><mo id="S6.SS1.p3.2.m2.1.1" xref="S6.SS1.p3.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.2.m2.1b"><times id="S6.SS1.p3.2.m2.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.2.m2.1c">\times</annotation></semantics></math>lstm_2 layers, and 6<math id="S6.SS1.p3.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p3.3.m3.1a"><mo id="S6.SS1.p3.3.m3.1.1" xref="S6.SS1.p3.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.3.m3.1b"><times id="S6.SS1.p3.3.m3.1.1.cmml" xref="S6.SS1.p3.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.3.m3.1c">\times</annotation></semantics></math>lstm_att_1 layers. If the search configuration has <math id="S6.SS1.p3.4.m4.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S6.SS1.p3.4.m4.1a"><mn id="S6.SS1.p3.4.m4.1.1" xref="S6.SS1.p3.4.m4.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.4.m4.1b"><cn type="integer" id="S6.SS1.p3.4.m4.1.1.cmml" xref="S6.SS1.p3.4.m4.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.4.m4.1c">4</annotation></semantics></math> cells, we have to initialise <math id="S6.SS1.p3.5.m5.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S6.SS1.p3.5.m5.1a"><mn id="S6.SS1.p3.5.m5.1.1" xref="S6.SS1.p3.5.m5.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.5.m5.1b"><cn type="integer" id="S6.SS1.p3.5.m5.1.1.cmml" xref="S6.SS1.p3.5.m5.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.5.m5.1c">4</annotation></semantics></math> instances of cells where all the weight and bias parameters have to be initialised in the computing device. This will increase the GPU memory utilisation.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.1" class="ltx_p">Providing a higher number of nodes in a cell, a higher number of cells, and expanding the array of candidate operations will increase the amount of GPU memory utilisation and eventually will exhaust the GPU memory capacity failing the search operation.</p>
</div>
<div id="S6.SS1.p5" class="ltx_para">
<p id="S6.SS1.p5.1" class="ltx_p">To optimise the GPU memory utilisation, we recommend conducting an assessment to determine the set of possible search operations and hyperparameters such as the number of layers, cells, and nodes inside the cell considering the GPU resources available.</p>
</div>
<div id="S6.SS1.p6" class="ltx_para">
<p id="S6.SS1.p6.1" class="ltx_p">On the other hand, based on the results of Table <a href="#S5.T7" title="TABLE VII ‣ 5.3 Restricting the search scope ‣ 5 Evaluation ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a>, we should not be restricted only to a single type of network but rather should consist of a variety of network architectures. We selected the set of candidate operations indicated in Table <a href="#S5.T6" title="TABLE VI ‣ 5.3 Restricting the search scope ‣ 5 Evaluation ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>
under ’emoDARTS’ based on GPU resource availability and on the premise that all sorts of candidate operations should be available in the search space.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span><span id="S6.SS2.1.1" class="ltx_text ltx_font_italic">Converging to a local minima</span>
</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">Throughout the course of our experiments, we attempted various configurations for the number of cells and nodes. We observed that the SeqNN module converges to a local minimum when the number of cells and number of nodes is greater than <math id="S6.SS2.p1.1.m1.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S6.SS2.p1.1.m1.1a"><mn id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><cn type="integer" id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">3</annotation></semantics></math>. The output of the searched genome for the SeqNN module contained all “skip_connect” which indicates identity operations are used instead of any RNN or LSTM operations. Figure <a href="#S6.F10" title="Figure 10 ‣ 6.3 High Standard Deviation in the results ‣ 6 Discussion ‣ emoDARTS: Joint Optimisation of CNN &amp; Sequential Neural Network Architectures for Superior Speech Emotion Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> shows one such instance DARTS SeqNN genome.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">We were able to address the challenge by reducing the complexity of the candidate search graph by reducing the number of cells and the number of nodes inside a cell. More research is, however, needed to manage a more complex search network.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span><span id="S6.SS3.1.1" class="ltx_text ltx_font_italic">High Standard Deviation in the results</span>
</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">An important observation derived from our results is the high standard deviation. This can be attributed to the dataset-splitting method we employed. Specifically, we adopt speaker-independent dataset splitting, where the training and validation sets are segregated based on the speaker. In this configuration, any audio utterance from a particular speaker in the validation set remains unseen by the model during training. Consequently, the DARTS-optimised model is not trained to handle the data distribution of the validation set. To tackle this challenge, potential solutions include dataset poisoning and enhancing the generalisation capabilities of the SER model by incorporating dropout layers.</p>
</div>
<figure id="S6.F10" class="ltx_figure"><img src="/html/2403.14083/assets/x10.png" id="S6.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="339" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>DARTS genome of the SeqNN module where the search algorithm selected the identity operation as all the operations.</figcaption>
</figure>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusions</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In conclusion, this paper introduced an innovative approach to enhancing speech emotion recognition (SER) using differentiable architecture search by DARTS. Our primary focus was on tailoring DARTS for a joint configuration of a Convolutional Neural Network (CNN) and a Sequential Neural Network, deviating from previous studies by allowing DARTS to autonomously determine the optimal layer order for the CNN within the DARTS cell without imposing constraints.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">A comprehensive evaluation was conducted, comparing our proposed method with
baseline models developed without DARTS
and various genome operations, including LSTM only, LSTM with attention only, RNN only, and RNN with attention only. The detailed assessments consistently demonstrate the superior performance of our proposed method. Contrasting with existing studies further validates the effectiveness and superiority of our approach, considering parameter size and accuracy as essential dimensions for comparison.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">Notably, our study extends beyond the confines of the commonly used IEMOCAP dataset, incorporating two additional datasets, MSP-IMPROV and MSP-Podcast. This extension showcases the superior performance of our proposed method across diverse datasets, affirming its generalisation capability.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.1" class="ltx_p">Furthermore, we shared valuable insights gained from our experiences, addressing challenges related to GPU exhaustion and converging to local minima. These insights serve as practical guidance for researchers, helping them navigate potential pitfalls and optimise the application of DARTS in SER.</p>
</div>
<div id="S7.p5" class="ltx_para">
<p id="S7.p5.1" class="ltx_p">Future efforts will need to deal with neural architecture for further modern architectures such as transformers and translating the made findings beyond the targeted field of application.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J. Zhao, X. Mao, and L. Chen, “Speech emotion recognition using deep 1D &amp; 2D CNN LSTM networks,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Biomedical Signal Processing and Control</em>, vol. 47, pp. 312–323, 1 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. A. Jalal, R. Milner, and T. Hain, “Empirical interpretation of speech emotion perception with attention based model for speech emotion recognition,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</em>, vol. 2020-October, pp. 4113–4117, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
E. Lieskovská, M. Jakubec, R. Jarina, M. Chmulík, Y.-F. Liao, P. Bours, and C. Kwan, “A Review on Speech Emotion Recognition Using Deep Learning and Attention Mechanism,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Electronics 2021, Vol. 10, Page 1163</em>, vol. 10, no. 10, p. 1163, 5 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. Latif, R. Rana, S. Khalifa, R. Jurdak, and B. W. Schuller, “Multitask Learning From Augmented Auxiliary Data for Improving Speech Emotion Recognition,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em>, pp. 1–13, 7 2022. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ieeexplore.ieee.org/document/9947296/</span>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
W. Han, H. Ruan, X. Chen, Z. Wang, H. Li, and B. Schuller, “Towards temporal modelling of categorical speech emotion recognition,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</em>, vol. 2018-September, pp. 932–936, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
K. N. Haque, M. A. Yousuf, and R. Rana, “Image denoising and restoration with CNN-LSTM Encoder Decoder with Direct Attention,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv Prepr.</em>, pp. 1–12, 1 2018. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/1801.05141v1</span>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y. Li, T. Zhao, and T. Kawahara, “Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</em>, vol. 2019-September, pp. 2803–2807, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S. Latif, R. Rana, S. Khalifa, R. Jurdak, and B. W. Schuller, “Self Supervised Adversarial Domain Adaptation for Cross-Corpus and Cross-Language Speech Emotion Recognition,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em>, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H. Sun, Z. Lian, B. Liu, Y. Li, L. Sun, C. Cai, J. Tao, M. Wang, and Y. Cheng, “EmotionNAS: Two-stream Neural Architecture Search for Speech Emotion Recognition,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</em>, vol. 2023-August, pp. 3597–3601, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
X. Wu, S. Hu, Z. Wu, X. Liu, and H. Meng, “Neural Architecture Search for Speech Emotion Recognition,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings</em>, vol. 2022-May, pp. 6902–6906, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Y. Xu, L. Xie, X. Zhang, X. Chen, G.-J. Qi, Q. Tian, and H. Xiong, “PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 7 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
G. M. Biju, G. N. Pillai, and J. Seshadrinath, “Electric load demand forecasting with RNN cell generated by DARTS,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Region 10 Annual International Conference, Proceedings/TENCON</em>, vol. 2019-October, pp. 2111–2116, 10 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
R. Maulik, R. Egele, C. Polytechnique, B. Lusch, and P. Balaprakash, “Recurrent Neural Network Architecture Search for Geophysical Emulation,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</em>, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
W. Q. Zheng, J. S. Yu, and Y. X. Zou, “An experimental study of speech emotion recognition based on deep convolutional neural networks,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2015 International Conference on Affective Computing and Intelligent Interaction, ACII 2015</em>, pp. 827–831, 12 2015.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A. Nicolaou, B. Schuller, and S. Zafeiriou, “Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings</em>, pp. 5200–5204, 5 2016.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Z. Zhang, J. Han, K. Qian, and B. Schuller, “Evolving Learning for Analysing Mood-Related Infant Vocalisation,” <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings INTERSPEECH 2018, 19. Annual Conference of the International Speech Communication Association</em>, pp. 142–146, 2018.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
B. Zoph and Q. V. Le, “Neural Architecture Search with Reinforcement Learning,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings</em>, 11 2016.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
E. Liberis, L. Dudziak, and N. D. Lane, “<math id="bib.bib18.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="bib.bib18.1.m1.1a"><mi id="bib.bib18.1.m1.1.1" xref="bib.bib18.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="bib.bib18.1.m1.1b"><ci id="bib.bib18.1.m1.1.1.cmml" xref="bib.bib18.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib18.1.m1.1c">\mu</annotation></semantics></math>NAS: Constrained Neural Architecture Search for Microcontrollers,” <em id="bib.bib18.2.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st Workshop on Machine Learning and Systems, EuroMLSys 2021</em>, pp. 70–79, 10 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
C. Gong, Z. Jiang, D. Wang, Y. Lin, Q. Liu, and D. Z. Pan, “Mixed precision neural architecture search for energy efficient deep learning,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM International Conference on Computer-Aided Design, Digest of Technical Papers, ICCAD</em>, vol. 2019-November, 11 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
H. Liu, K. Simonyan, and Y. Yang, “DARTS: Differentiable Architecture Search,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">7th International Conference on Learning Representations, ICLR 2019</em>, 6 2018. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/1806.09055v2</span>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei, A. Yuille, J. Huang, and K. Murphy, “Progressive Neural Architecture Search,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision (ECCV)</em>, 2018, pp. 19–34.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
H. Liang, S. Zhang, J. Sun, X. He, W. Huang, K. Zhuang, and Z. Li, “DARTS+: Improved Differentiable Architecture Search with Early Stopping,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>, 9 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A. Wan, X. Dai, P. Zhang, Z. He, Y. Tian, S. Xie, B. Wu, M. Yu, T. Xu, K. Chen, P. Vajda, and J. E. Gonzalez, “FBNetV2: Differentiable Neural Architecture Search for Spatial and Channel Dimensions,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020, pp. 12 965–12 974.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
X. Chen and C.-J. Hsieh, “Stabilizing Differentiable Architecture Search via Perturbation-based Regularization,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 37th International Conference on Machine Learning</em>, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
H. Yu, H. Peng, Y. Huang, J. Fu, H. Du, L. Wang, and H. Ling, “Cyclic Differentiable Architecture Search,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 45, no. 1, pp. 211–228, 1 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
T. Rajapakshe, R. Rana, S. Khalifa, B. Sisman, and B. W. Schuller, “Enhancing Speech Emotion Recognition Through Differentiable Architecture Search,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint</em>, 5 2023. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://arxiv.org/abs/2305.14402v3</span>

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Y. Zhang, J. Du, Z. Wang, J. Zhang, and Y. Tu, “Attention Based Fully Convolutional Network for Speech Emotion Recognition,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, APSIPA ASC 2018 - Proceedings</em>, pp. 1771–1775, 7 2018.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
M. Chen, X. He, J. Yang, and H. Zhang, “3-D Convolutional Recurrent Neural Networks with Attention Model for Speech Emotion Recognition,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Letters</em>, vol. 25, no. 10, pp. 1440–1444, 10 2018.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
H. Zou, Y. Si, C. Chen, D. Rajan, and E. S. Chng, “Speech Emotion Recognition with Co-Attention Based Multi-Level Acoustic Information,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings</em>, vol. 2022-May, pp. 7367–7371, 2022.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Z. T. Liu, M. T. Han, B. H. Wu, and A. Rehman, “Speech emotion recognition based on convolutional neural network with attention-based bidirectional long short-term memory network and multi-task learning,” <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Applied Acoustics</em>, vol. 202, p. 109178, 1 2023.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “IEMOCAP: interactive emotional dyadic motion capture database,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Language Resources and Evaluation</em>, vol. 42, no. 4, p. 335, 2008.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
C. Busso, S. Parthasarathy, A. Burmania, M. AbdelWahab, N. Sadoughi, and E. M. Provost, “MSP-IMPROV: An Acted Corpus of Dyadic Interactions to Study Emotion Perception,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em>, vol. 8, no. 1, pp. 67–80, 2017.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
R. Lotfian and C. Busso, “Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech from Existing Podcast Recordings,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em>, vol. 10, no. 4, pp. 471–483, 10 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
S. Davis and P. Mermelstein, “Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences,” <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on acoustics, speech, and signal processing</em>, vol. 28, no. 4, pp. 357–366, 1980.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
S. Latif, R. Rana, S. Khalifa, R. Jurdak, and J. Epps, “Direct Modelling of Speech Emotion from Raw Speech,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</em>, 2019, pp. 3920–3924.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
B. McFee, C. Raffel, D. Liang, D. P. W. Ellis, M. McVicar, E. Battenberg, and O. Nieto, “librosa: Audio and music signal analysis in python,” in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 14th python in science conference</em>, vol. 8, 2015.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury Google, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. K. Xamla, E. Yang, Z. Devito, M. Raison Nabla, A. Tejani, S. Chilamkurthy, Q. Ai, B. Steiner, L. F. Facebook, J. B. Facebook, and S. Chintala, “PyTorch: An Imperative Style, High-Performance Deep Learning Library,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, pp. 8024–8035, 2019.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.14082" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.14083" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.14083">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.14083" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.14084" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 13:35:17 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
