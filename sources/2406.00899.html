<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.00899] YODAS: YouTube-Oriented Dataset for Audio and Speech</title><meta property="og:description" content="In this study, we introduce YODAS (YouTube-Oriented Dataset for Audio and Speech), a large-scale, multilingual dataset comprising currently over 500k hours of speech data in more than 100 languages, sourced from both l…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="YODAS: YouTube-Oriented Dataset for Audio and Speech">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="YODAS: YouTube-Oriented Dataset for Audio and Speech">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.00899">

<!--Generated on Fri Jul  5 17:28:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">YODAS: YouTube-Oriented Dataset for Audio and Speech</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">In this study, we introduce YODAS (YouTube-Oriented Dataset for Audio and Speech), a large-scale, multilingual dataset comprising currently over 500k hours of speech data in more than 100 languages, sourced from both labeled and unlabeled YouTube speech datasets. The labeled subsets, including manual or automatic subtitles, facilitate supervised model training. Conversely, the unlabeled subsets are apt for self-supervised learning applications. YODAS is distinctive as the first publicly available dataset of its scale, and it is distributed under a Creative Commons license<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://huggingface.co/datasets/espnet/yodas</span></span></span>
. We introduce the collection methodology utilized for YODAS, which contributes to the large-scale speech dataset construction. Subsequently, we provide a comprehensive analysis of speech, text contained within the dataset. Finally, we describe the speech recognition baselines over the top-15 languages.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">— </span></span>
multilingual speech processing, speech recognition, large-scale speech dataset</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In recent years, significant advancements have been achieved in the field of speech recognition. With a sufficiently large speech dataset, it becomes feasible to train various end-to-end models using objectives such as CTC, ASG, seq2seq, RNN Transducer, and others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. We also observe the trend of using self-supervised learning models such as HuBERT and wav2vec2 to take advantage of unlabeled datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Those improvements have been realized primarily through the utilization of large-scale multilingual speech datasets. For example, the BABEL project was a pioneering endeavor that scaled multilingual capabilities significantly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. The Common Voice project, facilitates an online speech collection interface, offering speech datasets in over 100 languages and encompassing 18,000 hours validated recording hours <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. The MLS, a multilingual dataset, was derived from Librispeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Concerning linguistic diversity, the CMU Wilderness and MMS-Lab dataset, originating from the religious domain, covers nearly 1,000 languages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Unlabeled dataset such as Libri-light has also been applied successfully to train self-supervised models such as HuBERT, wav2vec2, and WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Despite the achievements with large-scale datasets, most public speech datasets available do not exceed 100,000 hours. In contrast, industry-utilized speech models are typically much more extensive. For instance, Whisper and Google’s USM, have been trained with over 100,000 hours and up to 1,000,000 hours of data, respectively <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. However, the details of the datasets used to train these models remain undisclosed, which makes it difficult to reproduce those models. Addressing the limitation of the lack of industry-scale large dataset, this paper presents YODAS (YouTube-Oriented Dataset of Audio and Speech)—a large-scale multilingual dataset, which comprises the following three subsets:</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2406.00899/assets/arch_v3.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="350" height="214" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text ltx_font_bold">Fig. 1</span>: </span>Diagram of our data collection architecture: It incorporates three types of clients: Keyword-based, Channel-based, and Download workers. Each worker fulfills specific tasks and interacts with both the master node and the YouTube platform. Additionally, the Download worker also handles the transfer of downloaded data to external storage.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">The <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">manual</span> subset encompasses 86,400 hours of audio data paired with manual transcriptions.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">The <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">automatic</span> subset, containing 335,845 hours of audio data, is supplemented with automatic transcriptions.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">The <span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">unlabeled</span> subset consists of 144,174 hours of raw audio data, devoid of any transcription.</p>
</div>
</li>
</ol>
</div>
<figure id="S1.T1" class="ltx_table">
<table id="S1.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.T1.2.1.1" class="ltx_tr">
<th id="S1.T1.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Dataset</th>
<th id="S1.T1.2.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_tt"># Languages</th>
<td id="S1.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Total Hours</td>
<td id="S1.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Speech Type</td>
<td id="S1.T1.2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">Labeled</td>
<td id="S1.T1.2.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">Public</td>
<td id="S1.T1.2.1.1.7" class="ltx_td ltx_align_center ltx_border_tt">License</td>
</tr>
<tr id="S1.T1.2.2.2" class="ltx_tr">
<th id="S1.T1.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BABEL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</th>
<th id="S1.T1.2.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">17</th>
<td id="S1.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t">1k hours</td>
<td id="S1.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t">Spontaneous</td>
<td id="S1.T1.2.2.2.5" class="ltx_td ltx_align_center ltx_border_t">Yes</td>
<td id="S1.T1.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t">Yes</td>
<td id="S1.T1.2.2.2.7" class="ltx_td ltx_align_center ltx_border_t">IARPA Babel License</td>
</tr>
<tr id="S1.T1.2.3.3" class="ltx_tr">
<th id="S1.T1.2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Common Voice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</th>
<th id="S1.T1.2.3.3.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">112</th>
<td id="S1.T1.2.3.3.3" class="ltx_td ltx_align_center">18k hours</td>
<td id="S1.T1.2.3.3.4" class="ltx_td ltx_align_center">Read</td>
<td id="S1.T1.2.3.3.5" class="ltx_td ltx_align_center">Yes</td>
<td id="S1.T1.2.3.3.6" class="ltx_td ltx_align_center">Yes</td>
<td id="S1.T1.2.3.3.7" class="ltx_td ltx_align_center">CC-0</td>
</tr>
<tr id="S1.T1.2.4.4" class="ltx_tr">
<th id="S1.T1.2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MLS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</th>
<th id="S1.T1.2.4.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">8</th>
<td id="S1.T1.2.4.4.3" class="ltx_td ltx_align_center">50.5k hours</td>
<td id="S1.T1.2.4.4.4" class="ltx_td ltx_align_center">Read</td>
<td id="S1.T1.2.4.4.5" class="ltx_td ltx_align_center">Yes</td>
<td id="S1.T1.2.4.4.6" class="ltx_td ltx_align_center">Yes</td>
<td id="S1.T1.2.4.4.7" class="ltx_td ltx_align_center">CC BY 4.0</td>
</tr>
<tr id="S1.T1.2.5.5" class="ltx_tr">
<th id="S1.T1.2.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FLEURS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>
</th>
<th id="S1.T1.2.5.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">102</th>
<td id="S1.T1.2.5.5.3" class="ltx_td ltx_align_center">1.4k hours</td>
<td id="S1.T1.2.5.5.4" class="ltx_td ltx_align_center">Read</td>
<td id="S1.T1.2.5.5.5" class="ltx_td ltx_align_center">Yes</td>
<td id="S1.T1.2.5.5.6" class="ltx_td ltx_align_center">Yes</td>
<td id="S1.T1.2.5.5.7" class="ltx_td ltx_align_center">CC BY 2.5</td>
</tr>
<tr id="S1.T1.2.6.6" class="ltx_tr">
<th id="S1.T1.2.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">CMU Wilderness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</th>
<th id="S1.T1.2.6.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">700</th>
<td id="S1.T1.2.6.6.3" class="ltx_td ltx_align_center">14k hours</td>
<td id="S1.T1.2.6.6.4" class="ltx_td ltx_align_center">Read</td>
<td id="S1.T1.2.6.6.5" class="ltx_td ltx_align_center">Yes</td>
<td id="S1.T1.2.6.6.6" class="ltx_td ltx_align_center">Yes</td>
<td id="S1.T1.2.6.6.7" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S1.T1.2.7.7" class="ltx_tr">
<th id="S1.T1.2.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MMS-Lab  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</th>
<th id="S1.T1.2.7.7.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">1,107</th>
<td id="S1.T1.2.7.7.3" class="ltx_td ltx_align_center">44.7k hours</td>
<td id="S1.T1.2.7.7.4" class="ltx_td ltx_align_center">Read</td>
<td id="S1.T1.2.7.7.5" class="ltx_td ltx_align_center">Yes</td>
<td id="S1.T1.2.7.7.6" class="ltx_td ltx_align_center">No</td>
<td id="S1.T1.2.7.7.7" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S1.T1.2.8.8" class="ltx_tr">
<th id="S1.T1.2.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VoxLingua107 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</th>
<th id="S1.T1.2.8.8.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">107</th>
<td id="S1.T1.2.8.8.3" class="ltx_td ltx_align_center">6.6k hours</td>
<td id="S1.T1.2.8.8.4" class="ltx_td ltx_align_center">Spontaneous</td>
<td id="S1.T1.2.8.8.5" class="ltx_td ltx_align_center">Yes</td>
<td id="S1.T1.2.8.8.6" class="ltx_td ltx_align_center">Yes</td>
<td id="S1.T1.2.8.8.7" class="ltx_td ltx_align_center">CC BY 4.0</td>
</tr>
<tr id="S1.T1.2.9.9" class="ltx_tr">
<th id="S1.T1.2.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Librilight <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</th>
<th id="S1.T1.2.9.9.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">1</th>
<td id="S1.T1.2.9.9.3" class="ltx_td ltx_align_center">60k hours</td>
<td id="S1.T1.2.9.9.4" class="ltx_td ltx_align_center">Read</td>
<td id="S1.T1.2.9.9.5" class="ltx_td ltx_align_center">No</td>
<td id="S1.T1.2.9.9.6" class="ltx_td ltx_align_center">Yes</td>
<td id="S1.T1.2.9.9.7" class="ltx_td ltx_align_center">CC BY 4.0</td>
</tr>
<tr id="S1.T1.2.10.10" class="ltx_tr">
<th id="S1.T1.2.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Whisper<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</th>
<th id="S1.T1.2.10.10.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">97</th>
<td id="S1.T1.2.10.10.3" class="ltx_td ltx_align_center">680k hours</td>
<td id="S1.T1.2.10.10.4" class="ltx_td ltx_align_center">Unknown</td>
<td id="S1.T1.2.10.10.5" class="ltx_td ltx_align_center">Yes/No</td>
<td id="S1.T1.2.10.10.6" class="ltx_td ltx_align_center">No</td>
<td id="S1.T1.2.10.10.7" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S1.T1.2.11.11" class="ltx_tr">
<th id="S1.T1.2.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">USM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</th>
<th id="S1.T1.2.11.11.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">300</th>
<td id="S1.T1.2.11.11.3" class="ltx_td ltx_align_center">12M hours</td>
<td id="S1.T1.2.11.11.4" class="ltx_td ltx_align_center">Spontaneous</td>
<td id="S1.T1.2.11.11.5" class="ltx_td ltx_align_center">Yes/No</td>
<td id="S1.T1.2.11.11.6" class="ltx_td ltx_align_center">No</td>
<td id="S1.T1.2.11.11.7" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S1.T1.2.12.12" class="ltx_tr">
<th id="S1.T1.2.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">YODAS (manual)</th>
<th id="S1.T1.2.12.12.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">140</th>
<td id="S1.T1.2.12.12.3" class="ltx_td ltx_align_center ltx_border_t">86k hours</td>
<td id="S1.T1.2.12.12.4" class="ltx_td ltx_align_center ltx_border_t">Spontaneous</td>
<td id="S1.T1.2.12.12.5" class="ltx_td ltx_align_center ltx_border_t">Yes</td>
<td id="S1.T1.2.12.12.6" class="ltx_td ltx_align_center ltx_border_t">Yes</td>
<td id="S1.T1.2.12.12.7" class="ltx_td ltx_align_center ltx_border_t">CC BY 3.0</td>
</tr>
<tr id="S1.T1.2.13.13" class="ltx_tr">
<th id="S1.T1.2.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">YODAS (automatic)</th>
<th id="S1.T1.2.13.13.2" class="ltx_td ltx_align_right ltx_th ltx_th_row">20</th>
<td id="S1.T1.2.13.13.3" class="ltx_td ltx_align_center">336k hours</td>
<td id="S1.T1.2.13.13.4" class="ltx_td ltx_align_center">Spontaneous</td>
<td id="S1.T1.2.13.13.5" class="ltx_td ltx_align_center">Yes</td>
<td id="S1.T1.2.13.13.6" class="ltx_td ltx_align_center">Yes</td>
<td id="S1.T1.2.13.13.7" class="ltx_td ltx_align_center">CC BY 3.0</td>
</tr>
<tr id="S1.T1.2.14.14" class="ltx_tr">
<th id="S1.T1.2.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">YODAS (unlabelled)</th>
<th id="S1.T1.2.14.14.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb">-</th>
<td id="S1.T1.2.14.14.3" class="ltx_td ltx_align_center ltx_border_bb">144k hours</td>
<td id="S1.T1.2.14.14.4" class="ltx_td ltx_align_center ltx_border_bb">Spontaneous</td>
<td id="S1.T1.2.14.14.5" class="ltx_td ltx_align_center ltx_border_bb">No</td>
<td id="S1.T1.2.14.14.6" class="ltx_td ltx_align_center ltx_border_bb">Yes</td>
<td id="S1.T1.2.14.14.7" class="ltx_td ltx_align_center ltx_border_bb">CC BY 3.0</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.3.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>A comparison of YODAS dataset with a few other large-scale multilingual datasets. Our YODAS dataset is the first public dataset to reach a scale of over 500k hours.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">When used conjointly, the <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">manual</span> and <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">automatic</span> subsets offer a comprehensive resource of 480k hours for supervised model training. Meanwhile, all three subsets may be used in conjunction with the application of self-supervised learning techniques.
It’s worth noting that the combined subsets will result in an extensive corpus of over 560k hours by July 2023, and this amount will continue to grow over time.
This marks the first time a dataset of this scale with the Creative Commons license has been made publicly available. We plan to release it from the Huggingface datasets.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Data Collection</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We designed our data collection architecture to fulfill two specific requirements. Firstly, the video content must be accompanied by a Creative Commons license. Secondly, the video should possess either an automatic subtitle or a manual subtitle as much as possible, although we also allow unlabeled videos. In order to meet these criteria, we devised a framework by improving upon an existing toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Our framework is depicted in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ YODAS: YouTube-Oriented Dataset for Audio and Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, which utilizes three distinct clients and a master node which we will discuss next.
Our data collection software will be open to the public for individual use or collaborative efforts.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Keyword-based Client</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The principal challenge within our data gathering pipeline lies in the efficient identification of proper videos to download. This is achieved by implementing keyword-based crawling and combining it with YouTube’s native filtering feature, allowing us to pinpoint a subset of relevant videos.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">In the first phase, we construct a list of keywords by extracting unique keywords from the dumps of multilingual Wikipedia articles. Figure <a href="#S2.F2" title="Figure 2 ‣ 2.1 Keyword-based Client ‣ 2 Data Collection ‣ YODAS: YouTube-Oriented Dataset for Audio and Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the language distribution of unique query keywords within one of our data shards. As depicted by the figure, English commands the majority share. Rather than querying every keyword from this distribution, we prioritize those derived from less prominent languages trying to enhance the diversity of our video dataset. Subsequently, we initiate a keyword-based query on YouTube by appending appropriate flags, enforcing that the videos listed in the search results should (mostly) possess subtitles and adhere to the Creative Commons license. The naive HTTP GET request tends to yield a subset of videos that have the highest relevance to the supplied keywords. In order to capitalize on the number of available videos, we use AJAX to dynamically crawl lower-ranked videos, mimicking the process of scrolling down the search result page.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2406.00899/assets/keyword_dist.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="350" height="190" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text ltx_font_bold">Fig. 2</span>: </span>language distribution of unique query keywords used in one of our shards (i.e., worker).</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Channel-based Client</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The keyword-based crawling approach alone, unfortunately, is insufficient in scaling our dataset to the substantial size that we desire. This is due to the tendency of YouTube to repetitively present the same popular video subset in its search results rather than proposing new content.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">To mitigate this challenge and broaden our video exploration, we combine keyword-based crawling with a channel-based crawling strategy. For each video discovered during the keyword-based search, we extract the corresponding channel. This channel then serves as a means to identify all affiliated videos with ease. This method significantly aids in the discovery of videos that are otherwise less likely to appear using the keyword-centric approach. Most crucially, videos hosted within the same channel typically share similar licensing and subtitle characteristics, simplifying the process of identifying new videos that meet our specific criteria.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Download Client</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The aforementioned workers solely undertake the task of identifying the correct video; however, the responsibility of downloading the video or its subtitles falls to the download worker. This worker perpetually monitors the database to ascertain whether a new video has been discovered by its predecessors. Upon identifying new videos, The download worker first retrieves the video, converting it into an audio format encoded at 1 channel and 24 kHz. Subsequently, it downloads the list of all available subtitles. Each video may either have multiple subtitles or none at all. The subtitles can be either manually uploaded by the user (manual subtitle) or automatically recognized by YouTube as enabled by the user (automatic subtitle). One significant challenge in this endeavor is determining the ”correct” subtitle and the language of each video. Surprisingly, many videos possess multiple subtitles across diverse languages. We employ a heuristic method to identify the language and choose which subtitle to download. If the target video only has a unique manual subtitle with no other subtitles, it’s highly probable this singular subtitle accurately reflects the language. We then proceed to download this subtitle and assign the language ID. Similarly, if the target video only has one automatic subtitle, we consider it to be accurate and proceed to download this subtitle. However, when a video has more than one manual or automatic subtitle with conflicting language IDs, the task of identifying the correct language becomes complicated. In such cases, we forego downloading the subtitles and leaving the video unlabeled. Although we attempt to identify the language using Language Identification (LID) tools, the results are not significantly successful. Therefore, we earmark this identification task for future work.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Master Node</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">In addition to those clients, we deploy a master node to monitor the overall progress. This node is connected to a PostgreSQL database and hosts an HTTP server that accepts GET/POST HTTP requests from each worker. The master node manages all resources—be it keywords, channels, or videos—each marked with one of three states: not-started, being processed, or done. The ’being processed’ state serves to prevent simultaneous downloads of the same resource by separate workers. All the workers typically function as HTTP clients, querying the master node to request the next available resource. Once the resource has been resolved (i.e., downloaded), these workers mark it as done.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Analysis</h2>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2406.00899/assets/lang_dist_v3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="699" height="148" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text ltx_font_bold">Fig. 3</span>: </span>Total duration (measured in hours) in the manual and automatic subset. The lower-blue bar shows the duration of the manual subset, the top-orange bar indicates the automatic subset. The combined duration is illustrated on top of each bar.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">As described in the previous section, the YODAS dataset was segmented into three subsets, namely: <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">Manual</span>, <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">Automatic</span>, and <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_italic">Unlabeled</span>. Videos within the manual subset are characterized by user-uploaded, (possibly) manually generated subtitles, while those included in the automatic subset have associated automated subtitles. Conversely, the unlabeled subset consists of videos devoid of subtitles, primarily due to our current inability to accurately identify the language.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Speech Analysis</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Table <a href="#S3.T2" title="Table 2 ‣ 3.2 Speech Analysis ‣ 3 Analysis ‣ YODAS: YouTube-Oriented Dataset for Audio and Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the key statistics concerning the distribution of raw video durations and utterance durations (shown in parentheses) within our datasets. Notably, the raw video duration of the automatic subset exhibits a notably higher mean duration and standard deviation compared to the other two datasets. Conversely, the average duration of utterances and its standard deviation are notably lower in the automatic subset as compared to the manual subset. This is because YouTube tends to chunk speech into small segments as we will discuss further in the next subsection. In total, we have compiled an extensive dataset comprising 86,000 hours for the manual subset, 336,000 hours for the automatic subset, and an additional 144,000 hours for the unlabeled subset.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.2.1.1" class="ltx_tr">
<th id="S3.T2.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S3.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Manual</th>
<th id="S3.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Automatic</th>
<th id="S3.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Unlabeled</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.2.2.1" class="ltx_tr">
<th id="S3.T2.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Mean</th>
<td id="S3.T2.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.15h (5.6s)</td>
<td id="S3.T2.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.23h (3.2s)</td>
<td id="S3.T2.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.15h (-)</td>
</tr>
<tr id="S3.T2.2.3.2" class="ltx_tr">
<th id="S3.T2.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Std</th>
<td id="S3.T2.2.3.2.2" class="ltx_td ltx_align_center">0.35h (8.9s)</td>
<td id="S3.T2.2.3.2.3" class="ltx_td ltx_align_center">0.37h (1.6s)</td>
<td id="S3.T2.2.3.2.4" class="ltx_td ltx_align_center">0.25h (-)</td>
</tr>
<tr id="S3.T2.2.4.3" class="ltx_tr">
<th id="S3.T2.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Min</th>
<td id="S3.T2.2.4.3.2" class="ltx_td ltx_align_center">0.00h (0.0s)</td>
<td id="S3.T2.2.4.3.3" class="ltx_td ltx_align_center">0.00h (0.1s)</td>
<td id="S3.T2.2.4.3.4" class="ltx_td ltx_align_center">0.00h (-)</td>
</tr>
<tr id="S3.T2.2.5.4" class="ltx_tr">
<th id="S3.T2.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Max</th>
<td id="S3.T2.2.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">24.9h (42.1s)</td>
<td id="S3.T2.2.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">24.9h (87.7s)</td>
<td id="S3.T2.2.5.4.4" class="ltx_td ltx_align_center ltx_border_bb">24.9h (-)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.3.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>Descriptive statistics of raw video duration distribution (measured in hours) and utterance duration distribution in parentheses (measured in seconds) in three subsets.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Next, the language distribution of the manual subset and the automatic subset is illustrated in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Analysis ‣ YODAS: YouTube-Oriented Dataset for Audio and Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. This figure presents the distribution of the top 30 languages out of a total of 140 evaluated. As anticipated, English (en) emerges as the most prevalently used language, with Spanish (es) and Russian (ru) occupying the second and third positions, respectively, when assessed based on duration. Although the language distribution trend appears similar between the automatic and manual subsets, the automatic subset has only a very limited number of languages (14 languages) compared with the manual subset (140 languages).</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Text Analysis</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The top 10 writing systems in our dataset are Latin, Cyrillic, CJK, Hiragana, Greek, Devanagari, Hangul, Malayalam, Katakana, and Arabic. It largely corresponds to the aforementioned language distribution, with the Latin alphabet appearing as the most frequently used writing system. The Cyrillic script, originating from Russian language videos, also features prominently within our dataset.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T3.2.1.1" class="ltx_tr">
<th id="S3.T3.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S3.T3.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Manual</td>
<td id="S3.T3.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Automatic</td>
</tr>
<tr id="S3.T3.2.2.2" class="ltx_tr">
<th id="S3.T3.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Mean</th>
<td id="S3.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t">58.2</td>
<td id="S3.T3.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t">33.5</td>
</tr>
<tr id="S3.T3.2.3.3" class="ltx_tr">
<th id="S3.T3.2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Std</th>
<td id="S3.T3.2.3.3.2" class="ltx_td ltx_align_center">27.6</td>
<td id="S3.T3.2.3.3.3" class="ltx_td ltx_align_center">8.6</td>
</tr>
<tr id="S3.T3.2.4.4" class="ltx_tr">
<th id="S3.T3.2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Min</th>
<td id="S3.T3.2.4.4.2" class="ltx_td ltx_align_center">0.0</td>
<td id="S3.T3.2.4.4.3" class="ltx_td ltx_align_center">0.0</td>
</tr>
<tr id="S3.T3.2.5.5" class="ltx_tr">
<th id="S3.T3.2.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Max</th>
<td id="S3.T3.2.5.5.2" class="ltx_td ltx_align_center ltx_border_bb">588</td>
<td id="S3.T3.2.5.5.3" class="ltx_td ltx_align_center ltx_border_bb">44</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.3.1.1" class="ltx_text ltx_font_bold">Table 3</span>: </span>Descriptive statistics of subtitle transcription measured in the number of characters in two subsets.</figcaption>
</figure>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Table <a href="#S3.T3" title="Table 3 ‣ 3.3 Text Analysis ‣ 3 Analysis ‣ YODAS: YouTube-Oriented Dataset for Audio and Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> is the comparison of character length distribution from the manual subset and automatic subset. The manual subset tends to have a larger number of characters per utterance and have a larger variance. Conversely, the automatic subset has an even distribution where most utterances are short and have little variance. This is because the automatic subtitle frequently divides long utterances into small chunks as Table <a href="#S3.T4" title="Table 4 ‣ 3.3 Text Analysis ‣ 3 Analysis ‣ YODAS: YouTube-Oriented Dataset for Audio and Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> indicates, this splitting might be a feature to help viewers to follow subtitles easier.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<table id="S3.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.2.1.1" class="ltx_tr">
<th id="S3.T4.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">utt id</th>
<th id="S3.T4.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">automatic transcription</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.2.2.1" class="ltx_tr">
<th id="S3.T4.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">00682</th>
<td id="S3.T4.2.2.1.2" class="ltx_td ltx_align_left ltx_border_t">if you’re trying to do something in your</td>
</tr>
<tr id="S3.T4.2.3.2" class="ltx_tr">
<th id="S3.T4.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">00683</th>
<td id="S3.T4.2.3.2.2" class="ltx_td ltx_align_left">community and you’re spending your money</td>
</tr>
<tr id="S3.T4.2.4.3" class="ltx_tr">
<th id="S3.T4.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">00684</th>
<td id="S3.T4.2.4.3.2" class="ltx_td ltx_align_left">public money or somebody’s money to</td>
</tr>
<tr id="S3.T4.2.5.4" class="ltx_tr">
<th id="S3.T4.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">00685</th>
<td id="S3.T4.2.5.4.2" class="ltx_td ltx_align_left ltx_border_bb">really do something that makes a</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T4.3.1.1" class="ltx_text ltx_font_bold">Table 4</span>: </span>A sample of the automatic transcriptions, one individual utterance is usually divided into multiple small chunks.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The YODAS dataset offers a versatile resource that can be employed for a variety of tasks including supervised training, weakly-labeled supervised training, and self-supervised training. In this work, we focus specifically on the use of the dataset for the monolingual speech recognition task.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Speech-Text Alignment</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The raw dataset, as collected, presents considerable noise with regard to speech-text alignment, suggesting that its unfiltered usage might be inappropriate. There are instances where neither the manual nor automatic subtitles accurately represent the underlying spoken discourse. For instance, subtitles occasionally serve as concise descriptors of the current scene in a video, annotating elements such as laughter or musical segments, rather than transcribing the actual spoken dialogue. Our heuristics to decide the language based on the list of subtitles might also introduce errors, those errors possibly arising from user inaccuracies or misidentifications in YouTube’s language detection system.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">To filter the dataset, we first apply the speech-text alignment. In particular, we use a pre-trained acoustic model to score every utterance in the audio and only consider using the high-scoring utterance pairs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. The score is obtained from the CTC loss where a lower value (loss) implies a better alignment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. Figure <a href="#S4.F4" title="Figure 4 ‣ 4.1 Speech-Text Alignment ‣ 4 Experiment ‣ YODAS: YouTube-Oriented Dataset for Audio and Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents a scatter plot depicting the relationship between the duration and alignment score of 1,000 utterances randomly sampled from the <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">manual</span> subset. From the plot, it is evident that while there are occasional outliers with poor alignment scores such as 18.0, the majority of utterances exhibit a duration of less than 10 seconds and possess an alignment score superior to 5. Conversely, Figure <a href="#S4.F5" title="Figure 5 ‣ 4.1 Speech-Text Alignment ‣ 4 Experiment ‣ YODAS: YouTube-Oriented Dataset for Audio and Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> portrays a scatter plot derived from the <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_italic">automatic</span> subset. This plot reveals a significant proportion of misaligned utterances. It should be noted that for analytical purposes, all scores exceeding 20 have been capped at 20. These misaligned utterances typically display a considerable duration, often extending to as long as 50 seconds, and are predominantly attributed to music or background noise.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2406.00899/assets/manual_score_dist_v2.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="350" height="262" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text ltx_font_bold">Fig. 4</span>: </span>the score histogram and scatter plot of the relationship between the duration and the alignment in the manual subset.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2406.00899/assets/auto_score_dist_v2.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="350" height="265" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text ltx_font_bold">Fig. 5</span>: </span>the score histogram and scatter plot between the duration and the alignment score in the automatic subset.</figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">In the subsequent experiment, we employ an alignment threshold to exclude utterances with scores worse than <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">2.0</span>. From the refined subset, a sample of 1,000,000 utterances (at most) is randomly selected to constitute the training set for each language, while a separate, smaller selection of 1,000 utterances is assigned to the testing dataset.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Baseline</h3>

<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T5.1.2.1" class="ltx_tr">
<th id="S4.T5.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">language</th>
<td id="S4.T5.1.2.1.2" class="ltx_td ltx_align_right ltx_border_tt">ell</td>
<td id="S4.T5.1.2.1.3" class="ltx_td ltx_align_right ltx_border_tt">nld</td>
<td id="S4.T5.1.2.1.4" class="ltx_td ltx_align_right ltx_border_tt">hun</td>
<td id="S4.T5.1.2.1.5" class="ltx_td ltx_align_right ltx_border_tt">pol</td>
<td id="S4.T5.1.2.1.6" class="ltx_td ltx_align_right ltx_border_tt">por</td>
<td id="S4.T5.1.2.1.7" class="ltx_td ltx_align_right ltx_border_tt">cmn</td>
<td id="S4.T5.1.2.1.8" class="ltx_td ltx_align_right ltx_border_tt">ind</td>
<td id="S4.T5.1.2.1.9" class="ltx_td ltx_align_right ltx_border_tt">jpn</td>
<td id="S4.T5.1.2.1.10" class="ltx_td ltx_align_right ltx_border_tt">tur</td>
<td id="S4.T5.1.2.1.11" class="ltx_td ltx_align_right ltx_border_tt">ita</td>
<td id="S4.T5.1.2.1.12" class="ltx_td ltx_align_right ltx_border_tt">deu</td>
<td id="S4.T5.1.2.1.13" class="ltx_td ltx_align_right ltx_border_tt">fra</td>
<td id="S4.T5.1.2.1.14" class="ltx_td ltx_align_right ltx_border_tt">spa</td>
<td id="S4.T5.1.2.1.15" class="ltx_td ltx_align_right ltx_border_tt">rus</td>
<td id="S4.T5.1.2.1.16" class="ltx_td ltx_align_right ltx_border_tt">eng</td>
<td id="S4.T5.1.2.1.17" class="ltx_td ltx_align_right ltx_border_tt">average</td>
</tr>
<tr id="S4.T5.1.1" class="ltx_tr">
<th id="S4.T5.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">CER (<math id="S4.T5.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T5.1.1.1.m1.1a"><mo stretchy="false" id="S4.T5.1.1.1.m1.1.1" xref="S4.T5.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.m1.1b"><ci id="S4.T5.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</th>
<td id="S4.T5.1.1.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">8.3</td>
<td id="S4.T5.1.1.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">8.4</td>
<td id="S4.T5.1.1.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">6.2</td>
<td id="S4.T5.1.1.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">6.4</td>
<td id="S4.T5.1.1.6" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">7.1</td>
<td id="S4.T5.1.1.7" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">12.5</td>
<td id="S4.T5.1.1.8" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">10.2</td>
<td id="S4.T5.1.1.9" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">14.7</td>
<td id="S4.T5.1.1.10" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">8.7</td>
<td id="S4.T5.1.1.11" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">8.6</td>
<td id="S4.T5.1.1.12" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">10.1</td>
<td id="S4.T5.1.1.13" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">12.9</td>
<td id="S4.T5.1.1.14" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">9.8</td>
<td id="S4.T5.1.1.15" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">12.8</td>
<td id="S4.T5.1.1.16" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">12.9</td>
<td id="S4.T5.1.1.17" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">9.97</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.3.1.1" class="ltx_text ltx_font_bold">Table 5</span>: </span>monolingual speech recognition performance on the top-15 languages (ordered by the duration) from the manual subset. The evaluation is done by using character error rate (CER) where a lower number indicates a better performance.</figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We build simple monolingual baseline models for the top-25 languages in the manual subset. Our model is based on the pre-trained XLSR representations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, where we have a linear layer randomly initialized on top of the pre-trained representations, which is then optimized with the CTC loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. The preparation is done by using ESPnet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and s3prl <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. The subword vocabulary is prepared with BPE using SentencePiece <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, where we use 300 as the vocabulary size for most languages except for CJK languages where we use 5000 for Mandarin and 3000 for Japanese. For simplicity, we do not perform speech augmentation such as SpecAugment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and Speed Perturbation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. The acoustic model is optimized with the AdamW optimizer with a fixed learning rate of 0.0001 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. The decoding is done greedily without any language models.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Table.<a href="#S4.T5" title="Table 5 ‣ 4.2 Baseline ‣ 4 Experiment ‣ YODAS: YouTube-Oriented Dataset for Audio and Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> displays the testing outcomes for the top-15 languages, measured by the Character Error Rate (CER). The respective CER for each language spans from 6 to 15. The best performance is recorded for Hungarian, with a CER of 6.2, while Japanese exhibits the least performance with a CER of 14.7. The average CER across all languages is 9.97. We observe that languages possessing a larger BPE vocabulary size, such as Mandarin (cmn) and Japanese (jpn), tend to correspond with higher character error rates (Mandarin has a CER of 12.5 and Japanese has a CER of 14.7). Conversely, languages that adhere to more straightforward spelling rules generally exhibit lower character error rates. For example, the writing system in Hungarian is mostly phonemic and achieves the lowest CER 6.2 in our experiment.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.4.4" class="ltx_tr">
<th id="S4.T6.4.4.5" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T6.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">CER (<math id="S4.T6.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T6.1.1.1.m1.1a"><mo stretchy="false" id="S4.T6.1.1.1.m1.1.1" xref="S4.T6.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.m1.1b"><ci id="S4.T6.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.m1.1c">\downarrow</annotation></semantics></math>)</th>
<th id="S4.T6.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Add (<math id="S4.T6.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T6.2.2.2.m1.1a"><mo stretchy="false" id="S4.T6.2.2.2.m1.1.1" xref="S4.T6.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.2.m1.1b"><ci id="S4.T6.2.2.2.m1.1.1.cmml" xref="S4.T6.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.2.m1.1c">\downarrow</annotation></semantics></math>)</th>
<th id="S4.T6.3.3.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Del (<math id="S4.T6.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T6.3.3.3.m1.1a"><mo stretchy="false" id="S4.T6.3.3.3.m1.1.1" xref="S4.T6.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.3.m1.1b"><ci id="S4.T6.3.3.3.m1.1.1.cmml" xref="S4.T6.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.3.m1.1c">\downarrow</annotation></semantics></math>)</th>
<th id="S4.T6.4.4.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Sub (<math id="S4.T6.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T6.4.4.4.m1.1a"><mo stretchy="false" id="S4.T6.4.4.4.m1.1.1" xref="S4.T6.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T6.4.4.4.m1.1b"><ci id="S4.T6.4.4.4.m1.1.1.cmml" xref="S4.T6.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.4.4.4.m1.1c">\downarrow</annotation></semantics></math>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.4.5.1" class="ltx_tr">
<th id="S4.T6.4.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Manual</th>
<td id="S4.T6.4.5.1.2" class="ltx_td ltx_align_right ltx_border_t">14.9</td>
<td id="S4.T6.4.5.1.3" class="ltx_td ltx_align_right ltx_border_t">3.2</td>
<td id="S4.T6.4.5.1.4" class="ltx_td ltx_align_right ltx_border_t">7.5</td>
<td id="S4.T6.4.5.1.5" class="ltx_td ltx_align_right ltx_border_t">4.2</td>
</tr>
<tr id="S4.T6.4.6.2" class="ltx_tr">
<th id="S4.T6.4.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Automatic</th>
<td id="S4.T6.4.6.2.2" class="ltx_td ltx_align_right ltx_border_bb">32.3</td>
<td id="S4.T6.4.6.2.3" class="ltx_td ltx_align_right ltx_border_bb">1.6</td>
<td id="S4.T6.4.6.2.4" class="ltx_td ltx_align_right ltx_border_bb">26.6</td>
<td id="S4.T6.4.6.2.5" class="ltx_td ltx_align_right ltx_border_bb">4.1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.6.1.1" class="ltx_text ltx_font_bold">Table 6</span>: </span>A comparison of the speech models trained with the manual subset and the automatic subset. We demonstrate both the CER and its error decomposition of Addition (Add), Deletion (Del), and Substitution (Sub).</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2406.00899/assets/score_trend_v2.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="350" height="148" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text ltx_font_bold">Fig. 6</span>: </span>the relationship between the alignment score and its performance on the speech recognition task.</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">We subsequently analyzed the quality of the dataset across both the manual and automatic subsets by training independent models solely on each subset. For a balanced comparison, 100,000 utterances were randomly selected from each subset, post application of the 2.0 score filter, as introduced in Section <a href="#S4.SS1" title="4.1 Speech-Text Alignment ‣ 4 Experiment ‣ YODAS: YouTube-Oriented Dataset for Audio and Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. This comparison was solely conducted within the English subset. The findings, as displayed in Table <a href="#S4.T6" title="Table 6 ‣ 4.3 Results ‣ 4 Experiment ‣ YODAS: YouTube-Oriented Dataset for Audio and Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, reveal that models trained on the manual subset yield significantly superior performance compared to those trained on the automatic subset. Further analysis indicates the primary cause of this discrepancy was the deletion error. The automatic subset presented a notably high deletion error rate of 26.6, whereas the manual subset recorded a markedly lower rate of 7.0. These findings align with previous research, which has indicated that the use of automatically-generated transcripts tends to undermine system performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. Consequently, these results underscore the importance of prioritizing the utilization of the manual subset over the automatic subset in the training of models.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2406.00899/assets/size_trend_v2.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="350" height="155" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text ltx_font_bold">Fig. 7</span>: </span>the relationship between the size of the training dataset and its performance on the speech recognition task.</figcaption>
</figure>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">In the previous experiments, an alignment score threshold of 2.0 was implemented as a cut-off. To investigate how altering this threshold might impact performance, we performed an experiment with varied threshold values, ranging from 1 to 16. For analytical purposes, scores exceeding 16 were normalized to 16. We conducted this analysis using 100,000 utterances (160 hours) from the manual subset with the same testing set. The outcome of this exercise is illustrated in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.3 Results ‣ 4 Experiment ‣ YODAS: YouTube-Oriented Dataset for Audio and Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. It is evident that the setting of the alignment threshold is critical to model performance where raising the threshold from 16 to 2 results in consistent performance improvement. However, further tightening of the threshold from 2.0 to 1.0 caused a minor degradation in performance, from a CER of 14.7 to 14.8. This slight decrease may be attributed to the fact that utterances in the training set with an alignment score of 1.0 tend to be shorter and comprise fewer words than those within the subset with a score of 2.0. These findings imply the significance of judiciously selecting the alignment threshold when training models. Although a lower threshold might seem intuitively beneficial, it may inadvertently exclude longer, richer utterances, thereby potentially impacting performance.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">Finally, we investigated the impact of modifying the size of the training dataset, ranging from 10,000 to 10 million utterances (16 to 16k hours). All utterances were randomly selected from the English manual subset, adhering to an alignment threshold of 2.0. The result is depicted in Table <a href="#S4.F7" title="Figure 7 ‣ 4.3 Results ‣ 4 Experiment ‣ YODAS: YouTube-Oriented Dataset for Audio and Speech" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. The results demonstrate that increasing the quantity of utterances consistently enhances model performance. Interestingly, the model trained with 1 million utterances and the model trained with 10 million utterances exhibit a similar CER. This phenomenon could potentially be attributed to the simplistic architecture we employed - namely a linear model built upon pre-trained features. Such an architecture may not be fully equipped to leverage the expanded dataset.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this study, we presented the YODAS dataset, a comprehensive, multilingual dataset compiled from the YouTube platform. We delineated our data collection pipeline and provided preliminary analyses and baseline models based on the dataset. We anticipate that the YODAS dataset will serve as a valuable resource for the speech research community</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgements</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We use PSC Bridges2 and NCSA Delta via ACCESS allocation CIS210014, supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen
Schmidhuber,

</span>
<span class="ltx_bibblock">“Connectionist temporal classification: labelling unsegmented
sequence data with recurrent neural networks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proceedings of the 23rd international conference on Machine
learning</span>, 2006, pp. 369–376.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Ronan Collobert, Christian Puhrsch, and Gabriel Synnaeve,

</span>
<span class="ltx_bibblock">“Wav2letter: an end-to-end convnet-based speech recognition
system,”

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1609.03193</span>, 2016.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton,

</span>
<span class="ltx_bibblock">“Speech recognition with deep recurrent neural networks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">2013 IEEE international conference on acoustics, speech and
signal processing</span>. Ieee, 2013, pp. 6645–6649.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Ilya Sutskever, Oriol Vinyals, and Quoc V Le,

</span>
<span class="ltx_bibblock">“Sequence to sequence learning with neural networks,”

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, vol. 27,
2014.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Rohit Prabhavalkar, Takaaki Hori, Tara N Sainath, Ralf Schlüter, and Shinji
Watanabe,

</span>
<span class="ltx_bibblock">“End-to-end speech recognition: A survey,”

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.03329</span>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli,

</span>
<span class="ltx_bibblock">“wav2vec 2.0: A framework for self-supervised learning of speech
representations,”

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, vol. 33, pp.
12449–12460, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan
Salakhutdinov, and Abdelrahman Mohamed,

</span>
<span class="ltx_bibblock">“Hubert: Self-supervised speech representation learning by masked
prediction of hidden units,”

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language
Processing</span>, vol. 29, pp. 3451–3460, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Abdelrahman Mohamed, Hung-yi Lee, Lasse Borgholt, Jakob D Havtorn, Joakim Edin,
Christian Igel, Katrin Kirchhoff, Shang-Wen Li, Karen Livescu, Lars
Maaløe, et al.,

</span>
<span class="ltx_bibblock">“Self-supervised speech representation learning: A review,”

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</span>, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Mark JF Gales, Kate M Knill, Anton Ragni, and Shakti P Rath,

</span>
<span class="ltx_bibblock">“Speech recognition and keyword spotting for low-resource languages:
Babel project research at cued,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Fourth International workshop on spoken language technologies
for under-resourced languages (SLTU-2014)</span>. International Speech
Communication Association (ISCA), 2014, pp. 16–23.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler,
Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M Tyers, and Gregor
Weber,

</span>
<span class="ltx_bibblock">“Common voice: A massively-multilingual speech corpus,”

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.06670</span>, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan
Collobert,

</span>
<span class="ltx_bibblock">“Mls: A large-scale multilingual dataset for speech research,”

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2012.03411</span>, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Alan W Black,

</span>
<span class="ltx_bibblock">“Cmu wilderness multilingual speech dataset,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</span>. IEEE, 2019, pp. 5971–5975.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani
Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, et al.,

</span>
<span class="ltx_bibblock">“Scaling speech technology to 1,000+ languages,”

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.13516</span>, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Jacob Kahn, Morgane Rivière, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu,
Pierre-Emmanuel Mazaré, Julien Karadayi, Vitaliy Liptchinsky, Ronan
Collobert, Christian Fuegen, et al.,

</span>
<span class="ltx_bibblock">“Libri-light: A benchmark for asr with limited or no supervision,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</span>. IEEE, 2020, pp. 7669–7673.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu
Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al.,

</span>
<span class="ltx_bibblock">“Wavlm: Large-scale self-supervised pre-training for full stack
speech processing,”

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</span>, vol. 16,
no. 6, pp. 1505–1518, 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and
Ilya Sutskever,

</span>
<span class="ltx_bibblock">“Robust speech recognition via large-scale weak supervision,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>. PMLR, 2023,
pp. 28492–28518.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin
Chen, Bo Li, Vera Axelrod, Gary Wang, et al.,

</span>
<span class="ltx_bibblock">“Google usm: Scaling automatic speech recognition beyond 100
languages,”

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.01037</span>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth
Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna,

</span>
<span class="ltx_bibblock">“Fleurs: Few-shot learning evaluation of universal representations
of speech,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">2022 IEEE Spoken Language Technology Workshop (SLT)</span>. IEEE,
2023, pp. 798–805.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Jörgen Valk and Tanel Alumäe,

</span>
<span class="ltx_bibblock">“Voxlingua107: a dataset for spoken language recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">2021 IEEE Spoken Language Technology Workshop (SLT)</span>. IEEE,
2021, pp. 652–658.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Shinnosuke Takamichi, Ludwig Kürzinger, Takaaki Saeki, Sayaka Shiota, and
Shinji Watanabe,

</span>
<span class="ltx_bibblock">“Jtubespeech: corpus of japanese speech collected from youtube for
speech recognition and speaker verification,”

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2112.09323</span>, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Xinjian Li, Siddharth Dalmia, Juncheng Li, Matthew Lee, Patrick Littell, Jiali
Yao, Antonios Anastasopoulos, David R Mortensen, Graham Neubig, Alan W Black,
et al.,

</span>
<span class="ltx_bibblock">“Universal phone recognition with a multilingual allophone system,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</span>. IEEE, 2020, pp. 8249–8253.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Alex Graves and Navdeep Jaitly,

</span>
<span class="ltx_bibblock">“Towards end-to-end speech recognition with recurrent neural
networks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>. PMLR, 2014,
pp. 1764–1772.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Ludwig Kürzinger, Dominik Winkelbauer, Lujun Li, Tobias Watzel, and Gerhard
Rigoll,

</span>
<span class="ltx_bibblock">“Ctc-segmentation of large corpora for german end-to-end speech
recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">International Conference on Speech and Computer</span>. Springer,
2020, pp. 267–278.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman
Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, et al.,

</span>
<span class="ltx_bibblock">“Xls-r: Self-supervised cross-lingual speech representation learning
at scale,”

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2111.09296</span>, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba,
Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner,
Nanxin Chen, Adithya Renduchintala, and Tsubasa Ochiai,

</span>
<span class="ltx_bibblock">“ESPnet: End-to-end speech processing toolkit,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proceedings of Interspeech</span>, 2018, pp. 2207–2211.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Shu wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia,
Yist Y. Lin, Andy T. Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin,
Tzu-Hsien Huang, Wei-Cheng Tseng, Ko tik Lee, Da-Rong Liu, Zili Huang, Shuyan
Dong, Shang-Wen Li, Shinji Watanabe, Abdelrahman Mohamed, and Hung yi Lee,

</span>
<span class="ltx_bibblock">“SUPERB: Speech Processing Universal PERformance Benchmark,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech 2021</span>, 2021, pp. 1194–1198.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch,

</span>
<span class="ltx_bibblock">“Neural machine translation of rare words with subword units,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Proceedings of the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</span>, Berlin, Germany, Aug.
2016, pp. 1715–1725, Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Taku Kudo,

</span>
<span class="ltx_bibblock">“Subword regularization: Improving neural network translation models
with multiple subword candidates,”

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1804.10959</span>, 2018.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D.
Cubuk, and Quoc V. Le,

</span>
<span class="ltx_bibblock">“SpecAugment: A Simple Data Augmentation Method for Automatic
Speech Recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Proc. Interspeech 2019</span>, 2019, pp. 2613–2617.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur,

</span>
<span class="ltx_bibblock">“Audio augmentation for speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Sixteenth annual conference of the international speech
communication association</span>, 2015.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter,

</span>
<span class="ltx_bibblock">“Decoupled weight decay regularization,”

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1711.05101</span>, 2017.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun,
Xavier Garcia, Ciprian Chelba, and Colin Cherry,

</span>
<span class="ltx_bibblock">“Scaling laws for neural machine translation,”

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2109.07740</span>, 2021.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.00898" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.00899" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.00899">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.00899" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.00900" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 17:28:16 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
