<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.02938] AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models</title><meta property="og:description" content="Since humans can listen to audio and watch videos at faster speeds than actually observed, we often listen to or watch these pieces of content at higher playback speeds to increase the time efficiency of content compre…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.02938">

<!--Generated on Fri Apr  5 17:36:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="video information,  playback speed,  deep neural network,  speech recognition,  self-supervised learning">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kazuki Kawamura
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">The University of Tokyo, Tokyo, Japan</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id3.3.id1" class="ltx_text ltx_affiliation_institution">Sony CSL Kyoto, Kyoto, Japan</span><span id="id4.4.id2" class="ltx_text ltx_affiliation_country"></span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:kwmr@acm.org">kwmr@acm.org</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jun Rekimoto
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">The University of Tokyo, Tokyo, Japan</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_country"></span>
</span>
<span class="ltx_contact ltx_role_affiliation"><span id="id7.3.id1" class="ltx_text ltx_affiliation_institution">Sony CSL Kyoto, Kyoto, Japan</span><span id="id8.4.id2" class="ltx_text ltx_affiliation_country"></span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:rekimoto@acm.org">rekimoto@acm.org</a>
</span></span></span>
</div>
<div class="ltx_dates">(2023)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id9.id1" class="ltx_p">Since humans can listen to audio and watch videos at faster speeds than actually observed, we often listen to or watch these pieces of content at higher playback speeds to increase the time efficiency of content comprehension.
To further utilize this capability, systems that automatically adjust the playback speed according to the user’s condition and the type of content to assist in more efficient comprehension of time-series content have been developed.
However, there is still room for these systems to further extend human speed-listening ability by generating speech with playback speed optimized for even finer time units and providing it to humans.
In this study, we determine whether humans can hear the optimized speech and propose a system that automatically adjusts playback speed at units as small as phonemes while ensuring speech intelligibility.
The system uses the speech recognizer score as a proxy for how well a human can hear a certain unit of speech and maximizes the speech playback speed to the extent that a human can hear.
This method can be used to produce fast but intelligible speech.
In the evaluation experiment, we compared the speech played back at a constant fast speed and the flexibly speed-up speech generated by the proposed method in a blind test and confirmed that the proposed method produced speech that was easier to listen to.</p>
</div>
<div class="ltx_keywords">video information, playback speed, deep neural network, speech recognition, self-supervised learning
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2023</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Augmented Humans Conference; March 12–14, 2023; Glasgow, United Kingdom</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_booktitle"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Augmented Humans Conference (AHs ’23), March 12–14, 2023, Glasgow, United Kingdom</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_price"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">price: </span>15.00</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3582700.3582722</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-9984-5/23/03</span></span></span>
<figure id="S0.F1" class="ltx_figure ltx_teaserfigure"><img src="/html/2403.02938/assets/overview.png" id="S0.F1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="187" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>. </span><span id="S0.F1.3.2" class="ltx_text" style="font-size:90%;">AIx Speed optimizes the playback speed of a video in units as small as phonemes. The system maximizes the speed of audio to the extent that humans can hear it. Users can watch videos at a comfortable speed without having to adjust the playback speed.</span></figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>INTRODUCTION</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With the widespread use of video distribution services, people are increasingly watching videos for various purposes, including information gathering, learning, and entertainment.
Humans are capable of understanding naturally observed phenomena at rates faster than the original.
Therefore, when watching video or listening to audio, we often increase the playback speed of the content to understand more content in a shorter amount of time.
Existing research has shown that when watching videos for learning, under certain conditions, differences in video playback speed do not affect learning effectiveness and may even improve performance <cite class="ltx_cite ltx_citemacro_citep">(Nagahama and
Morita, <a href="#bib.bib23" title="" class="ltx_ref">2017</a>; Lang
et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2020</a>; Murphy et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite>.
It has also been reported that 30% to 80% of users prefer to watch dramas at high speed, although this varies from country to country <cite class="ltx_cite ltx_citemacro_citep">(Duan and Chen, <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">There are many advantages to listening to video and audio at high speeds.
As a result, many methods have been proposed to automatically adjust the playback speed according to the structure of the content and the condition of the user, to further enhance the human ability to listen at high speeds.
They are widely studied as video summarization <cite class="ltx_cite ltx_citemacro_citep">(Apostolidis et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite> and audio summarization <cite class="ltx_cite ltx_citemacro_citep">(Vartakavi
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2021</a>)</cite>.
There are two basic strategies to these methods.
The first is to scan the user’s intentions and behavior and leave only the parts that need to be viewed or adjust the playback speed in proportion to the user’s concentration level <cite class="ltx_cite ltx_citemacro_citep">(Kurihara, <a href="#bib.bib16" title="" class="ltx_ref">2011</a>; Kawamura et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2014</a>)</cite>.
The second is to speed up unnecessary parts of the content (i.e., parts that do not contain speech) or slow down parts that contain speech <cite class="ltx_cite ltx_citemacro_citep">(Kayukawa et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2018</a>; Higuchi
et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2017</a>; Song
et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2015</a>; Zhang
et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite>.
However, these methods do not explicitly model whether the resulting speech at different playback speeds is intelligible to humans, so it is unclear whether a wide range of speech types can be made intelligible to users.
In addition, these systems vary the playback speed for each large chunk of speech, which leaves room for adjustment regarding playback speed for smaller units of time.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Therefore, we propose AIx Speed, a system that adjusts audiovisual output speed while maintaining intelligibility by measuring speech intelligibility after playback speed is increased.
As shown in Fig. <a href="#S0.F1" title="Figure 1 ‣ AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, this system flexibly optimizes the playback speed in a video at the phoneme level.
By utilizing the listening ability of a neural network–based speech recognition model, which is said to rival human performance <cite class="ltx_cite ltx_citemacro_citep">(Xiong et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2016</a>)</cite>, the system simultaneously maximizes the video playback speed and the speech recognition rate after changing the playback speed.
In this paper, the validity of using speech recognizers as a proxy for evaluating human listening performance was tested through the correlation of changes in human and speech listening performance when speed is increased.
We also whether speech where the playback speed is controlled at the phoneme level, as generated by the proposed method, or speech that is played at a constant speed and a high rate is easier for humans to listen to.
The results showed that the utterances generated by the proposed method were easier for humans to understand.
Furthermore, the experimental results confirmed that the speech of non-native speakers can be transformed into speech that is easier to understand for native speakers by speeding up the speech with AIx Speed.
In summary, the proposed method not only supports the improvement of human speed–listening ability, but also improves the intelligibility of speech by generating speech with adjustable playback speeds that consider the balance between playback speed and speech intelligibility.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The contributions of this paper are summarized as follows.</p>
<blockquote id="S1.p4.2" class="ltx_quote">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Demonstrates that speech recognizers can be a substitute for human listening performance assessment.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Proposes a method to increase playback speed while maintaining speech intelligibility at the phoneme level.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Improves the intelligibility of speech for non-native speakers by optimizing speech rate at the phoneme level.</p>
</div>
</li>
</ul>
</blockquote>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2403.02938/assets/diff_from_related_work.png" id="S1.F2.g1" class="ltx_graphics ltx_img_landscape" width="299" height="168" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>. </span><span id="S1.F2.3.2" class="ltx_text" style="font-size:90%;">Differences between existing methods and ours. Existing methods speed up the playback speed of silent or unimportant parts of the audio source and slow down the playback speed of important parts of the audio source or when the user is not concentrating on them. Our method, on the other hand, adjusts the playback speed of the sound source at finer intervals according to the audibility of the speaker’s speech in the sound source.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>RELATED WORK</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">There are two main methods for adjusting video playback speed to improve the time efficiency of video viewing.
The first is to remove unnecessary portions by focusing on the content, and the second is to retain the necessary portions based on user interaction.
The former removes portions that do not have audio, which is accomplished using systems such as CinemaGazer <cite class="ltx_cite ltx_citemacro_citep">(Kurihara, <a href="#bib.bib16" title="" class="ltx_ref">2011</a>)</cite>, or portions of sports games that are not highlights of the game <cite class="ltx_cite ltx_citemacro_citep">(Kawamura et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2014</a>)</cite>.
The latter has been studied extensively, especially in the field of human–computer interaction (HCI), and adjusts the playback speed based on the user’s behavior.
For example, SmartPlayer <cite class="ltx_cite ltx_citemacro_citep">(Cheng
et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2009</a>)</cite> learns the optimal playback speed based on a user’s past viewing history.
There are also technologies that allow a user to make a rough selection in advance of what AIx Speed considers important and then fast-forward the rest of the video <cite class="ltx_cite ltx_citemacro_citep">(Kayukawa et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2018</a>; Higuchi
et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2017</a>)</cite>.
Others monitor the user’s movements and adjust the playback speed according to the user’s level of concentration <cite class="ltx_cite ltx_citemacro_citep">(Song
et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2015</a>)</cite> and comprehension <cite class="ltx_cite ltx_citemacro_citep">(Zhang
et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2020</a>; Nishida
et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite>.
These technologies have the advantage of tracking the optimal playback speed for each user, but they cannot reflect important factors such as the intelligibility of the conversation in the video and its changes in the playback speed.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">These methods do not explicitly model whether the resulting speech is intelligible to humans when the playback speed is varied.
Therefore, speeding up the playback of various types of audio and video while making the audio understandable to the user is still an open problem.
In addition, these systems vary the playback speed for large chunks of speech, which leaves room for adjustment regarding the playback speed for smaller units of time.
In this respect, the proposed system can adjust the playback speed in finer phoneme units and can also generate speech that is easier for the user to understand.
The differences between the proposed system and the existing systems are shown in Fig. <a href="#S1.F2" title="Figure 2 ‣ 1. INTRODUCTION ‣ AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S2.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F3.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:433.6pt;"><img src="/html/2403.02938/assets/cer_ml.png" id="S2.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.1.1.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F3.1.2.2" class="ltx_text" style="font-size:90%;">CER of ML models</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F3.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:433.6pt;"><img src="/html/2403.02938/assets/cer_human.png" id="S2.F3.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F3.2.2.2" class="ltx_text" style="font-size:90%;">CER of humans</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F3.3" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:433.6pt;"><img src="/html/2403.02938/assets/wer_ml.png" id="S2.F3.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.3.1.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S2.F3.3.2.2" class="ltx_text" style="font-size:90%;">WER of ML models</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F3.4" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:433.6pt;"><img src="/html/2403.02938/assets/wer_human.png" id="S2.F3.4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.4.1.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S2.F3.4.2.2" class="ltx_text" style="font-size:90%;">WER of humans</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.6.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>. </span><span id="S2.F3.7.2" class="ltx_text" style="font-size:90%;">Comparison of playback speed and listening comprehension in machine learning (ML)-based speech recognizers and humans. Transitions in human and speech recognition model transcription performance at speeds greater than 1x are correlated by a coefficient of 0.99.</span></figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Pilot Survey</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The purpose of this study is to automate the maximization of speed to the extent that speech is understandable.
To this end, we hypothesize that as the speed increases, the recognition performance of both humans and speech recognizers will decrease in the same way.
If this hypothesis is correct, we can evaluate how well a human can hear when playback speed is increased using a speech recognizer instead of a human.
Several attempts have been made to evaluate human hearing with speech recognizers in this way. For example, it has been shown that the results of human mean opinion score (MOS) listening tests correlate with the results of the speech recognition-based MOS estimation method introduced in <cite class="ltx_cite ltx_citemacro_citep">(Jiang and
Schulzrinne, <a href="#bib.bib12" title="" class="ltx_ref">2002</a>)</cite>, and in <cite class="ltx_cite ltx_citemacro_citep">(Fontan
et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2017</a>)</cite>, the understanding and comprehension scores of a listener with simulated age-related hearing loss were highly correlated speech recognition-based system.
A similar hypothesis has been used in speech learning support research to evaluate a learner’s speech ability based on speech recognition performance <cite class="ltx_cite ltx_citemacro_citep">(Tejedor-GarcÃ­a et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite>.
In other words, if a speech recognizer can recognize speech, it judges that the person speaks well.
However, the relationship between the machine learning model and the ability to understand human speech when the playback speed is varied, which is the focus of this study, has not been evaluated. Therefore, we first investigated whether this hypothesis is true.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">This study compared human listening performance and speech recognition performance for speech at 0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, and 2.0x playback speeds.
To measure human listening performance, speech data of English sentences were prepared at each playback speed, and the subjects were asked to transcribe the data.
The target English sentences were selected from LibriSpeech <cite class="ltx_cite ltx_citemacro_citep">(Panayotov et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2015</a>)</cite>, a large English speech corpus created for the development and evaluation of automatic speech recognition systems.
This corpus contains over 1,000 hours of audiobook readings and transcriptions.
The participants were 140 English speakers who lived in the United States and had graduated from a US high school.
All participants were recruited from the Amazon Mechanical Turk and were compensated for their time.
Each subject was given 15 English sentences that had been randomly sped up by 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, or 2.0x and asked to transcribe them.
The speech recognizer transcription data was collected by inputting 15 English sentences at each playback speed into a Wav2Vec2-based speech recognition model, similar to the human performance evaluation <cite class="ltx_cite ltx_citemacro_citep">(Baevski
et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Figure <a href="#S2.F3" title="Figure 3 ‣ 2. RELATED WORK ‣ AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows a graph of the change in listening performance of the human and machine learning models when the playback speed was changed.
In these figures, the horizontal axis is the playback speed, and the vertical axis is the recognition performance.
Recognition performance was evaluated using character error rate (CER) and word error rate (WER), which are commonly used in speech recognition (the lower these values are, the better).
The WER was calculated as follows:</p>
<table id="S8.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex1.m1.2" class="ltx_Math" alttext="\displaystyle\cfrac{\rm(\#~{}of~{}inserted~{}words+\#~{}of~{}replaced~{}words+\#~{}of~{}deleted~{}words)}{\rm(\#~{}of~{}correct~{}words)}," display="inline"><semantics id="S3.Ex1.m1.2a"><mrow id="S3.Ex1.m1.2.3.2" xref="S3.Ex1.m1.2.2.cmml"><mstyle displaystyle="true" id="S3.Ex1.m1.2.2" xref="S3.Ex1.m1.2.2.cmml"><mfrac id="S3.Ex1.m1.2.2a" xref="S3.Ex1.m1.2.2.cmml"><mrow id="S3.Ex1.m1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.Ex1.m1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.cmml"><mrow id="S3.Ex1.m1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.2.cmml"><mi mathsize="80%" mathvariant="normal" id="S3.Ex1.m1.1.1.1.1.1.2.2" xref="S3.Ex1.m1.1.1.1.1.1.2.2.cmml">#</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.2.1" xref="S3.Ex1.m1.1.1.1.1.1.2.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex1.m1.1.1.1.1.1.2.3" xref="S3.Ex1.m1.1.1.1.1.1.2.3.cmml">of</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.2.1a" xref="S3.Ex1.m1.1.1.1.1.1.2.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex1.m1.1.1.1.1.1.2.4" xref="S3.Ex1.m1.1.1.1.1.1.2.4.cmml">inserted</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.2.1b" xref="S3.Ex1.m1.1.1.1.1.1.2.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex1.m1.1.1.1.1.1.2.5" xref="S3.Ex1.m1.1.1.1.1.1.2.5.cmml">words</mi></mrow><mo mathsize="80%" id="S3.Ex1.m1.1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.3.cmml"><mi mathsize="80%" mathvariant="normal" id="S3.Ex1.m1.1.1.1.1.1.3.2" xref="S3.Ex1.m1.1.1.1.1.1.3.2.cmml">#</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.3.1" xref="S3.Ex1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex1.m1.1.1.1.1.1.3.3" xref="S3.Ex1.m1.1.1.1.1.1.3.3.cmml">of</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.3.1a" xref="S3.Ex1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex1.m1.1.1.1.1.1.3.4" xref="S3.Ex1.m1.1.1.1.1.1.3.4.cmml">replaced</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.3.1b" xref="S3.Ex1.m1.1.1.1.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex1.m1.1.1.1.1.1.3.5" xref="S3.Ex1.m1.1.1.1.1.1.3.5.cmml">words</mi></mrow><mo mathsize="80%" id="S3.Ex1.m1.1.1.1.1.1.1a" xref="S3.Ex1.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.Ex1.m1.1.1.1.1.1.4" xref="S3.Ex1.m1.1.1.1.1.1.4.cmml"><mi mathsize="80%" mathvariant="normal" id="S3.Ex1.m1.1.1.1.1.1.4.2" xref="S3.Ex1.m1.1.1.1.1.1.4.2.cmml">#</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.4.1" xref="S3.Ex1.m1.1.1.1.1.1.4.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex1.m1.1.1.1.1.1.4.3" xref="S3.Ex1.m1.1.1.1.1.1.4.3.cmml">of</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.4.1a" xref="S3.Ex1.m1.1.1.1.1.1.4.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex1.m1.1.1.1.1.1.4.4" xref="S3.Ex1.m1.1.1.1.1.1.4.4.cmml">deleted</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex1.m1.1.1.1.1.1.4.1b" xref="S3.Ex1.m1.1.1.1.1.1.4.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex1.m1.1.1.1.1.1.4.5" xref="S3.Ex1.m1.1.1.1.1.1.4.5.cmml">words</mi></mrow></mrow><mo maxsize="80%" minsize="80%" id="S3.Ex1.m1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S3.Ex1.m1.2.2.2.1" xref="S3.Ex1.m1.2.2.2.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.Ex1.m1.2.2.2.1.2" xref="S3.Ex1.m1.2.2.2.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.2.2.2.1.1" xref="S3.Ex1.m1.2.2.2.1.1.cmml"><mi mathsize="80%" mathvariant="normal" id="S3.Ex1.m1.2.2.2.1.1.2" xref="S3.Ex1.m1.2.2.2.1.1.2.cmml">#</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex1.m1.2.2.2.1.1.1" xref="S3.Ex1.m1.2.2.2.1.1.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex1.m1.2.2.2.1.1.3" xref="S3.Ex1.m1.2.2.2.1.1.3.cmml">of</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex1.m1.2.2.2.1.1.1a" xref="S3.Ex1.m1.2.2.2.1.1.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex1.m1.2.2.2.1.1.4" xref="S3.Ex1.m1.2.2.2.1.1.4.cmml">correct</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex1.m1.2.2.2.1.1.1b" xref="S3.Ex1.m1.2.2.2.1.1.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex1.m1.2.2.2.1.1.5" xref="S3.Ex1.m1.2.2.2.1.1.5.cmml">words</mi></mrow><mo maxsize="80%" minsize="80%" id="S3.Ex1.m1.2.2.2.1.3" xref="S3.Ex1.m1.2.2.2.1.1.cmml">)</mo></mrow></mfrac></mstyle><mo mathsize="80%" id="S3.Ex1.m1.2.3.2.1" xref="S3.Ex1.m1.2.2.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.2b"><apply id="S3.Ex1.m1.2.2.cmml" xref="S3.Ex1.m1.2.3.2"><csymbol cd="latexml" id="S3.Ex1.m1.2.2.3.cmml" xref="S3.Ex1.m1.2.3.2">continued-fraction</csymbol><apply id="S3.Ex1.m1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1"><plus id="S3.Ex1.m1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.1"></plus><apply id="S3.Ex1.m1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.2"><times id="S3.Ex1.m1.1.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.2.1"></times><ci id="S3.Ex1.m1.1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.2.2">#</ci><ci id="S3.Ex1.m1.1.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.2.3">of</ci><ci id="S3.Ex1.m1.1.1.1.1.1.2.4.cmml" xref="S3.Ex1.m1.1.1.1.1.1.2.4">inserted</ci><ci id="S3.Ex1.m1.1.1.1.1.1.2.5.cmml" xref="S3.Ex1.m1.1.1.1.1.1.2.5">words</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3"><times id="S3.Ex1.m1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3.1"></times><ci id="S3.Ex1.m1.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3.2">#</ci><ci id="S3.Ex1.m1.1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3.3">of</ci><ci id="S3.Ex1.m1.1.1.1.1.1.3.4.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3.4">replaced</ci><ci id="S3.Ex1.m1.1.1.1.1.1.3.5.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3.5">words</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.1.4.cmml" xref="S3.Ex1.m1.1.1.1.1.1.4"><times id="S3.Ex1.m1.1.1.1.1.1.4.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1.4.1"></times><ci id="S3.Ex1.m1.1.1.1.1.1.4.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.4.2">#</ci><ci id="S3.Ex1.m1.1.1.1.1.1.4.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.4.3">of</ci><ci id="S3.Ex1.m1.1.1.1.1.1.4.4.cmml" xref="S3.Ex1.m1.1.1.1.1.1.4.4">deleted</ci><ci id="S3.Ex1.m1.1.1.1.1.1.4.5.cmml" xref="S3.Ex1.m1.1.1.1.1.1.4.5">words</ci></apply></apply><apply id="S3.Ex1.m1.2.2.2.1.1.cmml" xref="S3.Ex1.m1.2.2.2.1"><times id="S3.Ex1.m1.2.2.2.1.1.1.cmml" xref="S3.Ex1.m1.2.2.2.1.1.1"></times><ci id="S3.Ex1.m1.2.2.2.1.1.2.cmml" xref="S3.Ex1.m1.2.2.2.1.1.2">#</ci><ci id="S3.Ex1.m1.2.2.2.1.1.3.cmml" xref="S3.Ex1.m1.2.2.2.1.1.3">of</ci><ci id="S3.Ex1.m1.2.2.2.1.1.4.cmml" xref="S3.Ex1.m1.2.2.2.1.1.4">correct</ci><ci id="S3.Ex1.m1.2.2.2.1.1.5.cmml" xref="S3.Ex1.m1.2.2.2.1.1.5">words</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.2c">\displaystyle\cfrac{\rm(\#~{}of~{}inserted~{}words+\#~{}of~{}replaced~{}words+\#~{}of~{}deleted~{}words)}{\rm(\#~{}of~{}correct~{}words)},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.p3.2" class="ltx_p">and CER was calculated as</p>
<table id="S8.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex2.m1.2" class="ltx_Math" alttext="\displaystyle\cfrac{\rm(\#~{}of~{}inserted~{}characters+\#~{}of~{}replaced~{}characters+\#~{}of~{}deleted~{}characters)}{\rm(\#~{}of~{}correct~{}characters)}." display="inline"><semantics id="S3.Ex2.m1.2a"><mrow id="S3.Ex2.m1.2.3.2" xref="S3.Ex2.m1.2.2.cmml"><mstyle displaystyle="true" id="S3.Ex2.m1.2.2" xref="S3.Ex2.m1.2.2.cmml"><mfrac id="S3.Ex2.m1.2.2a" xref="S3.Ex2.m1.2.2.cmml"><mrow id="S3.Ex2.m1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.Ex2.m1.1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex2.m1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.cmml"><mrow id="S3.Ex2.m1.1.1.1.1.1.2" xref="S3.Ex2.m1.1.1.1.1.1.2.cmml"><mi mathsize="80%" mathvariant="normal" id="S3.Ex2.m1.1.1.1.1.1.2.2" xref="S3.Ex2.m1.1.1.1.1.1.2.2.cmml">#</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex2.m1.1.1.1.1.1.2.1" xref="S3.Ex2.m1.1.1.1.1.1.2.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex2.m1.1.1.1.1.1.2.3" xref="S3.Ex2.m1.1.1.1.1.1.2.3.cmml">of</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex2.m1.1.1.1.1.1.2.1a" xref="S3.Ex2.m1.1.1.1.1.1.2.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex2.m1.1.1.1.1.1.2.4" xref="S3.Ex2.m1.1.1.1.1.1.2.4.cmml">inserted</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex2.m1.1.1.1.1.1.2.1b" xref="S3.Ex2.m1.1.1.1.1.1.2.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex2.m1.1.1.1.1.1.2.5" xref="S3.Ex2.m1.1.1.1.1.1.2.5.cmml">characters</mi></mrow><mo mathsize="80%" id="S3.Ex2.m1.1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.Ex2.m1.1.1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.1.1.3.cmml"><mi mathsize="80%" mathvariant="normal" id="S3.Ex2.m1.1.1.1.1.1.3.2" xref="S3.Ex2.m1.1.1.1.1.1.3.2.cmml">#</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex2.m1.1.1.1.1.1.3.1" xref="S3.Ex2.m1.1.1.1.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex2.m1.1.1.1.1.1.3.3" xref="S3.Ex2.m1.1.1.1.1.1.3.3.cmml">of</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex2.m1.1.1.1.1.1.3.1a" xref="S3.Ex2.m1.1.1.1.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex2.m1.1.1.1.1.1.3.4" xref="S3.Ex2.m1.1.1.1.1.1.3.4.cmml">replaced</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex2.m1.1.1.1.1.1.3.1b" xref="S3.Ex2.m1.1.1.1.1.1.3.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex2.m1.1.1.1.1.1.3.5" xref="S3.Ex2.m1.1.1.1.1.1.3.5.cmml">characters</mi></mrow><mo mathsize="80%" id="S3.Ex2.m1.1.1.1.1.1.1a" xref="S3.Ex2.m1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.Ex2.m1.1.1.1.1.1.4" xref="S3.Ex2.m1.1.1.1.1.1.4.cmml"><mi mathsize="80%" mathvariant="normal" id="S3.Ex2.m1.1.1.1.1.1.4.2" xref="S3.Ex2.m1.1.1.1.1.1.4.2.cmml">#</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex2.m1.1.1.1.1.1.4.1" xref="S3.Ex2.m1.1.1.1.1.1.4.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex2.m1.1.1.1.1.1.4.3" xref="S3.Ex2.m1.1.1.1.1.1.4.3.cmml">of</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex2.m1.1.1.1.1.1.4.1a" xref="S3.Ex2.m1.1.1.1.1.1.4.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex2.m1.1.1.1.1.1.4.4" xref="S3.Ex2.m1.1.1.1.1.1.4.4.cmml">deleted</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex2.m1.1.1.1.1.1.4.1b" xref="S3.Ex2.m1.1.1.1.1.1.4.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex2.m1.1.1.1.1.1.4.5" xref="S3.Ex2.m1.1.1.1.1.1.4.5.cmml">characters</mi></mrow></mrow><mo maxsize="80%" minsize="80%" id="S3.Ex2.m1.1.1.1.1.3" xref="S3.Ex2.m1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S3.Ex2.m1.2.2.2.1" xref="S3.Ex2.m1.2.2.2.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S3.Ex2.m1.2.2.2.1.2" xref="S3.Ex2.m1.2.2.2.1.1.cmml">(</mo><mrow id="S3.Ex2.m1.2.2.2.1.1" xref="S3.Ex2.m1.2.2.2.1.1.cmml"><mi mathsize="80%" mathvariant="normal" id="S3.Ex2.m1.2.2.2.1.1.2" xref="S3.Ex2.m1.2.2.2.1.1.2.cmml">#</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex2.m1.2.2.2.1.1.1" xref="S3.Ex2.m1.2.2.2.1.1.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex2.m1.2.2.2.1.1.3" xref="S3.Ex2.m1.2.2.2.1.1.3.cmml">of</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex2.m1.2.2.2.1.1.1a" xref="S3.Ex2.m1.2.2.2.1.1.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex2.m1.2.2.2.1.1.4" xref="S3.Ex2.m1.2.2.2.1.1.4.cmml">correct</mi><mo lspace="0.270em" rspace="0em" id="S3.Ex2.m1.2.2.2.1.1.1b" xref="S3.Ex2.m1.2.2.2.1.1.1.cmml">​</mo><mi mathsize="80%" id="S3.Ex2.m1.2.2.2.1.1.5" xref="S3.Ex2.m1.2.2.2.1.1.5.cmml">characters</mi></mrow><mo maxsize="80%" minsize="80%" id="S3.Ex2.m1.2.2.2.1.3" xref="S3.Ex2.m1.2.2.2.1.1.cmml">)</mo></mrow></mfrac></mstyle><mo lspace="0em" mathsize="80%" id="S3.Ex2.m1.2.3.2.1" xref="S3.Ex2.m1.2.2.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.2b"><apply id="S3.Ex2.m1.2.2.cmml" xref="S3.Ex2.m1.2.3.2"><csymbol cd="latexml" id="S3.Ex2.m1.2.2.3.cmml" xref="S3.Ex2.m1.2.3.2">continued-fraction</csymbol><apply id="S3.Ex2.m1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1"><plus id="S3.Ex2.m1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1"></plus><apply id="S3.Ex2.m1.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.2"><times id="S3.Ex2.m1.1.1.1.1.1.2.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.2.1"></times><ci id="S3.Ex2.m1.1.1.1.1.1.2.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.2.2">#</ci><ci id="S3.Ex2.m1.1.1.1.1.1.2.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.2.3">of</ci><ci id="S3.Ex2.m1.1.1.1.1.1.2.4.cmml" xref="S3.Ex2.m1.1.1.1.1.1.2.4">inserted</ci><ci id="S3.Ex2.m1.1.1.1.1.1.2.5.cmml" xref="S3.Ex2.m1.1.1.1.1.1.2.5">characters</ci></apply><apply id="S3.Ex2.m1.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.3"><times id="S3.Ex2.m1.1.1.1.1.1.3.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.3.1"></times><ci id="S3.Ex2.m1.1.1.1.1.1.3.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.3.2">#</ci><ci id="S3.Ex2.m1.1.1.1.1.1.3.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.3.3">of</ci><ci id="S3.Ex2.m1.1.1.1.1.1.3.4.cmml" xref="S3.Ex2.m1.1.1.1.1.1.3.4">replaced</ci><ci id="S3.Ex2.m1.1.1.1.1.1.3.5.cmml" xref="S3.Ex2.m1.1.1.1.1.1.3.5">characters</ci></apply><apply id="S3.Ex2.m1.1.1.1.1.1.4.cmml" xref="S3.Ex2.m1.1.1.1.1.1.4"><times id="S3.Ex2.m1.1.1.1.1.1.4.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.4.1"></times><ci id="S3.Ex2.m1.1.1.1.1.1.4.2.cmml" xref="S3.Ex2.m1.1.1.1.1.1.4.2">#</ci><ci id="S3.Ex2.m1.1.1.1.1.1.4.3.cmml" xref="S3.Ex2.m1.1.1.1.1.1.4.3">of</ci><ci id="S3.Ex2.m1.1.1.1.1.1.4.4.cmml" xref="S3.Ex2.m1.1.1.1.1.1.4.4">deleted</ci><ci id="S3.Ex2.m1.1.1.1.1.1.4.5.cmml" xref="S3.Ex2.m1.1.1.1.1.1.4.5">characters</ci></apply></apply><apply id="S3.Ex2.m1.2.2.2.1.1.cmml" xref="S3.Ex2.m1.2.2.2.1"><times id="S3.Ex2.m1.2.2.2.1.1.1.cmml" xref="S3.Ex2.m1.2.2.2.1.1.1"></times><ci id="S3.Ex2.m1.2.2.2.1.1.2.cmml" xref="S3.Ex2.m1.2.2.2.1.1.2">#</ci><ci id="S3.Ex2.m1.2.2.2.1.1.3.cmml" xref="S3.Ex2.m1.2.2.2.1.1.3">of</ci><ci id="S3.Ex2.m1.2.2.2.1.1.4.cmml" xref="S3.Ex2.m1.2.2.2.1.1.4">correct</ci><ci id="S3.Ex2.m1.2.2.2.1.1.5.cmml" xref="S3.Ex2.m1.2.2.2.1.1.5">characters</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.2c">\displaystyle\cfrac{\rm(\#~{}of~{}inserted~{}characters+\#~{}of~{}replaced~{}characters+\#~{}of~{}deleted~{}characters)}{\rm(\#~{}of~{}correct~{}characters)}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.p3.3" class="ltx_p">For both the human and machine learning models, listening performance decreased as playback speed increased from 1.0x.
For playback speeds greater than 1.0x, the correlation coefficient between the change in listening performance for the human and machine learning models was 0.9977.
On the other hand, when the playback speed was slowed down from 1.0x, the machine learning model showed a decrease in recognition performance, but the decline barely observable for humans.
In particular, while the performance of WER decreased slowly, the performance of CER showed almost no decrease.
This indicates that human recognition performance on a character-by-character basis does not drop nearly as much when listening to slowed speech.
Thus, when the playback speed increased, the recognition performance of the human and machine learning models decreased similarly, but they exhibited different behavior when the playback speed was decreased.
However, since this study focuses on increasing the playback speed of speech, the difference in behavior when the playback speed is slower than 1.0x has no effect.
Therefore, by taking advantage of the fact that listening performance decreases as playback speed is increased for humans and speech recognition models alike, we replaced human listening performance with speech recognition performance to develop the desired system.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>AIx Speed</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">AIx Speed increases the speed as much as possible, as long as the user can understand it.
This system allows users to watch videos in a time-efficient manner without having to adjust the playback speed for each video.
In addition, the system can automatically improve intelligibility by adjusting the speech speed to accommodate non-native speakers who are not proficient in the target language.
The working process of AIx Speed is illustrated in Fig. <a href="#S4.F4" title="Figure 4 ‣ 4. AIx Speed ‣ AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
The system first extracts the human voice from the target video.
Next, it splits this voice into specified equal intervals.
Next, using each segmented voice as input, the system calculates the optimal playback speed for each segment voice, taking into account the characteristics of the voice as a whole.
Finally, the system changes each voice to the specified playback speed and combines them into a single voice.
At the same time, the combined voice is recognized by speech recognition to confirm that the resulting single voice is understandable.
This system consists of two mechanisms, as shown in Fig. <a href="#S4.F5" title="Figure 5 ‣ 4. AIx Speed ‣ AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
One is a playback speed adjuster (left), and the other is a speech recognizer (right).
The former is used to maximize the playback speed of the input speech, while the latter is used to evaluate how understandable the input speech is.
By training these two models simultaneously, it is possible to generate speech that plays back as fast as possible within the comprehension range.
The following subsections describe these two key features.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2403.02938/assets/aix_speed.png" id="S4.F4.g1" class="ltx_graphics ltx_figure_panel ltx_img_square" width="299" height="353" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>. </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">The work process of AIx Speed: \scriptsize1⃝ Extract human voices from the target video. \scriptsize2⃝ Divide the voices into specified equal intervals. \scriptsize3⃝ Calculate the optimal playback speed for each divided voice. \scriptsize4⃝ Change each voice to the fixed playback speed and synthesize it into one voice. \scriptsize5⃝ Perform speech recognition on the synthesized voice to confirm that the resulting voice is understandable.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F4.4" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2403.02938/assets/architecture_paper.png" id="S4.F5.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="568" height="361" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>. </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">AIx Speed architecture: Our system simultaneously optimizes the playback speed regulator (left) and the speech recognizer (right). By doing so, we can maximize the playback speed to the extent that the model can recognize.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F5.4" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2403.02938/assets/applications_paper.png" id="S4.F6.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="604" height="262" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>. </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Application example. This graph shows how the playback speed changed when AIx Speed was applied to two videos on YouTube. As you can see, the playback speed changes flexibly according to the speaker’s speech.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F6.4" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Playback speed adjuster</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In this study, we use Wav2Vec2, a self-supervised neural network designed for speech signal processing systems, to optimize the playback speed.
The playback speed controller is divided into a feature extractor layer and a linear layer, as shown in Fig. <a href="#S4.F5" title="Figure 5 ‣ 4. AIx Speed ‣ AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (left).
They are trained by pre-training through self-supervised representation learning on unlabeled speech data and regression learning, which outputs the playback speed based on the features after the representation learning.
The pre-training method for the feature extractor layer is similar to masked language modeling, as exemplified by bidirectional encoder representations from transformers (BERT) <cite class="ltx_cite ltx_citemacro_citep">(Devlin
et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite> in natural language processing, where a portion of the input is masked and the corresponding utterance features are estimated from the remaining input.
In this way, the model can learn good-quality features of the target language to which the rate of utterance should be adapted.
Typically, these self-supervised learners are used to tackle tasks such as speech recognition and speaker identification by pre-training and then fine-tuning with a small amount of label data.
For example, in speech recognition, we have added a projection layer and a connectionist temporal classification (CTC) layer <cite class="ltx_cite ltx_citemacro_citep">(Graves et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2006</a>)</cite> to the output of self-supervised learners, such as Wav2Vec2 and HuBERT <cite class="ltx_cite ltx_citemacro_citep">(Hsu et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite>, to enable transcription from speech waveforms.
Similar to these methods, we pre-train on unlabeled speech data and then connect and train a linear layer that outputs rates.
In general speech processing tasks, such as speech recognition <cite class="ltx_cite ltx_citemacro_citep">(Malik
et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite> and speaker identification <cite class="ltx_cite ltx_citemacro_citep">(Bai and Zhang, <a href="#bib.bib4" title="" class="ltx_ref">2021</a>; Kabir
et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite>, the difference from the class label is minimized as an error function.
On the other hand, the goal of the playback speed adjuster is to maximize the speed.
Therefore, we designed the following function whose value decreases as the speed increases.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<table id="S8.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.Ex3.m1.2" class="ltx_Math" alttext="\displaystyle{\rm Loss_{speed}}=\left(\frac{1}{10}\right)^{\frac{1}{S}\sum_{s=1}^{S}r_{s}}," display="inline"><semantics id="S4.Ex3.m1.2a"><mrow id="S4.Ex3.m1.2.2.1" xref="S4.Ex3.m1.2.2.1.1.cmml"><mrow id="S4.Ex3.m1.2.2.1.1" xref="S4.Ex3.m1.2.2.1.1.cmml"><msub id="S4.Ex3.m1.2.2.1.1.2" xref="S4.Ex3.m1.2.2.1.1.2.cmml"><mi id="S4.Ex3.m1.2.2.1.1.2.2" xref="S4.Ex3.m1.2.2.1.1.2.2.cmml">Loss</mi><mi id="S4.Ex3.m1.2.2.1.1.2.3" xref="S4.Ex3.m1.2.2.1.1.2.3.cmml">speed</mi></msub><mo id="S4.Ex3.m1.2.2.1.1.1" xref="S4.Ex3.m1.2.2.1.1.1.cmml">=</mo><msup id="S4.Ex3.m1.2.2.1.1.3" xref="S4.Ex3.m1.2.2.1.1.3.cmml"><mrow id="S4.Ex3.m1.2.2.1.1.3.2.2" xref="S4.Ex3.m1.1.1.cmml"><mo id="S4.Ex3.m1.2.2.1.1.3.2.2.1" xref="S4.Ex3.m1.1.1.cmml">(</mo><mstyle displaystyle="true" id="S4.Ex3.m1.1.1" xref="S4.Ex3.m1.1.1.cmml"><mfrac id="S4.Ex3.m1.1.1a" xref="S4.Ex3.m1.1.1.cmml"><mn id="S4.Ex3.m1.1.1.2" xref="S4.Ex3.m1.1.1.2.cmml">1</mn><mn id="S4.Ex3.m1.1.1.3" xref="S4.Ex3.m1.1.1.3.cmml">10</mn></mfrac></mstyle><mo id="S4.Ex3.m1.2.2.1.1.3.2.2.2" xref="S4.Ex3.m1.1.1.cmml">)</mo></mrow><mrow id="S4.Ex3.m1.2.2.1.1.3.3" xref="S4.Ex3.m1.2.2.1.1.3.3.cmml"><mfrac id="S4.Ex3.m1.2.2.1.1.3.3.2" xref="S4.Ex3.m1.2.2.1.1.3.3.2.cmml"><mn id="S4.Ex3.m1.2.2.1.1.3.3.2.2" xref="S4.Ex3.m1.2.2.1.1.3.3.2.2.cmml">1</mn><mi id="S4.Ex3.m1.2.2.1.1.3.3.2.3" xref="S4.Ex3.m1.2.2.1.1.3.3.2.3.cmml">S</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.Ex3.m1.2.2.1.1.3.3.1" xref="S4.Ex3.m1.2.2.1.1.3.3.1.cmml">​</mo><mrow id="S4.Ex3.m1.2.2.1.1.3.3.3" xref="S4.Ex3.m1.2.2.1.1.3.3.3.cmml"><mstyle displaystyle="false" id="S4.Ex3.m1.2.2.1.1.3.3.3.1" xref="S4.Ex3.m1.2.2.1.1.3.3.3.1.cmml"><msubsup id="S4.Ex3.m1.2.2.1.1.3.3.3.1a" xref="S4.Ex3.m1.2.2.1.1.3.3.3.1.cmml"><mo id="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.2" xref="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.2.cmml">∑</mo><mrow id="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.3" xref="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.3.cmml"><mi id="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.3.2" xref="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.3.2.cmml">s</mi><mo id="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.3.1" xref="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.3.1.cmml">=</mo><mn id="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.3.3" xref="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S4.Ex3.m1.2.2.1.1.3.3.3.1.3" xref="S4.Ex3.m1.2.2.1.1.3.3.3.1.3.cmml">S</mi></msubsup></mstyle><msub id="S4.Ex3.m1.2.2.1.1.3.3.3.2" xref="S4.Ex3.m1.2.2.1.1.3.3.3.2.cmml"><mi id="S4.Ex3.m1.2.2.1.1.3.3.3.2.2" xref="S4.Ex3.m1.2.2.1.1.3.3.3.2.2.cmml">r</mi><mi id="S4.Ex3.m1.2.2.1.1.3.3.3.2.3" xref="S4.Ex3.m1.2.2.1.1.3.3.3.2.3.cmml">s</mi></msub></mrow></mrow></msup></mrow><mo id="S4.Ex3.m1.2.2.1.2" xref="S4.Ex3.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex3.m1.2b"><apply id="S4.Ex3.m1.2.2.1.1.cmml" xref="S4.Ex3.m1.2.2.1"><eq id="S4.Ex3.m1.2.2.1.1.1.cmml" xref="S4.Ex3.m1.2.2.1.1.1"></eq><apply id="S4.Ex3.m1.2.2.1.1.2.cmml" xref="S4.Ex3.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S4.Ex3.m1.2.2.1.1.2.1.cmml" xref="S4.Ex3.m1.2.2.1.1.2">subscript</csymbol><ci id="S4.Ex3.m1.2.2.1.1.2.2.cmml" xref="S4.Ex3.m1.2.2.1.1.2.2">Loss</ci><ci id="S4.Ex3.m1.2.2.1.1.2.3.cmml" xref="S4.Ex3.m1.2.2.1.1.2.3">speed</ci></apply><apply id="S4.Ex3.m1.2.2.1.1.3.cmml" xref="S4.Ex3.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S4.Ex3.m1.2.2.1.1.3.1.cmml" xref="S4.Ex3.m1.2.2.1.1.3">superscript</csymbol><apply id="S4.Ex3.m1.1.1.cmml" xref="S4.Ex3.m1.2.2.1.1.3.2.2"><divide id="S4.Ex3.m1.1.1.1.cmml" xref="S4.Ex3.m1.2.2.1.1.3.2.2"></divide><cn type="integer" id="S4.Ex3.m1.1.1.2.cmml" xref="S4.Ex3.m1.1.1.2">1</cn><cn type="integer" id="S4.Ex3.m1.1.1.3.cmml" xref="S4.Ex3.m1.1.1.3">10</cn></apply><apply id="S4.Ex3.m1.2.2.1.1.3.3.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3"><times id="S4.Ex3.m1.2.2.1.1.3.3.1.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.1"></times><apply id="S4.Ex3.m1.2.2.1.1.3.3.2.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.2"><divide id="S4.Ex3.m1.2.2.1.1.3.3.2.1.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.2"></divide><cn type="integer" id="S4.Ex3.m1.2.2.1.1.3.3.2.2.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.2.2">1</cn><ci id="S4.Ex3.m1.2.2.1.1.3.3.2.3.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.2.3">𝑆</ci></apply><apply id="S4.Ex3.m1.2.2.1.1.3.3.3.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.3"><apply id="S4.Ex3.m1.2.2.1.1.3.3.3.1.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.3.1"><csymbol cd="ambiguous" id="S4.Ex3.m1.2.2.1.1.3.3.3.1.1.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.3.1">superscript</csymbol><apply id="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.3.1"><csymbol cd="ambiguous" id="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.1.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.3.1">subscript</csymbol><sum id="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.2.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.2"></sum><apply id="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.3.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.3"><eq id="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.3.1.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.3.1"></eq><ci id="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.3.2.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.3.2">𝑠</ci><cn type="integer" id="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.3.3.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.3.1.2.3.3">1</cn></apply></apply><ci id="S4.Ex3.m1.2.2.1.1.3.3.3.1.3.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.3.1.3">𝑆</ci></apply><apply id="S4.Ex3.m1.2.2.1.1.3.3.3.2.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.3.2"><csymbol cd="ambiguous" id="S4.Ex3.m1.2.2.1.1.3.3.3.2.1.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.3.2">subscript</csymbol><ci id="S4.Ex3.m1.2.2.1.1.3.3.3.2.2.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.3.2.2">𝑟</ci><ci id="S4.Ex3.m1.2.2.1.1.3.3.3.2.3.cmml" xref="S4.Ex3.m1.2.2.1.1.3.3.3.2.3">𝑠</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex3.m1.2c">\displaystyle{\rm Loss_{speed}}=\left(\frac{1}{10}\right)^{\frac{1}{S}\sum_{s=1}^{S}r_{s}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS1.p2.2" class="ltx_p">where <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="r_{t}" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><msub id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">r</mi><mi id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">𝑟</ci><ci id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">r_{t}</annotation></semantics></math> is the playback speed for each segment obtained using Wav2Vec2 and a linear layer with audio <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mi id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><ci id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">X</annotation></semantics></math> as input as follows:</p>
<table id="S8.EGx4" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.Ex4.m1.3" class="ltx_Math" alttext="\displaystyle C=c_{\left[1:T\right]}={\rm Wav2Vec2}\left(X\right)," display="inline"><semantics id="S4.Ex4.m1.3a"><mrow id="S4.Ex4.m1.3.3.1" xref="S4.Ex4.m1.3.3.1.1.cmml"><mrow id="S4.Ex4.m1.3.3.1.1" xref="S4.Ex4.m1.3.3.1.1.cmml"><mi id="S4.Ex4.m1.3.3.1.1.2" xref="S4.Ex4.m1.3.3.1.1.2.cmml">C</mi><mo id="S4.Ex4.m1.3.3.1.1.3" xref="S4.Ex4.m1.3.3.1.1.3.cmml">=</mo><msub id="S4.Ex4.m1.3.3.1.1.4" xref="S4.Ex4.m1.3.3.1.1.4.cmml"><mi id="S4.Ex4.m1.3.3.1.1.4.2" xref="S4.Ex4.m1.3.3.1.1.4.2.cmml">c</mi><mrow id="S4.Ex4.m1.1.1.1.1" xref="S4.Ex4.m1.1.1.1.2.cmml"><mo id="S4.Ex4.m1.1.1.1.1.2" xref="S4.Ex4.m1.1.1.1.2.1.cmml">[</mo><mrow id="S4.Ex4.m1.1.1.1.1.1" xref="S4.Ex4.m1.1.1.1.1.1.cmml"><mn id="S4.Ex4.m1.1.1.1.1.1.2" xref="S4.Ex4.m1.1.1.1.1.1.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S4.Ex4.m1.1.1.1.1.1.1" xref="S4.Ex4.m1.1.1.1.1.1.1.cmml">:</mo><mi id="S4.Ex4.m1.1.1.1.1.1.3" xref="S4.Ex4.m1.1.1.1.1.1.3.cmml">T</mi></mrow><mo id="S4.Ex4.m1.1.1.1.1.3" xref="S4.Ex4.m1.1.1.1.2.1.cmml">]</mo></mrow></msub><mo id="S4.Ex4.m1.3.3.1.1.5" xref="S4.Ex4.m1.3.3.1.1.5.cmml">=</mo><mrow id="S4.Ex4.m1.3.3.1.1.6" xref="S4.Ex4.m1.3.3.1.1.6.cmml"><mi id="S4.Ex4.m1.3.3.1.1.6.2" xref="S4.Ex4.m1.3.3.1.1.6.2.cmml">Wav2Vec2</mi><mo lspace="0em" rspace="0em" id="S4.Ex4.m1.3.3.1.1.6.1" xref="S4.Ex4.m1.3.3.1.1.6.1.cmml">​</mo><mrow id="S4.Ex4.m1.3.3.1.1.6.3.2" xref="S4.Ex4.m1.3.3.1.1.6.cmml"><mo id="S4.Ex4.m1.3.3.1.1.6.3.2.1" xref="S4.Ex4.m1.3.3.1.1.6.cmml">(</mo><mi id="S4.Ex4.m1.2.2" xref="S4.Ex4.m1.2.2.cmml">X</mi><mo id="S4.Ex4.m1.3.3.1.1.6.3.2.2" xref="S4.Ex4.m1.3.3.1.1.6.cmml">)</mo></mrow></mrow></mrow><mo id="S4.Ex4.m1.3.3.1.2" xref="S4.Ex4.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex4.m1.3b"><apply id="S4.Ex4.m1.3.3.1.1.cmml" xref="S4.Ex4.m1.3.3.1"><and id="S4.Ex4.m1.3.3.1.1a.cmml" xref="S4.Ex4.m1.3.3.1"></and><apply id="S4.Ex4.m1.3.3.1.1b.cmml" xref="S4.Ex4.m1.3.3.1"><eq id="S4.Ex4.m1.3.3.1.1.3.cmml" xref="S4.Ex4.m1.3.3.1.1.3"></eq><ci id="S4.Ex4.m1.3.3.1.1.2.cmml" xref="S4.Ex4.m1.3.3.1.1.2">𝐶</ci><apply id="S4.Ex4.m1.3.3.1.1.4.cmml" xref="S4.Ex4.m1.3.3.1.1.4"><csymbol cd="ambiguous" id="S4.Ex4.m1.3.3.1.1.4.1.cmml" xref="S4.Ex4.m1.3.3.1.1.4">subscript</csymbol><ci id="S4.Ex4.m1.3.3.1.1.4.2.cmml" xref="S4.Ex4.m1.3.3.1.1.4.2">𝑐</ci><apply id="S4.Ex4.m1.1.1.1.2.cmml" xref="S4.Ex4.m1.1.1.1.1"><csymbol cd="latexml" id="S4.Ex4.m1.1.1.1.2.1.cmml" xref="S4.Ex4.m1.1.1.1.1.2">delimited-[]</csymbol><apply id="S4.Ex4.m1.1.1.1.1.1.cmml" xref="S4.Ex4.m1.1.1.1.1.1"><ci id="S4.Ex4.m1.1.1.1.1.1.1.cmml" xref="S4.Ex4.m1.1.1.1.1.1.1">:</ci><cn type="integer" id="S4.Ex4.m1.1.1.1.1.1.2.cmml" xref="S4.Ex4.m1.1.1.1.1.1.2">1</cn><ci id="S4.Ex4.m1.1.1.1.1.1.3.cmml" xref="S4.Ex4.m1.1.1.1.1.1.3">𝑇</ci></apply></apply></apply></apply><apply id="S4.Ex4.m1.3.3.1.1c.cmml" xref="S4.Ex4.m1.3.3.1"><eq id="S4.Ex4.m1.3.3.1.1.5.cmml" xref="S4.Ex4.m1.3.3.1.1.5"></eq><share href="#S4.Ex4.m1.3.3.1.1.4.cmml" id="S4.Ex4.m1.3.3.1.1d.cmml" xref="S4.Ex4.m1.3.3.1"></share><apply id="S4.Ex4.m1.3.3.1.1.6.cmml" xref="S4.Ex4.m1.3.3.1.1.6"><times id="S4.Ex4.m1.3.3.1.1.6.1.cmml" xref="S4.Ex4.m1.3.3.1.1.6.1"></times><ci id="S4.Ex4.m1.3.3.1.1.6.2.cmml" xref="S4.Ex4.m1.3.3.1.1.6.2">Wav2Vec2</ci><ci id="S4.Ex4.m1.2.2.cmml" xref="S4.Ex4.m1.2.2">𝑋</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex4.m1.3c">\displaystyle C=c_{\left[1:T\right]}={\rm Wav2Vec2}\left(X\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S4.Ex5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.Ex5.m1.3" class="ltx_Math" alttext="\displaystyle R=r_{\left[1:S\right]}={\rm Linear}\left(C\right)." display="inline"><semantics id="S4.Ex5.m1.3a"><mrow id="S4.Ex5.m1.3.3.1" xref="S4.Ex5.m1.3.3.1.1.cmml"><mrow id="S4.Ex5.m1.3.3.1.1" xref="S4.Ex5.m1.3.3.1.1.cmml"><mi id="S4.Ex5.m1.3.3.1.1.2" xref="S4.Ex5.m1.3.3.1.1.2.cmml">R</mi><mo id="S4.Ex5.m1.3.3.1.1.3" xref="S4.Ex5.m1.3.3.1.1.3.cmml">=</mo><msub id="S4.Ex5.m1.3.3.1.1.4" xref="S4.Ex5.m1.3.3.1.1.4.cmml"><mi id="S4.Ex5.m1.3.3.1.1.4.2" xref="S4.Ex5.m1.3.3.1.1.4.2.cmml">r</mi><mrow id="S4.Ex5.m1.1.1.1.1" xref="S4.Ex5.m1.1.1.1.2.cmml"><mo id="S4.Ex5.m1.1.1.1.1.2" xref="S4.Ex5.m1.1.1.1.2.1.cmml">[</mo><mrow id="S4.Ex5.m1.1.1.1.1.1" xref="S4.Ex5.m1.1.1.1.1.1.cmml"><mn id="S4.Ex5.m1.1.1.1.1.1.2" xref="S4.Ex5.m1.1.1.1.1.1.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S4.Ex5.m1.1.1.1.1.1.1" xref="S4.Ex5.m1.1.1.1.1.1.1.cmml">:</mo><mi id="S4.Ex5.m1.1.1.1.1.1.3" xref="S4.Ex5.m1.1.1.1.1.1.3.cmml">S</mi></mrow><mo id="S4.Ex5.m1.1.1.1.1.3" xref="S4.Ex5.m1.1.1.1.2.1.cmml">]</mo></mrow></msub><mo id="S4.Ex5.m1.3.3.1.1.5" xref="S4.Ex5.m1.3.3.1.1.5.cmml">=</mo><mrow id="S4.Ex5.m1.3.3.1.1.6" xref="S4.Ex5.m1.3.3.1.1.6.cmml"><mi id="S4.Ex5.m1.3.3.1.1.6.2" xref="S4.Ex5.m1.3.3.1.1.6.2.cmml">Linear</mi><mo lspace="0em" rspace="0em" id="S4.Ex5.m1.3.3.1.1.6.1" xref="S4.Ex5.m1.3.3.1.1.6.1.cmml">​</mo><mrow id="S4.Ex5.m1.3.3.1.1.6.3.2" xref="S4.Ex5.m1.3.3.1.1.6.cmml"><mo id="S4.Ex5.m1.3.3.1.1.6.3.2.1" xref="S4.Ex5.m1.3.3.1.1.6.cmml">(</mo><mi id="S4.Ex5.m1.2.2" xref="S4.Ex5.m1.2.2.cmml">C</mi><mo id="S4.Ex5.m1.3.3.1.1.6.3.2.2" xref="S4.Ex5.m1.3.3.1.1.6.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S4.Ex5.m1.3.3.1.2" xref="S4.Ex5.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex5.m1.3b"><apply id="S4.Ex5.m1.3.3.1.1.cmml" xref="S4.Ex5.m1.3.3.1"><and id="S4.Ex5.m1.3.3.1.1a.cmml" xref="S4.Ex5.m1.3.3.1"></and><apply id="S4.Ex5.m1.3.3.1.1b.cmml" xref="S4.Ex5.m1.3.3.1"><eq id="S4.Ex5.m1.3.3.1.1.3.cmml" xref="S4.Ex5.m1.3.3.1.1.3"></eq><ci id="S4.Ex5.m1.3.3.1.1.2.cmml" xref="S4.Ex5.m1.3.3.1.1.2">𝑅</ci><apply id="S4.Ex5.m1.3.3.1.1.4.cmml" xref="S4.Ex5.m1.3.3.1.1.4"><csymbol cd="ambiguous" id="S4.Ex5.m1.3.3.1.1.4.1.cmml" xref="S4.Ex5.m1.3.3.1.1.4">subscript</csymbol><ci id="S4.Ex5.m1.3.3.1.1.4.2.cmml" xref="S4.Ex5.m1.3.3.1.1.4.2">𝑟</ci><apply id="S4.Ex5.m1.1.1.1.2.cmml" xref="S4.Ex5.m1.1.1.1.1"><csymbol cd="latexml" id="S4.Ex5.m1.1.1.1.2.1.cmml" xref="S4.Ex5.m1.1.1.1.1.2">delimited-[]</csymbol><apply id="S4.Ex5.m1.1.1.1.1.1.cmml" xref="S4.Ex5.m1.1.1.1.1.1"><ci id="S4.Ex5.m1.1.1.1.1.1.1.cmml" xref="S4.Ex5.m1.1.1.1.1.1.1">:</ci><cn type="integer" id="S4.Ex5.m1.1.1.1.1.1.2.cmml" xref="S4.Ex5.m1.1.1.1.1.1.2">1</cn><ci id="S4.Ex5.m1.1.1.1.1.1.3.cmml" xref="S4.Ex5.m1.1.1.1.1.1.3">𝑆</ci></apply></apply></apply></apply><apply id="S4.Ex5.m1.3.3.1.1c.cmml" xref="S4.Ex5.m1.3.3.1"><eq id="S4.Ex5.m1.3.3.1.1.5.cmml" xref="S4.Ex5.m1.3.3.1.1.5"></eq><share href="#S4.Ex5.m1.3.3.1.1.4.cmml" id="S4.Ex5.m1.3.3.1.1d.cmml" xref="S4.Ex5.m1.3.3.1"></share><apply id="S4.Ex5.m1.3.3.1.1.6.cmml" xref="S4.Ex5.m1.3.3.1.1.6"><times id="S4.Ex5.m1.3.3.1.1.6.1.cmml" xref="S4.Ex5.m1.3.3.1.1.6.1"></times><ci id="S4.Ex5.m1.3.3.1.1.6.2.cmml" xref="S4.Ex5.m1.3.3.1.1.6.2">Linear</ci><ci id="S4.Ex5.m1.2.2.cmml" xref="S4.Ex5.m1.2.2">𝐶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex5.m1.3c">\displaystyle R=r_{\left[1:S\right]}={\rm Linear}\left(C\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Speech recognizer</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p">The speech recognizer transcribes the speech converted to the playback speed obtained by the playback speed adjuster (Fig. <a href="#S4.F5" title="Figure 5 ‣ 4. AIx Speed ‣ AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> (right)).
When speech parts with different playback speeds are combined, noise is generated in the speech data and the sound quality is degraded.
We use voice separation technology <cite class="ltx_cite ltx_citemacro_citep">(McFee et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2015</a>)</cite> to extract only the speaker’s voice and reduce the effect of noise.
The resulting speech is then fed into a speech recognizer for speech recognition. The speech recognition process consists of the extraction of acoustic features from the speech waveform, estimation of the classes of acoustic features for each frame, and generation of hypotheses from the sequence of class probabilities.
Since the speech recognizer can partially share the neural network with the playback speed adjuster, the overall network size can be reduced.
As shown in Fig. <a href="#S4.F5" title="Figure 5 ‣ 4. AIx Speed ‣ AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the dotted speech feature extractor is shared.
The speech features obtained by this mechanism are used as input to generate text in the projection layer.
In this process, the speech recognizer is trained to minimize CTC loss, as in normal speech recognition.
In summary, the entire model is trained to minimize the following error function, which is a combination of this error function and the error function of the playback speed adjuster.</p>
<table id="S8.EGx5" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.Ex6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.Ex6.m1.1" class="ltx_Math" alttext="\displaystyle{\rm Loss}={\rm Loss_{speed}}+\lambda{\rm Loss_{ctc}}." display="inline"><semantics id="S4.Ex6.m1.1a"><mrow id="S4.Ex6.m1.1.1.1" xref="S4.Ex6.m1.1.1.1.1.cmml"><mrow id="S4.Ex6.m1.1.1.1.1" xref="S4.Ex6.m1.1.1.1.1.cmml"><mi id="S4.Ex6.m1.1.1.1.1.2" xref="S4.Ex6.m1.1.1.1.1.2.cmml">Loss</mi><mo id="S4.Ex6.m1.1.1.1.1.1" xref="S4.Ex6.m1.1.1.1.1.1.cmml">=</mo><mrow id="S4.Ex6.m1.1.1.1.1.3" xref="S4.Ex6.m1.1.1.1.1.3.cmml"><msub id="S4.Ex6.m1.1.1.1.1.3.2" xref="S4.Ex6.m1.1.1.1.1.3.2.cmml"><mi id="S4.Ex6.m1.1.1.1.1.3.2.2" xref="S4.Ex6.m1.1.1.1.1.3.2.2.cmml">Loss</mi><mi id="S4.Ex6.m1.1.1.1.1.3.2.3" xref="S4.Ex6.m1.1.1.1.1.3.2.3.cmml">speed</mi></msub><mo id="S4.Ex6.m1.1.1.1.1.3.1" xref="S4.Ex6.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S4.Ex6.m1.1.1.1.1.3.3" xref="S4.Ex6.m1.1.1.1.1.3.3.cmml"><mi id="S4.Ex6.m1.1.1.1.1.3.3.2" xref="S4.Ex6.m1.1.1.1.1.3.3.2.cmml">λ</mi><mo lspace="0em" rspace="0em" id="S4.Ex6.m1.1.1.1.1.3.3.1" xref="S4.Ex6.m1.1.1.1.1.3.3.1.cmml">​</mo><msub id="S4.Ex6.m1.1.1.1.1.3.3.3" xref="S4.Ex6.m1.1.1.1.1.3.3.3.cmml"><mi id="S4.Ex6.m1.1.1.1.1.3.3.3.2" xref="S4.Ex6.m1.1.1.1.1.3.3.3.2.cmml">Loss</mi><mi id="S4.Ex6.m1.1.1.1.1.3.3.3.3" xref="S4.Ex6.m1.1.1.1.1.3.3.3.3.cmml">ctc</mi></msub></mrow></mrow></mrow><mo lspace="0em" id="S4.Ex6.m1.1.1.1.2" xref="S4.Ex6.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex6.m1.1b"><apply id="S4.Ex6.m1.1.1.1.1.cmml" xref="S4.Ex6.m1.1.1.1"><eq id="S4.Ex6.m1.1.1.1.1.1.cmml" xref="S4.Ex6.m1.1.1.1.1.1"></eq><ci id="S4.Ex6.m1.1.1.1.1.2.cmml" xref="S4.Ex6.m1.1.1.1.1.2">Loss</ci><apply id="S4.Ex6.m1.1.1.1.1.3.cmml" xref="S4.Ex6.m1.1.1.1.1.3"><plus id="S4.Ex6.m1.1.1.1.1.3.1.cmml" xref="S4.Ex6.m1.1.1.1.1.3.1"></plus><apply id="S4.Ex6.m1.1.1.1.1.3.2.cmml" xref="S4.Ex6.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.Ex6.m1.1.1.1.1.3.2.1.cmml" xref="S4.Ex6.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S4.Ex6.m1.1.1.1.1.3.2.2.cmml" xref="S4.Ex6.m1.1.1.1.1.3.2.2">Loss</ci><ci id="S4.Ex6.m1.1.1.1.1.3.2.3.cmml" xref="S4.Ex6.m1.1.1.1.1.3.2.3">speed</ci></apply><apply id="S4.Ex6.m1.1.1.1.1.3.3.cmml" xref="S4.Ex6.m1.1.1.1.1.3.3"><times id="S4.Ex6.m1.1.1.1.1.3.3.1.cmml" xref="S4.Ex6.m1.1.1.1.1.3.3.1"></times><ci id="S4.Ex6.m1.1.1.1.1.3.3.2.cmml" xref="S4.Ex6.m1.1.1.1.1.3.3.2">𝜆</ci><apply id="S4.Ex6.m1.1.1.1.1.3.3.3.cmml" xref="S4.Ex6.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.Ex6.m1.1.1.1.1.3.3.3.1.cmml" xref="S4.Ex6.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S4.Ex6.m1.1.1.1.1.3.3.3.2.cmml" xref="S4.Ex6.m1.1.1.1.1.3.3.3.2">Loss</ci><ci id="S4.Ex6.m1.1.1.1.1.3.3.3.3.cmml" xref="S4.Ex6.m1.1.1.1.1.3.3.3.3">ctc</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex6.m1.1c">\displaystyle{\rm Loss}={\rm Loss_{speed}}+\lambda{\rm Loss_{ctc}}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS2.p1.1" class="ltx_p">Here, <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\lambda</annotation></semantics></math> is a hyperparameter that adjusts the importance of the playback speed calculation and speech recognition.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Prototype</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">As a prototype of AIx Speed, an application that optimizes audio playback speed using English as the target language has been implemented.
This section describes the implementation and usage of the application.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Implementation</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.4" class="ltx_p">Wav2Vec2 was used for the prototype’s shared utterance learning model (the utterance learning part shared by the speech recognizer and the playback speed adjuster).
For pre-training, LibriSpeech <cite class="ltx_cite ltx_citemacro_citep">(Panayotov et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2015</a>)</cite> was used as the dataset, train-clean-360 as the training data, and dev-clean as the validation data.
The dataset consist of clean speech data in LibriSpeech and were partitioned using the data partitioning method proposed in LibriSpeech for training/validation.
The pre-training did not require the corresponding transcribed text, only the speech data.
We then trained a speech recognizer and a playback speed adjuster using two sets of speech data, including the transcribed text.
One was LibriSpeech’s train-clean-100, and the other was the English speech database read by Japanese students (UME-ERJ) <cite class="ltx_cite ltx_citemacro_citep">(Minematsu et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2002</a>)</cite>.
The latter is a dataset of English spoken by non-native Japanese speakers, with 202 speakers (100 males and 102 females) reading simple English sentences.
The playback speed adjuster and the speech recognizer were trained separately.
First, the speech recognizer was trained using LibriSpeech and UME-ERJ, and then the playback speed adjuster was trained using the same data with fixed weights for the speech recognizer.
All speech files used for training were normalized to a sampling frequency of 16 kHz.
The Wav2Vec2 used the initial parameters implemented in PyTorch <cite class="ltx_cite ltx_citemacro_citep">(Paszke
et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>, and the final layers of both the playback rate adjuster and the speech recognizer were <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="768" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mn id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">768</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><cn type="integer" id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">768</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">768</annotation></semantics></math>-dimensional linear layers.
The hyperparameter <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mi id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><ci id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">\lambda</annotation></semantics></math> of the error function was set to <math id="S5.SS1.p1.3.m3.1" class="ltx_Math" alttext="10^{-7}" display="inline"><semantics id="S5.SS1.p1.3.m3.1a"><msup id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml"><mn id="S5.SS1.p1.3.m3.1.1.2" xref="S5.SS1.p1.3.m3.1.1.2.cmml">10</mn><mrow id="S5.SS1.p1.3.m3.1.1.3" xref="S5.SS1.p1.3.m3.1.1.3.cmml"><mo id="S5.SS1.p1.3.m3.1.1.3a" xref="S5.SS1.p1.3.m3.1.1.3.cmml">−</mo><mn id="S5.SS1.p1.3.m3.1.1.3.2" xref="S5.SS1.p1.3.m3.1.1.3.2.cmml">7</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><apply id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.3.m3.1.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1">superscript</csymbol><cn type="integer" id="S5.SS1.p1.3.m3.1.1.2.cmml" xref="S5.SS1.p1.3.m3.1.1.2">10</cn><apply id="S5.SS1.p1.3.m3.1.1.3.cmml" xref="S5.SS1.p1.3.m3.1.1.3"><minus id="S5.SS1.p1.3.m3.1.1.3.1.cmml" xref="S5.SS1.p1.3.m3.1.1.3"></minus><cn type="integer" id="S5.SS1.p1.3.m3.1.1.3.2.cmml" xref="S5.SS1.p1.3.m3.1.1.3.2">7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">10^{-7}</annotation></semantics></math>, and training was performed for 5 epochs with a batch size of <math id="S5.SS1.p1.4.m4.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S5.SS1.p1.4.m4.1a"><mn id="S5.SS1.p1.4.m4.1.1" xref="S5.SS1.p1.4.m4.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.4.m4.1b"><cn type="integer" id="S5.SS1.p1.4.m4.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.4.m4.1c">32</annotation></semantics></math> using the AdamW <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov and
Hutter, <a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> optimization algorithm.</p>
</div>
<figure id="S5.T1" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>. </span><span id="S5.T1.3.2" class="ltx_text" style="font-size:90%;">Performance comparison of models</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T1.fig1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:195.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.T1.fig1.1.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.T1.fig1.2.2" class="ltx_text" style="font-size:90%;">LibriSpeech</span></figcaption>
<table id="S5.T1.fig1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.fig1.3.1.1" class="ltx_tr">
<th id="S5.T1.fig1.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Model</th>
<th id="S5.T1.fig1.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Avg. Speed</th>
<th id="S5.T1.fig1.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">CER</th>
<th id="S5.T1.fig1.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">WER</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.fig1.3.2.1" class="ltx_tr">
<td id="S5.T1.fig1.3.2.1.1" class="ltx_td ltx_align_center ltx_border_t">Wav2Vec2 (1.00x)</td>
<td id="S5.T1.fig1.3.2.1.2" class="ltx_td ltx_align_center ltx_border_t">1.00</td>
<td id="S5.T1.fig1.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.fig1.3.2.1.3.1" class="ltx_text ltx_font_bold">4.38</span></td>
<td id="S5.T1.fig1.3.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.fig1.3.2.1.4.1" class="ltx_text ltx_font_bold">12.57</span></td>
</tr>
<tr id="S5.T1.fig1.3.3.2" class="ltx_tr">
<td id="S5.T1.fig1.3.3.2.1" class="ltx_td ltx_align_center">Wav2Vec2 (1.30x)</td>
<td id="S5.T1.fig1.3.3.2.2" class="ltx_td ltx_align_center">1.30</td>
<td id="S5.T1.fig1.3.3.2.3" class="ltx_td ltx_align_center">5.61</td>
<td id="S5.T1.fig1.3.3.2.4" class="ltx_td ltx_align_center">14.58</td>
</tr>
<tr id="S5.T1.fig1.3.4.3" class="ltx_tr">
<td id="S5.T1.fig1.3.4.3.1" class="ltx_td ltx_align_center">Wav2Vec2 (1.50x)</td>
<td id="S5.T1.fig1.3.4.3.2" class="ltx_td ltx_align_center">1.50</td>
<td id="S5.T1.fig1.3.4.3.3" class="ltx_td ltx_align_center">6.83</td>
<td id="S5.T1.fig1.3.4.3.4" class="ltx_td ltx_align_center">17.19</td>
</tr>
<tr id="S5.T1.fig1.3.5.4" class="ltx_tr">
<td id="S5.T1.fig1.3.5.4.1" class="ltx_td ltx_align_center"><span id="S5.T1.fig1.3.5.4.1.1" class="ltx_text ltx_font_bold">AIx Speed</span></td>
<td id="S5.T1.fig1.3.5.4.2" class="ltx_td ltx_align_center">1.30</td>
<td id="S5.T1.fig1.3.5.4.3" class="ltx_td ltx_align_center"><span id="S5.T1.fig1.3.5.4.3.1" class="ltx_text ltx_framed ltx_framed_underline">5.21</span></td>
<td id="S5.T1.fig1.3.5.4.4" class="ltx_td ltx_align_center"><span id="S5.T1.fig1.3.5.4.4.1" class="ltx_text ltx_framed ltx_framed_underline">12.96</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.T1.fig2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:195.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.T1.fig2.1.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.T1.fig2.2.2" class="ltx_text" style="font-size:90%;">UME-ERJ</span></figcaption>
<table id="S5.T1.fig2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.fig2.3.1.1" class="ltx_tr">
<th id="S5.T1.fig2.3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Model</th>
<th id="S5.T1.fig2.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Avg. Speed</th>
<th id="S5.T1.fig2.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">CER</th>
<th id="S5.T1.fig2.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">WER</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.fig2.3.2.1" class="ltx_tr">
<td id="S5.T1.fig2.3.2.1.1" class="ltx_td ltx_align_center ltx_border_t">Wav2Vec2 (1.00x)</td>
<td id="S5.T1.fig2.3.2.1.2" class="ltx_td ltx_align_center ltx_border_t">1.00</td>
<td id="S5.T1.fig2.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.fig2.3.2.1.3.1" class="ltx_text ltx_framed ltx_framed_underline">26.74</span></td>
<td id="S5.T1.fig2.3.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.fig2.3.2.1.4.1" class="ltx_text ltx_framed ltx_framed_underline">55.02</span></td>
</tr>
<tr id="S5.T1.fig2.3.3.2" class="ltx_tr">
<td id="S5.T1.fig2.3.3.2.1" class="ltx_td ltx_align_center">Wav2Vec2 (1.29x)</td>
<td id="S5.T1.fig2.3.3.2.2" class="ltx_td ltx_align_center">1.29</td>
<td id="S5.T1.fig2.3.3.2.3" class="ltx_td ltx_align_center">33.71</td>
<td id="S5.T1.fig2.3.3.2.4" class="ltx_td ltx_align_center">63.90</td>
</tr>
<tr id="S5.T1.fig2.3.4.3" class="ltx_tr">
<td id="S5.T1.fig2.3.4.3.1" class="ltx_td ltx_align_center">Wav2Vec2 (1.50x)</td>
<td id="S5.T1.fig2.3.4.3.2" class="ltx_td ltx_align_center">1.50</td>
<td id="S5.T1.fig2.3.4.3.3" class="ltx_td ltx_align_center">36.93</td>
<td id="S5.T1.fig2.3.4.3.4" class="ltx_td ltx_align_center">66.51</td>
</tr>
<tr id="S5.T1.fig2.3.5.4" class="ltx_tr">
<td id="S5.T1.fig2.3.5.4.1" class="ltx_td ltx_align_center"><span id="S5.T1.fig2.3.5.4.1.1" class="ltx_text ltx_font_bold">AIx Speed</span></td>
<td id="S5.T1.fig2.3.5.4.2" class="ltx_td ltx_align_center">1.29</td>
<td id="S5.T1.fig2.3.5.4.3" class="ltx_td ltx_align_center"><span id="S5.T1.fig2.3.5.4.3.1" class="ltx_text ltx_font_bold">26.45</span></td>
<td id="S5.T1.fig2.3.5.4.4" class="ltx_td ltx_align_center"><span id="S5.T1.fig2.3.5.4.4.1" class="ltx_text ltx_font_bold">53.13</span></td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Usage of the application</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Examples of using AIx Speed are shown in Fig. <a href="#S4.F6" title="Figure 6 ‣ 4. AIx Speed ‣ AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
This is an example of the prototype applied to a video uploaded to YouTube.
The horizontal axis is the playback time, and the vertical axis represents the playback speed output by the model for each playback time. The first is an example of speeding up a dialogue, movie, or lecture.
Two speakers appear in the video, and the optimal playback speed can be set for each speaker. Of particular interest is that the two speakers in the video speak at different speeds, so the average playback speed for the two speakers is different.
It can also be seen that the playback speed increases drastically from the moment when the dialog between the two speakers ends and there is no more speech from the person.
Although we did not intend to design this feature, we can see that our system, like conventional playback speed controllers, can speed up the playback speed in the parts where there is no speech.
The second example is speeding up the speech of non-native speakers to make it easier to understand.
Since the speech of non-native speakers is often slower than that of native speakers, moderately speeding up the speech makes it easier to understand.
The change in playback speed shows that the overall playback speed is faster than the speech between native speakers in the first video. This indicates that the model can speed up the speech more because the non-native speakers’ speech is slower than that of the native speakers.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Evaluation</h2>

<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Technical evaluation</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">To demonstrate that the proposed method can optimize playback speed while maintaining content understanding, we compared the CER and WER values at the AIx Speed–modified speech playback speed to those at a constant playback speed.
We compared the CER and WER with the speech playback speed modified by the AIx Speed to the CER and WER when the speech was simply played at a constant speed.
The speeds of the comparison targets were 1.0x, 1.5x and the average speed times the playback speed of AIx Speed.
A standard Wav2Vec2 based speech recognition model, which was the speech recognizer used in our method, was used to compute the CER and WER for comparison.
The performance of the models is shown in Table. <a href="#S5.T1.fig2" title="Table 1 ‣ 5.1. Implementation ‣ 5. Prototype ‣ AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (a) and <a href="#S5.T1.fig2" title="Table 1 ‣ 5.1. Implementation ‣ 5. Prototype ‣ AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (b) for LibriSpeech and UME-ERJ, respectively.
AIx Speed produces speech 1.30 times faster on average for LibriSpeech and 1.29 times faster on average for UME-ERJ.
Both results show that the playback speed optimized by AIx Speed has lower values for both CER and WER than the average constant speech speed at that playback speed.
From these results, it can be said that the proposed model maximizes the playback speed while guaranteeing the recognition performance.
In addition, for UME-ERJ, the speech generated at AIx Speed shows better recognition performance in terms of WER than at 1.0x playback speed.
Therefore, it is also suggested that the proposed method can be used to convert the speech of non-native speakers into more understandable speech.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>User evaluation</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">User experiments were conducted to confirm that the generated speech was understandable.
The quality of the speech generated by the proposed method was compared with that of the speech played at a constant speed, at the average playback speed of the speech.
The quality was evaluated using the mean opinion score, which is commonly used in speech synthesis research <cite class="ltx_cite ltx_citemacro_citep">(van den Oord
et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2016</a>; Wang
et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2017</a>)</cite>.
This measure rates speech quality on a five-point scale from 1 (poor) to 5 (excellent).
Participants were 50 US residents who used English on a daily basis.
20 sentences were extracted from each of the LibriSpeech and UME-ERJ datasets, and half were converted to speech with the proposed speed-up, while the other half were converted to speech with a constant speed-up.
Participants were given a total of 40 sentences of audio and assisted to rate the quality of the audio.
Figure <a href="#S6.F7" title="Figure 7 ‣ 6.2. User evaluation ‣ 6. Evaluation ‣ AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the quality of the generated speech at baseline and AIx Speed, LThe quality of LibreSpeech and UME-ERJ were 0.5 and 0.8 points higher at speeds generated by the proposed method, respectively.</p>
</div>
<figure id="S6.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2403.02938/assets/user_evaluation.png" id="S6.F7.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="287" height="174" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>. </span><span id="S6.F7.3.2" class="ltx_text" style="font-size:90%;">Voice quality at baseline (constant playback speed increase throughout) and AIx Speed (flexible playback speed increase)</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S6.F7.4" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
</div>
</figure>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Discussion</h2>

<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>Evaluation results</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">The technical evaluation shows that the proposed method can produce speech that is easier to understand than that produced by simply increasing the playback speed in terms of speech recognition performance.
The user evaluation also shows that the proposed method can produce speech that is easier to understand for real users. These results show that the proposed method can produce speech with a high playback speed within a range that is easy for users to understand.
This allows users to watch videos at a reasonable speed without having to adjust the playback speed for each video.
However, the improvement in MOS values by using the proposed method is by no means sufficient.
In the current model, the average conversion to a faster playback speed is about 1.3 times, but it is a future task to investigate whether it is possible to make this even faster.
In fact, many video playback services implement 1.5x and 2.0x playback speeds, and some people watch dramas and lectures at such speeds. Therefore, we expect that it will be possible to convert up to this speed and make the audio easy to understand.
In addition, since each user has a different preferred playback speed, personalizing the model so that it plays at the optimal playback speed for users is also a future issue.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2. </span>Listening comprehension and speech recognition</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">As discussed in the preliminary research chapter, it can be seen that there is a relationship between speech recognition performance and human transcription ability.
Thus, we expect to build automated systems for various tasks and evaluations by replacing human speech comprehension ability with machine learning models, as in this system.
On the other hand, speech recognition performance based on playback speed does not perfectly match human speech comprehension.
In other words, as the playback speed increases, the dictation performance decreases in both cases, but the performance values are not exactly the same.
Thus, we anticipate that by training speech recognition models to match these relationships as closely as possible, it will be possible to use them more generally as alternatives to humans. Distillation, a technique that learns to approximate an output that matches existing results, will be the technical key.</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3. </span>Adjustment of non-native speakers’ speech</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">Several suggestions can be made as to how increasing the playback speed by the proposed system improves the intelligibility of speech for non-native speakers. One of them is that when non-native speakers read English manuscripts, they may find it easier to understand if they speak naturally (slower from a native speaker’s point of view) and then artificially speed up their speech, rather than forcing them to speak quickly like a native speaker. In fact, this study also began with the realization that it is easier to listen to a video of a non-native speaker speaking his or her native language when it is played at a high speed.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">This paper presents a system that applies a speech recognition model to automatically and flexibly adjust the playback speed of video and audio within the range of human comprehension.
By using this system, users can consume audiovisual content at optimal speeds without having to manually adjust the playback speed.
Experiments have also confirmed that the system makes it easier for users to understand the speech of non-native speakers.
In the future, we expect the system to be used in a variety of applications, such as video distribution services and language learning tools.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This work was supported by JST Moonshot R&amp;D Grant Number JPMJMS2012, JST CREST Grant Number JPMJCR17A3, and The Univesity of Tokyo Human Augmentation Research Initiative.

</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Apostolidis et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Evlampios Apostolidis,
Eleni Adamantidou, Alexandros I Metsai,
Vasileios Mezaris, and Ioannis Patras.
2021.

</span>
<span class="ltx_bibblock">Video summarization using deep neural networks: A
survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">Proc. IEEE</em> 109,
11 (2021), 1838–1863.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski
et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao
Zhou, Abdelrahman Mohamed, and Michael
Auli. 2020.

</span>
<span class="ltx_bibblock">Wav2vec 2.0: A Framework for Self-Supervised
Learning of Speech Representations. In <em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">Advances in
Neural Information Processing Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai and Zhang (2021)</span>
<span class="ltx_bibblock">
Zx Bai and Xiao-Lei
Zhang. 2021.

</span>
<span class="ltx_bibblock">Speaker recognition based on deep learning: An
overview.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Neural Networks</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng
et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2009)</span>
<span class="ltx_bibblock">
Kai-Yin Cheng, Sheng-Jie
Luo, Bing-Yu Chen, and Hao-Hua Chu.
2009.

</span>
<span class="ltx_bibblock">SmartPlayer: User-Centric Video Fast-Forwarding.
In <em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Proc. of the SIGCHI Conference on Human Factors
in Computing Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin
et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei
Chang, Kenton Lee, and Kristina
Toutanova. 2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding. In
<em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers)</em>.
Association for Computational Linguistics,
Minneapolis, Minnesota, 4171–4186.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duan and Chen (2019)</span>
<span class="ltx_bibblock">
Songshuang Duan and
Xiaoqian Chen. 2019.

</span>
<span class="ltx_bibblock">Why College Students Watch Streaming Drama at
Higher Playback Speed: The Uses and Gratifications Perspective. In
<em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">International Joint Conference on Information,
Media and Engineering</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fontan
et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Lionel Fontan, Isabelle
Ferrané, Jérôme Farinas,
Julien Pinquier, Julien Tardieu,
Cynthia Magnen, Pascal Gaillard,
Xavier Aumont, and Christian
Füllgrabe. 2017.

</span>
<span class="ltx_bibblock">Automatic speech recognition predicts speech
intelligibility and comprehension for listeners with simulated age-related
hearing loss.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">Journal of Speech, Language, and Hearing
Research</em> 60, 9 (2017),
2394–2405.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2006)</span>
<span class="ltx_bibblock">
A. Graves, S. Fernandez,
F. Gomez, and J. Schmidhuber.
2006.

</span>
<span class="ltx_bibblock">Connectionist Temporal Classification: Labelling
Unsegmented Sequence Data with Recurrent Neural Nets. In
<em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">ICML ’06: Proceedings of the International
Conference on Machine Learning</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Higuchi
et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Keita Higuchi, Ryo
Yonetani, and Yoichi Sato.
2017.

</span>
<span class="ltx_bibblock">EgoScanning: Quickly Scanning First-Person Videos
with Egocentric Elastic Timelines. In <em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Proc. ACM
Conference on Human Factors in Computing Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Wei-Ning Hsu, Benjamin
Bolte, Yao-Hung Hubert Tsai, Kushal
Lakhotia, Ruslan Salakhutdinov, and
Abdelrahman Mohamed. 2021.

</span>
<span class="ltx_bibblock">HuBERT: Self-Supervised Speech Representation
Learning by Masked Prediction of Hidden Units.

</span>
<span class="ltx_bibblock">(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang and
Schulzrinne (2002)</span>
<span class="ltx_bibblock">
Wenyu Jiang and H.
Schulzrinne. 2002.

</span>
<span class="ltx_bibblock">Speech recognition performance as an effective
perceived quality predictor. In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE 2002 Tenth
IEEE International Workshop on Quality of Service (Cat. No.02EX564)</em>.
269–275.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kabir
et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Muhammad Mohsin Kabir,
Muhammad Firoz Mridha, Jungpil Shin,
Israt Jahan, and Abu Quwsar Ohi.
2021.

</span>
<span class="ltx_bibblock">A Survey of Speaker Recognition: Fundamental
Theories, Recognition Methods and Opportunities.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">IEEE Access</em> 9
(2021), 79236–79263.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kawamura et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Shunya Kawamura, Tsukasa
Fukusato, Tatsunori Hirai, and Shigeo
Morishima. 2014.

</span>
<span class="ltx_bibblock">Efficient Video Viewing System for Racquet Sports
with Automatic Summarization Focusing on Rally Scenes. In
<em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">ACM SIGGRAPH 2014 Posters</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kayukawa et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Seita Kayukawa, Keita
Higuchi, Ryo Yonetani, Masanori
Nakamura, Yoichi Sato, and Shigeo
Morishima. 2018.

</span>
<span class="ltx_bibblock">Dynamic Object Scanning: Object-Based Elastic
Timeline for Quickly Browsing First-Person Videos. In
<em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">Extended Abstracts of the 2018 CHI Conference on
Human Factors in Computing Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kurihara (2011)</span>
<span class="ltx_bibblock">
Kazutaka Kurihara.
2011.

</span>
<span class="ltx_bibblock">CinemaGazer: A System for Watching Video at Very
High Speed. In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proc. of the Workshop on Advanced
Visual Interfaces AVI</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lang
et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
David Lang, Guanling
Chen, Kathy Mirzaei, and Andreas
Paepcke. 2020.

</span>
<span class="ltx_bibblock">Is Faster Better? A Study of Video Playback Speed.
In <em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">Proc. of the Tenth International Conference on
Learning Analytics &amp; Knowledge</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and
Hutter (2019)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and
Frank Hutter. 2019.

</span>
<span class="ltx_bibblock">Decoupled Weight Decay Regularization. In
<em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning
Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malik
et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Mishaim Malik, Muhammad
Malik, Khawar Mehmood, and Imran
Makhdoom. 2021.

</span>
<span class="ltx_bibblock">Automatic speech recognition: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">Multimedia Tools and Applications</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McFee et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Brian McFee, Colin
Raffel, Dawen Liang, Daniel P Ellis,
Matt McVicar, Eric Battenberg, and
Oriol Nieto. 2015.

</span>
<span class="ltx_bibblock">Librosa: Audio and music signal analysis in
python. In <em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 14th python in
science conference</em>. Citeseer.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Minematsu et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2002)</span>
<span class="ltx_bibblock">
Nobuaki Minematsu,
Yoshihiro Tomiyama, Kei Yoshimoto,
Katsumasa Shimizu, Seiichi Nakagawa,
Masatake Dantsuji, and Shozo Makino.
2002.

</span>
<span class="ltx_bibblock">English Speech Database Read by Japanese Learners
for CALL System Development.. In <em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">LREC</em>. Citeseer.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Murphy et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Dillon H. Murphy, Kara M.
Hoover, Karina Agadzhanyan, Jesse C.
Kuehn, and Alan D. Castel.
2022.

</span>
<span class="ltx_bibblock">Learning in double time: The effect of lecture
video speed on immediate and delayed comprehension.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">Applied Cognitive Psychology</em>
36, 1 (2022),
69–82.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nagahama and
Morita (2017)</span>
<span class="ltx_bibblock">
Toru Nagahama and Yusuke
Morita. 2017.

</span>
<span class="ltx_bibblock">Effect Analysis of Playback Speed for Lecture Video
Including Instructor Images.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">International Journal for Educational Media
and Technology</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nishida
et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Naoto Nishida, Hinako
Nozaki, and Buntarou Shizuki.
2022.

</span>
<span class="ltx_bibblock">Laugh at Your Own Pace: Basic Performance
Evaluation of Language Learning Assistance by Adjustment of Video Playback
Speeds Based on Laughter Detection. In <em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">Proc. of
the Ninth ACM Conference on Learning @ Scale</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panayotov et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Vassil Panayotov, Guoguo
Chen, Daniel Povey, and Sanjeev
Khudanpur. 2015.

</span>
<span class="ltx_bibblock">Librispeech: An ASR corpus based on public domain
audio books. In <em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">IEEE International Conference on
Acoustics, Speech and Signal Processing</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke
et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross,
Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin,
Natalia Gimelshein, Luca Antiga,
Alban Desmaison, Andreas Kopf,
Edward Yang, Zachary DeVito,
Martin Raison, Alykhan Tejani,
Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and
Soumith Chintala. 2019.

</span>
<span class="ltx_bibblock">PyTorch: An Imperative Style, High-Performance Deep
Learning Library.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information
Processing Systems 32</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song
et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Sunghyun Song, Jeong-ki
Hong, Ian Oakley, Jun Dong Cho, and
Andrea Bianchi. 2015.

</span>
<span class="ltx_bibblock">Automatically Adjusting the Speed of E-Learning
Videos. In <em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">Proc. of the 33rd Annual ACM Conference
Extended Abstracts on Human Factors in Computing Systems.</em>

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tejedor-GarcÃ­a et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Cristian Tejedor-GarcÃ­a,
ValentÃ­n CardeÃ±oso-Payo, and
David Escudero-Mancebo. 2021.

</span>
<span class="ltx_bibblock">Automatic Speech Recognition (ASR) Systems Applied
to Pronunciation Assessment of L2 Spanish for Japanese Speakers.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">Applied Sciences</em> 11,
15 (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van den Oord
et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
AÃ¤ron van den Oord,
Sander Dieleman, Heiga Zen,
Karen Simonyan, Oriol Vinyals,
Alexander Graves, Nal Kalchbrenner,
Andrew Senior, and Koray Kavukcuoglu.
2016.

</span>
<span class="ltx_bibblock">WaveNet: A Generative Model for Raw Audio. In
<em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">Arxiv</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vartakavi
et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Aneesh Vartakavi, Amanmeet
Garg, and Zafar Rafii. 2021.

</span>
<span class="ltx_bibblock">Audio Summarization for Podcasts. In
<em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">2021 29th European Signal Processing Conference
(EUSIPCO)</em>. IEEE, 431–435.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Yuxuan Wang, R. J.
Skerry-Ryan, Daisy Stanton, Yonghui Wu,
Ron J. Weiss, Navdeep Jaitly,
Zongheng Yang, Ying Xiao,
Zhifeng Chen, Samy Bengio,
Quoc V. Le, Yannis Agiomyrgiannakis,
Rob Clark, and Rif A. Saurous.
2017.

</span>
<span class="ltx_bibblock">Tacotron: Towards End-to-End Speech Synthesis. In
<em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>. 4006–4010.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Wayne Xiong, Jasha
Droppo, Xuedong Huang, Frank Seide,
Michael Seltzer, Andreas Stolcke,
Dong Yu, and Geoffrey Zweig.
2016.

</span>
<span class="ltx_bibblock">Achieving Human Parity in Conversational Speech
Recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and
Language Processing</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang
et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Xinlei Zhang, Takashi
Miyaki, and Jun Rekimoto.
2020.

</span>
<span class="ltx_bibblock">WithYou: Automated Adaptive Speech Tutoring With
Context-Dependent Speech Recognition. In <em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">Proc. of
the 2020 CHI Conference on Human Factors in Computing Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.02937" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.02938" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.02938">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.02938" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.02939" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 17:36:47 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
