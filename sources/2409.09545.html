<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.09545] Multi-Microphone and Multi-Modal Emotion Recognition in Reverberant Environment</title><meta property="og:description" content="This paper presents a  Multi-modal Emotion Recognition (MER) system designed to enhance emotion recognition accuracy in challenging acoustic conditions. Our approach combines a modified and extended  Hierarchical Tokenâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multi-Microphone and Multi-Modal Emotion Recognition in Reverberant Environment">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multi-Microphone and Multi-Modal Emotion Recognition in Reverberant Environment">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.09545">

<!--Generated on Sun Oct  6 01:15:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Multi-Microphone and Multi-Modal Emotion Recognition 
<br class="ltx_break">in Reverberant Environment</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.1" class="ltx_p"><span id="id1.1.1" class="ltx_text" style="font-size:90%;">This paper presents a  <span title="Multi-modal Emotion Recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Multi-modal Emotion Recognition</span></span> (<abbr title="Multi-modal Emotion Recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MER</span></abbr>) system designed to enhance emotion recognition accuracy in challenging acoustic conditions. Our approach combines a modified and extended  <span title="Hierarchical Token-semantic Audio Transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Hierarchical Token-semantic Audio Transformer</span></span> (<abbr title="Hierarchical Token-semantic Audio Transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HTS-AT</span></abbr>) for multi-channel audio processing with an <math id="id1.1.1.m1.1" class="ltx_Math" alttext="R(2+1)" display="inline"><semantics id="id1.1.1.m1.1a"><mrow id="id1.1.1.m1.1.1" xref="id1.1.1.m1.1.1.cmml"><mi id="id1.1.1.m1.1.1.3" xref="id1.1.1.m1.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="id1.1.1.m1.1.1.2" xref="id1.1.1.m1.1.1.2.cmml">â€‹</mo><mrow id="id1.1.1.m1.1.1.1.1" xref="id1.1.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="id1.1.1.m1.1.1.1.1.2" xref="id1.1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="id1.1.1.m1.1.1.1.1.1" xref="id1.1.1.m1.1.1.1.1.1.cmml"><mn id="id1.1.1.m1.1.1.1.1.1.2" xref="id1.1.1.m1.1.1.1.1.1.2.cmml">2</mn><mo id="id1.1.1.m1.1.1.1.1.1.1" xref="id1.1.1.m1.1.1.1.1.1.1.cmml">+</mo><mn id="id1.1.1.m1.1.1.1.1.1.3" xref="id1.1.1.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="id1.1.1.m1.1.1.1.1.3" xref="id1.1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="id1.1.1.m1.1b"><apply id="id1.1.1.m1.1.1.cmml" xref="id1.1.1.m1.1.1"><times id="id1.1.1.m1.1.1.2.cmml" xref="id1.1.1.m1.1.1.2"></times><ci id="id1.1.1.m1.1.1.3.cmml" xref="id1.1.1.m1.1.1.3">ğ‘…</ci><apply id="id1.1.1.m1.1.1.1.1.1.cmml" xref="id1.1.1.m1.1.1.1.1"><plus id="id1.1.1.m1.1.1.1.1.1.1.cmml" xref="id1.1.1.m1.1.1.1.1.1.1"></plus><cn type="integer" id="id1.1.1.m1.1.1.1.1.1.2.cmml" xref="id1.1.1.m1.1.1.1.1.1.2">2</cn><cn type="integer" id="id1.1.1.m1.1.1.1.1.1.3.cmml" xref="id1.1.1.m1.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.m1.1c">R(2+1)</annotation></semantics></math>D  <span title="Convolutional Neural Networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Convolutional Neural Networks</span></span> (<abbr title="Convolutional Neural Networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CNN</span></abbr>) model for video analysis. We evaluate our proposed method on a reverberated version of the  <span title="Ryerson audio-visual database of emotional speech and song" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Ryerson audio-visual database of emotional speech and song</span></span> (<abbr title="Ryerson audio-visual database of emotional speech and song" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RAVDESS</span></abbr>) dataset using synthetic and real-world <span title="Room Impulse Responses" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">Room Impulse Responsess</span></span>.
Our results demonstrate that integrating audio and video modalities yields superior performance compared to uni-modal approaches, especially in challenging acoustic conditions.
Moreover, we show that the multimodal (audiovisual) approach that utilizes multiple microphones outperforms its single-microphone counterpart.</span></p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">â€”â€‰<span id="p1.1.1.1.1" class="ltx_text ltx_font_medium">
Emotion recognition, multi-modal, reverberant conditions, audio transformer</span></span></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span title="Emotion Recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Emotion Recognition</span></span><span id="S1.p1.1.1" class="ltx_text" style="font-size:90%;"> (</span><abbr title="Emotion Recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ER</span></abbr><span id="S1.p1.1.2" class="ltx_text" style="font-size:90%;">) is critical in human-computer interaction, with applications ranging from healthcare to customer service. Humans naturally express emotions across multiple modalities, including facial expressions, language, speech, and gestures. Accurately modeling the interactions between these modalities, which contain complementary and potentially redundant information, is essential for effective emotion recognition. Most existing studies primarily focus on uni-modal emotion recognition, concentrating on either text, speech, or video </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p1.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S1.p1.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p1.1.5" class="ltx_text" style="font-size:90%;">. Although significant advancements in single-modal emotion recognition have been demonstrated, these models often fall short in complex scenarios since they do not utilize the inherently multi-modal nature of emotional expression. Moreover, research on jointly employing multi-modal and multi-microphones for </span><abbr title="Emotion Recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ER</span></abbr><span id="S1.p1.1.6" class="ltx_text" style="font-size:90%;"> is relatively scarce.</span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p"><span id="S1.p2.1.1" class="ltx_text" style="font-size:90%;">Previous works have made significant strides in </span><abbr title="Multi-modal Emotion Recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MER</span></abbr><span id="S1.p2.1.2" class="ltx_text" style="font-size:90%;">. Studies such as </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S1.p2.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.5" class="ltx_text" style="font-size:90%;"> have developed systems that simultaneously analyze visual and acoustic data. In </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S1.p2.1.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.8" class="ltx_text" style="font-size:90%;">, researchers presented an unsupervised </span><abbr title="Multi-modal Emotion Recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MER</span></abbr><span id="S1.p2.1.9" class="ltx_text" style="font-size:90%;"> feature learning approach incorporating audio-visual and textual information. These studies often overlooked the challenges posed by real-world single-channel acoustic conditions, particularly reverberation and noise, which can significantly impact the performance of audio-based emotion recognition.</span></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p"><span id="S1.p3.1.1" class="ltx_text" style="font-size:90%;">Feature selection is vital in designing effective </span><abbr title="Multi-modal Emotion Recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MER</span></abbr><span id="S1.p3.1.2" class="ltx_text" style="font-size:90%;"> systems. For acoustic features, log-mel filterbank energies and log-mel spectrograms have been widely adopted </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S1.p3.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.5" class="ltx_text" style="font-size:90%;">. In the video domain, various deep learning architectures such as VGG16 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S1.p3.1.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.8" class="ltx_text" style="font-size:90%;">, I3D </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.9.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S1.p3.1.10.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.11" class="ltx_text" style="font-size:90%;">, and FaceNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.12.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S1.p3.1.13.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.14" class="ltx_text" style="font-size:90%;"> have been employed, along with facial features like landmarks and action units extracted using tools like OpenFace </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.15.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S1.p3.1.16.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.17" class="ltx_text" style="font-size:90%;">. For the text modality,  </span><span title="Global Vectors for Word Representation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Global Vectors for Word Representation</span></span><span id="S1.p3.1.18" class="ltx_text" style="font-size:90%;"> (</span><abbr title="Global Vectors for Word Representation" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">GloVe</span></abbr><span id="S1.p3.1.19" class="ltx_text" style="font-size:90%;">) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.20.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S1.p3.1.21.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.22" class="ltx_text" style="font-size:90%;"> have been frequently used </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.23.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S1.p3.1.24.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.25" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><span id="S1.p4.1.1" class="ltx_text" style="font-size:90%;">Despite these advancements, a gap remains in addressing the challenges posed by reverberant and noisy environments. Real-world acoustic conditions can significantly alter speech signals, potentially degrading the performance of audio-based emotion recognition systems. Moreover, the integration of multi-channel audio processing techniques with video analysis for emotion recognition in such challenging conditions has not been thoroughly explored.</span></p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text" style="font-size:90%;">This work addresses these limitations by proposing a </span><abbr title="Multi-modal Emotion Recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MER</span></abbr><span id="S1.p5.1.2" class="ltx_text" style="font-size:90%;"> with multi-channel audio that outperforms solutions solely based on single-channel audio. We propose a novel approach that combines two state-of-the-art architectures for audio-visual emotion recognition. The multi-channel extension of the </span><abbr title="Hierarchical Token-semantic Audio Transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HTS-AT</span></abbr><span id="S1.p5.1.3" class="ltx_text" style="font-size:90%;"> architecture for the audio modality </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p5.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S1.p5.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p5.1.6" class="ltx_text" style="font-size:90%;"> and the </span><math id="S1.p5.1.m1.1" class="ltx_Math" alttext="R(2+1)" display="inline"><semantics id="S1.p5.1.m1.1a"><mrow id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml"><mi mathsize="90%" id="S1.p5.1.m1.1.1.3" xref="S1.p5.1.m1.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S1.p5.1.m1.1.1.2" xref="S1.p5.1.m1.1.1.2.cmml">â€‹</mo><mrow id="S1.p5.1.m1.1.1.1.1" xref="S1.p5.1.m1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S1.p5.1.m1.1.1.1.1.2" xref="S1.p5.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S1.p5.1.m1.1.1.1.1.1" xref="S1.p5.1.m1.1.1.1.1.1.cmml"><mn mathsize="90%" id="S1.p5.1.m1.1.1.1.1.1.2" xref="S1.p5.1.m1.1.1.1.1.1.2.cmml">2</mn><mo mathsize="90%" id="S1.p5.1.m1.1.1.1.1.1.1" xref="S1.p5.1.m1.1.1.1.1.1.1.cmml">+</mo><mn mathsize="90%" id="S1.p5.1.m1.1.1.1.1.1.3" xref="S1.p5.1.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo maxsize="90%" minsize="90%" id="S1.p5.1.m1.1.1.1.1.3" xref="S1.p5.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><apply id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1"><times id="S1.p5.1.m1.1.1.2.cmml" xref="S1.p5.1.m1.1.1.2"></times><ci id="S1.p5.1.m1.1.1.3.cmml" xref="S1.p5.1.m1.1.1.3">ğ‘…</ci><apply id="S1.p5.1.m1.1.1.1.1.1.cmml" xref="S1.p5.1.m1.1.1.1.1"><plus id="S1.p5.1.m1.1.1.1.1.1.1.cmml" xref="S1.p5.1.m1.1.1.1.1.1.1"></plus><cn type="integer" id="S1.p5.1.m1.1.1.1.1.1.2.cmml" xref="S1.p5.1.m1.1.1.1.1.1.2">2</cn><cn type="integer" id="S1.p5.1.m1.1.1.1.1.1.3.cmml" xref="S1.p5.1.m1.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">R(2+1)</annotation></semantics></math><span id="S1.p5.1.7" class="ltx_text" style="font-size:90%;">D </span><abbr title="Convolutional Neural Networks" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CNN</span></abbr><span id="S1.p5.1.8" class="ltx_text" style="font-size:90%;"> model </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p5.1.9.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S1.p5.1.10.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p5.1.11" class="ltx_text" style="font-size:90%;"> for the video modality.</span></p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text" style="font-size:90%;">We use a reverberated version of the </span><abbr title="Ryerson audio-visual database of emotional speech and song" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RAVDESS</span></abbr><span id="S1.p6.1.2" class="ltx_text" style="font-size:90%;"> database to analyze the proposed schemeâ€™s performance. Reverberation was added by convolving the speech utterances with real-life </span><abbr title="Room Impulse Responses" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RIRs</span></abbr><span id="S1.p6.1.3" class="ltx_text" style="font-size:90%;"> drawn from the  </span><span title="Acoustic Characterisation of Environments" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Acoustic Characterisation of Environments</span></span><span id="S1.p6.1.4" class="ltx_text" style="font-size:90%;"> (</span><abbr title="Acoustic Characterisation of Environments" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ACE</span></abbr><span id="S1.p6.1.5" class="ltx_text" style="font-size:90%;">) challenge dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p6.1.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S1.p6.1.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p6.1.8" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">2 </span>Problem Formulation</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text" style="font-size:90%;">Denote the two modalities as </span><math id="S2.p1.1.m1.2" class="ltx_Math" alttext="M=\{\text{video},\text{audio}\}" display="inline"><semantics id="S2.p1.1.m1.2a"><mrow id="S2.p1.1.m1.2.3" xref="S2.p1.1.m1.2.3.cmml"><mi mathsize="90%" id="S2.p1.1.m1.2.3.2" xref="S2.p1.1.m1.2.3.2.cmml">M</mi><mo mathsize="90%" id="S2.p1.1.m1.2.3.1" xref="S2.p1.1.m1.2.3.1.cmml">=</mo><mrow id="S2.p1.1.m1.2.3.3.2" xref="S2.p1.1.m1.2.3.3.1.cmml"><mo maxsize="90%" minsize="90%" id="S2.p1.1.m1.2.3.3.2.1" xref="S2.p1.1.m1.2.3.3.1.cmml">{</mo><mtext mathsize="90%" id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1a.cmml">video</mtext><mo mathsize="90%" id="S2.p1.1.m1.2.3.3.2.2" xref="S2.p1.1.m1.2.3.3.1.cmml">,</mo><mtext mathsize="90%" id="S2.p1.1.m1.2.2" xref="S2.p1.1.m1.2.2a.cmml">audio</mtext><mo maxsize="90%" minsize="90%" id="S2.p1.1.m1.2.3.3.2.3" xref="S2.p1.1.m1.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.2b"><apply id="S2.p1.1.m1.2.3.cmml" xref="S2.p1.1.m1.2.3"><eq id="S2.p1.1.m1.2.3.1.cmml" xref="S2.p1.1.m1.2.3.1"></eq><ci id="S2.p1.1.m1.2.3.2.cmml" xref="S2.p1.1.m1.2.3.2">ğ‘€</ci><set id="S2.p1.1.m1.2.3.3.1.cmml" xref="S2.p1.1.m1.2.3.3.2"><ci id="S2.p1.1.m1.1.1a.cmml" xref="S2.p1.1.m1.1.1"><mtext mathsize="90%" id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">video</mtext></ci><ci id="S2.p1.1.m1.2.2a.cmml" xref="S2.p1.1.m1.2.2"><mtext mathsize="90%" id="S2.p1.1.m1.2.2.cmml" xref="S2.p1.1.m1.2.2">audio</mtext></ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.2c">M=\{\text{video},\text{audio}\}</annotation></semantics></math><span id="S2.p1.1.2" class="ltx_text" style="font-size:90%;">. Also, denote the set of emotions as:</span></p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.9" class="ltx_Math" alttext="E=\{\text{happy},\text{calm},\text{sad},\text{angry},\text{neutral},\text{fearful},\text{disgust},\text{surprised}\}." display="block"><semantics id="S2.E1.m1.9a"><mrow id="S2.E1.m1.9.9.1" xref="S2.E1.m1.9.9.1.1.cmml"><mrow id="S2.E1.m1.9.9.1.1" xref="S2.E1.m1.9.9.1.1.cmml"><mi mathsize="90%" id="S2.E1.m1.9.9.1.1.2" xref="S2.E1.m1.9.9.1.1.2.cmml">E</mi><mo mathsize="90%" id="S2.E1.m1.9.9.1.1.1" xref="S2.E1.m1.9.9.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.9.9.1.1.3.2" xref="S2.E1.m1.9.9.1.1.3.1.cmml"><mo maxsize="90%" minsize="90%" id="S2.E1.m1.9.9.1.1.3.2.1" xref="S2.E1.m1.9.9.1.1.3.1.cmml">{</mo><mtext mathsize="90%" id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1a.cmml">happy</mtext><mo mathsize="90%" id="S2.E1.m1.9.9.1.1.3.2.2" xref="S2.E1.m1.9.9.1.1.3.1.cmml">,</mo><mtext mathsize="90%" id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2a.cmml">calm</mtext><mo mathsize="90%" id="S2.E1.m1.9.9.1.1.3.2.3" xref="S2.E1.m1.9.9.1.1.3.1.cmml">,</mo><mtext mathsize="90%" id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3a.cmml">sad</mtext><mo mathsize="90%" id="S2.E1.m1.9.9.1.1.3.2.4" xref="S2.E1.m1.9.9.1.1.3.1.cmml">,</mo><mtext mathsize="90%" id="S2.E1.m1.4.4" xref="S2.E1.m1.4.4a.cmml">angry</mtext><mo mathsize="90%" id="S2.E1.m1.9.9.1.1.3.2.5" xref="S2.E1.m1.9.9.1.1.3.1.cmml">,</mo><mtext mathsize="90%" id="S2.E1.m1.5.5" xref="S2.E1.m1.5.5a.cmml">neutral</mtext><mo mathsize="90%" id="S2.E1.m1.9.9.1.1.3.2.6" xref="S2.E1.m1.9.9.1.1.3.1.cmml">,</mo><mtext mathsize="90%" id="S2.E1.m1.6.6" xref="S2.E1.m1.6.6a.cmml">fearful</mtext><mo mathsize="90%" id="S2.E1.m1.9.9.1.1.3.2.7" xref="S2.E1.m1.9.9.1.1.3.1.cmml">,</mo><mtext mathsize="90%" id="S2.E1.m1.7.7" xref="S2.E1.m1.7.7a.cmml">disgust</mtext><mo mathsize="90%" id="S2.E1.m1.9.9.1.1.3.2.8" xref="S2.E1.m1.9.9.1.1.3.1.cmml">,</mo><mtext mathsize="90%" id="S2.E1.m1.8.8" xref="S2.E1.m1.8.8a.cmml">surprised</mtext><mo maxsize="90%" minsize="90%" id="S2.E1.m1.9.9.1.1.3.2.9" xref="S2.E1.m1.9.9.1.1.3.1.cmml">}</mo></mrow></mrow><mo lspace="0em" mathsize="90%" id="S2.E1.m1.9.9.1.2" xref="S2.E1.m1.9.9.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.9b"><apply id="S2.E1.m1.9.9.1.1.cmml" xref="S2.E1.m1.9.9.1"><eq id="S2.E1.m1.9.9.1.1.1.cmml" xref="S2.E1.m1.9.9.1.1.1"></eq><ci id="S2.E1.m1.9.9.1.1.2.cmml" xref="S2.E1.m1.9.9.1.1.2">ğ¸</ci><set id="S2.E1.m1.9.9.1.1.3.1.cmml" xref="S2.E1.m1.9.9.1.1.3.2"><ci id="S2.E1.m1.1.1a.cmml" xref="S2.E1.m1.1.1"><mtext mathsize="90%" id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">happy</mtext></ci><ci id="S2.E1.m1.2.2a.cmml" xref="S2.E1.m1.2.2"><mtext mathsize="90%" id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">calm</mtext></ci><ci id="S2.E1.m1.3.3a.cmml" xref="S2.E1.m1.3.3"><mtext mathsize="90%" id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">sad</mtext></ci><ci id="S2.E1.m1.4.4a.cmml" xref="S2.E1.m1.4.4"><mtext mathsize="90%" id="S2.E1.m1.4.4.cmml" xref="S2.E1.m1.4.4">angry</mtext></ci><ci id="S2.E1.m1.5.5a.cmml" xref="S2.E1.m1.5.5"><mtext mathsize="90%" id="S2.E1.m1.5.5.cmml" xref="S2.E1.m1.5.5">neutral</mtext></ci><ci id="S2.E1.m1.6.6a.cmml" xref="S2.E1.m1.6.6"><mtext mathsize="90%" id="S2.E1.m1.6.6.cmml" xref="S2.E1.m1.6.6">fearful</mtext></ci><ci id="S2.E1.m1.7.7a.cmml" xref="S2.E1.m1.7.7"><mtext mathsize="90%" id="S2.E1.m1.7.7.cmml" xref="S2.E1.m1.7.7">disgust</mtext></ci><ci id="S2.E1.m1.8.8a.cmml" xref="S2.E1.m1.8.8"><mtext mathsize="90%" id="S2.E1.m1.8.8.cmml" xref="S2.E1.m1.8.8">surprised</mtext></ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.9c">E=\{\text{happy},\text{calm},\text{sad},\text{angry},\text{neutral},\text{fearful},\text{disgust},\text{surprised}\}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.p1.5" class="ltx_p"><span id="S2.p1.5.1" class="ltx_text" style="font-size:90%;">Let </span><math id="S2.p1.2.m1.1" class="ltx_Math" alttext="v(t)" display="inline"><semantics id="S2.p1.2.m1.1a"><mrow id="S2.p1.2.m1.1.2" xref="S2.p1.2.m1.1.2.cmml"><mi mathsize="90%" id="S2.p1.2.m1.1.2.2" xref="S2.p1.2.m1.1.2.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S2.p1.2.m1.1.2.1" xref="S2.p1.2.m1.1.2.1.cmml">â€‹</mo><mrow id="S2.p1.2.m1.1.2.3.2" xref="S2.p1.2.m1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S2.p1.2.m1.1.2.3.2.1" xref="S2.p1.2.m1.1.2.cmml">(</mo><mi mathsize="90%" id="S2.p1.2.m1.1.1" xref="S2.p1.2.m1.1.1.cmml">t</mi><mo maxsize="90%" minsize="90%" id="S2.p1.2.m1.1.2.3.2.2" xref="S2.p1.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.2.m1.1b"><apply id="S2.p1.2.m1.1.2.cmml" xref="S2.p1.2.m1.1.2"><times id="S2.p1.2.m1.1.2.1.cmml" xref="S2.p1.2.m1.1.2.1"></times><ci id="S2.p1.2.m1.1.2.2.cmml" xref="S2.p1.2.m1.1.2.2">ğ‘£</ci><ci id="S2.p1.2.m1.1.1.cmml" xref="S2.p1.2.m1.1.1">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m1.1c">v(t)</annotation></semantics></math><span id="S2.p1.5.2" class="ltx_text" style="font-size:90%;"> be the video signal and </span><math id="S2.p1.3.m2.1" class="ltx_Math" alttext="s(t)" display="inline"><semantics id="S2.p1.3.m2.1a"><mrow id="S2.p1.3.m2.1.2" xref="S2.p1.3.m2.1.2.cmml"><mi mathsize="90%" id="S2.p1.3.m2.1.2.2" xref="S2.p1.3.m2.1.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.p1.3.m2.1.2.1" xref="S2.p1.3.m2.1.2.1.cmml">â€‹</mo><mrow id="S2.p1.3.m2.1.2.3.2" xref="S2.p1.3.m2.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S2.p1.3.m2.1.2.3.2.1" xref="S2.p1.3.m2.1.2.cmml">(</mo><mi mathsize="90%" id="S2.p1.3.m2.1.1" xref="S2.p1.3.m2.1.1.cmml">t</mi><mo maxsize="90%" minsize="90%" id="S2.p1.3.m2.1.2.3.2.2" xref="S2.p1.3.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.3.m2.1b"><apply id="S2.p1.3.m2.1.2.cmml" xref="S2.p1.3.m2.1.2"><times id="S2.p1.3.m2.1.2.1.cmml" xref="S2.p1.3.m2.1.2.1"></times><ci id="S2.p1.3.m2.1.2.2.cmml" xref="S2.p1.3.m2.1.2.2">ğ‘ </ci><ci id="S2.p1.3.m2.1.1.cmml" xref="S2.p1.3.m2.1.1">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m2.1c">s(t)</annotation></semantics></math><span id="S2.p1.5.3" class="ltx_text" style="font-size:90%;"> the anechoic audio signal, with </span><math id="S2.p1.4.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.p1.4.m3.1a"><mi mathsize="90%" id="S2.p1.4.m3.1.1" xref="S2.p1.4.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.p1.4.m3.1b"><ci id="S2.p1.4.m3.1.1.cmml" xref="S2.p1.4.m3.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m3.1c">t</annotation></semantics></math><span id="S2.p1.5.4" class="ltx_text" style="font-size:90%;"> the time index. An array of </span><math id="S2.p1.5.m4.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S2.p1.5.m4.1a"><mi mathsize="90%" id="S2.p1.5.m4.1.1" xref="S2.p1.5.m4.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.p1.5.m4.1b"><ci id="S2.p1.5.m4.1.1.cmml" xref="S2.p1.5.m4.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m4.1c">C</annotation></semantics></math><span id="S2.p1.5.5" class="ltx_text" style="font-size:90%;"> microphones captures the audio signal after propagating in the acoustic environment. The signals, as captured by the microphones, are given by:</span></p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.7" class="ltx_Math" alttext="y_{i}(t)=\{s*h_{i}\}(t),\;i=1,2,\ldots,C," display="block"><semantics id="S2.E2.m1.7a"><mrow id="S2.E2.m1.7.7.1"><mrow id="S2.E2.m1.7.7.1.1.2" xref="S2.E2.m1.7.7.1.1.3.cmml"><mrow id="S2.E2.m1.7.7.1.1.1.1" xref="S2.E2.m1.7.7.1.1.1.1.cmml"><mrow id="S2.E2.m1.7.7.1.1.1.1.3" xref="S2.E2.m1.7.7.1.1.1.1.3.cmml"><msub id="S2.E2.m1.7.7.1.1.1.1.3.2" xref="S2.E2.m1.7.7.1.1.1.1.3.2.cmml"><mi mathsize="90%" id="S2.E2.m1.7.7.1.1.1.1.3.2.2" xref="S2.E2.m1.7.7.1.1.1.1.3.2.2.cmml">y</mi><mi mathsize="90%" id="S2.E2.m1.7.7.1.1.1.1.3.2.3" xref="S2.E2.m1.7.7.1.1.1.1.3.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E2.m1.7.7.1.1.1.1.3.1" xref="S2.E2.m1.7.7.1.1.1.1.3.1.cmml">â€‹</mo><mrow id="S2.E2.m1.7.7.1.1.1.1.3.3.2" xref="S2.E2.m1.7.7.1.1.1.1.3.cmml"><mo maxsize="90%" minsize="90%" id="S2.E2.m1.7.7.1.1.1.1.3.3.2.1" xref="S2.E2.m1.7.7.1.1.1.1.3.cmml">(</mo><mi mathsize="90%" id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">t</mi><mo maxsize="90%" minsize="90%" id="S2.E2.m1.7.7.1.1.1.1.3.3.2.2" xref="S2.E2.m1.7.7.1.1.1.1.3.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S2.E2.m1.7.7.1.1.1.1.2" xref="S2.E2.m1.7.7.1.1.1.1.2.cmml">=</mo><mrow id="S2.E2.m1.7.7.1.1.1.1.1" xref="S2.E2.m1.7.7.1.1.1.1.1.cmml"><mrow id="S2.E2.m1.7.7.1.1.1.1.1.1.1" xref="S2.E2.m1.7.7.1.1.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S2.E2.m1.7.7.1.1.1.1.1.1.1.2" xref="S2.E2.m1.7.7.1.1.1.1.1.1.2.cmml">{</mo><mrow id="S2.E2.m1.7.7.1.1.1.1.1.1.1.1" xref="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.2.cmml">s</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.1.cmml">âˆ—</mo><msub id="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.3.2.cmml">h</mi><mi mathsize="90%" id="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo maxsize="90%" minsize="90%" id="S2.E2.m1.7.7.1.1.1.1.1.1.1.3" xref="S2.E2.m1.7.7.1.1.1.1.1.1.2.cmml">}</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E2.m1.7.7.1.1.1.1.1.2" xref="S2.E2.m1.7.7.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E2.m1.7.7.1.1.1.1.1.3.2" xref="S2.E2.m1.7.7.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S2.E2.m1.7.7.1.1.1.1.1.3.2.1" xref="S2.E2.m1.7.7.1.1.1.1.1.cmml">(</mo><mi mathsize="90%" id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">t</mi><mo maxsize="90%" minsize="90%" id="S2.E2.m1.7.7.1.1.1.1.1.3.2.2" xref="S2.E2.m1.7.7.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo mathsize="90%" rspace="0.447em" id="S2.E2.m1.7.7.1.1.2.3" xref="S2.E2.m1.7.7.1.1.3a.cmml">,</mo><mrow id="S2.E2.m1.7.7.1.1.2.2" xref="S2.E2.m1.7.7.1.1.2.2.cmml"><mi mathsize="90%" id="S2.E2.m1.7.7.1.1.2.2.2" xref="S2.E2.m1.7.7.1.1.2.2.2.cmml">i</mi><mo mathsize="90%" id="S2.E2.m1.7.7.1.1.2.2.1" xref="S2.E2.m1.7.7.1.1.2.2.1.cmml">=</mo><mrow id="S2.E2.m1.7.7.1.1.2.2.3.2" xref="S2.E2.m1.7.7.1.1.2.2.3.1.cmml"><mn mathsize="90%" id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml">1</mn><mo mathsize="90%" id="S2.E2.m1.7.7.1.1.2.2.3.2.1" xref="S2.E2.m1.7.7.1.1.2.2.3.1.cmml">,</mo><mn mathsize="90%" id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml">2</mn><mo mathsize="90%" id="S2.E2.m1.7.7.1.1.2.2.3.2.2" xref="S2.E2.m1.7.7.1.1.2.2.3.1.cmml">,</mo><mi mathsize="90%" mathvariant="normal" id="S2.E2.m1.5.5" xref="S2.E2.m1.5.5.cmml">â€¦</mi><mo mathsize="90%" id="S2.E2.m1.7.7.1.1.2.2.3.2.3" xref="S2.E2.m1.7.7.1.1.2.2.3.1.cmml">,</mo><mi mathsize="90%" id="S2.E2.m1.6.6" xref="S2.E2.m1.6.6.cmml">C</mi></mrow></mrow></mrow><mo mathsize="90%" id="S2.E2.m1.7.7.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.7b"><apply id="S2.E2.m1.7.7.1.1.3.cmml" xref="S2.E2.m1.7.7.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.7.7.1.1.3a.cmml" xref="S2.E2.m1.7.7.1.1.2.3">formulae-sequence</csymbol><apply id="S2.E2.m1.7.7.1.1.1.1.cmml" xref="S2.E2.m1.7.7.1.1.1.1"><eq id="S2.E2.m1.7.7.1.1.1.1.2.cmml" xref="S2.E2.m1.7.7.1.1.1.1.2"></eq><apply id="S2.E2.m1.7.7.1.1.1.1.3.cmml" xref="S2.E2.m1.7.7.1.1.1.1.3"><times id="S2.E2.m1.7.7.1.1.1.1.3.1.cmml" xref="S2.E2.m1.7.7.1.1.1.1.3.1"></times><apply id="S2.E2.m1.7.7.1.1.1.1.3.2.cmml" xref="S2.E2.m1.7.7.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.7.7.1.1.1.1.3.2.1.cmml" xref="S2.E2.m1.7.7.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E2.m1.7.7.1.1.1.1.3.2.2.cmml" xref="S2.E2.m1.7.7.1.1.1.1.3.2.2">ğ‘¦</ci><ci id="S2.E2.m1.7.7.1.1.1.1.3.2.3.cmml" xref="S2.E2.m1.7.7.1.1.1.1.3.2.3">ğ‘–</ci></apply><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">ğ‘¡</ci></apply><apply id="S2.E2.m1.7.7.1.1.1.1.1.cmml" xref="S2.E2.m1.7.7.1.1.1.1.1"><times id="S2.E2.m1.7.7.1.1.1.1.1.2.cmml" xref="S2.E2.m1.7.7.1.1.1.1.1.2"></times><set id="S2.E2.m1.7.7.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.7.7.1.1.1.1.1.1.1"><apply id="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.7.7.1.1.1.1.1.1.1.1"><times id="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.1"></times><ci id="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.2">ğ‘ </ci><apply id="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.3.2">â„</ci><ci id="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.7.7.1.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply></set><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">ğ‘¡</ci></apply></apply><apply id="S2.E2.m1.7.7.1.1.2.2.cmml" xref="S2.E2.m1.7.7.1.1.2.2"><eq id="S2.E2.m1.7.7.1.1.2.2.1.cmml" xref="S2.E2.m1.7.7.1.1.2.2.1"></eq><ci id="S2.E2.m1.7.7.1.1.2.2.2.cmml" xref="S2.E2.m1.7.7.1.1.2.2.2">ğ‘–</ci><list id="S2.E2.m1.7.7.1.1.2.2.3.1.cmml" xref="S2.E2.m1.7.7.1.1.2.2.3.2"><cn type="integer" id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">1</cn><cn type="integer" id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.4">2</cn><ci id="S2.E2.m1.5.5.cmml" xref="S2.E2.m1.5.5">â€¦</ci><ci id="S2.E2.m1.6.6.cmml" xref="S2.E2.m1.6.6">ğ¶</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.7c">y_{i}(t)=\{s*h_{i}\}(t),\;i=1,2,\ldots,C,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.p1.9" class="ltx_p"><span id="S2.p1.9.1" class="ltx_text" style="font-size:90%;">where </span><math id="S2.p1.6.m1.6" class="ltx_Math" alttext="h_{i}(t),\;i=1,2,\ldots,C," display="inline"><semantics id="S2.p1.6.m1.6a"><mrow id="S2.p1.6.m1.6.6.1"><mrow id="S2.p1.6.m1.6.6.1.1.2" xref="S2.p1.6.m1.6.6.1.1.3.cmml"><mrow id="S2.p1.6.m1.6.6.1.1.1.1" xref="S2.p1.6.m1.6.6.1.1.1.1.cmml"><mrow id="S2.p1.6.m1.6.6.1.1.1.1.1.1" xref="S2.p1.6.m1.6.6.1.1.1.1.1.2.cmml"><mrow id="S2.p1.6.m1.6.6.1.1.1.1.1.1.1" xref="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.cmml"><msub id="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.2" xref="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.2.cmml"><mi mathsize="90%" id="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.2.2" xref="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.2.2.cmml">h</mi><mi mathsize="90%" id="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.2.3" xref="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.1" xref="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.1.cmml">â€‹</mo><mrow id="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.3.2" xref="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.3.2.1" xref="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.cmml">(</mo><mi mathsize="90%" id="S2.p1.6.m1.1.1" xref="S2.p1.6.m1.1.1.cmml">t</mi><mo maxsize="90%" minsize="90%" id="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.3.2.2" xref="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathsize="90%" rspace="0.447em" id="S2.p1.6.m1.6.6.1.1.1.1.1.1.2" xref="S2.p1.6.m1.6.6.1.1.1.1.1.2.cmml">,</mo><mi mathsize="90%" id="S2.p1.6.m1.2.2" xref="S2.p1.6.m1.2.2.cmml">i</mi></mrow><mo mathsize="90%" id="S2.p1.6.m1.6.6.1.1.1.1.2" xref="S2.p1.6.m1.6.6.1.1.1.1.2.cmml">=</mo><mn mathsize="90%" id="S2.p1.6.m1.6.6.1.1.1.1.3" xref="S2.p1.6.m1.6.6.1.1.1.1.3.cmml">1</mn></mrow><mo mathsize="90%" id="S2.p1.6.m1.6.6.1.1.2.3" xref="S2.p1.6.m1.6.6.1.1.3a.cmml">,</mo><mrow id="S2.p1.6.m1.6.6.1.1.2.2.2" xref="S2.p1.6.m1.6.6.1.1.2.2.1.cmml"><mn mathsize="90%" id="S2.p1.6.m1.3.3" xref="S2.p1.6.m1.3.3.cmml">2</mn><mo mathsize="90%" id="S2.p1.6.m1.6.6.1.1.2.2.2.1" xref="S2.p1.6.m1.6.6.1.1.2.2.1.cmml">,</mo><mi mathsize="90%" mathvariant="normal" id="S2.p1.6.m1.4.4" xref="S2.p1.6.m1.4.4.cmml">â€¦</mi><mo mathsize="90%" id="S2.p1.6.m1.6.6.1.1.2.2.2.2" xref="S2.p1.6.m1.6.6.1.1.2.2.1.cmml">,</mo><mi mathsize="90%" id="S2.p1.6.m1.5.5" xref="S2.p1.6.m1.5.5.cmml">C</mi></mrow></mrow><mo mathsize="90%" id="S2.p1.6.m1.6.6.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.6.m1.6b"><apply id="S2.p1.6.m1.6.6.1.1.3.cmml" xref="S2.p1.6.m1.6.6.1.1.2"><csymbol cd="ambiguous" id="S2.p1.6.m1.6.6.1.1.3a.cmml" xref="S2.p1.6.m1.6.6.1.1.2.3">formulae-sequence</csymbol><apply id="S2.p1.6.m1.6.6.1.1.1.1.cmml" xref="S2.p1.6.m1.6.6.1.1.1.1"><eq id="S2.p1.6.m1.6.6.1.1.1.1.2.cmml" xref="S2.p1.6.m1.6.6.1.1.1.1.2"></eq><list id="S2.p1.6.m1.6.6.1.1.1.1.1.2.cmml" xref="S2.p1.6.m1.6.6.1.1.1.1.1.1"><apply id="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S2.p1.6.m1.6.6.1.1.1.1.1.1.1"><times id="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.1.cmml" xref="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.1"></times><apply id="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.2.cmml" xref="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.2.1.cmml" xref="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.2.2.cmml" xref="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.2.2">â„</ci><ci id="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.2.3.cmml" xref="S2.p1.6.m1.6.6.1.1.1.1.1.1.1.2.3">ğ‘–</ci></apply><ci id="S2.p1.6.m1.1.1.cmml" xref="S2.p1.6.m1.1.1">ğ‘¡</ci></apply><ci id="S2.p1.6.m1.2.2.cmml" xref="S2.p1.6.m1.2.2">ğ‘–</ci></list><cn type="integer" id="S2.p1.6.m1.6.6.1.1.1.1.3.cmml" xref="S2.p1.6.m1.6.6.1.1.1.1.3">1</cn></apply><list id="S2.p1.6.m1.6.6.1.1.2.2.1.cmml" xref="S2.p1.6.m1.6.6.1.1.2.2.2"><cn type="integer" id="S2.p1.6.m1.3.3.cmml" xref="S2.p1.6.m1.3.3">2</cn><ci id="S2.p1.6.m1.4.4.cmml" xref="S2.p1.6.m1.4.4">â€¦</ci><ci id="S2.p1.6.m1.5.5.cmml" xref="S2.p1.6.m1.5.5">ğ¶</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m1.6c">h_{i}(t),\;i=1,2,\ldots,C,</annotation></semantics></math><span id="S2.p1.9.2" class="ltx_text" style="font-size:90%;"> are the </span><abbr title="Room Impulse Responses" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RIRs</span></abbr><span id="S2.p1.9.3" class="ltx_text" style="font-size:90%;"> from the source to the </span><math id="S2.p1.7.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.p1.7.m2.1a"><mi mathsize="90%" id="S2.p1.7.m2.1.1" xref="S2.p1.7.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.p1.7.m2.1b"><ci id="S2.p1.7.m2.1.1.cmml" xref="S2.p1.7.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.7.m2.1c">i</annotation></semantics></math><span id="S2.p1.9.4" class="ltx_text" style="font-size:90%;">th microphone. The feature embeddings for each modality are denoted </span><math id="S2.p1.8.m3.1" class="ltx_Math" alttext="f_{v}" display="inline"><semantics id="S2.p1.8.m3.1a"><msub id="S2.p1.8.m3.1.1" xref="S2.p1.8.m3.1.1.cmml"><mi mathsize="90%" id="S2.p1.8.m3.1.1.2" xref="S2.p1.8.m3.1.1.2.cmml">f</mi><mi mathsize="90%" id="S2.p1.8.m3.1.1.3" xref="S2.p1.8.m3.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.8.m3.1b"><apply id="S2.p1.8.m3.1.1.cmml" xref="S2.p1.8.m3.1.1"><csymbol cd="ambiguous" id="S2.p1.8.m3.1.1.1.cmml" xref="S2.p1.8.m3.1.1">subscript</csymbol><ci id="S2.p1.8.m3.1.1.2.cmml" xref="S2.p1.8.m3.1.1.2">ğ‘“</ci><ci id="S2.p1.8.m3.1.1.3.cmml" xref="S2.p1.8.m3.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.8.m3.1c">f_{v}</annotation></semantics></math><span id="S2.p1.9.5" class="ltx_text" style="font-size:90%;"> and </span><math id="S2.p1.9.m4.1" class="ltx_Math" alttext="f_{s}" display="inline"><semantics id="S2.p1.9.m4.1a"><msub id="S2.p1.9.m4.1.1" xref="S2.p1.9.m4.1.1.cmml"><mi mathsize="90%" id="S2.p1.9.m4.1.1.2" xref="S2.p1.9.m4.1.1.2.cmml">f</mi><mi mathsize="90%" id="S2.p1.9.m4.1.1.3" xref="S2.p1.9.m4.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.9.m4.1b"><apply id="S2.p1.9.m4.1.1.cmml" xref="S2.p1.9.m4.1.1"><csymbol cd="ambiguous" id="S2.p1.9.m4.1.1.1.cmml" xref="S2.p1.9.m4.1.1">subscript</csymbol><ci id="S2.p1.9.m4.1.1.2.cmml" xref="S2.p1.9.m4.1.1.2">ğ‘“</ci><ci id="S2.p1.9.m4.1.1.3.cmml" xref="S2.p1.9.m4.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.9.m4.1c">f_{s}</annotation></semantics></math><span id="S2.p1.9.6" class="ltx_text" style="font-size:90%;">, respectively. This study aims to classify the utterance to one of the emotions using the available information and utilizing the relations between the feature embeddings of both modalities:</span></p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.3" class="ltx_Math" alttext="M\left\{v(t),\{y_{i}(t)\}_{i=1}^{C}\right\}\Rightarrow f_{v}\oplus f_{s}\Rightarrow E," display="block"><semantics id="S2.E3.m1.3a"><mrow id="S2.E3.m1.3.3.1" xref="S2.E3.m1.3.3.1.1.cmml"><mrow id="S2.E3.m1.3.3.1.1" xref="S2.E3.m1.3.3.1.1.cmml"><mrow id="S2.E3.m1.3.3.1.1.2" xref="S2.E3.m1.3.3.1.1.2.cmml"><mi mathsize="90%" id="S2.E3.m1.3.3.1.1.2.4" xref="S2.E3.m1.3.3.1.1.2.4.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.1.1.2.3" xref="S2.E3.m1.3.3.1.1.2.3.cmml">â€‹</mo><mrow id="S2.E3.m1.3.3.1.1.2.2.2" xref="S2.E3.m1.3.3.1.1.2.2.3.cmml"><mo id="S2.E3.m1.3.3.1.1.2.2.2.3" xref="S2.E3.m1.3.3.1.1.2.2.3.cmml">{</mo><mrow id="S2.E3.m1.3.3.1.1.1.1.1.1" xref="S2.E3.m1.3.3.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S2.E3.m1.3.3.1.1.1.1.1.1.2" xref="S2.E3.m1.3.3.1.1.1.1.1.1.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.1.1.1.1.1.1.1" xref="S2.E3.m1.3.3.1.1.1.1.1.1.1.cmml">â€‹</mo><mrow id="S2.E3.m1.3.3.1.1.1.1.1.1.3.2" xref="S2.E3.m1.3.3.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S2.E3.m1.3.3.1.1.1.1.1.1.3.2.1" xref="S2.E3.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mi mathsize="90%" id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml">t</mi><mo maxsize="90%" minsize="90%" id="S2.E3.m1.3.3.1.1.1.1.1.1.3.2.2" xref="S2.E3.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S2.E3.m1.3.3.1.1.2.2.2.4" xref="S2.E3.m1.3.3.1.1.2.2.3.cmml">,</mo><msubsup id="S2.E3.m1.3.3.1.1.2.2.2.2" xref="S2.E3.m1.3.3.1.1.2.2.2.2.cmml"><mrow id="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.2" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.2.cmml">{</mo><mrow id="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.cmml"><msub id="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.2" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.2.cmml"><mi mathsize="90%" id="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.2.2" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.2.2.cmml">y</mi><mi mathsize="90%" id="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.2.3" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.1" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.1.cmml">â€‹</mo><mrow id="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.3.2" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.3.2.1" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.cmml">(</mo><mi mathsize="90%" id="S2.E3.m1.2.2" xref="S2.E3.m1.2.2.cmml">t</mi><mo maxsize="90%" minsize="90%" id="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.3.2.2" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo maxsize="90%" minsize="90%" id="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.3" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.2.cmml">}</mo></mrow><mrow id="S2.E3.m1.3.3.1.1.2.2.2.2.1.3" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.3.cmml"><mi mathsize="90%" id="S2.E3.m1.3.3.1.1.2.2.2.2.1.3.2" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.3.2.cmml">i</mi><mo mathsize="90%" id="S2.E3.m1.3.3.1.1.2.2.2.2.1.3.1" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.3.1.cmml">=</mo><mn mathsize="90%" id="S2.E3.m1.3.3.1.1.2.2.2.2.1.3.3" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.3.3.cmml">1</mn></mrow><mi mathsize="90%" id="S2.E3.m1.3.3.1.1.2.2.2.2.3" xref="S2.E3.m1.3.3.1.1.2.2.2.2.3.cmml">C</mi></msubsup><mo id="S2.E3.m1.3.3.1.1.2.2.2.5" xref="S2.E3.m1.3.3.1.1.2.2.3.cmml">}</mo></mrow></mrow><mo mathsize="90%" stretchy="false" id="S2.E3.m1.3.3.1.1.4" xref="S2.E3.m1.3.3.1.1.4.cmml">â‡’</mo><mrow id="S2.E3.m1.3.3.1.1.5" xref="S2.E3.m1.3.3.1.1.5.cmml"><msub id="S2.E3.m1.3.3.1.1.5.2" xref="S2.E3.m1.3.3.1.1.5.2.cmml"><mi mathsize="90%" id="S2.E3.m1.3.3.1.1.5.2.2" xref="S2.E3.m1.3.3.1.1.5.2.2.cmml">f</mi><mi mathsize="90%" id="S2.E3.m1.3.3.1.1.5.2.3" xref="S2.E3.m1.3.3.1.1.5.2.3.cmml">v</mi></msub><mo mathsize="90%" id="S2.E3.m1.3.3.1.1.5.1" xref="S2.E3.m1.3.3.1.1.5.1.cmml">âŠ•</mo><msub id="S2.E3.m1.3.3.1.1.5.3" xref="S2.E3.m1.3.3.1.1.5.3.cmml"><mi mathsize="90%" id="S2.E3.m1.3.3.1.1.5.3.2" xref="S2.E3.m1.3.3.1.1.5.3.2.cmml">f</mi><mi mathsize="90%" id="S2.E3.m1.3.3.1.1.5.3.3" xref="S2.E3.m1.3.3.1.1.5.3.3.cmml">s</mi></msub></mrow><mo mathsize="90%" stretchy="false" id="S2.E3.m1.3.3.1.1.6" xref="S2.E3.m1.3.3.1.1.6.cmml">â‡’</mo><mi mathsize="90%" id="S2.E3.m1.3.3.1.1.7" xref="S2.E3.m1.3.3.1.1.7.cmml">E</mi></mrow><mo mathsize="90%" id="S2.E3.m1.3.3.1.2" xref="S2.E3.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.3b"><apply id="S2.E3.m1.3.3.1.1.cmml" xref="S2.E3.m1.3.3.1"><and id="S2.E3.m1.3.3.1.1a.cmml" xref="S2.E3.m1.3.3.1"></and><apply id="S2.E3.m1.3.3.1.1b.cmml" xref="S2.E3.m1.3.3.1"><ci id="S2.E3.m1.3.3.1.1.4.cmml" xref="S2.E3.m1.3.3.1.1.4">â‡’</ci><apply id="S2.E3.m1.3.3.1.1.2.cmml" xref="S2.E3.m1.3.3.1.1.2"><times id="S2.E3.m1.3.3.1.1.2.3.cmml" xref="S2.E3.m1.3.3.1.1.2.3"></times><ci id="S2.E3.m1.3.3.1.1.2.4.cmml" xref="S2.E3.m1.3.3.1.1.2.4">ğ‘€</ci><set id="S2.E3.m1.3.3.1.1.2.2.3.cmml" xref="S2.E3.m1.3.3.1.1.2.2.2"><apply id="S2.E3.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E3.m1.3.3.1.1.1.1.1.1"><times id="S2.E3.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.3.3.1.1.1.1.1.1.1"></times><ci id="S2.E3.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.3.3.1.1.1.1.1.1.2">ğ‘£</ci><ci id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1">ğ‘¡</ci></apply><apply id="S2.E3.m1.3.3.1.1.2.2.2.2.cmml" xref="S2.E3.m1.3.3.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.1.1.2.2.2.2.2.cmml" xref="S2.E3.m1.3.3.1.1.2.2.2.2">superscript</csymbol><apply id="S2.E3.m1.3.3.1.1.2.2.2.2.1.cmml" xref="S2.E3.m1.3.3.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.1.1.2.2.2.2.1.2.cmml" xref="S2.E3.m1.3.3.1.1.2.2.2.2">subscript</csymbol><set id="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.2.cmml" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1"><apply id="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.cmml" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1"><times id="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.1.cmml" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.1"></times><apply id="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.2.cmml" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.2.1.cmml" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.2">subscript</csymbol><ci id="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.2.2.cmml" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.2.2">ğ‘¦</ci><ci id="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.2.3.cmml" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.1.1.1.2.3">ğ‘–</ci></apply><ci id="S2.E3.m1.2.2.cmml" xref="S2.E3.m1.2.2">ğ‘¡</ci></apply></set><apply id="S2.E3.m1.3.3.1.1.2.2.2.2.1.3.cmml" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.3"><eq id="S2.E3.m1.3.3.1.1.2.2.2.2.1.3.1.cmml" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.3.1"></eq><ci id="S2.E3.m1.3.3.1.1.2.2.2.2.1.3.2.cmml" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.3.2">ğ‘–</ci><cn type="integer" id="S2.E3.m1.3.3.1.1.2.2.2.2.1.3.3.cmml" xref="S2.E3.m1.3.3.1.1.2.2.2.2.1.3.3">1</cn></apply></apply><ci id="S2.E3.m1.3.3.1.1.2.2.2.2.3.cmml" xref="S2.E3.m1.3.3.1.1.2.2.2.2.3">ğ¶</ci></apply></set></apply><apply id="S2.E3.m1.3.3.1.1.5.cmml" xref="S2.E3.m1.3.3.1.1.5"><csymbol cd="latexml" id="S2.E3.m1.3.3.1.1.5.1.cmml" xref="S2.E3.m1.3.3.1.1.5.1">direct-sum</csymbol><apply id="S2.E3.m1.3.3.1.1.5.2.cmml" xref="S2.E3.m1.3.3.1.1.5.2"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.1.1.5.2.1.cmml" xref="S2.E3.m1.3.3.1.1.5.2">subscript</csymbol><ci id="S2.E3.m1.3.3.1.1.5.2.2.cmml" xref="S2.E3.m1.3.3.1.1.5.2.2">ğ‘“</ci><ci id="S2.E3.m1.3.3.1.1.5.2.3.cmml" xref="S2.E3.m1.3.3.1.1.5.2.3">ğ‘£</ci></apply><apply id="S2.E3.m1.3.3.1.1.5.3.cmml" xref="S2.E3.m1.3.3.1.1.5.3"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.1.1.5.3.1.cmml" xref="S2.E3.m1.3.3.1.1.5.3">subscript</csymbol><ci id="S2.E3.m1.3.3.1.1.5.3.2.cmml" xref="S2.E3.m1.3.3.1.1.5.3.2">ğ‘“</ci><ci id="S2.E3.m1.3.3.1.1.5.3.3.cmml" xref="S2.E3.m1.3.3.1.1.5.3.3">ğ‘ </ci></apply></apply></apply><apply id="S2.E3.m1.3.3.1.1c.cmml" xref="S2.E3.m1.3.3.1"><ci id="S2.E3.m1.3.3.1.1.6.cmml" xref="S2.E3.m1.3.3.1.1.6">â‡’</ci><share href="#S2.E3.m1.3.3.1.1.5.cmml" id="S2.E3.m1.3.3.1.1d.cmml" xref="S2.E3.m1.3.3.1"></share><ci id="S2.E3.m1.3.3.1.1.7.cmml" xref="S2.E3.m1.3.3.1.1.7">ğ¸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.3c">M\left\{v(t),\{y_{i}(t)\}_{i=1}^{C}\right\}\Rightarrow f_{v}\oplus f_{s}\Rightarrow E,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S2.p1.10" class="ltx_p"><span id="S2.p1.10.1" class="ltx_text" style="font-size:90%;">where </span><math id="S2.p1.10.m1.1" class="ltx_Math" alttext="\oplus" display="inline"><semantics id="S2.p1.10.m1.1a"><mo mathsize="90%" id="S2.p1.10.m1.1.1" xref="S2.p1.10.m1.1.1.cmml">âŠ•</mo><annotation-xml encoding="MathML-Content" id="S2.p1.10.m1.1b"><csymbol cd="latexml" id="S2.p1.10.m1.1.1.cmml" xref="S2.p1.10.m1.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.10.m1.1c">\oplus</annotation></semantics></math><span id="S2.p1.10.2" class="ltx_text" style="font-size:90%;"> stands for late fusion concatenation.</span></p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2409.09545/assets/MER.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="650" height="224" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.6.1.1" class="ltx_text ltx_font_bold">Fig.Â 1</span>: </span>The proposed  <span title="Multi-modal Emotion Recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Multi-modal Emotion Recognition</span></span> (<abbr title="Multi-modal Emotion Recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MER</span></abbr>).</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Model</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p"><span id="S3.p1.1.1" class="ltx_text" style="font-size:90%;">Our proposed </span><abbr title="Multi-modal Emotion Recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MER</span></abbr><span id="S3.p1.1.2" class="ltx_text" style="font-size:90%;"> architecture leverages two powerful models: the modified and extended </span><abbr title="Hierarchical Token-semantic Audio Transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HTS-AT</span></abbr><span id="S3.p1.1.3" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S3.p1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p1.1.6" class="ltx_text" style="font-size:90%;"> for multi-channel audio processing and the </span><math id="S3.p1.1.m1.1" class="ltx_Math" alttext="R(2+1)" display="inline"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mi mathsize="90%" id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">â€‹</mo><mrow id="S3.p1.1.m1.1.1.1.1" xref="S3.p1.1.m1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.p1.1.m1.1.1.1.1.2" xref="S3.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.p1.1.m1.1.1.1.1.1" xref="S3.p1.1.m1.1.1.1.1.1.cmml"><mn mathsize="90%" id="S3.p1.1.m1.1.1.1.1.1.2" xref="S3.p1.1.m1.1.1.1.1.1.2.cmml">2</mn><mo mathsize="90%" id="S3.p1.1.m1.1.1.1.1.1.1" xref="S3.p1.1.m1.1.1.1.1.1.1.cmml">+</mo><mn mathsize="90%" id="S3.p1.1.m1.1.1.1.1.1.3" xref="S3.p1.1.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo maxsize="90%" minsize="90%" id="S3.p1.1.m1.1.1.1.1.3" xref="S3.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><times id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2"></times><ci id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3">ğ‘…</ci><apply id="S3.p1.1.m1.1.1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1.1"><plus id="S3.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1.1.1.1"></plus><cn type="integer" id="S3.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.1.1.1.2">2</cn><cn type="integer" id="S3.p1.1.m1.1.1.1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">R(2+1)</annotation></semantics></math><span id="S3.p1.1.7" class="ltx_text" style="font-size:90%;">D model </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p1.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S3.p1.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p1.1.10" class="ltx_text" style="font-size:90%;"> for video analysis. These models are uni-modal models that are combined to create a robust multi-modal system for emotion recognition in adverse acoustic conditions.</span></p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text" style="font-size:90%;">The input features of the models are the mel-spectrograms for the audio track and the raw RGB facial images for the visual track. For the audio modality, we followed the same preprocessing procedure as in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S3.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p2.1.4" class="ltx_text" style="font-size:90%;"> and used the SpecAugment Library </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p2.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S3.p2.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p2.1.7" class="ltx_text" style="font-size:90%;"> to augment the mel-spectrograms. For augmenting the video modality, we used the TorchVision Library </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p2.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S3.p2.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p2.1.10" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Audio:</span><span id="S3.p3.1.2" class="ltx_text" style="font-size:90%;"> The extended multi-channel </span><abbr title="Hierarchical Token-semantic Audio Transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HTS-AT</span></abbr><span id="S3.p3.1.3" class="ltx_text" style="font-size:90%;"> model addresses the integration of multi-microphone information, employing the Swin-Transformer architecture </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p3.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S3.p3.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p3.1.6" class="ltx_text" style="font-size:90%;">,</span><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span id="footnote1.1" class="ltx_text ltx_font_typewriter">github.com/microsoft/Swin-Transformer</span></span></span></span><span id="S3.p3.1.7" class="ltx_text" style="font-size:90%;"> a variant of the  </span><span title="Vision Transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Vision Transformer</span></span><span id="S3.p3.1.8" class="ltx_text" style="font-size:90%;"> (</span><abbr title="Vision Transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ViT</span></abbr><span id="S3.p3.1.9" class="ltx_text" style="font-size:90%;">) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p3.1.10.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S3.p3.1.11.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p3.1.12" class="ltx_text" style="font-size:90%;"> architecture. The architecture is also applicable to the single-microphone configurations, namely </span><math id="S3.p3.1.m1.1" class="ltx_Math" alttext="C=1" display="inline"><semantics id="S3.p3.1.m1.1a"><mrow id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><mi mathsize="90%" id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml">C</mi><mo mathsize="90%" id="S3.p3.1.m1.1.1.1" xref="S3.p3.1.m1.1.1.1.cmml">=</mo><mn mathsize="90%" id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><eq id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.1.1"></eq><ci id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2">ğ¶</ci><cn type="integer" id="S3.p3.1.m1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">C=1</annotation></semantics></math><span id="S3.p3.1.13" class="ltx_text" style="font-size:90%;">. The modelâ€™s architecture consists of four networks, each made up of Swin-Transformer blocks with varying depths. In addition, the model uses a hierarchical structure and windowed attention mechanism to efficiently process mel-spectrograms, which serve as our audio feature extractor. We use the two multi-channel variants with the modified </span><abbr title="Hierarchical Token-semantic Audio Transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HTS-AT</span></abbr><span id="S3.p3.1.14" class="ltx_text" style="font-size:90%;"> module, as proposed in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p3.1.15.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S3.p3.1.16.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p3.1.17" class="ltx_text" style="font-size:90%;">: 1) Patch-Embed Summation - the mel-spectrogram of each channel is processed through a shared Patch-Embed layer, after which the outputs are summed across channels; and 2) Average mel-spectrograms - mel-spectrograms from multiple channels are averaged before being fed into the model. More details can be found in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p3.1.18.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S3.p3.1.19.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p3.1.20" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.2" class="ltx_p"><span id="S3.p4.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Video:</span><span id="S3.p4.2.2" class="ltx_text" style="font-size:90%;"> For video feature extraction, we employ the pre-trained </span><math id="S3.p4.1.m1.1" class="ltx_Math" alttext="R(2+1)" display="inline"><semantics id="S3.p4.1.m1.1a"><mrow id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml"><mi mathsize="90%" id="S3.p4.1.m1.1.1.3" xref="S3.p4.1.m1.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.p4.1.m1.1.1.2" xref="S3.p4.1.m1.1.1.2.cmml">â€‹</mo><mrow id="S3.p4.1.m1.1.1.1.1" xref="S3.p4.1.m1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.p4.1.m1.1.1.1.1.2" xref="S3.p4.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.p4.1.m1.1.1.1.1.1" xref="S3.p4.1.m1.1.1.1.1.1.cmml"><mn mathsize="90%" id="S3.p4.1.m1.1.1.1.1.1.2" xref="S3.p4.1.m1.1.1.1.1.1.2.cmml">2</mn><mo mathsize="90%" id="S3.p4.1.m1.1.1.1.1.1.1" xref="S3.p4.1.m1.1.1.1.1.1.1.cmml">+</mo><mn mathsize="90%" id="S3.p4.1.m1.1.1.1.1.1.3" xref="S3.p4.1.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo maxsize="90%" minsize="90%" id="S3.p4.1.m1.1.1.1.1.3" xref="S3.p4.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><apply id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1"><times id="S3.p4.1.m1.1.1.2.cmml" xref="S3.p4.1.m1.1.1.2"></times><ci id="S3.p4.1.m1.1.1.3.cmml" xref="S3.p4.1.m1.1.1.3">ğ‘…</ci><apply id="S3.p4.1.m1.1.1.1.1.1.cmml" xref="S3.p4.1.m1.1.1.1.1"><plus id="S3.p4.1.m1.1.1.1.1.1.1.cmml" xref="S3.p4.1.m1.1.1.1.1.1.1"></plus><cn type="integer" id="S3.p4.1.m1.1.1.1.1.1.2.cmml" xref="S3.p4.1.m1.1.1.1.1.1.2">2</cn><cn type="integer" id="S3.p4.1.m1.1.1.1.1.1.3.cmml" xref="S3.p4.1.m1.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">R(2+1)</annotation></semantics></math><span id="S3.p4.2.3" class="ltx_text" style="font-size:90%;">D model, an 18-layer ResNet-based architecture designed for action recognition. The </span><math id="S3.p4.2.m2.1" class="ltx_Math" alttext="R(2+1)" display="inline"><semantics id="S3.p4.2.m2.1a"><mrow id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml"><mi mathsize="90%" id="S3.p4.2.m2.1.1.3" xref="S3.p4.2.m2.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.p4.2.m2.1.1.2" xref="S3.p4.2.m2.1.1.2.cmml">â€‹</mo><mrow id="S3.p4.2.m2.1.1.1.1" xref="S3.p4.2.m2.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.p4.2.m2.1.1.1.1.2" xref="S3.p4.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.p4.2.m2.1.1.1.1.1" xref="S3.p4.2.m2.1.1.1.1.1.cmml"><mn mathsize="90%" id="S3.p4.2.m2.1.1.1.1.1.2" xref="S3.p4.2.m2.1.1.1.1.1.2.cmml">2</mn><mo mathsize="90%" id="S3.p4.2.m2.1.1.1.1.1.1" xref="S3.p4.2.m2.1.1.1.1.1.1.cmml">+</mo><mn mathsize="90%" id="S3.p4.2.m2.1.1.1.1.1.3" xref="S3.p4.2.m2.1.1.1.1.1.3.cmml">1</mn></mrow><mo maxsize="90%" minsize="90%" id="S3.p4.2.m2.1.1.1.1.3" xref="S3.p4.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><apply id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1"><times id="S3.p4.2.m2.1.1.2.cmml" xref="S3.p4.2.m2.1.1.2"></times><ci id="S3.p4.2.m2.1.1.3.cmml" xref="S3.p4.2.m2.1.1.3">ğ‘…</ci><apply id="S3.p4.2.m2.1.1.1.1.1.cmml" xref="S3.p4.2.m2.1.1.1.1"><plus id="S3.p4.2.m2.1.1.1.1.1.1.cmml" xref="S3.p4.2.m2.1.1.1.1.1.1"></plus><cn type="integer" id="S3.p4.2.m2.1.1.1.1.1.2.cmml" xref="S3.p4.2.m2.1.1.1.1.1.2">2</cn><cn type="integer" id="S3.p4.2.m2.1.1.1.1.1.3.cmml" xref="S3.p4.2.m2.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">R(2+1)</annotation></semantics></math><span id="S3.p4.2.4" class="ltx_text" style="font-size:90%;">D model decomposes the 3D convolutions into separate spatial (2D) and temporal (1D) convolutions, which allows it to effectively capture both spatial and temporal features in the video data. Fig.Â </span><a href="#S2.F1" title="Figure 1 â€£ 2 Problem Formulation â€£ Multi-Microphone and Multi-Modal Emotion Recognition in Reverberant Environment" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S3.p4.2.5" class="ltx_text" style="font-size:90%;"> presents the integration of the two modalities.</span></p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p"><span id="S3.p5.1.1" class="ltx_text" style="font-size:90%;">The feature embeddings are extracted from the extended multi-channel HTS-AT and the </span><math id="S3.p5.1.m1.1" class="ltx_Math" alttext="R(2+1)" display="inline"><semantics id="S3.p5.1.m1.1a"><mrow id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml"><mi mathsize="90%" id="S3.p5.1.m1.1.1.3" xref="S3.p5.1.m1.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.p5.1.m1.1.1.2" xref="S3.p5.1.m1.1.1.2.cmml">â€‹</mo><mrow id="S3.p5.1.m1.1.1.1.1" xref="S3.p5.1.m1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.p5.1.m1.1.1.1.1.2" xref="S3.p5.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.p5.1.m1.1.1.1.1.1" xref="S3.p5.1.m1.1.1.1.1.1.cmml"><mn mathsize="90%" id="S3.p5.1.m1.1.1.1.1.1.2" xref="S3.p5.1.m1.1.1.1.1.1.2.cmml">2</mn><mo mathsize="90%" id="S3.p5.1.m1.1.1.1.1.1.1" xref="S3.p5.1.m1.1.1.1.1.1.1.cmml">+</mo><mn mathsize="90%" id="S3.p5.1.m1.1.1.1.1.1.3" xref="S3.p5.1.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo maxsize="90%" minsize="90%" id="S3.p5.1.m1.1.1.1.1.3" xref="S3.p5.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><apply id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1"><times id="S3.p5.1.m1.1.1.2.cmml" xref="S3.p5.1.m1.1.1.2"></times><ci id="S3.p5.1.m1.1.1.3.cmml" xref="S3.p5.1.m1.1.1.3">ğ‘…</ci><apply id="S3.p5.1.m1.1.1.1.1.1.cmml" xref="S3.p5.1.m1.1.1.1.1"><plus id="S3.p5.1.m1.1.1.1.1.1.1.cmml" xref="S3.p5.1.m1.1.1.1.1.1.1"></plus><cn type="integer" id="S3.p5.1.m1.1.1.1.1.1.2.cmml" xref="S3.p5.1.m1.1.1.1.1.1.2">2</cn><cn type="integer" id="S3.p5.1.m1.1.1.1.1.1.3.cmml" xref="S3.p5.1.m1.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">R(2+1)</annotation></semantics></math><span id="S3.p5.1.2" class="ltx_text" style="font-size:90%;">D models, followed by concatenation to create a unified multi-modal representation. This combined feature vector captures both audio and visual cues relevant to emotion recognition. The concatenated features are then passed through two fully connected layers for final classification. These layers learn to interpret the combined audio-visual features and to map them to emotion categories. The output of the final layer corresponds to the predicted emotion class. This integrated scheme of both modalities ensures that both multi-channel audio and visual data are effectively processed and leveraged, allowing the model to capture and utilize complementary information from both modalities, thus achieving improved </span><abbr title="Emotion Recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ER</span></abbr><span id="S3.p5.1.3" class="ltx_text" style="font-size:90%;"> accuracy.</span></p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Study</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text" style="font-size:90%;">This section outlines the experimental setup and describes the comparative analysis between the proposed scheme and a baseline method.</span></p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">Our work utilized the </span><abbr title="Ryerson audio-visual database of emotional speech and song" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RAVDESS</span></abbr><span id="S4.SS1.p1.1.2" class="ltx_text" style="font-size:90%;"> dataset for emotion recognition. This dataset includes 24 actors, equally divided between male and female speakers, each delivering 60 English sentences. Hence, there are 1440 audio-video pairs representing eight different emotions (â€˜sad,â€™ â€˜happy,â€™ â€˜angry,â€™ â€˜calm,â€™ â€˜fearful,â€™ â€˜surprised,â€™ â€˜neutral,â€™ and â€˜disgustâ€™). All utterances are pre-transcribed. Therefore, the emotions are expressed more artificially compared to spontaneous conversation. The RAVDESS dataset is balanced across most classes, except for the neutral class, and has a relatively small number of utterances. We used an </span><span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_italic" style="font-size:90%;">actor-split</span><span id="S4.SS1.p1.1.4" class="ltx_text" style="font-size:90%;"> approach, dividing the data into 80% training, 10% validation, and 10% test sets, ensuring no actor appears in more than one split. As a result, model accuracy may be lower than reported in some prior works because the test set includes actors not seen during fine-tuning.</span></p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text" style="font-size:90%;">We used synthesized </span><abbr title="Room Impulse Responses" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RIR</span></abbr><span id="S4.SS1.p2.1.2" class="ltx_text" style="font-size:90%;">s for fine-tuning the model for the multi-channel experiments. We employed the â€˜gpuRIRâ€˜ Python package</span><span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span id="footnote2.1" class="ltx_text ltx_font_typewriter">github.com/DavidDiazGuerra/gpuRIR</span></span></span></span><span id="S4.SS1.p2.1.3" class="ltx_text" style="font-size:90%;"> to simulate reverberant multi-channel microphone signals (setting the number of microphones to </span><math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="C=3" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi mathsize="90%" id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">C</mi><mo mathsize="90%" id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">=</mo><mn mathsize="90%" id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><eq id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></eq><ci id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">ğ¶</ci><cn type="integer" id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">C=3</annotation></semantics></math><span id="S4.SS1.p2.1.4" class="ltx_text" style="font-size:90%;">). Each clean audio sample from the </span><abbr title="Ryerson audio-visual database of emotional speech and song" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RAVDESS</span></abbr><span id="S4.SS1.p2.1.5" class="ltx_text" style="font-size:90%;"> dataset was convolved with distinct multi-channel </span><abbr title="Room Impulse Responses" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RIRs</span></abbr><span id="S4.SS1.p2.1.6" class="ltx_text" style="font-size:90%;">, resulting in 1440 3-microphone audio samples. The associated video data is unaffected by reverberation.</span></p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.9" class="ltx_p"><span id="S4.SS1.p3.9.1" class="ltx_text" style="font-size:90%;">We simulated rooms with lengths and widths uniformly distributed between 3Â m and 8Â m, maintaining a constant height of 2.9Â m and an aspect ratio between 1 and 1.6. We randomly positioned the sound source and microphones within these simulated environments under the following constraints. The sound source was placed at a fixed height of 1.75Â m, with its </span><math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mi mathsize="90%" id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><ci id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">x</annotation></semantics></math><span id="S4.SS1.p3.9.2" class="ltx_text" style="font-size:90%;"> and </span><math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mi mathsize="90%" id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><ci id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">y</annotation></semantics></math><span id="S4.SS1.p3.9.3" class="ltx_text" style="font-size:90%;"> coordinates randomly determined within the room, ensuring a minimum distance of 0.5Â m from the room walls. Similarly, the microphones were positioned at a fixed height of 1.6Â m, with their </span><math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><mi mathsize="90%" id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><ci id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">x</annotation></semantics></math><span id="S4.SS1.p3.9.4" class="ltx_text" style="font-size:90%;"> and </span><math id="S4.SS1.p3.4.m4.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS1.p3.4.m4.1a"><mi mathsize="90%" id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><ci id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">y</annotation></semantics></math><span id="S4.SS1.p3.9.5" class="ltx_text" style="font-size:90%;"> coordinates also randomly determined within the room dimensions. The reverberation time was set at </span><math id="S4.SS1.p3.5.m5.1" class="ltx_Math" alttext="T_{60}=500-850" display="inline"><semantics id="S4.SS1.p3.5.m5.1a"><mrow id="S4.SS1.p3.5.m5.1.1" xref="S4.SS1.p3.5.m5.1.1.cmml"><msub id="S4.SS1.p3.5.m5.1.1.2" xref="S4.SS1.p3.5.m5.1.1.2.cmml"><mi mathsize="90%" id="S4.SS1.p3.5.m5.1.1.2.2" xref="S4.SS1.p3.5.m5.1.1.2.2.cmml">T</mi><mn mathsize="90%" id="S4.SS1.p3.5.m5.1.1.2.3" xref="S4.SS1.p3.5.m5.1.1.2.3.cmml">60</mn></msub><mo mathsize="90%" id="S4.SS1.p3.5.m5.1.1.1" xref="S4.SS1.p3.5.m5.1.1.1.cmml">=</mo><mrow id="S4.SS1.p3.5.m5.1.1.3" xref="S4.SS1.p3.5.m5.1.1.3.cmml"><mn mathsize="90%" id="S4.SS1.p3.5.m5.1.1.3.2" xref="S4.SS1.p3.5.m5.1.1.3.2.cmml">500</mn><mo mathsize="90%" id="S4.SS1.p3.5.m5.1.1.3.1" xref="S4.SS1.p3.5.m5.1.1.3.1.cmml">âˆ’</mo><mn mathsize="90%" id="S4.SS1.p3.5.m5.1.1.3.3" xref="S4.SS1.p3.5.m5.1.1.3.3.cmml">850</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.5.m5.1b"><apply id="S4.SS1.p3.5.m5.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1"><eq id="S4.SS1.p3.5.m5.1.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1.1"></eq><apply id="S4.SS1.p3.5.m5.1.1.2.cmml" xref="S4.SS1.p3.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p3.5.m5.1.1.2.1.cmml" xref="S4.SS1.p3.5.m5.1.1.2">subscript</csymbol><ci id="S4.SS1.p3.5.m5.1.1.2.2.cmml" xref="S4.SS1.p3.5.m5.1.1.2.2">ğ‘‡</ci><cn type="integer" id="S4.SS1.p3.5.m5.1.1.2.3.cmml" xref="S4.SS1.p3.5.m5.1.1.2.3">60</cn></apply><apply id="S4.SS1.p3.5.m5.1.1.3.cmml" xref="S4.SS1.p3.5.m5.1.1.3"><minus id="S4.SS1.p3.5.m5.1.1.3.1.cmml" xref="S4.SS1.p3.5.m5.1.1.3.1"></minus><cn type="integer" id="S4.SS1.p3.5.m5.1.1.3.2.cmml" xref="S4.SS1.p3.5.m5.1.1.3.2">500</cn><cn type="integer" id="S4.SS1.p3.5.m5.1.1.3.3.cmml" xref="S4.SS1.p3.5.m5.1.1.3.3">850</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.5.m5.1c">T_{60}=500-850</annotation></semantics></math><span id="S4.SS1.p3.9.6" class="ltx_text" style="font-size:90%;">Â ms. The distance between the sound source and microphones was randomly selected in the range </span><math id="S4.SS1.p3.6.m6.2" class="ltx_Math" alttext="[0.2,d_{c}]" display="inline"><semantics id="S4.SS1.p3.6.m6.2a"><mrow id="S4.SS1.p3.6.m6.2.2.1" xref="S4.SS1.p3.6.m6.2.2.2.cmml"><mo maxsize="90%" minsize="90%" id="S4.SS1.p3.6.m6.2.2.1.2" xref="S4.SS1.p3.6.m6.2.2.2.cmml">[</mo><mn mathsize="90%" id="S4.SS1.p3.6.m6.1.1" xref="S4.SS1.p3.6.m6.1.1.cmml">0.2</mn><mo mathsize="90%" id="S4.SS1.p3.6.m6.2.2.1.3" xref="S4.SS1.p3.6.m6.2.2.2.cmml">,</mo><msub id="S4.SS1.p3.6.m6.2.2.1.1" xref="S4.SS1.p3.6.m6.2.2.1.1.cmml"><mi mathsize="90%" id="S4.SS1.p3.6.m6.2.2.1.1.2" xref="S4.SS1.p3.6.m6.2.2.1.1.2.cmml">d</mi><mi mathsize="90%" id="S4.SS1.p3.6.m6.2.2.1.1.3" xref="S4.SS1.p3.6.m6.2.2.1.1.3.cmml">c</mi></msub><mo maxsize="90%" minsize="90%" id="S4.SS1.p3.6.m6.2.2.1.4" xref="S4.SS1.p3.6.m6.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.6.m6.2b"><interval closure="closed" id="S4.SS1.p3.6.m6.2.2.2.cmml" xref="S4.SS1.p3.6.m6.2.2.1"><cn type="float" id="S4.SS1.p3.6.m6.1.1.cmml" xref="S4.SS1.p3.6.m6.1.1">0.2</cn><apply id="S4.SS1.p3.6.m6.2.2.1.1.cmml" xref="S4.SS1.p3.6.m6.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.6.m6.2.2.1.1.1.cmml" xref="S4.SS1.p3.6.m6.2.2.1.1">subscript</csymbol><ci id="S4.SS1.p3.6.m6.2.2.1.1.2.cmml" xref="S4.SS1.p3.6.m6.2.2.1.1.2">ğ‘‘</ci><ci id="S4.SS1.p3.6.m6.2.2.1.1.3.cmml" xref="S4.SS1.p3.6.m6.2.2.1.1.3">ğ‘</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.6.m6.2c">[0.2,d_{c}]</annotation></semantics></math><span id="S4.SS1.p3.9.7" class="ltx_text" style="font-size:90%;">Â m, where </span><math id="S4.SS1.p3.7.m7.1" class="ltx_Math" alttext="d_{c}" display="inline"><semantics id="S4.SS1.p3.7.m7.1a"><msub id="S4.SS1.p3.7.m7.1.1" xref="S4.SS1.p3.7.m7.1.1.cmml"><mi mathsize="90%" id="S4.SS1.p3.7.m7.1.1.2" xref="S4.SS1.p3.7.m7.1.1.2.cmml">d</mi><mi mathsize="90%" id="S4.SS1.p3.7.m7.1.1.3" xref="S4.SS1.p3.7.m7.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.7.m7.1b"><apply id="S4.SS1.p3.7.m7.1.1.cmml" xref="S4.SS1.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.7.m7.1.1.1.cmml" xref="S4.SS1.p3.7.m7.1.1">subscript</csymbol><ci id="S4.SS1.p3.7.m7.1.1.2.cmml" xref="S4.SS1.p3.7.m7.1.1.2">ğ‘‘</ci><ci id="S4.SS1.p3.7.m7.1.1.3.cmml" xref="S4.SS1.p3.7.m7.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.7.m7.1c">d_{c}</annotation></semantics></math><span id="S4.SS1.p3.9.8" class="ltx_text" style="font-size:90%;"> to the critical distance, determined by the roomâ€™s volume and </span><math id="S4.SS1.p3.8.m8.1" class="ltx_Math" alttext="T_{60}" display="inline"><semantics id="S4.SS1.p3.8.m8.1a"><msub id="S4.SS1.p3.8.m8.1.1" xref="S4.SS1.p3.8.m8.1.1.cmml"><mi mathsize="90%" id="S4.SS1.p3.8.m8.1.1.2" xref="S4.SS1.p3.8.m8.1.1.2.cmml">T</mi><mn mathsize="90%" id="S4.SS1.p3.8.m8.1.1.3" xref="S4.SS1.p3.8.m8.1.1.3.cmml">60</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.8.m8.1b"><apply id="S4.SS1.p3.8.m8.1.1.cmml" xref="S4.SS1.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS1.p3.8.m8.1.1.1.cmml" xref="S4.SS1.p3.8.m8.1.1">subscript</csymbol><ci id="S4.SS1.p3.8.m8.1.1.2.cmml" xref="S4.SS1.p3.8.m8.1.1.2">ğ‘‡</ci><cn type="integer" id="S4.SS1.p3.8.m8.1.1.3.cmml" xref="S4.SS1.p3.8.m8.1.1.3">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.8.m8.1c">T_{60}</annotation></semantics></math><span id="S4.SS1.p3.9.9" class="ltx_text" style="font-size:90%;">. This ensures the dominance of direct sound over reflections.
Finally, we added spatially-white noise at an  </span><span title="signal-to-noise ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">signal-to-noise ratio</span></span><span id="S4.SS1.p3.9.10" class="ltx_text" style="font-size:90%;"> (</span><abbr title="signal-to-noise ratio" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SNR</span></abbr><span id="S4.SS1.p3.9.11" class="ltx_text" style="font-size:90%;">) of </span><math id="S4.SS1.p3.9.m9.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.SS1.p3.9.m9.1a"><mn mathsize="90%" id="S4.SS1.p3.9.m9.1.1" xref="S4.SS1.p3.9.m9.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.9.m9.1b"><cn type="integer" id="S4.SS1.p3.9.m9.1.1.cmml" xref="S4.SS1.p3.9.m9.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.9.m9.1c">20</annotation></semantics></math><span id="S4.SS1.p3.9.12" class="ltx_text" style="font-size:90%;">Â dB to each reverberant signal. This noise was synthesized by applying an auto-regressive filter of order 1 to white Gaussian noise, emphasizing lower frequencies.</span></p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text" style="font-size:90%;">The proposed scheme was evaluated using real-world </span><abbr title="Room Impulse Responses" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RIRs</span></abbr><span id="S4.SS1.p4.1.2" class="ltx_text" style="font-size:90%;"> drawn from the </span><abbr title="Acoustic Characterisation of Environments" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ACE</span></abbr><span id="S4.SS1.p4.1.3" class="ltx_text" style="font-size:90%;"> database </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p4.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S4.SS1.p4.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p4.1.6" class="ltx_text" style="font-size:90%;">. The </span><abbr title="Acoustic Characterisation of Environments" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ACE</span></abbr><span id="S4.SS1.p4.1.7" class="ltx_text" style="font-size:90%;"> </span><abbr title="Room Impulse Responses" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RIR</span></abbr><span id="S4.SS1.p4.1.8" class="ltx_text" style="font-size:90%;"> database comprises recordings from seven different rooms with varying dimensions and reverberation levels (see TableÂ </span><span class="ltx_ref ltx_missing_label ltx_ref_self" style="font-size:90%;">LABEL:ACE_results</span><span id="S4.SS1.p4.1.9" class="ltx_text" style="font-size:90%;">). We only used a subset of the database, recorded with a mobile phone equipped with three microphones in the ner-field scenario, which is a practical choice for real-world  </span><span title="Speech Emotion Recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Speech Emotion Recognition</span></span><span id="S4.SS1.p4.1.10" class="ltx_text" style="font-size:90%;"> (</span><abbr title="Speech Emotion Recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">SER</span></abbr><span id="S4.SS1.p4.1.11" class="ltx_text" style="font-size:90%;">) applications. We convolved all audio utterances of the test set with the </span><abbr title="Acoustic Characterisation of Environments" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ACE</span></abbr><span id="S4.SS1.p4.1.12" class="ltx_text" style="font-size:90%;"> </span><abbr title="Room Impulse Responses" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RIRs</span></abbr><span id="S4.SS1.p4.1.13" class="ltx_text" style="font-size:90%;"> to generate 3-microphone signals for each utterance.</span></p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.15.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Accuracy and the associated confidence intervals for the <abbr title="Ryerson audio-visual database of emotional speech and song" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RAVDESS</span></abbr> test set reverberated by <abbr title="Room Impulse Responses" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RIRs</span></abbr> drawn from the <abbr title="Acoustic Characterisation of Environments" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ACE</span></abbr> database (using the 3-microphone cellular phone recordings with the source in the near-field of the device). The â€˜Single-Channelâ€™ columns are using an arbitrarily chosen microphone. The â€˜Avg melâ€™ columns present results with mel-spectrograms averaged across three channels during fine-tuning and testing. The â€˜Sum PEâ€™ columns depict the Patch-Embed fusion approach fine-tuned and tested on the three channels. The asterisk in the video column describes the same result.</figcaption>
<div id="S4.T1.8" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:880.2pt;height:178.1pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(17.4pt,-3.5pt) scale(1.04121902032459,1.04121902032459) ;">
<table id="S4.T1.8.8" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-top:0.45pt;padding-bottom:0.45pt;" rowspan="2"><span id="S4.T1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Room (<math id="S4.T1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="T_{60}" display="inline"><semantics id="S4.T1.1.1.1.1.1.m1.1a"><msub id="S4.T1.1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T1.1.1.1.1.1.m1.1.1.2" xref="S4.T1.1.1.1.1.1.m1.1.1.2.cmml">T</mi><mn id="S4.T1.1.1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.1.1.m1.1.1.3.cmml">60</mn></msub><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T1.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1.2">ğ‘‡</ci><cn type="integer" id="S4.T1.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1.3">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.m1.1c">T_{60}</annotation></semantics></math>Â [ms])</span></th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.45pt;padding-bottom:0.45pt;" colspan="3"><span id="S4.T1.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Video</span></th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.45pt;padding-bottom:0.45pt;" colspan="4"><span id="S4.T1.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Audio</span></th>
<th id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:0.45pt;padding-bottom:0.45pt;" colspan="4"><span id="S4.T1.1.1.1.4.1" class="ltx_text" style="font-size:90%;">MER</span></th>
</tr>
<tr id="S4.T1.8.8.9.1" class="ltx_tr">
<th id="S4.T1.8.8.9.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"></th>
<th id="S4.T1.8.8.9.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"></th>
<th id="S4.T1.8.8.9.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"></th>
<th id="S4.T1.8.8.9.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.8.8.9.1.4.1" class="ltx_text" style="font-size:90%;">Clean Single-Channel</span></th>
<th id="S4.T1.8.8.9.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.8.8.9.1.5.1" class="ltx_text" style="font-size:90%;">Rev Single-Channel</span></th>
<th id="S4.T1.8.8.9.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.8.8.9.1.6.1" class="ltx_text" style="font-size:90%;">Avg mel</span></th>
<th id="S4.T1.8.8.9.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.8.8.9.1.7.1" class="ltx_text" style="font-size:90%;">Sum PE</span></th>
<th id="S4.T1.8.8.9.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.8.8.9.1.8.1" class="ltx_text" style="font-size:90%;">Clean Single-Channel</span></th>
<th id="S4.T1.8.8.9.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.8.8.9.1.9.1" class="ltx_text" style="font-size:90%;">Rev Single-Channel</span></th>
<th id="S4.T1.8.8.9.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.8.8.9.1.10.1" class="ltx_text" style="font-size:90%;">Avg mel</span></th>
<th id="S4.T1.8.8.9.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.8.8.9.1.11.1" class="ltx_text" style="font-size:90%;">Sum PE</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.2.2" class="ltx_tr">
<th id="S4.T1.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span id="S4.T1.2.2.2.1.1" class="ltx_text" style="font-size:90%;">Lecture 1 (</span><math id="S4.T1.2.2.2.1.m1.1" class="ltx_Math" alttext="638" display="inline"><semantics id="S4.T1.2.2.2.1.m1.1a"><mn mathsize="90%" id="S4.T1.2.2.2.1.m1.1.1" xref="S4.T1.2.2.2.1.m1.1.1.cmml">638</mn><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.1.m1.1b"><cn type="integer" id="S4.T1.2.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.2.1.m1.1.1">638</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.1.m1.1c">638</annotation></semantics></math><span id="S4.T1.2.2.2.1.2" class="ltx_text" style="font-size:90%;">)</span>
</th>
<td id="S4.T1.2.2.2.2" class="ltx_td ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"></td>
<td id="S4.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.2.2.2.3.1" class="ltx_text" style="font-size:90%;">69.4 (65.5-73.3)</span></td>
<td id="S4.T1.2.2.2.4" class="ltx_td ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"></td>
<td id="S4.T1.2.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.2.2.2.5.1" class="ltx_text" style="font-size:90%;">42.7 (38.8-47.2)</span></td>
<td id="S4.T1.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.2.2.2.6.1" class="ltx_text" style="font-size:90%;">60 (55.5-64.4)</span></td>
<td id="S4.T1.2.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.2.2.2.7.1" class="ltx_text" style="font-size:90%;">61.6 (57.7-65.5)</span></td>
<td id="S4.T1.2.2.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.2.2.2.8.1" class="ltx_text" style="font-size:90%;">61.1 (57.2-65)</span></td>
<td id="S4.T1.2.2.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.2.2.2.9.1" class="ltx_text" style="font-size:90%;">70 (66.1-73.8)</span></td>
<td id="S4.T1.2.2.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.2.2.2.10.1" class="ltx_text" style="font-size:90%;">75 (71.1-78.8)</span></td>
<td id="S4.T1.2.2.2.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.2.2.2.11.1" class="ltx_text" style="font-size:90%;">77.2 (73.3-81.1)</span></td>
<td id="S4.T1.2.2.2.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.2.2.2.12.1" class="ltx_text" style="font-size:90%;">78.3 (74.4-81.6)</span></td>
</tr>
<tr id="S4.T1.3.3.3" class="ltx_tr">
<th id="S4.T1.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span id="S4.T1.3.3.3.1.1" class="ltx_text" style="font-size:90%;">Lecture 2 (</span><math id="S4.T1.3.3.3.1.m1.1" class="ltx_Math" alttext="1220" display="inline"><semantics id="S4.T1.3.3.3.1.m1.1a"><mn mathsize="90%" id="S4.T1.3.3.3.1.m1.1.1" xref="S4.T1.3.3.3.1.m1.1.1.cmml">1220</mn><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.1.m1.1b"><cn type="integer" id="S4.T1.3.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.3.1.m1.1.1">1220</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.1.m1.1c">1220</annotation></semantics></math><span id="S4.T1.3.3.3.1.2" class="ltx_text" style="font-size:90%;">)</span>
</th>
<td id="S4.T1.3.3.3.2" class="ltx_td" style="padding-top:0.45pt;padding-bottom:0.45pt;"></td>
<td id="S4.T1.3.3.3.3" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.3.3.3.3.1" class="ltx_text" style="font-size:90%;">*</span></td>
<td id="S4.T1.3.3.3.4" class="ltx_td" style="padding-top:0.45pt;padding-bottom:0.45pt;"></td>
<td id="S4.T1.3.3.3.5" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.3.3.3.5.1" class="ltx_text" style="font-size:90%;">40.5 (36.6-45)</span></td>
<td id="S4.T1.3.3.3.6" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.3.3.3.6.1" class="ltx_text" style="font-size:90%;">55 (50.5-59.4)</span></td>
<td id="S4.T1.3.3.3.7" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.3.3.3.7.1" class="ltx_text" style="font-size:90%;">60.5 (56.1-64.4)</span></td>
<td id="S4.T1.3.3.3.8" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.3.3.3.8.1" class="ltx_text" style="font-size:90%;">58.8 (54.9-62.8)</span></td>
<td id="S4.T1.3.3.3.9" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.3.3.3.9.1" class="ltx_text" style="font-size:90%;">71.6 (67.7-75.5)</span></td>
<td id="S4.T1.3.3.3.10" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.3.3.3.10.1" class="ltx_text" style="font-size:90%;">76.1 (72.7-79.4)</span></td>
<td id="S4.T1.3.3.3.11" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.3.3.3.11.1" class="ltx_text" style="font-size:90%;">76.1 (72.7-80)</span></td>
<td id="S4.T1.3.3.3.12" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.3.3.3.12.1" class="ltx_text" style="font-size:90%;">78.3 (75-81.6)</span></td>
</tr>
<tr id="S4.T1.4.4.4" class="ltx_tr">
<th id="S4.T1.4.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span id="S4.T1.4.4.4.1.1" class="ltx_text" style="font-size:90%;">Lobby (</span><math id="S4.T1.4.4.4.1.m1.1" class="ltx_Math" alttext="646" display="inline"><semantics id="S4.T1.4.4.4.1.m1.1a"><mn mathsize="90%" id="S4.T1.4.4.4.1.m1.1.1" xref="S4.T1.4.4.4.1.m1.1.1.cmml">646</mn><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.1.m1.1b"><cn type="integer" id="S4.T1.4.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.4.1.m1.1.1">646</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.1.m1.1c">646</annotation></semantics></math><span id="S4.T1.4.4.4.1.2" class="ltx_text" style="font-size:90%;">)</span>
</th>
<td id="S4.T1.4.4.4.2" class="ltx_td" style="padding-top:0.45pt;padding-bottom:0.45pt;"></td>
<td id="S4.T1.4.4.4.3" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.4.4.4.3.1" class="ltx_text" style="font-size:90%;">*</span></td>
<td id="S4.T1.4.4.4.4" class="ltx_td" style="padding-top:0.45pt;padding-bottom:0.45pt;"></td>
<td id="S4.T1.4.4.4.5" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.4.4.4.5.1" class="ltx_text" style="font-size:90%;">40.5 (36.6-45)</span></td>
<td id="S4.T1.4.4.4.6" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.4.4.4.6.1" class="ltx_text" style="font-size:90%;">57.2 (53.3-61.1)</span></td>
<td id="S4.T1.4.4.4.7" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.4.4.4.7.1" class="ltx_text" style="font-size:90%;">63.3 (59.4-67.22)</span></td>
<td id="S4.T1.4.4.4.8" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.4.4.4.8.1" class="ltx_text" style="font-size:90%;">69.4 (65.5-73.3)</span></td>
<td id="S4.T1.4.4.4.9" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.4.4.4.9.1" class="ltx_text" style="font-size:90%;">68.3 (64.4-72.2)</span></td>
<td id="S4.T1.4.4.4.10" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.4.4.4.10.1" class="ltx_text" style="font-size:90%;">73.3 (69.4-77.2)</span></td>
<td id="S4.T1.4.4.4.11" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.4.4.4.11.1" class="ltx_text" style="font-size:90%;">77.2 (73.3-81.1)</span></td>
<td id="S4.T1.4.4.4.12" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.4.4.4.12.1" class="ltx_text" style="font-size:90%;">77.7 (74.4-81.1)</span></td>
</tr>
<tr id="S4.T1.5.5.5" class="ltx_tr">
<th id="S4.T1.5.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span id="S4.T1.5.5.5.1.1" class="ltx_text" style="font-size:90%;">Meeting 1 (</span><math id="S4.T1.5.5.5.1.m1.1" class="ltx_Math" alttext="437" display="inline"><semantics id="S4.T1.5.5.5.1.m1.1a"><mn mathsize="90%" id="S4.T1.5.5.5.1.m1.1.1" xref="S4.T1.5.5.5.1.m1.1.1.cmml">437</mn><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.1.m1.1b"><cn type="integer" id="S4.T1.5.5.5.1.m1.1.1.cmml" xref="S4.T1.5.5.5.1.m1.1.1">437</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.1.m1.1c">437</annotation></semantics></math><span id="S4.T1.5.5.5.1.2" class="ltx_text" style="font-size:90%;">)</span>
</th>
<td id="S4.T1.5.5.5.2" class="ltx_td" style="padding-top:0.45pt;padding-bottom:0.45pt;"></td>
<td id="S4.T1.5.5.5.3" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.5.5.5.3.1" class="ltx_text" style="font-size:90%;">*</span></td>
<td id="S4.T1.5.5.5.4" class="ltx_td" style="padding-top:0.45pt;padding-bottom:0.45pt;"></td>
<td id="S4.T1.5.5.5.5" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.5.5.5.5.1" class="ltx_text" style="font-size:90%;">42.7 (38.8-46.6)</span></td>
<td id="S4.T1.5.5.5.6" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.5.5.5.6.1" class="ltx_text" style="font-size:90%;">57.2 (52.7-61.6)</span></td>
<td id="S4.T1.5.5.5.7" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.5.5.5.7.1" class="ltx_text" style="font-size:90%;">60 (56.1-63.8)</span></td>
<td id="S4.T1.5.5.5.8" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.5.5.5.8.1" class="ltx_text" style="font-size:90%;">61.1 (57.2-65)</span></td>
<td id="S4.T1.5.5.5.9" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.5.5.5.9.1" class="ltx_text" style="font-size:90%;">70 (66.1-73.8)</span></td>
<td id="S4.T1.5.5.5.10" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.5.5.5.10.1" class="ltx_text" style="font-size:90%;">72.7 (68.8-76.6)</span></td>
<td id="S4.T1.5.5.5.11" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.5.5.5.11.1" class="ltx_text" style="font-size:90%;">77.2 (73.8-81.1)</span></td>
<td id="S4.T1.5.5.5.12" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.5.5.5.12.1" class="ltx_text" style="font-size:90%;">78.8 (75.5-82.2)</span></td>
</tr>
<tr id="S4.T1.6.6.6" class="ltx_tr">
<th id="S4.T1.6.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span id="S4.T1.6.6.6.1.1" class="ltx_text" style="font-size:90%;">Meeting 2 (</span><math id="S4.T1.6.6.6.1.m1.1" class="ltx_Math" alttext="371" display="inline"><semantics id="S4.T1.6.6.6.1.m1.1a"><mn mathsize="90%" id="S4.T1.6.6.6.1.m1.1.1" xref="S4.T1.6.6.6.1.m1.1.1.cmml">371</mn><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.1.m1.1b"><cn type="integer" id="S4.T1.6.6.6.1.m1.1.1.cmml" xref="S4.T1.6.6.6.1.m1.1.1">371</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.1.m1.1c">371</annotation></semantics></math><span id="S4.T1.6.6.6.1.2" class="ltx_text" style="font-size:90%;">)</span>
</th>
<td id="S4.T1.6.6.6.2" class="ltx_td" style="padding-top:0.45pt;padding-bottom:0.45pt;"></td>
<td id="S4.T1.6.6.6.3" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.6.6.6.3.1" class="ltx_text" style="font-size:90%;">*</span></td>
<td id="S4.T1.6.6.6.4" class="ltx_td" style="padding-top:0.45pt;padding-bottom:0.45pt;"></td>
<td id="S4.T1.6.6.6.5" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.6.6.6.5.1" class="ltx_text" style="font-size:90%;">38.8 (34.4-42.7)</span></td>
<td id="S4.T1.6.6.6.6" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.6.6.6.6.1" class="ltx_text" style="font-size:90%;">56.1 (51.6-60.5)</span></td>
<td id="S4.T1.6.6.6.7" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.6.6.6.7.1" class="ltx_text" style="font-size:90%;">62.7 (58.8-66.6)</span></td>
<td id="S4.T1.6.6.6.8" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.6.6.6.8.1" class="ltx_text" style="font-size:90%;">59.4 (55.5-63.8)</span></td>
<td id="S4.T1.6.6.6.9" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.6.6.6.9.1" class="ltx_text" style="font-size:90%;">71.1 (67.2-75)</span></td>
<td id="S4.T1.6.6.6.10" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.6.6.6.10.1" class="ltx_text" style="font-size:90%;">75 (71.1-78.8)</span></td>
<td id="S4.T1.6.6.6.11" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.6.6.6.11.1" class="ltx_text" style="font-size:90%;">78.8 (75.5-82.2)</span></td>
<td id="S4.T1.6.6.6.12" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.6.6.6.12.1" class="ltx_text" style="font-size:90%;">75.5 (71.6-78.8)</span></td>
</tr>
<tr id="S4.T1.7.7.7" class="ltx_tr">
<th id="S4.T1.7.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span id="S4.T1.7.7.7.1.1" class="ltx_text" style="font-size:90%;">Office 1 (</span><math id="S4.T1.7.7.7.1.m1.1" class="ltx_Math" alttext="332" display="inline"><semantics id="S4.T1.7.7.7.1.m1.1a"><mn mathsize="90%" id="S4.T1.7.7.7.1.m1.1.1" xref="S4.T1.7.7.7.1.m1.1.1.cmml">332</mn><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.7.1.m1.1b"><cn type="integer" id="S4.T1.7.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.7.1.m1.1.1">332</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.7.1.m1.1c">332</annotation></semantics></math><span id="S4.T1.7.7.7.1.2" class="ltx_text" style="font-size:90%;">)</span>
</th>
<td id="S4.T1.7.7.7.2" class="ltx_td" style="padding-top:0.45pt;padding-bottom:0.45pt;"></td>
<td id="S4.T1.7.7.7.3" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.7.7.7.3.1" class="ltx_text" style="font-size:90%;">*</span></td>
<td id="S4.T1.7.7.7.4" class="ltx_td" style="padding-top:0.45pt;padding-bottom:0.45pt;"></td>
<td id="S4.T1.7.7.7.5" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.7.7.7.5.1" class="ltx_text" style="font-size:90%;">42.7 (38.3-46.6)</span></td>
<td id="S4.T1.7.7.7.6" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.7.7.7.6.1" class="ltx_text" style="font-size:90%;">59.4 (55-63.3)</span></td>
<td id="S4.T1.7.7.7.7" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.7.7.7.7.1" class="ltx_text" style="font-size:90%;">62.7 (58.8-68.3)</span></td>
<td id="S4.T1.7.7.7.8" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.7.7.7.8.1" class="ltx_text" style="font-size:90%;">63.3 (58.8-67.2)</span></td>
<td id="S4.T1.7.7.7.9" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.7.7.7.9.1" class="ltx_text" style="font-size:90%;">70 (66.1-73.8)</span></td>
<td id="S4.T1.7.7.7.10" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.7.7.7.10.1" class="ltx_text" style="font-size:90%;">75 (71.1-78.8)</span></td>
<td id="S4.T1.7.7.7.11" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.7.7.7.11.1" class="ltx_text" style="font-size:90%;">77.7 (74.4-81.6)</span></td>
<td id="S4.T1.7.7.7.12" class="ltx_td ltx_align_center" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.7.7.7.12.1" class="ltx_text" style="font-size:90%;">77.2 (73.8-80.5)</span></td>
</tr>
<tr id="S4.T1.8.8.8" class="ltx_tr">
<th id="S4.T1.8.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-top:0.45pt;padding-bottom:0.45pt;">
<span id="S4.T1.8.8.8.1.1" class="ltx_text" style="font-size:90%;">Office 2 (</span><math id="S4.T1.8.8.8.1.m1.1" class="ltx_Math" alttext="390" display="inline"><semantics id="S4.T1.8.8.8.1.m1.1a"><mn mathsize="90%" id="S4.T1.8.8.8.1.m1.1.1" xref="S4.T1.8.8.8.1.m1.1.1.cmml">390</mn><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.8.1.m1.1b"><cn type="integer" id="S4.T1.8.8.8.1.m1.1.1.cmml" xref="S4.T1.8.8.8.1.m1.1.1">390</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.8.1.m1.1c">390</annotation></semantics></math><span id="S4.T1.8.8.8.1.2" class="ltx_text" style="font-size:90%;">)</span>
</th>
<td id="S4.T1.8.8.8.2" class="ltx_td ltx_border_bb" style="padding-top:0.45pt;padding-bottom:0.45pt;"></td>
<td id="S4.T1.8.8.8.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.8.8.8.3.1" class="ltx_text" style="font-size:90%;">*</span></td>
<td id="S4.T1.8.8.8.4" class="ltx_td ltx_border_bb" style="padding-top:0.45pt;padding-bottom:0.45pt;"></td>
<td id="S4.T1.8.8.8.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.8.8.8.5.1" class="ltx_text" style="font-size:90%;">47.7 (43.3-52.2)</span></td>
<td id="S4.T1.8.8.8.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.8.8.8.6.1" class="ltx_text" style="font-size:90%;">56.6 (52.2-60.5)</span></td>
<td id="S4.T1.8.8.8.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.8.8.8.7.1" class="ltx_text" style="font-size:90%;">64.4 (60.5-68.3)</span></td>
<td id="S4.T1.8.8.8.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.8.8.8.8.1" class="ltx_text" style="font-size:90%;">60.5 (56.6-64.4)</span></td>
<td id="S4.T1.8.8.8.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.8.8.8.9.1" class="ltx_text" style="font-size:90%;">68.8 (65-72.7)</span></td>
<td id="S4.T1.8.8.8.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.8.8.8.10.1" class="ltx_text" style="font-size:90%;">70 (66.1-73.8)</span></td>
<td id="S4.T1.8.8.8.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.8.8.8.11.1" class="ltx_text" style="font-size:90%;">74.4 (70.5-78.3)</span></td>
<td id="S4.T1.8.8.8.12" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.45pt;padding-bottom:0.45pt;"><span id="S4.T1.8.8.8.12.1" class="ltx_text" style="font-size:90%;">75.5 (71.6-78.8)</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.09545/assets/ACE_mp2_Lecture_Room_2_T60=1220ms_RAVDESS.png" id="S4.F2.sf1.g1" class="ltx_graphics ltx_img_square" width="287" height="284" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">(a) </span>RAVDESS in ACE Lecture RoomÂ 2 (<math id="S4.F2.sf1.2.m1.1" class="ltx_centering" alttext="T_{60}=1220" display="inline"><semantics id="S4.F2.sf1.2.m1.1b"><mrow id="S4.F2.sf1.2.m1.1.1" xref="S4.F2.sf1.2.m1.1.1.cmml"><msub id="S4.F2.sf1.2.m1.1.1.2" xref="S4.F2.sf1.2.m1.1.1.2.cmml"><mi id="S4.F2.sf1.2.m1.1.1.2.2" xref="S4.F2.sf1.2.m1.1.1.2.2.cmml">T</mi><mn id="S4.F2.sf1.2.m1.1.1.2.3" xref="S4.F2.sf1.2.m1.1.1.2.3.cmml">60</mn></msub><mo id="S4.F2.sf1.2.m1.1.1.1" xref="S4.F2.sf1.2.m1.1.1.1.cmml">=</mo><mn id="S4.F2.sf1.2.m1.1.1.3" xref="S4.F2.sf1.2.m1.1.1.3.cmml">1220</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.sf1.2.m1.1c"><apply id="S4.F2.sf1.2.m1.1.1.cmml" xref="S4.F2.sf1.2.m1.1.1"><eq id="S4.F2.sf1.2.m1.1.1.1.cmml" xref="S4.F2.sf1.2.m1.1.1.1"></eq><apply id="S4.F2.sf1.2.m1.1.1.2.cmml" xref="S4.F2.sf1.2.m1.1.1.2"><csymbol cd="ambiguous" id="S4.F2.sf1.2.m1.1.1.2.1.cmml" xref="S4.F2.sf1.2.m1.1.1.2">subscript</csymbol><ci id="S4.F2.sf1.2.m1.1.1.2.2.cmml" xref="S4.F2.sf1.2.m1.1.1.2.2">ğ‘‡</ci><cn type="integer" id="S4.F2.sf1.2.m1.1.1.2.3.cmml" xref="S4.F2.sf1.2.m1.1.1.2.3">60</cn></apply><cn type="integer" id="S4.F2.sf1.2.m1.1.1.3.cmml" xref="S4.F2.sf1.2.m1.1.1.3">1220</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.sf1.2.m1.1d">T_{60}=1220</annotation></semantics></math>Â ms).</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.09545/assets/ACE_mp2_Meeting_Room_1_T60=437ms_RAVDESS.png" id="S4.F2.sf2.g1" class="ltx_graphics ltx_img_square" width="287" height="284" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">(b) </span>RAVDESS in ACE Meeting RoomÂ 1 (<math id="S4.F2.sf2.2.m1.1" class="ltx_centering" alttext="T_{60}=437" display="inline"><semantics id="S4.F2.sf2.2.m1.1b"><mrow id="S4.F2.sf2.2.m1.1.1" xref="S4.F2.sf2.2.m1.1.1.cmml"><msub id="S4.F2.sf2.2.m1.1.1.2" xref="S4.F2.sf2.2.m1.1.1.2.cmml"><mi id="S4.F2.sf2.2.m1.1.1.2.2" xref="S4.F2.sf2.2.m1.1.1.2.2.cmml">T</mi><mn id="S4.F2.sf2.2.m1.1.1.2.3" xref="S4.F2.sf2.2.m1.1.1.2.3.cmml">60</mn></msub><mo id="S4.F2.sf2.2.m1.1.1.1" xref="S4.F2.sf2.2.m1.1.1.1.cmml">=</mo><mn id="S4.F2.sf2.2.m1.1.1.3" xref="S4.F2.sf2.2.m1.1.1.3.cmml">437</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.sf2.2.m1.1c"><apply id="S4.F2.sf2.2.m1.1.1.cmml" xref="S4.F2.sf2.2.m1.1.1"><eq id="S4.F2.sf2.2.m1.1.1.1.cmml" xref="S4.F2.sf2.2.m1.1.1.1"></eq><apply id="S4.F2.sf2.2.m1.1.1.2.cmml" xref="S4.F2.sf2.2.m1.1.1.2"><csymbol cd="ambiguous" id="S4.F2.sf2.2.m1.1.1.2.1.cmml" xref="S4.F2.sf2.2.m1.1.1.2">subscript</csymbol><ci id="S4.F2.sf2.2.m1.1.1.2.2.cmml" xref="S4.F2.sf2.2.m1.1.1.2.2">ğ‘‡</ci><cn type="integer" id="S4.F2.sf2.2.m1.1.1.2.3.cmml" xref="S4.F2.sf2.2.m1.1.1.2.3">60</cn></apply><cn type="integer" id="S4.F2.sf2.2.m1.1.1.3.cmml" xref="S4.F2.sf2.2.m1.1.1.3">437</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.sf2.2.m1.1d">T_{60}=437</annotation></semantics></math>Â ms).</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.09545/assets/ACE_mp1_Office_2_T60=390ms_RAVDESS.png" id="S4.F2.sf3.g1" class="ltx_graphics ltx_img_square" width="287" height="284" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">(c) </span>RAVDESS in ACE OfficeÂ 2 (<math id="S4.F2.sf3.2.m1.1" class="ltx_centering" alttext="T_{60}=390" display="inline"><semantics id="S4.F2.sf3.2.m1.1b"><mrow id="S4.F2.sf3.2.m1.1.1" xref="S4.F2.sf3.2.m1.1.1.cmml"><msub id="S4.F2.sf3.2.m1.1.1.2" xref="S4.F2.sf3.2.m1.1.1.2.cmml"><mi id="S4.F2.sf3.2.m1.1.1.2.2" xref="S4.F2.sf3.2.m1.1.1.2.2.cmml">T</mi><mn id="S4.F2.sf3.2.m1.1.1.2.3" xref="S4.F2.sf3.2.m1.1.1.2.3.cmml">60</mn></msub><mo id="S4.F2.sf3.2.m1.1.1.1" xref="S4.F2.sf3.2.m1.1.1.1.cmml">=</mo><mn id="S4.F2.sf3.2.m1.1.1.3" xref="S4.F2.sf3.2.m1.1.1.3.cmml">390</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.sf3.2.m1.1c"><apply id="S4.F2.sf3.2.m1.1.1.cmml" xref="S4.F2.sf3.2.m1.1.1"><eq id="S4.F2.sf3.2.m1.1.1.1.cmml" xref="S4.F2.sf3.2.m1.1.1.1"></eq><apply id="S4.F2.sf3.2.m1.1.1.2.cmml" xref="S4.F2.sf3.2.m1.1.1.2"><csymbol cd="ambiguous" id="S4.F2.sf3.2.m1.1.1.2.1.cmml" xref="S4.F2.sf3.2.m1.1.1.2">subscript</csymbol><ci id="S4.F2.sf3.2.m1.1.1.2.2.cmml" xref="S4.F2.sf3.2.m1.1.1.2.2">ğ‘‡</ci><cn type="integer" id="S4.F2.sf3.2.m1.1.1.2.3.cmml" xref="S4.F2.sf3.2.m1.1.1.2.3">60</cn></apply><cn type="integer" id="S4.F2.sf3.2.m1.1.1.3.cmml" xref="S4.F2.sf3.2.m1.1.1.3">390</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.sf3.2.m1.1d">T_{60}=390</annotation></semantics></math>Â ms).</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.09545/assets/ACE_mp1_Lobby_T60=646ms_RAVDESS.png" id="S4.F2.sf4.g1" class="ltx_graphics ltx_img_square" width="287" height="284" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">(d) </span>RAVDESS in ACE Lobby (<math id="S4.F2.sf4.2.m1.1" class="ltx_centering" alttext="T_{60}=646" display="inline"><semantics id="S4.F2.sf4.2.m1.1b"><mrow id="S4.F2.sf4.2.m1.1.1" xref="S4.F2.sf4.2.m1.1.1.cmml"><msub id="S4.F2.sf4.2.m1.1.1.2" xref="S4.F2.sf4.2.m1.1.1.2.cmml"><mi id="S4.F2.sf4.2.m1.1.1.2.2" xref="S4.F2.sf4.2.m1.1.1.2.2.cmml">T</mi><mn id="S4.F2.sf4.2.m1.1.1.2.3" xref="S4.F2.sf4.2.m1.1.1.2.3.cmml">60</mn></msub><mo id="S4.F2.sf4.2.m1.1.1.1" xref="S4.F2.sf4.2.m1.1.1.1.cmml">=</mo><mn id="S4.F2.sf4.2.m1.1.1.3" xref="S4.F2.sf4.2.m1.1.1.3.cmml">646</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.sf4.2.m1.1c"><apply id="S4.F2.sf4.2.m1.1.1.cmml" xref="S4.F2.sf4.2.m1.1.1"><eq id="S4.F2.sf4.2.m1.1.1.1.cmml" xref="S4.F2.sf4.2.m1.1.1.1"></eq><apply id="S4.F2.sf4.2.m1.1.1.2.cmml" xref="S4.F2.sf4.2.m1.1.1.2"><csymbol cd="ambiguous" id="S4.F2.sf4.2.m1.1.1.2.1.cmml" xref="S4.F2.sf4.2.m1.1.1.2">subscript</csymbol><ci id="S4.F2.sf4.2.m1.1.1.2.2.cmml" xref="S4.F2.sf4.2.m1.1.1.2.2">ğ‘‡</ci><cn type="integer" id="S4.F2.sf4.2.m1.1.1.2.3.cmml" xref="S4.F2.sf4.2.m1.1.1.2.3">60</cn></apply><cn type="integer" id="S4.F2.sf4.2.m1.1.1.3.cmml" xref="S4.F2.sf4.2.m1.1.1.3">646</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.sf4.2.m1.1d">T_{60}=646</annotation></semantics></math>Â ms).</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.6.1.1" class="ltx_text ltx_font_bold">Fig.Â 2</span>: </span> Accuracy and confidence intervals were assessed using the reverberated <abbr title="Ryerson audio-visual database of emotional speech and song" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">RAVDESS</span></abbr> test set across four different rooms from the <abbr title="Acoustic Characterisation of Environments" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ACE</span></abbr> database. The results consistently show that multi-modal and multi-channel schemes outperform uni-modal and single-channel audio schemes, with the â€˜Sum PEâ€™ scheme offering a slight advantage.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Algorithm Setup</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.5" class="ltx_p"><span id="S4.SS2.p1.5.1" class="ltx_text" style="font-size:90%;">The algorithm setup is now outlined.
As discussed earlier, the video modality leverages the </span><math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="R(2+1)" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi mathsize="90%" id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">â€‹</mo><mrow id="S4.SS2.p1.1.m1.1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S4.SS2.p1.1.m1.1.1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p1.1.m1.1.1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.1.1.cmml"><mn mathsize="90%" id="S4.SS2.p1.1.m1.1.1.1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.1.1.1.2.cmml">2</mn><mo mathsize="90%" id="S4.SS2.p1.1.m1.1.1.1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.1.1.1.cmml">+</mo><mn mathsize="90%" id="S4.SS2.p1.1.m1.1.1.1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo maxsize="90%" minsize="90%" id="S4.SS2.p1.1.m1.1.1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2"></times><ci id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">ğ‘…</ci><apply id="S4.SS2.p1.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1"><plus id="S4.SS2.p1.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1.1"></plus><cn type="integer" id="S4.SS2.p1.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1.2">2</cn><cn type="integer" id="S4.SS2.p1.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">R(2+1)</annotation></semantics></math><span id="S4.SS2.p1.5.2" class="ltx_text" style="font-size:90%;">D model pre-trained on the action recognition Kinetics dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p1.5.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="S4.SS2.p1.5.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS2.p1.5.5" class="ltx_text" style="font-size:90%;">. To better suit our </span><abbr title="Emotion Recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ER</span></abbr><span id="S4.SS2.p1.5.6" class="ltx_text" style="font-size:90%;"> task, we modified the modelâ€™s architecture by adjusting the final linear layer. Specifically, we reconfigured it to output 768-dimensional feature embeddings. This adjustment ensures that both modalities (video and audio) contribute equally-sized feature vectors to the multi-modal representation by fusion through concatenation.
The resolution of the RGB video frames was first reduced to </span><math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mn mathsize="90%" id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">224</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">Ã—</mo><mn mathsize="90%" id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><times id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">224</cn><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">224\times 224</annotation></semantics></math><span id="S4.SS2.p1.5.7" class="ltx_text" style="font-size:90%;"> pixels. Then, 8 frames from the video stream were randomly selected. To augment the dataset, these frames undergo refinement through random cropping using the TorchVision Library </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p1.5.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S4.SS2.p1.5.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS2.p1.5.10" class="ltx_text" style="font-size:90%;">, yielding </span><math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="180\times 180" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mn mathsize="90%" id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">180</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.cmml">Ã—</mo><mn mathsize="90%" id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml">180</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><times id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1"></times><cn type="integer" id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">180</cn><cn type="integer" id="S4.SS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3">180</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">180\times 180</annotation></semantics></math><span id="S4.SS2.p1.5.11" class="ltx_text" style="font-size:90%;"> images that enhance the modelâ€™s robustness to spatial variations. In addition, we added random horizontal and vertical flips, each with a 30% probability of application, coupled with arbitrary rotations within the range of [-30</span><sup id="S4.SS2.p1.5.12" class="ltx_sup"><span id="S4.SS2.p1.5.12.1" class="ltx_text" style="font-size:90%;">âˆ˜</span></sup><span id="S4.SS2.p1.5.13" class="ltx_text" style="font-size:90%;">, 30</span><sup id="S4.SS2.p1.5.14" class="ltx_sup"><span id="S4.SS2.p1.5.14.1" class="ltx_text" style="font-size:90%;">âˆ˜</span></sup><span id="S4.SS2.p1.5.15" class="ltx_text" style="font-size:90%;">].</span></p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.3" class="ltx_p"><span id="S4.SS2.p2.3.1" class="ltx_text" style="font-size:90%;">The audio modality applies an extended version of the </span><abbr title="Hierarchical Token-semantic Audio Transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HTS-AT</span></abbr><span id="S4.SS2.p2.3.2" class="ltx_text" style="font-size:90%;"> model </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p2.3.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S4.SS2.p2.3.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS2.p2.3.5" class="ltx_text" style="font-size:90%;">, suitable for both multi-channel and single-channel scenarios. As described in Sec.Â </span><a href="#S3" title="3 Proposed Model â€£ Multi-Microphone and Multi-Modal Emotion Recognition in Reverberant Environment" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S4.SS2.p2.3.6" class="ltx_text" style="font-size:90%;">, the network structure configuration is arranged into four groups, each containing several Swin-Transformer blocks: 2, 2, 6, and 2, respectively. The mel-spectrogram input is initially transformed into patches and linearly projected to a dimension of </span><math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="D=96" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi mathsize="90%" id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">D</mi><mo mathsize="90%" id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">=</mo><mn mathsize="90%" id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">96</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><eq id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></eq><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">ğ·</ci><cn type="integer" id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">96</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">D=96</annotation></semantics></math><span id="S4.SS2.p2.3.7" class="ltx_text" style="font-size:90%;">. This dimension expands exponentially through each transformer group. It finally reaches a dimension of </span><math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="768" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mn mathsize="90%" id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">768</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><cn type="integer" id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">768</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">768</annotation></semantics></math><span id="S4.SS2.p2.3.8" class="ltx_text" style="font-size:90%;"> (</span><math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="8D=768" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mrow id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mrow id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml"><mn mathsize="90%" id="S4.SS2.p2.3.m3.1.1.2.2" xref="S4.SS2.p2.3.m3.1.1.2.2.cmml">8</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.1.1.2.1" xref="S4.SS2.p2.3.m3.1.1.2.1.cmml">â€‹</mo><mi mathsize="90%" id="S4.SS2.p2.3.m3.1.1.2.3" xref="S4.SS2.p2.3.m3.1.1.2.3.cmml">D</mi></mrow><mo mathsize="90%" id="S4.SS2.p2.3.m3.1.1.1" xref="S4.SS2.p2.3.m3.1.1.1.cmml">=</mo><mn mathsize="90%" id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml">768</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><eq id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1"></eq><apply id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2"><times id="S4.SS2.p2.3.m3.1.1.2.1.cmml" xref="S4.SS2.p2.3.m3.1.1.2.1"></times><cn type="integer" id="S4.SS2.p2.3.m3.1.1.2.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2.2">8</cn><ci id="S4.SS2.p2.3.m3.1.1.2.3.cmml" xref="S4.SS2.p2.3.m3.1.1.2.3">ğ·</ci></apply><cn type="integer" id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3">768</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">8D=768</annotation></semantics></math><span id="S4.SS2.p2.3.9" class="ltx_text" style="font-size:90%;">), which matches the design principles of  </span><span title="Audio Spectrogram Transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Audio Spectrogram Transformer</span></span><span id="S4.SS2.p2.3.10" class="ltx_text" style="font-size:90%;"> (</span><abbr title="Audio Spectrogram Transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">AST</span></abbr><span id="S4.SS2.p2.3.11" class="ltx_text" style="font-size:90%;">). Pre-processing was carried out as explained in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p2.3.12.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S4.SS2.p2.3.13.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS2.p2.3.14" class="ltx_text" style="font-size:90%;"> both for multi-channel and single-channel experiments. We augmented the mel-spectrograms by using the SpecAugment Library </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p2.3.15.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S4.SS2.p2.3.16.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS2.p2.3.17" class="ltx_text" style="font-size:90%;">, which consists of temporal masking, occluding four distinct â€œstrips,â€ each 64 time-frames long. Complementing this, we applied frequency domain masking, obscuring two strips, each 8 frequency bands wide.</span></p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.2" class="ltx_p"><span id="S4.SS2.p3.2.1" class="ltx_text" style="font-size:90%;">Our multi-modal approach combines the feature embeddings from both the video and audio modalities. The 768-dimensional feature vectors extracted from the </span><math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="R(2+1)" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mi mathsize="90%" id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">â€‹</mo><mrow id="S4.SS2.p3.1.m1.1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S4.SS2.p3.1.m1.1.1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p3.1.m1.1.1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.1.1.cmml"><mn mathsize="90%" id="S4.SS2.p3.1.m1.1.1.1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.1.1.1.2.cmml">2</mn><mo mathsize="90%" id="S4.SS2.p3.1.m1.1.1.1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1.cmml">+</mo><mn mathsize="90%" id="S4.SS2.p3.1.m1.1.1.1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo maxsize="90%" minsize="90%" id="S4.SS2.p3.1.m1.1.1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><times id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2"></times><ci id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3">ğ‘…</ci><apply id="S4.SS2.p3.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1"><plus id="S4.SS2.p3.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.1"></plus><cn type="integer" id="S4.SS2.p3.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.2">2</cn><cn type="integer" id="S4.SS2.p3.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">R(2+1)</annotation></semantics></math><span id="S4.SS2.p3.2.2" class="ltx_text" style="font-size:90%;">D model and the extended </span><abbr title="Hierarchical Token-semantic Audio Transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HTS-AT</span></abbr><span id="S4.SS2.p3.2.3" class="ltx_text" style="font-size:90%;"> model are concatenated, resulting in a 1536-dimensional feature representation. This combined feature vector is then fed into a classification head for prediction. The right-hand side of Fig.Â </span><a href="#S2.F1" title="Figure 1 â€£ 2 Problem Formulation â€£ Multi-Microphone and Multi-Modal Emotion Recognition in Reverberant Environment" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S4.SS2.p3.2.4" class="ltx_text" style="font-size:90%;"> presents two fully connected layers </span><math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="(fc)" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mrow id="S4.SS2.p3.2.m2.1.1.1" xref="S4.SS2.p3.2.m2.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S4.SS2.p3.2.m2.1.1.1.2" xref="S4.SS2.p3.2.m2.1.1.1.1.cmml">(</mo><mrow id="S4.SS2.p3.2.m2.1.1.1.1" xref="S4.SS2.p3.2.m2.1.1.1.1.cmml"><mi mathsize="90%" id="S4.SS2.p3.2.m2.1.1.1.1.2" xref="S4.SS2.p3.2.m2.1.1.1.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p3.2.m2.1.1.1.1.1" xref="S4.SS2.p3.2.m2.1.1.1.1.1.cmml">â€‹</mo><mi mathsize="90%" id="S4.SS2.p3.2.m2.1.1.1.1.3" xref="S4.SS2.p3.2.m2.1.1.1.1.3.cmml">c</mi></mrow><mo maxsize="90%" minsize="90%" id="S4.SS2.p3.2.m2.1.1.1.3" xref="S4.SS2.p3.2.m2.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><apply id="S4.SS2.p3.2.m2.1.1.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1.1"><times id="S4.SS2.p3.2.m2.1.1.1.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1.1.1.1"></times><ci id="S4.SS2.p3.2.m2.1.1.1.1.2.cmml" xref="S4.SS2.p3.2.m2.1.1.1.1.2">ğ‘“</ci><ci id="S4.SS2.p3.2.m2.1.1.1.1.3.cmml" xref="S4.SS2.p3.2.m2.1.1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">(fc)</annotation></semantics></math><span id="S4.SS2.p3.2.5" class="ltx_text" style="font-size:90%;"> with a Relu activation function between them, forming the sequence:</span></p>
<table id="S4.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E4.m1.1" class="ltx_Math" alttext="f_{c_{1}}\to\text{ReLU}\to f_{c_{2}}\ \Rightarrow E" display="block"><semantics id="S4.E4.m1.1a"><mrow id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml"><msub id="S4.E4.m1.1.1.2" xref="S4.E4.m1.1.1.2.cmml"><mi mathsize="90%" id="S4.E4.m1.1.1.2.2" xref="S4.E4.m1.1.1.2.2.cmml">f</mi><msub id="S4.E4.m1.1.1.2.3" xref="S4.E4.m1.1.1.2.3.cmml"><mi mathsize="90%" id="S4.E4.m1.1.1.2.3.2" xref="S4.E4.m1.1.1.2.3.2.cmml">c</mi><mn mathsize="90%" id="S4.E4.m1.1.1.2.3.3" xref="S4.E4.m1.1.1.2.3.3.cmml">1</mn></msub></msub><mo mathsize="90%" stretchy="false" id="S4.E4.m1.1.1.3" xref="S4.E4.m1.1.1.3.cmml">â†’</mo><mtext mathsize="90%" id="S4.E4.m1.1.1.4" xref="S4.E4.m1.1.1.4a.cmml">ReLU</mtext><mo mathsize="90%" stretchy="false" id="S4.E4.m1.1.1.5" xref="S4.E4.m1.1.1.5.cmml">â†’</mo><msub id="S4.E4.m1.1.1.6" xref="S4.E4.m1.1.1.6.cmml"><mi mathsize="90%" id="S4.E4.m1.1.1.6.2" xref="S4.E4.m1.1.1.6.2.cmml">f</mi><msub id="S4.E4.m1.1.1.6.3" xref="S4.E4.m1.1.1.6.3.cmml"><mi mathsize="90%" id="S4.E4.m1.1.1.6.3.2" xref="S4.E4.m1.1.1.6.3.2.cmml">c</mi><mn mathsize="90%" id="S4.E4.m1.1.1.6.3.3" xref="S4.E4.m1.1.1.6.3.3.cmml">2</mn></msub></msub><mo mathsize="90%" stretchy="false" id="S4.E4.m1.1.1.7" xref="S4.E4.m1.1.1.7.cmml">â‡’</mo><mi mathsize="90%" id="S4.E4.m1.1.1.8" xref="S4.E4.m1.1.1.8.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.1b"><apply id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1"><and id="S4.E4.m1.1.1a.cmml" xref="S4.E4.m1.1.1"></and><apply id="S4.E4.m1.1.1b.cmml" xref="S4.E4.m1.1.1"><ci id="S4.E4.m1.1.1.3.cmml" xref="S4.E4.m1.1.1.3">â†’</ci><apply id="S4.E4.m1.1.1.2.cmml" xref="S4.E4.m1.1.1.2"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.2.1.cmml" xref="S4.E4.m1.1.1.2">subscript</csymbol><ci id="S4.E4.m1.1.1.2.2.cmml" xref="S4.E4.m1.1.1.2.2">ğ‘“</ci><apply id="S4.E4.m1.1.1.2.3.cmml" xref="S4.E4.m1.1.1.2.3"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.2.3.1.cmml" xref="S4.E4.m1.1.1.2.3">subscript</csymbol><ci id="S4.E4.m1.1.1.2.3.2.cmml" xref="S4.E4.m1.1.1.2.3.2">ğ‘</ci><cn type="integer" id="S4.E4.m1.1.1.2.3.3.cmml" xref="S4.E4.m1.1.1.2.3.3">1</cn></apply></apply><ci id="S4.E4.m1.1.1.4a.cmml" xref="S4.E4.m1.1.1.4"><mtext mathsize="90%" id="S4.E4.m1.1.1.4.cmml" xref="S4.E4.m1.1.1.4">ReLU</mtext></ci></apply><apply id="S4.E4.m1.1.1c.cmml" xref="S4.E4.m1.1.1"><ci id="S4.E4.m1.1.1.5.cmml" xref="S4.E4.m1.1.1.5">â†’</ci><share href="#S4.E4.m1.1.1.4.cmml" id="S4.E4.m1.1.1d.cmml" xref="S4.E4.m1.1.1"></share><apply id="S4.E4.m1.1.1.6.cmml" xref="S4.E4.m1.1.1.6"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.6.1.cmml" xref="S4.E4.m1.1.1.6">subscript</csymbol><ci id="S4.E4.m1.1.1.6.2.cmml" xref="S4.E4.m1.1.1.6.2">ğ‘“</ci><apply id="S4.E4.m1.1.1.6.3.cmml" xref="S4.E4.m1.1.1.6.3"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.6.3.1.cmml" xref="S4.E4.m1.1.1.6.3">subscript</csymbol><ci id="S4.E4.m1.1.1.6.3.2.cmml" xref="S4.E4.m1.1.1.6.3.2">ğ‘</ci><cn type="integer" id="S4.E4.m1.1.1.6.3.3.cmml" xref="S4.E4.m1.1.1.6.3.3">2</cn></apply></apply></apply><apply id="S4.E4.m1.1.1e.cmml" xref="S4.E4.m1.1.1"><ci id="S4.E4.m1.1.1.7.cmml" xref="S4.E4.m1.1.1.7">â‡’</ci><share href="#S4.E4.m1.1.1.6.cmml" id="S4.E4.m1.1.1f.cmml" xref="S4.E4.m1.1.1"></share><ci id="S4.E4.m1.1.1.8.cmml" xref="S4.E4.m1.1.1.8">ğ¸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.1c">f_{c_{1}}\to\text{ReLU}\to f_{c_{2}}\ \Rightarrow E</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.p3.3" class="ltx_p"><span id="S4.SS2.p3.3.1" class="ltx_text" style="font-size:90%;">The fine-tuning processes are applied using the Adam optimizer with a learning rate of </span><math id="S4.SS2.p3.3.m1.1" class="ltx_Math" alttext="1e^{-3}" display="inline"><semantics id="S4.SS2.p3.3.m1.1a"><mrow id="S4.SS2.p3.3.m1.1.1" xref="S4.SS2.p3.3.m1.1.1.cmml"><mn mathsize="90%" id="S4.SS2.p3.3.m1.1.1.2" xref="S4.SS2.p3.3.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p3.3.m1.1.1.1" xref="S4.SS2.p3.3.m1.1.1.1.cmml">â€‹</mo><msup id="S4.SS2.p3.3.m1.1.1.3" xref="S4.SS2.p3.3.m1.1.1.3.cmml"><mi mathsize="90%" id="S4.SS2.p3.3.m1.1.1.3.2" xref="S4.SS2.p3.3.m1.1.1.3.2.cmml">e</mi><mrow id="S4.SS2.p3.3.m1.1.1.3.3" xref="S4.SS2.p3.3.m1.1.1.3.3.cmml"><mo mathsize="90%" id="S4.SS2.p3.3.m1.1.1.3.3a" xref="S4.SS2.p3.3.m1.1.1.3.3.cmml">âˆ’</mo><mn mathsize="90%" id="S4.SS2.p3.3.m1.1.1.3.3.2" xref="S4.SS2.p3.3.m1.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m1.1b"><apply id="S4.SS2.p3.3.m1.1.1.cmml" xref="S4.SS2.p3.3.m1.1.1"><times id="S4.SS2.p3.3.m1.1.1.1.cmml" xref="S4.SS2.p3.3.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p3.3.m1.1.1.2.cmml" xref="S4.SS2.p3.3.m1.1.1.2">1</cn><apply id="S4.SS2.p3.3.m1.1.1.3.cmml" xref="S4.SS2.p3.3.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p3.3.m1.1.1.3.1.cmml" xref="S4.SS2.p3.3.m1.1.1.3">superscript</csymbol><ci id="S4.SS2.p3.3.m1.1.1.3.2.cmml" xref="S4.SS2.p3.3.m1.1.1.3.2">ğ‘’</ci><apply id="S4.SS2.p3.3.m1.1.1.3.3.cmml" xref="S4.SS2.p3.3.m1.1.1.3.3"><minus id="S4.SS2.p3.3.m1.1.1.3.3.1.cmml" xref="S4.SS2.p3.3.m1.1.1.3.3"></minus><cn type="integer" id="S4.SS2.p3.3.m1.1.1.3.3.2.cmml" xref="S4.SS2.p3.3.m1.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m1.1c">1e^{-3}</annotation></semantics></math><span id="S4.SS2.p3.3.2" class="ltx_text" style="font-size:90%;"> and a warm-up strategy. We used cross-entropy loss as the metric with a batch size of 32. The maximum number of epochs was set to 500 for all experiments, with an early stopping strategy with a patience of 12 to prevent overfitting. In practice, the maximum number of epochs was never reached, as the fine-tuning process was halted earlier due to the activation of the patience parameter. The overall number of parameters for the fine-tuned models are as follows: 32.3M for the uni-modal scheme based on video, 28.7M for the uni-modal scheme based on audio, and 62.7M for the multi-modal scheme.</span></p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p"><span id="S4.SS3.p1.1.1" class="ltx_text" style="font-size:90%;">TableÂ </span><span class="ltx_ref ltx_missing_label ltx_ref_self" style="font-size:90%;">LABEL:ACE_results</span><span id="S4.SS3.p1.1.2" class="ltx_text" style="font-size:90%;"> details the Accuracy of the various emotion recognition schemes, for seven different rooms from the </span><abbr title="Acoustic Characterisation of Environments" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">ACE</span></abbr><span id="S4.SS3.p1.1.3" class="ltx_text" style="font-size:90%;"> database. The video-only modality is compared with the audio-only modality (both single- and multi-channel models) and the combined multi-modal approach. As the video modality is unaffected by the acoustic conditions, we only report the results once.
To assess the reliability of our results, we report the mean results together with 75% confidence intervals.</span><span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span id="footnote3.1" class="ltx_text ltx_font_typewriter">github.com/luferrer/ConfidenceIntervals</span></span></span></span><span id="S4.SS3.p1.1.4" class="ltx_text" style="font-size:90%;">
Analyzing TableÂ </span><span class="ltx_ref ltx_missing_label ltx_ref_self" style="font-size:90%;">LABEL:ACE_results</span><span id="S4.SS3.p1.1.5" class="ltx_text" style="font-size:90%;">, it is observed that for the audio-only schemes, the multi-channel processing methods (Avg mel and Sum PE) consistently outperform the single-channel approaches. This is inline with the findings of </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS3.p1.1.6.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S4.SS3.p1.1.7.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS3.p1.1.8" class="ltx_text" style="font-size:90%;">. Notably, the multi-modal approaches significantly outperform their uni-modal counterparts.
These results are also visually demonstrated in Fig.Â </span><a href="#S4.F2" title="Figure 2 â€£ 4.1 Dataset â€£ 4 Experimental Study â€£ Multi-Microphone and Multi-Modal Emotion Recognition in Reverberant Environment" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S4.SS3.p1.1.9" class="ltx_text" style="font-size:90%;">, demonstrating the advantages of the multi-modal processing.</span></p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text" style="font-size:90%;">In this paper, we presented a </span><abbr title="Multi-modal Emotion Recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MER</span></abbr><span id="S5.p1.1.2" class="ltx_text" style="font-size:90%;"> system designed to operate in reverberant and noisy acoustic environments. Our approach demonstrates robust performance across a range of realistic acoustic conditions by combining an extended multi-channel </span><abbr title="Hierarchical Token-semantic Audio Transformer" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HTS-AT</span></abbr><span id="S5.p1.1.3" class="ltx_text" style="font-size:90%;"> for audio processing with an </span><math id="S5.p1.1.m1.1" class="ltx_Math" alttext="R(2+1)" display="inline"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mi mathsize="90%" id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">â€‹</mo><mrow id="S5.p1.1.m1.1.1.1.1" xref="S5.p1.1.m1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S5.p1.1.m1.1.1.1.1.2" xref="S5.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S5.p1.1.m1.1.1.1.1.1" xref="S5.p1.1.m1.1.1.1.1.1.cmml"><mn mathsize="90%" id="S5.p1.1.m1.1.1.1.1.1.2" xref="S5.p1.1.m1.1.1.1.1.1.2.cmml">2</mn><mo mathsize="90%" id="S5.p1.1.m1.1.1.1.1.1.1" xref="S5.p1.1.m1.1.1.1.1.1.1.cmml">+</mo><mn mathsize="90%" id="S5.p1.1.m1.1.1.1.1.1.3" xref="S5.p1.1.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo maxsize="90%" minsize="90%" id="S5.p1.1.m1.1.1.1.1.3" xref="S5.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><times id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2"></times><ci id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3">ğ‘…</ci><apply id="S5.p1.1.m1.1.1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1.1"><plus id="S5.p1.1.m1.1.1.1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1.1.1.1"></plus><cn type="integer" id="S5.p1.1.m1.1.1.1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.1.1.1.2">2</cn><cn type="integer" id="S5.p1.1.m1.1.1.1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.1.1.1.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">R(2+1)</annotation></semantics></math><span id="S5.p1.1.4" class="ltx_text" style="font-size:90%;">D model for video analysis. The </span><abbr title="Multi-modal Emotion Recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MER</span></abbr><span id="S5.p1.1.5" class="ltx_text" style="font-size:90%;"> system combines audio and visual modalities, consistently outperforming uni-modal approaches. By using synthetic </span><abbr title="Room Impulse Responses" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RIRs</span></abbr><span id="S5.p1.1.6" class="ltx_text" style="font-size:90%;"> for training and real-world </span><abbr title="Room Impulse Responses" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short-plural">RIRs</span></abbr><span id="S5.p1.1.7" class="ltx_text" style="font-size:90%;"> from the ACE database for testing, we provide a comprehensive assessment of our systemâ€™s performance in diverse acoustic environments. Moreover, the utilization of multi-channel audio processing, particularly the Patch-Embed summation, proves beneficial in mitigating the effects of reverberation and noise over the single-channel case.
This leads to the potential of our </span><abbr title="Multi-modal Emotion Recognition" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MER</span></abbr><span id="S5.p1.1.8" class="ltx_text" style="font-size:90%;"> system for applications in various real-world scenarios where acoustic conditions may be far from ideal. Future work could focus on further improving the systemâ€™s performance in extremely reverberant environments and exploring its effectiveness in other emotional datasets.</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
SantoshÂ Kumar Bharti, SÂ Varadhaganapathy, RajeevÂ Kumar Gupta, PrashantÂ Kumar
Shukla, Mohamed Bouye, SimonÂ Karanja Hingaa, and Amena Mahmoud,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">â€œText-based emotion recognition using deep learning approach,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computational Intelligence and Neuroscience</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, vol. 2022, no. 1,
pp. 2645381, 2022.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Dalia Sherman, Gershon Hazan, and Sharon Gannot,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">â€œStudy of speech emotion recognition using BLSTM with attention,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Signal Processing Conf. (EUSIPCO)</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, Helsinki,
Finland, Sept. 2023.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Siwei Zhou, Xuemei Wu, Fan Jiang, Qionghao Huang, and Changqin Huang,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">â€œEmotion recognition from large-scale video clips with
cross-attention and hybrid feature weighting neural networks,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Journal of Environmental Research and Public Health</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, vol.
20, no. 2, pp. 1400, 2023.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Vijay John and Yasutomo Kawanishi,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">â€œAudio and video-based emotion recognition using multimodal
transformers,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. on Pattern Recognition (ICPR)</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2022, pp.
2582â€“2588.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Fatemeh Noroozi, Marina Marjanovic, Angelina Njegus, Sergio Escalera, and
Gholamreza Anbarjafari,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">â€œAudio-visual emotion recognition in video clips,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Trans. on Affective Computing</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, vol. 10, no. 1, pp. 60â€“75,
2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Wenliang Dai, Samuel Cahyawijaya, Zihan Liu, and Pascale Fung,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">â€œMultimodal end-to-end sparse model for emotion recognition,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2103.09666</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Ioannis Kansizoglou, Loukas Bampis, and Antonios Gasteratos,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">â€œAn active learning paradigm for online audio-visual emotion
recognition,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Trans. on Affective Computing</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Riccardo Franceschini, Enrico Fini, Cigdem Beyan, Alessandro Conti, Federica
Arrigoni, and Elisa Ricci,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">â€œMultimodal emotion recognition with modality-pairwise unsupervised
contrastive loss,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2207.11482</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Che-Wei Huang and Shrikanth Narayanan,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">â€œDeep convolutional recurrent neural network with attention
mechanism for robust speech emotion recognition,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Int. Conf. on multimedia and expo (ICME)</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2017, pp.
583â€“588.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Minji Seo and Myungho Kim,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">â€œFusing visual attention cnn and bag of visual words for
cross-corpus speech emotion recognition,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Sensors</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, vol. 20, no. 19, pp. 5559, 2020.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Yanan Song, Yuanyang Cai, and Lizhe Tan,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">â€œVideo-audio emotion recognition based on feature fusion deep
learning method,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Int. Midwest Symposium on Circuits and Systems
(MWSCAS)</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, 2021, pp. 611â€“616.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Esam Ghaleb, Mirela Popa, and Stylianos Asteriadis,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">â€œMultimodal and temporal perception of audio-visual cues for emotion
recognition,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. on Affective Computing and Intelligent Interaction
(ACII)</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2019, pp. 552â€“558.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Florian Schroff, Dmitry Kalenichenko, and James Philbin,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">â€œFacenet: A unified embedding for face recognition and clustering,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conf. on computer vision and pattern
recognition</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2015, pp. 815â€“823.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
AmirAliÂ Bagher Zadeh, PaulÂ Pu Liang, Soujanya Poria, Erik Cambria, and
Louis-Philippe Morency,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">â€œMultimodal language analysis in the wild: CMU-MOSEI dataset and
interpretable dynamic fusion graph,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. of the Annual Meeting of the Association for
Computational Linguistics</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 2236â€“2246.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Jeffrey Pennington, Richard Socher, and ChristopherÂ D Manning,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">â€œGloVe: Global vectors for word representation,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conf. on Empirical Methods in Natural Language Processing
(EMNLP)</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2014, pp. 1532â€“1543.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, and Dinesh
Manocha,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">â€œM3er: Multiplicative multimodal emotion recognition using facial,
textual, and speech cues,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI Conf. on Artificial Intelligence</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2020, vol.Â 34, pp.
1359â€“1367.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Jean-Benoit Delbrouck, NoÃ© Tits, and StÃ©phane Dupont,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">â€œModulated fusion using transformer for linguistic-acoustic emotion
recognition,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2010.02057</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Ohad Cohen, Gershon Hazan, and Sharon Gannot,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">â€œMulti-microphone speech emotion recognition using the hierarchical
token-semantic audio transformer architecture,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2406.03272</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
DuÂ Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar
Paluri,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">â€œA closer look at spatiotemporal convolutions for action
recognition,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conf. on Computer Vision and Pattern
Recognition</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 6450â€“6459.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
James Eaton, NikolayÂ D. Gaubitch, AlastairÂ H. Moore, and PatrickÂ A. Naylor,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">â€œEstimation of room acoustic parameters: The ACE challenge,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/ACM Trans. on Audio, Speech, and Language Processing</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, vol.
24, no. 10, pp. 1681â€“1693, 2016.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
DanielÂ S Park, William Chan, YuÂ Zhang, Chung-Cheng Chiu, Barret Zoph, EkinÂ D
Cubuk, and QuocÂ V Le,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">â€œSpecaugment: A simple data augmentation method for automatic speech
recognition,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1904.08779</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
TorchVision maintainers and contributors,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">â€œTorchVision: PyTorchâ€™s Computer Vision library,â€
</span><span id="bib.bib22.3.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">https://github.com/pytorch/vision</span><span id="bib.bib22.4.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
ZeÂ Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">â€œSwin transformer: Hierarchical vision transformer using shifted
windows,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE/CVF Int. Conf. on computer vision</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2021, pp.
10012â€“10022.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">â€œAn image is worth 16x16 words: Transformers for image recognition
at scale,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. Conf. on Learning Representations (ICLR)</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra
Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, etÂ al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">â€œThe kinetics human action video dataset,â€
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1705.06950</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.09544" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.09545" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.09545">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.09545" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.09546" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 01:15:05 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
