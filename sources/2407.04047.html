<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.04047] Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis</title><meta property="og:description" content="This paper investigates the use of unsupervised text-to-speech synthesis (TTS) as a data augmentation method to improve accented speech recognition. TTS systems are trained with a small amount of accented speech traini…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.04047">

<!--Generated on Mon Aug  5 17:33:10 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Accented speech recognition,  text-to-speech synthesis,  data augmentation,  self-supervised learning,  Wav2vec2.0
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Cong-Thanh Do
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">Toshiba Research Europe</span>
<br class="ltx_break">Cambridge, UK 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter" style="font-size:80%;">cong-thanh.do@toshiba.eu</span>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuhei Imai
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_font_italic">Tohoku University</span>
<br class="ltx_break">Sendai, Japan 
<br class="ltx_break"><span id="id4.2.id2" class="ltx_text ltx_font_typewriter" style="font-size:80%;">shuhei.imai@tohoku.ac.jp</span>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rama Doddipatla
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_font_italic">Toshiba Research Europe</span>
<br class="ltx_break">Cambridge, UK 
<br class="ltx_break"><span id="id6.2.id2" class="ltx_text ltx_font_typewriter" style="font-size:80%;">rama.doddipatla@toshiba.eu</span>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Thomas Hain
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_font_italic">University of Sheffield</span>
<br class="ltx_break">Sheffield, UK 
<br class="ltx_break"><span id="id8.2.id2" class="ltx_text ltx_font_typewriter" style="font-size:80%;">t.hain@sheffield.ac.uk</span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">This paper investigates the use of unsupervised text-to-speech synthesis (TTS) as a data augmentation method to improve accented speech recognition. TTS systems are trained with a small amount of accented speech training data and their pseudo-labels rather than manual transcriptions, and hence unsupervised. This approach enables the use of accented speech data without manual transcriptions to perform data augmentation for accented speech recognition. Synthetic accented speech data, generated from text prompts by using the TTS systems, are then combined with available non-accented speech data to train automatic speech recognition (ASR) systems. ASR experiments are performed in a self-supervised learning framework using a Wav2vec2.0 model which was pre-trained on large amount of unsupervised accented speech data. The accented speech data for training the unsupervised TTS are read speech, selected from L2-ARCTIC and British Isles corpora, while spontaneous conversational speech from the Edinburgh international accents of English corpus are used as the evaluation data. Experimental results show that Wav2vec2.0 models which are fine-tuned to downstream ASR task with synthetic accented speech data, generated by the unsupervised TTS, yield up to 6.1% relative word error rate reductions compared to a Wav2vec2.0 baseline which is fine-tuned with the non-accented speech data from Librispeech corpus.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Accented speech recognition, text-to-speech synthesis, data augmentation, self-supervised learning, Wav2vec2.0

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Accented speech recognition is an important research topic of automatic speech recognition (ASR). Because of its importance, this research topic has been receiving attention and being addressed with various research approaches. In general, these approaches can be classified as accent-agnostic approaches, in which the modeling of accents inside the ASR systems is not made specific, and accent-aware approaches in which additional information about the accents of the input speech are used <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Among accent-agnostic approaches, adversarial learning was used to establish accent classifier and accent relabeling which led to performance improvement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. In addition, similarity losses such as cosine losses or contrastive losses were used to build accent neutral models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. In accent-aware approaches, multi-domain training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, accent embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, or accent information fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> are among the approaches which have been investigated.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Text-to-speech synthesis (TTS) is an useful technology which can be used to improve ASR in a number of ways, for instance to improve the pre-training of self-supervised learning (SSL) models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> or to improve the recognition of out-of-vocabulary words in end-to-end ASR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. TTS was also used as a data augmentation method to improve speech recognition for Librispeech task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and low-resource speech recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. More specifically, synthetic data were used for data augmentation in the context of low-resource ASR using conventional hybrid structure <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and to augment the training of RNN-T (recurrent neural network - transducer) ASR model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, cross-lingual multi-speaker speech synthesis and cross-lingual voice conversion were applied to data augmentation for ASR. The authors showed that it is possible to achieve promising results for ASR model training with just a single speaker dataset in a target language, making it viable for low-resource scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">While having been widely used in various ASR tasks, TTS, especially unsupervised TTS which is trained with unsupervised audio data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, has not been extensively studied as a data augmentation method in accented speech recognition. In a recent study on using TTS as data augmentation for accented speech recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, accented speech were generated by passing English text prompts through TTS system for a language corresponding to the target accent. For example, English text prompts passing through Spanish TTS will approximate Spanish-accented English. The study in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> used commercial TTS systems whose training data were not accessible by users.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we investigate the use of unsupervised TTS as a data augmentation method to improve accented speech recognition. In our approach, we make use of a small amount of accented speech data which do not have manual transcriptions to train TTS systems. This approach enables the use of accented speech data without manual transcriptions to perform data augmentation for accented speech recognition. Indeed, from a small amount of unsupervised accented speech data used to train the TTS systems, we can generate larger amount of synthetic accented speech data once the TTS systems are trained. In this paper, from 58 hours of accented speech data, selected from two speech corpora of read speech: L2-ARCTIC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and British Isles <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, we train unsupervised TTS and generate 250 hours more of synthetic accented speech data which help to achieve better gains on the evaluation data of spontaneous conversational speech from the Edinburgh international accents of English corpus (EdAcc) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The paper is organized as follows. In section <a href="#S2" title="II Data Augmentation for Accented Speech Recognition based on Unsupervised TTS ‣ Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, details of the data augmentation for accented speech recognition based on unsupervised TTS are introduced. The training and inference of TTS systems are presented in section <a href="#S3" title="III Text-to-speech Synthesis ‣ Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. Section <a href="#S4" title="IV Experiments ‣ Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> introduces the data used in the experiments, experimental results, and discussion. Finally, section <a href="#S5" title="V Conclusion ‣ Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> concludes the paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Data Augmentation for Accented Speech Recognition based on Unsupervised TTS</span>
</h2>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2407.04047/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="438" height="428" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span> Unsupervised accented speech training data and theirs pseudo-labels are used to train unsupervised TTS. The pseudo-labels are generated by decoding the unsupervised accented speech training data using the baseline ASR model obtained from the supervised fine-tuning of the SSL pre-trained model with the supervised non-accented speech data. The unsupervised accented speech training data may be included in the semi-supervised fine-tuning for ASR, and the non-accented speech data may be used to train a TTS system.</figcaption>
</figure>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2407.04047/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="327" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span> Supervised accented speech training data are used to train supervised TTS. These data may be included in the supervised fine-tuning for ASR, and the non-accented speech training data may be used to train a TTS system. The fine-tuned ASR model is used in the ASR inference, with an external language model (LM), to decode test speech. </figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We use Wav2vec2.0 SSL framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> for our experiments with accented speech recognition. Assume that a Wav2vec2.0 model was pre-trained via SSL on large amount of unsupervised speech data to cover various English accents and speakers, we can fine-tune this pre-trained model to downstream ASR task using available non-accented speech data. The non-accented speech data could be any available data which can be used to train ASR systems, for instance Librispeech training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. When using publicly available Wav2vec2.0 pre-trained models, we assume that only the models are available and their training data are not available.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In addition to the non-accented speech data, we assume that a small amount of accented speech training data, named AccD, is available. These accented speech training data will be used to train the TTS systems. In accented speech recognition, it is not practical to find accented speech training data spoken in the same speaking styles and by the same speakers in the evaluation data. It is actually more viable to find accented speech training data which are spoken by speakers whose first languages are similar to those of the speakers in the evaluation data. Using these speech data to train TTS systems and generate more accented speech data for ASR training should create more accent variability, and hence, improve accented speech recognition performance.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Unsupervised scenario</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Fig. <a href="#S2.F1" title="Figure 1 ‣ II Data Augmentation for Accented Speech Recognition based on Unsupervised TTS ‣ Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows unsupervised scenario where the manual transcriptions of the accented speech training data AccD are not available. Hence, pseudo-labels for the unsupervised accented speech training data are generated by decoding these data using the baseline ASR model obtained by fine-tuning the SSL pre-trained model with the supervised non-accented speech data. The unsupervised accented speech training data AccD and their pseudo-labels are then used to train TTS model, which is a Variational Inference with adversarial learning for end-to-end Text-to-Speech (VITS) model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, to generate synthetic accented speech data for data augmentation. The unsupervised accented speech training data and their pseudo-labels may be included in the semi-supervised fine-tuning of the SSL pre-trained model. The ASR model obtained after the semi-supervised fine-tuning is used in the ASR inference to decode input test speech. The ASR inferences use an external language model (LM) when decoding audio data.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">A word-level 4-gram LM is used as external LM during ASR inferences. This 4-gram LM is trained on the manual transcriptions of Librispeech training data. The pre-training and fine-tuning of the Wav2vec2.0 models as well as the inference follow the same settings used for the LARGE Wav2vec2.0 models in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. These large models consist of 6 convolutional neural network (CNN) and 24 transformer layers, and have 350 millions parameters.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Supervised scenario</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">For comparison, we also examine supervised scenario where the manual transcriptions of the accented speech training data AccD are available. In Fig. <a href="#S2.F2" title="Figure 2 ‣ II Data Augmentation for Accented Speech Recognition based on Unsupervised TTS ‣ Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the accented speech training data AccD and their manual transcriptions can be directly used to train supervised TTS model. They may also be included in the training data which are used for the supervised fine-tuning of the SSL pre-trained model. Once the TTS model is trained, it can be used during the TTS inference to generate synthetic accented speech data using independent text prompts. Both the synthetic accented speech data and the text prompts can then be used for data augmentation in the supervised fine-tuning of the SSL pre-trained model.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Text-to-speech Synthesis</span>
</h2>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2407.04047/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="276" height="403" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> Training of VITS parallel end-to-end TTS system. The input text can be either manual transcriptions or pseudo-labels.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.4" class="ltx_p">VITS is an end-to-end multi-speaker TTS system which can generate high-quality waveforms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. During the training of VITS (see Fig. <a href="#S3.F3" title="Figure 3 ‣ III Text-to-speech Synthesis ‣ Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), a Posterior Encoder encodes linear spectrogram from natural speech into a latent variable <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">z</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> which is then used in a Decoder to restore waveform. HiFi-GAN (Generative Adversarial Network) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, a GAN-based neural vocoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, is used in the decoder to synthesize high-fidelity speech. The latent variable <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">z</annotation></semantics></math> is also fed into the Flow <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">f</annotation></semantics></math> which computes the Kullback-Leibler divergence with the Text Encoder outputs. The Flow <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.p1.4.m4.1a"><mi id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">f</annotation></semantics></math> is trained to remove speaker information and reduce posterior complexity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. During training, speakers identities (IDs) are used to extract speaker embeddings for training multi-speaker TTS.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.4" class="ltx_p">During TTS inference (see Fig. <a href="#S4.F4" title="Figure 4 ‣ IV Experiments ‣ Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), an inverse transform <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="f^{-1}" display="inline"><semantics id="S3.p2.1.m1.1a"><msup id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><mi id="S3.p2.1.m1.1.1.2" xref="S3.p2.1.m1.1.1.2.cmml">f</mi><mrow id="S3.p2.1.m1.1.1.3" xref="S3.p2.1.m1.1.1.3.cmml"><mo id="S3.p2.1.m1.1.1.3a" xref="S3.p2.1.m1.1.1.3.cmml">−</mo><mn id="S3.p2.1.m1.1.1.3.2" xref="S3.p2.1.m1.1.1.3.2.cmml">1</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p2.1.m1.1.1.1.cmml" xref="S3.p2.1.m1.1.1">superscript</csymbol><ci id="S3.p2.1.m1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.2">𝑓</ci><apply id="S3.p2.1.m1.1.1.3.cmml" xref="S3.p2.1.m1.1.1.3"><minus id="S3.p2.1.m1.1.1.3.1.cmml" xref="S3.p2.1.m1.1.1.3"></minus><cn type="integer" id="S3.p2.1.m1.1.1.3.2.cmml" xref="S3.p2.1.m1.1.1.3.2">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">f^{-1}</annotation></semantics></math> of the Flow <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.p2.2.m2.1a"><mi id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><ci id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">f</annotation></semantics></math> is used to synthesize speech. The output of the Text Encoder is stretched by the Length Regulator based on the predicted duration, and then the sampled latent variable <math id="S3.p2.3.m3.3" class="ltx_Math" alttext="z^{\prime}\sim\mathcal{N}(z^{\prime};\mu_{\theta}(text),\sigma_{\theta}(text))" display="inline"><semantics id="S3.p2.3.m3.3a"><mrow id="S3.p2.3.m3.3.3" xref="S3.p2.3.m3.3.3.cmml"><msup id="S3.p2.3.m3.3.3.5" xref="S3.p2.3.m3.3.3.5.cmml"><mi id="S3.p2.3.m3.3.3.5.2" xref="S3.p2.3.m3.3.3.5.2.cmml">z</mi><mo id="S3.p2.3.m3.3.3.5.3" xref="S3.p2.3.m3.3.3.5.3.cmml">′</mo></msup><mo id="S3.p2.3.m3.3.3.4" xref="S3.p2.3.m3.3.3.4.cmml">∼</mo><mrow id="S3.p2.3.m3.3.3.3" xref="S3.p2.3.m3.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p2.3.m3.3.3.3.5" xref="S3.p2.3.m3.3.3.3.5.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.p2.3.m3.3.3.3.4" xref="S3.p2.3.m3.3.3.3.4.cmml">​</mo><mrow id="S3.p2.3.m3.3.3.3.3.3" xref="S3.p2.3.m3.3.3.3.3.4.cmml"><mo stretchy="false" id="S3.p2.3.m3.3.3.3.3.3.4" xref="S3.p2.3.m3.3.3.3.3.4.cmml">(</mo><msup id="S3.p2.3.m3.1.1.1.1.1.1" xref="S3.p2.3.m3.1.1.1.1.1.1.cmml"><mi id="S3.p2.3.m3.1.1.1.1.1.1.2" xref="S3.p2.3.m3.1.1.1.1.1.1.2.cmml">z</mi><mo id="S3.p2.3.m3.1.1.1.1.1.1.3" xref="S3.p2.3.m3.1.1.1.1.1.1.3.cmml">′</mo></msup><mo id="S3.p2.3.m3.3.3.3.3.3.5" xref="S3.p2.3.m3.3.3.3.3.4.cmml">;</mo><mrow id="S3.p2.3.m3.2.2.2.2.2.2" xref="S3.p2.3.m3.2.2.2.2.2.2.cmml"><msub id="S3.p2.3.m3.2.2.2.2.2.2.3" xref="S3.p2.3.m3.2.2.2.2.2.2.3.cmml"><mi id="S3.p2.3.m3.2.2.2.2.2.2.3.2" xref="S3.p2.3.m3.2.2.2.2.2.2.3.2.cmml">μ</mi><mi id="S3.p2.3.m3.2.2.2.2.2.2.3.3" xref="S3.p2.3.m3.2.2.2.2.2.2.3.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S3.p2.3.m3.2.2.2.2.2.2.2" xref="S3.p2.3.m3.2.2.2.2.2.2.2.cmml">​</mo><mrow id="S3.p2.3.m3.2.2.2.2.2.2.1.1" xref="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.p2.3.m3.2.2.2.2.2.2.1.1.2" xref="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.cmml">(</mo><mrow id="S3.p2.3.m3.2.2.2.2.2.2.1.1.1" xref="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.cmml"><mi id="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.2" xref="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.1" xref="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.1.cmml">​</mo><mi id="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.3" xref="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.1a" xref="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.1.cmml">​</mo><mi id="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.4" xref="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.1b" xref="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.1.cmml">​</mo><mi id="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.5" xref="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.5.cmml">t</mi></mrow><mo stretchy="false" id="S3.p2.3.m3.2.2.2.2.2.2.1.1.3" xref="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.p2.3.m3.3.3.3.3.3.6" xref="S3.p2.3.m3.3.3.3.3.4.cmml">,</mo><mrow id="S3.p2.3.m3.3.3.3.3.3.3" xref="S3.p2.3.m3.3.3.3.3.3.3.cmml"><msub id="S3.p2.3.m3.3.3.3.3.3.3.3" xref="S3.p2.3.m3.3.3.3.3.3.3.3.cmml"><mi id="S3.p2.3.m3.3.3.3.3.3.3.3.2" xref="S3.p2.3.m3.3.3.3.3.3.3.3.2.cmml">σ</mi><mi id="S3.p2.3.m3.3.3.3.3.3.3.3.3" xref="S3.p2.3.m3.3.3.3.3.3.3.3.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S3.p2.3.m3.3.3.3.3.3.3.2" xref="S3.p2.3.m3.3.3.3.3.3.3.2.cmml">​</mo><mrow id="S3.p2.3.m3.3.3.3.3.3.3.1.1" xref="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.p2.3.m3.3.3.3.3.3.3.1.1.2" xref="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.cmml">(</mo><mrow id="S3.p2.3.m3.3.3.3.3.3.3.1.1.1" xref="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.cmml"><mi id="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.2" xref="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.1" xref="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.1.cmml">​</mo><mi id="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.3" xref="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.1a" xref="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.1.cmml">​</mo><mi id="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.4" xref="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.1b" xref="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.1.cmml">​</mo><mi id="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.5" xref="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.5.cmml">t</mi></mrow><mo stretchy="false" id="S3.p2.3.m3.3.3.3.3.3.3.1.1.3" xref="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.p2.3.m3.3.3.3.3.3.7" xref="S3.p2.3.m3.3.3.3.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.3b"><apply id="S3.p2.3.m3.3.3.cmml" xref="S3.p2.3.m3.3.3"><csymbol cd="latexml" id="S3.p2.3.m3.3.3.4.cmml" xref="S3.p2.3.m3.3.3.4">similar-to</csymbol><apply id="S3.p2.3.m3.3.3.5.cmml" xref="S3.p2.3.m3.3.3.5"><csymbol cd="ambiguous" id="S3.p2.3.m3.3.3.5.1.cmml" xref="S3.p2.3.m3.3.3.5">superscript</csymbol><ci id="S3.p2.3.m3.3.3.5.2.cmml" xref="S3.p2.3.m3.3.3.5.2">𝑧</ci><ci id="S3.p2.3.m3.3.3.5.3.cmml" xref="S3.p2.3.m3.3.3.5.3">′</ci></apply><apply id="S3.p2.3.m3.3.3.3.cmml" xref="S3.p2.3.m3.3.3.3"><times id="S3.p2.3.m3.3.3.3.4.cmml" xref="S3.p2.3.m3.3.3.3.4"></times><ci id="S3.p2.3.m3.3.3.3.5.cmml" xref="S3.p2.3.m3.3.3.3.5">𝒩</ci><list id="S3.p2.3.m3.3.3.3.3.4.cmml" xref="S3.p2.3.m3.3.3.3.3.3"><apply id="S3.p2.3.m3.1.1.1.1.1.1.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1">superscript</csymbol><ci id="S3.p2.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.p2.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.p2.3.m3.1.1.1.1.1.1.3">′</ci></apply><apply id="S3.p2.3.m3.2.2.2.2.2.2.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2"><times id="S3.p2.3.m3.2.2.2.2.2.2.2.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2.2"></times><apply id="S3.p2.3.m3.2.2.2.2.2.2.3.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.p2.3.m3.2.2.2.2.2.2.3.1.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2.3">subscript</csymbol><ci id="S3.p2.3.m3.2.2.2.2.2.2.3.2.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2.3.2">𝜇</ci><ci id="S3.p2.3.m3.2.2.2.2.2.2.3.3.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2.3.3">𝜃</ci></apply><apply id="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2.1.1"><times id="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.1.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.1"></times><ci id="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.2.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.2">𝑡</ci><ci id="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.3.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.3">𝑒</ci><ci id="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.4.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.4">𝑥</ci><ci id="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.5.cmml" xref="S3.p2.3.m3.2.2.2.2.2.2.1.1.1.5">𝑡</ci></apply></apply><apply id="S3.p2.3.m3.3.3.3.3.3.3.cmml" xref="S3.p2.3.m3.3.3.3.3.3.3"><times id="S3.p2.3.m3.3.3.3.3.3.3.2.cmml" xref="S3.p2.3.m3.3.3.3.3.3.3.2"></times><apply id="S3.p2.3.m3.3.3.3.3.3.3.3.cmml" xref="S3.p2.3.m3.3.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.p2.3.m3.3.3.3.3.3.3.3.1.cmml" xref="S3.p2.3.m3.3.3.3.3.3.3.3">subscript</csymbol><ci id="S3.p2.3.m3.3.3.3.3.3.3.3.2.cmml" xref="S3.p2.3.m3.3.3.3.3.3.3.3.2">𝜎</ci><ci id="S3.p2.3.m3.3.3.3.3.3.3.3.3.cmml" xref="S3.p2.3.m3.3.3.3.3.3.3.3.3">𝜃</ci></apply><apply id="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.cmml" xref="S3.p2.3.m3.3.3.3.3.3.3.1.1"><times id="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.1.cmml" xref="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.1"></times><ci id="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.2.cmml" xref="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.2">𝑡</ci><ci id="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.3.cmml" xref="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.3">𝑒</ci><ci id="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.4.cmml" xref="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.4">𝑥</ci><ci id="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.5.cmml" xref="S3.p2.3.m3.3.3.3.3.3.3.1.1.1.5">𝑡</ci></apply></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.3c">z^{\prime}\sim\mathcal{N}(z^{\prime};\mu_{\theta}(text),\sigma_{\theta}(text))</annotation></semantics></math> is transformed by the inverse Flow <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="f^{-1}" display="inline"><semantics id="S3.p2.4.m4.1a"><msup id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml"><mi id="S3.p2.4.m4.1.1.2" xref="S3.p2.4.m4.1.1.2.cmml">f</mi><mrow id="S3.p2.4.m4.1.1.3" xref="S3.p2.4.m4.1.1.3.cmml"><mo id="S3.p2.4.m4.1.1.3a" xref="S3.p2.4.m4.1.1.3.cmml">−</mo><mn id="S3.p2.4.m4.1.1.3.2" xref="S3.p2.4.m4.1.1.3.2.cmml">1</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><apply id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p2.4.m4.1.1.1.cmml" xref="S3.p2.4.m4.1.1">superscript</csymbol><ci id="S3.p2.4.m4.1.1.2.cmml" xref="S3.p2.4.m4.1.1.2">𝑓</ci><apply id="S3.p2.4.m4.1.1.3.cmml" xref="S3.p2.4.m4.1.1.3"><minus id="S3.p2.4.m4.1.1.3.1.cmml" xref="S3.p2.4.m4.1.1.3"></minus><cn type="integer" id="S3.p2.4.m4.1.1.3.2.cmml" xref="S3.p2.4.m4.1.1.3.2">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">f^{-1}</annotation></semantics></math> together with speaker information. Speech is subsequently generated by the decoder. All the VITS systems in this paper use the same architecture and are trained with the same number of training iterations, i.e. 300K. We observe that after 300K training iterations, the quality of the synthesized waveforms saturates. Greater details of the VITS models and their implementation can be found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The Wav2vec2.0 model is pre-trained with more than 60K hours of unsupervised speech data from Libri-Light, CommonVoice, Switchboard, and Fisher corpora. These speech training data were spoken in various English accents. The non-accented speech data consist of 960 hours of training data from Librispeech corpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> which include 2200 speakers. Although these speakers spoke US-English, we consider Librispeech as non-accented in the context of this study because US-English is only one of the English accents in the evaluation data. Subsequent experimental results confirm our assumption.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2407.04047/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="198" height="405" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span> TTS inference using VITS model where text prompt and speaker ID are used as input. </figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Data</span>
</h3>

<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS1.5.1.1" class="ltx_text">IV-A</span>1 </span>Accented speech training data (AccD)</h4>

<figure id="S4.T1" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Word error rates (WERs) on the development and test sets of the Edinburgh international accents of English corpus (EdAcc), and on the test-clean and test-other sets of Librispeech (LS) corpus.</figcaption>
<br class="ltx_break ltx_break">
<table id="S4.SS1.SSS1.1.1.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.SS1.SSS1.1.1.1.1" class="ltx_tr">
<td id="S4.SS1.SSS1.1.1.1.1.1" class="ltx_td ltx_nopad ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><svg version="1.1" height="19.72" width="364.17" overflow="visible"><g transform="translate(0,19.72) scale(1,-1)"><path d="M 0,19.72 364.17,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,11.07) scale(1, -1)"><foreignObject width="91.84" height="11.07" overflow="visible">
<span id="S4.SS1.SSS1.1.1.1.1.1.pic1.1.1" class="ltx_inline-block">
<span id="S4.SS1.SSS1.1.1.1.1.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S4.SS1.SSS1.1.1.1.1.1.pic1.1.1.1.1" class="ltx_p"><span id="S4.SS1.SSS1.1.1.1.1.1.pic1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Fine-tuning data</span></span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(312.67,11.07)"><g transform="translate(0,8.65) scale(1, -1)"><foreignObject width="51.5" height="8.65" overflow="visible">
<span id="S4.SS1.SSS1.1.1.1.1.1.pic1.2.1" class="ltx_inline-block">
<span id="S4.SS1.SSS1.1.1.1.1.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S4.SS1.SSS1.1.1.1.1.1.pic1.2.1.1.1" class="ltx_p"><span id="S4.SS1.SSS1.1.1.1.1.1.pic1.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Test data</span></span>
</span>
</span></foreignObject></g></g></g></svg></td>
<td id="S4.SS1.SSS1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.1.2.1" class="ltx_text" style="font-size:90%;">EdAcc dev-set</span></td>
<td id="S4.SS1.SSS1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.1.3.1" class="ltx_text" style="font-size:90%;">EdAcc test-set</span></td>
<td id="S4.SS1.SSS1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.1.4.1" class="ltx_text" style="font-size:90%;">LS test-clean</span></td>
<td id="S4.SS1.SSS1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.1.5.1" class="ltx_text" style="font-size:90%;">LS test-other</span></td>
</tr>
<tr id="S4.SS1.SSS1.1.1.1.2.1" class="ltx_tr">
<td id="S4.SS1.SSS1.1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.2.1.1.1" class="ltx_text" style="font-size:90%;">LS 960h (M) (baseline in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>)</span></td>
<td id="S4.SS1.SSS1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.2.1.2.1" class="ltx_text" style="font-size:90%;">33.4</span></td>
<td id="S4.SS1.SSS1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.2.1.3.1" class="ltx_text" style="font-size:90%;">36.1</span></td>
<td id="S4.SS1.SSS1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.2.1.4.1" class="ltx_text" style="font-size:90%;">2.9</span></td>
<td id="S4.SS1.SSS1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.2.1.5.1" class="ltx_text" style="font-size:90%;">5.6</span></td>
</tr>
<tr id="S4.SS1.SSS1.1.1.1.3.2" class="ltx_tr">
<td id="S4.SS1.SSS1.1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">LS 960h (M) (our baseline)</span></td>
<td id="S4.SS1.SSS1.1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.3.2.2.1" class="ltx_text" style="font-size:90%;">32.8</span></td>
<td id="S4.SS1.SSS1.1.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.3.2.3.1" class="ltx_text" style="font-size:90%;">35.1</span></td>
<td id="S4.SS1.SSS1.1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.3.2.4.1" class="ltx_text" style="font-size:90%;">2.2</span></td>
<td id="S4.SS1.SSS1.1.1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.3.2.5.1" class="ltx_text" style="font-size:90%;">4.2</span></td>
</tr>
<tr id="S4.SS1.SSS1.1.1.1.4.3" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.SS1.SSS1.1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.4.3.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">LS 960h (M) + AccD (P)</span></td>
<td id="S4.SS1.SSS1.1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.4.3.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">32.4</span></td>
<td id="S4.SS1.SSS1.1.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.4.3.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">34.6</span></td>
<td id="S4.SS1.SSS1.1.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.4.3.4.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">2.1</span></td>
<td id="S4.SS1.SSS1.1.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.4.3.5.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">4.1</span></td>
</tr>
<tr id="S4.SS1.SSS1.1.1.1.5.4" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.SS1.SSS1.1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.5.4.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">LS 960h (M) + AccD (M)</span></td>
<td id="S4.SS1.SSS1.1.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.5.4.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">31.1</span></td>
<td id="S4.SS1.SSS1.1.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.5.4.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">33.4</span></td>
<td id="S4.SS1.SSS1.1.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.5.4.4.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">2.1</span></td>
<td id="S4.SS1.SSS1.1.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.5.4.5.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">4.0</span></td>
</tr>
<tr id="S4.SS1.SSS1.1.1.1.6.5" class="ltx_tr">
<td id="S4.SS1.SSS1.1.1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.6.5.1.1" class="ltx_text" style="font-size:90%;">LS 960h (M) + TTS-LS 960h (M)</span></td>
<td id="S4.SS1.SSS1.1.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.6.5.2.1" class="ltx_text" style="font-size:90%;">31.4</span></td>
<td id="S4.SS1.SSS1.1.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.6.5.3.1" class="ltx_text" style="font-size:90%;">33.8</span></td>
<td id="S4.SS1.SSS1.1.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.6.5.4.1" class="ltx_text" style="font-size:90%;">2.1</span></td>
<td id="S4.SS1.SSS1.1.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.6.5.5.1" class="ltx_text" style="font-size:90%;">4.0</span></td>
</tr>
<tr id="S4.SS1.SSS1.1.1.1.7.6" class="ltx_tr">
<td id="S4.SS1.SSS1.1.1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.7.6.1.1" class="ltx_text" style="font-size:90%;">LS 960h (M) + TTS-AccD (P)</span></td>
<td id="S4.SS1.SSS1.1.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.7.6.2.1" class="ltx_text" style="font-size:90%;">31.0</span></td>
<td id="S4.SS1.SSS1.1.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.7.6.3.1" class="ltx_text" style="font-size:90%;">33.2</span></td>
<td id="S4.SS1.SSS1.1.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.7.6.4.1" class="ltx_text" style="font-size:90%;">2.1</span></td>
<td id="S4.SS1.SSS1.1.1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.7.6.5.1" class="ltx_text" style="font-size:90%;">4.1</span></td>
</tr>
<tr id="S4.SS1.SSS1.1.1.1.8.7" class="ltx_tr">
<td id="S4.SS1.SSS1.1.1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.8.7.1.1" class="ltx_text" style="font-size:90%;">LS 960h (M) + TTS-AccD (M)</span></td>
<td id="S4.SS1.SSS1.1.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.8.7.2.1" class="ltx_text" style="font-size:90%;">30.8</span></td>
<td id="S4.SS1.SSS1.1.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.8.7.3.1" class="ltx_text" style="font-size:90%;">33.0</span></td>
<td id="S4.SS1.SSS1.1.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.8.7.4.1" class="ltx_text" style="font-size:90%;">2.1</span></td>
<td id="S4.SS1.SSS1.1.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.8.7.5.1" class="ltx_text" style="font-size:90%;">4.1</span></td>
</tr>
<tr id="S4.SS1.SSS1.1.1.1.9.8" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.SS1.SSS1.1.1.1.9.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.9.8.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">LS 960h (M) + AccD (P) + TTS-AccD (P) + TTS-LS 960h (M)</span></td>
<td id="S4.SS1.SSS1.1.1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.9.8.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">30.8</span></td>
<td id="S4.SS1.SSS1.1.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.9.8.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">33.2</span></td>
<td id="S4.SS1.SSS1.1.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.9.8.4.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">2.1</span></td>
<td id="S4.SS1.SSS1.1.1.1.9.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.SS1.SSS1.1.1.1.9.8.5.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">4.2</span></td>
</tr>
<tr id="S4.SS1.SSS1.1.1.1.10.9" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.SS1.SSS1.1.1.1.10.9.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.10.9.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">LS 960h (M) + AccD (M) + TTS-AccD (M) + TTS-LS 960h (M)</span></td>
<td id="S4.SS1.SSS1.1.1.1.10.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.10.9.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">30.4</span></td>
<td id="S4.SS1.SSS1.1.1.1.10.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.10.9.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">32.7</span></td>
<td id="S4.SS1.SSS1.1.1.1.10.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.10.9.4.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">2.1</span></td>
<td id="S4.SS1.SSS1.1.1.1.10.9.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.SS1.SSS1.1.1.1.10.9.5.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">4.1</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">We combine data from the L2-ARCTIC corpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and the British Isles corpus <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> as accented speech training data. These are corpora of read speech which were recorded in controlled environments. The L2-ARCTIC corpus is a speech corpus of non-native English which contains 26,867 utterances from 24 non-native English speakers with equally distributed number of speakers per accent. The total duration of the corpus is 27.1 hours, with an average of 67.7 minutes of speech per speaker. On average, each utterance is 3.6 seconds in duration. The utterances in L2-ARCTIC are spoken in 6 non-native accents: Arabic, Chinese, Hindi, Korean, Spanish, and Vietnamese.</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<p id="S4.SS1.SSS1.p2.1" class="ltx_p">The British Isles corpus includes speech utterances recorded by volunteers speaking with different accents of the British Isles, namely Ireland, Scotland, Wales, the Midlands, Northern, and Southern of England. The corpus consists of 17,877 utterances spoken by 120 speakers of which 49 are female and 71 are male. The total duration of the corpus is 31 hours. When being decoded in the unsupervised scenario, the WERs of the pseudo-labels obtained on the L2-ARCTIC and British Isles training data are 10.7% and 10.2%, respectively.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS2.4.1.1" class="ltx_text">IV-A</span>2 </span>Evaluation data</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">We use the development and test sets from the Edinburgh international accents of English corpus (EdAcc) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, which consist of spontaneous conversational speech, as evaluation data. The corpus includes a wide range of first- and second-language varieties of English in the form of dyadic video call conversations between friends. The conversations range in durations from 20 to 60 minutes. These conversations are segmented into shorter utterances based on manual annotations and are then separated into development and test sets which consist of 9079 and 8494 utterances, respectively. In total, the development set contains 14 hours and the test set contains 15 hours of speech. There are more than 40 self-reported English accents from 51 different first languages. The statistics and analyses show that EdAcc is linguistically diverse and challenging for current English ASR systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. With more than 40 English accents, the EdAcc corpus covers English accents from four continents, including Africa, America, Asia, and Europe. The conversations were manually transcribed by professional transcribers to obtain manual transcriptions which are used in the evaluation.</p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS3.4.1.1" class="ltx_text">IV-A</span>3 </span>Synthetic speech data</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">Synthetic speech data are generated using the TTS systems and English text prompts. The text prompts used in the TTS inference are selected from the manual transcriptions of the training data in three speech corpora: LJSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, TED-LIUM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, and VCTK <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. The objective of selecting text prompts from independent TTS and ASR corpora is to ensure that these prompts are not related to the evaluation data and are phonetically balanced, since they were designed for TTS and ASR applications. In total, there are 120K text prompts resulting in 250 hours of synthetic speech data which are spoken by the speakers presented in the training data of the TTS systems.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Results &amp; Discussion</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Experimental results, in terms of WERs, are shown in Table <a href="#S4.T1" title="TABLE I ‣ IV-A1 Accented speech training data (AccD) ‣ IV-A Data ‣ IV Experiments ‣ Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. In Table <a href="#S4.T1" title="TABLE I ‣ IV-A1 Accented speech training data (AccD) ‣ IV-A Data ‣ IV Experiments ‣ Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, the WERs computed on the EdAcc development &amp; test sets and the Librispeech (LS) test-clean &amp; test-other sets are shown. The ASR models in Table <a href="#S4.T1" title="TABLE I ‣ IV-A1 Accented speech training data (AccD) ‣ IV-A Data ‣ IV Experiments ‣ Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> are fine-tuned from one Wav2vec2.0 pre-trained model, which was pre-trained on the unsupervised training data of Libri-Light, Common Voice, Switchboard, and Fisher, using different fine-tuning data. The abbreviations used in Table <a href="#S4.T1" title="TABLE I ‣ IV-A1 Accented speech training data (AccD) ‣ IV-A Data ‣ IV Experiments ‣ Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> have the meaning as follows:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">LS 960h (M): 960 hours of training speech from Librispeech, manual (M) transcriptions are used as labels.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">AccD (P), AccD (M): 58 hours of accented speech training data, using either pseudo-labels (P) or manual (M) transcriptions as labels.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">TTS-LS 960h (M): 250 hours of synthetic non-accented speech data generated by TTS system trained on LS 960h (M) data. The speakers are from the LS 960h data.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">TTS-AccD (P), TTS-AccD (M): 250 hours of synthetic accented speech data generated by TTS systems trained on either AccD (P) or AccD (M) data, with speakers from the AccD data.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">We build a baseline model by fine-tuning the Wav2vec2.0 pre-trained model with the LS 960h (M) data. The Wav2vec2.0 pre-trained model and the fine-tuning data that we use are the same as those used to train the baseline model in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. We will compare the results with our baseline model which has lower WERs, compared to those of the Wav2vec2.0 model reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, on the development and test data of both EdAcc and Librispeech (see Table <a href="#S4.T1" title="TABLE I ‣ IV-A1 Accented speech training data (AccD) ‣ IV-A Data ‣ IV Experiments ‣ Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>). Combining the unsupervised accented speech training data AccD (P) with the non-accented speech data LS 960h (M) to fine-tune the pre-trained model yields 1.2% and 1.4% relative WER reductions on EdAcc dev and test sets, respectively, while the respective relative WER reductions on these sets are 5.2% and 4.8% when the supervised accented speech training data AccD (M) are used.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">When the synthetic non-accented speech data TTS-LS 960h (M) which are generated by the supervised TTS system, trained on the non-accented speech data LS 960h (M) with manual transcriptions, are included in the fine-tuning, 4.3% and 3.7% relative WER reductions are obtained on the EdAcc dev and test sets, respectively. Since the synthetic non-accented speech data TTS-LS 960h (M) are spoken by the same speakers in the LS 960h (M) data, the relative WER reductions are made mainly thanks to more acoustic realizations, based on the independent text prompts, are added to the fine-tuning data from the synthetic non-accented speech data. Larger gains are obtained when the synthetic accented speech data TTS-AccD (P) and TTS-AccD (M) are used, even though the amount of data and the number of speakers in the AccD data used to train TTS systems are much smaller compared to those of the non-accented speech data LS 960h (M): 58 hours compared to 960 hours, and 144 speakers compared to 2200 speakers. More specifically, the TTS-AccD (P) data generated by unsupervised TTS help to achieve 5.5% and 5.4% relative WER reductions on the EdAcc dev and test sets, respectively, while the TTS-AccD (M) data generated by supervised TTS help to achieve 6.1% and 6.0% relative WER reductions on the EdAcc dev and test sets, respectively.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">When the accented speech training data AccD and all the synthetic speech data are combined with the non-accented speech data to fine-tune the pre-trained model, further gains are obtained. In the unsupervised scenarios, 6.1% and 5.4% relative WER reductions are obtained on the EdAcc dev and test sets, respectively, while the respective relative WER reductions obtained on these sets in the supervised scenario are 7.3% and 6.8%, respectively. Actually, using natural accented speech training data and synthetic accented speech data improves the performance on EdAcc dev and test sets but does not harm or improve the ASR performance on Librispeech test sets. This confirms that considering Librispeech training data as non-accented speech data in our experiments is relevant.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Unsupervised TTS, trained on unsupervised accented speech training data, was used to generate synthetic accented speech data for data augmentation in accented speech recognition. Experiments showed that the Wav2vec2.0 models which used the synthetic accented speech data yielded up to 6.1% relative WER reductions compared to a large Wav2vec2.0 baseline. These gains are close to those obtained in the supervised scenario. The results demonstrate that unsupervised accented speech data, even when available in limited quantities and are spoken in different styles by speakers who differ from those in the evaluation data, can be effectively used to train TTS systems for data augmentation. This approach improves accented speech recognition, particularly when the speakers in the unsupervised accented speech data and those in the evaluation data have some overlaps on speakers’ first languages.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
D. Prabhu, P. Jyothi, S. Ganapathy, and V. Unni,

</span>
<span class="ltx_bibblock">“Accented speech recognition with accent-specific codebooks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proc. 2023 Conference on Empirical Methods in Natual Language
Processing (EMNLP)</span>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Y.-C. Chen, Z. Yang, C.-F. Yeh, M. Jain, and M. Seltzer,

</span>
<span class="ltx_bibblock">“Aipnet: generative adversarial pre-training of accent-invariant
networks for end-to-end speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proc. 2020 IEEE ICASSP</span>, pp. 6979–6983.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
N. Das, S. Bodapati, M. Sunkara, S. Srinivasan, and D. Horng Chau,

</span>
<span class="ltx_bibblock">“Best of both worlds: robust accented speech recognition with
adversarial transfer learning,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH 2021</span>, pp. 1314–1318.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
H. Hu et al.,

</span>
<span class="ltx_bibblock">“REDAT: accent-invariant representation for end-to-end ASR by
domain adversarial training with relabeling,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Proc. 2021 IEEE ICASSP</span>, pp. 6408–6412.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
K. Deng, S. Cao, and L. Ma,

</span>
<span class="ltx_bibblock">“Improving accent identification and accented speech recognition
under a framework of self-supervised learning,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH 2021</span>, pp. 1504–1508.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
M. Lucas and Y. Estève,

</span>
<span class="ltx_bibblock">“Improving accented speech recognition with multi-domain training,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proc. 2023 IEEE ICASSP</span>, pp. 1–5.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Jain, M. Upreti, and P. Jyothi,

</span>
<span class="ltx_bibblock">“Improved accented speech recognition using accent embeddings and
multi-task learning,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH 2018</span>, pp. 2454–2458.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
X. Wang, Y. Long, Y. Li, and H. Wei,

</span>
<span class="ltx_bibblock">“Multi-pass training and cross-information fusion for low-resource
end-to-end accented speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH 2023</span>, pp. 2923–2927.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, G. Wang, and P. Moreno,

</span>
<span class="ltx_bibblock">“Injecting text in self-supervised speech pretraining,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proc. 2021 IEEE ASRU Workshop</span>, pp. 251–258.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
X. Zheng, Y. Liu, D. Gunceler, and D. Willett,

</span>
<span class="ltx_bibblock">“Using synthetic audio to improve the recognition of
out-of-vocabulary words in end-to-end ASR systems,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proc. 2021 IEEE ICASSP</span>, pp. 5674–5678.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. Rosenberg, Y. Zhang, B. Ramabhadran, Y. Jia, P. Moreno, Y. Wu, and Z. Wu,

</span>
<span class="ltx_bibblock">“Speech recognition with augmented synthesized speech,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proc. 2019 IEEE ASRU</span>, pp. 996–1002.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Ueno, M. Mimura, S. Sakai, and T. Kawahara,

</span>
<span class="ltx_bibblock">“Data augmentation for ASR using TTS via discrete
representation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proc. 2021 IEEE Automatic Speech Recognition and
Understanding Workshop</span>, pp. 68–75.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
G. Zhong, H. Song, R. Wang, L. Sun, D. Liu, J. Pan, X. Fang, J. Du, J. Zhang,
and L. Dai,

</span>
<span class="ltx_bibblock">“External text based data augmentation for low-resource speech
recognition in the constrained condition of OpenASR21 challenge,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH 2022</span>, pp. 4860–4864.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
E. Casanova, C. Shulby, A. Korolev, A.C. Junior, A.d.S. Soares, S. Aluísio,
and M.A. Ponti,

</span>
<span class="ltx_bibblock">“ASR data augmentation in low-resource settings using
cross-lingual multi-speaker TTS and cross-lingual voice conversion,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH 2023</span>, pp. 1244–1248.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Fazel, W. Yang, Y. Liu, R. Barra-Chicote, Y. Meng, R. Maas, and J. Droppo,

</span>
<span class="ltx_bibblock">“SynthASR: unlocking synthetic data for speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH 2021</span>, pp. 896–900.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J. Ni, L. Wang, H. Gao, K. Qian, Y. Zhang, S. Chang, and M. Hasegawa-Johnson,

</span>
<span class="ltx_bibblock">“Unsupervised text-to-speech synthesis by unsupervised automatic
speech recognition,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH 2022</span>, pp. 461–465.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
G. Karakasidis, N. Robinson, Y. Getman, A. Ogayo, R. Al-Ghezi, A. Ayasi,
S. Watanabe, Mortensen D. R., and M. Kurimo,

</span>
<span class="ltx_bibblock">“Multilingual TTS accent impressions for accented ASR,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proc. 2023 International Conference on Text, Speech, and
Dialogue (TSD)</span>, pp. 317–327.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
G. Zhao, S. Sonsaat, A. Silpachai, I. Lucic, E. Chukharev-Hudilainen, J. Levis,
and R. Gutierrez-Osuna,

</span>
<span class="ltx_bibblock">“L2-ARCTIC: a non-native English speech corpus,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proc. INTERSPEECH 2018</span>, pp. 2783–2787.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
I. Demirsahin, O. Kjartansson, A. Gutkin, and C. Rivera,

</span>
<span class="ltx_bibblock">“Open-source multi-speaker corpora of the English accents in the
British Isles,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proc. 2020 Conference on Language Resources and Evaluation
(LREC)</span>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
R. Sanabria, N. Bogoychev, N. Markl, A. Carmantini, O. Klejch, and P. Bell,

</span>
<span class="ltx_bibblock">“The Edinburgh international accents of English corpus: towards
the democratization of English ASR,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proc. 2023 IEEE ICASSP</span>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. Baevski, Y. Zhou, A. Mohamed, and M. Auli,

</span>
<span class="ltx_bibblock">“Wav2vec 2.0: a framework for self-supervised learning of speech
representations,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proc. 2020 Advances in Neural Information Processing
Systems</span>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
V. Panayotov, G. Chen, D. Povey, and S. Khudanpur,

</span>
<span class="ltx_bibblock">“Librispeech: An ASR corpus based on public domain audio books,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proc. 2015 IEEE ICASSP</span>, pp. 5206–5210.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J. Kim, J. Kong, and J. Son,

</span>
<span class="ltx_bibblock">“Conditional variational autoencoder with adversarial learning for
end-to-end text-to-speech,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">2021 International Conference on Machine Learning</span>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
D. P. Kingma and M. Welling,

</span>
<span class="ltx_bibblock">“Auto-encoding variational bayes,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">2014 International Conference on Learning Representations
(ICLR)</span>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, and Y. Bengio,

</span>
<span class="ltx_bibblock">“Generative adversarial nets,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proc. 2014 Advances in Neural Information Processing Systems
(NIPS)</span>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
J. Kong, J. Kim, and J. Bae,

</span>
<span class="ltx_bibblock">“Hifi-gan: Generative adversarial networks for efficient and high
fidelity speech synthesis,”

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proc. 2020 Advances in Neural Information Processing Systems</span>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
D. Rezende and S. Mohamed,

</span>
<span class="ltx_bibblock">“Variational inference with normalizing flows,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Proc. 2015 International Conference on Machine Learning</span>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
K. Ito and L. Johnson,

</span>
<span class="ltx_bibblock">“The LJ Speech dataset,”
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://keithito.com/LJ-Speech-Dataset</span>, 2017.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A. Rousseau, P. Deléglise, and Y. Estève,

</span>
<span class="ltx_bibblock">“TED-LIUM: an automatic speech recognition dedicated corpus,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Proc. 2012 Conference on Language Resources and Evaluation
(LREC)</span>, pp. 125–129.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J. Yamagishi, C. Veaux, and K. MacDonald,

</span>
<span class="ltx_bibblock">“CSTR VCTK corpus: English multi-speaker corpus for CSTR voice
cloning toolkit (version 0.92),” <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://doi.org/10.7488/ds/2645</span>,
2017.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.04046" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.04047" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.04047">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.04047" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.04048" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 17:33:10 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
