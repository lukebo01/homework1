<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.13483] A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering</title><meta property="og:description" content="Speech-based open-domain question answering (QA over a large corpus of text passages with spoken questions) has emerged as an important task due to the increasing number of users interacting with QA systems via speech ‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.13483">

<!--Generated on Sun Oct  6 01:04:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Multimodal¬†Dense¬†Retrieval Approach for Speech-Based Open-Domain Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Georgios Sidiropoulos 
<br class="ltx_break">University of Amsterdam 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">g.sidiropoulos@uva.nl</span> 
<br class="ltx_break">&amp;Evangelos Kanoulas
<br class="ltx_break">University of Amsterdam 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">e.kanoulas@uva.nl</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">Speech-based open-domain question answering (QA over a large corpus of text passages with spoken questions) has emerged as an important task due to the increasing number of users interacting with QA systems via speech interfaces. Passage retrieval is a key task in speech-based open-domain QA. So far, previous works adopted pipelines consisting of an automatic speech recognition (ASR) model that transcribes the spoken question before feeding it to a dense text retriever. Such pipelines have several limitations. The need for an ASR model limits the applicability to low-resource languages and specialized domains with no annotated speech data. Furthermore, the ASR model propagates its errors to the retriever. In this work, we try to alleviate these limitations by proposing an ASR-free, end-to-end trained multimodal dense retriever that can work directly on spoken questions. Our experimental results showed that, on shorter questions, our retriever is a promising alternative to the <span id="id3.id1.1" class="ltx_text ltx_font_italic">ASR and Retriever</span> pipeline, achieving better retrieval performance in cases where ASR would have mistranscribed important words in the question or have produced a transcription with a high word error rate.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Voice assistants are convenient, easy to use, and can support users with visual and motor impairments that cannot use conventional text entry devices. Voice assistants such as Amazon Alexa, Apple Siri, and Google Assistant are used daily by everyday users. As a result, nowadays, users interact with a wide range of commercial Question Answering (QA) systems via such speech interfaces. In other words, millions of users are voicing their questions on virtual voice assistants instead of typing them. That has led to the emergence of speech-based open-domain QA, a QA task on open-domain textual datasets where questions are in speech form.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.2" class="ltx_p">Most of the research in speech-based open-domain QA focuses on reading comprehension as a component of the commonly adopted <span id="S1.p2.2.1" class="ltx_text ltx_font_italic">retriever and reader</span> framework <cite class="ltx_cite ltx_citemacro_cite">Faisal et¬†al. (<a href="#bib.bib4" title="" class="ltx_ref">2021</a>); Ravichander et¬†al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>. However, for effective answer extraction, we need an effective retriever that reduces the search space from millions of passages to the <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="top" display="inline"><semantics id="S1.p2.1.m1.1a"><mrow id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml"><mi id="S1.p2.1.m1.1.1.2" xref="S1.p2.1.m1.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S1.p2.1.m1.1.1.1" xref="S1.p2.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S1.p2.1.m1.1.1.3" xref="S1.p2.1.m1.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S1.p2.1.m1.1.1.1a" xref="S1.p2.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S1.p2.1.m1.1.1.4" xref="S1.p2.1.m1.1.1.4.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><apply id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1"><times id="S1.p2.1.m1.1.1.1.cmml" xref="S1.p2.1.m1.1.1.1"></times><ci id="S1.p2.1.m1.1.1.2.cmml" xref="S1.p2.1.m1.1.1.2">ùë°</ci><ci id="S1.p2.1.m1.1.1.3.cmml" xref="S1.p2.1.m1.1.1.3">ùëú</ci><ci id="S1.p2.1.m1.1.1.4.cmml" xref="S1.p2.1.m1.1.1.4">ùëù</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">top</annotation></semantics></math>-<math id="S1.p2.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S1.p2.2.m2.1a"><mi id="S1.p2.2.m2.1.1" xref="S1.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S1.p2.2.m2.1b"><ci id="S1.p2.2.m2.1.1.cmml" xref="S1.p2.2.m2.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.2.m2.1c">k</annotation></semantics></math> most relevant; the performance of passage retrieval bounds that of reading comprehension. Hence, studying passage retrieval in speech-based open-domain QA is crucial. Despite the community attention that passage retrieval for traditional open-domain QA receives <cite class="ltx_cite ltx_citemacro_cite">Zhu et¬†al. (<a href="#bib.bib22" title="" class="ltx_ref">2021</a>)</cite>, there are surprisingly limited efforts in studying passage retrieval for speech-based open-domain QA.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Recently, <cite class="ltx_cite ltx_citemacro_citet">Sidiropoulos et¬†al. (<a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite> studied passage retrieval for speech-based open-domain QA.
In particular, following a pipeline approach consisting of an automatic speech recognition (ASR) model for transcribing the spoken question and a dense retriever
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Dense text retrievers use transformers to encode the question and passage into a single vector each and further use the similarity of these vectors to indicate the relevance between the question and passage.</span></span></span>
for text retrieval (<span id="S1.p3.1.1" class="ltx_text ltx_font_italic">ASR-Retriever</span>), they investigated the effectiveness of dense retrievers on questions with ASR mistranscriptions. In their work, they build on the assumption that clean questions are available at training, and the ASR mistranscriptions appear only at inference. That said, such an approach is not directly applicable in a real-world scenario where only the spoken questions are available during training. In a real-world scenario, following an <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipeline would require either (i) using the ASR model to produce the training questions or (ii) training on a different dataset where questions in text form are available.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><span id="S1.p4.1.1" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipeline has several limitations. First and foremost, it does not support training in an end-to-end manner. As a direct consequence, ASR propagates its errors to the downstream retriever; the higher the word error rate from ASR, the higher the negative impact on the performance of the retriever. ASR models suffer from mistranscribing long-tail named entities and domain-specific words <cite class="ltx_cite ltx_citemacro_cite">Hwang et¬†al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>); Mao et¬†al. (<a href="#bib.bib11" title="" class="ltx_ref">2021</a>); Wang et¬†al. (<a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite>. The former is of great importance when working on questions because corrupting the main entity of a question can dramatically affect retrieval <cite class="ltx_cite ltx_citemacro_cite">Sidiropoulos and Kanoulas (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite>. For example, when the question ‚Äúwhat is the meaning of the name sinead?‚Äù is transcribed as ‚Äúwhat is the meaning of the name chinade?‚Äù, then the retrieval will be centered around the wrong entity ‚Äúchinade‚Äù. Additionally, training an ASR model requires obtaining a large amount of annotated speech which is not always available (e.g., low-resource languages). Finally, during query time, feeding the spoken question to the transformer-based ASR model to obtain its transcription before performing retrieval results in high query latency.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In contrast, we propose a multimodal dual-encoder dense retriever that does not require an ASR model and can be trained end-to-end. Our method adapts the dual-encoder architecture used for dense text retrieval (where both question and passage are in text form) <cite class="ltx_cite ltx_citemacro_cite">Karpukhin et¬†al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> by replacing the backbone language model used to encode the questions with a self-supervised speech model (Figure <a href="#S2.F1" title="Figure 1 ‚Ä£ 2.2 Multimodal Dense Retriever ‚Ä£ 2 Methodology ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). To the best of our knowledge, this is the first multimodal dual-encoder approach for speech-based open-domain QA. Furthermore, in this work, we benchmark passage retrieval for the speech-based open-domain QA using <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipeline approaches, where the spoken questions are transcribed with an ASR model and then used for training the dense text retrievers.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We aim to answer the following research questions:
<span id="S1.p6.1.1" class="ltx_text ltx_font_bold">RQ1</span> How does our ASR-free, end-to-end trained multimodal dense retriever performs compared to its <span id="S1.p6.1.2" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipeline counterparts?</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p"><span id="S1.p7.1.1" class="ltx_text ltx_font_bold">RQ2</span> Can our ASR-free retriever alleviate the limitations of <span id="S1.p7.1.2" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipelines when it comes to ASR mistranscribing important words in a question?
<span id="S1.p7.1.3" class="ltx_text ltx_font_bold">RQ3</span> How does our ASR-free retriever perform against <span id="S1.p7.1.4" class="ltx_text ltx_font_italic">ASR-Retriever</span> when the latter has to deal with previously unseen ASR mistranscription?
<span id="S1.p7.1.5" class="ltx_text ltx_font_bold">RQ4</span> What is the most effective training scheme for learning a multimodal dense retriever?</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">We make the following contributions:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We benchmark the speech-based open-domain QA task with stronger baselines. In detail, we train the retriever in the <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipeline on spoken question transcriptions and not clean textual questions as in <cite class="ltx_cite ltx_citemacro_citet">Sidiropoulos et¬†al. (<a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose an ASR-free, end-to-end trained multimodal dense retriever that can work on speech, and we demonstrate a setup for effective training.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Experimental results showed that our retriever is a promising alternative to <span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipelines and competitive on shorter questions.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We demonstrate through thorough analysis the robustness of our approach compared to the <span id="S1.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipeline, where its retrieval performance is strongly related to the ASR error; consequently, it varies significantly under different situations. We unveil that pipelines witness a dramatic drop in their retrieval performance (i) as the word error rate of the ASR model increases, (ii) when important words in the spoken question are mistranscribed, and (iii) on mistranscriptions that did not encounter during training. Our ASR-free retriever can alleviate these problems and yield better results.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Problem Definition</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.8" class="ltx_p">Let <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="p\in\mathcal{C}" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mrow id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S2.SS1.p1.1.m1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.cmml">‚àà</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml">ùíû</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><in id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1"></in><ci id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">ùëù</ci><ci id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3">ùíû</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">p\in\mathcal{C}</annotation></semantics></math> denote a passage in text form within a passage collection <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">ùíû</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">ùíû</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">\mathcal{C}</annotation></semantics></math> in the scale of millions and <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">ùëû</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">q</annotation></semantics></math> a question in speech form. In passage retrieval for speech-based open-domain QA, given <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mi id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">ùëû</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">q</annotation></semantics></math> and <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">ùíû</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">ùíû</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">\mathcal{C}</annotation></semantics></math>, the task is to retrieve a set of relevant passages <math id="S2.SS1.p1.6.m6.4" class="ltx_Math" alttext="\mathcal{P}=\{p_{1},p_{2},\dots,p_{n}\}" display="inline"><semantics id="S2.SS1.p1.6.m6.4a"><mrow id="S2.SS1.p1.6.m6.4.4" xref="S2.SS1.p1.6.m6.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.6.m6.4.4.5" xref="S2.SS1.p1.6.m6.4.4.5.cmml">ùí´</mi><mo id="S2.SS1.p1.6.m6.4.4.4" xref="S2.SS1.p1.6.m6.4.4.4.cmml">=</mo><mrow id="S2.SS1.p1.6.m6.4.4.3.3" xref="S2.SS1.p1.6.m6.4.4.3.4.cmml"><mo stretchy="false" id="S2.SS1.p1.6.m6.4.4.3.3.4" xref="S2.SS1.p1.6.m6.4.4.3.4.cmml">{</mo><msub id="S2.SS1.p1.6.m6.2.2.1.1.1" xref="S2.SS1.p1.6.m6.2.2.1.1.1.cmml"><mi id="S2.SS1.p1.6.m6.2.2.1.1.1.2" xref="S2.SS1.p1.6.m6.2.2.1.1.1.2.cmml">p</mi><mn id="S2.SS1.p1.6.m6.2.2.1.1.1.3" xref="S2.SS1.p1.6.m6.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.p1.6.m6.4.4.3.3.5" xref="S2.SS1.p1.6.m6.4.4.3.4.cmml">,</mo><msub id="S2.SS1.p1.6.m6.3.3.2.2.2" xref="S2.SS1.p1.6.m6.3.3.2.2.2.cmml"><mi id="S2.SS1.p1.6.m6.3.3.2.2.2.2" xref="S2.SS1.p1.6.m6.3.3.2.2.2.2.cmml">p</mi><mn id="S2.SS1.p1.6.m6.3.3.2.2.2.3" xref="S2.SS1.p1.6.m6.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.p1.6.m6.4.4.3.3.6" xref="S2.SS1.p1.6.m6.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml">‚Ä¶</mi><mo id="S2.SS1.p1.6.m6.4.4.3.3.7" xref="S2.SS1.p1.6.m6.4.4.3.4.cmml">,</mo><msub id="S2.SS1.p1.6.m6.4.4.3.3.3" xref="S2.SS1.p1.6.m6.4.4.3.3.3.cmml"><mi id="S2.SS1.p1.6.m6.4.4.3.3.3.2" xref="S2.SS1.p1.6.m6.4.4.3.3.3.2.cmml">p</mi><mi id="S2.SS1.p1.6.m6.4.4.3.3.3.3" xref="S2.SS1.p1.6.m6.4.4.3.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="S2.SS1.p1.6.m6.4.4.3.3.8" xref="S2.SS1.p1.6.m6.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.4b"><apply id="S2.SS1.p1.6.m6.4.4.cmml" xref="S2.SS1.p1.6.m6.4.4"><eq id="S2.SS1.p1.6.m6.4.4.4.cmml" xref="S2.SS1.p1.6.m6.4.4.4"></eq><ci id="S2.SS1.p1.6.m6.4.4.5.cmml" xref="S2.SS1.p1.6.m6.4.4.5">ùí´</ci><set id="S2.SS1.p1.6.m6.4.4.3.4.cmml" xref="S2.SS1.p1.6.m6.4.4.3.3"><apply id="S2.SS1.p1.6.m6.2.2.1.1.1.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.2.2.1.1.1.1.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.6.m6.2.2.1.1.1.2.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1.2">ùëù</ci><cn type="integer" id="S2.SS1.p1.6.m6.2.2.1.1.1.3.cmml" xref="S2.SS1.p1.6.m6.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS1.p1.6.m6.3.3.2.2.2.cmml" xref="S2.SS1.p1.6.m6.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.3.3.2.2.2.1.cmml" xref="S2.SS1.p1.6.m6.3.3.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.6.m6.3.3.2.2.2.2.cmml" xref="S2.SS1.p1.6.m6.3.3.2.2.2.2">ùëù</ci><cn type="integer" id="S2.SS1.p1.6.m6.3.3.2.2.2.3.cmml" xref="S2.SS1.p1.6.m6.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">‚Ä¶</ci><apply id="S2.SS1.p1.6.m6.4.4.3.3.3.cmml" xref="S2.SS1.p1.6.m6.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.4.4.3.3.3.1.cmml" xref="S2.SS1.p1.6.m6.4.4.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.6.m6.4.4.3.3.3.2.cmml" xref="S2.SS1.p1.6.m6.4.4.3.3.3.2">ùëù</ci><ci id="S2.SS1.p1.6.m6.4.4.3.3.3.3.cmml" xref="S2.SS1.p1.6.m6.4.4.3.3.3.3">ùëõ</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.4c">\mathcal{P}=\{p_{1},p_{2},\dots,p_{n}\}</annotation></semantics></math>, where <math id="S2.SS1.p1.7.m7.1" class="ltx_Math" alttext="p_{i}\in\mathcal{C}" display="inline"><semantics id="S2.SS1.p1.7.m7.1a"><mrow id="S2.SS1.p1.7.m7.1.1" xref="S2.SS1.p1.7.m7.1.1.cmml"><msub id="S2.SS1.p1.7.m7.1.1.2" xref="S2.SS1.p1.7.m7.1.1.2.cmml"><mi id="S2.SS1.p1.7.m7.1.1.2.2" xref="S2.SS1.p1.7.m7.1.1.2.2.cmml">p</mi><mi id="S2.SS1.p1.7.m7.1.1.2.3" xref="S2.SS1.p1.7.m7.1.1.2.3.cmml">i</mi></msub><mo id="S2.SS1.p1.7.m7.1.1.1" xref="S2.SS1.p1.7.m7.1.1.1.cmml">‚àà</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.7.m7.1.1.3" xref="S2.SS1.p1.7.m7.1.1.3.cmml">ùíû</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><apply id="S2.SS1.p1.7.m7.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1"><in id="S2.SS1.p1.7.m7.1.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1.1"></in><apply id="S2.SS1.p1.7.m7.1.1.2.cmml" xref="S2.SS1.p1.7.m7.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m7.1.1.2.1.cmml" xref="S2.SS1.p1.7.m7.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.7.m7.1.1.2.2.cmml" xref="S2.SS1.p1.7.m7.1.1.2.2">ùëù</ci><ci id="S2.SS1.p1.7.m7.1.1.2.3.cmml" xref="S2.SS1.p1.7.m7.1.1.2.3">ùëñ</ci></apply><ci id="S2.SS1.p1.7.m7.1.1.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3">ùíû</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">p_{i}\in\mathcal{C}</annotation></semantics></math> and can answer <math id="S2.SS1.p1.8.m8.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S2.SS1.p1.8.m8.1a"><mi id="S2.SS1.p1.8.m8.1.1" xref="S2.SS1.p1.8.m8.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><ci id="S2.SS1.p1.8.m8.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1">ùëû</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">q</annotation></semantics></math>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Multimodal Dense Retriever</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In passage retrieval for speech-based open-domain QA, the <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipelines that were used so far suffer from propagating ASR errors to the retriever. Such approaches are not trained end-to-end; thus, the quality of the ASR transcriptions bounds the retrieval performance. Furthermore, the requirement for an ASR model limits the applicability of such pipelines to scenarios where annotated speech is available for training the ASR model. To alleviate the above-mentioned problems, we propose an ASR-free multimodal dense retriever that can support speech, and can be trained end-to-end.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2409.13483/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="141" height="132" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Overview of our multimodal dense retriever.</figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.4" class="ltx_p">Specifically, we modify the dual-encoder architecture for dense text retrieval in traditional open-domain QA <cite class="ltx_cite ltx_citemacro_cite">Karpukhin et¬†al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> to account for spoken questions. We replace the BERT-based question encoder with HuBERT <cite class="ltx_cite ltx_citemacro_cite">Hsu et¬†al. (<a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite> and leave the BERT-based passage encoder as is. HuBERT is a self-supervised model for speech representation learning, which leverages a BERT-like masked prediction loss. It utilizes offline clustering to provide target labels for masked language model pertaining. Figure <a href="#S2.F1" title="Figure 1 ‚Ä£ 2.2 Multimodal Dense Retriever ‚Ä£ 2 Methodology ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the architecture of our ASR-free, multimodal dense retriever. Suppose a pair of question q, in speech form, and a passage p, in text form. The speech and language encoders produce the output representations:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.7" class="ltx_Math" alttext="\leavevmode\resizebox{433.62pt}{}{$HuBERT(q)=(\bm{q}_{1},...,\bm{q}_{n}),\mbox{\hskip 5.0pt}BERT(p)=(\bm{p}_{1},...,\bm{p}_{m})$}." display="block"><semantics id="S2.E1.m1.7a"><mrow id="S2.E1.m1.7.8.2"><mpadded depth="4.8pt" height="14.3pt" width="433.6pt" id="S2.E1.m1.7.7.7" xref="S2.E1.m1.7.7.8.cmml"><mrow id="S2.E1.m1.6.6.6.1" xref="S2.E1.m1.6.6.6.1.cmml"><mrow id="S2.E1.m1.6.6.6.1.4" xref="S2.E1.m1.6.6.6.1.4.cmml"><mi id="S2.E1.m1.6.6.6.1.4.2" xref="S2.E1.m1.6.6.6.1.4.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.6.6.6.1.4.1" xref="S2.E1.m1.6.6.6.1.4.1.cmml">‚Äã</mo><mi id="S2.E1.m1.6.6.6.1.4.3" xref="S2.E1.m1.6.6.6.1.4.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.6.6.6.1.4.1a" xref="S2.E1.m1.6.6.6.1.4.1.cmml">‚Äã</mo><mi id="S2.E1.m1.6.6.6.1.4.4" xref="S2.E1.m1.6.6.6.1.4.4.cmml">B</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.6.6.6.1.4.1b" xref="S2.E1.m1.6.6.6.1.4.1.cmml">‚Äã</mo><mi id="S2.E1.m1.6.6.6.1.4.5" xref="S2.E1.m1.6.6.6.1.4.5.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.6.6.6.1.4.1c" xref="S2.E1.m1.6.6.6.1.4.1.cmml">‚Äã</mo><mi id="S2.E1.m1.6.6.6.1.4.6" xref="S2.E1.m1.6.6.6.1.4.6.cmml">R</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.6.6.6.1.4.1d" xref="S2.E1.m1.6.6.6.1.4.1.cmml">‚Äã</mo><mi id="S2.E1.m1.6.6.6.1.4.7" xref="S2.E1.m1.6.6.6.1.4.7.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.6.6.6.1.4.1e" xref="S2.E1.m1.6.6.6.1.4.1.cmml">‚Äã</mo><mrow id="S2.E1.m1.6.6.6.1.4.8.2" xref="S2.E1.m1.6.6.6.1.4.cmml"><mo stretchy="false" id="S2.E1.m1.6.6.6.1.4.8.2.1" xref="S2.E1.m1.6.6.6.1.4.cmml">(</mo><mi id="S2.E1.m1.2.2.2" xref="S2.E1.m1.2.2.2.cmml">q</mi><mo stretchy="false" id="S2.E1.m1.6.6.6.1.4.8.2.2" xref="S2.E1.m1.6.6.6.1.4.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.6.6.6.1.3" xref="S2.E1.m1.6.6.6.1.3.cmml">=</mo><mrow id="S2.E1.m1.6.6.6.1.2.2" xref="S2.E1.m1.6.6.6.1.2.3.cmml"><mo stretchy="false" id="S2.E1.m1.6.6.6.1.2.2.3" xref="S2.E1.m1.6.6.6.1.2.3.cmml">(</mo><msub id="S2.E1.m1.6.6.6.1.1.1.1" xref="S2.E1.m1.6.6.6.1.1.1.1.cmml"><mi id="S2.E1.m1.6.6.6.1.1.1.1.2" xref="S2.E1.m1.6.6.6.1.1.1.1.2.cmml">ùíí</mi><mn id="S2.E1.m1.6.6.6.1.1.1.1.3" xref="S2.E1.m1.6.6.6.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.E1.m1.6.6.6.1.2.2.4" xref="S2.E1.m1.6.6.6.1.2.3.cmml">,</mo><mi mathvariant="normal" id="S2.E1.m1.3.3.3" xref="S2.E1.m1.3.3.3.cmml">‚Ä¶</mi><mo id="S2.E1.m1.6.6.6.1.2.2.5" xref="S2.E1.m1.6.6.6.1.2.3.cmml">,</mo><msub id="S2.E1.m1.6.6.6.1.2.2.2" xref="S2.E1.m1.6.6.6.1.2.2.2.cmml"><mi id="S2.E1.m1.6.6.6.1.2.2.2.2" xref="S2.E1.m1.6.6.6.1.2.2.2.2.cmml">ùíí</mi><mi id="S2.E1.m1.6.6.6.1.2.2.2.3" xref="S2.E1.m1.6.6.6.1.2.2.2.3.cmml">n</mi></msub><mo stretchy="false" id="S2.E1.m1.6.6.6.1.2.2.6" xref="S2.E1.m1.6.6.6.1.2.3.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.7.7.7.3" xref="S2.E1.m1.7.7.8a.cmml">,</mo><mrow id="S2.E1.m1.7.7.7.2" xref="S2.E1.m1.7.7.7.2.cmml"><mrow id="S2.E1.m1.7.7.7.2.4" xref="S2.E1.m1.7.7.7.2.4.cmml"><mtext id="S2.E1.m1.7.7.7.2.4.2" xref="S2.E1.m1.7.7.7.2.4.2a.cmml">¬†</mtext><mo lspace="0em" rspace="0em" id="S2.E1.m1.7.7.7.2.4.1" xref="S2.E1.m1.7.7.7.2.4.1.cmml">‚Äã</mo><mi id="S2.E1.m1.7.7.7.2.4.3" xref="S2.E1.m1.7.7.7.2.4.3.cmml">B</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.7.7.7.2.4.1a" xref="S2.E1.m1.7.7.7.2.4.1.cmml">‚Äã</mo><mi id="S2.E1.m1.7.7.7.2.4.4" xref="S2.E1.m1.7.7.7.2.4.4.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.7.7.7.2.4.1b" xref="S2.E1.m1.7.7.7.2.4.1.cmml">‚Äã</mo><mi id="S2.E1.m1.7.7.7.2.4.5" xref="S2.E1.m1.7.7.7.2.4.5.cmml">R</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.7.7.7.2.4.1c" xref="S2.E1.m1.7.7.7.2.4.1.cmml">‚Äã</mo><mi id="S2.E1.m1.7.7.7.2.4.6" xref="S2.E1.m1.7.7.7.2.4.6.cmml">T</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.7.7.7.2.4.1d" xref="S2.E1.m1.7.7.7.2.4.1.cmml">‚Äã</mo><mrow id="S2.E1.m1.7.7.7.2.4.7.2" xref="S2.E1.m1.7.7.7.2.4.cmml"><mo stretchy="false" id="S2.E1.m1.7.7.7.2.4.7.2.1" xref="S2.E1.m1.7.7.7.2.4.cmml">(</mo><mi id="S2.E1.m1.4.4.4" xref="S2.E1.m1.4.4.4.cmml">p</mi><mo stretchy="false" id="S2.E1.m1.7.7.7.2.4.7.2.2" xref="S2.E1.m1.7.7.7.2.4.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.7.7.7.2.3" xref="S2.E1.m1.7.7.7.2.3.cmml">=</mo><mrow id="S2.E1.m1.7.7.7.2.2.2" xref="S2.E1.m1.7.7.7.2.2.3.cmml"><mo stretchy="false" id="S2.E1.m1.7.7.7.2.2.2.3" xref="S2.E1.m1.7.7.7.2.2.3.cmml">(</mo><msub id="S2.E1.m1.7.7.7.2.1.1.1" xref="S2.E1.m1.7.7.7.2.1.1.1.cmml"><mi id="S2.E1.m1.7.7.7.2.1.1.1.2" xref="S2.E1.m1.7.7.7.2.1.1.1.2.cmml">ùíë</mi><mn id="S2.E1.m1.7.7.7.2.1.1.1.3" xref="S2.E1.m1.7.7.7.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.E1.m1.7.7.7.2.2.2.4" xref="S2.E1.m1.7.7.7.2.2.3.cmml">,</mo><mi mathvariant="normal" id="S2.E1.m1.5.5.5" xref="S2.E1.m1.5.5.5.cmml">‚Ä¶</mi><mo id="S2.E1.m1.7.7.7.2.2.2.5" xref="S2.E1.m1.7.7.7.2.2.3.cmml">,</mo><msub id="S2.E1.m1.7.7.7.2.2.2.2" xref="S2.E1.m1.7.7.7.2.2.2.2.cmml"><mi id="S2.E1.m1.7.7.7.2.2.2.2.2" xref="S2.E1.m1.7.7.7.2.2.2.2.2.cmml">ùíë</mi><mi id="S2.E1.m1.7.7.7.2.2.2.2.3" xref="S2.E1.m1.7.7.7.2.2.2.2.3.cmml">m</mi></msub><mo stretchy="false" id="S2.E1.m1.7.7.7.2.2.2.6" xref="S2.E1.m1.7.7.7.2.2.3.cmml">)</mo></mrow></mrow></mpadded><mo lspace="0em" id="S2.E1.m1.7.8.2.1">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.7b"><apply id="S2.E1.m1.7.7.8.cmml" xref="S2.E1.m1.7.7.7"><csymbol cd="ambiguous" id="S2.E1.m1.7.7.8a.cmml" xref="S2.E1.m1.7.7.7.3">formulae-sequence</csymbol><apply id="S2.E1.m1.6.6.6.1.cmml" xref="S2.E1.m1.6.6.6.1"><eq id="S2.E1.m1.6.6.6.1.3.cmml" xref="S2.E1.m1.6.6.6.1.3"></eq><apply id="S2.E1.m1.6.6.6.1.4.cmml" xref="S2.E1.m1.6.6.6.1.4"><times id="S2.E1.m1.6.6.6.1.4.1.cmml" xref="S2.E1.m1.6.6.6.1.4.1"></times><ci id="S2.E1.m1.6.6.6.1.4.2.cmml" xref="S2.E1.m1.6.6.6.1.4.2">ùêª</ci><ci id="S2.E1.m1.6.6.6.1.4.3.cmml" xref="S2.E1.m1.6.6.6.1.4.3">ùë¢</ci><ci id="S2.E1.m1.6.6.6.1.4.4.cmml" xref="S2.E1.m1.6.6.6.1.4.4">ùêµ</ci><ci id="S2.E1.m1.6.6.6.1.4.5.cmml" xref="S2.E1.m1.6.6.6.1.4.5">ùê∏</ci><ci id="S2.E1.m1.6.6.6.1.4.6.cmml" xref="S2.E1.m1.6.6.6.1.4.6">ùëÖ</ci><ci id="S2.E1.m1.6.6.6.1.4.7.cmml" xref="S2.E1.m1.6.6.6.1.4.7">ùëá</ci><ci id="S2.E1.m1.2.2.2.cmml" xref="S2.E1.m1.2.2.2">ùëû</ci></apply><vector id="S2.E1.m1.6.6.6.1.2.3.cmml" xref="S2.E1.m1.6.6.6.1.2.2"><apply id="S2.E1.m1.6.6.6.1.1.1.1.cmml" xref="S2.E1.m1.6.6.6.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.6.6.6.1.1.1.1.1.cmml" xref="S2.E1.m1.6.6.6.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.6.6.6.1.1.1.1.2.cmml" xref="S2.E1.m1.6.6.6.1.1.1.1.2">ùíí</ci><cn type="integer" id="S2.E1.m1.6.6.6.1.1.1.1.3.cmml" xref="S2.E1.m1.6.6.6.1.1.1.1.3">1</cn></apply><ci id="S2.E1.m1.3.3.3.cmml" xref="S2.E1.m1.3.3.3">‚Ä¶</ci><apply id="S2.E1.m1.6.6.6.1.2.2.2.cmml" xref="S2.E1.m1.6.6.6.1.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.6.6.6.1.2.2.2.1.cmml" xref="S2.E1.m1.6.6.6.1.2.2.2">subscript</csymbol><ci id="S2.E1.m1.6.6.6.1.2.2.2.2.cmml" xref="S2.E1.m1.6.6.6.1.2.2.2.2">ùíí</ci><ci id="S2.E1.m1.6.6.6.1.2.2.2.3.cmml" xref="S2.E1.m1.6.6.6.1.2.2.2.3">ùëõ</ci></apply></vector></apply><apply id="S2.E1.m1.7.7.7.2.cmml" xref="S2.E1.m1.7.7.7.2"><eq id="S2.E1.m1.7.7.7.2.3.cmml" xref="S2.E1.m1.7.7.7.2.3"></eq><apply id="S2.E1.m1.7.7.7.2.4.cmml" xref="S2.E1.m1.7.7.7.2.4"><times id="S2.E1.m1.7.7.7.2.4.1.cmml" xref="S2.E1.m1.7.7.7.2.4.1"></times><ci id="S2.E1.m1.7.7.7.2.4.2a.cmml" xref="S2.E1.m1.7.7.7.2.4.2"><mtext id="S2.E1.m1.7.7.7.2.4.2.cmml" xref="S2.E1.m1.7.7.7.2.4.2">¬†</mtext></ci><ci id="S2.E1.m1.7.7.7.2.4.3.cmml" xref="S2.E1.m1.7.7.7.2.4.3">ùêµ</ci><ci id="S2.E1.m1.7.7.7.2.4.4.cmml" xref="S2.E1.m1.7.7.7.2.4.4">ùê∏</ci><ci id="S2.E1.m1.7.7.7.2.4.5.cmml" xref="S2.E1.m1.7.7.7.2.4.5">ùëÖ</ci><ci id="S2.E1.m1.7.7.7.2.4.6.cmml" xref="S2.E1.m1.7.7.7.2.4.6">ùëá</ci><ci id="S2.E1.m1.4.4.4.cmml" xref="S2.E1.m1.4.4.4">ùëù</ci></apply><vector id="S2.E1.m1.7.7.7.2.2.3.cmml" xref="S2.E1.m1.7.7.7.2.2.2"><apply id="S2.E1.m1.7.7.7.2.1.1.1.cmml" xref="S2.E1.m1.7.7.7.2.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.7.7.7.2.1.1.1.1.cmml" xref="S2.E1.m1.7.7.7.2.1.1.1">subscript</csymbol><ci id="S2.E1.m1.7.7.7.2.1.1.1.2.cmml" xref="S2.E1.m1.7.7.7.2.1.1.1.2">ùíë</ci><cn type="integer" id="S2.E1.m1.7.7.7.2.1.1.1.3.cmml" xref="S2.E1.m1.7.7.7.2.1.1.1.3">1</cn></apply><ci id="S2.E1.m1.5.5.5.cmml" xref="S2.E1.m1.5.5.5">‚Ä¶</ci><apply id="S2.E1.m1.7.7.7.2.2.2.2.cmml" xref="S2.E1.m1.7.7.7.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.7.7.7.2.2.2.2.1.cmml" xref="S2.E1.m1.7.7.7.2.2.2.2">subscript</csymbol><ci id="S2.E1.m1.7.7.7.2.2.2.2.2.cmml" xref="S2.E1.m1.7.7.7.2.2.2.2.2">ùíë</ci><ci id="S2.E1.m1.7.7.7.2.2.2.2.3.cmml" xref="S2.E1.m1.7.7.7.2.2.2.2.3">ùëö</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.7c">\leavevmode\resizebox{433.62pt}{}{$HuBERT(q)=(\bm{q}_{1},...,\bm{q}_{n}),\mbox{\hskip 5.0pt}BERT(p)=(\bm{p}_{1},...,\bm{p}_{m})$}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p2.5" class="ltx_p">We use the first token embedding output from the speech and language modules to encode questions and passages into a single vector each:</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.1" class="ltx_Math" alttext="\vec{q}=\bm{q}_{1},\mbox{\hskip 5.0pt}\vec{p}=\bm{p}_{1}." display="block"><semantics id="S2.E2.m1.1a"><mrow id="S2.E2.m1.1.1.1"><mrow id="S2.E2.m1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.3.cmml"><mrow id="S2.E2.m1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.cmml"><mover accent="true" id="S2.E2.m1.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.2.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.2.2" xref="S2.E2.m1.1.1.1.1.1.1.2.2.cmml">q</mi><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.1.2.1" xref="S2.E2.m1.1.1.1.1.1.1.2.1.cmml">‚Üí</mo></mover><mo id="S2.E2.m1.1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.1.cmml">=</mo><msub id="S2.E2.m1.1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.1.1.1.1.1.1.3.2.cmml">ùíí</mi><mn id="S2.E2.m1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.1.1.1.1.1.1.3.3.cmml">1</mn></msub></mrow><mo id="S2.E2.m1.1.1.1.1.2.3" xref="S2.E2.m1.1.1.1.1.3a.cmml">,</mo><mrow id="S2.E2.m1.1.1.1.1.2.2" xref="S2.E2.m1.1.1.1.1.2.2.cmml"><mrow id="S2.E2.m1.1.1.1.1.2.2.2" xref="S2.E2.m1.1.1.1.1.2.2.2.cmml"><mtext id="S2.E2.m1.1.1.1.1.2.2.2.2" xref="S2.E2.m1.1.1.1.1.2.2.2.2a.cmml">¬†</mtext><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.1.2.2.2.1" xref="S2.E2.m1.1.1.1.1.2.2.2.1.cmml">‚Äã</mo><mover accent="true" id="S2.E2.m1.1.1.1.1.2.2.2.3" xref="S2.E2.m1.1.1.1.1.2.2.2.3.cmml"><mi id="S2.E2.m1.1.1.1.1.2.2.2.3.2" xref="S2.E2.m1.1.1.1.1.2.2.2.3.2.cmml">p</mi><mo stretchy="false" id="S2.E2.m1.1.1.1.1.2.2.2.3.1" xref="S2.E2.m1.1.1.1.1.2.2.2.3.1.cmml">‚Üí</mo></mover></mrow><mo id="S2.E2.m1.1.1.1.1.2.2.1" xref="S2.E2.m1.1.1.1.1.2.2.1.cmml">=</mo><msub id="S2.E2.m1.1.1.1.1.2.2.3" xref="S2.E2.m1.1.1.1.1.2.2.3.cmml"><mi id="S2.E2.m1.1.1.1.1.2.2.3.2" xref="S2.E2.m1.1.1.1.1.2.2.3.2.cmml">ùíë</mi><mn id="S2.E2.m1.1.1.1.1.2.2.3.3" xref="S2.E2.m1.1.1.1.1.2.2.3.3.cmml">1</mn></msub></mrow></mrow><mo lspace="0em" id="S2.E2.m1.1.1.1.2">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.3a.cmml" xref="S2.E2.m1.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S2.E2.m1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1"><eq id="S2.E2.m1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1"></eq><apply id="S2.E2.m1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2"><ci id="S2.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.1">‚Üí</ci><ci id="S2.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.2">ùëû</ci></apply><apply id="S2.E2.m1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.2">ùíí</ci><cn type="integer" id="S2.E2.m1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S2.E2.m1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.1.1.1.1.2.2"><eq id="S2.E2.m1.1.1.1.1.2.2.1.cmml" xref="S2.E2.m1.1.1.1.1.2.2.1"></eq><apply id="S2.E2.m1.1.1.1.1.2.2.2.cmml" xref="S2.E2.m1.1.1.1.1.2.2.2"><times id="S2.E2.m1.1.1.1.1.2.2.2.1.cmml" xref="S2.E2.m1.1.1.1.1.2.2.2.1"></times><ci id="S2.E2.m1.1.1.1.1.2.2.2.2a.cmml" xref="S2.E2.m1.1.1.1.1.2.2.2.2"><mtext id="S2.E2.m1.1.1.1.1.2.2.2.2.cmml" xref="S2.E2.m1.1.1.1.1.2.2.2.2">¬†</mtext></ci><apply id="S2.E2.m1.1.1.1.1.2.2.2.3.cmml" xref="S2.E2.m1.1.1.1.1.2.2.2.3"><ci id="S2.E2.m1.1.1.1.1.2.2.2.3.1.cmml" xref="S2.E2.m1.1.1.1.1.2.2.2.3.1">‚Üí</ci><ci id="S2.E2.m1.1.1.1.1.2.2.2.3.2.cmml" xref="S2.E2.m1.1.1.1.1.2.2.2.3.2">ùëù</ci></apply></apply><apply id="S2.E2.m1.1.1.1.1.2.2.3.cmml" xref="S2.E2.m1.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.2.2.3.1.cmml" xref="S2.E2.m1.1.1.1.1.2.2.3">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.2.2.3.2.cmml" xref="S2.E2.m1.1.1.1.1.2.2.3.2">ùíë</ci><cn type="integer" id="S2.E2.m1.1.1.1.1.2.2.3.3.cmml" xref="S2.E2.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">\vec{q}=\bm{q}_{1},\mbox{\hskip 5.0pt}\vec{p}=\bm{p}_{1}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p2.6" class="ltx_p">The relevance between a question and a passage is computed as the dot product of their vectors:</p>
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.3" class="ltx_Math" alttext="s(q,p)=\vec{q}^{T}\cdot\vec{p}." display="block"><semantics id="S2.E3.m1.3a"><mrow id="S2.E3.m1.3.3.1" xref="S2.E3.m1.3.3.1.1.cmml"><mrow id="S2.E3.m1.3.3.1.1" xref="S2.E3.m1.3.3.1.1.cmml"><mrow id="S2.E3.m1.3.3.1.1.2" xref="S2.E3.m1.3.3.1.1.2.cmml"><mi id="S2.E3.m1.3.3.1.1.2.2" xref="S2.E3.m1.3.3.1.1.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.1.1.2.1" xref="S2.E3.m1.3.3.1.1.2.1.cmml">‚Äã</mo><mrow id="S2.E3.m1.3.3.1.1.2.3.2" xref="S2.E3.m1.3.3.1.1.2.3.1.cmml"><mo stretchy="false" id="S2.E3.m1.3.3.1.1.2.3.2.1" xref="S2.E3.m1.3.3.1.1.2.3.1.cmml">(</mo><mi id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml">q</mi><mo id="S2.E3.m1.3.3.1.1.2.3.2.2" xref="S2.E3.m1.3.3.1.1.2.3.1.cmml">,</mo><mi id="S2.E3.m1.2.2" xref="S2.E3.m1.2.2.cmml">p</mi><mo stretchy="false" id="S2.E3.m1.3.3.1.1.2.3.2.3" xref="S2.E3.m1.3.3.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.3.3.1.1.1" xref="S2.E3.m1.3.3.1.1.1.cmml">=</mo><mrow id="S2.E3.m1.3.3.1.1.3" xref="S2.E3.m1.3.3.1.1.3.cmml"><msup id="S2.E3.m1.3.3.1.1.3.2" xref="S2.E3.m1.3.3.1.1.3.2.cmml"><mover accent="true" id="S2.E3.m1.3.3.1.1.3.2.2" xref="S2.E3.m1.3.3.1.1.3.2.2.cmml"><mi id="S2.E3.m1.3.3.1.1.3.2.2.2" xref="S2.E3.m1.3.3.1.1.3.2.2.2.cmml">q</mi><mo stretchy="false" id="S2.E3.m1.3.3.1.1.3.2.2.1" xref="S2.E3.m1.3.3.1.1.3.2.2.1.cmml">‚Üí</mo></mover><mi id="S2.E3.m1.3.3.1.1.3.2.3" xref="S2.E3.m1.3.3.1.1.3.2.3.cmml">T</mi></msup><mo lspace="0.222em" rspace="0.222em" id="S2.E3.m1.3.3.1.1.3.1" xref="S2.E3.m1.3.3.1.1.3.1.cmml">‚ãÖ</mo><mover accent="true" id="S2.E3.m1.3.3.1.1.3.3" xref="S2.E3.m1.3.3.1.1.3.3.cmml"><mi id="S2.E3.m1.3.3.1.1.3.3.2" xref="S2.E3.m1.3.3.1.1.3.3.2.cmml">p</mi><mo stretchy="false" id="S2.E3.m1.3.3.1.1.3.3.1" xref="S2.E3.m1.3.3.1.1.3.3.1.cmml">‚Üí</mo></mover></mrow></mrow><mo lspace="0em" id="S2.E3.m1.3.3.1.2" xref="S2.E3.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.3b"><apply id="S2.E3.m1.3.3.1.1.cmml" xref="S2.E3.m1.3.3.1"><eq id="S2.E3.m1.3.3.1.1.1.cmml" xref="S2.E3.m1.3.3.1.1.1"></eq><apply id="S2.E3.m1.3.3.1.1.2.cmml" xref="S2.E3.m1.3.3.1.1.2"><times id="S2.E3.m1.3.3.1.1.2.1.cmml" xref="S2.E3.m1.3.3.1.1.2.1"></times><ci id="S2.E3.m1.3.3.1.1.2.2.cmml" xref="S2.E3.m1.3.3.1.1.2.2">ùë†</ci><interval closure="open" id="S2.E3.m1.3.3.1.1.2.3.1.cmml" xref="S2.E3.m1.3.3.1.1.2.3.2"><ci id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1">ùëû</ci><ci id="S2.E3.m1.2.2.cmml" xref="S2.E3.m1.2.2">ùëù</ci></interval></apply><apply id="S2.E3.m1.3.3.1.1.3.cmml" xref="S2.E3.m1.3.3.1.1.3"><ci id="S2.E3.m1.3.3.1.1.3.1.cmml" xref="S2.E3.m1.3.3.1.1.3.1">‚ãÖ</ci><apply id="S2.E3.m1.3.3.1.1.3.2.cmml" xref="S2.E3.m1.3.3.1.1.3.2"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.1.1.3.2.1.cmml" xref="S2.E3.m1.3.3.1.1.3.2">superscript</csymbol><apply id="S2.E3.m1.3.3.1.1.3.2.2.cmml" xref="S2.E3.m1.3.3.1.1.3.2.2"><ci id="S2.E3.m1.3.3.1.1.3.2.2.1.cmml" xref="S2.E3.m1.3.3.1.1.3.2.2.1">‚Üí</ci><ci id="S2.E3.m1.3.3.1.1.3.2.2.2.cmml" xref="S2.E3.m1.3.3.1.1.3.2.2.2">ùëû</ci></apply><ci id="S2.E3.m1.3.3.1.1.3.2.3.cmml" xref="S2.E3.m1.3.3.1.1.3.2.3">ùëá</ci></apply><apply id="S2.E3.m1.3.3.1.1.3.3.cmml" xref="S2.E3.m1.3.3.1.1.3.3"><ci id="S2.E3.m1.3.3.1.1.3.3.1.cmml" xref="S2.E3.m1.3.3.1.1.3.3.1">‚Üí</ci><ci id="S2.E3.m1.3.3.1.1.3.3.2.cmml" xref="S2.E3.m1.3.3.1.1.3.3.2">ùëù</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.3c">s(q,p)=\vec{q}^{T}\cdot\vec{p}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p2.3" class="ltx_p">To this extent, we train our model so that relevant passages to the question (i.e., passages that include the answer) have a higher similarity score than the irrelevant passages. We followed the original dual-encoder training setting from <cite class="ltx_cite ltx_citemacro_citet">Karpukhin et¬†al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> where, given a question <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mi id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">ùëû</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">q</annotation></semantics></math>, a relevant passage <math id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="p^{+}" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><msup id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml"><mi id="S2.SS2.p2.2.m2.1.1.2" xref="S2.SS2.p2.2.m2.1.1.2.cmml">p</mi><mo id="S2.SS2.p2.2.m2.1.1.3" xref="S2.SS2.p2.2.m2.1.1.3.cmml">+</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><apply id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.2.m2.1.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">superscript</csymbol><ci id="S2.SS2.p2.2.m2.1.1.2.cmml" xref="S2.SS2.p2.2.m2.1.1.2">ùëù</ci><plus id="S2.SS2.p2.2.m2.1.1.3.cmml" xref="S2.SS2.p2.2.m2.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">p^{+}</annotation></semantics></math> and a set of irrelevant passages <math id="S2.SS2.p2.3.m3.4" class="ltx_Math" alttext="\{p_{1}^{-},p_{2}^{-},\dots,p_{n}^{-}\}" display="inline"><semantics id="S2.SS2.p2.3.m3.4a"><mrow id="S2.SS2.p2.3.m3.4.4.3" xref="S2.SS2.p2.3.m3.4.4.4.cmml"><mo stretchy="false" id="S2.SS2.p2.3.m3.4.4.3.4" xref="S2.SS2.p2.3.m3.4.4.4.cmml">{</mo><msubsup id="S2.SS2.p2.3.m3.2.2.1.1" xref="S2.SS2.p2.3.m3.2.2.1.1.cmml"><mi id="S2.SS2.p2.3.m3.2.2.1.1.2.2" xref="S2.SS2.p2.3.m3.2.2.1.1.2.2.cmml">p</mi><mn id="S2.SS2.p2.3.m3.2.2.1.1.2.3" xref="S2.SS2.p2.3.m3.2.2.1.1.2.3.cmml">1</mn><mo id="S2.SS2.p2.3.m3.2.2.1.1.3" xref="S2.SS2.p2.3.m3.2.2.1.1.3.cmml">‚àí</mo></msubsup><mo id="S2.SS2.p2.3.m3.4.4.3.5" xref="S2.SS2.p2.3.m3.4.4.4.cmml">,</mo><msubsup id="S2.SS2.p2.3.m3.3.3.2.2" xref="S2.SS2.p2.3.m3.3.3.2.2.cmml"><mi id="S2.SS2.p2.3.m3.3.3.2.2.2.2" xref="S2.SS2.p2.3.m3.3.3.2.2.2.2.cmml">p</mi><mn id="S2.SS2.p2.3.m3.3.3.2.2.2.3" xref="S2.SS2.p2.3.m3.3.3.2.2.2.3.cmml">2</mn><mo id="S2.SS2.p2.3.m3.3.3.2.2.3" xref="S2.SS2.p2.3.m3.3.3.2.2.3.cmml">‚àí</mo></msubsup><mo id="S2.SS2.p2.3.m3.4.4.3.6" xref="S2.SS2.p2.3.m3.4.4.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml">‚Ä¶</mi><mo id="S2.SS2.p2.3.m3.4.4.3.7" xref="S2.SS2.p2.3.m3.4.4.4.cmml">,</mo><msubsup id="S2.SS2.p2.3.m3.4.4.3.3" xref="S2.SS2.p2.3.m3.4.4.3.3.cmml"><mi id="S2.SS2.p2.3.m3.4.4.3.3.2.2" xref="S2.SS2.p2.3.m3.4.4.3.3.2.2.cmml">p</mi><mi id="S2.SS2.p2.3.m3.4.4.3.3.2.3" xref="S2.SS2.p2.3.m3.4.4.3.3.2.3.cmml">n</mi><mo id="S2.SS2.p2.3.m3.4.4.3.3.3" xref="S2.SS2.p2.3.m3.4.4.3.3.3.cmml">‚àí</mo></msubsup><mo stretchy="false" id="S2.SS2.p2.3.m3.4.4.3.8" xref="S2.SS2.p2.3.m3.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.4b"><set id="S2.SS2.p2.3.m3.4.4.4.cmml" xref="S2.SS2.p2.3.m3.4.4.3"><apply id="S2.SS2.p2.3.m3.2.2.1.1.cmml" xref="S2.SS2.p2.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.3.m3.2.2.1.1.1.cmml" xref="S2.SS2.p2.3.m3.2.2.1.1">superscript</csymbol><apply id="S2.SS2.p2.3.m3.2.2.1.1.2.cmml" xref="S2.SS2.p2.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.3.m3.2.2.1.1.2.1.cmml" xref="S2.SS2.p2.3.m3.2.2.1.1">subscript</csymbol><ci id="S2.SS2.p2.3.m3.2.2.1.1.2.2.cmml" xref="S2.SS2.p2.3.m3.2.2.1.1.2.2">ùëù</ci><cn type="integer" id="S2.SS2.p2.3.m3.2.2.1.1.2.3.cmml" xref="S2.SS2.p2.3.m3.2.2.1.1.2.3">1</cn></apply><minus id="S2.SS2.p2.3.m3.2.2.1.1.3.cmml" xref="S2.SS2.p2.3.m3.2.2.1.1.3"></minus></apply><apply id="S2.SS2.p2.3.m3.3.3.2.2.cmml" xref="S2.SS2.p2.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS2.p2.3.m3.3.3.2.2.1.cmml" xref="S2.SS2.p2.3.m3.3.3.2.2">superscript</csymbol><apply id="S2.SS2.p2.3.m3.3.3.2.2.2.cmml" xref="S2.SS2.p2.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS2.p2.3.m3.3.3.2.2.2.1.cmml" xref="S2.SS2.p2.3.m3.3.3.2.2">subscript</csymbol><ci id="S2.SS2.p2.3.m3.3.3.2.2.2.2.cmml" xref="S2.SS2.p2.3.m3.3.3.2.2.2.2">ùëù</ci><cn type="integer" id="S2.SS2.p2.3.m3.3.3.2.2.2.3.cmml" xref="S2.SS2.p2.3.m3.3.3.2.2.2.3">2</cn></apply><minus id="S2.SS2.p2.3.m3.3.3.2.2.3.cmml" xref="S2.SS2.p2.3.m3.3.3.2.2.3"></minus></apply><ci id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1">‚Ä¶</ci><apply id="S2.SS2.p2.3.m3.4.4.3.3.cmml" xref="S2.SS2.p2.3.m3.4.4.3.3"><csymbol cd="ambiguous" id="S2.SS2.p2.3.m3.4.4.3.3.1.cmml" xref="S2.SS2.p2.3.m3.4.4.3.3">superscript</csymbol><apply id="S2.SS2.p2.3.m3.4.4.3.3.2.cmml" xref="S2.SS2.p2.3.m3.4.4.3.3"><csymbol cd="ambiguous" id="S2.SS2.p2.3.m3.4.4.3.3.2.1.cmml" xref="S2.SS2.p2.3.m3.4.4.3.3">subscript</csymbol><ci id="S2.SS2.p2.3.m3.4.4.3.3.2.2.cmml" xref="S2.SS2.p2.3.m3.4.4.3.3.2.2">ùëù</ci><ci id="S2.SS2.p2.3.m3.4.4.3.3.2.3.cmml" xref="S2.SS2.p2.3.m3.4.4.3.3.2.3">ùëõ</ci></apply><minus id="S2.SS2.p2.3.m3.4.4.3.3.3.cmml" xref="S2.SS2.p2.3.m3.4.4.3.3.3"></minus></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.4c">\{p_{1}^{-},p_{2}^{-},\dots,p_{n}^{-}\}</annotation></semantics></math>, the model is fine-tuned via the minimization of the softmax cross-entropy:</p>
<table id="S2.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E4.m1.7" class="ltx_Math" alttext="\mathcal{L}_{CE}=-\log\frac{e^{s(q,p^{+})}}{e^{s(q,p^{+})}+\sum_{p^{-}}e^{s(q,p^{-})}}." display="block"><semantics id="S2.E4.m1.7a"><mrow id="S2.E4.m1.7.7.1" xref="S2.E4.m1.7.7.1.1.cmml"><mrow id="S2.E4.m1.7.7.1.1" xref="S2.E4.m1.7.7.1.1.cmml"><msub id="S2.E4.m1.7.7.1.1.2" xref="S2.E4.m1.7.7.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.7.7.1.1.2.2" xref="S2.E4.m1.7.7.1.1.2.2.cmml">‚Ñí</mi><mrow id="S2.E4.m1.7.7.1.1.2.3" xref="S2.E4.m1.7.7.1.1.2.3.cmml"><mi id="S2.E4.m1.7.7.1.1.2.3.2" xref="S2.E4.m1.7.7.1.1.2.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.7.7.1.1.2.3.1" xref="S2.E4.m1.7.7.1.1.2.3.1.cmml">‚Äã</mo><mi id="S2.E4.m1.7.7.1.1.2.3.3" xref="S2.E4.m1.7.7.1.1.2.3.3.cmml">E</mi></mrow></msub><mo id="S2.E4.m1.7.7.1.1.1" xref="S2.E4.m1.7.7.1.1.1.cmml">=</mo><mrow id="S2.E4.m1.7.7.1.1.3" xref="S2.E4.m1.7.7.1.1.3.cmml"><mo rspace="0.167em" id="S2.E4.m1.7.7.1.1.3a" xref="S2.E4.m1.7.7.1.1.3.cmml">‚àí</mo><mrow id="S2.E4.m1.7.7.1.1.3.2" xref="S2.E4.m1.7.7.1.1.3.2.cmml"><mi id="S2.E4.m1.7.7.1.1.3.2.1" xref="S2.E4.m1.7.7.1.1.3.2.1.cmml">log</mi><mo lspace="0.167em" id="S2.E4.m1.7.7.1.1.3.2a" xref="S2.E4.m1.7.7.1.1.3.2.cmml">‚Å°</mo><mfrac id="S2.E4.m1.6.6" xref="S2.E4.m1.6.6.cmml"><msup id="S2.E4.m1.2.2.2" xref="S2.E4.m1.2.2.2.cmml"><mi id="S2.E4.m1.2.2.2.4" xref="S2.E4.m1.2.2.2.4.cmml">e</mi><mrow id="S2.E4.m1.2.2.2.2.2" xref="S2.E4.m1.2.2.2.2.2.cmml"><mi id="S2.E4.m1.2.2.2.2.2.4" xref="S2.E4.m1.2.2.2.2.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.2.2.2.2.2.3" xref="S2.E4.m1.2.2.2.2.2.3.cmml">‚Äã</mo><mrow id="S2.E4.m1.2.2.2.2.2.2.1" xref="S2.E4.m1.2.2.2.2.2.2.2.cmml"><mo stretchy="false" id="S2.E4.m1.2.2.2.2.2.2.1.2" xref="S2.E4.m1.2.2.2.2.2.2.2.cmml">(</mo><mi id="S2.E4.m1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.cmml">q</mi><mo id="S2.E4.m1.2.2.2.2.2.2.1.3" xref="S2.E4.m1.2.2.2.2.2.2.2.cmml">,</mo><msup id="S2.E4.m1.2.2.2.2.2.2.1.1" xref="S2.E4.m1.2.2.2.2.2.2.1.1.cmml"><mi id="S2.E4.m1.2.2.2.2.2.2.1.1.2" xref="S2.E4.m1.2.2.2.2.2.2.1.1.2.cmml">p</mi><mo id="S2.E4.m1.2.2.2.2.2.2.1.1.3" xref="S2.E4.m1.2.2.2.2.2.2.1.1.3.cmml">+</mo></msup><mo stretchy="false" id="S2.E4.m1.2.2.2.2.2.2.1.4" xref="S2.E4.m1.2.2.2.2.2.2.2.cmml">)</mo></mrow></mrow></msup><mrow id="S2.E4.m1.6.6.6" xref="S2.E4.m1.6.6.6.cmml"><msup id="S2.E4.m1.6.6.6.6" xref="S2.E4.m1.6.6.6.6.cmml"><mi id="S2.E4.m1.6.6.6.6.2" xref="S2.E4.m1.6.6.6.6.2.cmml">e</mi><mrow id="S2.E4.m1.4.4.4.2.2" xref="S2.E4.m1.4.4.4.2.2.cmml"><mi id="S2.E4.m1.4.4.4.2.2.4" xref="S2.E4.m1.4.4.4.2.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.4.4.4.2.2.3" xref="S2.E4.m1.4.4.4.2.2.3.cmml">‚Äã</mo><mrow id="S2.E4.m1.4.4.4.2.2.2.1" xref="S2.E4.m1.4.4.4.2.2.2.2.cmml"><mo stretchy="false" id="S2.E4.m1.4.4.4.2.2.2.1.2" xref="S2.E4.m1.4.4.4.2.2.2.2.cmml">(</mo><mi id="S2.E4.m1.3.3.3.1.1.1" xref="S2.E4.m1.3.3.3.1.1.1.cmml">q</mi><mo id="S2.E4.m1.4.4.4.2.2.2.1.3" xref="S2.E4.m1.4.4.4.2.2.2.2.cmml">,</mo><msup id="S2.E4.m1.4.4.4.2.2.2.1.1" xref="S2.E4.m1.4.4.4.2.2.2.1.1.cmml"><mi id="S2.E4.m1.4.4.4.2.2.2.1.1.2" xref="S2.E4.m1.4.4.4.2.2.2.1.1.2.cmml">p</mi><mo id="S2.E4.m1.4.4.4.2.2.2.1.1.3" xref="S2.E4.m1.4.4.4.2.2.2.1.1.3.cmml">+</mo></msup><mo stretchy="false" id="S2.E4.m1.4.4.4.2.2.2.1.4" xref="S2.E4.m1.4.4.4.2.2.2.2.cmml">)</mo></mrow></mrow></msup><mo rspace="0.055em" id="S2.E4.m1.6.6.6.5" xref="S2.E4.m1.6.6.6.5.cmml">+</mo><mrow id="S2.E4.m1.6.6.6.7" xref="S2.E4.m1.6.6.6.7.cmml"><msub id="S2.E4.m1.6.6.6.7.1" xref="S2.E4.m1.6.6.6.7.1.cmml"><mo id="S2.E4.m1.6.6.6.7.1.2" xref="S2.E4.m1.6.6.6.7.1.2.cmml">‚àë</mo><msup id="S2.E4.m1.6.6.6.7.1.3" xref="S2.E4.m1.6.6.6.7.1.3.cmml"><mi id="S2.E4.m1.6.6.6.7.1.3.2" xref="S2.E4.m1.6.6.6.7.1.3.2.cmml">p</mi><mo id="S2.E4.m1.6.6.6.7.1.3.3" xref="S2.E4.m1.6.6.6.7.1.3.3.cmml">‚àí</mo></msup></msub><msup id="S2.E4.m1.6.6.6.7.2" xref="S2.E4.m1.6.6.6.7.2.cmml"><mi id="S2.E4.m1.6.6.6.7.2.2" xref="S2.E4.m1.6.6.6.7.2.2.cmml">e</mi><mrow id="S2.E4.m1.6.6.6.4.2" xref="S2.E4.m1.6.6.6.4.2.cmml"><mi id="S2.E4.m1.6.6.6.4.2.4" xref="S2.E4.m1.6.6.6.4.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E4.m1.6.6.6.4.2.3" xref="S2.E4.m1.6.6.6.4.2.3.cmml">‚Äã</mo><mrow id="S2.E4.m1.6.6.6.4.2.2.1" xref="S2.E4.m1.6.6.6.4.2.2.2.cmml"><mo stretchy="false" id="S2.E4.m1.6.6.6.4.2.2.1.2" xref="S2.E4.m1.6.6.6.4.2.2.2.cmml">(</mo><mi id="S2.E4.m1.5.5.5.3.1.1" xref="S2.E4.m1.5.5.5.3.1.1.cmml">q</mi><mo id="S2.E4.m1.6.6.6.4.2.2.1.3" xref="S2.E4.m1.6.6.6.4.2.2.2.cmml">,</mo><msup id="S2.E4.m1.6.6.6.4.2.2.1.1" xref="S2.E4.m1.6.6.6.4.2.2.1.1.cmml"><mi id="S2.E4.m1.6.6.6.4.2.2.1.1.2" xref="S2.E4.m1.6.6.6.4.2.2.1.1.2.cmml">p</mi><mo id="S2.E4.m1.6.6.6.4.2.2.1.1.3" xref="S2.E4.m1.6.6.6.4.2.2.1.1.3.cmml">‚àí</mo></msup><mo stretchy="false" id="S2.E4.m1.6.6.6.4.2.2.1.4" xref="S2.E4.m1.6.6.6.4.2.2.2.cmml">)</mo></mrow></mrow></msup></mrow></mrow></mfrac></mrow></mrow></mrow><mo lspace="0em" id="S2.E4.m1.7.7.1.2" xref="S2.E4.m1.7.7.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.7b"><apply id="S2.E4.m1.7.7.1.1.cmml" xref="S2.E4.m1.7.7.1"><eq id="S2.E4.m1.7.7.1.1.1.cmml" xref="S2.E4.m1.7.7.1.1.1"></eq><apply id="S2.E4.m1.7.7.1.1.2.cmml" xref="S2.E4.m1.7.7.1.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.7.7.1.1.2.1.cmml" xref="S2.E4.m1.7.7.1.1.2">subscript</csymbol><ci id="S2.E4.m1.7.7.1.1.2.2.cmml" xref="S2.E4.m1.7.7.1.1.2.2">‚Ñí</ci><apply id="S2.E4.m1.7.7.1.1.2.3.cmml" xref="S2.E4.m1.7.7.1.1.2.3"><times id="S2.E4.m1.7.7.1.1.2.3.1.cmml" xref="S2.E4.m1.7.7.1.1.2.3.1"></times><ci id="S2.E4.m1.7.7.1.1.2.3.2.cmml" xref="S2.E4.m1.7.7.1.1.2.3.2">ùê∂</ci><ci id="S2.E4.m1.7.7.1.1.2.3.3.cmml" xref="S2.E4.m1.7.7.1.1.2.3.3">ùê∏</ci></apply></apply><apply id="S2.E4.m1.7.7.1.1.3.cmml" xref="S2.E4.m1.7.7.1.1.3"><minus id="S2.E4.m1.7.7.1.1.3.1.cmml" xref="S2.E4.m1.7.7.1.1.3"></minus><apply id="S2.E4.m1.7.7.1.1.3.2.cmml" xref="S2.E4.m1.7.7.1.1.3.2"><log id="S2.E4.m1.7.7.1.1.3.2.1.cmml" xref="S2.E4.m1.7.7.1.1.3.2.1"></log><apply id="S2.E4.m1.6.6.cmml" xref="S2.E4.m1.6.6"><divide id="S2.E4.m1.6.6.7.cmml" xref="S2.E4.m1.6.6"></divide><apply id="S2.E4.m1.2.2.2.cmml" xref="S2.E4.m1.2.2.2"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.2.3.cmml" xref="S2.E4.m1.2.2.2">superscript</csymbol><ci id="S2.E4.m1.2.2.2.4.cmml" xref="S2.E4.m1.2.2.2.4">ùëí</ci><apply id="S2.E4.m1.2.2.2.2.2.cmml" xref="S2.E4.m1.2.2.2.2.2"><times id="S2.E4.m1.2.2.2.2.2.3.cmml" xref="S2.E4.m1.2.2.2.2.2.3"></times><ci id="S2.E4.m1.2.2.2.2.2.4.cmml" xref="S2.E4.m1.2.2.2.2.2.4">ùë†</ci><interval closure="open" id="S2.E4.m1.2.2.2.2.2.2.2.cmml" xref="S2.E4.m1.2.2.2.2.2.2.1"><ci id="S2.E4.m1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.1.1.1.1.1.1">ùëû</ci><apply id="S2.E4.m1.2.2.2.2.2.2.1.1.cmml" xref="S2.E4.m1.2.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.2.2.2.2.1.1.1.cmml" xref="S2.E4.m1.2.2.2.2.2.2.1.1">superscript</csymbol><ci id="S2.E4.m1.2.2.2.2.2.2.1.1.2.cmml" xref="S2.E4.m1.2.2.2.2.2.2.1.1.2">ùëù</ci><plus id="S2.E4.m1.2.2.2.2.2.2.1.1.3.cmml" xref="S2.E4.m1.2.2.2.2.2.2.1.1.3"></plus></apply></interval></apply></apply><apply id="S2.E4.m1.6.6.6.cmml" xref="S2.E4.m1.6.6.6"><plus id="S2.E4.m1.6.6.6.5.cmml" xref="S2.E4.m1.6.6.6.5"></plus><apply id="S2.E4.m1.6.6.6.6.cmml" xref="S2.E4.m1.6.6.6.6"><csymbol cd="ambiguous" id="S2.E4.m1.6.6.6.6.1.cmml" xref="S2.E4.m1.6.6.6.6">superscript</csymbol><ci id="S2.E4.m1.6.6.6.6.2.cmml" xref="S2.E4.m1.6.6.6.6.2">ùëí</ci><apply id="S2.E4.m1.4.4.4.2.2.cmml" xref="S2.E4.m1.4.4.4.2.2"><times id="S2.E4.m1.4.4.4.2.2.3.cmml" xref="S2.E4.m1.4.4.4.2.2.3"></times><ci id="S2.E4.m1.4.4.4.2.2.4.cmml" xref="S2.E4.m1.4.4.4.2.2.4">ùë†</ci><interval closure="open" id="S2.E4.m1.4.4.4.2.2.2.2.cmml" xref="S2.E4.m1.4.4.4.2.2.2.1"><ci id="S2.E4.m1.3.3.3.1.1.1.cmml" xref="S2.E4.m1.3.3.3.1.1.1">ùëû</ci><apply id="S2.E4.m1.4.4.4.2.2.2.1.1.cmml" xref="S2.E4.m1.4.4.4.2.2.2.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.4.4.4.2.2.2.1.1.1.cmml" xref="S2.E4.m1.4.4.4.2.2.2.1.1">superscript</csymbol><ci id="S2.E4.m1.4.4.4.2.2.2.1.1.2.cmml" xref="S2.E4.m1.4.4.4.2.2.2.1.1.2">ùëù</ci><plus id="S2.E4.m1.4.4.4.2.2.2.1.1.3.cmml" xref="S2.E4.m1.4.4.4.2.2.2.1.1.3"></plus></apply></interval></apply></apply><apply id="S2.E4.m1.6.6.6.7.cmml" xref="S2.E4.m1.6.6.6.7"><apply id="S2.E4.m1.6.6.6.7.1.cmml" xref="S2.E4.m1.6.6.6.7.1"><csymbol cd="ambiguous" id="S2.E4.m1.6.6.6.7.1.1.cmml" xref="S2.E4.m1.6.6.6.7.1">subscript</csymbol><sum id="S2.E4.m1.6.6.6.7.1.2.cmml" xref="S2.E4.m1.6.6.6.7.1.2"></sum><apply id="S2.E4.m1.6.6.6.7.1.3.cmml" xref="S2.E4.m1.6.6.6.7.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.6.6.6.7.1.3.1.cmml" xref="S2.E4.m1.6.6.6.7.1.3">superscript</csymbol><ci id="S2.E4.m1.6.6.6.7.1.3.2.cmml" xref="S2.E4.m1.6.6.6.7.1.3.2">ùëù</ci><minus id="S2.E4.m1.6.6.6.7.1.3.3.cmml" xref="S2.E4.m1.6.6.6.7.1.3.3"></minus></apply></apply><apply id="S2.E4.m1.6.6.6.7.2.cmml" xref="S2.E4.m1.6.6.6.7.2"><csymbol cd="ambiguous" id="S2.E4.m1.6.6.6.7.2.1.cmml" xref="S2.E4.m1.6.6.6.7.2">superscript</csymbol><ci id="S2.E4.m1.6.6.6.7.2.2.cmml" xref="S2.E4.m1.6.6.6.7.2.2">ùëí</ci><apply id="S2.E4.m1.6.6.6.4.2.cmml" xref="S2.E4.m1.6.6.6.4.2"><times id="S2.E4.m1.6.6.6.4.2.3.cmml" xref="S2.E4.m1.6.6.6.4.2.3"></times><ci id="S2.E4.m1.6.6.6.4.2.4.cmml" xref="S2.E4.m1.6.6.6.4.2.4">ùë†</ci><interval closure="open" id="S2.E4.m1.6.6.6.4.2.2.2.cmml" xref="S2.E4.m1.6.6.6.4.2.2.1"><ci id="S2.E4.m1.5.5.5.3.1.1.cmml" xref="S2.E4.m1.5.5.5.3.1.1">ùëû</ci><apply id="S2.E4.m1.6.6.6.4.2.2.1.1.cmml" xref="S2.E4.m1.6.6.6.4.2.2.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.6.6.6.4.2.2.1.1.1.cmml" xref="S2.E4.m1.6.6.6.4.2.2.1.1">superscript</csymbol><ci id="S2.E4.m1.6.6.6.4.2.2.1.1.2.cmml" xref="S2.E4.m1.6.6.6.4.2.2.1.1.2">ùëù</ci><minus id="S2.E4.m1.6.6.6.4.2.2.1.1.3.cmml" xref="S2.E4.m1.6.6.6.4.2.2.1.1.3"></minus></apply></interval></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.7c">\mathcal{L}_{CE}=-\log\frac{e^{s(q,p^{+})}}{e^{s(q,p^{+})}+\sum_{p^{-}}e^{s(q,p^{-})}}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p2.7" class="ltx_p">The inference phase of our multimodal dense retriever remains the same as in traditional dual-encoders for dense text retrieval. Specifically, we compute the similarity of a question-passage pair as the inner product of the respective question embedding and passage embedding. At query time, only the question needs to be encoded. In detail, we build a dense index of passage vectors (offline) by encoding the whole corpus and storing it in an index structure that supports efficient retrieval of the relevant passages via approximate nearest neighbor search <cite class="ltx_cite ltx_citemacro_cite">Johnson et¬†al. (<a href="#bib.bib7" title="" class="ltx_ref">2021</a>)</cite>. At this point, we want to highlight that we choose a dual-encoder architecture because it has shown high efficiency as a first-stage ranker in large-scale settings. On the contrary, even though cross-encoder architectures can achieve higher performance due to jointly encoding questions and passages, they are not indexable and hence are re-rankers.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Baselines and Implementation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p">We compare our proposed multimodal method against pipeline approaches that consist of (i) an ASR model for transcribing the spoken question and (ii) a retriever. In detail, similar to <cite class="ltx_cite ltx_citemacro_citet">Sidiropoulos et¬†al. (<a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite> we use <span id="S3.SS1.p1.2.1" class="ltx_text ltx_font_italic">wav2vec</span> <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="2.0" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">2.0</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><cn type="float" id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">2.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">2.0</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">Baevski et¬†al. (<a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite> pretrained and fine-tuned on <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="960" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">960</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><cn type="integer" id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">960</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">960</annotation></semantics></math> hours of annotated speech data <cite class="ltx_cite ltx_citemacro_cite">Panayotov et¬†al. (<a href="#bib.bib13" title="" class="ltx_ref">2015</a>)</cite> as the ASR model. Concerning the retrievers, we experiment with popular lexical and dense retrievers. We further experiment with dense retrievers explicitly trained to improve robustness against questions with typos since they improve robustness against ASR noise as well <cite class="ltx_cite ltx_citemacro_cite">Sidiropoulos et¬†al. (<a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We need to underline that contrary to <cite class="ltx_cite ltx_citemacro_citet">Sidiropoulos et¬†al. (<a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>, which assumed that the clean textual version of the spoken question is provided for training the retrievers, we follow a real-world scenario where only spoken questions are available; thus, we need to transcribe them for training the retrievers. Our experimental results showed that we build stronger baselines by training on the transcriptions of spoken questions. In particular, we experiment with the following retrievers:</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">BM25</span> is a traditional retriever based on term-matching. Question and retrieved passages have lexical overlap.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">Dense Retriever (DR)</span> is used for scoring question-passage pairs and consists of two separate neural networks (dual-encoder), each representing a question and a passage. Given a question, a positive, and a set of negative passages, the learning task trains the two encoders by minimizing a loss function, typically softmax cross-entropy, to encourage positive question-passage pairs to have smaller distances than the negative ones. To train the <span id="S3.SS1.p4.1.2" class="ltx_text ltx_font_italic">DR</span> model, we use the training scheme and hyperparameters described in <cite class="ltx_cite ltx_citemacro_cite">Karpukhin et¬†al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> with a batch size of 64 which is the largest we can fit in our GPU setup.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p"><span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_bold">Dense retriever with data augmentation (DR+Augm.)</span> alternates the training scheme of classic dense retrieval via the addition of data augmentation. Recently, there were works that explored a data augmentation approach for robustifying dense retrievers against typoed questions <cite class="ltx_cite ltx_citemacro_cite">Zhuang and Zuccon (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>); Sidiropoulos and Kanoulas (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite>. Specifically, during the training phase of the dense retriever, an unbiased coin is drawn for each question that appears. If tails, the unchanged question is used for training. Otherwise, the question is injected with typos. Typos are sampled uniformly from one of the available typo generators (e.g., random insertion/deletion/substitution, neighbor swapping).</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">The <span id="S3.SS1.p6.1.1" class="ltx_text ltx_font_italic">DR+Augm.</span> model we use in our experiments is trained following the training process described in <cite class="ltx_cite ltx_citemacro_cite">Sidiropoulos and Kanoulas (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite>. The hyperparameters of the dense retriever remain the same as in the <span id="S3.SS1.p6.1.2" class="ltx_text ltx_font_italic">DR</span> case.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p"><span id="S3.SS1.p7.1.1" class="ltx_text ltx_font_bold">Dense retriever with CharacterBERT and self-teaching (CharacterBERT-DR+ST)</span> <cite class="ltx_cite ltx_citemacro_cite">Zhuang and Zuccon (<a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite> extends <span id="S3.SS1.p7.1.2" class="ltx_text ltx_font_italic">DR+Augm.</span> by (i) incorporating CharacterBERT, which drops the WordPiece tokenizer and replaces it with a CharacterCNN module, and (ii) by adding a loss that distills knowledge from questions without typos into the questions with typos. The distillation loss forces the retrieval model to generate similar rankings for the original question and its typoed variations. That is achieved by minimizing the KL-divergence:</p>
</div>
<div id="S3.SS1.p8" class="ltx_para">
<table id="S7.EGx1" class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table">

<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E5.m1.6" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{KL}=\tilde{s}(q^{\prime},p)\cdot\log\frac{\tilde{s}(q^{\prime},p)}{\tilde{s}(q,p)}," display="inline"><semantics id="S3.E5.m1.6a"><mrow id="S3.E5.m1.6.6.1" xref="S3.E5.m1.6.6.1.1.cmml"><mrow id="S3.E5.m1.6.6.1.1" xref="S3.E5.m1.6.6.1.1.cmml"><msub id="S3.E5.m1.6.6.1.1.3" xref="S3.E5.m1.6.6.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E5.m1.6.6.1.1.3.2" xref="S3.E5.m1.6.6.1.1.3.2.cmml">‚Ñí</mi><mrow id="S3.E5.m1.6.6.1.1.3.3" xref="S3.E5.m1.6.6.1.1.3.3.cmml"><mi id="S3.E5.m1.6.6.1.1.3.3.2" xref="S3.E5.m1.6.6.1.1.3.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.E5.m1.6.6.1.1.3.3.1" xref="S3.E5.m1.6.6.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.E5.m1.6.6.1.1.3.3.3" xref="S3.E5.m1.6.6.1.1.3.3.3.cmml">L</mi></mrow></msub><mo id="S3.E5.m1.6.6.1.1.2" xref="S3.E5.m1.6.6.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.6.6.1.1.1" xref="S3.E5.m1.6.6.1.1.1.cmml"><mrow id="S3.E5.m1.6.6.1.1.1.1" xref="S3.E5.m1.6.6.1.1.1.1.cmml"><mover accent="true" id="S3.E5.m1.6.6.1.1.1.1.3" xref="S3.E5.m1.6.6.1.1.1.1.3.cmml"><mi id="S3.E5.m1.6.6.1.1.1.1.3.2" xref="S3.E5.m1.6.6.1.1.1.1.3.2.cmml">s</mi><mo id="S3.E5.m1.6.6.1.1.1.1.3.1" xref="S3.E5.m1.6.6.1.1.1.1.3.1.cmml">~</mo></mover><mo lspace="0em" rspace="0em" id="S3.E5.m1.6.6.1.1.1.1.2" xref="S3.E5.m1.6.6.1.1.1.1.2.cmml">‚Äã</mo><mrow id="S3.E5.m1.6.6.1.1.1.1.1.1" xref="S3.E5.m1.6.6.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E5.m1.6.6.1.1.1.1.1.1.2" xref="S3.E5.m1.6.6.1.1.1.1.1.2.cmml">(</mo><msup id="S3.E5.m1.6.6.1.1.1.1.1.1.1" xref="S3.E5.m1.6.6.1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.6.6.1.1.1.1.1.1.1.2" xref="S3.E5.m1.6.6.1.1.1.1.1.1.1.2.cmml">q</mi><mo id="S3.E5.m1.6.6.1.1.1.1.1.1.1.3" xref="S3.E5.m1.6.6.1.1.1.1.1.1.1.3.cmml">‚Ä≤</mo></msup><mo id="S3.E5.m1.6.6.1.1.1.1.1.1.3" xref="S3.E5.m1.6.6.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E5.m1.5.5" xref="S3.E5.m1.5.5.cmml">p</mi><mo rspace="0.055em" stretchy="false" id="S3.E5.m1.6.6.1.1.1.1.1.1.4" xref="S3.E5.m1.6.6.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S3.E5.m1.6.6.1.1.1.2" xref="S3.E5.m1.6.6.1.1.1.2.cmml">‚ãÖ</mo><mrow id="S3.E5.m1.6.6.1.1.1.3" xref="S3.E5.m1.6.6.1.1.1.3.cmml"><mi id="S3.E5.m1.6.6.1.1.1.3.1" xref="S3.E5.m1.6.6.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.E5.m1.6.6.1.1.1.3a" xref="S3.E5.m1.6.6.1.1.1.3.cmml">‚Å°</mo><mstyle displaystyle="true" id="S3.E5.m1.4.4" xref="S3.E5.m1.4.4.cmml"><mfrac id="S3.E5.m1.4.4a" xref="S3.E5.m1.4.4.cmml"><mrow id="S3.E5.m1.2.2.2" xref="S3.E5.m1.2.2.2.cmml"><mover accent="true" id="S3.E5.m1.2.2.2.4" xref="S3.E5.m1.2.2.2.4.cmml"><mi id="S3.E5.m1.2.2.2.4.2" xref="S3.E5.m1.2.2.2.4.2.cmml">s</mi><mo id="S3.E5.m1.2.2.2.4.1" xref="S3.E5.m1.2.2.2.4.1.cmml">~</mo></mover><mo lspace="0em" rspace="0em" id="S3.E5.m1.2.2.2.3" xref="S3.E5.m1.2.2.2.3.cmml">‚Äã</mo><mrow id="S3.E5.m1.2.2.2.2.1" xref="S3.E5.m1.2.2.2.2.2.cmml"><mo stretchy="false" id="S3.E5.m1.2.2.2.2.1.2" xref="S3.E5.m1.2.2.2.2.2.cmml">(</mo><msup id="S3.E5.m1.2.2.2.2.1.1" xref="S3.E5.m1.2.2.2.2.1.1.cmml"><mi id="S3.E5.m1.2.2.2.2.1.1.2" xref="S3.E5.m1.2.2.2.2.1.1.2.cmml">q</mi><mo id="S3.E5.m1.2.2.2.2.1.1.3" xref="S3.E5.m1.2.2.2.2.1.1.3.cmml">‚Ä≤</mo></msup><mo id="S3.E5.m1.2.2.2.2.1.3" xref="S3.E5.m1.2.2.2.2.2.cmml">,</mo><mi id="S3.E5.m1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml">p</mi><mo stretchy="false" id="S3.E5.m1.2.2.2.2.1.4" xref="S3.E5.m1.2.2.2.2.2.cmml">)</mo></mrow></mrow><mrow id="S3.E5.m1.4.4.4" xref="S3.E5.m1.4.4.4.cmml"><mover accent="true" id="S3.E5.m1.4.4.4.4" xref="S3.E5.m1.4.4.4.4.cmml"><mi id="S3.E5.m1.4.4.4.4.2" xref="S3.E5.m1.4.4.4.4.2.cmml">s</mi><mo id="S3.E5.m1.4.4.4.4.1" xref="S3.E5.m1.4.4.4.4.1.cmml">~</mo></mover><mo lspace="0em" rspace="0em" id="S3.E5.m1.4.4.4.3" xref="S3.E5.m1.4.4.4.3.cmml">‚Äã</mo><mrow id="S3.E5.m1.4.4.4.5.2" xref="S3.E5.m1.4.4.4.5.1.cmml"><mo stretchy="false" id="S3.E5.m1.4.4.4.5.2.1" xref="S3.E5.m1.4.4.4.5.1.cmml">(</mo><mi id="S3.E5.m1.3.3.3.1" xref="S3.E5.m1.3.3.3.1.cmml">q</mi><mo id="S3.E5.m1.4.4.4.5.2.2" xref="S3.E5.m1.4.4.4.5.1.cmml">,</mo><mi id="S3.E5.m1.4.4.4.2" xref="S3.E5.m1.4.4.4.2.cmml">p</mi><mo stretchy="false" id="S3.E5.m1.4.4.4.5.2.3" xref="S3.E5.m1.4.4.4.5.1.cmml">)</mo></mrow></mrow></mfrac></mstyle></mrow></mrow></mrow><mo id="S3.E5.m1.6.6.1.2" xref="S3.E5.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.6b"><apply id="S3.E5.m1.6.6.1.1.cmml" xref="S3.E5.m1.6.6.1"><eq id="S3.E5.m1.6.6.1.1.2.cmml" xref="S3.E5.m1.6.6.1.1.2"></eq><apply id="S3.E5.m1.6.6.1.1.3.cmml" xref="S3.E5.m1.6.6.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.6.6.1.1.3.1.cmml" xref="S3.E5.m1.6.6.1.1.3">subscript</csymbol><ci id="S3.E5.m1.6.6.1.1.3.2.cmml" xref="S3.E5.m1.6.6.1.1.3.2">‚Ñí</ci><apply id="S3.E5.m1.6.6.1.1.3.3.cmml" xref="S3.E5.m1.6.6.1.1.3.3"><times id="S3.E5.m1.6.6.1.1.3.3.1.cmml" xref="S3.E5.m1.6.6.1.1.3.3.1"></times><ci id="S3.E5.m1.6.6.1.1.3.3.2.cmml" xref="S3.E5.m1.6.6.1.1.3.3.2">ùêæ</ci><ci id="S3.E5.m1.6.6.1.1.3.3.3.cmml" xref="S3.E5.m1.6.6.1.1.3.3.3">ùêø</ci></apply></apply><apply id="S3.E5.m1.6.6.1.1.1.cmml" xref="S3.E5.m1.6.6.1.1.1"><ci id="S3.E5.m1.6.6.1.1.1.2.cmml" xref="S3.E5.m1.6.6.1.1.1.2">‚ãÖ</ci><apply id="S3.E5.m1.6.6.1.1.1.1.cmml" xref="S3.E5.m1.6.6.1.1.1.1"><times id="S3.E5.m1.6.6.1.1.1.1.2.cmml" xref="S3.E5.m1.6.6.1.1.1.1.2"></times><apply id="S3.E5.m1.6.6.1.1.1.1.3.cmml" xref="S3.E5.m1.6.6.1.1.1.1.3"><ci id="S3.E5.m1.6.6.1.1.1.1.3.1.cmml" xref="S3.E5.m1.6.6.1.1.1.1.3.1">~</ci><ci id="S3.E5.m1.6.6.1.1.1.1.3.2.cmml" xref="S3.E5.m1.6.6.1.1.1.1.3.2">ùë†</ci></apply><interval closure="open" id="S3.E5.m1.6.6.1.1.1.1.1.2.cmml" xref="S3.E5.m1.6.6.1.1.1.1.1.1"><apply id="S3.E5.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.6.6.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.6.6.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.6.6.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.E5.m1.6.6.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.6.6.1.1.1.1.1.1.1.2">ùëû</ci><ci id="S3.E5.m1.6.6.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.6.6.1.1.1.1.1.1.1.3">‚Ä≤</ci></apply><ci id="S3.E5.m1.5.5.cmml" xref="S3.E5.m1.5.5">ùëù</ci></interval></apply><apply id="S3.E5.m1.6.6.1.1.1.3.cmml" xref="S3.E5.m1.6.6.1.1.1.3"><log id="S3.E5.m1.6.6.1.1.1.3.1.cmml" xref="S3.E5.m1.6.6.1.1.1.3.1"></log><apply id="S3.E5.m1.4.4.cmml" xref="S3.E5.m1.4.4"><divide id="S3.E5.m1.4.4.5.cmml" xref="S3.E5.m1.4.4"></divide><apply id="S3.E5.m1.2.2.2.cmml" xref="S3.E5.m1.2.2.2"><times id="S3.E5.m1.2.2.2.3.cmml" xref="S3.E5.m1.2.2.2.3"></times><apply id="S3.E5.m1.2.2.2.4.cmml" xref="S3.E5.m1.2.2.2.4"><ci id="S3.E5.m1.2.2.2.4.1.cmml" xref="S3.E5.m1.2.2.2.4.1">~</ci><ci id="S3.E5.m1.2.2.2.4.2.cmml" xref="S3.E5.m1.2.2.2.4.2">ùë†</ci></apply><interval closure="open" id="S3.E5.m1.2.2.2.2.2.cmml" xref="S3.E5.m1.2.2.2.2.1"><apply id="S3.E5.m1.2.2.2.2.1.1.cmml" xref="S3.E5.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.2.2.2.2.1.1.1.cmml" xref="S3.E5.m1.2.2.2.2.1.1">superscript</csymbol><ci id="S3.E5.m1.2.2.2.2.1.1.2.cmml" xref="S3.E5.m1.2.2.2.2.1.1.2">ùëû</ci><ci id="S3.E5.m1.2.2.2.2.1.1.3.cmml" xref="S3.E5.m1.2.2.2.2.1.1.3">‚Ä≤</ci></apply><ci id="S3.E5.m1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1">ùëù</ci></interval></apply><apply id="S3.E5.m1.4.4.4.cmml" xref="S3.E5.m1.4.4.4"><times id="S3.E5.m1.4.4.4.3.cmml" xref="S3.E5.m1.4.4.4.3"></times><apply id="S3.E5.m1.4.4.4.4.cmml" xref="S3.E5.m1.4.4.4.4"><ci id="S3.E5.m1.4.4.4.4.1.cmml" xref="S3.E5.m1.4.4.4.4.1">~</ci><ci id="S3.E5.m1.4.4.4.4.2.cmml" xref="S3.E5.m1.4.4.4.4.2">ùë†</ci></apply><interval closure="open" id="S3.E5.m1.4.4.4.5.1.cmml" xref="S3.E5.m1.4.4.4.5.2"><ci id="S3.E5.m1.3.3.3.1.cmml" xref="S3.E5.m1.3.3.3.1">ùëû</ci><ci id="S3.E5.m1.4.4.4.2.cmml" xref="S3.E5.m1.4.4.4.2">ùëù</ci></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.6c">\displaystyle\mathcal{L}_{KL}=\tilde{s}(q^{\prime},p)\cdot\log\frac{\tilde{s}(q^{\prime},p)}{\tilde{s}(q,p)},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p8.5" class="ltx_p">where <math id="S3.SS1.p8.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS1.p8.1.m1.1a"><mi id="S3.SS1.p8.1.m1.1.1" xref="S3.SS1.p8.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.1.m1.1b"><ci id="S3.SS1.p8.1.m1.1.1.cmml" xref="S3.SS1.p8.1.m1.1.1">ùëû</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.1.m1.1c">q</annotation></semantics></math> and <math id="S3.SS1.p8.2.m2.1" class="ltx_Math" alttext="q^{\prime}" display="inline"><semantics id="S3.SS1.p8.2.m2.1a"><msup id="S3.SS1.p8.2.m2.1.1" xref="S3.SS1.p8.2.m2.1.1.cmml"><mi id="S3.SS1.p8.2.m2.1.1.2" xref="S3.SS1.p8.2.m2.1.1.2.cmml">q</mi><mo id="S3.SS1.p8.2.m2.1.1.3" xref="S3.SS1.p8.2.m2.1.1.3.cmml">‚Ä≤</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.2.m2.1b"><apply id="S3.SS1.p8.2.m2.1.1.cmml" xref="S3.SS1.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p8.2.m2.1.1.1.cmml" xref="S3.SS1.p8.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.p8.2.m2.1.1.2.cmml" xref="S3.SS1.p8.2.m2.1.1.2">ùëû</ci><ci id="S3.SS1.p8.2.m2.1.1.3.cmml" xref="S3.SS1.p8.2.m2.1.1.3">‚Ä≤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.2.m2.1c">q^{\prime}</annotation></semantics></math> represent the original question and its typoed variant, respectively while <math id="S3.SS1.p8.3.m3.1" class="ltx_Math" alttext="\tilde{s}" display="inline"><semantics id="S3.SS1.p8.3.m3.1a"><mover accent="true" id="S3.SS1.p8.3.m3.1.1" xref="S3.SS1.p8.3.m3.1.1.cmml"><mi id="S3.SS1.p8.3.m3.1.1.2" xref="S3.SS1.p8.3.m3.1.1.2.cmml">s</mi><mo id="S3.SS1.p8.3.m3.1.1.1" xref="S3.SS1.p8.3.m3.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.3.m3.1b"><apply id="S3.SS1.p8.3.m3.1.1.cmml" xref="S3.SS1.p8.3.m3.1.1"><ci id="S3.SS1.p8.3.m3.1.1.1.cmml" xref="S3.SS1.p8.3.m3.1.1.1">~</ci><ci id="S3.SS1.p8.3.m3.1.1.2.cmml" xref="S3.SS1.p8.3.m3.1.1.2">ùë†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.3.m3.1c">\tilde{s}</annotation></semantics></math> is the softmax normalized similarity score. The <math id="S3.SS1.p8.4.m4.1" class="ltx_Math" alttext="\mathcal{L}_{KL}" display="inline"><semantics id="S3.SS1.p8.4.m4.1a"><msub id="S3.SS1.p8.4.m4.1.1" xref="S3.SS1.p8.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p8.4.m4.1.1.2" xref="S3.SS1.p8.4.m4.1.1.2.cmml">‚Ñí</mi><mrow id="S3.SS1.p8.4.m4.1.1.3" xref="S3.SS1.p8.4.m4.1.1.3.cmml"><mi id="S3.SS1.p8.4.m4.1.1.3.2" xref="S3.SS1.p8.4.m4.1.1.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p8.4.m4.1.1.3.1" xref="S3.SS1.p8.4.m4.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p8.4.m4.1.1.3.3" xref="S3.SS1.p8.4.m4.1.1.3.3.cmml">L</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.4.m4.1b"><apply id="S3.SS1.p8.4.m4.1.1.cmml" xref="S3.SS1.p8.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p8.4.m4.1.1.1.cmml" xref="S3.SS1.p8.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p8.4.m4.1.1.2.cmml" xref="S3.SS1.p8.4.m4.1.1.2">‚Ñí</ci><apply id="S3.SS1.p8.4.m4.1.1.3.cmml" xref="S3.SS1.p8.4.m4.1.1.3"><times id="S3.SS1.p8.4.m4.1.1.3.1.cmml" xref="S3.SS1.p8.4.m4.1.1.3.1"></times><ci id="S3.SS1.p8.4.m4.1.1.3.2.cmml" xref="S3.SS1.p8.4.m4.1.1.3.2">ùêæ</ci><ci id="S3.SS1.p8.4.m4.1.1.3.3.cmml" xref="S3.SS1.p8.4.m4.1.1.3.3">ùêø</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.4.m4.1c">\mathcal{L}_{KL}</annotation></semantics></math> loss (Equation <a href="#S3.E5" title="In 3.1 Baselines and Implementation ‚Ä£ 3 Experimental Setup ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) is combined with the supervise softmax cross-entropy loss <math id="S3.SS1.p8.5.m5.1" class="ltx_Math" alttext="\mathcal{L}_{CE}" display="inline"><semantics id="S3.SS1.p8.5.m5.1a"><msub id="S3.SS1.p8.5.m5.1.1" xref="S3.SS1.p8.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p8.5.m5.1.1.2" xref="S3.SS1.p8.5.m5.1.1.2.cmml">‚Ñí</mi><mrow id="S3.SS1.p8.5.m5.1.1.3" xref="S3.SS1.p8.5.m5.1.1.3.cmml"><mi id="S3.SS1.p8.5.m5.1.1.3.2" xref="S3.SS1.p8.5.m5.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p8.5.m5.1.1.3.1" xref="S3.SS1.p8.5.m5.1.1.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p8.5.m5.1.1.3.3" xref="S3.SS1.p8.5.m5.1.1.3.3.cmml">E</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p8.5.m5.1b"><apply id="S3.SS1.p8.5.m5.1.1.cmml" xref="S3.SS1.p8.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p8.5.m5.1.1.1.cmml" xref="S3.SS1.p8.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p8.5.m5.1.1.2.cmml" xref="S3.SS1.p8.5.m5.1.1.2">‚Ñí</ci><apply id="S3.SS1.p8.5.m5.1.1.3.cmml" xref="S3.SS1.p8.5.m5.1.1.3"><times id="S3.SS1.p8.5.m5.1.1.3.1.cmml" xref="S3.SS1.p8.5.m5.1.1.3.1"></times><ci id="S3.SS1.p8.5.m5.1.1.3.2.cmml" xref="S3.SS1.p8.5.m5.1.1.3.2">ùê∂</ci><ci id="S3.SS1.p8.5.m5.1.1.3.3.cmml" xref="S3.SS1.p8.5.m5.1.1.3.3">ùê∏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p8.5.m5.1c">\mathcal{L}_{CE}</annotation></semantics></math> to form the final loss:</p>
<table id="S7.EGx2" class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table">

<tbody id="S3.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E6.m1.1" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{ST}=\mathcal{L}_{KL}+\mathcal{L}_{CE}." display="inline"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><mrow id="S3.E6.m1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.cmml"><msub id="S3.E6.m1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.1.1.1.1.2.2" xref="S3.E6.m1.1.1.1.1.2.2.cmml">‚Ñí</mi><mrow id="S3.E6.m1.1.1.1.1.2.3" xref="S3.E6.m1.1.1.1.1.2.3.cmml"><mi id="S3.E6.m1.1.1.1.1.2.3.2" xref="S3.E6.m1.1.1.1.1.2.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.2.3.1" xref="S3.E6.m1.1.1.1.1.2.3.1.cmml">‚Äã</mo><mi id="S3.E6.m1.1.1.1.1.2.3.3" xref="S3.E6.m1.1.1.1.1.2.3.3.cmml">T</mi></mrow></msub><mo id="S3.E6.m1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E6.m1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.3.cmml"><msub id="S3.E6.m1.1.1.1.1.3.2" xref="S3.E6.m1.1.1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.1.1.1.1.3.2.2" xref="S3.E6.m1.1.1.1.1.3.2.2.cmml">‚Ñí</mi><mrow id="S3.E6.m1.1.1.1.1.3.2.3" xref="S3.E6.m1.1.1.1.1.3.2.3.cmml"><mi id="S3.E6.m1.1.1.1.1.3.2.3.2" xref="S3.E6.m1.1.1.1.1.3.2.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.3.2.3.1" xref="S3.E6.m1.1.1.1.1.3.2.3.1.cmml">‚Äã</mo><mi id="S3.E6.m1.1.1.1.1.3.2.3.3" xref="S3.E6.m1.1.1.1.1.3.2.3.3.cmml">L</mi></mrow></msub><mo id="S3.E6.m1.1.1.1.1.3.1" xref="S3.E6.m1.1.1.1.1.3.1.cmml">+</mo><msub id="S3.E6.m1.1.1.1.1.3.3" xref="S3.E6.m1.1.1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.1.1.1.1.3.3.2" xref="S3.E6.m1.1.1.1.1.3.3.2.cmml">‚Ñí</mi><mrow id="S3.E6.m1.1.1.1.1.3.3.3" xref="S3.E6.m1.1.1.1.1.3.3.3.cmml"><mi id="S3.E6.m1.1.1.1.1.3.3.3.2" xref="S3.E6.m1.1.1.1.1.3.3.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.1.3.3.3.1" xref="S3.E6.m1.1.1.1.1.3.3.3.1.cmml">‚Äã</mo><mi id="S3.E6.m1.1.1.1.1.3.3.3.3" xref="S3.E6.m1.1.1.1.1.3.3.3.3.cmml">E</mi></mrow></msub></mrow></mrow><mo lspace="0em" id="S3.E6.m1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"><eq id="S3.E6.m1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1"></eq><apply id="S3.E6.m1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.1.1.1.1.2.2">‚Ñí</ci><apply id="S3.E6.m1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.1.1.1.1.2.3"><times id="S3.E6.m1.1.1.1.1.2.3.1.cmml" xref="S3.E6.m1.1.1.1.1.2.3.1"></times><ci id="S3.E6.m1.1.1.1.1.2.3.2.cmml" xref="S3.E6.m1.1.1.1.1.2.3.2">ùëÜ</ci><ci id="S3.E6.m1.1.1.1.1.2.3.3.cmml" xref="S3.E6.m1.1.1.1.1.2.3.3">ùëá</ci></apply></apply><apply id="S3.E6.m1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.3"><plus id="S3.E6.m1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.1.1.3.1"></plus><apply id="S3.E6.m1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.3.2.1.cmml" xref="S3.E6.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.3.2.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2.2">‚Ñí</ci><apply id="S3.E6.m1.1.1.1.1.3.2.3.cmml" xref="S3.E6.m1.1.1.1.1.3.2.3"><times id="S3.E6.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.E6.m1.1.1.1.1.3.2.3.1"></times><ci id="S3.E6.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.E6.m1.1.1.1.1.3.2.3.2">ùêæ</ci><ci id="S3.E6.m1.1.1.1.1.3.2.3.3.cmml" xref="S3.E6.m1.1.1.1.1.3.2.3.3">ùêø</ci></apply></apply><apply id="S3.E6.m1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.3.3.1.cmml" xref="S3.E6.m1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.3.3.2.cmml" xref="S3.E6.m1.1.1.1.1.3.3.2">‚Ñí</ci><apply id="S3.E6.m1.1.1.1.1.3.3.3.cmml" xref="S3.E6.m1.1.1.1.1.3.3.3"><times id="S3.E6.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E6.m1.1.1.1.1.3.3.3.1"></times><ci id="S3.E6.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E6.m1.1.1.1.1.3.3.3.2">ùê∂</ci><ci id="S3.E6.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E6.m1.1.1.1.1.3.3.3.3">ùê∏</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">\displaystyle\mathcal{L}_{ST}=\mathcal{L}_{KL}+\mathcal{L}_{CE}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p9" class="ltx_para">
<p id="S3.SS1.p9.1" class="ltx_p">For training <span id="S3.SS1.p9.1.1" class="ltx_text ltx_font_italic">CharacterBERT-DR+ST</span>, we follow the original work by <cite class="ltx_cite ltx_citemacro_citet">Zhuang and Zuccon (<a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Datasets and Evaluation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.12" class="ltx_p">We conduct our experiments on the spoken versions of MSMARCO passage ranking <cite class="ltx_cite ltx_citemacro_cite">Nguyen et¬†al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite>, and Natural Questions <cite class="ltx_cite ltx_citemacro_cite">Kwiatkowski et¬†al. (<a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite> introduced by <cite class="ltx_cite ltx_citemacro_citet">Sidiropoulos et¬†al. (<a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>. In the Spoken-MSMARCO and Spoken-NQ, the question is in spoken form, while the passage is in the form of text. For the former dataset, the underlying corpus consists of <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="8.8" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mn id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">8.8</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><cn type="float" id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">8.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">8.8</annotation></semantics></math> million passages, <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="~{}400K" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mn id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">400</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">‚Äã</mo><mi id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><times id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">400</cn><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">ùêæ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">~{}400K</annotation></semantics></math> training samples, and <math id="S3.SS2.p1.3.m3.2" class="ltx_Math" alttext="6,980" display="inline"><semantics id="S3.SS2.p1.3.m3.2a"><mrow id="S3.SS2.p1.3.m3.2.3.2" xref="S3.SS2.p1.3.m3.2.3.1.cmml"><mn id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">6</mn><mo id="S3.SS2.p1.3.m3.2.3.2.1" xref="S3.SS2.p1.3.m3.2.3.1.cmml">,</mo><mn id="S3.SS2.p1.3.m3.2.2" xref="S3.SS2.p1.3.m3.2.2.cmml">980</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.2b"><list id="S3.SS2.p1.3.m3.2.3.1.cmml" xref="S3.SS2.p1.3.m3.2.3.2"><cn type="integer" id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">6</cn><cn type="integer" id="S3.SS2.p1.3.m3.2.2.cmml" xref="S3.SS2.p1.3.m3.2.2">980</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.2c">6,980</annotation></semantics></math> development samples, with an average sample duration of <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="3sec" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mrow id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mn id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p1.4.m4.1.1.1" xref="S3.SS2.p1.4.m4.1.1.1.cmml">‚Äã</mo><mi id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.4.m4.1.1.1a" xref="S3.SS2.p1.4.m4.1.1.1.cmml">‚Äã</mo><mi id="S3.SS2.p1.4.m4.1.1.4" xref="S3.SS2.p1.4.m4.1.1.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.4.m4.1.1.1b" xref="S3.SS2.p1.4.m4.1.1.1.cmml">‚Äã</mo><mi id="S3.SS2.p1.4.m4.1.1.5" xref="S3.SS2.p1.4.m4.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><times id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1"></times><cn type="integer" id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">3</cn><ci id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">ùë†</ci><ci id="S3.SS2.p1.4.m4.1.1.4.cmml" xref="S3.SS2.p1.4.m4.1.1.4">ùëí</ci><ci id="S3.SS2.p1.4.m4.1.1.5.cmml" xref="S3.SS2.p1.4.m4.1.1.5">ùëê</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">3sec</annotation></semantics></math> (<math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="{\sim}6" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mrow id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml"></mi><mo id="S3.SS2.p1.5.m5.1.1.1" xref="S3.SS2.p1.5.m5.1.1.1.cmml">‚àº</mo><mn id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><csymbol cd="latexml" id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2">absent</csymbol><cn type="integer" id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">{\sim}6</annotation></semantics></math> words). Spoken-NQ facilitates <math id="S3.SS2.p1.6.m6.2" class="ltx_Math" alttext="58,880" display="inline"><semantics id="S3.SS2.p1.6.m6.2a"><mrow id="S3.SS2.p1.6.m6.2.3.2" xref="S3.SS2.p1.6.m6.2.3.1.cmml"><mn id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">58</mn><mo id="S3.SS2.p1.6.m6.2.3.2.1" xref="S3.SS2.p1.6.m6.2.3.1.cmml">,</mo><mn id="S3.SS2.p1.6.m6.2.2" xref="S3.SS2.p1.6.m6.2.2.cmml">880</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.2b"><list id="S3.SS2.p1.6.m6.2.3.1.cmml" xref="S3.SS2.p1.6.m6.2.3.2"><cn type="integer" id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">58</cn><cn type="integer" id="S3.SS2.p1.6.m6.2.2.cmml" xref="S3.SS2.p1.6.m6.2.2">880</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.2c">58,880</annotation></semantics></math> training, <math id="S3.SS2.p1.7.m7.2" class="ltx_Math" alttext="6,515" display="inline"><semantics id="S3.SS2.p1.7.m7.2a"><mrow id="S3.SS2.p1.7.m7.2.3.2" xref="S3.SS2.p1.7.m7.2.3.1.cmml"><mn id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">6</mn><mo id="S3.SS2.p1.7.m7.2.3.2.1" xref="S3.SS2.p1.7.m7.2.3.1.cmml">,</mo><mn id="S3.SS2.p1.7.m7.2.2" xref="S3.SS2.p1.7.m7.2.2.cmml">515</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.2b"><list id="S3.SS2.p1.7.m7.2.3.1.cmml" xref="S3.SS2.p1.7.m7.2.3.2"><cn type="integer" id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">6</cn><cn type="integer" id="S3.SS2.p1.7.m7.2.2.cmml" xref="S3.SS2.p1.7.m7.2.2">515</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.2c">6,515</annotation></semantics></math> development, and <math id="S3.SS2.p1.8.m8.2" class="ltx_Math" alttext="3,610" display="inline"><semantics id="S3.SS2.p1.8.m8.2a"><mrow id="S3.SS2.p1.8.m8.2.3.2" xref="S3.SS2.p1.8.m8.2.3.1.cmml"><mn id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml">3</mn><mo id="S3.SS2.p1.8.m8.2.3.2.1" xref="S3.SS2.p1.8.m8.2.3.1.cmml">,</mo><mn id="S3.SS2.p1.8.m8.2.2" xref="S3.SS2.p1.8.m8.2.2.cmml">610</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.2b"><list id="S3.SS2.p1.8.m8.2.3.1.cmml" xref="S3.SS2.p1.8.m8.2.3.2"><cn type="integer" id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">3</cn><cn type="integer" id="S3.SS2.p1.8.m8.2.2.cmml" xref="S3.SS2.p1.8.m8.2.2">610</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.2c">3,610</annotation></semantics></math> test samples with an average sample duration of <math id="S3.SS2.p1.9.m9.1" class="ltx_Math" alttext="3.86sec" display="inline"><semantics id="S3.SS2.p1.9.m9.1a"><mrow id="S3.SS2.p1.9.m9.1.1" xref="S3.SS2.p1.9.m9.1.1.cmml"><mn id="S3.SS2.p1.9.m9.1.1.2" xref="S3.SS2.p1.9.m9.1.1.2.cmml">3.86</mn><mo lspace="0em" rspace="0em" id="S3.SS2.p1.9.m9.1.1.1" xref="S3.SS2.p1.9.m9.1.1.1.cmml">‚Äã</mo><mi id="S3.SS2.p1.9.m9.1.1.3" xref="S3.SS2.p1.9.m9.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.9.m9.1.1.1a" xref="S3.SS2.p1.9.m9.1.1.1.cmml">‚Äã</mo><mi id="S3.SS2.p1.9.m9.1.1.4" xref="S3.SS2.p1.9.m9.1.1.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.9.m9.1.1.1b" xref="S3.SS2.p1.9.m9.1.1.1.cmml">‚Äã</mo><mi id="S3.SS2.p1.9.m9.1.1.5" xref="S3.SS2.p1.9.m9.1.1.5.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m9.1b"><apply id="S3.SS2.p1.9.m9.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1"><times id="S3.SS2.p1.9.m9.1.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1.1"></times><cn type="float" id="S3.SS2.p1.9.m9.1.1.2.cmml" xref="S3.SS2.p1.9.m9.1.1.2">3.86</cn><ci id="S3.SS2.p1.9.m9.1.1.3.cmml" xref="S3.SS2.p1.9.m9.1.1.3">ùë†</ci><ci id="S3.SS2.p1.9.m9.1.1.4.cmml" xref="S3.SS2.p1.9.m9.1.1.4">ùëí</ci><ci id="S3.SS2.p1.9.m9.1.1.5.cmml" xref="S3.SS2.p1.9.m9.1.1.5">ùëê</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m9.1c">3.86sec</annotation></semantics></math> (<math id="S3.SS2.p1.10.m10.1" class="ltx_Math" alttext="{\sim}9" display="inline"><semantics id="S3.SS2.p1.10.m10.1a"><mrow id="S3.SS2.p1.10.m10.1.1" xref="S3.SS2.p1.10.m10.1.1.cmml"><mi id="S3.SS2.p1.10.m10.1.1.2" xref="S3.SS2.p1.10.m10.1.1.2.cmml"></mi><mo id="S3.SS2.p1.10.m10.1.1.1" xref="S3.SS2.p1.10.m10.1.1.1.cmml">‚àº</mo><mn id="S3.SS2.p1.10.m10.1.1.3" xref="S3.SS2.p1.10.m10.1.1.3.cmml">9</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m10.1b"><apply id="S3.SS2.p1.10.m10.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1"><csymbol cd="latexml" id="S3.SS2.p1.10.m10.1.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.SS2.p1.10.m10.1.1.2.cmml" xref="S3.SS2.p1.10.m10.1.1.2">absent</csymbol><cn type="integer" id="S3.SS2.p1.10.m10.1.1.3.cmml" xref="S3.SS2.p1.10.m10.1.1.3">9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m10.1c">{\sim}9</annotation></semantics></math> words). Questions can be answered over Wikipedia. Concerning the pipeline approaches that employ ASR for the transcription of the spoken questions, the WER for Spoken-NQ (test) is <math id="S3.SS2.p1.11.m11.1" class="ltx_Math" alttext="20.10\%" display="inline"><semantics id="S3.SS2.p1.11.m11.1a"><mrow id="S3.SS2.p1.11.m11.1.1" xref="S3.SS2.p1.11.m11.1.1.cmml"><mn id="S3.SS2.p1.11.m11.1.1.2" xref="S3.SS2.p1.11.m11.1.1.2.cmml">20.10</mn><mo id="S3.SS2.p1.11.m11.1.1.1" xref="S3.SS2.p1.11.m11.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.11.m11.1b"><apply id="S3.SS2.p1.11.m11.1.1.cmml" xref="S3.SS2.p1.11.m11.1.1"><csymbol cd="latexml" id="S3.SS2.p1.11.m11.1.1.1.cmml" xref="S3.SS2.p1.11.m11.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.p1.11.m11.1.1.2.cmml" xref="S3.SS2.p1.11.m11.1.1.2">20.10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.11.m11.1c">20.10\%</annotation></semantics></math> and for Spoken-MSMARCO (dev) is <math id="S3.SS2.p1.12.m12.1" class="ltx_Math" alttext="32.87\%" display="inline"><semantics id="S3.SS2.p1.12.m12.1a"><mrow id="S3.SS2.p1.12.m12.1.1" xref="S3.SS2.p1.12.m12.1.1.cmml"><mn id="S3.SS2.p1.12.m12.1.1.2" xref="S3.SS2.p1.12.m12.1.1.2.cmml">32.87</mn><mo id="S3.SS2.p1.12.m12.1.1.1" xref="S3.SS2.p1.12.m12.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.12.m12.1b"><apply id="S3.SS2.p1.12.m12.1.1.cmml" xref="S3.SS2.p1.12.m12.1.1"><csymbol cd="latexml" id="S3.SS2.p1.12.m12.1.1.1.cmml" xref="S3.SS2.p1.12.m12.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.p1.12.m12.1.1.2.cmml" xref="S3.SS2.p1.12.m12.1.1.2">32.87</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.12.m12.1c">32.87\%</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.9" class="ltx_p">To measure the retrieval effectiveness of the models on Spoken-MSMARCO, we use the official metric of the original MSMARCO, namely, Mean Reciprocal Rank (<math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="MRR@10" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.1a" xref="S3.SS2.p2.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.1.m1.1.1.4" xref="S3.SS2.p2.1.m1.1.1.4.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.1b" xref="S3.SS2.p2.1.m1.1.1.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S3.SS2.p2.1.m1.1.1.5" xref="S3.SS2.p2.1.m1.1.1.5.cmml">@</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.1c" xref="S3.SS2.p2.1.m1.1.1.1.cmml">‚Äã</mo><mn id="S3.SS2.p2.1.m1.1.1.6" xref="S3.SS2.p2.1.m1.1.1.6.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><times id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></times><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">ùëÄ</ci><ci id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">ùëÖ</ci><ci id="S3.SS2.p2.1.m1.1.1.4.cmml" xref="S3.SS2.p2.1.m1.1.1.4">ùëÖ</ci><ci id="S3.SS2.p2.1.m1.1.1.5.cmml" xref="S3.SS2.p2.1.m1.1.1.5">@</ci><cn type="integer" id="S3.SS2.p2.1.m1.1.1.6.cmml" xref="S3.SS2.p2.1.m1.1.1.6">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">MRR@10</annotation></semantics></math>) and the commonly reported Recall (<math id="S3.SS2.p2.2.m2.2" class="ltx_Math" alttext="R@50,R@1000" display="inline"><semantics id="S3.SS2.p2.2.m2.2a"><mrow id="S3.SS2.p2.2.m2.2.2.2" xref="S3.SS2.p2.2.m2.2.2.3.cmml"><mrow id="S3.SS2.p2.2.m2.1.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.1.1.2" xref="S3.SS2.p2.2.m2.1.1.1.1.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.1.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S3.SS2.p2.2.m2.1.1.1.1.3" xref="S3.SS2.p2.2.m2.1.1.1.1.3.cmml">@</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.1.1.1a" xref="S3.SS2.p2.2.m2.1.1.1.1.1.cmml">‚Äã</mo><mn id="S3.SS2.p2.2.m2.1.1.1.1.4" xref="S3.SS2.p2.2.m2.1.1.1.1.4.cmml">50</mn></mrow><mo id="S3.SS2.p2.2.m2.2.2.2.3" xref="S3.SS2.p2.2.m2.2.2.3.cmml">,</mo><mrow id="S3.SS2.p2.2.m2.2.2.2.2" xref="S3.SS2.p2.2.m2.2.2.2.2.cmml"><mi id="S3.SS2.p2.2.m2.2.2.2.2.2" xref="S3.SS2.p2.2.m2.2.2.2.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.2.2.2.2.1" xref="S3.SS2.p2.2.m2.2.2.2.2.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S3.SS2.p2.2.m2.2.2.2.2.3" xref="S3.SS2.p2.2.m2.2.2.2.2.3.cmml">@</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.2.2.2.2.1a" xref="S3.SS2.p2.2.m2.2.2.2.2.1.cmml">‚Äã</mo><mn id="S3.SS2.p2.2.m2.2.2.2.2.4" xref="S3.SS2.p2.2.m2.2.2.2.2.4.cmml">1000</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.2b"><list id="S3.SS2.p2.2.m2.2.2.3.cmml" xref="S3.SS2.p2.2.m2.2.2.2"><apply id="S3.SS2.p2.2.m2.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1"><times id="S3.SS2.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1"></times><ci id="S3.SS2.p2.2.m2.1.1.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.2">ùëÖ</ci><ci id="S3.SS2.p2.2.m2.1.1.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.3">@</ci><cn type="integer" id="S3.SS2.p2.2.m2.1.1.1.1.4.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.4">50</cn></apply><apply id="S3.SS2.p2.2.m2.2.2.2.2.cmml" xref="S3.SS2.p2.2.m2.2.2.2.2"><times id="S3.SS2.p2.2.m2.2.2.2.2.1.cmml" xref="S3.SS2.p2.2.m2.2.2.2.2.1"></times><ci id="S3.SS2.p2.2.m2.2.2.2.2.2.cmml" xref="S3.SS2.p2.2.m2.2.2.2.2.2">ùëÖ</ci><ci id="S3.SS2.p2.2.m2.2.2.2.2.3.cmml" xref="S3.SS2.p2.2.m2.2.2.2.2.3">@</ci><cn type="integer" id="S3.SS2.p2.2.m2.2.2.2.2.4.cmml" xref="S3.SS2.p2.2.m2.2.2.2.2.4">1000</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.2c">R@50,R@1000</annotation></semantics></math>). Similar to the original MSMARCO, we report the metrics on the development set, since the ground-truths for the test set are not publicly available. Following previous works on NQ and Spoken-NQ, we report we use answer recall at the <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="top" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.3.m3.1.1.1a" xref="S3.SS2.p2.3.m3.1.1.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.3.m3.1.1.4" xref="S3.SS2.p2.3.m3.1.1.4.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><times id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></times><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">ùë°</ci><ci id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">ùëú</ci><ci id="S3.SS2.p2.3.m3.1.1.4.cmml" xref="S3.SS2.p2.3.m3.1.1.4">ùëù</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">top</annotation></semantics></math>-<math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">k</annotation></semantics></math> (<math id="S3.SS2.p2.5.m5.2" class="ltx_Math" alttext="AR@20,AR@100" display="inline"><semantics id="S3.SS2.p2.5.m5.2a"><mrow id="S3.SS2.p2.5.m5.2.2.2" xref="S3.SS2.p2.5.m5.2.2.3.cmml"><mrow id="S3.SS2.p2.5.m5.1.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.1.1.2" xref="S3.SS2.p2.5.m5.1.1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.5.m5.1.1.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.1.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.5.m5.1.1.1.1.3" xref="S3.SS2.p2.5.m5.1.1.1.1.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.5.m5.1.1.1.1.1a" xref="S3.SS2.p2.5.m5.1.1.1.1.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S3.SS2.p2.5.m5.1.1.1.1.4" xref="S3.SS2.p2.5.m5.1.1.1.1.4.cmml">@</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.5.m5.1.1.1.1.1b" xref="S3.SS2.p2.5.m5.1.1.1.1.1.cmml">‚Äã</mo><mn id="S3.SS2.p2.5.m5.1.1.1.1.5" xref="S3.SS2.p2.5.m5.1.1.1.1.5.cmml">20</mn></mrow><mo id="S3.SS2.p2.5.m5.2.2.2.3" xref="S3.SS2.p2.5.m5.2.2.3.cmml">,</mo><mrow id="S3.SS2.p2.5.m5.2.2.2.2" xref="S3.SS2.p2.5.m5.2.2.2.2.cmml"><mi id="S3.SS2.p2.5.m5.2.2.2.2.2" xref="S3.SS2.p2.5.m5.2.2.2.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.5.m5.2.2.2.2.1" xref="S3.SS2.p2.5.m5.2.2.2.2.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.5.m5.2.2.2.2.3" xref="S3.SS2.p2.5.m5.2.2.2.2.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.5.m5.2.2.2.2.1a" xref="S3.SS2.p2.5.m5.2.2.2.2.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S3.SS2.p2.5.m5.2.2.2.2.4" xref="S3.SS2.p2.5.m5.2.2.2.2.4.cmml">@</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.5.m5.2.2.2.2.1b" xref="S3.SS2.p2.5.m5.2.2.2.2.1.cmml">‚Äã</mo><mn id="S3.SS2.p2.5.m5.2.2.2.2.5" xref="S3.SS2.p2.5.m5.2.2.2.2.5.cmml">100</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.2b"><list id="S3.SS2.p2.5.m5.2.2.3.cmml" xref="S3.SS2.p2.5.m5.2.2.2"><apply id="S3.SS2.p2.5.m5.1.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1"><times id="S3.SS2.p2.5.m5.1.1.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1.1"></times><ci id="S3.SS2.p2.5.m5.1.1.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1.2">ùê¥</ci><ci id="S3.SS2.p2.5.m5.1.1.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1.3">ùëÖ</ci><ci id="S3.SS2.p2.5.m5.1.1.1.1.4.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1.4">@</ci><cn type="integer" id="S3.SS2.p2.5.m5.1.1.1.1.5.cmml" xref="S3.SS2.p2.5.m5.1.1.1.1.5">20</cn></apply><apply id="S3.SS2.p2.5.m5.2.2.2.2.cmml" xref="S3.SS2.p2.5.m5.2.2.2.2"><times id="S3.SS2.p2.5.m5.2.2.2.2.1.cmml" xref="S3.SS2.p2.5.m5.2.2.2.2.1"></times><ci id="S3.SS2.p2.5.m5.2.2.2.2.2.cmml" xref="S3.SS2.p2.5.m5.2.2.2.2.2">ùê¥</ci><ci id="S3.SS2.p2.5.m5.2.2.2.2.3.cmml" xref="S3.SS2.p2.5.m5.2.2.2.2.3">ùëÖ</ci><ci id="S3.SS2.p2.5.m5.2.2.2.2.4.cmml" xref="S3.SS2.p2.5.m5.2.2.2.2.4">@</ci><cn type="integer" id="S3.SS2.p2.5.m5.2.2.2.2.5.cmml" xref="S3.SS2.p2.5.m5.2.2.2.2.5">100</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.2c">AR@20,AR@100</annotation></semantics></math>) retrieved passages. Answer recall measures whether at least one of the <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="top" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><mrow id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.6.m6.1.1.1" xref="S3.SS2.p2.6.m6.1.1.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.6.m6.1.1.1a" xref="S3.SS2.p2.6.m6.1.1.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.6.m6.1.1.4" xref="S3.SS2.p2.6.m6.1.1.4.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><times id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1.1"></times><ci id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2">ùë°</ci><ci id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">ùëú</ci><ci id="S3.SS2.p2.6.m6.1.1.4.cmml" xref="S3.SS2.p2.6.m6.1.1.4">ùëù</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">top</annotation></semantics></math>-<math id="S3.SS2.p2.7.m7.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS2.p2.7.m7.1a"><mi id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><ci id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">k</annotation></semantics></math> retrieved passages contains the ground-truth answer string, then <math id="S3.SS2.p2.8.m8.1" class="ltx_Math" alttext="AR@k=1" display="inline"><semantics id="S3.SS2.p2.8.m8.1a"><mrow id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml"><mrow id="S3.SS2.p2.8.m8.1.1.2" xref="S3.SS2.p2.8.m8.1.1.2.cmml"><mi id="S3.SS2.p2.8.m8.1.1.2.2" xref="S3.SS2.p2.8.m8.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m8.1.1.2.1" xref="S3.SS2.p2.8.m8.1.1.2.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.8.m8.1.1.2.3" xref="S3.SS2.p2.8.m8.1.1.2.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m8.1.1.2.1a" xref="S3.SS2.p2.8.m8.1.1.2.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S3.SS2.p2.8.m8.1.1.2.4" xref="S3.SS2.p2.8.m8.1.1.2.4.cmml">@</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m8.1.1.2.1b" xref="S3.SS2.p2.8.m8.1.1.2.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.8.m8.1.1.2.5" xref="S3.SS2.p2.8.m8.1.1.2.5.cmml">k</mi></mrow><mo id="S3.SS2.p2.8.m8.1.1.1" xref="S3.SS2.p2.8.m8.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.8.m8.1.1.3" xref="S3.SS2.p2.8.m8.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><apply id="S3.SS2.p2.8.m8.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1"><eq id="S3.SS2.p2.8.m8.1.1.1.cmml" xref="S3.SS2.p2.8.m8.1.1.1"></eq><apply id="S3.SS2.p2.8.m8.1.1.2.cmml" xref="S3.SS2.p2.8.m8.1.1.2"><times id="S3.SS2.p2.8.m8.1.1.2.1.cmml" xref="S3.SS2.p2.8.m8.1.1.2.1"></times><ci id="S3.SS2.p2.8.m8.1.1.2.2.cmml" xref="S3.SS2.p2.8.m8.1.1.2.2">ùê¥</ci><ci id="S3.SS2.p2.8.m8.1.1.2.3.cmml" xref="S3.SS2.p2.8.m8.1.1.2.3">ùëÖ</ci><ci id="S3.SS2.p2.8.m8.1.1.2.4.cmml" xref="S3.SS2.p2.8.m8.1.1.2.4">@</ci><ci id="S3.SS2.p2.8.m8.1.1.2.5.cmml" xref="S3.SS2.p2.8.m8.1.1.2.5">ùëò</ci></apply><cn type="integer" id="S3.SS2.p2.8.m8.1.1.3.cmml" xref="S3.SS2.p2.8.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">AR@k=1</annotation></semantics></math> otherwise <math id="S3.SS2.p2.9.m9.1" class="ltx_Math" alttext="AR@k=0" display="inline"><semantics id="S3.SS2.p2.9.m9.1a"><mrow id="S3.SS2.p2.9.m9.1.1" xref="S3.SS2.p2.9.m9.1.1.cmml"><mrow id="S3.SS2.p2.9.m9.1.1.2" xref="S3.SS2.p2.9.m9.1.1.2.cmml"><mi id="S3.SS2.p2.9.m9.1.1.2.2" xref="S3.SS2.p2.9.m9.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.9.m9.1.1.2.1" xref="S3.SS2.p2.9.m9.1.1.2.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.9.m9.1.1.2.3" xref="S3.SS2.p2.9.m9.1.1.2.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.9.m9.1.1.2.1a" xref="S3.SS2.p2.9.m9.1.1.2.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S3.SS2.p2.9.m9.1.1.2.4" xref="S3.SS2.p2.9.m9.1.1.2.4.cmml">@</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.9.m9.1.1.2.1b" xref="S3.SS2.p2.9.m9.1.1.2.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.9.m9.1.1.2.5" xref="S3.SS2.p2.9.m9.1.1.2.5.cmml">k</mi></mrow><mo id="S3.SS2.p2.9.m9.1.1.1" xref="S3.SS2.p2.9.m9.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.9.m9.1.1.3" xref="S3.SS2.p2.9.m9.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m9.1b"><apply id="S3.SS2.p2.9.m9.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1"><eq id="S3.SS2.p2.9.m9.1.1.1.cmml" xref="S3.SS2.p2.9.m9.1.1.1"></eq><apply id="S3.SS2.p2.9.m9.1.1.2.cmml" xref="S3.SS2.p2.9.m9.1.1.2"><times id="S3.SS2.p2.9.m9.1.1.2.1.cmml" xref="S3.SS2.p2.9.m9.1.1.2.1"></times><ci id="S3.SS2.p2.9.m9.1.1.2.2.cmml" xref="S3.SS2.p2.9.m9.1.1.2.2">ùê¥</ci><ci id="S3.SS2.p2.9.m9.1.1.2.3.cmml" xref="S3.SS2.p2.9.m9.1.1.2.3">ùëÖ</ci><ci id="S3.SS2.p2.9.m9.1.1.2.4.cmml" xref="S3.SS2.p2.9.m9.1.1.2.4">@</ci><ci id="S3.SS2.p2.9.m9.1.1.2.5.cmml" xref="S3.SS2.p2.9.m9.1.1.2.5">ùëò</ci></apply><cn type="integer" id="S3.SS2.p2.9.m9.1.1.3.cmml" xref="S3.SS2.p2.9.m9.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m9.1c">AR@k=0</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Implementation Details</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.5" class="ltx_p">We train our multimodal dense retriever using the in-batch negative setting described in <cite class="ltx_cite ltx_citemacro_cite">Karpukhin et¬†al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite>; with one hard negative passage per question and a batch size of 64, the largest batch we could fit in our GPU setup. The question and the passage encoders are implemented by <span id="S3.SS3.p1.5.1" class="ltx_text ltx_font_italic">HuBERT-Base</span> <cite class="ltx_cite ltx_citemacro_cite">Hsu et¬†al. (<a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite> and <span id="S3.SS3.p1.5.2" class="ltx_text ltx_font_italic">BERT-Base</span> <cite class="ltx_cite ltx_citemacro_cite">Devlin et¬†al. (<a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite> networks, respectively. We take the first embedding from the two output representations of each sequence to represent their corresponding speech and text sequences. Additionally, our experimental results showed that having different learning rates for the question and passage encoders leads to more effective training. Specifically, we set the learning rate to <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="2e" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mn id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><times id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">2</cn><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">ùëí</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">2e</annotation></semantics></math>-<math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mn id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><cn type="integer" id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">4</annotation></semantics></math> for the question encoder and <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="2e" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mrow id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mn id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p1.3.m3.1.1.1" xref="S3.SS3.p1.3.m3.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><times id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1"></times><cn type="integer" id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">2</cn><ci id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3">ùëí</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">2e</annotation></semantics></math>-<math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mn id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><cn type="integer" id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">5</annotation></semantics></math> for the passage encoder. To this end, we train with the Adam optimizer and linear scheduling with <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><mn id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><cn type="float" id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">0.1</annotation></semantics></math> warm-up for up to 80 epochs for the small Spoken-NQ dataset and 10 for the larger Spoken-MSMARCO dataset.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.4" class="ltx_p">To end up with the abovementioned parameters, we searched learning rates ranging from <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="2e" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mn id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><times id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">2</cn><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">ùëí</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">2e</annotation></semantics></math>-<math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mn id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><cn type="integer" id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">6</annotation></semantics></math> to <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="2e" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mrow id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mn id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S3.SS3.p2.3.m3.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.cmml">‚Äã</mo><mi id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><times id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1"></times><cn type="integer" id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2">2</cn><ci id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3">ùëí</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">2e</annotation></semantics></math>-<math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mn id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><cn type="integer" id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">4</annotation></semantics></math> (for cases where question and passage encoders share the same learning rate or have different ones) and explored <span id="S3.SS3.p2.4.1" class="ltx_text ltx_font_italic">HuBERT-Base</span> and <span id="S3.SS3.p2.4.2" class="ltx_text ltx_font_italic">Wav2Vec2-Base</span> question encoders. We also experiment with warming up the question embedding space before training our retriever, following the sequence-level alignment approach described in <cite class="ltx_cite ltx_citemacro_citet">Chung et¬†al. (<a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite>. However, we did not see any improvements. We chose the best hyperparameters with respect to the best average rank in the development split <cite class="ltx_cite ltx_citemacro_cite">Karpukhin et¬†al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results &amp; Discussion</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Main Results</h3>

<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:104pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-45.8pt,11.0pt) scale(0.825504739916095,0.825504739916095) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" rowspan="2"></th>
<th id="S4.T1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T1.1.1.1.1.2.1" class="ltx_text">Training questions</span></th>
<th id="S4.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T1.1.1.1.1.3.1" class="ltx_text">End-to-end</span></th>
<th id="S4.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Spoken-NQ (test)</th>
<th id="S4.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Spoken-MSMARCO (dev)</th>
</tr>
<tr id="S4.T1.1.1.2.2" class="ltx_tr">
<th id="S4.T1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">AR@20</th>
<th id="S4.T1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">AR@100</th>
<th id="S4.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">R@50</th>
<th id="S4.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">R@1000</th>
<th id="S4.T1.1.1.2.2.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column">MRR@10</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.3.1" class="ltx_tr">
<td id="S4.T1.1.1.3.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">BM25</td>
<td id="S4.T1.1.1.3.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">-</td>
<td id="S4.T1.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S4.T1.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">36.98</td>
<td id="S4.T1.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">52.49</td>
<td id="S4.T1.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">24.71</td>
<td id="S4.T1.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">45.34</td>
<td id="S4.T1.1.1.3.1.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">6.97</td>
</tr>
<tr id="S4.T1.1.1.4.2" class="ltx_tr">
<td id="S4.T1.1.1.4.2.1" class="ltx_td ltx_align_left ltx_border_r">DR</td>
<td id="S4.T1.1.1.4.2.2" class="ltx_td ltx_align_left ltx_border_r">Transcriptions</td>
<td id="S4.T1.1.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r">‚úó</td>
<td id="S4.T1.1.1.4.2.4" class="ltx_td ltx_align_center">68.36</td>
<td id="S4.T1.1.1.4.2.5" class="ltx_td ltx_align_center ltx_border_r">78.53</td>
<td id="S4.T1.1.1.4.2.6" class="ltx_td ltx_align_center">52.06</td>
<td id="S4.T1.1.1.4.2.7" class="ltx_td ltx_align_center">79.35</td>
<td id="S4.T1.1.1.4.2.8" class="ltx_td ltx_nopad_r ltx_align_center">17.74</td>
</tr>
<tr id="S4.T1.1.1.5.3" class="ltx_tr">
<td id="S4.T1.1.1.5.3.1" class="ltx_td ltx_align_left ltx_border_r">DR+Augm.</td>
<td id="S4.T1.1.1.5.3.2" class="ltx_td ltx_align_left ltx_border_r">Transcriptions &amp; Typos</td>
<td id="S4.T1.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r">‚úó</td>
<td id="S4.T1.1.1.5.3.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.5.3.4.1" class="ltx_text ltx_font_bold">69.77</span></td>
<td id="S4.T1.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_r">80.05</td>
<td id="S4.T1.1.1.5.3.6" class="ltx_td ltx_align_center">52.97</td>
<td id="S4.T1.1.1.5.3.7" class="ltx_td ltx_align_center">80.56</td>
<td id="S4.T1.1.1.5.3.8" class="ltx_td ltx_nopad_r ltx_align_center">17.87</td>
</tr>
<tr id="S4.T1.1.1.6.4" class="ltx_tr">
<td id="S4.T1.1.1.6.4.1" class="ltx_td ltx_align_left ltx_border_r">CharacterBERT-DR+ST</td>
<td id="S4.T1.1.1.6.4.2" class="ltx_td ltx_align_left ltx_border_r">Transcriptions &amp; Typos</td>
<td id="S4.T1.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_r">‚úó</td>
<td id="S4.T1.1.1.6.4.4" class="ltx_td ltx_align_center">68.22</td>
<td id="S4.T1.1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.1.6.4.5.1" class="ltx_text ltx_font_bold">80.3</span></td>
<td id="S4.T1.1.1.6.4.6" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.6.4.6.1" class="ltx_text ltx_font_bold">53.88</span></td>
<td id="S4.T1.1.1.6.4.7" class="ltx_td ltx_align_center">78.95</td>
<td id="S4.T1.1.1.6.4.8" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T1.1.1.6.4.8.1" class="ltx_text ltx_font_bold">20.53</span></td>
</tr>
<tr id="S4.T1.1.1.7.5" class="ltx_tr">
<td id="S4.T1.1.1.7.5.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">Multimodal DR (Ours)</td>
<td id="S4.T1.1.1.7.5.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">Speech</td>
<td id="S4.T1.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">‚úì</td>
<td id="S4.T1.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">57.25</td>
<td id="S4.T1.1.1.7.5.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">70.11</td>
<td id="S4.T1.1.1.7.5.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">51.37</td>
<td id="S4.T1.1.1.7.5.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.1.1.7.5.7.1" class="ltx_text ltx_font_bold">81.34</span></td>
<td id="S4.T1.1.1.7.5.8" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t">15.77</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Retrieval results on Spoken-NQ and Spoken-MSMARCO. End-to-end ‚Äú‚úó‚Äù accounts for <span id="S4.T1.3.1" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipeline approaches that can not be trained in an end-to-end manner.</figcaption>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To answer <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">RQ1</span>, we compare the retrieval performance of our multimodal dense retriever against the <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipelines we described in Section <a href="#S3.SS1" title="3.1 Baselines and Implementation ‚Ä£ 3 Experimental Setup ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. From Table <a href="#S4.T1" title="Table 1 ‚Ä£ 4.1 Main Results ‚Ä£ 4 Results &amp; Discussion ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> we note that our model is highly competitive on the Spoken-MSMARCO dataset, while the pipeline approaches perform significantly better on the Spoken-NQ dataset. This discrepancy is twofold. First, our multimodal dense retriever performs better on shorter questions. We conjecture that the low performance of our model on longer questions is due to encoding the spoken question into a single vector which might not be enough to capture the necessary information as the length of the question increases. Second, the higher the word error rate the higher the negative impact on the <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipelines. Spoken-MSMARCO has shorter questions and higher word error rate compared to Spoken-NQ (Section <a href="#S3.SS2" title="3.2 Datasets and Evaluation ‚Ä£ 3 Experimental Setup ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">In Figure <a href="#S4.F2" title="Figure 2 ‚Ä£ 4.1 Main Results ‚Ä£ 4 Results &amp; Discussion ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we verify our aforementioned claims. Figure <a href="#S4.F2" title="Figure 2 ‚Ä£ 4.1 Main Results ‚Ä£ 4 Results &amp; Discussion ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> reports the retrieval performance of the <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipelines concerning the different word error rates of the ASR model and where our ASR-free method stands.
Pipelines show strong results when no corrupted words are in the transcriptions (WER is <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><cn type="integer" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">0</cn></annotation-xml></semantics></math>). However, there is a significant drop in the retrieval performance of all the pipeline approaches as the word error rate increases.
On the other hand, our ASR-free, multimodal dense retriever is significantly more stable across the different settings. At the same time, we see an increase in the performance of our approach as the length of the question decreases (see the average length under each bin in Figure <a href="#S4.F2" title="Figure 2 ‚Ä£ 4.1 Main Results ‚Ä£ 4 Results &amp; Discussion ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
That said, we can conclude that adopting our multimodal dense retriever as the word error rate of the ASR model increases yields better results.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2409.13483/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="256" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Retrieval performance w.r.t the WER of questions; on Spoken-MSMARCO (dev). Each bin with a non-zero WER has <math id="S4.F2.3.m1.1" class="ltx_Math" alttext="{\sim}1300" display="inline"><semantics id="S4.F2.3.m1.1b"><mrow id="S4.F2.3.m1.1.1" xref="S4.F2.3.m1.1.1.cmml"><mi id="S4.F2.3.m1.1.1.2" xref="S4.F2.3.m1.1.1.2.cmml"></mi><mo id="S4.F2.3.m1.1.1.1" xref="S4.F2.3.m1.1.1.1.cmml">‚àº</mo><mn id="S4.F2.3.m1.1.1.3" xref="S4.F2.3.m1.1.1.3.cmml">1300</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.3.m1.1c"><apply id="S4.F2.3.m1.1.1.cmml" xref="S4.F2.3.m1.1.1"><csymbol cd="latexml" id="S4.F2.3.m1.1.1.1.cmml" xref="S4.F2.3.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.F2.3.m1.1.1.2.cmml" xref="S4.F2.3.m1.1.1.2">absent</csymbol><cn type="integer" id="S4.F2.3.m1.1.1.3.cmml" xref="S4.F2.3.m1.1.1.3">1300</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.3.m1.1d">{\sim}1300</annotation></semantics></math> samples, while the one with a zero WER has <math id="S4.F2.4.m2.1" class="ltx_Math" alttext="{\sim}1600" display="inline"><semantics id="S4.F2.4.m2.1b"><mrow id="S4.F2.4.m2.1.1" xref="S4.F2.4.m2.1.1.cmml"><mi id="S4.F2.4.m2.1.1.2" xref="S4.F2.4.m2.1.1.2.cmml"></mi><mo id="S4.F2.4.m2.1.1.1" xref="S4.F2.4.m2.1.1.1.cmml">‚àº</mo><mn id="S4.F2.4.m2.1.1.3" xref="S4.F2.4.m2.1.1.3.cmml">1600</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.4.m2.1c"><apply id="S4.F2.4.m2.1.1.cmml" xref="S4.F2.4.m2.1.1"><csymbol cd="latexml" id="S4.F2.4.m2.1.1.1.cmml" xref="S4.F2.4.m2.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.F2.4.m2.1.1.2.cmml" xref="S4.F2.4.m2.1.1.2">absent</csymbol><cn type="integer" id="S4.F2.4.m2.1.1.3.cmml" xref="S4.F2.4.m2.1.1.3">1600</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.4.m2.1d">{\sim}1600</annotation></semantics></math> samples. We also report the average question length, in tokens, per bin.</figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Despite the strong performance that <span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipelines achieve, there are several limitations we need to highlight. In Table <a href="#S4.T2" title="Table 2 ‚Ä£ 4.1 Main Results ‚Ä£ 4 Results &amp; Discussion ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we compare our model to <span id="S4.SS1.p3.1.2" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipelines in terms of query time, the need for annotated speech data, and parameters. A major constraint for pipelines is the requirement in annotated speech for training an ASR model. In the real world, such data are not always in abundance. Annotated speech can be hard to obtain when dealing with low-resource languages or specialized domains such as the medical domain, where a general-purpose ASR system will underperform. In such scenarios, the applicability of <span id="S4.SS1.p3.1.3" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipelines is limited. In contrast, our approach is ASR-free, hence, does not need annotated speech. Regarding query time, as shown in Table <a href="#S4.T2" title="Table 2 ‚Ä£ 4.1 Main Results ‚Ä£ 4 Results &amp; Discussion ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the query time of our model is substantially shorter than that of the <span id="S4.SS1.p3.1.4" class="ltx_text ltx_font_italic">ASR-Retriever</span>. Passing the spoken question through an ASR model to obtain its transcription introduces additional overhead.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">In our work, we are interested in the cases where ASR generates transcriptions with a higher word error rate; therefore, we conduct extensive analysis focusing on such cases.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.2" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:125.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(51.6pt,-14.9pt) scale(1.31212010048082,1.31212010048082) ;">
<table id="S4.T2.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.2.3" class="ltx_td ltx_border_tt"></td>
<th id="S4.T2.2.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">
<table id="S4.T2.2.2.2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.2.2.2.4.1.1" class="ltx_tr">
<td id="S4.T2.2.2.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Annotated</td>
</tr>
<tr id="S4.T2.2.2.2.4.1.2" class="ltx_tr">
<td id="S4.T2.2.2.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Speech</td>
</tr>
</table>
</th>
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">
<table id="S4.T2.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.1.1.1.2" class="ltx_tr">
<td id="S4.T2.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">ASR</td>
</tr>
<tr id="S4.T2.1.1.1.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">
<math id="S4.T2.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\#" display="inline"><semantics id="S4.T2.1.1.1.1.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T2.1.1.1.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.1.1.1.m1.1.1.cmml">#</mi><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.1.1.m1.1.1">#</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.1.1.m1.1c">\#</annotation></semantics></math>params</td>
</tr>
</table>
</th>
<th id="S4.T2.2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">
<table id="S4.T2.2.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.2.2.2.2.1.2" class="ltx_tr">
<td id="S4.T2.2.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Retriever</td>
</tr>
<tr id="S4.T2.2.2.2.2.1.1" class="ltx_tr">
<td id="S4.T2.2.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">
<math id="S4.T2.2.2.2.2.1.1.1.m1.1" class="ltx_Math" alttext="\#" display="inline"><semantics id="S4.T2.2.2.2.2.1.1.1.m1.1a"><mi mathvariant="normal" id="S4.T2.2.2.2.2.1.1.1.m1.1.1" xref="S4.T2.2.2.2.2.1.1.1.m1.1.1.cmml">#</mi><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.1.1.1.m1.1b"><ci id="S4.T2.2.2.2.2.1.1.1.m1.1.1.cmml" xref="S4.T2.2.2.2.2.1.1.1.m1.1.1">#</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.1.1.1.m1.1c">\#</annotation></semantics></math>params</td>
</tr>
</table>
</th>
<th id="S4.T2.2.2.2.5" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt">Time</th>
</tr>
<tr id="S4.T2.2.2.3.1" class="ltx_tr">
<td id="S4.T2.2.2.3.1.1" class="ltx_td ltx_align_left ltx_border_t">DR+Augm.</td>
<td id="S4.T2.2.2.3.1.2" class="ltx_td ltx_align_left ltx_border_t">960h</td>
<td id="S4.T2.2.2.3.1.3" class="ltx_td ltx_align_left ltx_border_t">95M</td>
<td id="S4.T2.2.2.3.1.4" class="ltx_td ltx_align_left ltx_border_t">220M</td>
<td id="S4.T2.2.2.3.1.5" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">45.3ms</td>
</tr>
<tr id="S4.T2.2.2.4.2" class="ltx_tr">
<td id="S4.T2.2.2.4.2.1" class="ltx_td ltx_align_left">CharacterBert-DR+ST</td>
<td id="S4.T2.2.2.4.2.2" class="ltx_td ltx_align_left">960h</td>
<td id="S4.T2.2.2.4.2.3" class="ltx_td ltx_align_left">95M</td>
<td id="S4.T2.2.2.4.2.4" class="ltx_td ltx_align_left">210M</td>
<td id="S4.T2.2.2.4.2.5" class="ltx_td ltx_nopad_r ltx_align_left">42.5ms</td>
</tr>
<tr id="S4.T2.2.2.5.3" class="ltx_tr">
<td id="S4.T2.2.2.5.3.1" class="ltx_td ltx_align_left ltx_border_bb">Multimodal DR (Ours)</td>
<td id="S4.T2.2.2.5.3.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T2.2.2.5.3.2.1" class="ltx_text ltx_font_bold">-</span></td>
<td id="S4.T2.2.2.5.3.3" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T2.2.2.5.3.3.1" class="ltx_text ltx_font_bold">-</span></td>
<td id="S4.T2.2.2.5.3.4" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T2.2.2.5.3.4.1" class="ltx_text ltx_font_bold">200M</span></td>
<td id="S4.T2.2.2.5.3.5" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb"><span id="S4.T2.2.2.5.3.5.1" class="ltx_text ltx_font_bold">21.9ms</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of our <span id="S4.T2.5.1" class="ltx_text ltx_font_italic">Multimodal DR</span> and <span id="S4.T2.6.2" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipeline w.r.t needs in annotated speech data, the number
of model parameters, and query time.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Analysis</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p">Our multimodal dense retriever is ASR-free. Thus, there are no ASR errors that can propagate to the retriever. On the contrary, in the <span id="S4.SS2.p1.2.1" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipeline, the transcribed question that arrives as input to the retriever will often contain mistranscribed words. Nevertheless, not every word in a question is of equal importance. Let us take as examples the following two transcriptions: ‚Äúwhat channel is young sheldon on‚Äù <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mo stretchy="false" id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\rightarrow</annotation></semantics></math> ‚Äúwhat channel is young shelternon‚Äù and ‚Äúwho took the first steps on the moon in 1969‚Äù <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mo stretchy="false" id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\rightarrow</annotation></semantics></math> ‚Äúhe took the first steps on the moon in nineteen sixty nine‚Äù. Concerning the former, the corruption can lead the retrieval far from the underline entity ‚Äúyoung sheldon‚Äù, while in the latter, the error will have a limited impact on the retrieval. We claim that mistranscribing an important word can hurt retrieval performance more than mistranscribing less important ones. To verify our claim, we explore how the importance of the mistranscribed word impacts the retrieval performance of pipelines and how it compares against our ASR-free multimodal dense retriever (<span id="S4.SS2.p1.2.2" class="ltx_text ltx_font_bold">RQ2</span>).</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">For our experiments, we define the relevant importance of a word in a question as the ratio of its IDF to the sum of the IDFs of every word in the question <cite class="ltx_cite ltx_citemacro_cite">Sidiropoulos and Kanoulas (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite>. Figure <a href="#S4.F3" title="Figure 3 ‚Ä£ 4.2 Analysis ‚Ä£ 4 Results &amp; Discussion ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows that as the importance of the mistranscribed word increases, there is a dramatic drop in the retrieval performance of the <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipelines. At the same time, as more significant words are corrupted due to the failure of the ASR model, following our ASR-free multimodal dense retriever method is a promising alternative.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2409.13483/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="247" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Retrieval results w.r.t the relevant importance of
the mistranscribed words; on Spoken-MSMARCO (dev). For questions with multiple mistranscribed words, we use the word with the highest relevant importance to assign the question to a bin. Bins have <math id="S4.F3.2.m1.1" class="ltx_Math" alttext="{\sim}1000" display="inline"><semantics id="S4.F3.2.m1.1b"><mrow id="S4.F3.2.m1.1.1" xref="S4.F3.2.m1.1.1.cmml"><mi id="S4.F3.2.m1.1.1.2" xref="S4.F3.2.m1.1.1.2.cmml"></mi><mo id="S4.F3.2.m1.1.1.1" xref="S4.F3.2.m1.1.1.1.cmml">‚àº</mo><mn id="S4.F3.2.m1.1.1.3" xref="S4.F3.2.m1.1.1.3.cmml">1000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F3.2.m1.1c"><apply id="S4.F3.2.m1.1.1.cmml" xref="S4.F3.2.m1.1.1"><csymbol cd="latexml" id="S4.F3.2.m1.1.1.1.cmml" xref="S4.F3.2.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S4.F3.2.m1.1.1.2.cmml" xref="S4.F3.2.m1.1.1.2">absent</csymbol><cn type="integer" id="S4.F3.2.m1.1.1.3.cmml" xref="S4.F3.2.m1.1.1.3">1000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.2.m1.1d">{\sim}1000</annotation></semantics></math> samples.</figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.2" class="ltx_p">Our ASR-free multimodal dense retriever is trained on spoken questions. On the other hand, in an <span id="S4.SS2.p3.2.1" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipeline, the retrieval model is trained on ASR transcriptions. As a result, the retriever encounters ASR mistranscription during training, similar to the ones it encounters during inference. However, a mistranscription of a particular word is strongly connected to its context. For instance, we can have the following two mistranscriptions for the word ‚Äúexxon‚Äù: ‚Äúwhen did standard oil of new jersey become exxon‚Äù <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mo stretchy="false" id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\rightarrow</annotation></semantics></math> ‚Äúwhen did standard oil of new jersey become exon‚Äù
and ‚Äúwhere are exxon‚Äôs refineries located‚Äù <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mo stretchy="false" id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">‚Üí</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><ci id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">‚Üí</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">\rightarrow</annotation></semantics></math> ‚Äúwhere a rexons refineries located‚Äù.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:82.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-67.7pt,12.8pt) scale(0.762146994978304,0.762146994978304) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"></th>
<th id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T3.1.1.1.1.2.1" class="ltx_text">Training Questions</span></th>
<th id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T3.1.1.1.1.3.1" class="ltx_text">End-to-end</span></th>
<th id="S4.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="3">Unseen</th>
<th id="S4.T3.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Seen</th>
</tr>
<tr id="S4.T3.1.1.2.2" class="ltx_tr">
<th id="S4.T3.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">MRR@10</th>
<th id="S4.T3.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">R@50</th>
<th id="S4.T3.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">R@1000</th>
<th id="S4.T3.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">MRR@10</th>
<th id="S4.T3.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">R@50</th>
<th id="S4.T3.1.1.2.2.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column">R@1000</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.3.1" class="ltx_tr">
<th id="S4.T3.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">DR</th>
<th id="S4.T3.1.1.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Transcriptions</th>
<th id="S4.T3.1.1.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">‚úó</th>
<td id="S4.T3.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">13.45</td>
<td id="S4.T3.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">40.29</td>
<td id="S4.T3.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">67.29</td>
<td id="S4.T3.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">16.69</td>
<td id="S4.T3.1.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t">50.32</td>
<td id="S4.T3.1.1.3.1.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">79.55</td>
</tr>
<tr id="S4.T3.1.1.4.2" class="ltx_tr">
<th id="S4.T3.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">DR+Augm.</th>
<th id="S4.T3.1.1.4.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Transcriptions &amp; Typos</th>
<th id="S4.T3.1.1.4.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">‚úó</th>
<td id="S4.T3.1.1.4.2.4" class="ltx_td ltx_align_center">13.41</td>
<td id="S4.T3.1.1.4.2.5" class="ltx_td ltx_align_center">41.08</td>
<td id="S4.T3.1.1.4.2.6" class="ltx_td ltx_align_center ltx_border_r">69.63</td>
<td id="S4.T3.1.1.4.2.7" class="ltx_td ltx_align_center">17.18</td>
<td id="S4.T3.1.1.4.2.8" class="ltx_td ltx_align_center">51.75</td>
<td id="S4.T3.1.1.4.2.9" class="ltx_td ltx_nopad_r ltx_align_center">80.83</td>
</tr>
<tr id="S4.T3.1.1.5.3" class="ltx_tr">
<th id="S4.T3.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">CharacterBERT-DR+ST</th>
<th id="S4.T3.1.1.5.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">Transcriptions &amp; Typos</th>
<th id="S4.T3.1.1.5.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">‚úó</th>
<td id="S4.T3.1.1.5.3.4" class="ltx_td ltx_align_center">15.15</td>
<td id="S4.T3.1.1.5.3.5" class="ltx_td ltx_align_center">40.86</td>
<td id="S4.T3.1.1.5.3.6" class="ltx_td ltx_align_center ltx_border_r">67.27</td>
<td id="S4.T3.1.1.5.3.7" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.5.3.7.1" class="ltx_text ltx_font_bold">19.75</span></td>
<td id="S4.T3.1.1.5.3.8" class="ltx_td ltx_align_center"><span id="S4.T3.1.1.5.3.8.1" class="ltx_text ltx_font_bold">52.32</span></td>
<td id="S4.T3.1.1.5.3.9" class="ltx_td ltx_nopad_r ltx_align_center">78.62</td>
</tr>
<tr id="S4.T3.1.1.6.4" class="ltx_tr">
<th id="S4.T3.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">Multimodal DR (Ours)</th>
<th id="S4.T3.1.1.6.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">Speech</th>
<th id="S4.T3.1.1.6.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">‚úì</th>
<td id="S4.T3.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.1.6.4.4.1" class="ltx_text ltx_font_bold">18.52</span></td>
<td id="S4.T3.1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.1.6.4.5.1" class="ltx_text ltx_font_bold">54.26</span></td>
<td id="S4.T3.1.1.6.4.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T3.1.1.6.4.6.1" class="ltx_text ltx_font_bold">82.11</span></td>
<td id="S4.T3.1.1.6.4.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">15.47</td>
<td id="S4.T3.1.1.6.4.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">50.64</td>
<td id="S4.T3.1.1.6.4.9" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.1.6.4.9.1" class="ltx_text ltx_font_bold">81.58</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Retrieval performance on Spoken-MSMARCO (dev) for the cases where at inference time (i) the question has a mistranscription that the model encountered during training (Seen), or (ii) has a mistranscription that the model never encountered during training (Unseen). Unseen covers 1,883 samples, while Seen 3,437.</figcaption>
</figure>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">For <span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">RQ3</span>, we study the effectiveness of our ASR-free multimodal dense retriever against <span id="S4.SS2.p4.1.2" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipelines when the latter encounters previously unseen ASR mistranscribed words. In detail, we explore cases where a particular corrupted word during inference time was not part of the training set. In Table <a href="#S4.T3" title="Table 3 ‚Ä£ 4.2 Analysis ‚Ä£ 4 Results &amp; Discussion ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> we compare the retrieval performance for the cases of (i) previously seen and (ii) previously unseen ASR corrupted words.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‚Ä£ 4.2 Analysis ‚Ä£ 4 Results &amp; Discussion ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> unveils that a big part of the strong performance we observe on the <span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipelines stems from the fact that retrievers are trained on the exact same mistranscriptions they encounter during inference. There is a decrease of more than <math id="S4.SS2.p5.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS2.p5.1.m1.1a"><mn id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.1b"><cn type="integer" id="S4.SS2.p5.1.m1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.1c">10</annotation></semantics></math> points in Recall for all pipelines when they deal with corruption due to ASR that was not part of the training set. Additionally, our multimodal dense retriever significantly outperforms all the pipelines under the unseen scenario.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p">Inspired by the results in Table <a href="#S4.T3" title="Table 3 ‚Ä£ 4.2 Analysis ‚Ä£ 4 Results &amp; Discussion ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we set to study an extreme case of unseen ASR noise. In particular, we study how much the retrieval performance of pipeline approaches deteriorates when their dense retrievers are not explicitly trained on ASR noise (transcriptions). To do so, we train from scratch all the dense retrievers on clean questions instead of transcriptions. We report the results in Table <a href="#S4.T4" title="Table 4 ‚Ä£ 4.2 Analysis ‚Ä£ 4 Results &amp; Discussion ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. By comparing the retrieval models when they are explicitly aware of ASR corrupted words during training (as shown in Table <a href="#S4.T1" title="Table 1 ‚Ä£ 4.1 Main Results ‚Ä£ 4 Results &amp; Discussion ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) vs. when they are not (Table <a href="#S4.T4" title="Table 4 ‚Ä£ 4.2 Analysis ‚Ä£ 4 Results &amp; Discussion ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>), we see a dramatic drop in performance of more than 20 point drop in Recall. From Table <a href="#S4.T4" title="Table 4 ‚Ä£ 4.2 Analysis ‚Ä£ 4 Results &amp; Discussion ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we can further conclude that for the extreme case where the <span id="S4.SS2.p6.1.1" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipeline is not trained on ASR noise, following our multimodal dense retriever approach is necessary.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<div id="S4.T4.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:160pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(38.7pt,-14.3pt) scale(1.21736685299043,1.21736685299043) ;">
<table id="S4.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T4.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<table id="S4.T4.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T4.1.1.1.1.2.1.1" class="ltx_tr">
<td id="S4.T4.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Training</td>
</tr>
<tr id="S4.T4.1.1.1.1.2.1.2" class="ltx_tr">
<td id="S4.T4.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">questions</td>
</tr>
</table>
</th>
<th id="S4.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">End-to-end</th>
<th id="S4.T4.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">R@1000</th>
<th id="S4.T4.1.1.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">MRR@10</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.2.1" class="ltx_tr">
<th id="S4.T4.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BM25</th>
<th id="S4.T4.1.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">-</th>
<td id="S4.T4.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">‚úó</td>
<td id="S4.T4.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">45.34</td>
<td id="S4.T4.1.1.2.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">6.97</td>
</tr>
<tr id="S4.T4.1.1.3.2" class="ltx_tr">
<th id="S4.T4.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DR</th>
<th id="S4.T4.1.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Clean</th>
<td id="S4.T4.1.1.3.2.3" class="ltx_td ltx_align_center">‚úó</td>
<td id="S4.T4.1.1.3.2.4" class="ltx_td ltx_align_center">56.83</td>
<td id="S4.T4.1.1.3.2.5" class="ltx_td ltx_nopad_r ltx_align_center">11.36</td>
</tr>
<tr id="S4.T4.1.1.4.3" class="ltx_tr">
<th id="S4.T4.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DR+Augm.</th>
<th id="S4.T4.1.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Clean &amp; Typos</th>
<td id="S4.T4.1.1.4.3.3" class="ltx_td ltx_align_center">‚úó</td>
<td id="S4.T4.1.1.4.3.4" class="ltx_td ltx_align_center">64.57</td>
<td id="S4.T4.1.1.4.3.5" class="ltx_td ltx_nopad_r ltx_align_center">13.02</td>
</tr>
<tr id="S4.T4.1.1.5.4" class="ltx_tr">
<th id="S4.T4.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">CharacterBERT-DR+ST</th>
<th id="S4.T4.1.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Clean &amp; Typos</th>
<td id="S4.T4.1.1.5.4.3" class="ltx_td ltx_align_center">‚úó</td>
<td id="S4.T4.1.1.5.4.4" class="ltx_td ltx_align_center">66.94</td>
<td id="S4.T4.1.1.5.4.5" class="ltx_td ltx_nopad_r ltx_align_center">15.75</td>
</tr>
<tr id="S4.T4.1.1.6.5" class="ltx_tr">
<th id="S4.T4.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Multimodal DR (Ours)</th>
<th id="S4.T4.1.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Speech</th>
<td id="S4.T4.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">‚úì</td>
<td id="S4.T4.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.6.5.4.1" class="ltx_text ltx_font_bold">81.34</span></td>
<td id="S4.T4.1.1.6.5.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T4.1.1.6.5.5.1" class="ltx_text ltx_font_bold">15.77</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Retrieval results on Spoken-MSMARCO (dev) for the setting where the dense retrievers of <span id="S4.T4.3.1" class="ltx_text ltx_font_italic">ASR-Retriever</span> are not explicitly trained on ASR corrupted words.</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation Study on Model Training</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">To better understand how different model training schemes affect retrieval performance (<span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">RQ4</span>), we perform an ablation on our multimodal dense retriever and discuss our findings below.</p>
</div>
<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Learning Rate</h4>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p">In traditional dense text retrieval, the same learning rate is used to update all the weights in the retriever <cite class="ltx_cite ltx_citemacro_cite">Karpukhin et¬†al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>); Qu et¬†al. (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>. However, in our multimodal dense retrieval setup, the HuBERT (question encoder) and BERT (passage encoder) models are pre-trained independently to allow the usage of available large-scale unsupervised data. Therefore, there can be disparities between the two modalities that can hurt performance. To alleviate this problem we follow an alternative setting where the two encoders have different learning rates. In particular, since the language model contains more information than the speech model, we increase the learning rate of the passage encoder by a factor of 10. Comparing the values in the first block of Table <a href="#S4.T5" title="Table 5 ‚Ä£ Learning Rate ‚Ä£ 4.3 Ablation Study on Model Training ‚Ä£ 4 Results &amp; Discussion ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we find that the choice of learning rate is important for effectively training our multimodal dense retriever.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<div id="S4.T5.16" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:185.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(111.6pt,-47.8pt) scale(2.06151302676809,2.06151302676809) ;">
<table id="S4.T5.16.16" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.16.16.17.1" class="ltx_tr">
<th id="S4.T5.16.16.17.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Pooling</th>
<th id="S4.T5.16.16.17.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Learning Rate</th>
<th id="S4.T5.16.16.17.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">AR@20</th>
<th id="S4.T5.16.16.17.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">AR@100</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.4.4.4" class="ltx_tr">
<th id="S4.T5.4.4.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">first</th>
<td id="S4.T5.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t">p: <math id="S4.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="2e" display="inline"><semantics id="S4.T5.1.1.1.1.m1.1a"><mrow id="S4.T5.1.1.1.1.m1.1.1" xref="S4.T5.1.1.1.1.m1.1.1.cmml"><mn id="S4.T5.1.1.1.1.m1.1.1.2" xref="S4.T5.1.1.1.1.m1.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.T5.1.1.1.1.m1.1.1.1" xref="S4.T5.1.1.1.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S4.T5.1.1.1.1.m1.1.1.3" xref="S4.T5.1.1.1.1.m1.1.1.3.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.m1.1b"><apply id="S4.T5.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.m1.1.1"><times id="S4.T5.1.1.1.1.m1.1.1.1.cmml" xref="S4.T5.1.1.1.1.m1.1.1.1"></times><cn type="integer" id="S4.T5.1.1.1.1.m1.1.1.2.cmml" xref="S4.T5.1.1.1.1.m1.1.1.2">2</cn><ci id="S4.T5.1.1.1.1.m1.1.1.3.cmml" xref="S4.T5.1.1.1.1.m1.1.1.3">ùëí</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.m1.1c">2e</annotation></semantics></math>-<math id="S4.T5.2.2.2.2.m2.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.T5.2.2.2.2.m2.1a"><mn id="S4.T5.2.2.2.2.m2.1.1" xref="S4.T5.2.2.2.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.2.m2.1b"><cn type="integer" id="S4.T5.2.2.2.2.m2.1.1.cmml" xref="S4.T5.2.2.2.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.2.m2.1c">5</annotation></semantics></math>, q: <math id="S4.T5.3.3.3.3.m3.1" class="ltx_Math" alttext="2e" display="inline"><semantics id="S4.T5.3.3.3.3.m3.1a"><mrow id="S4.T5.3.3.3.3.m3.1.1" xref="S4.T5.3.3.3.3.m3.1.1.cmml"><mn id="S4.T5.3.3.3.3.m3.1.1.2" xref="S4.T5.3.3.3.3.m3.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.T5.3.3.3.3.m3.1.1.1" xref="S4.T5.3.3.3.3.m3.1.1.1.cmml">‚Äã</mo><mi id="S4.T5.3.3.3.3.m3.1.1.3" xref="S4.T5.3.3.3.3.m3.1.1.3.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.3.3.m3.1b"><apply id="S4.T5.3.3.3.3.m3.1.1.cmml" xref="S4.T5.3.3.3.3.m3.1.1"><times id="S4.T5.3.3.3.3.m3.1.1.1.cmml" xref="S4.T5.3.3.3.3.m3.1.1.1"></times><cn type="integer" id="S4.T5.3.3.3.3.m3.1.1.2.cmml" xref="S4.T5.3.3.3.3.m3.1.1.2">2</cn><ci id="S4.T5.3.3.3.3.m3.1.1.3.cmml" xref="S4.T5.3.3.3.3.m3.1.1.3">ùëí</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.3.3.m3.1c">2e</annotation></semantics></math>-<math id="S4.T5.4.4.4.4.m4.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.T5.4.4.4.4.m4.1a"><mn id="S4.T5.4.4.4.4.m4.1.1" xref="S4.T5.4.4.4.4.m4.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.T5.4.4.4.4.m4.1b"><cn type="integer" id="S4.T5.4.4.4.4.m4.1.1.cmml" xref="S4.T5.4.4.4.4.m4.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.4.4.4.4.m4.1c">5</annotation></semantics></math>
</td>
<td id="S4.T5.4.4.4.6" class="ltx_td ltx_align_center ltx_border_t">50.77</td>
<td id="S4.T5.4.4.4.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">64.48</td>
</tr>
<tr id="S4.T5.8.8.8" class="ltx_tr">
<th id="S4.T5.8.8.8.5" class="ltx_td ltx_align_left ltx_th ltx_th_row">first</th>
<td id="S4.T5.8.8.8.4" class="ltx_td ltx_align_center">p: <math id="S4.T5.5.5.5.1.m1.1" class="ltx_Math" alttext="2e" display="inline"><semantics id="S4.T5.5.5.5.1.m1.1a"><mrow id="S4.T5.5.5.5.1.m1.1.1" xref="S4.T5.5.5.5.1.m1.1.1.cmml"><mn id="S4.T5.5.5.5.1.m1.1.1.2" xref="S4.T5.5.5.5.1.m1.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.T5.5.5.5.1.m1.1.1.1" xref="S4.T5.5.5.5.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S4.T5.5.5.5.1.m1.1.1.3" xref="S4.T5.5.5.5.1.m1.1.1.3.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.5.5.5.1.m1.1b"><apply id="S4.T5.5.5.5.1.m1.1.1.cmml" xref="S4.T5.5.5.5.1.m1.1.1"><times id="S4.T5.5.5.5.1.m1.1.1.1.cmml" xref="S4.T5.5.5.5.1.m1.1.1.1"></times><cn type="integer" id="S4.T5.5.5.5.1.m1.1.1.2.cmml" xref="S4.T5.5.5.5.1.m1.1.1.2">2</cn><ci id="S4.T5.5.5.5.1.m1.1.1.3.cmml" xref="S4.T5.5.5.5.1.m1.1.1.3">ùëí</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.5.5.5.1.m1.1c">2e</annotation></semantics></math>-<math id="S4.T5.6.6.6.2.m2.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.T5.6.6.6.2.m2.1a"><mn id="S4.T5.6.6.6.2.m2.1.1" xref="S4.T5.6.6.6.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.T5.6.6.6.2.m2.1b"><cn type="integer" id="S4.T5.6.6.6.2.m2.1.1.cmml" xref="S4.T5.6.6.6.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.6.6.6.2.m2.1c">5</annotation></semantics></math>, q: <math id="S4.T5.7.7.7.3.m3.1" class="ltx_Math" alttext="2e" display="inline"><semantics id="S4.T5.7.7.7.3.m3.1a"><mrow id="S4.T5.7.7.7.3.m3.1.1" xref="S4.T5.7.7.7.3.m3.1.1.cmml"><mn id="S4.T5.7.7.7.3.m3.1.1.2" xref="S4.T5.7.7.7.3.m3.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.T5.7.7.7.3.m3.1.1.1" xref="S4.T5.7.7.7.3.m3.1.1.1.cmml">‚Äã</mo><mi id="S4.T5.7.7.7.3.m3.1.1.3" xref="S4.T5.7.7.7.3.m3.1.1.3.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.7.7.7.3.m3.1b"><apply id="S4.T5.7.7.7.3.m3.1.1.cmml" xref="S4.T5.7.7.7.3.m3.1.1"><times id="S4.T5.7.7.7.3.m3.1.1.1.cmml" xref="S4.T5.7.7.7.3.m3.1.1.1"></times><cn type="integer" id="S4.T5.7.7.7.3.m3.1.1.2.cmml" xref="S4.T5.7.7.7.3.m3.1.1.2">2</cn><ci id="S4.T5.7.7.7.3.m3.1.1.3.cmml" xref="S4.T5.7.7.7.3.m3.1.1.3">ùëí</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.7.7.7.3.m3.1c">2e</annotation></semantics></math>-<math id="S4.T5.8.8.8.4.m4.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.T5.8.8.8.4.m4.1a"><mn id="S4.T5.8.8.8.4.m4.1.1" xref="S4.T5.8.8.8.4.m4.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.T5.8.8.8.4.m4.1b"><cn type="integer" id="S4.T5.8.8.8.4.m4.1.1.cmml" xref="S4.T5.8.8.8.4.m4.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.8.8.8.4.m4.1c">4</annotation></semantics></math>
</td>
<td id="S4.T5.8.8.8.6" class="ltx_td ltx_align_center"><span id="S4.T5.8.8.8.6.1" class="ltx_text ltx_font_bold">57.25</span></td>
<td id="S4.T5.8.8.8.7" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T5.8.8.8.7.1" class="ltx_text ltx_font_bold">70.11</span></td>
</tr>
<tr id="S4.T5.12.12.12" class="ltx_tr">
<th id="S4.T5.12.12.12.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">mean</th>
<td id="S4.T5.12.12.12.4" class="ltx_td ltx_align_center ltx_border_t">p: <math id="S4.T5.9.9.9.1.m1.1" class="ltx_Math" alttext="2e" display="inline"><semantics id="S4.T5.9.9.9.1.m1.1a"><mrow id="S4.T5.9.9.9.1.m1.1.1" xref="S4.T5.9.9.9.1.m1.1.1.cmml"><mn id="S4.T5.9.9.9.1.m1.1.1.2" xref="S4.T5.9.9.9.1.m1.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.T5.9.9.9.1.m1.1.1.1" xref="S4.T5.9.9.9.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S4.T5.9.9.9.1.m1.1.1.3" xref="S4.T5.9.9.9.1.m1.1.1.3.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.9.9.9.1.m1.1b"><apply id="S4.T5.9.9.9.1.m1.1.1.cmml" xref="S4.T5.9.9.9.1.m1.1.1"><times id="S4.T5.9.9.9.1.m1.1.1.1.cmml" xref="S4.T5.9.9.9.1.m1.1.1.1"></times><cn type="integer" id="S4.T5.9.9.9.1.m1.1.1.2.cmml" xref="S4.T5.9.9.9.1.m1.1.1.2">2</cn><ci id="S4.T5.9.9.9.1.m1.1.1.3.cmml" xref="S4.T5.9.9.9.1.m1.1.1.3">ùëí</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.9.9.9.1.m1.1c">2e</annotation></semantics></math>-<math id="S4.T5.10.10.10.2.m2.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.T5.10.10.10.2.m2.1a"><mn id="S4.T5.10.10.10.2.m2.1.1" xref="S4.T5.10.10.10.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.T5.10.10.10.2.m2.1b"><cn type="integer" id="S4.T5.10.10.10.2.m2.1.1.cmml" xref="S4.T5.10.10.10.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.10.10.10.2.m2.1c">5</annotation></semantics></math>, q: <math id="S4.T5.11.11.11.3.m3.1" class="ltx_Math" alttext="2e" display="inline"><semantics id="S4.T5.11.11.11.3.m3.1a"><mrow id="S4.T5.11.11.11.3.m3.1.1" xref="S4.T5.11.11.11.3.m3.1.1.cmml"><mn id="S4.T5.11.11.11.3.m3.1.1.2" xref="S4.T5.11.11.11.3.m3.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.T5.11.11.11.3.m3.1.1.1" xref="S4.T5.11.11.11.3.m3.1.1.1.cmml">‚Äã</mo><mi id="S4.T5.11.11.11.3.m3.1.1.3" xref="S4.T5.11.11.11.3.m3.1.1.3.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.11.11.11.3.m3.1b"><apply id="S4.T5.11.11.11.3.m3.1.1.cmml" xref="S4.T5.11.11.11.3.m3.1.1"><times id="S4.T5.11.11.11.3.m3.1.1.1.cmml" xref="S4.T5.11.11.11.3.m3.1.1.1"></times><cn type="integer" id="S4.T5.11.11.11.3.m3.1.1.2.cmml" xref="S4.T5.11.11.11.3.m3.1.1.2">2</cn><ci id="S4.T5.11.11.11.3.m3.1.1.3.cmml" xref="S4.T5.11.11.11.3.m3.1.1.3">ùëí</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.11.11.11.3.m3.1c">2e</annotation></semantics></math>-<math id="S4.T5.12.12.12.4.m4.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.T5.12.12.12.4.m4.1a"><mn id="S4.T5.12.12.12.4.m4.1.1" xref="S4.T5.12.12.12.4.m4.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.T5.12.12.12.4.m4.1b"><cn type="integer" id="S4.T5.12.12.12.4.m4.1.1.cmml" xref="S4.T5.12.12.12.4.m4.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.12.12.12.4.m4.1c">4</annotation></semantics></math>
</td>
<td id="S4.T5.12.12.12.6" class="ltx_td ltx_align_center ltx_border_t">56.59</td>
<td id="S4.T5.12.12.12.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">69.97</td>
</tr>
<tr id="S4.T5.16.16.16" class="ltx_tr">
<th id="S4.T5.16.16.16.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">max</th>
<td id="S4.T5.16.16.16.4" class="ltx_td ltx_align_center ltx_border_bb">p: <math id="S4.T5.13.13.13.1.m1.1" class="ltx_Math" alttext="2e" display="inline"><semantics id="S4.T5.13.13.13.1.m1.1a"><mrow id="S4.T5.13.13.13.1.m1.1.1" xref="S4.T5.13.13.13.1.m1.1.1.cmml"><mn id="S4.T5.13.13.13.1.m1.1.1.2" xref="S4.T5.13.13.13.1.m1.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.T5.13.13.13.1.m1.1.1.1" xref="S4.T5.13.13.13.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S4.T5.13.13.13.1.m1.1.1.3" xref="S4.T5.13.13.13.1.m1.1.1.3.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.13.13.13.1.m1.1b"><apply id="S4.T5.13.13.13.1.m1.1.1.cmml" xref="S4.T5.13.13.13.1.m1.1.1"><times id="S4.T5.13.13.13.1.m1.1.1.1.cmml" xref="S4.T5.13.13.13.1.m1.1.1.1"></times><cn type="integer" id="S4.T5.13.13.13.1.m1.1.1.2.cmml" xref="S4.T5.13.13.13.1.m1.1.1.2">2</cn><ci id="S4.T5.13.13.13.1.m1.1.1.3.cmml" xref="S4.T5.13.13.13.1.m1.1.1.3">ùëí</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.13.13.13.1.m1.1c">2e</annotation></semantics></math>-<math id="S4.T5.14.14.14.2.m2.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.T5.14.14.14.2.m2.1a"><mn id="S4.T5.14.14.14.2.m2.1.1" xref="S4.T5.14.14.14.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.T5.14.14.14.2.m2.1b"><cn type="integer" id="S4.T5.14.14.14.2.m2.1.1.cmml" xref="S4.T5.14.14.14.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.14.14.14.2.m2.1c">5</annotation></semantics></math>, q: <math id="S4.T5.15.15.15.3.m3.1" class="ltx_Math" alttext="2e" display="inline"><semantics id="S4.T5.15.15.15.3.m3.1a"><mrow id="S4.T5.15.15.15.3.m3.1.1" xref="S4.T5.15.15.15.3.m3.1.1.cmml"><mn id="S4.T5.15.15.15.3.m3.1.1.2" xref="S4.T5.15.15.15.3.m3.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S4.T5.15.15.15.3.m3.1.1.1" xref="S4.T5.15.15.15.3.m3.1.1.1.cmml">‚Äã</mo><mi id="S4.T5.15.15.15.3.m3.1.1.3" xref="S4.T5.15.15.15.3.m3.1.1.3.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.15.15.15.3.m3.1b"><apply id="S4.T5.15.15.15.3.m3.1.1.cmml" xref="S4.T5.15.15.15.3.m3.1.1"><times id="S4.T5.15.15.15.3.m3.1.1.1.cmml" xref="S4.T5.15.15.15.3.m3.1.1.1"></times><cn type="integer" id="S4.T5.15.15.15.3.m3.1.1.2.cmml" xref="S4.T5.15.15.15.3.m3.1.1.2">2</cn><ci id="S4.T5.15.15.15.3.m3.1.1.3.cmml" xref="S4.T5.15.15.15.3.m3.1.1.3">ùëí</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.15.15.15.3.m3.1c">2e</annotation></semantics></math>-<math id="S4.T5.16.16.16.4.m4.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.T5.16.16.16.4.m4.1a"><mn id="S4.T5.16.16.16.4.m4.1.1" xref="S4.T5.16.16.16.4.m4.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.T5.16.16.16.4.m4.1b"><cn type="integer" id="S4.T5.16.16.16.4.m4.1.1.cmml" xref="S4.T5.16.16.16.4.m4.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.16.16.16.4.m4.1c">4</annotation></semantics></math>
</td>
<td id="S4.T5.16.16.16.6" class="ltx_td ltx_align_center ltx_border_bb">53.57</td>
<td id="S4.T5.16.16.16.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">67.45</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison of different training schemes on Spoken-NQ. We indicate the learning rate of the question and passage encoder as q and p, respectively.</figcaption>
</figure>
</section>
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Pooling</h4>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p1.1" class="ltx_p">The next ablation involves different pooling methods for encoding the spoken question into a single vector. Following previous works on dense retrieval, we use the <span id="S4.SS3.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">[CLS]</span> token embedding output from BERT to encode the text passage. In contrast, this decision is not that straightforward in the case of the spoken question. The HuBERT speech transformer we use for encoding the spoken questions does not have a next-sentence prediction pre-training task as in BERT. Thus, there is no <span id="S4.SS3.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">[CLS]</span> token available. To this extent, we asses different pooling strategies for encoding the spoken question, namely, max and mean pooling or taking the first embedding of the sequence as a pooling strategy. As we can see in Table <a href="#S4.T5" title="Table 5 ‚Ä£ Learning Rate ‚Ä£ 4.3 Ablation Study on Model Training ‚Ä£ 4 Results &amp; Discussion ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, using the first token embedding output from HuBERT to represent the speech utterance holds the best results.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph"></h4>

<div id="S4.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px3.p1.1" class="ltx_p">For our ablation study we reported results on the test split of Spoken-NQ (Table <a href="#S4.T5" title="Table 5 ‚Ä£ Learning Rate ‚Ä£ 4.3 Ablation Study on Model Training ‚Ä£ 4 Results &amp; Discussion ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). However, we want to clarify that the decision for our best multimodal retriever was based on the development, as discussed in Section <a href="#S3.SS3" title="3.3 Implementation Details ‚Ä£ 3 Experimental Setup ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Passage retrieval is a key task in traditional open-domain QA and speech-based open-domain QA. In detail, following the retriever and reader framework, the retriever reduces the search space for effective answer extraction and provides the support context that users can use to verify the answer. Traditional open-domain QA has seen significant advancements with the introduction of dense retrievers <cite class="ltx_cite ltx_citemacro_cite">Karpukhin et¬†al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>); Qu et¬†al. (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>); Zhu et¬†al. (<a href="#bib.bib22" title="" class="ltx_ref">2021</a>)</cite> that have demonstrated higher effectiveness than bag-of-words methods.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Despite their effectiveness, dense retrievers can still perform poorly in the presence of noise. <cite class="ltx_cite ltx_citemacro_citet">Zhuang and Zuccon (<a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite> investigated the robustness of dense retrievers against questions with typos. Their work suggested that dense retrievers perform poorly on questions that have typos, and to this extent, they proposed a typo-aware training strategy (data augmentation) to alleviate this problem. <cite class="ltx_cite ltx_citemacro_citet">Sidiropoulos and Kanoulas (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite> built upon this and further proposed to combine data augmentation with a contrastive loss to bring closer the representations of a question and its typoed variants. <cite class="ltx_cite ltx_citemacro_citet">Zhuang and Zuccon (<a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite> showed that replacing the backbone BERT encoders of the dense retriever with CharacterBERT can increase robustness against typos. They further improved robustness via a novel self-teaching training strategy that distills knowledge from questions without typos into typoed questions.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Recently, <cite class="ltx_cite ltx_citemacro_citet">Sidiropoulos et¬†al. (<a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite> studied the robustness of dense retrievers under ASR noise. By employing a pipeline approach with an ASR system and a dense retriever for text retrieval, they showed that dense retrievers, trained only on clean questions, are not robust against ASR noise. They further suggested that training the retriever to be robust against typos can increase the robustness against ASR noise. To the best of our knowledge, this is the first work that studies passage retrieval for speech-based open-domain QA.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">On the other end of the spectrum, recent works in speech-based QA explored reading comprehension as a component of the retriever and reader framework. <cite class="ltx_cite ltx_citemacro_citet">Ravichander et¬†al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite> showed that mistranscription from the ASR model leads to a huge performance degradation in the transformer-based reading comprehension models while <cite class="ltx_cite ltx_citemacro_citet">Faisal et¬†al. (<a href="#bib.bib4" title="" class="ltx_ref">2021</a>)</cite> suggested that background differences in the users (e.g., accent) have different impacts on the performance of reading comprehension models.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">At this point, we want to highlight the differences between speech-based open-domain QA and spoken QA <cite class="ltx_cite ltx_citemacro_cite">You et¬†al. (<a href="#bib.bib21" title="" class="ltx_ref">2021</a>); Li et¬†al. (<a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite> since there can be misconceptions due to the similarity in their names. Spoken QA is a searching through speech task where given a text question and a spoken document the goal is to find the answer from this single spoken document. Therefore, this is a fundamentally different problem compared to the problem we consider in this work.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work, we thoroughly analyzed the behavior of ASR-Retriever pipelines for passage retrieval for speech-based open-domain QA, showing their limitations, and we further introduced the first end-to-end trained, ASR-free multimodal dense retriever in order to tackle these limitations. Our experimental results showed that our multimodal dense retriever is a promising alternative to the <span id="S6.p1.1.1" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipelines on shorter questions and under higher word error rate scenarios. Furthermore, we unveiled that <span id="S6.p1.1.2" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipelines show a dramatic performance decrease as the word error rate of the ASR model increases, when important words in the spoken question are mistranscribed, and when dealing with mistranscriptions that have not been encountered during training. To this extent, we showcased that our ASR-free multimodal dense retriever can overcome the aforementioned issues. Finally, despite the limited performance of our proposed method on longer questions, we believe that our thorough analysis can spur community interest in passage retrieval for speech-based open-domain QA.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitations</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this work, all the models are trained using the hard negatives provided by the original datasets or mined from BM25 <cite class="ltx_cite ltx_citemacro_cite">Karpukhin et¬†al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> and by employing the base versions of the speech and language transformer models (see Section <a href="#S3.SS3" title="3.3 Implementation Details ‚Ä£ 3 Experimental Setup ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>). Therefore, we leave the application of more complex hard negatives mining techniques, such as mining from the dense index of a previous checkpoint of the dense retriever itself <cite class="ltx_cite ltx_citemacro_cite">Xiong et¬†al. (<a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite>, and larger models (e.g., BERT-Large and HuBERT-Large) to future work.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">As we saw in Section <a href="#S4.SS1" title="4.1 Main Results ‚Ä£ 4 Results &amp; Discussion ‚Ä£ A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>, our multimodal dense retriever showed promising results against its <span id="S7.p2.1.1" class="ltx_text ltx_font_italic">ASR-Retriever</span> counterparts on shorter questions under high word error rate scenarios.
We conjecture that the limited performance of our approach on long questions is due to encoding all the information from the speech signal in a single vector, and we leave exploring a multi-vector retrieval approach as future work. <span id="S7.p2.1.2" class="ltx_text ltx_font_italic">ASR-Retriever</span> pipelines can produce significantly better results compared to our method for cases where the ASR model can perform high-quality transcriptions with low word error rate. To this extent, we did not experiment with more advanced ASR models, such as the recently proposed Whisper <cite class="ltx_cite ltx_citemacro_cite">Radford et¬†al. (<a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite>, since our goal in this work was to provide alternatives for cases with high word error rates in the transcriptions.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et¬†al. (2020)</span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html" title="" class="ltx_ref ltx_href">wav2vec 2.0: A framework for self-supervised learning of speech representations</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et¬†al. (2021)</span>
<span class="ltx_bibblock">
Yu-An Chung, Chenguang Zhu, and Michael Zeng. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.naacl-main.152" title="" class="ltx_ref ltx_href">SPLAT: speech-language joint pre-training for spoken language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021</em>, pages 1897‚Äì1907. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et¬†al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/n19-1423" title="" class="ltx_ref ltx_href">BERT: pre-training of deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</em>, pages 4171‚Äì4186. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Faisal et¬†al. (2021)</span>
<span class="ltx_bibblock">
Fahim Faisal, Sharlina Keshava, Md¬†Mahfuz¬†Ibn Alam, and Antonios Anastasopoulos. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.findings-emnlp.281" title="" class="ltx_ref ltx_href">SD-QA: spoken dialectal question answering for the real world</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021</em>, pages 3296‚Äì3315. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et¬†al. (2021)</span>
<span class="ltx_bibblock">
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung¬†Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TASLP.2021.3122291" title="" class="ltx_ref ltx_href">Hubert: Self-supervised speech representation learning by masked prediction of hidden units</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE ACM Trans. Audio Speech Lang. Process.</em>, 29:3451‚Äì3460.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hwang et¬†al. (2022)</span>
<span class="ltx_bibblock">
Dongseong Hwang, Ananya Misra, Zhouyuan Huo, Nikhil Siddhartha, Shefali Garg, David Qiu, Khe¬†Chai Sim, Trevor Strohman, Fran√ßoise Beaufays, and Yanzhang He. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP43922.2022.9746719" title="" class="ltx_ref ltx_href">Large-scale ASR domain adaptation using self- and semi-supervised learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore, 23-27 May 2022</em>, pages 6627‚Äì6631. IEEE.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et¬†al. (2021)</span>
<span class="ltx_bibblock">
Jeff Johnson, Matthijs Douze, and Herv√© J√©gou. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TBDATA.2019.2921572" title="" class="ltx_ref ltx_href">Billion-scale similarity search with gpus</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Big Data</em>, 7(3):535‚Äì547.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et¬†al. (2020)</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S.¬†H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.550" title="" class="ltx_ref ltx_href">Dense passage retrieval for open-domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020</em>, pages 6769‚Äì6781. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et¬†al. (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur¬†P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew¬†M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://transacl.org/ojs/index.php/tacl/article/view/1455" title="" class="ltx_ref ltx_href">Natural questions: a benchmark for question answering research</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Trans. Assoc. Comput. Linguistics</em>, 7:452‚Äì466.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et¬†al. (2018)</span>
<span class="ltx_bibblock">
Chia-Hsuan Li, Szu-Lin Wu, Chi-Liang Liu, and Hung-yi Lee. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.21437/Interspeech.2018-1714" title="" class="ltx_ref ltx_href">Spoken squad: A study of mitigating the impact of speech recognition errors on listening comprehension</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Interspeech 2018, 19th Annual Conference of the International Speech Communication Association, Hyderabad, India, 2-6 September 2018</em>, pages 3459‚Äì3463. ISCA.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et¬†al. (2021)</span>
<span class="ltx_bibblock">
Tingzhi Mao, Yerbolat Khassanov, Van¬†Tung Pham, Haihua Xu, Hao Huang, and Eng¬†Siong Chng. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ISCSLP49672.2021.9362062" title="" class="ltx_ref ltx_href">Approaches to improving recognition of underrepresented named entities in hybrid ASR systems</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">12th International Symposium on Chinese Spoken Language Processing, ISCSLP 2021, Hong Kong, January 24-27, 2021</em>, pages 1‚Äì5. IEEE.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et¬†al. (2016)</span>
<span class="ltx_bibblock">
Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li¬†Deng. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf" title="" class="ltx_ref ltx_href">MS MARCO: A human generated machine reading comprehension dataset</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016</em>, volume 1773 of <em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic">CEUR Workshop Proceedings</em>. CEUR-WS.org.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panayotov et¬†al. (2015)</span>
<span class="ltx_bibblock">
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP.2015.7178964" title="" class="ltx_ref ltx_href">Librispeech: An ASR corpus based on public domain audio books</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2015 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2015, South Brisbane, Queensland, Australia, April 19-24, 2015</em>, pages 5206‚Äì5210. IEEE.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et¬†al. (2021)</span>
<span class="ltx_bibblock">
Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne¬†Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.naacl-main.466" title="" class="ltx_ref ltx_href">Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021</em>, pages 5835‚Äì5847. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et¬†al. (2022)</span>
<span class="ltx_bibblock">
Alec Radford, Jong¬†Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2212.04356" title="" class="ltx_ref ltx_href">Robust speech recognition via large-scale weak supervision</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2212.04356.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ravichander et¬†al. (2021)</span>
<span class="ltx_bibblock">
Abhilasha Ravichander, Siddharth Dalmia, Maria Ryskina, Florian Metze, Eduard¬†H. Hovy, and Alan¬†W. Black. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.eacl-main.259" title="" class="ltx_ref ltx_href">Noiseqa: Challenge set evaluation for user-centric question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021</em>, pages 2976‚Äì2992. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sidiropoulos and Kanoulas (2022)</span>
<span class="ltx_bibblock">
Georgios Sidiropoulos and Evangelos Kanoulas. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3477495.3531818" title="" class="ltx_ref ltx_href">Analysing the robustness of dual encoders for dense retrieval against misspellings</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">SIGIR ‚Äô22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022</em>, pages 2132‚Äì2136. ACM.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sidiropoulos et¬†al. (2022)</span>
<span class="ltx_bibblock">
Georgios Sidiropoulos, Svitlana Vakulenko, and Evangelos Kanoulas. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3511808.3557662" title="" class="ltx_ref ltx_href">On the impact of speech recognition errors in passage retrieval for spoken question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management, Atlanta, GA, USA, October 17-21, 2022</em>, pages 4485‚Äì4489. ACM.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et¬†al. (2020)</span>
<span class="ltx_bibblock">
Haoyu Wang, Shuyan Dong, Yue Liu, James Logan, Ashish¬†Kumar Agrawal, and Yang Liu. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.21437/Interspeech.2020-1753" title="" class="ltx_ref ltx_href">ASR error correction with augmented transformer for entity retrieval</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020</em>, pages 1550‚Äì1554. ISCA.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et¬†al. (2021)</span>
<span class="ltx_bibblock">
Lee Xiong, Chenyan Xiong, Ye¬†Li, Kwok-Fung Tang, Jialin Liu, Paul¬†N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=zeFrfgyZln" title="" class="ltx_ref ltx_href">Approximate nearest neighbor negative contrastive learning for dense text retrieval</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">You et¬†al. (2021)</span>
<span class="ltx_bibblock">
Chenyu You, Nuo Chen, and Yuexian Zou. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP39728.2021.9414999" title="" class="ltx_ref ltx_href">Knowledge distillation for improved accuracy in spoken question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2021, Toronto, ON, Canada, June 6-11, 2021</em>, pages 7793‚Äì7797. IEEE.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et¬†al. (2021)</span>
<span class="ltx_bibblock">
Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2101.00774" title="" class="ltx_ref ltx_href">Retrieving and reading: A comprehensive survey on open-domain question answering</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2101.00774.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuang and Zuccon (2021)</span>
<span class="ltx_bibblock">
Shengyao Zhuang and Guido Zuccon. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.emnlp-main.225" title="" class="ltx_ref ltx_href">Dealing with typos for bert-based passage retrieval and ranking</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021</em>, pages 2836‚Äì2842. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuang and Zuccon (2022)</span>
<span class="ltx_bibblock">
Shengyao Zhuang and Guido Zuccon. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/arXiv.2204.00716" title="" class="ltx_ref ltx_href">Characterbert and self-teaching for improving the robustness of dense retrievers on queries with typos</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2204.00716.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.13482" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.13483" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.13483">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.13483" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.13485" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 01:04:13 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
