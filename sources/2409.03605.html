<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.03605] SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing</title><meta property="og:description" content="Audio-driven talking face generation aims to synthesize video with lip movements synchronized to input audio. However, current generative techniques face challenges in preserving intricate regional textures (skin, teetâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.03605">

<!--Generated on Sun Oct  6 01:37:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Video Generation,  Talking Face Generation,  Attribute Editing">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lingyu Xiong
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">South China University of Technology</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_city">Guangzhou</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:xiongly01@gmail.com">xiongly01@gmail.com</a>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xize Cheng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_affiliation_institution">Zhejiang University</span><span id="id5.2.id2" class="ltx_text ltx_affiliation_city">Hangzhou</span><span id="id6.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:chengxize@zju.edu.cn">chengxize@zju.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jintao Tan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">South China University of Technology</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_city">Guangzhou</span><span id="id9.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xianjia Wu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id10.1.id1" class="ltx_text ltx_affiliation_institution">Huawei Cloud Computing Technologies Co., Ltd</span><span id="id11.2.id2" class="ltx_text ltx_affiliation_city">Shenzhen</span><span id="id12.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiandong Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id13.1.id1" class="ltx_text ltx_affiliation_institution">Huawei Cloud Computing Technologies Co., Ltd</span><span id="id14.2.id2" class="ltx_text ltx_affiliation_city">Shenzhen</span><span id="id15.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lei Zhu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id16.1.id1" class="ltx_text ltx_affiliation_institution">Peking University</span><span id="id17.2.id2" class="ltx_text ltx_affiliation_city">Shenzhen</span><span id="id18.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fei Ma
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id19.1.id1" class="ltx_text ltx_affiliation_institution">Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)</span><span id="id20.2.id2" class="ltx_text ltx_affiliation_city">Shenzhen</span><span id="id21.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Minglei Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id22.1.id1" class="ltx_text ltx_affiliation_institution">Huawei Cloud Computing Technologies Co., Ltd</span><span id="id23.2.id2" class="ltx_text ltx_affiliation_city">Shenzhen</span><span id="id24.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Huang Xu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id25.1.id1" class="ltx_text ltx_affiliation_institution">Huawei Cloud Computing Technologies Co., Ltd</span><span id="id26.2.id2" class="ltx_text ltx_affiliation_city">Shenzhen</span><span id="id27.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span></span></span>
<span class="ltx_author_before">Â andÂ </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhihu Hu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id28.1.id1" class="ltx_text ltx_affiliation_institution">South China University of Technology</span><span id="id29.2.id2" class="ltx_text ltx_affiliation_city">Guangzhou</span><span id="id30.3.id3" class="ltx_text ltx_affiliation_country">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:eezhhu@scut.edu.cn">eezhhu@scut.edu.cn</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id31.id1" class="ltx_p">Audio-driven talking face generation aims to synthesize video with lip movements synchronized to input audio. However, current generative techniques face challenges in preserving intricate regional textures (skin, teeth). To address the aforementioned challenges, we propose a novel framework called SegTalker to decouple lip movements and image textures by introducing segmentation as intermediate representation. Specifically, given the mask of image employed by a parsing network, we first leverage the speech to drive the mask and generate talking segmentation. Then we disentangle semantic regions of image into style codes using a mask-guided encoder. Ultimately, we inject the previously generated talking segmentation and style codes into a mask-guided StyleGAN to synthesize video frame. In this way, most of textures are fully preserved. Moreover, our approach can inherently achieve background separation and facilitate mask-guided facial local editing. In particular, by editing the mask and swapping the region textures from a given reference image (e.g. hair, lip, eyebrows), our approach enables facial editing seamlessly when generating talking face video. Experiments demonstrate that our proposed approach can effectively preserve texture details and generate temporally consistent video while remaining competitive in lip synchronization. Quantitative and qualitative results on the HDTF and MEAD datasets illustrate the superior performance of our method over existing methods.</p>
</div>
<div class="ltx_keywords">Video Generation, Talking Face Generation, Attribute Editing
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">doi: </span>10.1145/3664647.3681108</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">conference: </span>Proceedings of the 32nd ACM International Conference on Multimedia; October 28-November 1, 2024; Melbourne, VIC, Australia</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">isbn: </span>979-8-4007-0686-8/24/10</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Computing methodologiesÂ Computer vision tasks</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Information systemsÂ Multimedia content creation</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Talking face generation, which aims to synthesize facial imagery precisely synchronized with input speech, has garnered substantial research attention in the field of computer vision and multimediaÂ <cite class="ltx_cite ltx_citemacro_citep">(Huang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2023</a>; Cheng etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2023c</a>; Jin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite> due to its numerous applications including digital human, virtual conference and video dubbingÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2021</a>; Meshry etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>; Wu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2023</a>; Cheng etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib10" title="" class="ltx_ref">b</a>)</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.03605/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="207" height="106" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Given a talking video and another speech, SegTalker can produce high-fidelity and synchronized video with rich textures (row 2), enabling swapping background (row 3) and local editing such as blinking (row 4).</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">There are many attempts to realize high-fidelity talking face. Early approaches first predict mouth shapes from speech using recurrent neural networks, then generate the face conditioned on the shapesÂ <cite class="ltx_cite ltx_citemacro_citep">(Suwajanakorn etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2017</a>)</cite>. Recent end-to-end methods directly map speech spectrograms to video frames leveraging different intermediate representationsÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2021</a>; Song etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2022</a>; Zhou etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2020</a>; Meshry etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>; Tan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2024a</a>)</cite>. Zhang et al. Â <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2021</a>)</cite> takes advantage of 3D Morphable Models (3DMMs), a parametric model that decomposes expression, pose, and identity, to transfer facial motions. Zhou et al.Â <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2020</a>)</cite> employs the landmark as the representation. Meshry et al.Â <cite class="ltx_cite ltx_citemacro_citep">(Meshry etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite> factorize the talking-head synthesis process into spatial and style components through coarse-grained masks, but they do not facilitate texture disentanglement and facial editing. More recently, Kicanaoglu et al. <cite class="ltx_cite ltx_citemacro_citep">(Kicanaoglu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite> perform unsupervised vector quantization on intermediate feature maps of StyleGAN to generate abundant semantic regions for local editing. Despite improvements in photo-realism, current talking face methods still face challenges in preserving identity-specific details such as hair, skin textures and teeth. Furthermore, within the current landscape of talking face generation methods, there is no single technique that can concurrently accomplish facial editing and background replacement. Our method elegantly incorporates facial editing into talking face generation in an end-to-end manner through the intermediate representation of segmentation.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we aim to design a unified approach that realizes the controllable talking face synthesis and editing. We propose a novel framework termed SegTalker that explicitly disentangles textural details with lip movements by utilizing segmentation. Our framework consists of an audio-driven talking segmentation generation (TSG) module, followed by a segmentation-guided GAN injection (SGI) network to synthesize animation video. We utilize a pre-trained networkÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2018</a>)</cite> to extract segmentation as prior information to decompose semantic regions and enhance textural details, seamlessly enabling fine-grained facial local editing and background replacement. Specifically, given the input image and speech, we first conduct face parsing to obtain the segmentation. Subsequently, TSG module extracts image and speech embedding and then combines these embeddings to synthesize new segmentation with lips synchronized to the input speech. After that, SGI module employs a multi-scale encoder to project the input face into the latent space of StyleGANÂ <cite class="ltx_cite ltx_citemacro_citep">(Karras etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite>. Each facial region has a set of style codes for different layers of the StyleGAN generator. Then We inject the synthesized mask and style codes into the mask-guided generator to obtain the talking face. In this way, the structural information and textures of facial components are fully disentangled. Furthermore, facial local editing can be accomplished by simply modifying the synthesized mask or swapping the region textures from a given reference image, achieving seamless integration with talking face synthesis.
In summary, our contributions are:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a novel framework that utilizes segmentation as an intermediate representation to disentangle the lip movements with image reconstruction for talking face generation, achieving consistent lip movements and preserving fine-grained textures.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We employ a multi-scale encoder and mask-guided generator to realize the local control for different semantic regions. By manipulating the masks and smoothly swapping the textures, we can seamlessly integrate the facial local editing into the talking face pipeline and conduct swapping background.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Experiments on HDTF and MEAD datasets demonstrate our superiority over state-of-the-art methods in visual quality, ID preservation and temporal consistency.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Audio-driven Talking Face Generation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Talking face generation, which aims to synthesize photo-realistic video of a talking person giving a speech as input, has garnered increasing research attention in recent years. With the emergence of generative adversarial networks (GANs)Â <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2014</a>)</cite>, many methodsÂ <cite class="ltx_cite ltx_citemacro_citep">(Prajwal etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2020</a>; Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2021</a>; Song etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2022</a>; Zhou etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2020</a>; Meshry etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite> have been proposed for synthesizing animation video. In terms of the intermediate representations, the existing works can be categorized into landmark-based, 3D-based and others. In the landmark-based methods, Suwajanakorn et al. Â <cite class="ltx_cite ltx_citemacro_citep">(Suwajanakorn etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2017</a>)</cite> use recurrent neural network (RNN) to build the mapping from the input speech to mouth landmark, and then generate mouth texture. Zhou et al.Â <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2020</a>)</cite> combine LSTM and self-attention to predict the locations of landmarks. Zhong et al. Â <cite class="ltx_cite ltx_citemacro_citep">(Zhong etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2023</a>)</cite> utilizes transformer to predict landmarks, then combines multi-source features (prior information, landmarks, speech) to synthesize talking face. Recently, DiffTalkÂ <cite class="ltx_cite ltx_citemacro_citep">(Shen etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite> takes speech and landmarks as conditioned inputs and utilizes a latent diffusion modelÂ <cite class="ltx_cite ltx_citemacro_citep">(Rombach etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2022</a>)</cite> to generate talking faces. For 3DMM-based method, SadTalkerÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2023</a>)</cite> learns realistic 3D motion coefficients for stylized audio-driven single-image talking face animation, achieving high-quality results by explicitly modeling audio-motion connections. Some styleGAN-based methodÂ <cite class="ltx_cite ltx_citemacro_citep">(Alghamdi etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2022</a>; Tan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2024b</a>; Yin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2022</a>)</cite> such as StyleHEATÂ <cite class="ltx_cite ltx_citemacro_citep">(Yin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2022</a>)</cite> leverages a pre-trained StyleGAN to achieve high-resolution editable talking face generation from a single portrait image, allowing disentangled control via audio. More recently, the emergence of neural radiance field (NeRF) provides a new perspective for 3D-aware talking face generationÂ <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2021</a>; Shen etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite>.
However, these intermediate representations have difficulty in capturing fine-grained details and preserving identity i.e., teeth and skin textures which degrade the visual quality heavily. Wav2LipÂ <cite class="ltx_cite ltx_citemacro_citep">(Prajwal etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite> adopts the encoder-decoder architecture to synthesize animation videos. However, there are conspicuous artifacts with a low resolution in the synthesized videos. In this work, we employ a novel representation, segmentation, to disentangle lip movement with image reconstruction, and further extract per-region features to preserve texture details.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>GAN Inversion</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">GAN inversion aims to invert real images into the latent space of a generator for reconstruction and editing. Several StyleGAN inversion methods have been proposed, they can typically be divided into three major groups of methods: 1) gradient-based optimization of the latent codeÂ <cite class="ltx_cite ltx_citemacro_citep">(Abdal etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2019</a>, <a href="#bib.bib3" title="" class="ltx_ref">2020</a>; Saha etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2021</a>; Kang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>, 2) encoder-basedÂ <cite class="ltx_cite ltx_citemacro_citep">(Tov etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2021</a>; Richardson etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2021</a>; Alaluf etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2021</a>; Xu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2023</a>; Yao etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2022</a>)</cite> and 3) fine-tune methodsÂ <cite class="ltx_cite ltx_citemacro_citep">(Roich etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2022</a>; Nitzan etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2022</a>; Alaluf etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2022</a>; Zeng etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2023</a>)</cite>. The gradient-based optimization methods directly optimize the latent code using gradient from the loss between the real image and the generated one. The encoder-based methods train an encoder network over a large number of samples to directly map the RGB image to latent code. The gradient-based optimization methods always give better performance while the encoder-based cost less time. The fine-tuning methods make a trade-off between the above two and use the inverted latent code from encoder as the initialization code to further optimization. However, existing works focus on global editing and cannot make fine-grained control of the local regions. Our method uses a variation ofÂ <cite class="ltx_cite ltx_citemacro_citep">(Richardson etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2021</a>)</cite> to realize local editing via manipulating a novel <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{W}^{c+}" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><msup id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">ğ’²</mi><mrow id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml"><mi id="S2.SS2.p1.1.m1.1.1.3.2" xref="S2.SS2.p1.1.m1.1.1.3.2.cmml">c</mi><mo id="S2.SS2.p1.1.m1.1.1.3.3" xref="S2.SS2.p1.1.m1.1.1.3.3.cmml">+</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">superscript</csymbol><ci id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">ğ’²</ci><apply id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S2.SS2.p1.1.m1.1.1.3.1.cmml" xref="S2.SS2.p1.1.m1.1.1.3">limit-from</csymbol><ci id="S2.SS2.p1.1.m1.1.1.3.2.cmml" xref="S2.SS2.p1.1.m1.1.1.3.2">ğ‘</ci><plus id="S2.SS2.p1.1.m1.1.1.3.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3.3"></plus></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">\mathcal{W}^{c+}</annotation></semantics></math>Â <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite> latent space.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Proposed Methods</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Overview</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To tackle the lack of regional textures in talking face generation, we explicitly disentangle semantic regions by introducing segmentation mechanism. Leveraging segmentation as an intermediate representation, our approach decouples audio-driven mouth animation and image texture injection. The speech is solely responsible for driving the lip contours, while the injection module focuses on extracting per-region textures to generate the animation video. The overall framework of our proposed model, termed SegTalker, is illustrated in <a href="#S3.F2" title="In 3.2. Talking Segmentation Generation â€£ 3. Proposed Methods â€£ SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">fig.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>. The pipeline consists of two sub-networks: (1) talking segmentation generation (TSG) and (2) segmentation-guided GAN injection network (SGI), which are elaborated in <a href="#S3.SS2" title="3.2. Talking Segmentation Generation â€£ 3. Proposed Methods â€£ SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">section</span>Â <span class="ltx_text ltx_ref_tag">3.2</span></a> and <a href="#S3.SS3" title="3.3. Segmentation-guided GAN Injection â€£ 3. Proposed Methods â€£ SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">section</span>Â <span class="ltx_text ltx_ref_tag">3.3</span></a>, respectively.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Talking Segmentation Generation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The first proxy sub-network is the talking segmentation generation (TSG) module. Given speech and image frame, this network first employs parsing networkÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2018</a>)</cite> to extract mask and then synthesizes talking segmentation. The original network generates 19 categories in total. For the sake of simplicity, we merge the same semantic class (e.g. left and right eyes), resulting in 12 final classes. During pre-processing, video is unified to 25fps with speech sampled at 16kHz. To incorporate temporal information, FollowingÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2023a</a>)</cite>, the global and local features are extracted as speech embedding. We employ <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">mask encoder</span> to extract visual embedding from two masks: a pose source and an identity reference. The two masks are concatenated in the channel dimension. The pose source aligns with the target segmentation but with the lower half occluded. The identity reference provides facial structural information of the lower half to facilitate training and convergence. Without concerning textural information, the model only focuses on learning the structural mapping from speech to lip movements.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We employ a CNN-based network to extract the embedding of a 0.2-second audio segment whose center is synchronized with the pose source. Similar to text, speech always contains sequential information. To better capture temporally relevant features, we employ the pre-trained AV-HubertÂ <cite class="ltx_cite ltx_citemacro_citep">(Shi etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2022a</a>, <a href="#bib.bib35" title="" class="ltx_ref">b</a>)</cite> as a part of <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">audio encoder</span> to extract long-range dependencies. AV-Hubert has conducted pre-training for audio-visual alignmentÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2023b</a>, <a href="#bib.bib47" title="" class="ltx_ref">2024</a>)</cite>, so the extracted embedding is very close to the semantic space of the video. When using AV-Hubert to extract audio embedding, we only need to feed speech and the visual signal is masked. Specifically, given a 3s speech chunk, we feed it into the Transformer-based AV-Hubert to produce contextualized speech features. We then extract the feature embedding corresponding to the given image segment. Given the mixed speech embedding and visual embedding, <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_bold">Generator</span> synthesizes the final talking segmentation. We adopt U-NetÂ <cite class="ltx_cite ltx_citemacro_citep">(Ronneberger etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2015</a>)</cite> as the backbone architecture. In addition, skip connections and transposed convolutions are utilized for feature fusion and up-sampling.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Given a mask synthesized by the model and the ground truth mask, we employ two types of losses to improve generation quality i.e., the reconstruction loss and the syncnet loss.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.4" class="ltx_p"><span id="S3.SS2.p4.4.1" class="ltx_text ltx_font_bold">Reconstruction Loss</span>
Unlike previous generative tasks that often synthesize RGB image and adopt L1 loss for reconstruction, talking segmentation involves generating segmentation where each pixel denotes a particular class. To stay consistent with semantic segmentation task, we employ cross entropy loss as our reconstruction loss. Given <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="N_{i}" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><msub id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">N</mi><mi id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">ğ‘</ci><ci id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">N_{i}</annotation></semantics></math> generated masks <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><msub id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><mi id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml">y</mi><mi id="S3.SS2.p4.2.m2.1.1.3" xref="S3.SS2.p4.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2">ğ‘¦</ci><ci id="S3.SS2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">y_{i}</annotation></semantics></math> and ground truth <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="\hat{y_{i}}" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><mover accent="true" id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml"><msub id="S3.SS2.p4.3.m3.1.1.2" xref="S3.SS2.p4.3.m3.1.1.2.cmml"><mi id="S3.SS2.p4.3.m3.1.1.2.2" xref="S3.SS2.p4.3.m3.1.1.2.2.cmml">y</mi><mi id="S3.SS2.p4.3.m3.1.1.2.3" xref="S3.SS2.p4.3.m3.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS2.p4.3.m3.1.1.1" xref="S3.SS2.p4.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"><ci id="S3.SS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1.1">^</ci><apply id="S3.SS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p4.3.m3.1.1.2.1.cmml" xref="S3.SS2.p4.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS2.p4.3.m3.1.1.2.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2.2">ğ‘¦</ci><ci id="S3.SS2.p4.3.m3.1.1.2.3.cmml" xref="S3.SS2.p4.3.m3.1.1.2.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">\hat{y_{i}}</annotation></semantics></math> with a categories of <math id="S3.SS2.p4.4.m4.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.p4.4.m4.1a"><mi id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><ci id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">M</annotation></semantics></math> regions, the cross entropy loss is defined as:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\mathcal{L_{\text{ce}}}=\frac{1}{N_{i}}\sum_{i}^{N_{i}}\sum_{c=1}^{M}y_{ic}\log{(\hat{y_{ic}})}" display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.3" xref="S3.E1.m1.2.3.cmml"><msub id="S3.E1.m1.2.3.2" xref="S3.E1.m1.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.3.2.2" xref="S3.E1.m1.2.3.2.2.cmml">â„’</mi><mtext id="S3.E1.m1.2.3.2.3" xref="S3.E1.m1.2.3.2.3a.cmml">ce</mtext></msub><mo id="S3.E1.m1.2.3.1" xref="S3.E1.m1.2.3.1.cmml">=</mo><mrow id="S3.E1.m1.2.3.3" xref="S3.E1.m1.2.3.3.cmml"><mfrac id="S3.E1.m1.2.3.3.2" xref="S3.E1.m1.2.3.3.2.cmml"><mn id="S3.E1.m1.2.3.3.2.2" xref="S3.E1.m1.2.3.3.2.2.cmml">1</mn><msub id="S3.E1.m1.2.3.3.2.3" xref="S3.E1.m1.2.3.3.2.3.cmml"><mi id="S3.E1.m1.2.3.3.2.3.2" xref="S3.E1.m1.2.3.3.2.3.2.cmml">N</mi><mi id="S3.E1.m1.2.3.3.2.3.3" xref="S3.E1.m1.2.3.3.2.3.3.cmml">i</mi></msub></mfrac><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.3.3.1" xref="S3.E1.m1.2.3.3.1.cmml">â€‹</mo><mrow id="S3.E1.m1.2.3.3.3" xref="S3.E1.m1.2.3.3.3.cmml"><munderover id="S3.E1.m1.2.3.3.3.1" xref="S3.E1.m1.2.3.3.3.1.cmml"><mo movablelimits="false" rspace="0em" id="S3.E1.m1.2.3.3.3.1.2.2" xref="S3.E1.m1.2.3.3.3.1.2.2.cmml">âˆ‘</mo><mi id="S3.E1.m1.2.3.3.3.1.2.3" xref="S3.E1.m1.2.3.3.3.1.2.3.cmml">i</mi><msub id="S3.E1.m1.2.3.3.3.1.3" xref="S3.E1.m1.2.3.3.3.1.3.cmml"><mi id="S3.E1.m1.2.3.3.3.1.3.2" xref="S3.E1.m1.2.3.3.3.1.3.2.cmml">N</mi><mi id="S3.E1.m1.2.3.3.3.1.3.3" xref="S3.E1.m1.2.3.3.3.1.3.3.cmml">i</mi></msub></munderover><mrow id="S3.E1.m1.2.3.3.3.2" xref="S3.E1.m1.2.3.3.3.2.cmml"><munderover id="S3.E1.m1.2.3.3.3.2.1" xref="S3.E1.m1.2.3.3.3.2.1.cmml"><mo movablelimits="false" id="S3.E1.m1.2.3.3.3.2.1.2.2" xref="S3.E1.m1.2.3.3.3.2.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E1.m1.2.3.3.3.2.1.2.3" xref="S3.E1.m1.2.3.3.3.2.1.2.3.cmml"><mi id="S3.E1.m1.2.3.3.3.2.1.2.3.2" xref="S3.E1.m1.2.3.3.3.2.1.2.3.2.cmml">c</mi><mo id="S3.E1.m1.2.3.3.3.2.1.2.3.1" xref="S3.E1.m1.2.3.3.3.2.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.2.3.3.3.2.1.2.3.3" xref="S3.E1.m1.2.3.3.3.2.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.2.3.3.3.2.1.3" xref="S3.E1.m1.2.3.3.3.2.1.3.cmml">M</mi></munderover><mrow id="S3.E1.m1.2.3.3.3.2.2" xref="S3.E1.m1.2.3.3.3.2.2.cmml"><msub id="S3.E1.m1.2.3.3.3.2.2.2" xref="S3.E1.m1.2.3.3.3.2.2.2.cmml"><mi id="S3.E1.m1.2.3.3.3.2.2.2.2" xref="S3.E1.m1.2.3.3.3.2.2.2.2.cmml">y</mi><mrow id="S3.E1.m1.2.3.3.3.2.2.2.3" xref="S3.E1.m1.2.3.3.3.2.2.2.3.cmml"><mi id="S3.E1.m1.2.3.3.3.2.2.2.3.2" xref="S3.E1.m1.2.3.3.3.2.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.3.3.3.2.2.2.3.1" xref="S3.E1.m1.2.3.3.3.2.2.2.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.2.3.3.3.2.2.2.3.3" xref="S3.E1.m1.2.3.3.3.2.2.2.3.3.cmml">c</mi></mrow></msub><mo lspace="0.167em" rspace="0em" id="S3.E1.m1.2.3.3.3.2.2.1" xref="S3.E1.m1.2.3.3.3.2.2.1.cmml">â€‹</mo><mrow id="S3.E1.m1.2.3.3.3.2.2.3.2" xref="S3.E1.m1.2.3.3.3.2.2.3.1.cmml"><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">log</mi><mo id="S3.E1.m1.2.3.3.3.2.2.3.2a" xref="S3.E1.m1.2.3.3.3.2.2.3.1.cmml">â¡</mo><mrow id="S3.E1.m1.2.3.3.3.2.2.3.2.1" xref="S3.E1.m1.2.3.3.3.2.2.3.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.3.3.3.2.2.3.2.1.1" xref="S3.E1.m1.2.3.3.3.2.2.3.1.cmml">(</mo><mover accent="true" id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><msub id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">y</mi><mrow id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml"><mi id="S3.E1.m1.2.2.2.3.2" xref="S3.E1.m1.2.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.3.1" xref="S3.E1.m1.2.2.2.3.1.cmml">â€‹</mo><mi id="S3.E1.m1.2.2.2.3.3" xref="S3.E1.m1.2.2.2.3.3.cmml">c</mi></mrow></msub><mo id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.cmml">^</mo></mover><mo stretchy="false" id="S3.E1.m1.2.3.3.3.2.2.3.2.1.2" xref="S3.E1.m1.2.3.3.3.2.2.3.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.3.cmml" xref="S3.E1.m1.2.3"><eq id="S3.E1.m1.2.3.1.cmml" xref="S3.E1.m1.2.3.1"></eq><apply id="S3.E1.m1.2.3.2.cmml" xref="S3.E1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.2.1.cmml" xref="S3.E1.m1.2.3.2">subscript</csymbol><ci id="S3.E1.m1.2.3.2.2.cmml" xref="S3.E1.m1.2.3.2.2">â„’</ci><ci id="S3.E1.m1.2.3.2.3a.cmml" xref="S3.E1.m1.2.3.2.3"><mtext mathsize="70%" id="S3.E1.m1.2.3.2.3.cmml" xref="S3.E1.m1.2.3.2.3">ce</mtext></ci></apply><apply id="S3.E1.m1.2.3.3.cmml" xref="S3.E1.m1.2.3.3"><times id="S3.E1.m1.2.3.3.1.cmml" xref="S3.E1.m1.2.3.3.1"></times><apply id="S3.E1.m1.2.3.3.2.cmml" xref="S3.E1.m1.2.3.3.2"><divide id="S3.E1.m1.2.3.3.2.1.cmml" xref="S3.E1.m1.2.3.3.2"></divide><cn type="integer" id="S3.E1.m1.2.3.3.2.2.cmml" xref="S3.E1.m1.2.3.3.2.2">1</cn><apply id="S3.E1.m1.2.3.3.2.3.cmml" xref="S3.E1.m1.2.3.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.2.3.1.cmml" xref="S3.E1.m1.2.3.3.2.3">subscript</csymbol><ci id="S3.E1.m1.2.3.3.2.3.2.cmml" xref="S3.E1.m1.2.3.3.2.3.2">ğ‘</ci><ci id="S3.E1.m1.2.3.3.2.3.3.cmml" xref="S3.E1.m1.2.3.3.2.3.3">ğ‘–</ci></apply></apply><apply id="S3.E1.m1.2.3.3.3.cmml" xref="S3.E1.m1.2.3.3.3"><apply id="S3.E1.m1.2.3.3.3.1.cmml" xref="S3.E1.m1.2.3.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.3.1.1.cmml" xref="S3.E1.m1.2.3.3.3.1">superscript</csymbol><apply id="S3.E1.m1.2.3.3.3.1.2.cmml" xref="S3.E1.m1.2.3.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.3.1.2.1.cmml" xref="S3.E1.m1.2.3.3.3.1">subscript</csymbol><sum id="S3.E1.m1.2.3.3.3.1.2.2.cmml" xref="S3.E1.m1.2.3.3.3.1.2.2"></sum><ci id="S3.E1.m1.2.3.3.3.1.2.3.cmml" xref="S3.E1.m1.2.3.3.3.1.2.3">ğ‘–</ci></apply><apply id="S3.E1.m1.2.3.3.3.1.3.cmml" xref="S3.E1.m1.2.3.3.3.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.3.1.3.1.cmml" xref="S3.E1.m1.2.3.3.3.1.3">subscript</csymbol><ci id="S3.E1.m1.2.3.3.3.1.3.2.cmml" xref="S3.E1.m1.2.3.3.3.1.3.2">ğ‘</ci><ci id="S3.E1.m1.2.3.3.3.1.3.3.cmml" xref="S3.E1.m1.2.3.3.3.1.3.3">ğ‘–</ci></apply></apply><apply id="S3.E1.m1.2.3.3.3.2.cmml" xref="S3.E1.m1.2.3.3.3.2"><apply id="S3.E1.m1.2.3.3.3.2.1.cmml" xref="S3.E1.m1.2.3.3.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.3.2.1.1.cmml" xref="S3.E1.m1.2.3.3.3.2.1">superscript</csymbol><apply id="S3.E1.m1.2.3.3.3.2.1.2.cmml" xref="S3.E1.m1.2.3.3.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.3.2.1.2.1.cmml" xref="S3.E1.m1.2.3.3.3.2.1">subscript</csymbol><sum id="S3.E1.m1.2.3.3.3.2.1.2.2.cmml" xref="S3.E1.m1.2.3.3.3.2.1.2.2"></sum><apply id="S3.E1.m1.2.3.3.3.2.1.2.3.cmml" xref="S3.E1.m1.2.3.3.3.2.1.2.3"><eq id="S3.E1.m1.2.3.3.3.2.1.2.3.1.cmml" xref="S3.E1.m1.2.3.3.3.2.1.2.3.1"></eq><ci id="S3.E1.m1.2.3.3.3.2.1.2.3.2.cmml" xref="S3.E1.m1.2.3.3.3.2.1.2.3.2">ğ‘</ci><cn type="integer" id="S3.E1.m1.2.3.3.3.2.1.2.3.3.cmml" xref="S3.E1.m1.2.3.3.3.2.1.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.2.3.3.3.2.1.3.cmml" xref="S3.E1.m1.2.3.3.3.2.1.3">ğ‘€</ci></apply><apply id="S3.E1.m1.2.3.3.3.2.2.cmml" xref="S3.E1.m1.2.3.3.3.2.2"><times id="S3.E1.m1.2.3.3.3.2.2.1.cmml" xref="S3.E1.m1.2.3.3.3.2.2.1"></times><apply id="S3.E1.m1.2.3.3.3.2.2.2.cmml" xref="S3.E1.m1.2.3.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.3.2.2.2.1.cmml" xref="S3.E1.m1.2.3.3.3.2.2.2">subscript</csymbol><ci id="S3.E1.m1.2.3.3.3.2.2.2.2.cmml" xref="S3.E1.m1.2.3.3.3.2.2.2.2">ğ‘¦</ci><apply id="S3.E1.m1.2.3.3.3.2.2.2.3.cmml" xref="S3.E1.m1.2.3.3.3.2.2.2.3"><times id="S3.E1.m1.2.3.3.3.2.2.2.3.1.cmml" xref="S3.E1.m1.2.3.3.3.2.2.2.3.1"></times><ci id="S3.E1.m1.2.3.3.3.2.2.2.3.2.cmml" xref="S3.E1.m1.2.3.3.3.2.2.2.3.2">ğ‘–</ci><ci id="S3.E1.m1.2.3.3.3.2.2.2.3.3.cmml" xref="S3.E1.m1.2.3.3.3.2.2.2.3.3">ğ‘</ci></apply></apply><apply id="S3.E1.m1.2.3.3.3.2.2.3.1.cmml" xref="S3.E1.m1.2.3.3.3.2.2.3.2"><log id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"></log><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><ci id="S3.E1.m1.2.2.1.cmml" xref="S3.E1.m1.2.2.1">^</ci><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">ğ‘¦</ci><apply id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"><times id="S3.E1.m1.2.2.2.3.1.cmml" xref="S3.E1.m1.2.2.2.3.1"></times><ci id="S3.E1.m1.2.2.2.3.2.cmml" xref="S3.E1.m1.2.2.2.3.2">ğ‘–</ci><ci id="S3.E1.m1.2.2.2.3.3.cmml" xref="S3.E1.m1.2.2.2.3.3">ğ‘</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\mathcal{L_{\text{ce}}}=\frac{1}{N_{i}}\sum_{i}^{N_{i}}\sum_{c=1}^{M}y_{ic}\log{(\hat{y_{ic}})}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.p4.5" class="ltx_p">where <math id="S3.SS2.p4.5.m1.1" class="ltx_Math" alttext="y_{ic}" display="inline"><semantics id="S3.SS2.p4.5.m1.1a"><msub id="S3.SS2.p4.5.m1.1.1" xref="S3.SS2.p4.5.m1.1.1.cmml"><mi id="S3.SS2.p4.5.m1.1.1.2" xref="S3.SS2.p4.5.m1.1.1.2.cmml">y</mi><mrow id="S3.SS2.p4.5.m1.1.1.3" xref="S3.SS2.p4.5.m1.1.1.3.cmml"><mi id="S3.SS2.p4.5.m1.1.1.3.2" xref="S3.SS2.p4.5.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p4.5.m1.1.1.3.1" xref="S3.SS2.p4.5.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p4.5.m1.1.1.3.3" xref="S3.SS2.p4.5.m1.1.1.3.3.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m1.1b"><apply id="S3.SS2.p4.5.m1.1.1.cmml" xref="S3.SS2.p4.5.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.5.m1.1.1.1.cmml" xref="S3.SS2.p4.5.m1.1.1">subscript</csymbol><ci id="S3.SS2.p4.5.m1.1.1.2.cmml" xref="S3.SS2.p4.5.m1.1.1.2">ğ‘¦</ci><apply id="S3.SS2.p4.5.m1.1.1.3.cmml" xref="S3.SS2.p4.5.m1.1.1.3"><times id="S3.SS2.p4.5.m1.1.1.3.1.cmml" xref="S3.SS2.p4.5.m1.1.1.3.1"></times><ci id="S3.SS2.p4.5.m1.1.1.3.2.cmml" xref="S3.SS2.p4.5.m1.1.1.3.2">ğ‘–</ci><ci id="S3.SS2.p4.5.m1.1.1.3.3.cmml" xref="S3.SS2.p4.5.m1.1.1.3.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m1.1c">y_{ic}</annotation></semantics></math> denotes the one-hot encoded vector for the i-th generated segmentation belonging to the c-th category. For generated segmentation, different classes occupy varying proportions of areas. Semantically important regions like lips and eyes constitute small fractions, while background dominates most areas. To mitigate the class imbalance issue, a weighted cross entropy is formulated as:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.2" class="ltx_Math" alttext="\mathcal{L_{\text{w-ce}}}=\frac{1}{N_{i}}\sum_{i}^{N_{i}}\sum_{c=1}^{M}w_{c}\ y_{ic}\log{(\hat{y_{ic}})}" display="block"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.3" xref="S3.E2.m1.2.3.cmml"><msub id="S3.E2.m1.2.3.2" xref="S3.E2.m1.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.2.3.2.2" xref="S3.E2.m1.2.3.2.2.cmml">â„’</mi><mtext id="S3.E2.m1.2.3.2.3" xref="S3.E2.m1.2.3.2.3a.cmml">w-ce</mtext></msub><mo id="S3.E2.m1.2.3.1" xref="S3.E2.m1.2.3.1.cmml">=</mo><mrow id="S3.E2.m1.2.3.3" xref="S3.E2.m1.2.3.3.cmml"><mfrac id="S3.E2.m1.2.3.3.2" xref="S3.E2.m1.2.3.3.2.cmml"><mn id="S3.E2.m1.2.3.3.2.2" xref="S3.E2.m1.2.3.3.2.2.cmml">1</mn><msub id="S3.E2.m1.2.3.3.2.3" xref="S3.E2.m1.2.3.3.2.3.cmml"><mi id="S3.E2.m1.2.3.3.2.3.2" xref="S3.E2.m1.2.3.3.2.3.2.cmml">N</mi><mi id="S3.E2.m1.2.3.3.2.3.3" xref="S3.E2.m1.2.3.3.2.3.3.cmml">i</mi></msub></mfrac><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.3.3.1" xref="S3.E2.m1.2.3.3.1.cmml">â€‹</mo><mrow id="S3.E2.m1.2.3.3.3" xref="S3.E2.m1.2.3.3.3.cmml"><munderover id="S3.E2.m1.2.3.3.3.1" xref="S3.E2.m1.2.3.3.3.1.cmml"><mo movablelimits="false" rspace="0em" id="S3.E2.m1.2.3.3.3.1.2.2" xref="S3.E2.m1.2.3.3.3.1.2.2.cmml">âˆ‘</mo><mi id="S3.E2.m1.2.3.3.3.1.2.3" xref="S3.E2.m1.2.3.3.3.1.2.3.cmml">i</mi><msub id="S3.E2.m1.2.3.3.3.1.3" xref="S3.E2.m1.2.3.3.3.1.3.cmml"><mi id="S3.E2.m1.2.3.3.3.1.3.2" xref="S3.E2.m1.2.3.3.3.1.3.2.cmml">N</mi><mi id="S3.E2.m1.2.3.3.3.1.3.3" xref="S3.E2.m1.2.3.3.3.1.3.3.cmml">i</mi></msub></munderover><mrow id="S3.E2.m1.2.3.3.3.2" xref="S3.E2.m1.2.3.3.3.2.cmml"><munderover id="S3.E2.m1.2.3.3.3.2.1" xref="S3.E2.m1.2.3.3.3.2.1.cmml"><mo movablelimits="false" id="S3.E2.m1.2.3.3.3.2.1.2.2" xref="S3.E2.m1.2.3.3.3.2.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E2.m1.2.3.3.3.2.1.2.3" xref="S3.E2.m1.2.3.3.3.2.1.2.3.cmml"><mi id="S3.E2.m1.2.3.3.3.2.1.2.3.2" xref="S3.E2.m1.2.3.3.3.2.1.2.3.2.cmml">c</mi><mo id="S3.E2.m1.2.3.3.3.2.1.2.3.1" xref="S3.E2.m1.2.3.3.3.2.1.2.3.1.cmml">=</mo><mn id="S3.E2.m1.2.3.3.3.2.1.2.3.3" xref="S3.E2.m1.2.3.3.3.2.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E2.m1.2.3.3.3.2.1.3" xref="S3.E2.m1.2.3.3.3.2.1.3.cmml">M</mi></munderover><mrow id="S3.E2.m1.2.3.3.3.2.2" xref="S3.E2.m1.2.3.3.3.2.2.cmml"><msub id="S3.E2.m1.2.3.3.3.2.2.2" xref="S3.E2.m1.2.3.3.3.2.2.2.cmml"><mi id="S3.E2.m1.2.3.3.3.2.2.2.2" xref="S3.E2.m1.2.3.3.3.2.2.2.2.cmml">w</mi><mi id="S3.E2.m1.2.3.3.3.2.2.2.3" xref="S3.E2.m1.2.3.3.3.2.2.2.3.cmml">c</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.3.3.3.2.2.1" xref="S3.E2.m1.2.3.3.3.2.2.1.cmml">â€‹</mo><msub id="S3.E2.m1.2.3.3.3.2.2.3" xref="S3.E2.m1.2.3.3.3.2.2.3.cmml"><mi id="S3.E2.m1.2.3.3.3.2.2.3.2" xref="S3.E2.m1.2.3.3.3.2.2.3.2.cmml">y</mi><mrow id="S3.E2.m1.2.3.3.3.2.2.3.3" xref="S3.E2.m1.2.3.3.3.2.2.3.3.cmml"><mi id="S3.E2.m1.2.3.3.3.2.2.3.3.2" xref="S3.E2.m1.2.3.3.3.2.2.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.3.3.3.2.2.3.3.1" xref="S3.E2.m1.2.3.3.3.2.2.3.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.3.3.3.2.2.3.3.3" xref="S3.E2.m1.2.3.3.3.2.2.3.3.3.cmml">c</mi></mrow></msub><mo lspace="0.167em" rspace="0em" id="S3.E2.m1.2.3.3.3.2.2.1a" xref="S3.E2.m1.2.3.3.3.2.2.1.cmml">â€‹</mo><mrow id="S3.E2.m1.2.3.3.3.2.2.4.2" xref="S3.E2.m1.2.3.3.3.2.2.4.1.cmml"><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">log</mi><mo id="S3.E2.m1.2.3.3.3.2.2.4.2a" xref="S3.E2.m1.2.3.3.3.2.2.4.1.cmml">â¡</mo><mrow id="S3.E2.m1.2.3.3.3.2.2.4.2.1" xref="S3.E2.m1.2.3.3.3.2.2.4.1.cmml"><mo stretchy="false" id="S3.E2.m1.2.3.3.3.2.2.4.2.1.1" xref="S3.E2.m1.2.3.3.3.2.2.4.1.cmml">(</mo><mover accent="true" id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><msub id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml"><mi id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml">y</mi><mrow id="S3.E2.m1.2.2.2.3" xref="S3.E2.m1.2.2.2.3.cmml"><mi id="S3.E2.m1.2.2.2.3.2" xref="S3.E2.m1.2.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.2.2.2.3.1" xref="S3.E2.m1.2.2.2.3.1.cmml">â€‹</mo><mi id="S3.E2.m1.2.2.2.3.3" xref="S3.E2.m1.2.2.2.3.3.cmml">c</mi></mrow></msub><mo id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml">^</mo></mover><mo stretchy="false" id="S3.E2.m1.2.3.3.3.2.2.4.2.1.2" xref="S3.E2.m1.2.3.3.3.2.2.4.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.3.cmml" xref="S3.E2.m1.2.3"><eq id="S3.E2.m1.2.3.1.cmml" xref="S3.E2.m1.2.3.1"></eq><apply id="S3.E2.m1.2.3.2.cmml" xref="S3.E2.m1.2.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.3.2.1.cmml" xref="S3.E2.m1.2.3.2">subscript</csymbol><ci id="S3.E2.m1.2.3.2.2.cmml" xref="S3.E2.m1.2.3.2.2">â„’</ci><ci id="S3.E2.m1.2.3.2.3a.cmml" xref="S3.E2.m1.2.3.2.3"><mtext mathsize="70%" id="S3.E2.m1.2.3.2.3.cmml" xref="S3.E2.m1.2.3.2.3">w-ce</mtext></ci></apply><apply id="S3.E2.m1.2.3.3.cmml" xref="S3.E2.m1.2.3.3"><times id="S3.E2.m1.2.3.3.1.cmml" xref="S3.E2.m1.2.3.3.1"></times><apply id="S3.E2.m1.2.3.3.2.cmml" xref="S3.E2.m1.2.3.3.2"><divide id="S3.E2.m1.2.3.3.2.1.cmml" xref="S3.E2.m1.2.3.3.2"></divide><cn type="integer" id="S3.E2.m1.2.3.3.2.2.cmml" xref="S3.E2.m1.2.3.3.2.2">1</cn><apply id="S3.E2.m1.2.3.3.2.3.cmml" xref="S3.E2.m1.2.3.3.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.3.3.2.3.1.cmml" xref="S3.E2.m1.2.3.3.2.3">subscript</csymbol><ci id="S3.E2.m1.2.3.3.2.3.2.cmml" xref="S3.E2.m1.2.3.3.2.3.2">ğ‘</ci><ci id="S3.E2.m1.2.3.3.2.3.3.cmml" xref="S3.E2.m1.2.3.3.2.3.3">ğ‘–</ci></apply></apply><apply id="S3.E2.m1.2.3.3.3.cmml" xref="S3.E2.m1.2.3.3.3"><apply id="S3.E2.m1.2.3.3.3.1.cmml" xref="S3.E2.m1.2.3.3.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.3.3.3.1.1.cmml" xref="S3.E2.m1.2.3.3.3.1">superscript</csymbol><apply id="S3.E2.m1.2.3.3.3.1.2.cmml" xref="S3.E2.m1.2.3.3.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.3.3.3.1.2.1.cmml" xref="S3.E2.m1.2.3.3.3.1">subscript</csymbol><sum id="S3.E2.m1.2.3.3.3.1.2.2.cmml" xref="S3.E2.m1.2.3.3.3.1.2.2"></sum><ci id="S3.E2.m1.2.3.3.3.1.2.3.cmml" xref="S3.E2.m1.2.3.3.3.1.2.3">ğ‘–</ci></apply><apply id="S3.E2.m1.2.3.3.3.1.3.cmml" xref="S3.E2.m1.2.3.3.3.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.3.3.3.1.3.1.cmml" xref="S3.E2.m1.2.3.3.3.1.3">subscript</csymbol><ci id="S3.E2.m1.2.3.3.3.1.3.2.cmml" xref="S3.E2.m1.2.3.3.3.1.3.2">ğ‘</ci><ci id="S3.E2.m1.2.3.3.3.1.3.3.cmml" xref="S3.E2.m1.2.3.3.3.1.3.3">ğ‘–</ci></apply></apply><apply id="S3.E2.m1.2.3.3.3.2.cmml" xref="S3.E2.m1.2.3.3.3.2"><apply id="S3.E2.m1.2.3.3.3.2.1.cmml" xref="S3.E2.m1.2.3.3.3.2.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.3.3.3.2.1.1.cmml" xref="S3.E2.m1.2.3.3.3.2.1">superscript</csymbol><apply id="S3.E2.m1.2.3.3.3.2.1.2.cmml" xref="S3.E2.m1.2.3.3.3.2.1"><csymbol cd="ambiguous" id="S3.E2.m1.2.3.3.3.2.1.2.1.cmml" xref="S3.E2.m1.2.3.3.3.2.1">subscript</csymbol><sum id="S3.E2.m1.2.3.3.3.2.1.2.2.cmml" xref="S3.E2.m1.2.3.3.3.2.1.2.2"></sum><apply id="S3.E2.m1.2.3.3.3.2.1.2.3.cmml" xref="S3.E2.m1.2.3.3.3.2.1.2.3"><eq id="S3.E2.m1.2.3.3.3.2.1.2.3.1.cmml" xref="S3.E2.m1.2.3.3.3.2.1.2.3.1"></eq><ci id="S3.E2.m1.2.3.3.3.2.1.2.3.2.cmml" xref="S3.E2.m1.2.3.3.3.2.1.2.3.2">ğ‘</ci><cn type="integer" id="S3.E2.m1.2.3.3.3.2.1.2.3.3.cmml" xref="S3.E2.m1.2.3.3.3.2.1.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.2.3.3.3.2.1.3.cmml" xref="S3.E2.m1.2.3.3.3.2.1.3">ğ‘€</ci></apply><apply id="S3.E2.m1.2.3.3.3.2.2.cmml" xref="S3.E2.m1.2.3.3.3.2.2"><times id="S3.E2.m1.2.3.3.3.2.2.1.cmml" xref="S3.E2.m1.2.3.3.3.2.2.1"></times><apply id="S3.E2.m1.2.3.3.3.2.2.2.cmml" xref="S3.E2.m1.2.3.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.3.3.3.2.2.2.1.cmml" xref="S3.E2.m1.2.3.3.3.2.2.2">subscript</csymbol><ci id="S3.E2.m1.2.3.3.3.2.2.2.2.cmml" xref="S3.E2.m1.2.3.3.3.2.2.2.2">ğ‘¤</ci><ci id="S3.E2.m1.2.3.3.3.2.2.2.3.cmml" xref="S3.E2.m1.2.3.3.3.2.2.2.3">ğ‘</ci></apply><apply id="S3.E2.m1.2.3.3.3.2.2.3.cmml" xref="S3.E2.m1.2.3.3.3.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.2.3.3.3.2.2.3.1.cmml" xref="S3.E2.m1.2.3.3.3.2.2.3">subscript</csymbol><ci id="S3.E2.m1.2.3.3.3.2.2.3.2.cmml" xref="S3.E2.m1.2.3.3.3.2.2.3.2">ğ‘¦</ci><apply id="S3.E2.m1.2.3.3.3.2.2.3.3.cmml" xref="S3.E2.m1.2.3.3.3.2.2.3.3"><times id="S3.E2.m1.2.3.3.3.2.2.3.3.1.cmml" xref="S3.E2.m1.2.3.3.3.2.2.3.3.1"></times><ci id="S3.E2.m1.2.3.3.3.2.2.3.3.2.cmml" xref="S3.E2.m1.2.3.3.3.2.2.3.3.2">ğ‘–</ci><ci id="S3.E2.m1.2.3.3.3.2.2.3.3.3.cmml" xref="S3.E2.m1.2.3.3.3.2.2.3.3.3">ğ‘</ci></apply></apply><apply id="S3.E2.m1.2.3.3.3.2.2.4.1.cmml" xref="S3.E2.m1.2.3.3.3.2.2.4.2"><log id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"></log><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><ci id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1">^</ci><apply id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2">subscript</csymbol><ci id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">ğ‘¦</ci><apply id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.3"><times id="S3.E2.m1.2.2.2.3.1.cmml" xref="S3.E2.m1.2.2.2.3.1"></times><ci id="S3.E2.m1.2.2.2.3.2.cmml" xref="S3.E2.m1.2.2.2.3.2">ğ‘–</ci><ci id="S3.E2.m1.2.2.2.3.3.cmml" xref="S3.E2.m1.2.2.2.3.3">ğ‘</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\mathcal{L_{\text{w-ce}}}=\frac{1}{N_{i}}\sum_{i}^{N_{i}}\sum_{c=1}^{M}w_{c}\ y_{ic}\log{(\hat{y_{ic}})}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.p4.6" class="ltx_p">where <math id="S3.SS2.p4.6.m1.1" class="ltx_Math" alttext="w_{c}" display="inline"><semantics id="S3.SS2.p4.6.m1.1a"><msub id="S3.SS2.p4.6.m1.1.1" xref="S3.SS2.p4.6.m1.1.1.cmml"><mi id="S3.SS2.p4.6.m1.1.1.2" xref="S3.SS2.p4.6.m1.1.1.2.cmml">w</mi><mi id="S3.SS2.p4.6.m1.1.1.3" xref="S3.SS2.p4.6.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m1.1b"><apply id="S3.SS2.p4.6.m1.1.1.cmml" xref="S3.SS2.p4.6.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.6.m1.1.1.1.cmml" xref="S3.SS2.p4.6.m1.1.1">subscript</csymbol><ci id="S3.SS2.p4.6.m1.1.1.2.cmml" xref="S3.SS2.p4.6.m1.1.1.2">ğ‘¤</ci><ci id="S3.SS2.p4.6.m1.1.1.3.cmml" xref="S3.SS2.p4.6.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m1.1c">w_{c}</annotation></semantics></math> denotes the weight for the corresponding category and is determined on the inverse proportionality of the areas of different regions on the whole dataset.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2409.03605/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="369" height="233" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Overview of the proposed <span id="S3.F2.4.1" class="ltx_text ltx_font_bold">SegTalker</span> framework for talking face generation. (a) talking segmentation generation (<span id="S3.F2.5.2" class="ltx_text ltx_font_bold">TSG</span>) module takes mel and mask as inputs, then synthesizes the talking segmentation with lip synchronized to input speech. (b) Given reference image and mask from TSG, segmentation-guided GAN injection (<span id="S3.F2.6.3" class="ltx_text ltx_font_bold">SGI</span>) network utilizes a mask-guided multi-scale encoder to extract different semantic region codes, then injects the style codes and synthesized mask from TSG into the mask-guided generator to obtain the final talking face image.</figcaption>
</figure>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.5" class="ltx_p"><span id="S3.SS2.p5.5.1" class="ltx_text ltx_font_bold">SyncNet Loss</span>
The reconstruction loss mainly restores images at the pixel level without effective semantic supervision. Therefore, we train a segmentation-domain SyncNet fromÂ <cite class="ltx_cite ltx_citemacro_citep">(Prajwal etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite> to supervise lip synchronization. During training, a speech chunk is randomly sampled from speech sequences, which can be either synchronized (positive example) or unsynchronized (negative example). The SyncNet consists of a speech encoder and a mask encoder. For the mask, we use one-hot encoding as input and concatenate <math id="S3.SS2.p5.1.m1.1" class="ltx_Math" alttext="T_{v}" display="inline"><semantics id="S3.SS2.p5.1.m1.1a"><msub id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml"><mi id="S3.SS2.p5.1.m1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.2.cmml">T</mi><mi id="S3.SS2.p5.1.m1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2">ğ‘‡</ci><ci id="S3.SS2.p5.1.m1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">T_{v}</annotation></semantics></math> masks along the channel dimension. Specifically, the SyncNet takes inputs of a window <math id="S3.SS2.p5.2.m2.1" class="ltx_Math" alttext="T_{v}" display="inline"><semantics id="S3.SS2.p5.2.m2.1a"><msub id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml"><mi id="S3.SS2.p5.2.m2.1.1.2" xref="S3.SS2.p5.2.m2.1.1.2.cmml">T</mi><mi id="S3.SS2.p5.2.m2.1.1.3" xref="S3.SS2.p5.2.m2.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><apply id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.2.m2.1.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p5.2.m2.1.1.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2">ğ‘‡</ci><ci id="S3.SS2.p5.2.m2.1.1.3.cmml" xref="S3.SS2.p5.2.m2.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">T_{v}</annotation></semantics></math> of consecutive lower-half frames and a speech segment <math id="S3.SS2.p5.3.m3.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS2.p5.3.m3.1a"><mi id="S3.SS2.p5.3.m3.1.1" xref="S3.SS2.p5.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.1b"><ci id="S3.SS2.p5.3.m3.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">S</annotation></semantics></math>. After passing through the speech encoder and mask encoder, 512-dim embeddings <math id="S3.SS2.p5.4.m4.1" class="ltx_Math" alttext="s=E_{\text{speech}}(S)" display="inline"><semantics id="S3.SS2.p5.4.m4.1a"><mrow id="S3.SS2.p5.4.m4.1.2" xref="S3.SS2.p5.4.m4.1.2.cmml"><mi id="S3.SS2.p5.4.m4.1.2.2" xref="S3.SS2.p5.4.m4.1.2.2.cmml">s</mi><mo id="S3.SS2.p5.4.m4.1.2.1" xref="S3.SS2.p5.4.m4.1.2.1.cmml">=</mo><mrow id="S3.SS2.p5.4.m4.1.2.3" xref="S3.SS2.p5.4.m4.1.2.3.cmml"><msub id="S3.SS2.p5.4.m4.1.2.3.2" xref="S3.SS2.p5.4.m4.1.2.3.2.cmml"><mi id="S3.SS2.p5.4.m4.1.2.3.2.2" xref="S3.SS2.p5.4.m4.1.2.3.2.2.cmml">E</mi><mtext id="S3.SS2.p5.4.m4.1.2.3.2.3" xref="S3.SS2.p5.4.m4.1.2.3.2.3a.cmml">speech</mtext></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p5.4.m4.1.2.3.1" xref="S3.SS2.p5.4.m4.1.2.3.1.cmml">â€‹</mo><mrow id="S3.SS2.p5.4.m4.1.2.3.3.2" xref="S3.SS2.p5.4.m4.1.2.3.cmml"><mo stretchy="false" id="S3.SS2.p5.4.m4.1.2.3.3.2.1" xref="S3.SS2.p5.4.m4.1.2.3.cmml">(</mo><mi id="S3.SS2.p5.4.m4.1.1" xref="S3.SS2.p5.4.m4.1.1.cmml">S</mi><mo stretchy="false" id="S3.SS2.p5.4.m4.1.2.3.3.2.2" xref="S3.SS2.p5.4.m4.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.4.m4.1b"><apply id="S3.SS2.p5.4.m4.1.2.cmml" xref="S3.SS2.p5.4.m4.1.2"><eq id="S3.SS2.p5.4.m4.1.2.1.cmml" xref="S3.SS2.p5.4.m4.1.2.1"></eq><ci id="S3.SS2.p5.4.m4.1.2.2.cmml" xref="S3.SS2.p5.4.m4.1.2.2">ğ‘ </ci><apply id="S3.SS2.p5.4.m4.1.2.3.cmml" xref="S3.SS2.p5.4.m4.1.2.3"><times id="S3.SS2.p5.4.m4.1.2.3.1.cmml" xref="S3.SS2.p5.4.m4.1.2.3.1"></times><apply id="S3.SS2.p5.4.m4.1.2.3.2.cmml" xref="S3.SS2.p5.4.m4.1.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.p5.4.m4.1.2.3.2.1.cmml" xref="S3.SS2.p5.4.m4.1.2.3.2">subscript</csymbol><ci id="S3.SS2.p5.4.m4.1.2.3.2.2.cmml" xref="S3.SS2.p5.4.m4.1.2.3.2.2">ğ¸</ci><ci id="S3.SS2.p5.4.m4.1.2.3.2.3a.cmml" xref="S3.SS2.p5.4.m4.1.2.3.2.3"><mtext mathsize="70%" id="S3.SS2.p5.4.m4.1.2.3.2.3.cmml" xref="S3.SS2.p5.4.m4.1.2.3.2.3">speech</mtext></ci></apply><ci id="S3.SS2.p5.4.m4.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1">ğ‘†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.4.m4.1c">s=E_{\text{speech}}(S)</annotation></semantics></math> and <math id="S3.SS2.p5.5.m5.1" class="ltx_Math" alttext="m=E_{\text{mask}}(T_{v})" display="inline"><semantics id="S3.SS2.p5.5.m5.1a"><mrow id="S3.SS2.p5.5.m5.1.1" xref="S3.SS2.p5.5.m5.1.1.cmml"><mi id="S3.SS2.p5.5.m5.1.1.3" xref="S3.SS2.p5.5.m5.1.1.3.cmml">m</mi><mo id="S3.SS2.p5.5.m5.1.1.2" xref="S3.SS2.p5.5.m5.1.1.2.cmml">=</mo><mrow id="S3.SS2.p5.5.m5.1.1.1" xref="S3.SS2.p5.5.m5.1.1.1.cmml"><msub id="S3.SS2.p5.5.m5.1.1.1.3" xref="S3.SS2.p5.5.m5.1.1.1.3.cmml"><mi id="S3.SS2.p5.5.m5.1.1.1.3.2" xref="S3.SS2.p5.5.m5.1.1.1.3.2.cmml">E</mi><mtext id="S3.SS2.p5.5.m5.1.1.1.3.3" xref="S3.SS2.p5.5.m5.1.1.1.3.3a.cmml">mask</mtext></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p5.5.m5.1.1.1.2" xref="S3.SS2.p5.5.m5.1.1.1.2.cmml">â€‹</mo><mrow id="S3.SS2.p5.5.m5.1.1.1.1.1" xref="S3.SS2.p5.5.m5.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p5.5.m5.1.1.1.1.1.2" xref="S3.SS2.p5.5.m5.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p5.5.m5.1.1.1.1.1.1" xref="S3.SS2.p5.5.m5.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p5.5.m5.1.1.1.1.1.1.2" xref="S3.SS2.p5.5.m5.1.1.1.1.1.1.2.cmml">T</mi><mi id="S3.SS2.p5.5.m5.1.1.1.1.1.1.3" xref="S3.SS2.p5.5.m5.1.1.1.1.1.1.3.cmml">v</mi></msub><mo stretchy="false" id="S3.SS2.p5.5.m5.1.1.1.1.1.3" xref="S3.SS2.p5.5.m5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.5.m5.1b"><apply id="S3.SS2.p5.5.m5.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1"><eq id="S3.SS2.p5.5.m5.1.1.2.cmml" xref="S3.SS2.p5.5.m5.1.1.2"></eq><ci id="S3.SS2.p5.5.m5.1.1.3.cmml" xref="S3.SS2.p5.5.m5.1.1.3">ğ‘š</ci><apply id="S3.SS2.p5.5.m5.1.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1.1"><times id="S3.SS2.p5.5.m5.1.1.1.2.cmml" xref="S3.SS2.p5.5.m5.1.1.1.2"></times><apply id="S3.SS2.p5.5.m5.1.1.1.3.cmml" xref="S3.SS2.p5.5.m5.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p5.5.m5.1.1.1.3.1.cmml" xref="S3.SS2.p5.5.m5.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p5.5.m5.1.1.1.3.2.cmml" xref="S3.SS2.p5.5.m5.1.1.1.3.2">ğ¸</ci><ci id="S3.SS2.p5.5.m5.1.1.1.3.3a.cmml" xref="S3.SS2.p5.5.m5.1.1.1.3.3"><mtext mathsize="70%" id="S3.SS2.p5.5.m5.1.1.1.3.3.cmml" xref="S3.SS2.p5.5.m5.1.1.1.3.3">mask</mtext></ci></apply><apply id="S3.SS2.p5.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p5.5.m5.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p5.5.m5.1.1.1.1.1.1.2">ğ‘‡</ci><ci id="S3.SS2.p5.5.m5.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p5.5.m5.1.1.1.1.1.1.3">ğ‘£</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.5.m5.1c">m=E_{\text{mask}}(T_{v})</annotation></semantics></math> are obtained respectively. Cosine similarity distance and binary cross entropy loss are then calculated between the embeddings. The losses are formally defined as:</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.5" class="ltx_Math" alttext="{P_{\text{sync}}}=\frac{s\cdot m}{\max({||s||}_{2}\cdot{||m||}_{2},\epsilon)}" display="block"><semantics id="S3.E3.m1.5a"><mrow id="S3.E3.m1.5.6" xref="S3.E3.m1.5.6.cmml"><msub id="S3.E3.m1.5.6.2" xref="S3.E3.m1.5.6.2.cmml"><mi id="S3.E3.m1.5.6.2.2" xref="S3.E3.m1.5.6.2.2.cmml">P</mi><mtext id="S3.E3.m1.5.6.2.3" xref="S3.E3.m1.5.6.2.3a.cmml">sync</mtext></msub><mo id="S3.E3.m1.5.6.1" xref="S3.E3.m1.5.6.1.cmml">=</mo><mfrac id="S3.E3.m1.5.5" xref="S3.E3.m1.5.5.cmml"><mrow id="S3.E3.m1.5.5.7" xref="S3.E3.m1.5.5.7.cmml"><mi id="S3.E3.m1.5.5.7.2" xref="S3.E3.m1.5.5.7.2.cmml">s</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.5.5.7.1" xref="S3.E3.m1.5.5.7.1.cmml">â‹…</mo><mi id="S3.E3.m1.5.5.7.3" xref="S3.E3.m1.5.5.7.3.cmml">m</mi></mrow><mrow id="S3.E3.m1.5.5.5.5" xref="S3.E3.m1.5.5.5.6.cmml"><mi id="S3.E3.m1.3.3.3.3" xref="S3.E3.m1.3.3.3.3.cmml">max</mi><mo id="S3.E3.m1.5.5.5.5a" xref="S3.E3.m1.5.5.5.6.cmml">â¡</mo><mrow id="S3.E3.m1.5.5.5.5.1" xref="S3.E3.m1.5.5.5.6.cmml"><mo stretchy="false" id="S3.E3.m1.5.5.5.5.1.2" xref="S3.E3.m1.5.5.5.6.cmml">(</mo><mrow id="S3.E3.m1.5.5.5.5.1.1" xref="S3.E3.m1.5.5.5.5.1.1.cmml"><msub id="S3.E3.m1.5.5.5.5.1.1.2" xref="S3.E3.m1.5.5.5.5.1.1.2.cmml"><mrow id="S3.E3.m1.5.5.5.5.1.1.2.2.2" xref="S3.E3.m1.5.5.5.5.1.1.2.2.1.cmml"><mo stretchy="false" id="S3.E3.m1.5.5.5.5.1.1.2.2.2.1" xref="S3.E3.m1.5.5.5.5.1.1.2.2.1.1.cmml">â€–</mo><mi id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">s</mi><mo rspace="0.055em" stretchy="false" id="S3.E3.m1.5.5.5.5.1.1.2.2.2.2" xref="S3.E3.m1.5.5.5.5.1.1.2.2.1.1.cmml">â€–</mo></mrow><mn id="S3.E3.m1.5.5.5.5.1.1.2.3" xref="S3.E3.m1.5.5.5.5.1.1.2.3.cmml">2</mn></msub><mo rspace="0.222em" id="S3.E3.m1.5.5.5.5.1.1.1" xref="S3.E3.m1.5.5.5.5.1.1.1.cmml">â‹…</mo><msub id="S3.E3.m1.5.5.5.5.1.1.3" xref="S3.E3.m1.5.5.5.5.1.1.3.cmml"><mrow id="S3.E3.m1.5.5.5.5.1.1.3.2.2" xref="S3.E3.m1.5.5.5.5.1.1.3.2.1.cmml"><mo stretchy="false" id="S3.E3.m1.5.5.5.5.1.1.3.2.2.1" xref="S3.E3.m1.5.5.5.5.1.1.3.2.1.1.cmml">â€–</mo><mi id="S3.E3.m1.2.2.2.2" xref="S3.E3.m1.2.2.2.2.cmml">m</mi><mo stretchy="false" id="S3.E3.m1.5.5.5.5.1.1.3.2.2.2" xref="S3.E3.m1.5.5.5.5.1.1.3.2.1.1.cmml">â€–</mo></mrow><mn id="S3.E3.m1.5.5.5.5.1.1.3.3" xref="S3.E3.m1.5.5.5.5.1.1.3.3.cmml">2</mn></msub></mrow><mo id="S3.E3.m1.5.5.5.5.1.3" xref="S3.E3.m1.5.5.5.6.cmml">,</mo><mi id="S3.E3.m1.4.4.4.4" xref="S3.E3.m1.4.4.4.4.cmml">Ïµ</mi><mo stretchy="false" id="S3.E3.m1.5.5.5.5.1.4" xref="S3.E3.m1.5.5.5.6.cmml">)</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.5b"><apply id="S3.E3.m1.5.6.cmml" xref="S3.E3.m1.5.6"><eq id="S3.E3.m1.5.6.1.cmml" xref="S3.E3.m1.5.6.1"></eq><apply id="S3.E3.m1.5.6.2.cmml" xref="S3.E3.m1.5.6.2"><csymbol cd="ambiguous" id="S3.E3.m1.5.6.2.1.cmml" xref="S3.E3.m1.5.6.2">subscript</csymbol><ci id="S3.E3.m1.5.6.2.2.cmml" xref="S3.E3.m1.5.6.2.2">ğ‘ƒ</ci><ci id="S3.E3.m1.5.6.2.3a.cmml" xref="S3.E3.m1.5.6.2.3"><mtext mathsize="70%" id="S3.E3.m1.5.6.2.3.cmml" xref="S3.E3.m1.5.6.2.3">sync</mtext></ci></apply><apply id="S3.E3.m1.5.5.cmml" xref="S3.E3.m1.5.5"><divide id="S3.E3.m1.5.5.6.cmml" xref="S3.E3.m1.5.5"></divide><apply id="S3.E3.m1.5.5.7.cmml" xref="S3.E3.m1.5.5.7"><ci id="S3.E3.m1.5.5.7.1.cmml" xref="S3.E3.m1.5.5.7.1">â‹…</ci><ci id="S3.E3.m1.5.5.7.2.cmml" xref="S3.E3.m1.5.5.7.2">ğ‘ </ci><ci id="S3.E3.m1.5.5.7.3.cmml" xref="S3.E3.m1.5.5.7.3">ğ‘š</ci></apply><apply id="S3.E3.m1.5.5.5.6.cmml" xref="S3.E3.m1.5.5.5.5"><max id="S3.E3.m1.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3"></max><apply id="S3.E3.m1.5.5.5.5.1.1.cmml" xref="S3.E3.m1.5.5.5.5.1.1"><ci id="S3.E3.m1.5.5.5.5.1.1.1.cmml" xref="S3.E3.m1.5.5.5.5.1.1.1">â‹…</ci><apply id="S3.E3.m1.5.5.5.5.1.1.2.cmml" xref="S3.E3.m1.5.5.5.5.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.5.5.1.1.2.1.cmml" xref="S3.E3.m1.5.5.5.5.1.1.2">subscript</csymbol><apply id="S3.E3.m1.5.5.5.5.1.1.2.2.1.cmml" xref="S3.E3.m1.5.5.5.5.1.1.2.2.2"><csymbol cd="latexml" id="S3.E3.m1.5.5.5.5.1.1.2.2.1.1.cmml" xref="S3.E3.m1.5.5.5.5.1.1.2.2.2.1">norm</csymbol><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">ğ‘ </ci></apply><cn type="integer" id="S3.E3.m1.5.5.5.5.1.1.2.3.cmml" xref="S3.E3.m1.5.5.5.5.1.1.2.3">2</cn></apply><apply id="S3.E3.m1.5.5.5.5.1.1.3.cmml" xref="S3.E3.m1.5.5.5.5.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.5.5.1.1.3.1.cmml" xref="S3.E3.m1.5.5.5.5.1.1.3">subscript</csymbol><apply id="S3.E3.m1.5.5.5.5.1.1.3.2.1.cmml" xref="S3.E3.m1.5.5.5.5.1.1.3.2.2"><csymbol cd="latexml" id="S3.E3.m1.5.5.5.5.1.1.3.2.1.1.cmml" xref="S3.E3.m1.5.5.5.5.1.1.3.2.2.1">norm</csymbol><ci id="S3.E3.m1.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2">ğ‘š</ci></apply><cn type="integer" id="S3.E3.m1.5.5.5.5.1.1.3.3.cmml" xref="S3.E3.m1.5.5.5.5.1.1.3.3">2</cn></apply></apply><ci id="S3.E3.m1.4.4.4.4.cmml" xref="S3.E3.m1.4.4.4.4">italic-Ïµ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.5c">{P_{\text{sync}}}=\frac{s\cdot m}{\max({||s||}_{2}\cdot{||m||}_{2},\epsilon)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.2" class="ltx_Math" alttext="\mathcal{L_{\text{sync}}}=\frac{1}{N}\sum_{i}^{N}-\log(P^{i}_{\text{sync}})" display="block"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml"><msub id="S3.E4.m1.2.2.3" xref="S3.E4.m1.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4.m1.2.2.3.2" xref="S3.E4.m1.2.2.3.2.cmml">â„’</mi><mtext id="S3.E4.m1.2.2.3.3" xref="S3.E4.m1.2.2.3.3a.cmml">sync</mtext></msub><mo id="S3.E4.m1.2.2.2" xref="S3.E4.m1.2.2.2.cmml">=</mo><mrow id="S3.E4.m1.2.2.1" xref="S3.E4.m1.2.2.1.cmml"><mrow id="S3.E4.m1.2.2.1.3" xref="S3.E4.m1.2.2.1.3.cmml"><mfrac id="S3.E4.m1.2.2.1.3.2" xref="S3.E4.m1.2.2.1.3.2.cmml"><mn id="S3.E4.m1.2.2.1.3.2.2" xref="S3.E4.m1.2.2.1.3.2.2.cmml">1</mn><mi id="S3.E4.m1.2.2.1.3.2.3" xref="S3.E4.m1.2.2.1.3.2.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E4.m1.2.2.1.3.1" xref="S3.E4.m1.2.2.1.3.1.cmml">â€‹</mo><munderover id="S3.E4.m1.2.2.1.3.3" xref="S3.E4.m1.2.2.1.3.3.cmml"><mo movablelimits="false" rspace="0em" id="S3.E4.m1.2.2.1.3.3.2.2" xref="S3.E4.m1.2.2.1.3.3.2.2.cmml">âˆ‘</mo><mi id="S3.E4.m1.2.2.1.3.3.2.3" xref="S3.E4.m1.2.2.1.3.3.2.3.cmml">i</mi><mi id="S3.E4.m1.2.2.1.3.3.3" xref="S3.E4.m1.2.2.1.3.3.3.cmml">N</mi></munderover></mrow><mo lspace="0em" id="S3.E4.m1.2.2.1.2" xref="S3.E4.m1.2.2.1.2.cmml">âˆ’</mo><mrow id="S3.E4.m1.2.2.1.1.1" xref="S3.E4.m1.2.2.1.1.2.cmml"><mi id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">log</mi><mo id="S3.E4.m1.2.2.1.1.1a" xref="S3.E4.m1.2.2.1.1.2.cmml">â¡</mo><mrow id="S3.E4.m1.2.2.1.1.1.1" xref="S3.E4.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.2.cmml">(</mo><msubsup id="S3.E4.m1.2.2.1.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.1.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.1.2.2" xref="S3.E4.m1.2.2.1.1.1.1.1.2.2.cmml">P</mi><mtext id="S3.E4.m1.2.2.1.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.1.3a.cmml">sync</mtext><mi id="S3.E4.m1.2.2.1.1.1.1.1.2.3" xref="S3.E4.m1.2.2.1.1.1.1.1.2.3.cmml">i</mi></msubsup><mo stretchy="false" id="S3.E4.m1.2.2.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2"><eq id="S3.E4.m1.2.2.2.cmml" xref="S3.E4.m1.2.2.2"></eq><apply id="S3.E4.m1.2.2.3.cmml" xref="S3.E4.m1.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.3.1.cmml" xref="S3.E4.m1.2.2.3">subscript</csymbol><ci id="S3.E4.m1.2.2.3.2.cmml" xref="S3.E4.m1.2.2.3.2">â„’</ci><ci id="S3.E4.m1.2.2.3.3a.cmml" xref="S3.E4.m1.2.2.3.3"><mtext mathsize="70%" id="S3.E4.m1.2.2.3.3.cmml" xref="S3.E4.m1.2.2.3.3">sync</mtext></ci></apply><apply id="S3.E4.m1.2.2.1.cmml" xref="S3.E4.m1.2.2.1"><minus id="S3.E4.m1.2.2.1.2.cmml" xref="S3.E4.m1.2.2.1.2"></minus><apply id="S3.E4.m1.2.2.1.3.cmml" xref="S3.E4.m1.2.2.1.3"><times id="S3.E4.m1.2.2.1.3.1.cmml" xref="S3.E4.m1.2.2.1.3.1"></times><apply id="S3.E4.m1.2.2.1.3.2.cmml" xref="S3.E4.m1.2.2.1.3.2"><divide id="S3.E4.m1.2.2.1.3.2.1.cmml" xref="S3.E4.m1.2.2.1.3.2"></divide><cn type="integer" id="S3.E4.m1.2.2.1.3.2.2.cmml" xref="S3.E4.m1.2.2.1.3.2.2">1</cn><ci id="S3.E4.m1.2.2.1.3.2.3.cmml" xref="S3.E4.m1.2.2.1.3.2.3">ğ‘</ci></apply><apply id="S3.E4.m1.2.2.1.3.3.cmml" xref="S3.E4.m1.2.2.1.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.3.3.1.cmml" xref="S3.E4.m1.2.2.1.3.3">superscript</csymbol><apply id="S3.E4.m1.2.2.1.3.3.2.cmml" xref="S3.E4.m1.2.2.1.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.3.3.2.1.cmml" xref="S3.E4.m1.2.2.1.3.3">subscript</csymbol><sum id="S3.E4.m1.2.2.1.3.3.2.2.cmml" xref="S3.E4.m1.2.2.1.3.3.2.2"></sum><ci id="S3.E4.m1.2.2.1.3.3.2.3.cmml" xref="S3.E4.m1.2.2.1.3.3.2.3">ğ‘–</ci></apply><ci id="S3.E4.m1.2.2.1.3.3.3.cmml" xref="S3.E4.m1.2.2.1.3.3.3">ğ‘</ci></apply></apply><apply id="S3.E4.m1.2.2.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1"><log id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"></log><apply id="S3.E4.m1.2.2.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1">subscript</csymbol><apply id="S3.E4.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1">superscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.2.2">ğ‘ƒ</ci><ci id="S3.E4.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.2.3">ğ‘–</ci></apply><ci id="S3.E4.m1.2.2.1.1.1.1.1.3a.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.3"><mtext mathsize="70%" id="S3.E4.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.1.3">sync</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">\mathcal{L_{\text{sync}}}=\frac{1}{N}\sum_{i}^{N}-\log(P^{i}_{\text{sync}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.p5.10" class="ltx_p">where <math id="S3.SS2.p5.6.m1.1" class="ltx_Math" alttext="P_{\text{sync}}" display="inline"><semantics id="S3.SS2.p5.6.m1.1a"><msub id="S3.SS2.p5.6.m1.1.1" xref="S3.SS2.p5.6.m1.1.1.cmml"><mi id="S3.SS2.p5.6.m1.1.1.2" xref="S3.SS2.p5.6.m1.1.1.2.cmml">P</mi><mtext id="S3.SS2.p5.6.m1.1.1.3" xref="S3.SS2.p5.6.m1.1.1.3a.cmml">sync</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.6.m1.1b"><apply id="S3.SS2.p5.6.m1.1.1.cmml" xref="S3.SS2.p5.6.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.6.m1.1.1.1.cmml" xref="S3.SS2.p5.6.m1.1.1">subscript</csymbol><ci id="S3.SS2.p5.6.m1.1.1.2.cmml" xref="S3.SS2.p5.6.m1.1.1.2">ğ‘ƒ</ci><ci id="S3.SS2.p5.6.m1.1.1.3a.cmml" xref="S3.SS2.p5.6.m1.1.1.3"><mtext mathsize="70%" id="S3.SS2.p5.6.m1.1.1.3.cmml" xref="S3.SS2.p5.6.m1.1.1.3">sync</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.6.m1.1c">P_{\text{sync}}</annotation></semantics></math> is a single value between [0, 1] and <math id="S3.SS2.p5.7.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.p5.7.m2.1a"><mi id="S3.SS2.p5.7.m2.1.1" xref="S3.SS2.p5.7.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.7.m2.1b"><ci id="S3.SS2.p5.7.m2.1.1.cmml" xref="S3.SS2.p5.7.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.7.m2.1c">N</annotation></semantics></math> is the batch size. <math id="S3.SS2.p5.8.m3.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS2.p5.8.m3.1a"><mi id="S3.SS2.p5.8.m3.1.1" xref="S3.SS2.p5.8.m3.1.1.cmml">Ïµ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.8.m3.1b"><ci id="S3.SS2.p5.8.m3.1.1.cmml" xref="S3.SS2.p5.8.m3.1.1">italic-Ïµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.8.m3.1c">\epsilon</annotation></semantics></math> is used to prevent division by zero.
We train the lip-sync expert on the HDFT datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2021</a>)</cite> with a batch size of 8, <math id="S3.SS2.p5.9.m4.1" class="ltx_Math" alttext="T_{v}" display="inline"><semantics id="S3.SS2.p5.9.m4.1a"><msub id="S3.SS2.p5.9.m4.1.1" xref="S3.SS2.p5.9.m4.1.1.cmml"><mi id="S3.SS2.p5.9.m4.1.1.2" xref="S3.SS2.p5.9.m4.1.1.2.cmml">T</mi><mi id="S3.SS2.p5.9.m4.1.1.3" xref="S3.SS2.p5.9.m4.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.9.m4.1b"><apply id="S3.SS2.p5.9.m4.1.1.cmml" xref="S3.SS2.p5.9.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.9.m4.1.1.1.cmml" xref="S3.SS2.p5.9.m4.1.1">subscript</csymbol><ci id="S3.SS2.p5.9.m4.1.1.2.cmml" xref="S3.SS2.p5.9.m4.1.1.2">ğ‘‡</ci><ci id="S3.SS2.p5.9.m4.1.1.3.cmml" xref="S3.SS2.p5.9.m4.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.9.m4.1c">T_{v}</annotation></semantics></math> = 5 frames, <math id="S3.SS2.p5.10.m5.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S3.SS2.p5.10.m5.1a"><mi id="S3.SS2.p5.10.m5.1.1" xref="S3.SS2.p5.10.m5.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.10.m5.1b"><ci id="S3.SS2.p5.10.m5.1.1.cmml" xref="S3.SS2.p5.10.m5.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.10.m5.1c">S</annotation></semantics></math> = 0.2s segment, using the Adam optimizer with a learning rate of 1e-4. After approximately one day of training, the model converges. Our expert network eventually achieves 81% accuracy on the test set.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Quantitative comparisons on the HDTF dataset. The best performance is highlighted in <span id="S3.T1.12.1" class="ltx_text" style="color:#FF0000;">red</span> (1st best) and <span id="S3.T1.13.2" class="ltx_text" style="color:#0000FF;">blue</span> (2nd best). Local Editing and Swapping Background demonstrate the methodâ€™s capabilities for facial attribute editing and swapping background. For better visualization, we scale up the FVD by a factor of 0.1.</figcaption>
<table id="S3.T1.9" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.9.10.1" class="ltx_tr">
<td id="S3.T1.9.10.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T1.9.10.1.1.1" class="ltx_text">Method</span></td>
<td id="S3.T1.9.10.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Local</td>
<td id="S3.T1.9.10.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Swapping</td>
<td id="S3.T1.9.10.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3">Synchronization</td>
<td id="S3.T1.9.10.1.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="5">Visual Quality</td>
</tr>
<tr id="S3.T1.9.9" class="ltx_tr">
<td id="S3.T1.9.9.10" class="ltx_td ltx_align_center ltx_border_r">Editing</td>
<td id="S3.T1.9.9.11" class="ltx_td ltx_align_center ltx_border_r">Background</td>
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">Sync<math id="S3.T1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S3.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">F-LMD<math id="S3.T1.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.2.2.2.m1.1a"><mo stretchy="false" id="S3.T1.2.2.2.m1.1.1" xref="S3.T1.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.m1.1b"><ci id="S3.T1.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S3.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">M-LMD<math id="S3.T1.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.3.3.3.m1.1a"><mo stretchy="false" id="S3.T1.3.3.3.m1.1.1" xref="S3.T1.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.m1.1b"><ci id="S3.T1.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S3.T1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">FID<math id="S3.T1.4.4.4.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.4.4.4.m1.1a"><mo stretchy="false" id="S3.T1.4.4.4.m1.1.1" xref="S3.T1.4.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.m1.1b"><ci id="S3.T1.4.4.4.m1.1.1.cmml" xref="S3.T1.4.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S3.T1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">LPIPS<math id="S3.T1.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.5.5.5.m1.1a"><mo stretchy="false" id="S3.T1.5.5.5.m1.1.1" xref="S3.T1.5.5.5.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.m1.1b"><ci id="S3.T1.5.5.5.m1.1.1.cmml" xref="S3.T1.5.5.5.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S3.T1.6.6.6" class="ltx_td ltx_align_center ltx_border_t">PSNR<math id="S3.T1.6.6.6.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.6.6.6.m1.1a"><mo stretchy="false" id="S3.T1.6.6.6.m1.1.1" xref="S3.T1.6.6.6.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.m1.1b"><ci id="S3.T1.6.6.6.m1.1.1.cmml" xref="S3.T1.6.6.6.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S3.T1.7.7.7" class="ltx_td ltx_align_center ltx_border_t">SSIM<math id="S3.T1.7.7.7.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T1.7.7.7.m1.1a"><mo stretchy="false" id="S3.T1.7.7.7.m1.1.1" xref="S3.T1.7.7.7.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.7.m1.1b"><ci id="S3.T1.7.7.7.m1.1.1.cmml" xref="S3.T1.7.7.7.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.7.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S3.T1.9.9.9" class="ltx_td ltx_align_center ltx_border_t">FVD<math id="S3.T1.8.8.8.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S3.T1.8.8.8.m1.1a"><mo stretchy="false" id="S3.T1.8.8.8.m1.1.1" xref="S3.T1.8.8.8.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.8.m1.1b"><ci id="S3.T1.8.8.8.m1.1.1.cmml" xref="S3.T1.8.8.8.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.8.m1.1c">\downarrow</annotation></semantics></math> <math id="S3.T1.9.9.9.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.9.9.9.m2.1a"><mo id="S3.T1.9.9.9.m2.1.1" xref="S3.T1.9.9.9.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.9.m2.1b"><times id="S3.T1.9.9.9.m2.1.1.cmml" xref="S3.T1.9.9.9.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.9.m2.1c">\times</annotation></semantics></math> 0.1</td>
</tr>
<tr id="S3.T1.9.11.2" class="ltx_tr">
<td id="S3.T1.9.11.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Real Video</td>
<td id="S3.T1.9.11.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T1.9.11.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T1.9.11.2.4" class="ltx_td ltx_align_center ltx_border_t">7.305</td>
<td id="S3.T1.9.11.2.5" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S3.T1.9.11.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S3.T1.9.11.2.7" class="ltx_td ltx_align_center ltx_border_t">2.623</td>
<td id="S3.T1.9.11.2.8" class="ltx_td ltx_align_center ltx_border_t">0.0109</td>
<td id="S3.T1.9.11.2.9" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T1.9.11.2.10" class="ltx_td ltx_align_center ltx_border_t">1.000</td>
<td id="S3.T1.9.11.2.11" class="ltx_td ltx_align_center ltx_border_t">1.048</td>
</tr>
<tr id="S3.T1.9.12.3" class="ltx_tr">
<td id="S3.T1.9.12.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Wav2LipÂ <cite class="ltx_cite ltx_citemacro_citep">(Prajwal etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="S3.T1.9.12.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T1.9.12.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">N/A</td>
<td id="S3.T1.9.12.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.9.12.3.4.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">8.413</span></td>
<td id="S3.T1.9.12.3.5" class="ltx_td ltx_align_center ltx_border_t">3.727</td>
<td id="S3.T1.9.12.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.528</td>
<td id="S3.T1.9.12.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.9.12.3.7.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">16.299</span></td>
<td id="S3.T1.9.12.3.8" class="ltx_td ltx_align_center ltx_border_t">0.1031</td>
<td id="S3.T1.9.12.3.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.9.12.3.9.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">31.821</span></td>
<td id="S3.T1.9.12.3.10" class="ltx_td ltx_align_center ltx_border_t">0.921</td>
<td id="S3.T1.9.12.3.11" class="ltx_td ltx_align_center ltx_border_t">17.423</td>
</tr>
<tr id="S3.T1.9.13.4" class="ltx_tr">
<td id="S3.T1.9.13.4.1" class="ltx_td ltx_align_left ltx_border_r">SadTalkerÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S3.T1.9.13.4.2" class="ltx_td ltx_align_center ltx_border_r">Blink Only</td>
<td id="S3.T1.9.13.4.3" class="ltx_td ltx_align_center ltx_border_r">N/A</td>
<td id="S3.T1.9.13.4.4" class="ltx_td ltx_align_center">6.725</td>
<td id="S3.T1.9.13.4.5" class="ltx_td ltx_align_center">34.970</td>
<td id="S3.T1.9.13.4.6" class="ltx_td ltx_align_center ltx_border_r">36.172</td>
<td id="S3.T1.9.13.4.7" class="ltx_td ltx_align_center">59.059</td>
<td id="S3.T1.9.13.4.8" class="ltx_td ltx_align_center">0.7567</td>
<td id="S3.T1.9.13.4.9" class="ltx_td ltx_align_center">15.967</td>
<td id="S3.T1.9.13.4.10" class="ltx_td ltx_align_center">0.496</td>
<td id="S3.T1.9.13.4.11" class="ltx_td ltx_align_center">85.638</td>
</tr>
<tr id="S3.T1.9.14.5" class="ltx_tr">
<td id="S3.T1.9.14.5.1" class="ltx_td ltx_align_left ltx_border_r">DiffTalkÂ <cite class="ltx_cite ltx_citemacro_citep">(Shen etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="S3.T1.9.14.5.2" class="ltx_td ltx_align_center ltx_border_r">N/A</td>
<td id="S3.T1.9.14.5.3" class="ltx_td ltx_align_center ltx_border_r">N/A</td>
<td id="S3.T1.9.14.5.4" class="ltx_td ltx_align_center">4.615</td>
<td id="S3.T1.9.14.5.5" class="ltx_td ltx_align_center"><span id="S3.T1.9.14.5.5.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">2.004</span></td>
<td id="S3.T1.9.14.5.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.9.14.5.6.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">1.614</span></td>
<td id="S3.T1.9.14.5.7" class="ltx_td ltx_align_center">23.498</td>
<td id="S3.T1.9.14.5.8" class="ltx_td ltx_align_center">0.111</td>
<td id="S3.T1.9.14.5.9" class="ltx_td ltx_align_center">31.654</td>
<td id="S3.T1.9.14.5.10" class="ltx_td ltx_align_center">0.913</td>
<td id="S3.T1.9.14.5.11" class="ltx_td ltx_align_center"><span id="S3.T1.9.14.5.11.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">16.606</span></td>
</tr>
<tr id="S3.T1.9.15.6" class="ltx_tr">
<td id="S3.T1.9.15.6.1" class="ltx_td ltx_align_left ltx_border_r">StyleHEATÂ <cite class="ltx_cite ltx_citemacro_citep">(Yin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="S3.T1.9.15.6.2" class="ltx_td ltx_align_center ltx_border_r">Blink Only</td>
<td id="S3.T1.9.15.6.3" class="ltx_td ltx_align_center ltx_border_r">N/A</td>
<td id="S3.T1.9.15.6.4" class="ltx_td ltx_align_center">6.296</td>
<td id="S3.T1.9.15.6.5" class="ltx_td ltx_align_center">29.672</td>
<td id="S3.T1.9.15.6.6" class="ltx_td ltx_align_center ltx_border_r">31.390</td>
<td id="S3.T1.9.15.6.7" class="ltx_td ltx_align_center">111.229</td>
<td id="S3.T1.9.15.6.8" class="ltx_td ltx_align_center">0.7969</td>
<td id="S3.T1.9.15.6.9" class="ltx_td ltx_align_center">14.968</td>
<td id="S3.T1.9.15.6.10" class="ltx_td ltx_align_center">0.465</td>
<td id="S3.T1.9.15.6.11" class="ltx_td ltx_align_center">91.2356</td>
</tr>
<tr id="S3.T1.9.16.7" class="ltx_tr">
<td id="S3.T1.9.16.7.1" class="ltx_td ltx_align_left ltx_border_r">AD-NeRFÂ <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="S3.T1.9.16.7.2" class="ltx_td ltx_align_center ltx_border_r">N/A</td>
<td id="S3.T1.9.16.7.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S3.T1.9.16.7.3.1" class="ltx_text ltx_font_bold">Yes</span></td>
<td id="S3.T1.9.16.7.4" class="ltx_td ltx_align_center">5.216</td>
<td id="S3.T1.9.16.7.5" class="ltx_td ltx_align_center">3.574</td>
<td id="S3.T1.9.16.7.6" class="ltx_td ltx_align_center ltx_border_r">3.825</td>
<td id="S3.T1.9.16.7.7" class="ltx_td ltx_align_center">18.614</td>
<td id="S3.T1.9.16.7.8" class="ltx_td ltx_align_center"><span id="S3.T1.9.16.7.8.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">0.1013</span></td>
<td id="S3.T1.9.16.7.9" class="ltx_td ltx_align_center">30.640</td>
<td id="S3.T1.9.16.7.10" class="ltx_td ltx_align_center"><span id="S3.T1.9.16.7.10.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">0.923</span></td>
<td id="S3.T1.9.16.7.11" class="ltx_td ltx_align_center">18.438</td>
</tr>
<tr id="S3.T1.9.17.8" class="ltx_tr">
<td id="S3.T1.9.17.8.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t">Ours</td>
<td id="S3.T1.9.17.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S3.T1.9.17.8.2.1" class="ltx_text ltx_font_bold">Yes</span></td>
<td id="S3.T1.9.17.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S3.T1.9.17.8.3.1" class="ltx_text ltx_font_bold">Yes</span></td>
<td id="S3.T1.9.17.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.9.17.8.4.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">6.872</span></td>
<td id="S3.T1.9.17.8.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.9.17.8.5.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">3.405</span></td>
<td id="S3.T1.9.17.8.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S3.T1.9.17.8.6.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">3.173</span></td>
<td id="S3.T1.9.17.8.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.9.17.8.7.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">10.348</span></td>
<td id="S3.T1.9.17.8.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.9.17.8.8.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">0.0494</span></td>
<td id="S3.T1.9.17.8.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.9.17.8.9.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">33.590</span></td>
<td id="S3.T1.9.17.8.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.9.17.8.10.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">0.934</span></td>
<td id="S3.T1.9.17.8.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.9.17.8.11.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">9.205</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Segmentation-guided GAN Injection</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The second sub-network illustrated in <a href="#S3.F2" title="In 3.2. Talking Segmentation Generation â€£ 3. Proposed Methods â€£ SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">fig.</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> is the segmentation-guided GAN injection (SGI) network. Giving a portrait and its corresponding mask, SGI first encodes the image into the latent space to obtain the latent code, then inverts the generated latent code back to the image domain through style injection.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.6" class="ltx_p">There exist various latent spaces such as <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{W}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">ğ’²</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">ğ’²</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\mathcal{W}</annotation></semantics></math>, <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{W+}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mrow id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">ğ’²</mi><mo id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">+</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="latexml" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">limit-from</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">ğ’²</ci><plus id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\mathcal{W+}</annotation></semantics></math> and <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">ğ’®</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">ğ’®</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">\mathcal{S}</annotation></semantics></math> space. Many worksÂ <cite class="ltx_cite ltx_citemacro_citep">(Shen etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2020</a>; Abdal etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2019</a>; Richardson etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2021</a>; Tov etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2021</a>; Wu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2021</a>)</cite> have investigated their representational abilities from the perspectives of distortion, perception, and editability. Here, we choose <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{W}^{c+}" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><msup id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.4.m4.1.1.2" xref="S3.SS3.p2.4.m4.1.1.2.cmml">ğ’²</mi><mrow id="S3.SS3.p2.4.m4.1.1.3" xref="S3.SS3.p2.4.m4.1.1.3.cmml"><mi id="S3.SS3.p2.4.m4.1.1.3.2" xref="S3.SS3.p2.4.m4.1.1.3.2.cmml">c</mi><mo id="S3.SS3.p2.4.m4.1.1.3.3" xref="S3.SS3.p2.4.m4.1.1.3.3.cmml">+</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><apply id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.4.m4.1.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">superscript</csymbol><ci id="S3.SS3.p2.4.m4.1.1.2.cmml" xref="S3.SS3.p2.4.m4.1.1.2">ğ’²</ci><apply id="S3.SS3.p2.4.m4.1.1.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3"><csymbol cd="latexml" id="S3.SS3.p2.4.m4.1.1.3.1.cmml" xref="S3.SS3.p2.4.m4.1.1.3">limit-from</csymbol><ci id="S3.SS3.p2.4.m4.1.1.3.2.cmml" xref="S3.SS3.p2.4.m4.1.1.3.2">ğ‘</ci><plus id="S3.SS3.p2.4.m4.1.1.3.3.cmml" xref="S3.SS3.p2.4.m4.1.1.3.3"></plus></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">\mathcal{W}^{c+}</annotation></semantics></math> space, a variation of <math id="S3.SS3.p2.5.m5.1" class="ltx_Math" alttext="\mathcal{W+}" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><mrow id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.5.m5.1.1.2" xref="S3.SS3.p2.5.m5.1.1.2.cmml">ğ’²</mi><mo id="S3.SS3.p2.5.m5.1.1.3" xref="S3.SS3.p2.5.m5.1.1.3.cmml">+</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><apply id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1"><csymbol cd="latexml" id="S3.SS3.p2.5.m5.1.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">limit-from</csymbol><ci id="S3.SS3.p2.5.m5.1.1.2.cmml" xref="S3.SS3.p2.5.m5.1.1.2">ğ’²</ci><plus id="S3.SS3.p2.5.m5.1.1.3.cmml" xref="S3.SS3.p2.5.m5.1.1.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">\mathcal{W+}</annotation></semantics></math> space originated fromÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2023</a>)</cite> as representation of latent code. To leverage this representation, a powerful encoder is required to accurately map each input image to a corresponding code. Although many encodersÂ <cite class="ltx_cite ltx_citemacro_citep">(Richardson etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2021</a>; Tov etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2021</a>; Wu etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2021</a>)</cite> have been proposed, they focus on extracting global latent code for global editing, such as age, emotion, making them unsuitable for textures disentanglement and local editing. To this end, we adopt a variation ofÂ <cite class="ltx_cite ltx_citemacro_citep">(Richardson etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2021</a>)</cite> for latent code extraction. The encoder utilizes a feature pyramid network (FPN)Â <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite> for feature fusion, ultimately generating fine-grained, medium-grained, and coarse-grained feature maps at three different scales. The mask is then resized to match each feature map. Subsequently, a global average pooling (GAP) is employed to extract semantic region features according to the segmentation, resulting in multi-scale style vectors. These are concatenated and further passed through an MLP to obtain the <math id="S3.SS3.p2.6.m6.1" class="ltx_Math" alttext="\mathcal{W}^{c+}" display="inline"><semantics id="S3.SS3.p2.6.m6.1a"><msup id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.6.m6.1.1.2" xref="S3.SS3.p2.6.m6.1.1.2.cmml">ğ’²</mi><mrow id="S3.SS3.p2.6.m6.1.1.3" xref="S3.SS3.p2.6.m6.1.1.3.cmml"><mi id="S3.SS3.p2.6.m6.1.1.3.2" xref="S3.SS3.p2.6.m6.1.1.3.2.cmml">c</mi><mo id="S3.SS3.p2.6.m6.1.1.3.3" xref="S3.SS3.p2.6.m6.1.1.3.3.cmml">+</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><apply id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m6.1.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">superscript</csymbol><ci id="S3.SS3.p2.6.m6.1.1.2.cmml" xref="S3.SS3.p2.6.m6.1.1.2">ğ’²</ci><apply id="S3.SS3.p2.6.m6.1.1.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3"><csymbol cd="latexml" id="S3.SS3.p2.6.m6.1.1.3.1.cmml" xref="S3.SS3.p2.6.m6.1.1.3">limit-from</csymbol><ci id="S3.SS3.p2.6.m6.1.1.3.2.cmml" xref="S3.SS3.p2.6.m6.1.1.3.2">ğ‘</ci><plus id="S3.SS3.p2.6.m6.1.1.3.3.cmml" xref="S3.SS3.p2.6.m6.1.1.3.3"></plus></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">\mathcal{W}^{c+}</annotation></semantics></math> style codes.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.4" class="ltx_p">Specifically, given a source image <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">I</annotation></semantics></math> and its corresponding mask <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">M</annotation></semantics></math>, we first utilize a multi-scale encoder <math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="E_{\phi}" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><msub id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml"><mi id="S3.SS3.p3.3.m3.1.1.2" xref="S3.SS3.p3.3.m3.1.1.2.cmml">E</mi><mi id="S3.SS3.p3.3.m3.1.1.3" xref="S3.SS3.p3.3.m3.1.1.3.cmml">Ï•</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2">ğ¸</ci><ci id="S3.SS3.p3.3.m3.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3">italic-Ï•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">E_{\phi}</annotation></semantics></math> to obtain the feature maps <math id="S3.SS3.p3.4.m4.1" class="ltx_Math" alttext="F=[F_{i}]_{i=1}^{N}" display="inline"><semantics id="S3.SS3.p3.4.m4.1a"><mrow id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml"><mi id="S3.SS3.p3.4.m4.1.1.3" xref="S3.SS3.p3.4.m4.1.1.3.cmml">F</mi><mo id="S3.SS3.p3.4.m4.1.1.2" xref="S3.SS3.p3.4.m4.1.1.2.cmml">=</mo><msubsup id="S3.SS3.p3.4.m4.1.1.1" xref="S3.SS3.p3.4.m4.1.1.1.cmml"><mrow id="S3.SS3.p3.4.m4.1.1.1.1.1.1" xref="S3.SS3.p3.4.m4.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p3.4.m4.1.1.1.1.1.1.2" xref="S3.SS3.p3.4.m4.1.1.1.1.1.2.1.cmml">[</mo><msub id="S3.SS3.p3.4.m4.1.1.1.1.1.1.1" xref="S3.SS3.p3.4.m4.1.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p3.4.m4.1.1.1.1.1.1.1.2" xref="S3.SS3.p3.4.m4.1.1.1.1.1.1.1.2.cmml">F</mi><mi id="S3.SS3.p3.4.m4.1.1.1.1.1.1.1.3" xref="S3.SS3.p3.4.m4.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS3.p3.4.m4.1.1.1.1.1.1.3" xref="S3.SS3.p3.4.m4.1.1.1.1.1.2.1.cmml">]</mo></mrow><mrow id="S3.SS3.p3.4.m4.1.1.1.1.3" xref="S3.SS3.p3.4.m4.1.1.1.1.3.cmml"><mi id="S3.SS3.p3.4.m4.1.1.1.1.3.2" xref="S3.SS3.p3.4.m4.1.1.1.1.3.2.cmml">i</mi><mo id="S3.SS3.p3.4.m4.1.1.1.1.3.1" xref="S3.SS3.p3.4.m4.1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS3.p3.4.m4.1.1.1.1.3.3" xref="S3.SS3.p3.4.m4.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS3.p3.4.m4.1.1.1.3" xref="S3.SS3.p3.4.m4.1.1.1.3.cmml">N</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><apply id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1"><eq id="S3.SS3.p3.4.m4.1.1.2.cmml" xref="S3.SS3.p3.4.m4.1.1.2"></eq><ci id="S3.SS3.p3.4.m4.1.1.3.cmml" xref="S3.SS3.p3.4.m4.1.1.3">ğ¹</ci><apply id="S3.SS3.p3.4.m4.1.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.4.m4.1.1.1.2.cmml" xref="S3.SS3.p3.4.m4.1.1.1">superscript</csymbol><apply id="S3.SS3.p3.4.m4.1.1.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.4.m4.1.1.1.1.2.cmml" xref="S3.SS3.p3.4.m4.1.1.1">subscript</csymbol><apply id="S3.SS3.p3.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS3.p3.4.m4.1.1.1.1.1.2.1.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS3.p3.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.4.m4.1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p3.4.m4.1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1.1.1.1.2">ğ¹</ci><ci id="S3.SS3.p3.4.m4.1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply><apply id="S3.SS3.p3.4.m4.1.1.1.1.3.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1.3"><eq id="S3.SS3.p3.4.m4.1.1.1.1.3.1.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1.3.1"></eq><ci id="S3.SS3.p3.4.m4.1.1.1.1.3.2.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.SS3.p3.4.m4.1.1.1.1.3.3.cmml" xref="S3.SS3.p3.4.m4.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS3.p3.4.m4.1.1.1.3.cmml" xref="S3.SS3.p3.4.m4.1.1.1.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">F=[F_{i}]_{i=1}^{N}</annotation></semantics></math> at different resolutions:</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E5.m1.1" class="ltx_Math" alttext="F=E_{\phi}(I)" display="block"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.2" xref="S3.E5.m1.1.2.cmml"><mi id="S3.E5.m1.1.2.2" xref="S3.E5.m1.1.2.2.cmml">F</mi><mo id="S3.E5.m1.1.2.1" xref="S3.E5.m1.1.2.1.cmml">=</mo><mrow id="S3.E5.m1.1.2.3" xref="S3.E5.m1.1.2.3.cmml"><msub id="S3.E5.m1.1.2.3.2" xref="S3.E5.m1.1.2.3.2.cmml"><mi id="S3.E5.m1.1.2.3.2.2" xref="S3.E5.m1.1.2.3.2.2.cmml">E</mi><mi id="S3.E5.m1.1.2.3.2.3" xref="S3.E5.m1.1.2.3.2.3.cmml">Ï•</mi></msub><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.2.3.1" xref="S3.E5.m1.1.2.3.1.cmml">â€‹</mo><mrow id="S3.E5.m1.1.2.3.3.2" xref="S3.E5.m1.1.2.3.cmml"><mo stretchy="false" id="S3.E5.m1.1.2.3.3.2.1" xref="S3.E5.m1.1.2.3.cmml">(</mo><mi id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml">I</mi><mo stretchy="false" id="S3.E5.m1.1.2.3.3.2.2" xref="S3.E5.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.2.cmml" xref="S3.E5.m1.1.2"><eq id="S3.E5.m1.1.2.1.cmml" xref="S3.E5.m1.1.2.1"></eq><ci id="S3.E5.m1.1.2.2.cmml" xref="S3.E5.m1.1.2.2">ğ¹</ci><apply id="S3.E5.m1.1.2.3.cmml" xref="S3.E5.m1.1.2.3"><times id="S3.E5.m1.1.2.3.1.cmml" xref="S3.E5.m1.1.2.3.1"></times><apply id="S3.E5.m1.1.2.3.2.cmml" xref="S3.E5.m1.1.2.3.2"><csymbol cd="ambiguous" id="S3.E5.m1.1.2.3.2.1.cmml" xref="S3.E5.m1.1.2.3.2">subscript</csymbol><ci id="S3.E5.m1.1.2.3.2.2.cmml" xref="S3.E5.m1.1.2.3.2.2">ğ¸</ci><ci id="S3.E5.m1.1.2.3.2.3.cmml" xref="S3.E5.m1.1.2.3.2.3">italic-Ï•</ci></apply><ci id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1">ğ¼</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">F=E_{\phi}(I)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS3.p3.8" class="ltx_p">Here <math id="S3.SS3.p3.5.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS3.p3.5.m1.1a"><mi id="S3.SS3.p3.5.m1.1.1" xref="S3.SS3.p3.5.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m1.1b"><ci id="S3.SS3.p3.5.m1.1.1.cmml" xref="S3.SS3.p3.5.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m1.1c">N</annotation></semantics></math> is equal to three. We then aggregate per-region features based on the mask <math id="S3.SS3.p3.6.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS3.p3.6.m2.1a"><mi id="S3.SS3.p3.6.m2.1.1" xref="S3.SS3.p3.6.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.6.m2.1b"><ci id="S3.SS3.p3.6.m2.1.1.cmml" xref="S3.SS3.p3.6.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.6.m2.1c">M</annotation></semantics></math> and features <math id="S3.SS3.p3.7.m3.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.SS3.p3.7.m3.1a"><mi id="S3.SS3.p3.7.m3.1.1" xref="S3.SS3.p3.7.m3.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.7.m3.1b"><ci id="S3.SS3.p3.7.m3.1.1.cmml" xref="S3.SS3.p3.7.m3.1.1">ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.7.m3.1c">F</annotation></semantics></math>. Specifically, for each feature map <math id="S3.SS3.p3.8.m4.1" class="ltx_Math" alttext="F_{i}" display="inline"><semantics id="S3.SS3.p3.8.m4.1a"><msub id="S3.SS3.p3.8.m4.1.1" xref="S3.SS3.p3.8.m4.1.1.cmml"><mi id="S3.SS3.p3.8.m4.1.1.2" xref="S3.SS3.p3.8.m4.1.1.2.cmml">F</mi><mi id="S3.SS3.p3.8.m4.1.1.3" xref="S3.SS3.p3.8.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.8.m4.1b"><apply id="S3.SS3.p3.8.m4.1.1.cmml" xref="S3.SS3.p3.8.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.8.m4.1.1.1.cmml" xref="S3.SS3.p3.8.m4.1.1">subscript</csymbol><ci id="S3.SS3.p3.8.m4.1.1.2.cmml" xref="S3.SS3.p3.8.m4.1.1.2">ğ¹</ci><ci id="S3.SS3.p3.8.m4.1.1.3.cmml" xref="S3.SS3.p3.8.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.8.m4.1c">F_{i}</annotation></semantics></math>, we first downsample the mask to match the feature map size, then perform global average pooling (GAP) to aggregate features for different regions:</p>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(6)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E6.m1.7" class="ltx_Math" alttext="u_{ij}=\texttt{GAP}(F_{i}\circ(\texttt{Down}(M)_{i}=j)),\{j=1,2,...,C\}" display="block"><semantics id="S3.E6.m1.7a"><mrow id="S3.E6.m1.7.7" xref="S3.E6.m1.7.7.cmml"><msub id="S3.E6.m1.7.7.4" xref="S3.E6.m1.7.7.4.cmml"><mi id="S3.E6.m1.7.7.4.2" xref="S3.E6.m1.7.7.4.2.cmml">u</mi><mrow id="S3.E6.m1.7.7.4.3" xref="S3.E6.m1.7.7.4.3.cmml"><mi id="S3.E6.m1.7.7.4.3.2" xref="S3.E6.m1.7.7.4.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E6.m1.7.7.4.3.1" xref="S3.E6.m1.7.7.4.3.1.cmml">â€‹</mo><mi id="S3.E6.m1.7.7.4.3.3" xref="S3.E6.m1.7.7.4.3.3.cmml">j</mi></mrow></msub><mo id="S3.E6.m1.7.7.3" xref="S3.E6.m1.7.7.3.cmml">=</mo><mrow id="S3.E6.m1.7.7.2.2" xref="S3.E6.m1.7.7.2.3.cmml"><mrow id="S3.E6.m1.6.6.1.1.1" xref="S3.E6.m1.6.6.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E6.m1.6.6.1.1.1.3" xref="S3.E6.m1.6.6.1.1.1.3a.cmml">GAP</mtext><mo lspace="0em" rspace="0em" id="S3.E6.m1.6.6.1.1.1.2" xref="S3.E6.m1.6.6.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E6.m1.6.6.1.1.1.1.1" xref="S3.E6.m1.6.6.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E6.m1.6.6.1.1.1.1.1.2" xref="S3.E6.m1.6.6.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.6.6.1.1.1.1.1.1" xref="S3.E6.m1.6.6.1.1.1.1.1.1.cmml"><msub id="S3.E6.m1.6.6.1.1.1.1.1.1.3" xref="S3.E6.m1.6.6.1.1.1.1.1.1.3.cmml"><mi id="S3.E6.m1.6.6.1.1.1.1.1.1.3.2" xref="S3.E6.m1.6.6.1.1.1.1.1.1.3.2.cmml">F</mi><mi id="S3.E6.m1.6.6.1.1.1.1.1.1.3.3" xref="S3.E6.m1.6.6.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E6.m1.6.6.1.1.1.1.1.1.2" xref="S3.E6.m1.6.6.1.1.1.1.1.1.2.cmml">âˆ˜</mo><mrow id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.2a.cmml">Down</mtext><mo lspace="0em" rspace="0em" id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.1" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.1.cmml">â€‹</mo><msub id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.3.cmml"><mrow id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.3.2.2" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.3.2.2.1" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.3.cmml">(</mo><mi id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml">M</mi><mo stretchy="false" id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.3.2.2.2" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow><mi id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.3.3.cmml">i</mi></msub></mrow><mo id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml">=</mo><mi id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.3.cmml">j</mi></mrow><mo stretchy="false" id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.3" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E6.m1.6.6.1.1.1.1.1.3" xref="S3.E6.m1.6.6.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E6.m1.7.7.2.2.3" xref="S3.E6.m1.7.7.2.3.cmml">,</mo><mrow id="S3.E6.m1.7.7.2.2.2.1" xref="S3.E6.m1.7.7.2.2.2.2.cmml"><mo stretchy="false" id="S3.E6.m1.7.7.2.2.2.1.2" xref="S3.E6.m1.7.7.2.2.2.2.cmml">{</mo><mrow id="S3.E6.m1.7.7.2.2.2.1.1" xref="S3.E6.m1.7.7.2.2.2.1.1.cmml"><mi id="S3.E6.m1.7.7.2.2.2.1.1.2" xref="S3.E6.m1.7.7.2.2.2.1.1.2.cmml">j</mi><mo id="S3.E6.m1.7.7.2.2.2.1.1.1" xref="S3.E6.m1.7.7.2.2.2.1.1.1.cmml">=</mo><mrow id="S3.E6.m1.7.7.2.2.2.1.1.3.2" xref="S3.E6.m1.7.7.2.2.2.1.1.3.1.cmml"><mn id="S3.E6.m1.2.2" xref="S3.E6.m1.2.2.cmml">1</mn><mo id="S3.E6.m1.7.7.2.2.2.1.1.3.2.1" xref="S3.E6.m1.7.7.2.2.2.1.1.3.1.cmml">,</mo><mn id="S3.E6.m1.3.3" xref="S3.E6.m1.3.3.cmml">2</mn><mo id="S3.E6.m1.7.7.2.2.2.1.1.3.2.2" xref="S3.E6.m1.7.7.2.2.2.1.1.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.E6.m1.4.4" xref="S3.E6.m1.4.4.cmml">â€¦</mi><mo id="S3.E6.m1.7.7.2.2.2.1.1.3.2.3" xref="S3.E6.m1.7.7.2.2.2.1.1.3.1.cmml">,</mo><mi id="S3.E6.m1.5.5" xref="S3.E6.m1.5.5.cmml">C</mi></mrow></mrow><mo stretchy="false" id="S3.E6.m1.7.7.2.2.2.1.3" xref="S3.E6.m1.7.7.2.2.2.2.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.7b"><apply id="S3.E6.m1.7.7.cmml" xref="S3.E6.m1.7.7"><eq id="S3.E6.m1.7.7.3.cmml" xref="S3.E6.m1.7.7.3"></eq><apply id="S3.E6.m1.7.7.4.cmml" xref="S3.E6.m1.7.7.4"><csymbol cd="ambiguous" id="S3.E6.m1.7.7.4.1.cmml" xref="S3.E6.m1.7.7.4">subscript</csymbol><ci id="S3.E6.m1.7.7.4.2.cmml" xref="S3.E6.m1.7.7.4.2">ğ‘¢</ci><apply id="S3.E6.m1.7.7.4.3.cmml" xref="S3.E6.m1.7.7.4.3"><times id="S3.E6.m1.7.7.4.3.1.cmml" xref="S3.E6.m1.7.7.4.3.1"></times><ci id="S3.E6.m1.7.7.4.3.2.cmml" xref="S3.E6.m1.7.7.4.3.2">ğ‘–</ci><ci id="S3.E6.m1.7.7.4.3.3.cmml" xref="S3.E6.m1.7.7.4.3.3">ğ‘—</ci></apply></apply><list id="S3.E6.m1.7.7.2.3.cmml" xref="S3.E6.m1.7.7.2.2"><apply id="S3.E6.m1.6.6.1.1.1.cmml" xref="S3.E6.m1.6.6.1.1.1"><times id="S3.E6.m1.6.6.1.1.1.2.cmml" xref="S3.E6.m1.6.6.1.1.1.2"></times><ci id="S3.E6.m1.6.6.1.1.1.3a.cmml" xref="S3.E6.m1.6.6.1.1.1.3"><mtext class="ltx_mathvariant_monospace" id="S3.E6.m1.6.6.1.1.1.3.cmml" xref="S3.E6.m1.6.6.1.1.1.3">GAP</mtext></ci><apply id="S3.E6.m1.6.6.1.1.1.1.1.1.cmml" xref="S3.E6.m1.6.6.1.1.1.1.1"><compose id="S3.E6.m1.6.6.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.6.6.1.1.1.1.1.1.2"></compose><apply id="S3.E6.m1.6.6.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.6.6.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.6.6.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m1.6.6.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E6.m1.6.6.1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m1.6.6.1.1.1.1.1.1.3.2">ğ¹</ci><ci id="S3.E6.m1.6.6.1.1.1.1.1.1.3.3.cmml" xref="S3.E6.m1.6.6.1.1.1.1.1.1.3.3">ğ‘–</ci></apply><apply id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1"><eq id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.1"></eq><apply id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2"><times id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.1"></times><ci id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.2a.cmml" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.2"><mtext class="ltx_mathvariant_monospace" id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.2">Down</mtext></ci><apply id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1">ğ‘€</ci><ci id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.2.3.3">ğ‘–</ci></apply></apply><ci id="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.6.6.1.1.1.1.1.1.1.1.1.3">ğ‘—</ci></apply></apply></apply><set id="S3.E6.m1.7.7.2.2.2.2.cmml" xref="S3.E6.m1.7.7.2.2.2.1"><apply id="S3.E6.m1.7.7.2.2.2.1.1.cmml" xref="S3.E6.m1.7.7.2.2.2.1.1"><eq id="S3.E6.m1.7.7.2.2.2.1.1.1.cmml" xref="S3.E6.m1.7.7.2.2.2.1.1.1"></eq><ci id="S3.E6.m1.7.7.2.2.2.1.1.2.cmml" xref="S3.E6.m1.7.7.2.2.2.1.1.2">ğ‘—</ci><list id="S3.E6.m1.7.7.2.2.2.1.1.3.1.cmml" xref="S3.E6.m1.7.7.2.2.2.1.1.3.2"><cn type="integer" id="S3.E6.m1.2.2.cmml" xref="S3.E6.m1.2.2">1</cn><cn type="integer" id="S3.E6.m1.3.3.cmml" xref="S3.E6.m1.3.3">2</cn><ci id="S3.E6.m1.4.4.cmml" xref="S3.E6.m1.4.4">â€¦</ci><ci id="S3.E6.m1.5.5.cmml" xref="S3.E6.m1.5.5">ğ¶</ci></list></apply></set></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.7c">u_{ij}=\texttt{GAP}(F_{i}\circ(\texttt{Down}(M)_{i}=j)),\{j=1,2,...,C\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.9" class="ltx_p">Where <math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="u_{ij}" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><msub id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml"><mi id="S3.SS3.p4.1.m1.1.1.2" xref="S3.SS3.p4.1.m1.1.1.2.cmml">u</mi><mrow id="S3.SS3.p4.1.m1.1.1.3" xref="S3.SS3.p4.1.m1.1.1.3.cmml"><mi id="S3.SS3.p4.1.m1.1.1.3.2" xref="S3.SS3.p4.1.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.1.m1.1.1.3.1" xref="S3.SS3.p4.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p4.1.m1.1.1.3.3" xref="S3.SS3.p4.1.m1.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><apply id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p4.1.m1.1.1.2.cmml" xref="S3.SS3.p4.1.m1.1.1.2">ğ‘¢</ci><apply id="S3.SS3.p4.1.m1.1.1.3.cmml" xref="S3.SS3.p4.1.m1.1.1.3"><times id="S3.SS3.p4.1.m1.1.1.3.1.cmml" xref="S3.SS3.p4.1.m1.1.1.3.1"></times><ci id="S3.SS3.p4.1.m1.1.1.3.2.cmml" xref="S3.SS3.p4.1.m1.1.1.3.2">ğ‘–</ci><ci id="S3.SS3.p4.1.m1.1.1.3.3.cmml" xref="S3.SS3.p4.1.m1.1.1.3.3">ğ‘—</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">u_{ij}</annotation></semantics></math> denotes the averaged feature of region <math id="S3.SS3.p4.2.m2.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS3.p4.2.m2.1a"><mi id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><ci id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">j</annotation></semantics></math> in feature map <math id="S3.SS3.p4.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS3.p4.3.m3.1a"><mi id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><ci id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">i</annotation></semantics></math>, <math id="S3.SS3.p4.4.m4.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS3.p4.4.m4.1a"><mi id="S3.SS3.p4.4.m4.1.1" xref="S3.SS3.p4.4.m4.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.4.m4.1b"><ci id="S3.SS3.p4.4.m4.1.1.cmml" xref="S3.SS3.p4.4.m4.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m4.1c">C</annotation></semantics></math> is the number of semantic regions, <span id="S3.SS3.p4.9.1" class="ltx_text ltx_font_typewriter">Down</span>(<math id="S3.SS3.p4.5.m5.1" class="ltx_Math" alttext="\ldots" display="inline"><semantics id="S3.SS3.p4.5.m5.1a"><mi mathvariant="normal" id="S3.SS3.p4.5.m5.1.1" xref="S3.SS3.p4.5.m5.1.1.cmml">â€¦</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.5.m5.1b"><ci id="S3.SS3.p4.5.m5.1.1.cmml" xref="S3.SS3.p4.5.m5.1.1">â€¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.5.m5.1c">\ldots</annotation></semantics></math>) is the downsampling operation to align with <math id="S3.SS3.p4.6.m6.1" class="ltx_Math" alttext="F_{i}" display="inline"><semantics id="S3.SS3.p4.6.m6.1a"><msub id="S3.SS3.p4.6.m6.1.1" xref="S3.SS3.p4.6.m6.1.1.cmml"><mi id="S3.SS3.p4.6.m6.1.1.2" xref="S3.SS3.p4.6.m6.1.1.2.cmml">F</mi><mi id="S3.SS3.p4.6.m6.1.1.3" xref="S3.SS3.p4.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.6.m6.1b"><apply id="S3.SS3.p4.6.m6.1.1.cmml" xref="S3.SS3.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.6.m6.1.1.1.cmml" xref="S3.SS3.p4.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.p4.6.m6.1.1.2.cmml" xref="S3.SS3.p4.6.m6.1.1.2">ğ¹</ci><ci id="S3.SS3.p4.6.m6.1.1.3.cmml" xref="S3.SS3.p4.6.m6.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.6.m6.1c">F_{i}</annotation></semantics></math>, and <math id="S3.SS3.p4.7.m7.1" class="ltx_Math" alttext="\langle\circ\rangle" display="inline"><semantics id="S3.SS3.p4.7.m7.1a"><mrow id="S3.SS3.p4.7.m7.1.2.2" xref="S3.SS3.p4.7.m7.1.2.1.cmml"><mo stretchy="false" id="S3.SS3.p4.7.m7.1.2.2.1" xref="S3.SS3.p4.7.m7.1.2.1.1.cmml">âŸ¨</mo><mo lspace="0em" rspace="0em" id="S3.SS3.p4.7.m7.1.1" xref="S3.SS3.p4.7.m7.1.1.cmml">âˆ˜</mo><mo stretchy="false" id="S3.SS3.p4.7.m7.1.2.2.2" xref="S3.SS3.p4.7.m7.1.2.1.1.cmml">âŸ©</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.7.m7.1b"><apply id="S3.SS3.p4.7.m7.1.2.1.cmml" xref="S3.SS3.p4.7.m7.1.2.2"><csymbol cd="latexml" id="S3.SS3.p4.7.m7.1.2.1.1.cmml" xref="S3.SS3.p4.7.m7.1.2.2.1">delimited-âŸ¨âŸ©</csymbol><compose id="S3.SS3.p4.7.m7.1.1.cmml" xref="S3.SS3.p4.7.m7.1.1"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.7.m7.1c">\langle\circ\rangle</annotation></semantics></math> is the element-wise product. Subsequently, the multi-scale feature vectors <math id="S3.SS3.p4.8.m8.1" class="ltx_Math" alttext="\{u_{ij}\}^{N}_{i=1}" display="inline"><semantics id="S3.SS3.p4.8.m8.1a"><msubsup id="S3.SS3.p4.8.m8.1.1" xref="S3.SS3.p4.8.m8.1.1.cmml"><mrow id="S3.SS3.p4.8.m8.1.1.1.1.1" xref="S3.SS3.p4.8.m8.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p4.8.m8.1.1.1.1.1.2" xref="S3.SS3.p4.8.m8.1.1.1.1.2.cmml">{</mo><msub id="S3.SS3.p4.8.m8.1.1.1.1.1.1" xref="S3.SS3.p4.8.m8.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p4.8.m8.1.1.1.1.1.1.2" xref="S3.SS3.p4.8.m8.1.1.1.1.1.1.2.cmml">u</mi><mrow id="S3.SS3.p4.8.m8.1.1.1.1.1.1.3" xref="S3.SS3.p4.8.m8.1.1.1.1.1.1.3.cmml"><mi id="S3.SS3.p4.8.m8.1.1.1.1.1.1.3.2" xref="S3.SS3.p4.8.m8.1.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.8.m8.1.1.1.1.1.1.3.1" xref="S3.SS3.p4.8.m8.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS3.p4.8.m8.1.1.1.1.1.1.3.3" xref="S3.SS3.p4.8.m8.1.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo stretchy="false" id="S3.SS3.p4.8.m8.1.1.1.1.1.3" xref="S3.SS3.p4.8.m8.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS3.p4.8.m8.1.1.3" xref="S3.SS3.p4.8.m8.1.1.3.cmml"><mi id="S3.SS3.p4.8.m8.1.1.3.2" xref="S3.SS3.p4.8.m8.1.1.3.2.cmml">i</mi><mo id="S3.SS3.p4.8.m8.1.1.3.1" xref="S3.SS3.p4.8.m8.1.1.3.1.cmml">=</mo><mn id="S3.SS3.p4.8.m8.1.1.3.3" xref="S3.SS3.p4.8.m8.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS3.p4.8.m8.1.1.1.3" xref="S3.SS3.p4.8.m8.1.1.1.3.cmml">N</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.8.m8.1b"><apply id="S3.SS3.p4.8.m8.1.1.cmml" xref="S3.SS3.p4.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.8.m8.1.1.2.cmml" xref="S3.SS3.p4.8.m8.1.1">subscript</csymbol><apply id="S3.SS3.p4.8.m8.1.1.1.cmml" xref="S3.SS3.p4.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.8.m8.1.1.1.2.cmml" xref="S3.SS3.p4.8.m8.1.1">superscript</csymbol><set id="S3.SS3.p4.8.m8.1.1.1.1.2.cmml" xref="S3.SS3.p4.8.m8.1.1.1.1.1"><apply id="S3.SS3.p4.8.m8.1.1.1.1.1.1.cmml" xref="S3.SS3.p4.8.m8.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.8.m8.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p4.8.m8.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p4.8.m8.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p4.8.m8.1.1.1.1.1.1.2">ğ‘¢</ci><apply id="S3.SS3.p4.8.m8.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p4.8.m8.1.1.1.1.1.1.3"><times id="S3.SS3.p4.8.m8.1.1.1.1.1.1.3.1.cmml" xref="S3.SS3.p4.8.m8.1.1.1.1.1.1.3.1"></times><ci id="S3.SS3.p4.8.m8.1.1.1.1.1.1.3.2.cmml" xref="S3.SS3.p4.8.m8.1.1.1.1.1.1.3.2">ğ‘–</ci><ci id="S3.SS3.p4.8.m8.1.1.1.1.1.1.3.3.cmml" xref="S3.SS3.p4.8.m8.1.1.1.1.1.1.3.3">ğ‘—</ci></apply></apply></set><ci id="S3.SS3.p4.8.m8.1.1.1.3.cmml" xref="S3.SS3.p4.8.m8.1.1.1.3">ğ‘</ci></apply><apply id="S3.SS3.p4.8.m8.1.1.3.cmml" xref="S3.SS3.p4.8.m8.1.1.3"><eq id="S3.SS3.p4.8.m8.1.1.3.1.cmml" xref="S3.SS3.p4.8.m8.1.1.3.1"></eq><ci id="S3.SS3.p4.8.m8.1.1.3.2.cmml" xref="S3.SS3.p4.8.m8.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.SS3.p4.8.m8.1.1.3.3.cmml" xref="S3.SS3.p4.8.m8.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.8.m8.1c">\{u_{ij}\}^{N}_{i=1}</annotation></semantics></math> of region <math id="S3.SS3.p4.9.m9.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS3.p4.9.m9.1a"><mi id="S3.SS3.p4.9.m9.1.1" xref="S3.SS3.p4.9.m9.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.9.m9.1b"><ci id="S3.SS3.p4.9.m9.1.1.cmml" xref="S3.SS3.p4.9.m9.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.9.m9.1c">j</annotation></semantics></math> are concatenated and passed through a multi-layer perceptron (MLP) to obtain the style codes:</p>
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left"><span class="ltx_tag ltx_tag_equation ltx_align_left">(7)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E7.m1.1" class="ltx_Math" alttext="s_{j}=\texttt{MLP}([u_{ij}]_{i=1}^{N})" display="block"><semantics id="S3.E7.m1.1a"><mrow id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml"><msub id="S3.E7.m1.1.1.3" xref="S3.E7.m1.1.1.3.cmml"><mi id="S3.E7.m1.1.1.3.2" xref="S3.E7.m1.1.1.3.2.cmml">s</mi><mi id="S3.E7.m1.1.1.3.3" xref="S3.E7.m1.1.1.3.3.cmml">j</mi></msub><mo id="S3.E7.m1.1.1.2" xref="S3.E7.m1.1.1.2.cmml">=</mo><mrow id="S3.E7.m1.1.1.1" xref="S3.E7.m1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E7.m1.1.1.1.3" xref="S3.E7.m1.1.1.1.3a.cmml">MLP</mtext><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.1.2" xref="S3.E7.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E7.m1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.E7.m1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.cmml"><mrow id="S3.E7.m1.1.1.1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.cmml">[</mo><msub id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">u</mi><mrow id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.cmml">]</mo></mrow><mrow id="S3.E7.m1.1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E7.m1.1.1.1.1.1.1.1.3.2" xref="S3.E7.m1.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo id="S3.E7.m1.1.1.1.1.1.1.1.3.1" xref="S3.E7.m1.1.1.1.1.1.1.1.3.1.cmml">=</mo><mn id="S3.E7.m1.1.1.1.1.1.1.1.3.3" xref="S3.E7.m1.1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.E7.m1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.3.cmml">N</mi></msubsup><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><apply id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1"><eq id="S3.E7.m1.1.1.2.cmml" xref="S3.E7.m1.1.1.2"></eq><apply id="S3.E7.m1.1.1.3.cmml" xref="S3.E7.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.3">subscript</csymbol><ci id="S3.E7.m1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.3.2">ğ‘ </ci><ci id="S3.E7.m1.1.1.3.3.cmml" xref="S3.E7.m1.1.1.3.3">ğ‘—</ci></apply><apply id="S3.E7.m1.1.1.1.cmml" xref="S3.E7.m1.1.1.1"><times id="S3.E7.m1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.2"></times><ci id="S3.E7.m1.1.1.1.3a.cmml" xref="S3.E7.m1.1.1.1.3"><mtext class="ltx_mathvariant_monospace" id="S3.E7.m1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.3">MLP</mtext></ci><apply id="S3.E7.m1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.1">superscript</csymbol><apply id="S3.E7.m1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E7.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.2">ğ‘¢</ci><apply id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3"><times id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.2">ğ‘–</ci><ci id="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.1.1.1.3.3">ğ‘—</ci></apply></apply></apply><apply id="S3.E7.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.3"><eq id="S3.E7.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.3.1"></eq><ci id="S3.E7.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S3.E7.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.E7.m1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.3">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">s_{j}=\texttt{MLP}([u_{ij}]_{i=1}^{N})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS3.p4.12" class="ltx_p">where <math id="S3.SS3.p4.10.m1.1" class="ltx_Math" alttext="s_{j}" display="inline"><semantics id="S3.SS3.p4.10.m1.1a"><msub id="S3.SS3.p4.10.m1.1.1" xref="S3.SS3.p4.10.m1.1.1.cmml"><mi id="S3.SS3.p4.10.m1.1.1.2" xref="S3.SS3.p4.10.m1.1.1.2.cmml">s</mi><mi id="S3.SS3.p4.10.m1.1.1.3" xref="S3.SS3.p4.10.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.10.m1.1b"><apply id="S3.SS3.p4.10.m1.1.1.cmml" xref="S3.SS3.p4.10.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.10.m1.1.1.1.cmml" xref="S3.SS3.p4.10.m1.1.1">subscript</csymbol><ci id="S3.SS3.p4.10.m1.1.1.2.cmml" xref="S3.SS3.p4.10.m1.1.1.2">ğ‘ </ci><ci id="S3.SS3.p4.10.m1.1.1.3.cmml" xref="S3.SS3.p4.10.m1.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.10.m1.1c">s_{j}</annotation></semantics></math> denotes the style code of <math id="S3.SS3.p4.11.m2.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS3.p4.11.m2.1a"><mi id="S3.SS3.p4.11.m2.1.1" xref="S3.SS3.p4.11.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.11.m2.1b"><ci id="S3.SS3.p4.11.m2.1.1.cmml" xref="S3.SS3.p4.11.m2.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.11.m2.1c">j</annotation></semantics></math>-th categories. Then, the mask and style codes <math id="S3.SS3.p4.12.m3.1" class="ltx_Math" alttext="s\in\mathbb{R}^{C\times 18\times 512}" display="inline"><semantics id="S3.SS3.p4.12.m3.1a"><mrow id="S3.SS3.p4.12.m3.1.1" xref="S3.SS3.p4.12.m3.1.1.cmml"><mi id="S3.SS3.p4.12.m3.1.1.2" xref="S3.SS3.p4.12.m3.1.1.2.cmml">s</mi><mo id="S3.SS3.p4.12.m3.1.1.1" xref="S3.SS3.p4.12.m3.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS3.p4.12.m3.1.1.3" xref="S3.SS3.p4.12.m3.1.1.3.cmml"><mi id="S3.SS3.p4.12.m3.1.1.3.2" xref="S3.SS3.p4.12.m3.1.1.3.2.cmml">â„</mi><mrow id="S3.SS3.p4.12.m3.1.1.3.3" xref="S3.SS3.p4.12.m3.1.1.3.3.cmml"><mi id="S3.SS3.p4.12.m3.1.1.3.3.2" xref="S3.SS3.p4.12.m3.1.1.3.3.2.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p4.12.m3.1.1.3.3.1" xref="S3.SS3.p4.12.m3.1.1.3.3.1.cmml">Ã—</mo><mn id="S3.SS3.p4.12.m3.1.1.3.3.3" xref="S3.SS3.p4.12.m3.1.1.3.3.3.cmml">18</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p4.12.m3.1.1.3.3.1a" xref="S3.SS3.p4.12.m3.1.1.3.3.1.cmml">Ã—</mo><mn id="S3.SS3.p4.12.m3.1.1.3.3.4" xref="S3.SS3.p4.12.m3.1.1.3.3.4.cmml">512</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.12.m3.1b"><apply id="S3.SS3.p4.12.m3.1.1.cmml" xref="S3.SS3.p4.12.m3.1.1"><in id="S3.SS3.p4.12.m3.1.1.1.cmml" xref="S3.SS3.p4.12.m3.1.1.1"></in><ci id="S3.SS3.p4.12.m3.1.1.2.cmml" xref="S3.SS3.p4.12.m3.1.1.2">ğ‘ </ci><apply id="S3.SS3.p4.12.m3.1.1.3.cmml" xref="S3.SS3.p4.12.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p4.12.m3.1.1.3.1.cmml" xref="S3.SS3.p4.12.m3.1.1.3">superscript</csymbol><ci id="S3.SS3.p4.12.m3.1.1.3.2.cmml" xref="S3.SS3.p4.12.m3.1.1.3.2">â„</ci><apply id="S3.SS3.p4.12.m3.1.1.3.3.cmml" xref="S3.SS3.p4.12.m3.1.1.3.3"><times id="S3.SS3.p4.12.m3.1.1.3.3.1.cmml" xref="S3.SS3.p4.12.m3.1.1.3.3.1"></times><ci id="S3.SS3.p4.12.m3.1.1.3.3.2.cmml" xref="S3.SS3.p4.12.m3.1.1.3.3.2">ğ¶</ci><cn type="integer" id="S3.SS3.p4.12.m3.1.1.3.3.3.cmml" xref="S3.SS3.p4.12.m3.1.1.3.3.3">18</cn><cn type="integer" id="S3.SS3.p4.12.m3.1.1.3.3.4.cmml" xref="S3.SS3.p4.12.m3.1.1.3.3.4">512</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.12.m3.1c">s\in\mathbb{R}^{C\times 18\times 512}</annotation></semantics></math> are fed into the mask-guided StyleGAN generator to synthesize the talking face. For the detailed architecture of the Mask-guided StyleGAN, please refer to the supplement.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p"><span id="S3.SS3.p5.1.1" class="ltx_text ltx_font_bold">Prior Learning</span>
To seamlessly integrate SGI into the overall framework, we randomly select a mask from the images within a 15-frame range of the input image. Through such a training strategy, the model can learn the priors of semantic regions like teeth and eyes. Specifically, when given an image with closed mouth but a randomly selected mask corresponds to a visible-teeth state, it learns to model the teethâ€™ prior information and can naturally connect with the TSG module.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p"><span id="S3.SS3.p6.1.1" class="ltx_text ltx_font_bold">Loss Functions</span> SGI is trained with a series of weighted objectives: pixel-wise and LPIPS loss for perpetual quality, id loss to prevent identity drift, a face parsing loss and an adversarial loss. The details are described in our Appendix.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Experimental settings</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Dataset</span> Since StyleGANÂ <cite class="ltx_cite ltx_citemacro_citep">(Karras etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite> typically generates high resolution images, e.g. 512 or 1024, while most existing talking face datasets have a lower resolution of 256 or below, we opt to train on the HDTF datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2021</a>)</cite> for high-quality talking face synthesis. The HDTF dataset is collected from YouTube website published in the last two years, comprising around 16 hours of videos ranging from 720P to 1080P resolution. It contains over 300 subjects and 10k distinct sentences. We collected a total of 392 videos, with 347 used for training and the remaining 45 for testing. The test set comprises videos with complex backgrounds and rich textures, thereby offering a comprehensive evaluation of the model performance.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2409.03605/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="212" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Qualitative results on MEAD dataset. Our model demonstrates superior overall performance, particularly in preserving fine-grained textures (e.g. teeth)</figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Metrics</span>
We conduct quantitative evaluations on several widely used metrics. To evaluate the lip synchronization, we adopt the
confidence score of SyncNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Chung and Zisserman, <a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite> (<span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_bold">Sync</span>) and Landmark Distance around mouths (<span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_bold">M-LMD</span>)Â <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite>. To evaluate the accuracy of generated facial expressions, we adopt the Landmark Distance on the whole face (<span id="S4.SS1.p2.1.4" class="ltx_text ltx_font_bold">F-LMD</span>). To evaluate the quality of generated talking face videos, we adopt <span id="S4.SS1.p2.1.5" class="ltx_text ltx_font_bold">PSNR</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2023a</a>)</cite>, <span id="S4.SS1.p2.1.6" class="ltx_text ltx_font_bold">SSIM</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2004</a>)</cite>, <span id="S4.SS1.p2.1.7" class="ltx_text ltx_font_bold">FID</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Heusel etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2017</a>)</cite> and <span id="S4.SS1.p2.1.8" class="ltx_text ltx_font_bold">LPIPS</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2018</a>)</cite>. To measure the Temporal coherence of generated videos, we employ <span id="S4.SS1.p2.1.9" class="ltx_text ltx_font_bold">FVD</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Unterthiner etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite>. Higher scores indicate better performance for Sync, PSNR, and SSIM, while lower scores are better for F-LMD, M-LMD, FID, LPIPS and FVD.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2409.03605/assets/resource/talking_face.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="586" height="379" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Qualitative comparisons of our results with several state-of-the-art methods for talking face synthesis. our method produces high-fidelity video frames with rich textural details, while other methods struggle to preserve identity and contain artifacts. It is worth noting that AD-NeRF needs to train on these two identities respectively to produce the results.</figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.3" class="ltx_p"><span id="S4.SS1.p3.3.1" class="ltx_text ltx_font_bold">Implementation Details</span>
We use PyTorchÂ <cite class="ltx_cite ltx_citemacro_citep">(Paszke etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite> to implement our framework. We train TSG module on a single NVIDIA A100 GPU with 40GB, while SGI module is trained on 4 NVIDIA A100 GPUs. In stage 1, We crop and resize face to 512<math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mo id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><times id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">\times</annotation></semantics></math>512. Speech waveforms are pre-processed to mel-spectrogram with hop and window lengths, and mel bins are 12.5ms, 50ms, and 80. The batch size is set to 20 and the Adam solver with an initial learning rate of 1e-4 (<math id="S4.SS1.p3.2.m2.2" class="ltx_Math" alttext="\beta_{1}=0.5,\beta_{2}=0.999" display="inline"><semantics id="S4.SS1.p3.2.m2.2a"><mrow id="S4.SS1.p3.2.m2.2.2.2" xref="S4.SS1.p3.2.m2.2.2.3.cmml"><mrow id="S4.SS1.p3.2.m2.1.1.1.1" xref="S4.SS1.p3.2.m2.1.1.1.1.cmml"><msub id="S4.SS1.p3.2.m2.1.1.1.1.2" xref="S4.SS1.p3.2.m2.1.1.1.1.2.cmml"><mi id="S4.SS1.p3.2.m2.1.1.1.1.2.2" xref="S4.SS1.p3.2.m2.1.1.1.1.2.2.cmml">Î²</mi><mn id="S4.SS1.p3.2.m2.1.1.1.1.2.3" xref="S4.SS1.p3.2.m2.1.1.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS1.p3.2.m2.1.1.1.1.1" xref="S4.SS1.p3.2.m2.1.1.1.1.1.cmml">=</mo><mn id="S4.SS1.p3.2.m2.1.1.1.1.3" xref="S4.SS1.p3.2.m2.1.1.1.1.3.cmml">0.5</mn></mrow><mo id="S4.SS1.p3.2.m2.2.2.2.3" xref="S4.SS1.p3.2.m2.2.2.3a.cmml">,</mo><mrow id="S4.SS1.p3.2.m2.2.2.2.2" xref="S4.SS1.p3.2.m2.2.2.2.2.cmml"><msub id="S4.SS1.p3.2.m2.2.2.2.2.2" xref="S4.SS1.p3.2.m2.2.2.2.2.2.cmml"><mi id="S4.SS1.p3.2.m2.2.2.2.2.2.2" xref="S4.SS1.p3.2.m2.2.2.2.2.2.2.cmml">Î²</mi><mn id="S4.SS1.p3.2.m2.2.2.2.2.2.3" xref="S4.SS1.p3.2.m2.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS1.p3.2.m2.2.2.2.2.1" xref="S4.SS1.p3.2.m2.2.2.2.2.1.cmml">=</mo><mn id="S4.SS1.p3.2.m2.2.2.2.2.3" xref="S4.SS1.p3.2.m2.2.2.2.2.3.cmml">0.999</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.2b"><apply id="S4.SS1.p3.2.m2.2.2.3.cmml" xref="S4.SS1.p3.2.m2.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p3.2.m2.2.2.3a.cmml" xref="S4.SS1.p3.2.m2.2.2.2.3">formulae-sequence</csymbol><apply id="S4.SS1.p3.2.m2.1.1.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1.1.1"><eq id="S4.SS1.p3.2.m2.1.1.1.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1.1.1.1"></eq><apply id="S4.SS1.p3.2.m2.1.1.1.1.2.cmml" xref="S4.SS1.p3.2.m2.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p3.2.m2.1.1.1.1.2.1.cmml" xref="S4.SS1.p3.2.m2.1.1.1.1.2">subscript</csymbol><ci id="S4.SS1.p3.2.m2.1.1.1.1.2.2.cmml" xref="S4.SS1.p3.2.m2.1.1.1.1.2.2">ğ›½</ci><cn type="integer" id="S4.SS1.p3.2.m2.1.1.1.1.2.3.cmml" xref="S4.SS1.p3.2.m2.1.1.1.1.2.3">1</cn></apply><cn type="float" id="S4.SS1.p3.2.m2.1.1.1.1.3.cmml" xref="S4.SS1.p3.2.m2.1.1.1.1.3">0.5</cn></apply><apply id="S4.SS1.p3.2.m2.2.2.2.2.cmml" xref="S4.SS1.p3.2.m2.2.2.2.2"><eq id="S4.SS1.p3.2.m2.2.2.2.2.1.cmml" xref="S4.SS1.p3.2.m2.2.2.2.2.1"></eq><apply id="S4.SS1.p3.2.m2.2.2.2.2.2.cmml" xref="S4.SS1.p3.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p3.2.m2.2.2.2.2.2.1.cmml" xref="S4.SS1.p3.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S4.SS1.p3.2.m2.2.2.2.2.2.2.cmml" xref="S4.SS1.p3.2.m2.2.2.2.2.2.2">ğ›½</ci><cn type="integer" id="S4.SS1.p3.2.m2.2.2.2.2.2.3.cmml" xref="S4.SS1.p3.2.m2.2.2.2.2.2.3">2</cn></apply><cn type="float" id="S4.SS1.p3.2.m2.2.2.2.2.3.cmml" xref="S4.SS1.p3.2.m2.2.2.2.2.3">0.999</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.2c">\beta_{1}=0.5,\beta_{2}=0.999</annotation></semantics></math>) is utilized for optimization. In stage 2, we set the batch size to 4 for each GPU and initialize the learning rate as 1e-4 with the Adam optimizer (<math id="S4.SS1.p3.3.m3.2" class="ltx_Math" alttext="\beta_{1}=0.9,\beta_{2}=0.999" display="inline"><semantics id="S4.SS1.p3.3.m3.2a"><mrow id="S4.SS1.p3.3.m3.2.2.2" xref="S4.SS1.p3.3.m3.2.2.3.cmml"><mrow id="S4.SS1.p3.3.m3.1.1.1.1" xref="S4.SS1.p3.3.m3.1.1.1.1.cmml"><msub id="S4.SS1.p3.3.m3.1.1.1.1.2" xref="S4.SS1.p3.3.m3.1.1.1.1.2.cmml"><mi id="S4.SS1.p3.3.m3.1.1.1.1.2.2" xref="S4.SS1.p3.3.m3.1.1.1.1.2.2.cmml">Î²</mi><mn id="S4.SS1.p3.3.m3.1.1.1.1.2.3" xref="S4.SS1.p3.3.m3.1.1.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS1.p3.3.m3.1.1.1.1.1" xref="S4.SS1.p3.3.m3.1.1.1.1.1.cmml">=</mo><mn id="S4.SS1.p3.3.m3.1.1.1.1.3" xref="S4.SS1.p3.3.m3.1.1.1.1.3.cmml">0.9</mn></mrow><mo id="S4.SS1.p3.3.m3.2.2.2.3" xref="S4.SS1.p3.3.m3.2.2.3a.cmml">,</mo><mrow id="S4.SS1.p3.3.m3.2.2.2.2" xref="S4.SS1.p3.3.m3.2.2.2.2.cmml"><msub id="S4.SS1.p3.3.m3.2.2.2.2.2" xref="S4.SS1.p3.3.m3.2.2.2.2.2.cmml"><mi id="S4.SS1.p3.3.m3.2.2.2.2.2.2" xref="S4.SS1.p3.3.m3.2.2.2.2.2.2.cmml">Î²</mi><mn id="S4.SS1.p3.3.m3.2.2.2.2.2.3" xref="S4.SS1.p3.3.m3.2.2.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS1.p3.3.m3.2.2.2.2.1" xref="S4.SS1.p3.3.m3.2.2.2.2.1.cmml">=</mo><mn id="S4.SS1.p3.3.m3.2.2.2.2.3" xref="S4.SS1.p3.3.m3.2.2.2.2.3.cmml">0.999</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.2b"><apply id="S4.SS1.p3.3.m3.2.2.3.cmml" xref="S4.SS1.p3.3.m3.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p3.3.m3.2.2.3a.cmml" xref="S4.SS1.p3.3.m3.2.2.2.3">formulae-sequence</csymbol><apply id="S4.SS1.p3.3.m3.1.1.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1.1.1"><eq id="S4.SS1.p3.3.m3.1.1.1.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1.1.1.1"></eq><apply id="S4.SS1.p3.3.m3.1.1.1.1.2.cmml" xref="S4.SS1.p3.3.m3.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p3.3.m3.1.1.1.1.2.1.cmml" xref="S4.SS1.p3.3.m3.1.1.1.1.2">subscript</csymbol><ci id="S4.SS1.p3.3.m3.1.1.1.1.2.2.cmml" xref="S4.SS1.p3.3.m3.1.1.1.1.2.2">ğ›½</ci><cn type="integer" id="S4.SS1.p3.3.m3.1.1.1.1.2.3.cmml" xref="S4.SS1.p3.3.m3.1.1.1.1.2.3">1</cn></apply><cn type="float" id="S4.SS1.p3.3.m3.1.1.1.1.3.cmml" xref="S4.SS1.p3.3.m3.1.1.1.1.3">0.9</cn></apply><apply id="S4.SS1.p3.3.m3.2.2.2.2.cmml" xref="S4.SS1.p3.3.m3.2.2.2.2"><eq id="S4.SS1.p3.3.m3.2.2.2.2.1.cmml" xref="S4.SS1.p3.3.m3.2.2.2.2.1"></eq><apply id="S4.SS1.p3.3.m3.2.2.2.2.2.cmml" xref="S4.SS1.p3.3.m3.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p3.3.m3.2.2.2.2.2.1.cmml" xref="S4.SS1.p3.3.m3.2.2.2.2.2">subscript</csymbol><ci id="S4.SS1.p3.3.m3.2.2.2.2.2.2.cmml" xref="S4.SS1.p3.3.m3.2.2.2.2.2.2">ğ›½</ci><cn type="integer" id="S4.SS1.p3.3.m3.2.2.2.2.2.3.cmml" xref="S4.SS1.p3.3.m3.2.2.2.2.2.3">2</cn></apply><cn type="float" id="S4.SS1.p3.3.m3.2.2.2.2.3.cmml" xref="S4.SS1.p3.3.m3.2.2.2.2.3">0.999</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.2c">\beta_{1}=0.9,\beta_{2}=0.999</annotation></semantics></math>). The generator is initialized with StyleGAN weightsÂ <cite class="ltx_cite ltx_citemacro_citep">(Karras etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite>. In the first stage, we train the model with only the cross entropy loss for approximately 50K iterations, then incorporate the expert SyncNet to supervise lip movements for an extra 50k iterations. In the second stage, we train the model for 400K iterations.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Experimental Results</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Qualitative Talking Segmentation Results</span>
In the first sub-network, we visualize the talking segmentation results illustrated in <a href="#S4.F5" title="In 4.2. Experimental Results â€£ 4. Experiments â€£ SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">fig.</span>Â <span class="ltx_text ltx_ref_tag">5</span></a>. It can be observed that the generated segmentations effectively delineate distinct facial regions, even elaborating details such as earrings. Additionally, the synthesized lips exhibit strong synchronization with the ground truth. Subsequently, the high-quality segmentations produced by TSG are utilized as guidance for the SGI to deliver the final output.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Quantitative Results</span>
We compare several state of the art methods: Wav2LipÂ <cite class="ltx_cite ltx_citemacro_citep">(Prajwal etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite>, SadTalkerÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2023</a>)</cite> (3DMM-based), DiffTalkÂ <cite class="ltx_cite ltx_citemacro_citep">(Shen etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite> (diffusion-based), StyleHEATÂ <cite class="ltx_cite ltx_citemacro_citep">(Yin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2022</a>)</cite> (styleGAN-based) and AD-NeRF <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite> (NeRF-based). We conduct the experiments in the self-driven setting on the test set, where the videos are not seen during training. In these methods, the head poses of Wav2Lip, DiffTalk, and SegTalker are fixed in their samples. For other methods, head poses are randomly generated. The results of the quantitative evaluation are reported in <a href="#S3.T1" title="In 3.2. Talking Segmentation Generation â€£ 3. Proposed Methods â€£ SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">table</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Our method achieves better visual quality, temporal consistency, and also shows comparable performance in terms of lip synchronization metrics. Since DiffTalk takes ground truth landmark as conditional input, it is reasonable for DiffTalk to achieve the lowest LMD in self-driven sets. However, DiffTalk performs poorly in frame-to-frame coherence, especially with significant jitter in the mouth region (see supplementary video). In synchronization, despite scoring slightly lower on metrics relative to Wav2lip, our method achieves a similar score with ground truth videos. Furthermore, Our method outperforms existing state-of-the-art approaches on both pixel-level metrics such as PSNR, as well as high-level perceptual metrics including FID and LPIPS, thereby achieving enhanced visual quality. We additionally measure the FVD metric and Our FVD score is the best. This means that our method is able to generate temporal consistency and visual-satisfied videos. This is largely attributed to the implementation of SGI module. By explicitly disentangling different semantic regions via segmentation, SGI can better preserve texture details during image reconstruction. Moreover, our method is the only approach that can simultaneously achieve facial editing and background replacement which will be discussed in the following section.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2409.03605/assets/x4.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="307" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Visualization of synthesized segmentation(row 1, row 2) and real images(row 2, row 4).</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2409.03605/assets/x5.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="243" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>(a) Visualization of the regional style codes of a speaker. (b) Visualization of the style codes of 8 speakers in a particular region (here hair for example).</figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2409.03605/assets/x6.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="424" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Qualitative results of local editing. Our method produces more high-fidelity results while maintaining the details and identity information of other regions.</figcaption>
</figure>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">Qualitative Results</span>
To qualitatively evaluate the different methods, we perform uniformly sampled images from two synthesized talking face videos which are shown in <a href="#S4.F4" title="In 4.1. Experimental settings â€£ 4. Experiments â€£ SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">fig.</span>Â <span class="ltx_text ltx_ref_tag">4</span></a>. Specifically, the ground truth videos are provided in the first row where synthesized images of different methods follow the next and ours are illustrated in the bottom row. In comparison to Wav2lipÂ <cite class="ltx_cite ltx_citemacro_citep">(Prajwal etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite>, our results exhibit enhanced detail in the lip and teeth regions.
For SadTalkerÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2023</a>)</cite>, It employs single-frame animation, which inevitably causes background movement and generates artifacts when wrapping motion sequences. Additionally, it also cannot handle the scenarios with changing background. The incorporation of segmentation in our approach allows high-quality background replacement. DiffTalkÂ <cite class="ltx_cite ltx_citemacro_citep">(Cheng etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite> can generate visually satisfying results; however, diffusion-based methods still face significant challenges in terms of temporal consistency. The mouth area of DiffTalk is prone to shaking and leads to poor lip synchronization performance. StyleHEATÂ <cite class="ltx_cite ltx_citemacro_citep">(Yin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2022</a>)</cite> is also a StyleGAN-based approach, but it cannot directly drive speech to generate a talking face video. Instead, it requires the assistance of SadTalker to extract features from the first stage, then warps the features to generate video. Therefore, the quality of the video generated by StyleHEAT is limited by the quality of the output generated by SadTalker.
AD-NeRFÂ <cite class="ltx_cite ltx_citemacro_citep">(Guo etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite> is a NeRF-based method capable of generating high-quality head part but consistently exists artifact in the connection between the head and neck. Moreover, its inference is time-consuming (10s per image) and requires fine-tuning for each speaker (about 20 hours).
In addition, to demonstrate the generalization ability of the proposed method, we conduct validation on another dataset, as illustrated in <a href="#S4.F3" title="In 4.1. Experimental settings â€£ 4. Experiments â€£ SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">fig.</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>. Compared to other methods, our method excels in preserving texture details, particularly in fine-grained structural regions such as teeth.
In contrast, our method can produce more realistic and high-fidelity results while achieving accurate lip sync, satisfactory identity preservation and rich facial textures. For more comparison results, please refer to our demo videos in the supplement materials.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_bold">Disentangled Semantics Visualization</span>
To demonstrate the Disentanglement of the model across different semantic regions, we employ t-distributed stochastic neighbor embedding(t-SNE)Â <cite class="ltx_cite ltx_citemacro_citep">(vanÂ der Maaten and Hinton, <a href="#bib.bib43" title="" class="ltx_ref">2008</a>)</cite> visualization to illustrate the per-region features, as depicted in <a href="#S4.F6" title="In 4.2. Experimental Results â€£ 4. Experiments â€£ SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>(a). For Clarity, We select eight sufficiently representative regions (appear in all videos) and utilize the mask-guided encoder to extract style codes from these semantic regions. In <a href="#S4.F6" title="In 4.2. Experimental Results â€£ 4. Experiments â€£ SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>(a), each region is marked with a distinct color. As shown, the style codes of same region cluster in the style space and different semantic regions are explicitly separated substantially. This demonstrates that our mask-guided encoder can accurately disentangle different region features. Furthermore, in <a href="#S4.F6" title="In 4.2. Experimental Results â€£ 4. Experiments â€£ SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">fig.</span>Â <span class="ltx_text ltx_ref_tag">6</span></a>(b), we visualize the features of different IDs within a particular region to demonstrate the capability of the encoder. It can be seen that the style codes of different IDs are fully disentangled and our model can learn meaningful features.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p"><span id="S4.SS2.p6.1.1" class="ltx_text ltx_font_bold">Facial Editing and Swapping Results</span>
Our method also supports facial editing and background swaps while generating video. Given a reference image and a sequence of source images, our method can transfer the candidate region texture to the source images. As depicted in <a href="#S4.F7" title="In 4.2. Experimental Results â€£ 4. Experiments â€£ SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">fig.</span>Â <span class="ltx_text ltx_ref_tag">7</span></a>, we illustrate three local editing tasks, including fine-grained hair editing, lip makeup, and eyebrow modifications. Besides, we also can manipulate blinking in a controllable manner by simply editing the eye regions of mask, illustrated in <a href="#S1.F1" title="In 1. Introduction â€£ SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">fig.</span>Â <span class="ltx_text ltx_ref_tag">1</span></a>. Compared with existing blink methods, our method does not design a specialized module for blinking editing, as well as enables other types of local editing, substantially enhancing model applicability and scalability. Additionally, our model intrinsically disentangles the foreground and background, allowing for seamless background swapping and widening the application scenarios of talking faces. As shown in <a href="#S4.F8" title="In 4.3. Ablation Study â€£ 4. Experiments â€£ SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">fig.</span>Â <span class="ltx_text ltx_ref_tag">8</span></a>, with a provided reference background image and a video segment, we can not only generate synchronized talking face video but also achieve video background swapping, resulting in high-fidelity and photo-realistic video.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Ablation results under different configurations. P.L. and C.S. denote prior learning and cross entropy, respectively.</figcaption>
<table id="S4.T2.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.4.4" class="ltx_tr">
<th id="S4.T2.4.4.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Conf.</th>
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Sync<math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mo stretchy="false" id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">FID<math id="S4.T2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">\downarrow</annotation></semantics></math>
</th>
<th id="S4.T2.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">PSNR<math id="S4.T2.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.3.3.3.m1.1a"><mo stretchy="false" id="S4.T2.3.3.3.m1.1.1" xref="S4.T2.3.3.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T2.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">SSIM<math id="S4.T2.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.4.4.4.m1.1a"><mo stretchy="false" id="S4.T2.4.4.4.m1.1.1" xref="S4.T2.4.4.4.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.m1.1b"><ci id="S4.T2.4.4.4.m1.1.1.cmml" xref="S4.T2.4.4.4.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.4.5.1" class="ltx_tr">
<th id="S4.T2.4.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">w/o P.L.</th>
<td id="S4.T2.4.5.1.2" class="ltx_td ltx_align_center ltx_border_t">5.314</td>
<td id="S4.T2.4.5.1.3" class="ltx_td ltx_align_center ltx_border_t">28.254</td>
<td id="S4.T2.4.5.1.4" class="ltx_td ltx_align_center ltx_border_t">28.685</td>
<td id="S4.T2.4.5.1.5" class="ltx_td ltx_align_center ltx_border_t">0.847</td>
</tr>
<tr id="S4.T2.4.6.2" class="ltx_tr">
<th id="S4.T2.4.6.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">w/o C.S.</th>
<td id="S4.T2.4.6.2.2" class="ltx_td ltx_align_center">3.126</td>
<td id="S4.T2.4.6.2.3" class="ltx_td ltx_align_center">63.264</td>
<td id="S4.T2.4.6.2.4" class="ltx_td ltx_align_center">14.257</td>
<td id="S4.T2.4.6.2.5" class="ltx_td ltx_align_center">0.479</td>
</tr>
<tr id="S4.T2.4.7.3" class="ltx_tr">
<th id="S4.T2.4.7.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">w/o SyncNet</th>
<td id="S4.T2.4.7.3.2" class="ltx_td ltx_align_center">4.174</td>
<td id="S4.T2.4.7.3.3" class="ltx_td ltx_align_center">10.647</td>
<td id="S4.T2.4.7.3.4" class="ltx_td ltx_align_center">32.036</td>
<td id="S4.T2.4.7.3.5" class="ltx_td ltx_align_center">0.913</td>
</tr>
<tr id="S4.T2.4.8.4" class="ltx_tr">
<th id="S4.T2.4.8.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">All pipeline</th>
<td id="S4.T2.4.8.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.4.8.4.2.1" class="ltx_text ltx_font_bold">6.872</span></td>
<td id="S4.T2.4.8.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.4.8.4.3.1" class="ltx_text ltx_font_bold">10.348</span></td>
<td id="S4.T2.4.8.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.4.8.4.4.1" class="ltx_text ltx_font_bold">33.590</span></td>
<td id="S4.T2.4.8.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.4.8.4.5.1" class="ltx_text ltx_font_bold">0.934</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>User Study</figcaption>
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:195.1pt;height:45.2pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-137.7pt,31.6pt) scale(0.41467735381497,0.41467735381497) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S4.T3.1.1.1.1.1.1" class="ltx_text">Method</span></th>
<th id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Lip</th>
<th id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Face</th>
<th id="S4.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ID</th>
<th id="S4.T3.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Overall</th>
</tr>
<tr id="S4.T3.1.1.2.2" class="ltx_tr">
<th id="S4.T3.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Sync.</th>
<th id="S4.T3.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Quality</th>
<th id="S4.T3.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Preserve.</th>
<th id="S4.T3.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Naturalness</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.3.1" class="ltx_tr">
<th id="S4.T3.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Wav2LipÂ <cite class="ltx_cite ltx_citemacro_citep">(Prajwal etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2020</a>)</cite>
</th>
<td id="S4.T3.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">32.4%</td>
<td id="S4.T3.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">5.6%</td>
<td id="S4.T3.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">20.4%</td>
<td id="S4.T3.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">15.3%</td>
</tr>
<tr id="S4.T3.1.1.4.2" class="ltx_tr">
<th id="S4.T3.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">SadTalkerÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="S4.T3.1.1.4.2.2" class="ltx_td ltx_align_center">16.3%</td>
<td id="S4.T3.1.1.4.2.3" class="ltx_td ltx_align_center">13.7%</td>
<td id="S4.T3.1.1.4.2.4" class="ltx_td ltx_align_center">22.1%</td>
<td id="S4.T3.1.1.4.2.5" class="ltx_td ltx_align_center">12.9%</td>
</tr>
<tr id="S4.T3.1.1.5.3" class="ltx_tr">
<th id="S4.T3.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">StyleHEATÂ <cite class="ltx_cite ltx_citemacro_citep">(Yin etÂ al<span class="ltx_text">.</span>, <a href="#bib.bib53" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.T3.1.1.5.3.2" class="ltx_td ltx_align_center">9.8%</td>
<td id="S4.T3.1.1.5.3.3" class="ltx_td ltx_align_center">10.5%</td>
<td id="S4.T3.1.1.5.3.4" class="ltx_td ltx_align_center">5.4%</td>
<td id="S4.T3.1.1.5.3.5" class="ltx_td ltx_align_center">6.4%</td>
</tr>
<tr id="S4.T3.1.1.6.4" class="ltx_tr">
<th id="S4.T3.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">Ours</th>
<td id="S4.T3.1.1.6.4.2" class="ltx_td ltx_align_center ltx_border_bb">41.5%</td>
<td id="S4.T3.1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_bb">70.2%</td>
<td id="S4.T3.1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_bb">52.1%</td>
<td id="S4.T3.1.1.6.4.5" class="ltx_td ltx_align_center ltx_border_bb">65.4%</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Ablation Study</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In this section, we perform an ablation study to evaluate the 2 sub-agent networks, which are shown in <a href="#S4.T2" title="In 4.2. Experimental Results â€£ 4. Experiments â€£ SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>. We develop 3 variants with the modification of the framework corresponding to the 2 sub-network: 1) <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_italic">w/o prior learning</span>, 2) <span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_italic">w/o cross entropy</span> and 3) <span id="S4.SS3.p1.1.3" class="ltx_text ltx_font_italic">w/o SyncNet</span>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The first component is the implementation of <span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_italic">priors learning</span>. Without <span id="S4.SS3.p2.1.2" class="ltx_text ltx_font_italic">priors learning</span>, the method produces poor visual quality. This mechanism offers structural prior information for the mouth and teeth regions, which facilitates the model learning personalized details of these areas.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">The second component is the <span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_italic">cross entropy</span>. Without <span id="S4.SS3.p3.1.2" class="ltx_text ltx_font_italic">cross entropy</span>, the method exhibits very poor performance whenever on both lip synchronization and visual quality. By employing cross-entropy loss instead of L1 loss, we overcome the issue of erroneous segmentation predictions around region boundaries, improving the modelâ€™s control over different semantic areas. Furthermore, <span id="S4.SS3.p3.1.3" class="ltx_text ltx_font_italic">cross entropy</span> also facilitates learning lip movements from speech, exhibiting a certain extent of lip synchronization.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">The last component is the <span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_italic">SyncNet</span>, which is performed to reinforce the model learning the mapping from speech to lip. The performance of visual quality is comparable to the baseline when we do not apply <span id="S4.SS3.p4.1.2" class="ltx_text ltx_font_italic">SyncNet</span>. However, without <span id="S4.SS3.p4.1.3" class="ltx_text ltx_font_italic">SyncNet</span> would lead to poor lip synchronization performance, which is demonstrated in <a href="#S4.T2" title="In 4.2. Experimental Results â€£ 4. Experiments â€£ SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2409.03605/assets/x7.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="427" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>Example of Swapping background. Given a video and a background image, our method can produce natural and photo-realistic swapping videos.</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>User Study</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We conduct a user study to evaluate the performance of all the methods. We choose 20 videos with 10 seconds clips as our test set. These samples contain different poses, ages, backgrounds and expressions to show the generalization of our method. We invite 15 participants and let them choose the best method in terms of face quality, lip synchronization, identity preservation and overall naturalness. The results are shown in <a href="#S4.T3" title="In 4.2. Experimental Results â€£ 4. Experiments â€£ SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">table</span>Â <span class="ltx_text ltx_ref_tag">3</span></a>. It demonstrates that our model outperforms other methods across multiple dimensions.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we present a new framework SegTalker for talking face generation, which disentangles the lip movements and textures of different facial components by employing a new intermediate representation of segmentation. The overall framework consists of two sub-networks: TSG and SGI network. TSG is responsible for the mapping from speech to lip movement in the segmentation domain and SGI employs a multi-scale encoder to project source image into per-region style codes. Then, a mask-guided generator integrates the style codes and synthesized segmentation to obtain the final frame. Moreover, By simply manipulating different semantic regions of segmentation or swapping the different textures from a reference image, Our method can seamlessly integrate local editing and support coherent swapping background.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdal etÂ al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Rameen Abdal, Yipeng Qin, and Peter Wonka. 2019.

</span>
<span class="ltx_bibblock">Image2stylegan: How to embed images into the stylegan latent space?. In <em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer vision</em>. 4432â€“4441.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdal etÂ al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Rameen Abdal, Yipeng Qin, and Peter Wonka. 2020.

</span>
<span class="ltx_bibblock">Image2stylegan++: How to edit the embedded images?. In <em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 8296â€“8305.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alaluf etÂ al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Yuval Alaluf, Or Patashnik, and Daniel Cohen-Or. 2021.

</span>
<span class="ltx_bibblock">Restyle: A residual-based stylegan encoder via iterative refinement. In <em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 6711â€“6720.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alaluf etÂ al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and Amit Bermano. 2022.

</span>
<span class="ltx_bibblock">Hyperstyle: Stylegan inversion with hypernetworks for real image editing. In <em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer Vision and pattern recognition</em>. 18511â€“18521.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alghamdi etÂ al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
MohammedÂ Mousa Alghamdi, He Wang, AndrewÂ J. Bulpitt, and DavidÂ C. Hogg. 2022.

</span>
<span class="ltx_bibblock">Talking Head from Speech Audio using a Pre-trained Image Generator.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 30th ACM International Conference on Multimedia</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Lele Chen, Zhiheng Li, RossÂ K. Maddox, Zhiyao Duan, and Chenliang Xu. 2018.

</span>
<span class="ltx_bibblock">Lip Movements Generation at a Glance. In <em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>. 520â€“535.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng etÂ al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Kun Cheng, Xiaodong Cun, Yong Zhang, Menghan Xia, Fei Yin, Mingrui Zhu, Xuan Wang, Jue Wang, and Nannan Wang. 2022.

</span>
<span class="ltx_bibblock">Videoretalking: Audio-based lip synchronization for talking head video editing in the wild. In <em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">SIGGRAPH Asia 2022 Conference Papers</em>. 1â€“9.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng etÂ al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Xize Cheng, Rongjie Huang, Linjun Li, Tao Jin, Zehan Wang, Aoxiong Yin, Minglei Li, Xinyu Duan, Zhou Zhao, etÂ al<span id="bib.bib9.3.1" class="ltx_text">.</span> 2023a.

</span>
<span class="ltx_bibblock">TransFace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head Translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.15197</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng etÂ al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Xize Cheng, Tao Jin, Rongjie Huang, Linjun Li, Wang Lin, Zehan Wang, Ye Wang, Huadai Liu, Aoxiong Yin, and Zhou Zhao. 2023b.

</span>
<span class="ltx_bibblock">Mixspeech: Cross-modality self-learning with audio-visual stream mixup for visual speech translation and recognition. In <em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 15735â€“15745.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng etÂ al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2023c)</span>
<span class="ltx_bibblock">
Xize Cheng, Tao Jin, Linjun Li, Wang Lin, Xinyu Duan, and Zhou Zhao. 2023c.

</span>
<span class="ltx_bibblock">Opensr: Open-modality speech recognition via maintaining multi-modality alignment.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.06410</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung and Zisserman (2016)</span>
<span class="ltx_bibblock">
JoonÂ Son Chung and Andrew Zisserman. 2016.

</span>
<span class="ltx_bibblock">Out of Time: Automated Lip Sync in the Wild. In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ACCV Workshops</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow etÂ al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.

</span>
<span class="ltx_bibblock">Generative adversarial nets.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 27 (2014).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo etÂ al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun Bao, and Juyong Zhang. 2021.

</span>
<span class="ltx_bibblock">Ad-nerf: Audio driven neural radiance fields for talking head synthesis. In <em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 5784â€“5794.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heusel etÂ al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017.

</span>
<span class="ltx_bibblock">Gans trained by a two time-scale update rule converge to a local nash equilibrium.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Rongjie Huang, Huadai Liu, Xize Cheng, Yi Ren, Linjun Li, Zhenhui Ye, Jinzheng He, Lichao Zhang, Jinglin Liu, Xiang Yin, etÂ al<span id="bib.bib16.3.1" class="ltx_text">.</span> 2023.

</span>
<span class="ltx_bibblock">Av-transpeech: Audio-visual robust speech-to-speech translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.15403</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin etÂ al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Tao Jin, Xize Cheng, Linjun Li, Wang Lin, Ye Wang, and Zhou Zhao. 2023.

</span>
<span class="ltx_bibblock">Rethinking missing modality learning from a decoding perspective. In <em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st ACM International Conference on Multimedia</em>. 4431â€“4439.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang etÂ al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Kyoungkook Kang, Seongtae Kim, and Sunghyun Cho. 2021.

</span>
<span class="ltx_bibblock">Gan inversion for out-of-range images with geometric transformations. In <em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 13941â€“13949.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karras etÂ al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020.

</span>
<span class="ltx_bibblock">Analyzing and improving the image quality of stylegan. In <em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 8110â€“8119.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kicanaoglu etÂ al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Berkay Kicanaoglu, Pablo Garrido, and Gaurav Bharaj. 2023.

</span>
<span class="ltx_bibblock">Unsupervised Facial Performance Editing via Vector-Quantized StyleGAN Representations. In <em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 2371â€“2382.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Piotr DollÃ¡r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. 2017.

</span>
<span class="ltx_bibblock">Feature pyramid networks for object detection. In <em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2117â€“2125.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhian Liu, Maomao Li, Yong Zhang, Cairong Wang, Qi Zhang, Jue Wang, and Yongwei Nie. 2023.

</span>
<span class="ltx_bibblock">Fine-Grained Face Swapping via Regional GAN Inversion. In <em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 8578â€“8587.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meshry etÂ al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Moustafa Meshry, Saksham Suri, LarryÂ S Davis, and Abhinav Shrivastava. 2021.

</span>
<span class="ltx_bibblock">Learned spatial representations for few-shot talking-head synthesis. In <em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 13829â€“13838.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nitzan etÂ al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal Yarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, and Daniel Cohen-Or. 2022.

</span>
<span class="ltx_bibblock">Mystyle: A personalized generative prior.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">ACM Transactions on Graphics (TOG)</em> 41, 6 (2022), 1â€“10.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke etÂ al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, etÂ al<span id="bib.bib25.3.1" class="ltx_text">.</span> 2019.

</span>
<span class="ltx_bibblock">Pytorch: An imperative style, high-performance deep learning library.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.4.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 32 (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prajwal etÂ al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
KR Prajwal, Rudrabha Mukhopadhyay, VinayÂ P Namboodiri, and CV Jawahar. 2020.

</span>
<span class="ltx_bibblock">A lip sync expert is all you need for speech to lip generation in the wild. In <em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM international conference on multimedia</em>. 484â€“492.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Richardson etÂ al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. 2021.

</span>
<span class="ltx_bibblock">Encoding in style: a stylegan encoder for image-to-image translation. In <em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 2287â€“2296.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roich etÂ al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Daniel Roich, Ron Mokady, AmitÂ H Bermano, and Daniel Cohen-Or. 2022.

</span>
<span class="ltx_bibblock">Pivotal tuning for latent-based editing of real images.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">ACM Transactions on graphics (TOG)</em> 42, 1 (2022), 1â€“13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach etÂ al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. 2022.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models. In <em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 10684â€“10695.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ronneberger etÂ al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation. In <em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">Medical Image Computing and Computer-Assisted Intervention</em>. 234â€“241.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saha etÂ al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Rohit Saha, Brendan Duke, Florian Shkurti, GrahamÂ W Taylor, and Parham Aarabi. 2021.

</span>
<span class="ltx_bibblock">Loho: Latent optimization of hairstyles via orthogonalization. In <em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 1984â€“1993.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen etÂ al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Shuai Shen, Wanhua Li, Zheng Zhu, Yueqi Duan, Jie Zhou, and Jiwen Lu. 2022.

</span>
<span class="ltx_bibblock">Learning dynamic facial radiance fields for few-shot talking head synthesis. In <em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>. 666â€“682.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen etÂ al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng Zhu, Jie Zhou, and Jiwen Lu. 2023.

</span>
<span class="ltx_bibblock">DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation. In <em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 1982â€“1991.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen etÂ al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. 2020.

</span>
<span class="ltx_bibblock">Interpreting the latent space of gans for semantic face editing. In <em id="bib.bib34.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 9243â€“9252.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi etÂ al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2022b)</span>
<span class="ltx_bibblock">
Bowen Shi, Wei-Ning Hsu, Kushal Lakhotia, and Abdelrahman Mohamed. 2022b.

</span>
<span class="ltx_bibblock">Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.02184</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi etÂ al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2022a)</span>
<span class="ltx_bibblock">
Bowen Shi, Wei-Ning Hsu, and Abdelrahman Mohamed. 2022a.

</span>
<span class="ltx_bibblock">Robust Self-Supervised Audio-Visual Speech Recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.01763</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song etÂ al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Linsen Song, Wayne Wu, Chen Qian, Ran He, and ChenÂ Change Loy. 2022.

</span>
<span class="ltx_bibblock">Everybodyâ€™s talkinâ€™: Let me talk as you want.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Forensics and Security</em> 17 (2022), 585â€“598.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suwajanakorn etÂ al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Supasorn Suwajanakorn, StevenÂ M Seitz, and Ira Kemelmacher-Shlizerman. 2017.

</span>
<span class="ltx_bibblock">Synthesizing obama: learning lip sync from audio.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.3.1" class="ltx_emph ltx_font_italic">ACM Transactions on Graphics (ToG)</em> 36, 4 (2017), 1â€“13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan etÂ al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2024a)</span>
<span class="ltx_bibblock">
Jintao Tan, Xize Cheng, Lingyu Xiong, Lei Zhu, Xiandong Li, Xianjia Wu, Kai Gong, Minglei Li, and Yi Cai. 2024a.

</span>
<span class="ltx_bibblock">Landmark-guided Diffusion Model for High-fidelity and Temporally Coherent Talking Head Generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2408.01732</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan etÂ al<span id="bib.bib40.2.2.1" class="ltx_text">.</span> (2024b)</span>
<span class="ltx_bibblock">
Shuai Tan, Bin Ji, and Ye Pan. 2024b.

</span>
<span class="ltx_bibblock">Style2Talker: High-Resolution Talking Head Generation with Emotion Style and Art Style. In <em id="bib.bib40.3.1" class="ltx_emph ltx_font_italic">AAAI Conference on Artificial Intelligence</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tov etÂ al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. 2021.

</span>
<span class="ltx_bibblock">Designing an encoder for stylegan image manipulation.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">ACM Transactions on Graphics (TOG)</em> 40, 4 (2021), 1â€“14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Unterthiner etÂ al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, RaphaÃ«l Marinier, Marcin Michalski, and Sylvain Gelly. 2019.

</span>
<span class="ltx_bibblock">FVD: A new Metric for Video Generation. In <em id="bib.bib42.3.1" class="ltx_emph ltx_font_italic">ICLR Workshop on Deep Generative Models for Highly Structured Data</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">vanÂ der Maaten and Hinton (2008)</span>
<span class="ltx_bibblock">
Laurens vanÂ der Maaten and Geoffrey Hinton. 2008.

</span>
<span class="ltx_bibblock">Visualizing Data using t-SNE.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em> 9, 86 (2008), 2579â€“2605.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jiadong Wang, Xinyuan Qian, Malu Zhang, RobbyÂ T Tan, and Haizhou Li. 2023a.

</span>
<span class="ltx_bibblock">Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert. In <em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 14653â€“14662.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. 2021.

</span>
<span class="ltx_bibblock">One-shot free-view neural talking-head synthesis for video conferencing. In <em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>. 10039â€“10049.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2004)</span>
<span class="ltx_bibblock">
Zhou Wang, AlanÂ C Bovik, HamidÂ R Sheikh, and EeroÂ P Simoncelli. 2004.

</span>
<span class="ltx_bibblock">Image quality assessment: from error visibility to structural similarity.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">IEEE transactions on image processing</em> 13, 4 (2004), 600â€“612.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Zehan Wang, Ziang Zhang, Xize Cheng, Rongjie Huang, Luping Liu, Zhenhui Ye, Haifeng Huang, Yang Zhao, Tao Jin, Peng Gao, etÂ al<span id="bib.bib47.3.1" class="ltx_text">.</span> 2024.

</span>
<span class="ltx_bibblock">Molecule-Space: Free Lunch in Unified Multimodal Space via Knowledge Fusion.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2405.04883</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Zehan Wang, Yang Zhao, Haifeng Huang, Jiageng Liu, Aoxiong Yin, Li Tang, Linjun Li, Yongqi Wang, Ziang Zhang, and Zhou Zhao. 2023b.

</span>
<span class="ltx_bibblock">Connecting multi-modal contrastive representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em> 36 (2023), 22099â€“22114.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Yihan Wu, Junliang Guo, Xu Tan, Chen Zhang, Bohan Li, Ruihua Song, Lei He, Sheng Zhao, Arul Menezes, and Jiang Bian. 2023.

</span>
<span class="ltx_bibblock">Videodubber: Machine translation with speech-aware length control for video dubbing. In <em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>. 13772â€“13779.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Zongze Wu, Dani Lischinski, and Eli Shechtman. 2021.

</span>
<span class="ltx_bibblock">Stylespace analysis: Disentangled controls for stylegan image generation. In <em id="bib.bib50.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 12863â€“12872.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Yangyang Xu, Shengfeng He, Kwan-YeeÂ K Wong, and Ping Luo. 2023.

</span>
<span class="ltx_bibblock">RIGID: Recurrent GAN Inversion and Editing of Real Face Videos. In <em id="bib.bib51.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>. 13691â€“13701.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao etÂ al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Xu Yao, Alasdair Newson, Yann Gousseau, and Pierre Hellier. 2022.

</span>
<span class="ltx_bibblock">Feature-style encoder for style-based gan inversion.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.02183</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin etÂ al<span id="bib.bib53.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Fei Yin, Yong Zhang, Xiaodong Cun, Mingdeng Cao, Yanbo Fan, Xuan Wang, Qingyan Bai, Baoyuan Wu, Jue Wang, and Yujiu Yang. 2022.

</span>
<span class="ltx_bibblock">StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.3.1" class="ltx_emph ltx_font_italic">arxiv:2203.04036</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. 2018.

</span>
<span class="ltx_bibblock">Bisenet: Bilateral segmentation network for real-time semantic segmentation. In <em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">Proceedings of the European conference on computer vision</em>. 325â€“341.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng etÂ al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Libing Zeng, Lele Chen, Yi Xu, and Nima Kalantari. 2023.

</span>
<span class="ltx_bibblock">MyStyle++: A Controllable Personalized Generative Prior.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.04865</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al<span id="bib.bib56.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Richard Zhang, Phillip Isola, AlexeiÂ A Efros, Eli Shechtman, and Oliver Wang. 2018.

</span>
<span class="ltx_bibblock">The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. In <em id="bib.bib56.3.1" class="ltx_emph ltx_font_italic">CVPR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. 2023.

</span>
<span class="ltx_bibblock">SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation. In <em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 8652â€“8661.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. 2021.

</span>
<span class="ltx_bibblock">Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset. In <em id="bib.bib58.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 3661â€“3670.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong etÂ al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Weizhi Zhong, Chaowei Fang, Yinqi Cai, Pengxu Wei, Gangming Zhao, Liang Lin, and Guanbin Li. 2023.

</span>
<span class="ltx_bibblock">Identity-Preserving Talking Face Generation with Landmark and Appearance Priors. In <em id="bib.bib59.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 9729â€“9738.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al<span id="bib.bib60.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos Kalogerakis, and Dingzeyu Li. 2020.

</span>
<span class="ltx_bibblock">Makelttalk: speaker-aware talking-head animation.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.3.1" class="ltx_emph ltx_font_italic">ACM Transactions On Graphics (TOG)</em> 39, 6 (2020), 1â€“15.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.03603" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.03605" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.03605">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.03605" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.03606" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 01:37:24 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
