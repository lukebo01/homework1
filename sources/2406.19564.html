<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.19564] Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects</title><meta property="og:description" content="Yorùbá—an African language with roughly 47 million speakers—encompasses a continuum with several dialects. Recent efforts to develop NLP technologies for African languages have focused on their standard dialects, resul…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.19564">

<!--Generated on Fri Jul  5 23:40:10 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects </h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Orevaoghene Ahia<sup id="id1.1.id1" class="ltx_sup">1,5</sup>   Anuoluwapo Aremu<sup id="id2.2.id2" class="ltx_sup">4,5</sup>   Diana Abagyan<sup id="id3.3.id3" class="ltx_sup">1</sup>   Hila Gonen<sup id="id4.4.id4" class="ltx_sup">1</sup> 
<br class="ltx_break"><span id="id5.5.id5" class="ltx_text ltx_font_bold">David Ifeoluwa Adelani<sup id="id5.5.id5.1" class="ltx_sup">3,5</sup></span>    <span id="id6.6.id6" class="ltx_text ltx_font_bold">Daud Abolade<sup id="id6.6.id6.1" class="ltx_sup">5</sup></span>    <span id="id7.7.id7" class="ltx_text ltx_font_bold">Noah A. Smith<sup id="id7.7.id7.1" class="ltx_sup">1,2</sup></span>    <span id="id8.8.id8" class="ltx_text ltx_font_bold">Yulia Tsvetkov<sup id="id8.8.id8.1" class="ltx_sup">1</sup></span> 
<br class="ltx_break"><sup id="id9.9.id9" class="ltx_sup">1</sup>University of Washington   <sup id="id10.10.id10" class="ltx_sup">2</sup>Allen Institute for AI   <sup id="id11.11.id11" class="ltx_sup">3</sup>University College London 
<br class="ltx_break"><sup id="id12.12.id12" class="ltx_sup">4</sup>Lelapa AI   <sup id="id13.13.id13" class="ltx_sup">5</sup>Masakhane NLP 
<br class="ltx_break"><a href="mailto:oahia@cs.washington.edu" title="" class="ltx_ref ltx_href ltx_font_typewriter">oahia@cs.washington.edu</a>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id14.id1" class="ltx_p">Yorùbá—an African language with roughly 47 million speakers—encompasses a continuum with several dialects. Recent efforts to develop NLP technologies for African languages have focused on their standard dialects, resulting in disparities for dialects and varieties for which there are little to no resources or tools. We take steps towards bridging this gap by introducing a new high-quality parallel text and speech corpus <span id="id14.id1.1" class="ltx_text ltx_font_smallcaps">YorùLect</span> across three domains and four regional Yorùbá dialects. To develop this corpus, we engaged native speakers, travelling to communities where these dialects are spoken, to collect text and speech data. Using our newly created corpus, we conducted extensive experiments on (text) machine translation, automatic speech recognition, and speech-to-text translation. Our results reveal substantial performance disparities between standard Yorùbá and the other dialects across all tasks. However, we also show that with dialect-adaptive finetuning, we are able to narrow this gap. We believe our dataset and experimental analysis will contribute greatly to developing NLP tools for Yorùbá and its dialects, and potentially for other African languages, by improving our understanding of existing challenges and offering a high-quality dataset for further development.
We release <span id="id14.id1.2" class="ltx_text ltx_font_smallcaps">YorùLect</span> dataset and models publicly under an open license <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Code and data available at <a target="_blank" href="https://github.com/orevaahia/yorulect" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/orevaahia/yorulect</a></span></span></span>.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.1" class="ltx_block ltx_align_bottom">
<p id="p1.1.1" class="ltx_p"><span id="p1.1.1.1" class="ltx_text ltx_font_bold">Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.2" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.2.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.2.1.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.2.1.1.1.1" class="ltx_tr">
<span id="p1.1.2.1.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Orevaoghene Ahia<sup id="p1.1.2.1.1.1.1.1.1.1" class="ltx_sup">1,5</sup>   Anuoluwapo Aremu<sup id="p1.1.2.1.1.1.1.1.1.2" class="ltx_sup">4,5</sup>   Diana Abagyan<sup id="p1.1.2.1.1.1.1.1.1.3" class="ltx_sup">1</sup>   Hila Gonen<sup id="p1.1.2.1.1.1.1.1.1.4" class="ltx_sup">1</sup></span></span></span>
<span id="p1.1.2.1.1.2.2" class="ltx_tr">
<span id="p1.1.2.1.1.2.2.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.2.2.1.1" class="ltx_text ltx_font_bold">David Ifeoluwa Adelani<sup id="p1.1.2.1.1.2.2.1.1.1" class="ltx_sup">3,5</sup></span>    <span id="p1.1.2.1.1.2.2.1.2" class="ltx_text ltx_font_bold">Daud Abolade<sup id="p1.1.2.1.1.2.2.1.2.1" class="ltx_sup">5</sup></span>    <span id="p1.1.2.1.1.2.2.1.3" class="ltx_text ltx_font_bold">Noah A. Smith<sup id="p1.1.2.1.1.2.2.1.3.1" class="ltx_sup">1,2</sup></span>    <span id="p1.1.2.1.1.2.2.1.4" class="ltx_text ltx_font_bold">Yulia Tsvetkov<sup id="p1.1.2.1.1.2.2.1.4.1" class="ltx_sup">1</sup></span></span></span>
<span id="p1.1.2.1.1.3.3" class="ltx_tr">
<span id="p1.1.2.1.1.3.3.1" class="ltx_td ltx_align_center"><sup id="p1.1.2.1.1.3.3.1.1" class="ltx_sup">1</sup>University of Washington   <sup id="p1.1.2.1.1.3.3.1.2" class="ltx_sup">2</sup>Allen Institute for AI   <sup id="p1.1.2.1.1.3.3.1.3" class="ltx_sup">3</sup>University College London</span></span>
<span id="p1.1.2.1.1.4.4" class="ltx_tr">
<span id="p1.1.2.1.1.4.4.1" class="ltx_td ltx_align_center"><sup id="p1.1.2.1.1.4.4.1.1" class="ltx_sup">4</sup>Lelapa AI   <sup id="p1.1.2.1.1.4.4.1.2" class="ltx_sup">5</sup>Masakhane NLP</span></span>
<span id="p1.1.2.1.1.5.5" class="ltx_tr">
<span id="p1.1.2.1.1.5.5.1" class="ltx_td ltx_align_center"><a href="mailto:oahia@cs.washington.edu" title="" class="ltx_ref ltx_href ltx_font_typewriter">oahia@cs.washington.edu</a></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">While great strides have been made in developing NLP resources for low-resource languages, the majority of these efforts have been directed towards the “standard” dialect of these languages, largely neglecting the long tail of non-standard dialects spoken by millions <cite class="ltx_cite ltx_citemacro_cite">Faisal et al. (<a href="#bib.bib36" title="" class="ltx_ref">2024</a>); Alam et al. (<a href="#bib.bib17" title="" class="ltx_ref">2024</a>)</cite>. Dialects of a language exhibit nuanced yet distinguishable differences in lexicon, pronunciation, spelling, and syntax, mirroring regional, societal, and cultural differences <cite class="ltx_cite ltx_citemacro_cite">Chambers and Trudgill (<a href="#bib.bib26" title="" class="ltx_ref">1998</a>)</cite>. Usually, a “standard”
dialect is the dialect with the highest population of speakers, and sometimes the only dialect with a standard orthography <cite class="ltx_cite ltx_citemacro_cite">Milroy and Milroy (<a href="#bib.bib47" title="" class="ltx_ref">2012</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">African languages are linguistically diverse <cite class="ltx_cite ltx_citemacro_cite">Adebara and Abdul-Mageed (<a href="#bib.bib3" title="" class="ltx_ref">2022</a>); Siminyu and Freshia (<a href="#bib.bib74" title="" class="ltx_ref">2020</a>)</cite>, yet severely under-resourced. Most of these languages have numerous varieties, (usually regional), some of which are mostly-spoken and lack a standard orthography <cite class="ltx_cite ltx_citemacro_cite">Batibo (<a href="#bib.bib22" title="" class="ltx_ref">2005</a>); Heine and Nurse (<a href="#bib.bib39" title="" class="ltx_ref">2000</a>)</cite>. Developing language technologies has been incredibly challenging for African languages <cite class="ltx_cite ltx_citemacro_cite">Nekoto et al. (<a href="#bib.bib51" title="" class="ltx_ref">2020</a>); Muhammad et al. (<a href="#bib.bib50" title="" class="ltx_ref">2023</a>); Ogundepo et al. (<a href="#bib.bib56" title="" class="ltx_ref">2023</a>); Adelani et al. (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>); Dione et al. (<a href="#bib.bib33" title="" class="ltx_ref">2023</a>); Adelani et al. (<a href="#bib.bib9" title="" class="ltx_ref">2024</a>, <a href="#bib.bib7" title="" class="ltx_ref">2021b</a>)</cite>, partly due to the scarcity of extensive language resources required for developing systems that are robust to the variations in linguistic features <cite class="ltx_cite ltx_citemacro_cite">Adebara and Abdul-Mageed (<a href="#bib.bib3" title="" class="ltx_ref">2022</a>); Siminyu and Freshia (<a href="#bib.bib74" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address this problem, in this work we focus on curating dialectal resources for Yorùbá, a low-resource language with 47 million native
speakers around the world. Yorùbá language is native to Southwestern Nigeria, Republic of Benin, and Republic of Togo. Yorùbá encompasses a dialect continuum including several distinct regional dialects <cite class="ltx_cite ltx_citemacro_cite">Rowlands (<a href="#bib.bib71" title="" class="ltx_ref">1967</a>)</cite>. Due to Yorùbá’s low-resource status, the majority of published NLP work have been done on the Standard Yorùbá dialect <cite class="ltx_cite ltx_citemacro_cite">Ogunremi et al. (<a href="#bib.bib57" title="" class="ltx_ref">2024</a>); Aremu et al. (<a href="#bib.bib18" title="" class="ltx_ref">2023</a>); Ahia et al. (<a href="#bib.bib12" title="" class="ltx_ref">2021</a>); Dione et al. (<a href="#bib.bib33" title="" class="ltx_ref">2023</a>); Shode et al. (<a href="#bib.bib72" title="" class="ltx_ref">2023</a>); Ogundepo et al. (<a href="#bib.bib56" title="" class="ltx_ref">2023</a>); Akinade et al. (<a href="#bib.bib15" title="" class="ltx_ref">2023</a>); Adelani et al. (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>); Muhammad et al. (<a href="#bib.bib50" title="" class="ltx_ref">2023</a>); Adelani et al. (<a href="#bib.bib6" title="" class="ltx_ref">2021a</a>); Adebara et al. (<a href="#bib.bib5" title="" class="ltx_ref">2022</a>, <a href="#bib.bib4" title="" class="ltx_ref">2021</a>); Lee et al. (<a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<div id="S1.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:245.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-34.6pt,21.8pt) scale(0.849405113924944,0.849405113924944) ;">
<table id="S1.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.1.1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S1.T1.1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.1.1.1.1" class="ltx_p" style="width:71.1pt;"><span id="S1.T1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">English</span></span>
</span>
</th>
<th id="S1.T1.1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S1.T1.1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.1.2.1.1" class="ltx_p" style="width:71.1pt;"><span id="S1.T1.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Standard</span></span>
</span>
</th>
<th id="S1.T1.1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S1.T1.1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.1.3.1.1" class="ltx_p" style="width:71.1pt;"><span id="S1.T1.1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Ìjẹ̀bú</span></span>
</span>
</th>
<th id="S1.T1.1.1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S1.T1.1.1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.1.4.1.1" class="ltx_p" style="width:71.1pt;"><span id="S1.T1.1.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Ifẹ̀</span></span>
</span>
</th>
<th id="S1.T1.1.1.1.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S1.T1.1.1.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.1.5.1.1" class="ltx_p" style="width:71.1pt;"><span id="S1.T1.1.1.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Ìlàjẹ</span></span>
</span>
</th>
<th id="S1.T1.1.1.1.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S1.T1.1.1.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.1.6.1.1" class="ltx_p" style="width:28.5pt;"><span id="S1.T1.1.1.1.1.6.1.1.1" class="ltx_text ltx_font_bold">Domain</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.1.1.2.1" class="ltx_tr">
<td id="S1.T1.1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T1.1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.2.1.1.1.1" class="ltx_p" style="width:71.1pt;">All the efforts to talk to ASUU chairman failed because he said he has nothing to say</span>
</span>
</td>
<td id="S1.T1.1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T1.1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.2.1.2.1.1" class="ltx_p" style="width:71.1pt;">Gbogbo ọ̀
ì<span id="S1.T1.1.1.2.1.2.1.1.1" class="ltx_text" style="color:#FF0000;">gbiỳànjú</span> láti <span id="S1.T1.1.1.2.1.2.1.1.2" class="ltx_text" style="color:#FF0000;">bá alága ASUU</span> sọ̀rọ̀ lò jásí pàbó nitori ó ni òun kò ni ohunkóhun láti sọ .</span>
</span>
</td>
<td id="S1.T1.1.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T1.1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.2.1.3.1.1" class="ltx_p" style="width:71.1pt;">Gbogbo ì<span id="S1.T1.1.1.2.1.3.1.1.1" class="ltx_text" style="color:#FF0000;">gbiỳànjú</span> láti <span id="S1.T1.1.1.2.1.3.1.1.2" class="ltx_text" style="color:#FF0000;">bá alága ASUU</span> sọ̀rọ̀ re jasi afo to ri ó sọ fo òún ni ohun kóhun láti sọ .</span>
</span>
</td>
<td id="S1.T1.1.1.2.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T1.1.1.2.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.2.1.4.1.1" class="ltx_p" style="width:71.1pt;">Gbogbo è<span id="S1.T1.1.1.2.1.4.1.1.1" class="ltx_text" style="color:#FF0000;">gbiỳànjú</span> látẹ <span id="S1.T1.1.1.2.1.4.1.1.2" class="ltx_text" style="color:#FF0000;">ba alága ASUU</span> sọ̀rọ̀ lọ̀ jásí pàbó tori ó ghii òun nẹ́ ihunkihun ún sọ .</span>
</span>
</td>
<td id="S1.T1.1.1.2.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T1.1.1.2.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.2.1.5.1.1" class="ltx_p" style="width:71.1pt;">Dede ì<span id="S1.T1.1.1.2.1.5.1.1.1" class="ltx_text" style="color:#FF0000;">gbiỳànjú</span> áti <span id="S1.T1.1.1.2.1.5.1.1.2" class="ltx_text" style="color:#FF0000;">bá alága ASUU</span> fọ̀ rèé já ni pàbo tori ó fọ̀rọ́ pé ó ghún né irú kirun gho fé fò .</span>
</span>
</td>
<td id="S1.T1.1.1.2.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T1.1.1.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.2.1.6.1.1" class="ltx_p" style="width:28.5pt;">News</span>
</span>
</td>
</tr>
<tr id="S1.T1.1.1.3.2" class="ltx_tr">
<td id="S1.T1.1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T1.1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.3.2.1.1.1" class="ltx_p" style="width:71.1pt;">They called unto God in the upper room for the release of the holy spirit .</span>
</span>
</td>
<td id="S1.T1.1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T1.1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.3.2.2.1.1" class="ltx_p" style="width:71.1pt;">Wón ké pe Ọlórun ni <span id="S1.T1.1.1.3.2.2.1.1.1" class="ltx_text" style="color:#FF0000;">yàrá</span> ori <span id="S1.T1.1.1.3.2.2.1.1.2" class="ltx_text" style="color:#FF0000;">òkè</span> fún i<span id="S1.T1.1.1.3.2.2.1.1.3" class="ltx_text" style="color:#FF0000;">tújáde</span> <span id="S1.T1.1.1.3.2.2.1.1.4" class="ltx_text" style="color:#FF0000;">ẹ̀mi</span> mimó .</span>
</span>
</td>
<td id="S1.T1.1.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T1.1.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.3.2.3.1.1" class="ltx_p" style="width:71.1pt;">Wón ké pè Ọlórun ni <span id="S1.T1.1.1.3.2.3.1.1.1" class="ltx_text" style="color:#FF0000;">yàrá</span> ori <span id="S1.T1.1.1.3.2.3.1.1.2" class="ltx_text" style="color:#FF0000;">òkè</span> fún i<span id="S1.T1.1.1.3.2.3.1.1.3" class="ltx_text" style="color:#FF0000;">tú jáde</span> <span id="S1.T1.1.1.3.2.3.1.1.4" class="ltx_text" style="color:#FF0000;">ẹ̀mi</span> mimó .</span>
</span>
</td>
<td id="S1.T1.1.1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T1.1.1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.3.2.4.1.1" class="ltx_p" style="width:71.1pt;">Igán ké pe Ọlóun nẹ́ <span id="S1.T1.1.1.3.2.4.1.1.1" class="ltx_text" style="color:#FF0000;">yàrá</span> ori <span id="S1.T1.1.1.3.2.4.1.1.2" class="ltx_text" style="color:#FF0000;">òkè</span> ún ẹ̀<span id="S1.T1.1.1.3.2.4.1.1.3" class="ltx_text" style="color:#FF0000;">tújáde</span> <span id="S1.T1.1.1.3.2.4.1.1.4" class="ltx_text" style="color:#FF0000;">ẹ̀mi</span> mẹ́mọ́</span>
</span>
</td>
<td id="S1.T1.1.1.3.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T1.1.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.3.2.5.1.1" class="ltx_p" style="width:71.1pt;">Ghón kélè kpè Ọlórun ni <span id="S1.T1.1.1.3.2.5.1.1.1" class="ltx_text" style="color:#FF0000;">yàrá</span> origho <span id="S1.T1.1.1.3.2.5.1.1.2" class="ltx_text" style="color:#FF0000;">òkè</span> ghún i<span id="S1.T1.1.1.3.2.5.1.1.3" class="ltx_text" style="color:#FF0000;">tújáde</span> ẹ̀mi mimó .</span>
</span>
</td>
<td id="S1.T1.1.1.3.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S1.T1.1.1.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.3.2.6.1.1" class="ltx_p" style="width:28.5pt;">Religion</span>
</span>
</td>
</tr>
<tr id="S1.T1.1.1.4.3" class="ltx_tr">
<td id="S1.T1.1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S1.T1.1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.4.3.1.1.1" class="ltx_p" style="width:71.1pt;">We all look for characteristics that has to do with self-centeredness, and they are similar to this.</span>
</span>
</td>
<td id="S1.T1.1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S1.T1.1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.4.3.2.1.1" class="ltx_p" style="width:71.1pt;">Gbogbo wa la máa ń wá àwọn ànimọ́ tó ni i ṣe pẹ̀lú iwa imọtara nikan, ìrisi wọn si jọ èyi.</span>
</span>
</td>
<td id="S1.T1.1.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S1.T1.1.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.4.3.3.1.1" class="ltx_p" style="width:71.1pt;">Dede wa re n wa iwa ànimọ́ rè nii ṣẹ pẹ̀lú iwa imọlara nikan, irisi wọ si jọ ìwé</span>
</span>
</td>
<td id="S1.T1.1.1.4.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S1.T1.1.1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.4.3.4.1.1" class="ltx_p" style="width:71.1pt;">Gbogbo ria la máa ghá inọn ànẹ́mọ́ kọ́ nẹ́ẹ́ i se pẹ̀lú èghà èmọtara ọni nikàn, ẹ̀risi rian sèè jọ yèé .</span>
</span>
</td>
<td id="S1.T1.1.1.4.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S1.T1.1.1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.4.3.5.1.1" class="ltx_p" style="width:71.1pt;">Dede gha rèé mi fẹ́ àghan ànimá yii nẹ́ i se kpẹ̀lu ighà imòtara ọnẹ nùkàn, irisi ghàn si jọ èyi</span>
</span>
</td>
<td id="S1.T1.1.1.4.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S1.T1.1.1.4.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.4.3.6.1.1" class="ltx_p" style="width:28.5pt;">Ted Talks</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Examples of parallel translations across all dialects and domains in <span id="S1.T1.4.1" class="ltx_text ltx_font_smallcaps">YorùLect</span>. Words that are unique across all dialects are highlighted in <span id="S1.T1.5.2" class="ltx_text ltx_font_italic" style="color:#FF0000;">red</span>.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We introduce the first-ever corpus of high quality, contemporary Yorùbá speech and text data parallel across four Yorùbá dialects; Standard Yorùbá, Ifẹ̀ <span id="S1.p4.1.1" class="ltx_ERROR undefined">\tipaencoding</span>/ i f E /, Ìlàjẹ <span id="S1.p4.1.2" class="ltx_ERROR undefined">\tipaencoding</span>/ i l a dZ E / and Ìjẹ̀bú <span id="S1.p4.1.3" class="ltx_ERROR undefined">\tipaencoding</span>/ i dZ E b u / in three domains (religious, news, and Ted talks). This newly curated benchmark, developed with native speakers, can be used in (text-to-text) machine translation (MT), automatic speech recognition (ASR), speech-to-text translation (S2TT), and speech-to-speech translation (STST) tasks. We discuss in detail the data curation process, criteria for data selection, and the steps we took to ensure data quality and integrity (<a href="#S3" title="3 YorùLect Corpus ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">§3</span></a>). We first conduct extensive experiments evaluating the zero-shot performance of recent state-of-the-art models for MT, ASR, and S2TT (<a href="#S4" title="4 Zero-shot Experiments ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">§4</span></a>, <a href="#S5" title="5 Finetuning Experiments ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">§5</span></a>). Our results and analysis indicate that current models are not robust enough to handle existing variation in Yorùbá dialects. Given these poor results, we proceed to adapt (fine-tune) existing models on our training data across all tasks to boost overall performance. With 802 training instances in each dialect, this approach leads to an average increase of 14 and 5 BLEU points for both MT and S2TT respectively, as well as a 20-point decrease in word-error-rate for ASR.
Our work aims to motivate the community to build technology for languages alongside their dialects, especially for low-resource dialects of low-resource languages, as this will promote linguistic diversity, and ensure that technological advancements benefit <em id="S1.p4.1.4" class="ltx_emph ltx_font_italic">all</em> language communities.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Yorùbá and its Regional Dialects</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The Yorùbá language is spoken natively by roughly 47 million people in Nigeria<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://en.wikipedia.org/wiki/Yoruba_language" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://en.wikipedia.org/wiki/Yoruba_language</a></span></span></span> and in the neighboring countries of the Republic of Benin and Togo and also Côte d’Ivoire, Sierra Leone, Cuba, and Brazil. In Nigeria, Yorùbá speakers are mainly concentrated in the Southwest region, spanning states like Oyo, Ogun, Osun, Ondo, Ekiti, and Lagos, and North Central states like Kogi, and Kwara.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2406.19564/assets/figures/regional_map.jpg" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="423" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Geographical distribution of Yorùbá dialects in West Africa. Map from <cite class="ltx_cite ltx_citemacro_cite">Ozburn (<a href="#bib.bib66" title="" class="ltx_ref">2023</a>)</cite>.
</figcaption>
</figure>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The extensive Yorùbá-speaking population and their dispersion across various regions have led to the emergence of geography-specific linguistic variations <cite class="ltx_cite ltx_citemacro_cite">Ballard (<a href="#bib.bib21" title="" class="ltx_ref">1971</a>)</cite>.
The number of existing Yorùbá dialects is estimated between twelve to twenty-six <cite class="ltx_cite ltx_citemacro_cite">Ojo (<a href="#bib.bib58" title="" class="ltx_ref">1977</a>); Adetugbo (<a href="#bib.bib11" title="" class="ltx_ref">1982</a>); Oyelaran (<a href="#bib.bib64" title="" class="ltx_ref">1971</a>); Oyelaran and Watson (<a href="#bib.bib63" title="" class="ltx_ref">1991</a>)</cite> and the differences present in these dialects are evident in pronunciation, grammatical structure, and vocabulary <cite class="ltx_cite ltx_citemacro_cite">Adetugbo (<a href="#bib.bib11" title="" class="ltx_ref">1982</a>); Przezdziecki (<a href="#bib.bib69" title="" class="ltx_ref">2005</a>); Olumuyiwa (<a href="#bib.bib60" title="" class="ltx_ref">2009</a>); Arokoyo et al. (<a href="#bib.bib19" title="" class="ltx_ref">2019</a>); Olánrewájú (<a href="#bib.bib59" title="" class="ltx_ref">2022</a>)</cite>. Also categorized as a Volta-Niger language within the Yoruboid subgroup of the Niger-Congo family, Yorùbá is a tonal language with three basic tones: low, middle, and high <cite class="ltx_cite ltx_citemacro_cite">Courtenay (<a href="#bib.bib30" title="" class="ltx_ref">1969</a>); Oyetade (<a href="#bib.bib65" title="" class="ltx_ref">1988</a>)</cite>, as well as two or three contour tones.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>A contour tone combines two or more basic tones such as a falling tone made up of a high tone and a low tone, or a rising tone consisting of a low tone followed by a high tone.</span></span></span> Previous research <cite class="ltx_cite ltx_citemacro_cite">Adeniyi (<a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite> has indicated that the phonetic nuances of contour tones are a major distinguishing feature among Yorùbá dialects.
Yorùbá dialectal forms in Nigeria can be classified into five regional groupings: Northwest Yorùbá (NWY), Northeastern Yorùbá (NEY), Central Yorùbá (CY), Southwest Yorùbá (SWY), and Southeast Yorùbá (SEY).
Phonological, lexical, and grammatical differences distinguish these groupings, given the diverse levels of mutual intelligibility among the “regional” dialects within each category <cite class="ltx_cite ltx_citemacro_cite">Arokoyo et al. (<a href="#bib.bib19" title="" class="ltx_ref">2019</a>); Olumuyiwa (<a href="#bib.bib61" title="" class="ltx_ref">2016</a>); <a href="#bib.bib2" title="" class="ltx_ref">Abiodun et al. </a></cite>. In this work, our focus lies on Ifẹ̀, a dialect in the Central Yoruba classification, Ìjẹ̀bú, and Ìlàjẹ dialects, which belong to the Southeast Yoruba classification. We display the geographical distribution of Yorùbá dialects in West Africa in <a href="#S2.F1" title="Figure 1 ‣ 2 Yorùbá and its Regional Dialects ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Comparative dialectal analysis</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Standard Yorùbá, Ifẹ̀, Ìjẹ̀bú and Ìlàjẹ dialects exhibit both similarities and differences in their orthographic representations, morphology, and semantics. For instance, standard Yorùbá dialect has fused velar fricative /<span id="S2.SS0.SSS0.Px1.p1.1.1" class="ltx_ERROR undefined">\tipaencoding</span>G/ and labialised voiced velar /<span id="S2.SS0.SSS0.Px1.p1.1.2" class="ltx_ERROR undefined">\tipaencoding</span>g<sup id="S2.SS0.SSS0.Px1.p1.1.3" class="ltx_sup">w</sup>/ into /w/ <cite class="ltx_cite ltx_citemacro_cite">Adetugbo (<a href="#bib.bib11" title="" class="ltx_ref">1982</a>)</cite> and our curated data revealed a similar pattern for Ìjẹ̀bú. In contrast, Ifẹ̀ uses /<span id="S2.SS0.SSS0.Px1.p1.1.4" class="ltx_ERROR undefined">\tipaencoding</span>G/ in certain occurrences while Ìlàjẹ has heavily retained the /<span id="S2.SS0.SSS0.Px1.p1.1.5" class="ltx_ERROR undefined">\tipaencoding</span>g<sup id="S2.SS0.SSS0.Px1.p1.1.6" class="ltx_sup">w</sup>/ and /<span id="S2.SS0.SSS0.Px1.p1.1.7" class="ltx_ERROR undefined">\tipaencoding</span>G/ in its representations.
As a result, at the word level, “àwọn”
(3p pl.) is represented similarly in standard dialect and Ìjẹ̀bú but as “ighọn” in Ifẹ̀ and
“àghan” in Ìlàjẹ. Besides the contrastive consonant nature, the oral and nasal vowels are also both contrastive in Ifẹ̀ and Ìlàjẹ dialects respectively.
Further analsyses of <span id="S2.SS0.SSS0.Px1.p1.1.8" class="ltx_text ltx_font_smallcaps">YorùLect</span> reveal that the low nasalised vowel /<span id="S2.SS0.SSS0.Px1.p1.1.9" class="ltx_ERROR undefined">\tipaencoding</span>ã/ mostly follows “gh” in Ìlàjẹ while the back lower- mid nasalised vowel /<span id="S2.SS0.SSS0.Px1.p1.1.10" class="ltx_ERROR undefined">\tipaencoding</span>Õ/ accompanies “gh” in Ifẹ̀ dialect. One remarkable semantic variation is that standard Yorùbá dialect uses “sọ” and “wi pe” as <em id="S2.SS0.SSS0.Px1.p1.1.11" class="ltx_emph ltx_font_italic">say/talk</em>, however for Ìlàjẹ and Ìjẹ̀bú the morpheme mostly used is
“fọ” while Ifẹ̀ uses “ghii”, all of which have the same semantics.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">YorùLect</span> Corpus</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We curated parallel text and recorded high quality speech data across Ifẹ̀, Ìjẹ̀bú, Ìlàjẹ, and Standard Yorùbá dialects. Our data curation process involves three main steps: (i) text curation and dialect localization; (ii) speech recording; and (iii) text and audio alignment.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Text Curation and Dialect Localization</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We collected textual Standard Yorùbá data from the following sources: (i) Bible study manuals;<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://faithrebuilder.org/conference-bible-study-manuals" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://faithrebuilder.org/conference-bible-study-manuals</a></span></span></span>
(ii) the Yorùbá portion of MTTT, a collection of multitarget bitexts based on TED Talks <cite class="ltx_cite ltx_citemacro_cite">Duh (<a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite>; and (iii) Yorùbá news articles within the MAFT corpus <cite class="ltx_cite ltx_citemacro_cite">Alabi et al. (<a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite>. Given resource limitations and the demanding nature of this task, we gathered 352 sentences from the Bible study manuals, 247 sentences from TED Talks, and 907 sentences from news articles, amounting to a total of 1,506 sentences. Next, we proceeded to localising the compiled Standard Yorùbá text into the three respective dialects: Ifẹ̀, Ìjẹ̀bú, and Ìlàjẹ by recruiting trained linguists and translators who are literate and also native speakers of the respective dialects. We hired two translators or linguists per dialect and gave each a different domains to localise. The localisation process took about six to eight weeks and this included the localisation, quality assessment and incorporation of corrections. We provided monetary compensation for the localisation of the text.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.2.2" class="ltx_tr">
<th id="S3.T2.2.2.3" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt"></th>
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">
<span id="S3.T2.1.1.1.1" class="ltx_text ltx_font_bold">BLEU</span> <math id="S3.T2.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T2.1.1.1.m1.1a"><mo stretchy="false" id="S3.T2.1.1.1.m1.1.1" xref="S3.T2.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S3.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">
<span id="S3.T2.2.2.2.1" class="ltx_text ltx_font_bold">AfriCOMET</span> <math id="S3.T2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.T2.2.2.2.m1.1a"><mo stretchy="false" id="S3.T2.2.2.2.m1.1.1" xref="S3.T2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.m1.1b"><ci id="S3.T2.2.2.2.m1.1.1.cmml" xref="S3.T2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr id="S3.T2.2.3.1" class="ltx_tr">
<th id="S3.T2.2.3.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S3.T2.2.3.1.2" class="ltx_td ltx_align_right ltx_border_t">Ìjẹ̀bú</td>
<td id="S3.T2.2.3.1.3" class="ltx_td ltx_align_right ltx_border_t">Ifẹ̀</td>
<td id="S3.T2.2.3.1.4" class="ltx_td ltx_align_right ltx_border_t">Ìlàjẹ</td>
<td id="S3.T2.2.3.1.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">Standard</td>
<td id="S3.T2.2.3.1.6" class="ltx_td ltx_align_right ltx_border_t">Ìjẹ̀bú</td>
<td id="S3.T2.2.3.1.7" class="ltx_td ltx_align_right ltx_border_t">Ifẹ̀</td>
<td id="S3.T2.2.3.1.8" class="ltx_td ltx_align_right ltx_border_t">Ìlàjẹ</td>
<td id="S3.T2.2.3.1.9" class="ltx_td ltx_align_right ltx_border_t">Standard</td>
</tr>
<tr id="S3.T2.2.4.2" class="ltx_tr">
<th id="S3.T2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T2.2.4.2.1.1" class="ltx_text ltx_font_bold">M2M100</span></th>
<td id="S3.T2.2.4.2.2" class="ltx_td ltx_align_right ltx_border_t">0.00</td>
<td id="S3.T2.2.4.2.3" class="ltx_td ltx_align_right ltx_border_t">0.49</td>
<td id="S3.T2.2.4.2.4" class="ltx_td ltx_align_right ltx_border_t">0.25</td>
<td id="S3.T2.2.4.2.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.49</td>
<td id="S3.T2.2.4.2.6" class="ltx_td ltx_align_right ltx_border_t">0.26</td>
<td id="S3.T2.2.4.2.7" class="ltx_td ltx_align_right ltx_border_t">0.27</td>
<td id="S3.T2.2.4.2.8" class="ltx_td ltx_align_right ltx_border_t">0.26</td>
<td id="S3.T2.2.4.2.9" class="ltx_td ltx_align_right ltx_border_t">0.30</td>
</tr>
<tr id="S3.T2.2.5.3" class="ltx_tr">
<th id="S3.T2.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T2.2.5.3.1.1" class="ltx_text ltx_font_bold">NLLB-600M</span></th>
<td id="S3.T2.2.5.3.2" class="ltx_td ltx_align_right">7.26</td>
<td id="S3.T2.2.5.3.3" class="ltx_td ltx_align_right">7.52</td>
<td id="S3.T2.2.5.3.4" class="ltx_td ltx_align_right">5.78</td>
<td id="S3.T2.2.5.3.5" class="ltx_td ltx_align_right ltx_border_r">16.51</td>
<td id="S3.T2.2.5.3.6" class="ltx_td ltx_align_right">0.52</td>
<td id="S3.T2.2.5.3.7" class="ltx_td ltx_align_right">0.50</td>
<td id="S3.T2.2.5.3.8" class="ltx_td ltx_align_right">0.49</td>
<td id="S3.T2.2.5.3.9" class="ltx_td ltx_align_right">0.65</td>
</tr>
<tr id="S3.T2.2.6.4" class="ltx_tr">
<th id="S3.T2.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T2.2.6.4.1.1" class="ltx_text ltx_font_bold">GMNMT</span></th>
<td id="S3.T2.2.6.4.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T2.2.6.4.2.1" class="ltx_text ltx_font_bold">18.24</span></td>
<td id="S3.T2.2.6.4.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T2.2.6.4.3.1" class="ltx_text ltx_font_bold">17.16</span></td>
<td id="S3.T2.2.6.4.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T2.2.6.4.4.1" class="ltx_text ltx_font_bold">12.66</span></td>
<td id="S3.T2.2.6.4.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S3.T2.2.6.4.5.1" class="ltx_text ltx_font_bold">43.46</span></td>
<td id="S3.T2.2.6.4.6" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T2.2.6.4.6.1" class="ltx_text ltx_font_bold">0.59</span></td>
<td id="S3.T2.2.6.4.7" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T2.2.6.4.7.1" class="ltx_text ltx_font_bold">0.57</span></td>
<td id="S3.T2.2.6.4.8" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T2.2.6.4.8.1" class="ltx_text ltx_font_bold">0.56</span></td>
<td id="S3.T2.2.6.4.9" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T2.2.6.4.9.1" class="ltx_text ltx_font_bold">0.74</span></td>
</tr>
<tr id="S3.T2.2.7.5" class="ltx_tr">
<th id="S3.T2.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S3.T2.2.7.5.1.1" class="ltx_text ltx_font_bold">Menyo</span></th>
<td id="S3.T2.2.7.5.2" class="ltx_td ltx_align_right ltx_border_t">2.76</td>
<td id="S3.T2.2.7.5.3" class="ltx_td ltx_align_right ltx_border_t">2.66</td>
<td id="S3.T2.2.7.5.4" class="ltx_td ltx_align_right ltx_border_t">1.57</td>
<td id="S3.T2.2.7.5.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">7.49</td>
<td id="S3.T2.2.7.5.6" class="ltx_td ltx_align_right ltx_border_t">0.44</td>
<td id="S3.T2.2.7.5.7" class="ltx_td ltx_align_right ltx_border_t">0.40</td>
<td id="S3.T2.2.7.5.8" class="ltx_td ltx_align_right ltx_border_t">0.40</td>
<td id="S3.T2.2.7.5.9" class="ltx_td ltx_align_right ltx_border_t">0.52</td>
</tr>
<tr id="S3.T2.2.8.6" class="ltx_tr">
<th id="S3.T2.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S3.T2.2.8.6.1.1" class="ltx_text ltx_font_bold">MT0</span></th>
<td id="S3.T2.2.8.6.2" class="ltx_td ltx_align_right">5.81</td>
<td id="S3.T2.2.8.6.3" class="ltx_td ltx_align_right">6.68</td>
<td id="S3.T2.2.8.6.4" class="ltx_td ltx_align_right">4.61</td>
<td id="S3.T2.2.8.6.5" class="ltx_td ltx_align_right ltx_border_r">17.22</td>
<td id="S3.T2.2.8.6.6" class="ltx_td ltx_align_right">0.52</td>
<td id="S3.T2.2.8.6.7" class="ltx_td ltx_align_right">0.50</td>
<td id="S3.T2.2.8.6.8" class="ltx_td ltx_align_right">0.47</td>
<td id="S3.T2.2.8.6.9" class="ltx_td ltx_align_right">0.65</td>
</tr>
<tr id="S3.T2.2.9.7" class="ltx_tr">
<th id="S3.T2.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S3.T2.2.9.7.1.1" class="ltx_text ltx_font_bold">Aya</span></th>
<td id="S3.T2.2.9.7.2" class="ltx_td ltx_align_right ltx_border_bb">7.18</td>
<td id="S3.T2.2.9.7.3" class="ltx_td ltx_align_right ltx_border_bb">7.71</td>
<td id="S3.T2.2.9.7.4" class="ltx_td ltx_align_right ltx_border_bb">4.91</td>
<td id="S3.T2.2.9.7.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r">16.46</td>
<td id="S3.T2.2.9.7.6" class="ltx_td ltx_align_right ltx_border_bb">0.49</td>
<td id="S3.T2.2.9.7.7" class="ltx_td ltx_align_right ltx_border_bb">0.50</td>
<td id="S3.T2.2.9.7.8" class="ltx_td ltx_align_right ltx_border_bb">0.45</td>
<td id="S3.T2.2.9.7.9" class="ltx_td ltx_align_right ltx_border_bb">0.63</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Zero-shot MT evaluation across all models. Google Translate outperforms other systems and is more robust to dialectal variation. However, a significant performance gap remains compared to the Standard Yoruba dialect.
</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Speech Recording</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Speaker selection is crucial when creating an ASR corpus; a speaker should be fluent, literate, trained, and familiar with voice recording <cite class="ltx_cite ltx_citemacro_cite">Ogayo et al. (<a href="#bib.bib55" title="" class="ltx_ref">2022</a>); van Niekerk et al. (<a href="#bib.bib78" title="" class="ltx_ref">2017</a>)</cite>. Due to time constraints and speaker availability, we were only able to record speech in standard Yorùbá, Ifẹ̀, and Ìlàjẹ dialects, leaving Ìjẹ̀bú for a later version of the dataset. We retained the linguists and translators who localised the standard Yorùbá text into Ifẹ̀ and Ìlàjẹ dialects. We then recruited two additional native speakers per dialect that are literate in rendering the localised text into audio. All dialectal voice talents received monetary compensation. We first conducted an interview
, then asked the new recruits to record random samples of the text and send the recordings for assessment. The audio and corresponding text are vetted, after which we selected native speakers with high reading competence, good voice texture, and reading pace. This brought the total number of voice talents per dialect to four. To ensure that each voice talent within a dialect recorded text across all domains, we divided text in each domain (religion, Ted, news) into four parts. Each person recorded roughly 375 sentences from each domain resulting in a total of 3 hours of speech per dialect.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Recording is conducted using the speech recorder application designed by the YorubaVoice project <cite class="ltx_cite ltx_citemacro_cite">Ogunremi et al. (<a href="#bib.bib57" title="" class="ltx_ref">2024</a>)</cite>. The text files were uploaded per domain for each speaker on the YorubaVoice Recorder app. We used an M1 Pro 2021 chip MacBook with an audio-technica AT2020USB-X microphone set-up in an anechoic and sound-isolated voice recording booth for the recording process. Each text is recorded at 48 kHz and the audio files are provided in 16 bit linear PCM RIFF format. The app generates metadata that includes a unique speaker ID, audio ID with corresponding text, and the audio file. Finally, all the recordings were subjected to a quality control process by the data coordinator. We manually verified that the correct text was aligned with the appropriate audio file and re-aligned them when necessary. We also discovered one empty audio file in a particular dialect and proceeded to delete it, along with its corresponding text-audio pairs in all other dialects.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Final data statistics </h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">In total, the text portion of <span id="S3.SS2.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">YorùLect</span> consists of 1506 parallel sentences per dialect and 6024 sentences overall, while the speech portion consists roughly 3 hours of audio each in standard Yorùbá, Ifẹ̀ and Ìlàjẹ, resulting in 9 hours of speech in total. We split the text and audio pairs in each dialect into 804 training samples, 200 validation samples and 502 test samples.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<div id="S3.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:124.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(57.4pt,-17.4pt) scale(1.38621502857667,1.38621502857667) ;">
<table id="S3.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.1.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Dialect</th>
<th id="S3.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">length (hours)</th>
<th id="S3.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Avg. length (seconds)</th>
<th id="S3.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Avg. tokens</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.1.1.2.1" class="ltx_tr">
<td id="S3.T3.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">Standard</td>
<td id="S3.T3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">2.93</td>
<td id="S3.T3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">6.99</td>
<td id="S3.T3.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">15.81</td>
</tr>
<tr id="S3.T3.1.1.3.2" class="ltx_tr">
<td id="S3.T3.1.1.3.2.1" class="ltx_td ltx_align_center">Ìlàjẹ</td>
<td id="S3.T3.1.1.3.2.2" class="ltx_td ltx_align_center">3.30</td>
<td id="S3.T3.1.1.3.2.3" class="ltx_td ltx_align_center">7.89</td>
<td id="S3.T3.1.1.3.2.4" class="ltx_td ltx_align_center">15.84</td>
</tr>
<tr id="S3.T3.1.1.4.3" class="ltx_tr">
<td id="S3.T3.1.1.4.3.1" class="ltx_td ltx_align_center">Ifẹ̀</td>
<td id="S3.T3.1.1.4.3.2" class="ltx_td ltx_align_center">3.03</td>
<td id="S3.T3.1.1.4.3.3" class="ltx_td ltx_align_center">7.23</td>
<td id="S3.T3.1.1.4.3.4" class="ltx_td ltx_align_center">15.53</td>
</tr>
<tr id="S3.T3.1.1.5.4" class="ltx_tr">
<td id="S3.T3.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb">Ìjẹ̀bú</td>
<td id="S3.T3.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S3.T3.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S3.T3.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb">15.25</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Statistics of <span id="S3.T3.3.1" class="ltx_text ltx_font_smallcaps">YorùLect</span>. The number of train, validation and test samples is consistently (804/200/502) for each dialect.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Zero-shot Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We start by evaluating the zero-shot performance of current state-of-the-art models on the test portion of <span id="S4.p1.1.1" class="ltx_text ltx_font_smallcaps">YorùLect</span>. Based on the results from this initial evaluation, we then adapt the top-performing zero-shot models by finetuning on the training portion of <span id="S4.p1.1.2" class="ltx_text ltx_font_smallcaps">YorùLect</span> and report results in <a href="#S5.SS1" title="5.1 Machine Translation ‣ 5 Finetuning Experiments ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">§5.1</span></a>. MT experiments are conducted on all dialects, while ASR and S2TT experiments are conducted on all expect Ìjẹ̀bú.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Machine Translation</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We evaluate two classes of translation systems: MT-specific models and LMs. Here, the MT-specific
models use an encoder-decoder architecture and are
trained on large amounts of parallel data in multiple languages, whereas
the LMs are decoder-only models trained to maximize likelihood (i.e., next-token prediction) on text in multiple languages. All models we evaluate have standard Yorùbá text in their training data. We only evaluate translation from the standard language or dialect into English since these experiments are zero-shot and we cannot expect the models to generate text in one of the dialects. This essentially enables us to measure the robustness of all of these models to variation in the Yorùbá language.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.2" class="ltx_tr">
<th id="S4.T4.2.2.3" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S4.T4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span id="S4.T4.1.1.1.1" class="ltx_text ltx_font_bold">ASR (WER) <math id="S4.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T4.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></th>
<th id="S4.T4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span id="S4.T4.2.2.2.1" class="ltx_text ltx_font_bold">S2TT (BLEU) <math id="S4.T4.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.2.2.2.1.m1.1a"><mo stretchy="false" id="S4.T4.2.2.2.1.m1.1.1" xref="S4.T4.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.1.m1.1b"><ci id="S4.T4.2.2.2.1.m1.1.1.cmml" xref="S4.T4.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math></span></th>
</tr>
<tr id="S4.T4.2.3.1" class="ltx_tr">
<th id="S4.T4.2.3.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S4.T4.2.3.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">Ifẹ̀</th>
<th id="S4.T4.2.3.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">Ìlàjẹ</th>
<th id="S4.T4.2.3.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t">Standard</th>
<th id="S4.T4.2.3.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">Ifẹ̀</th>
<th id="S4.T4.2.3.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">Ìlàjẹ</th>
<th id="S4.T4.2.3.1.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">Standard</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.4.1" class="ltx_tr">
<th id="S4.T4.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T4.2.4.1.1.1" class="ltx_text ltx_font_bold">MMS</span></th>
<td id="S4.T4.2.4.1.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S4.T4.2.4.1.2.1" class="ltx_text ltx_font_bold">85.38</span></td>
<td id="S4.T4.2.4.1.3" class="ltx_td ltx_align_right ltx_border_t"><span id="S4.T4.2.4.1.3.1" class="ltx_text ltx_font_bold">83.79</span></td>
<td id="S4.T4.2.4.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span id="S4.T4.2.4.1.4.1" class="ltx_text ltx_font_bold">72.50</span></td>
<td id="S4.T4.2.4.1.5" class="ltx_td ltx_align_right ltx_border_t">-</td>
<td id="S4.T4.2.4.1.6" class="ltx_td ltx_align_right ltx_border_t">-</td>
<td id="S4.T4.2.4.1.7" class="ltx_td ltx_align_right ltx_border_t">-</td>
</tr>
<tr id="S4.T4.2.5.2" class="ltx_tr">
<th id="S4.T4.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S4.T4.2.5.2.1.1" class="ltx_text ltx_font_bold">SeamlessM4T</span></th>
<td id="S4.T4.2.5.2.2" class="ltx_td ltx_align_right">96.14</td>
<td id="S4.T4.2.5.2.3" class="ltx_td ltx_align_right">101.99</td>
<td id="S4.T4.2.5.2.4" class="ltx_td ltx_align_right ltx_border_r">80.14</td>
<td id="S4.T4.2.5.2.5" class="ltx_td ltx_align_right">5.52</td>
<td id="S4.T4.2.5.2.6" class="ltx_td ltx_align_right">3.30</td>
<td id="S4.T4.2.5.2.7" class="ltx_td ltx_align_right">13.16</td>
</tr>
<tr id="S4.T4.2.6.3" class="ltx_tr">
<th id="S4.T4.2.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S4.T4.2.6.3.1.1" class="ltx_text ltx_font_bold">Whisper</span></th>
<td id="S4.T4.2.6.3.2" class="ltx_td ltx_align_right ltx_border_bb">104.50</td>
<td id="S4.T4.2.6.3.3" class="ltx_td ltx_align_right ltx_border_bb">127.21</td>
<td id="S4.T4.2.6.3.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r">130.96</td>
<td id="S4.T4.2.6.3.5" class="ltx_td ltx_align_right ltx_border_bb">0.17</td>
<td id="S4.T4.2.6.3.6" class="ltx_td ltx_align_right ltx_border_bb">0.21</td>
<td id="S4.T4.2.6.3.7" class="ltx_td ltx_align_right ltx_border_bb">0.23</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Zero-shot performance on automatic speech recognition and speech translation.
</figcaption>
</figure>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">MT-Specific Models</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">We evaluate M2M-100 <cite class="ltx_cite ltx_citemacro_cite">Fan et al. (<a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite>, NLLB <cite class="ltx_cite ltx_citemacro_cite">Costa-jussà et al. (<a href="#bib.bib29" title="" class="ltx_ref">2022</a>)</cite>, and MENYO-20k <cite class="ltx_cite ltx_citemacro_cite">Adelani et al. (<a href="#bib.bib6" title="" class="ltx_ref">2021a</a>)</cite>. M2M-100 and NLLB are multilingual MT models trained on data spanning 100 and 202 languages respectively.
MENYO-20k is a Yorùbá-to-English-specific model fine-tuned on top of the multilingual pretrained mT5 model <cite class="ltx_cite ltx_citemacro_cite">Xue et al. (<a href="#bib.bib83" title="" class="ltx_ref">2021</a>)</cite>. MENYO-20k’s model is trained with the MENYO-20k dataset, a curated multi-domain standard Yorùbá dataset with proper orthography.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Language Models</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">We evaluate two multilingual LMs, Aya <cite class="ltx_cite ltx_citemacro_cite">Üstün et al. (<a href="#bib.bib85" title="" class="ltx_ref">2024</a>)</cite> and MT-0 <cite class="ltx_cite ltx_citemacro_cite">Muennighoff et al. (<a href="#bib.bib49" title="" class="ltx_ref">2023</a>)</cite>, trained on 101 and 46 languages, respectively (standard Yorùbá included). We prompt the LM to generate translations in a zero-shot setting with the prefix “Translate to English: " added to each sentence and greedily decode the continuation. We do not provide in-context examples in order to create a comparable setting to the evaluation of MT-specific models.</p>
</div>
<div id="S4.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p2.1" class="ltx_p">Finally, we include Google Translate (GMNMT)<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://translate.google.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://translate.google.com/</a>. API last accessed on June 7, 2024.</span></span></span> due to its widespread commercial use. We request the NMT model through the API, and cannot control any other aspects of its usage.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results</h4>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p">We measure translation quality using AfriCOMET <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib81" title="" class="ltx_ref">2023</a>)</cite> and BLEU <cite class="ltx_cite ltx_citemacro_cite">Papineni et al. (<a href="#bib.bib67" title="" class="ltx_ref">2002</a>)</cite>. Firstly, we report zero-shot performance across all models in <a href="#S3.T2" title="Table 2 ‣ 3.1 Text Curation and Dialect Localization ‣ 3 YorùLect Corpus ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.
Although performance is relatively low across the board, among MT-specific models, NLLB performs best across all dialects, outperforming M2M100 and MENYO-20k. Comparing performance on LMs, Aya performs better than MT0 on all dialects except standard Yorùbá. Google Translate outperforms all systems across all dialects. Overall, we see a huge performance gap between standard Yorùbá and the rest of the dialects.
This observation is not surprising and is very consistent across all systems. The results in <a href="#S3.T2" title="Table 2 ‣ 3.1 Text Curation and Dialect Localization ‣ 3 YorùLect Corpus ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 2</span></a> also show that Ìlàjẹ has the worst-performing BLEU score across all models. We hypothesize that this is because Ìlàjẹ is largely spoken in Ọ̀ndọ́ state, which is geographically distant from Ọ̀yọ́ state where standard Yorùbá originated from.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Automatic Speech Recognition</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We evaluate three models: Whisper <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib70" title="" class="ltx_ref">2022</a>)</cite>, SeamlessM4T <cite class="ltx_cite ltx_citemacro_cite">Communication et al. (<a href="#bib.bib28" title="" class="ltx_ref">2023</a>)</cite>, and MMS <cite class="ltx_cite ltx_citemacro_cite">Pratap et al. (<a href="#bib.bib68" title="" class="ltx_ref">2024</a>)</cite>. All models include standard Yorùbá in their pretraining data. Whisper is an end-to-end ASR model, implemented as an encoder-decoder transformer, trained on 680,000 hours of multilingual and multitask supervised data collected from the web. The authors argue that it is robust to accents and variations in speech. It was optimized to perform the tasks of transcribing audio into its original language and translating the audio into English text. SeamlessM4T is a multilingual and multi-modal model that also translates and transcribes across speech and text. It is trained on 470,000 hours of mined speech and text-aligned data and supports ASR, S2TT, speech-to-speech translation, text-to-text translation and text-to-speech translation, although our focus here is ASR and S2TT. MMS is an ASR-only model finetuned on top of wav2vec 2.0 <cite class="ltx_cite ltx_citemacro_cite">Baevski et al. (<a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite> models across 1,107 languages. In addition to dense finetuning, they also finetune language-specific adapter modules <cite class="ltx_cite ltx_citemacro_cite">Houlsby et al. (<a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite> for each language in their pretraining data.</p>
</div>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">We report word error rate (WER) with the models MMS, SeamlessM4T, and Whisper in <a href="#S4.T4" title="Table 4 ‣ 4.1 Machine Translation ‣ 4 Zero-shot Experiments ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 4</span></a> (left). Performance is generally poor across all models, with MMS performing the best. We hypothesize that MMS performs best due to its training with parameter-efficient finetuning using language-specific adapters. We see an average performance gap of 12 points between standard Yorùbá and the other dialects on MMS and SeamlessM4T. With Whisper, the case is different: while the WER is generally very high, we see that only Ifẹ̀ is substantially better across all dialects. Upon manually reviewing the transcriptions from all models, we noticed that Whisper did not include diacritics in its generated transcriptions. Yorùbá is a tonal language, and diacritics play a crucial role in disambiguating word meanings. We believe that this, coupled with the generation of overly segmented transcriptions contributes to Whisper’s exceptionally high word-error rate exceeding 100.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Speech Translation</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We only evaluate Whisper <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib70" title="" class="ltx_ref">2022</a>)</cite> and SeamlessM4T <cite class="ltx_cite ltx_citemacro_cite">Communication et al. (<a href="#bib.bib28" title="" class="ltx_ref">2023</a>)</cite>. Just like MT, we only evaluate translation from the standard language or dialect into English as we cannot expect the models to generate text in any of the dialects without explictly finetuning it do so.</p>
</div>
<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results</h4>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p">In <a href="#S4.T4" title="Table 4 ‣ 4.1 Machine Translation ‣ 4 Zero-shot Experiments ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 4</span></a> (right), we present the zero-shot speech-to-text translation (S2TT) results of SeamlessM4T and Whisper models, the only open-source models we are aware of that include coverage for Standard Yorùbá. Among all the tasks we evaluated, S2TT appears to be the most challenging. Performance is absolutely low for both models with Whisper performing particularly poorly. Across dialects, with SeamlessM4T, Standard Yoruba performs better yet again with an average of 9 points performance gap compared to Ìlàjẹ and Ifẹ̀.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Finetuning Experiments</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Machine Translation</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.2" class="ltx_p">Next, we finetune NLLB-600M <cite class="ltx_cite ltx_citemacro_cite">Team et al. (<a href="#bib.bib77" title="" class="ltx_ref">2022</a>)</cite> on the training portion of our dataset in both directions, English<math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mo stretchy="false" id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\rightarrow</annotation></semantics></math>Dialect and Dialect<math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mo stretchy="false" id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><ci id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">\rightarrow</annotation></semantics></math>English. We experiment with training all dialects jointly under the Yorùbá language code, and training the dialects separately by adding new language codes for each dialect and initializing them with the Yorùbá embedding. In an attempt to further boost performance, we augment our training data with 10k instances from MENYO-20k <cite class="ltx_cite ltx_citemacro_cite">Adelani et al. (<a href="#bib.bib6" title="" class="ltx_ref">2021a</a>)</cite>.<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>MENYO-20k was included in NLLB’s pretraining data, however we try to include it in another step of language-specific finetuning.</span></span></span></p>
</div>
<section id="S5.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results</h4>

<div id="S5.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px1.p1.1" class="ltx_p">In <a href="#S5.F2" title="Figure 2 ‣ Results ‣ 5.1 Machine Translation ‣ 5 Finetuning Experiments ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a> we analyze the translation quality following NLLB finetuning from Dialect<math id="S5.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.SS1.SSS0.Px1.p1.1.m1.1a"><mo stretchy="false" id="S5.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.1.m1.1b"><ci id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.1.m1.1c">\rightarrow</annotation></semantics></math>English, comparing it with both the translation quality prior to finetuning and with Google Translate, which serves as the top-performing zero-shot system (<a href="#S3.T2" title="Table 2 ‣ 3.1 Text Curation and Dialect Localization ‣ 3 YorùLect Corpus ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 2</span></a>). Our results demonstrate that with only 802 training instances per dialects we outperform Google Translate on the non-standard dialects. While the performance of Google Translate remains notably superior for the standard dialect, we anticipate that scaling up the data could potentially bridge this gap.</p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2406.19564/assets/x1.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="285" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>MT results <math id="S5.F2.2.m1.1" class="ltx_Math" alttext="(\uparrow)" display="inline"><semantics id="S5.F2.2.m1.1b"><mrow id="S5.F2.2.m1.1.2.2"><mo stretchy="false" id="S5.F2.2.m1.1.2.2.1">(</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S5.F2.2.m1.1.1" xref="S5.F2.2.m1.1.1.cmml">↑</mo><mo stretchy="false" id="S5.F2.2.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.F2.2.m1.1c"><ci id="S5.F2.2.m1.1.1.cmml" xref="S5.F2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F2.2.m1.1d">(\uparrow)</annotation></semantics></math>. We compare BLEU across Google Translate, NLLB prior to finetuning, and NLLB after finetuning.
</figcaption>
</figure>
<div id="S5.SS1.SSS0.Px1.p2" class="ltx_para">
<p id="S5.SS1.SSS0.Px1.p2.2" class="ltx_p">We present results for fine-tuning from English<math id="S5.SS1.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.SS1.SSS0.Px1.p2.1.m1.1a"><mo stretchy="false" id="S5.SS1.SSS0.Px1.p2.1.m1.1.1" xref="S5.SS1.SSS0.Px1.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p2.1.m1.1b"><ci id="S5.SS1.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p2.1.m1.1c">\rightarrow</annotation></semantics></math>Dialect in <a href="#A1.T12" title="Table 12 ‣ A.4 Human evaluation ‣ Appendix A Appendix ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 12</span></a> in the Appendix. Our observation is that performance is generally worse than fine-tuning in Dialect<math id="S5.SS1.SSS0.Px1.p2.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.SS1.SSS0.Px1.p2.2.m2.1a"><mo stretchy="false" id="S5.SS1.SSS0.Px1.p2.2.m2.1.1" xref="S5.SS1.SSS0.Px1.p2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p2.2.m2.1b"><ci id="S5.SS1.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S5.SS1.SSS0.Px1.p2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p2.2.m2.1c">\rightarrow</annotation></semantics></math>English direction. This is consistent with previous findings that translating into English
could be easier than translating from it <cite class="ltx_cite ltx_citemacro_cite">Belinkov et al. (<a href="#bib.bib23" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Automatic Speech Recognition</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We finetune MMS <cite class="ltx_cite ltx_citemacro_cite">Pratap et al. (<a href="#bib.bib68" title="" class="ltx_ref">2024</a>)</cite> and XLSR-Wav2Vec2 <cite class="ltx_cite ltx_citemacro_cite">Baevski et al. (<a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite>. For the MMS model, we only finetune the Yorùbá adapter layer, while the other weights of the model are kept frozen.</p>
</div>
<section id="S5.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results</h4>

<div id="S5.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p1.1" class="ltx_p">We compare performance after finetuning XLSR and MMS with two different model sizes each: 300M and 1.3B parameters. MMS is a more suitable choice for finetuning because of its parameter efficiency, since we only have to tune the Yorùbá adapter layers. However, we choose to compare it with XLSR as well, as previous studies have reported significant performance improvements by finetuning XLSR <cite class="ltx_cite ltx_citemacro_cite">Ogunremi et al. (<a href="#bib.bib57" title="" class="ltx_ref">2024</a>)</cite>. In <a href="#S5.F3" title="Figure 3 ‣ Results ‣ 5.2 Automatic Speech Recognition ‣ 5 Finetuning Experiments ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>, we first see that for XLSR, fine tuning a model with less capacity (300M parameters) yields better performance across all dialects compared to fine tuning a model with about <math id="S5.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_math_unparsed" alttext="4\times" display="inline"><semantics id="S5.SS2.SSS0.Px1.p1.1.m1.1a"><mrow id="S5.SS2.SSS0.Px1.p1.1.m1.1b"><mn id="S5.SS2.SSS0.Px1.p1.1.m1.1.1">4</mn><mo lspace="0.222em" id="S5.SS2.SSS0.Px1.p1.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.1.m1.1c">4\times</annotation></semantics></math> more parameters. However, with MMS, we see that finetuning the 1.3B model yields a lower WER compared to finetuning the 300M model. Here, the performance gap is not as drastic as with XLSR.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2406.19564/assets/x2.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="287" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>ASR results. <math id="S5.F3.2.m1.1" class="ltx_Math" alttext="(\downarrow)" display="inline"><semantics id="S5.F3.2.m1.1b"><mrow id="S5.F3.2.m1.1.2.2"><mo stretchy="false" id="S5.F3.2.m1.1.2.2.1">(</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S5.F3.2.m1.1.1" xref="S5.F3.2.m1.1.1.cmml">↓</mo><mo stretchy="false" id="S5.F3.2.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.F3.2.m1.1c"><ci id="S5.F3.2.m1.1.1.cmml" xref="S5.F3.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.2.m1.1d">(\downarrow)</annotation></semantics></math> We compare WER between zero-shot and jointly fine-tuning on all dialects on XLSR and MMS models.
</figcaption>
</figure>
<div id="S5.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p2.1" class="ltx_p">On average, there is a performance improvement of approximately 20% after finetuning. As expected, across all models, the performance on the Standard Yorùbá dialect remains considerably better than that of Ìlàjẹ and Ifẹ̀. We expect that increasing the size of the finetuning data could help close this gap and could be addressed in future work.</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Speech-to-Text Translation</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">SeamlessM4T <cite class="ltx_cite ltx_citemacro_cite">Communication et al. (<a href="#bib.bib28" title="" class="ltx_ref">2023</a>)</cite> is the only model we finetune for speech-to-text-translation, since it its the best performing model from zero-shot experiments (see <a href="#S4.T4" title="Table 4 ‣ 4.1 Machine Translation ‣ 4 Zero-shot Experiments ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 4</span></a> and the only other S2TT model (to the best of our knowledge) with Yoruba in its training data asides. We finetune in the (Dialect<math id="S5.SS3.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.SS3.p1.1.m1.1a"><mo stretchy="false" id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><ci id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">\rightarrow</annotation></semantics></math>English) direction.</p>
</div>
<section id="S5.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results</h4>

<figure id="S5.F4" class="ltx_figure"><img src="/html/2406.19564/assets/x3.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="285" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>S2TT results <math id="S5.F4.2.m1.1" class="ltx_Math" alttext="(\uparrow)" display="inline"><semantics id="S5.F4.2.m1.1b"><mrow id="S5.F4.2.m1.1.2.2"><mo stretchy="false" id="S5.F4.2.m1.1.2.2.1">(</mo><mo lspace="0em" rspace="0em" stretchy="false" id="S5.F4.2.m1.1.1" xref="S5.F4.2.m1.1.1.cmml">↑</mo><mo stretchy="false" id="S5.F4.2.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.2.m1.1c"><ci id="S5.F4.2.m1.1.1.cmml" xref="S5.F4.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.m1.1d">(\uparrow)</annotation></semantics></math>. We compare BLEU prior to finetuning and after finetuning SeamlessM4T.</figcaption>
</figure>
<div id="S5.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS3.SSS0.Px1.p1.1" class="ltx_p">The results in <a href="#S5.F4" title="Figure 4 ‣ Results ‣ 5.3 Speech-to-Text Translation ‣ 5 Finetuning Experiments ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 4</span></a> show that while we can reasonably boost performance on Standard Yorùbá after finetuning, it still remains a hard task for the other dialects with just finetuning. We hypothesize that this occurs for two reasons, firstly the amount of Yorùbá S2TT data in SeamlessM4T is smaller than the data available to train ASR <cite class="ltx_cite ltx_citemacro_cite">Communication et al. (<a href="#bib.bib28" title="" class="ltx_ref">2023</a>)</cite>. Secondly, while there is notable lexical variation across Yorùbá dialects, the differences are even more pronounced in spoken language. This significant variation in pronunciation and intonation, coupled with the fact that S2TT data for Yorùbá is scarcer than ASR data makes the task of adaptation particularly challenging.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Human Evaluation</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We complement automatic evaluation metrics with a human evaluation study to assess translation and transcription quality from the best models after fine-tuning for MT and ASR. Previous research has shown that word error rate (WER) is not nuanced, as it treats all errors in ASR text—insertions, deletions, and substitutions—the same, without considering their impact on readability <cite class="ltx_cite ltx_citemacro_cite">Itoh et al. (<a href="#bib.bib41" title="" class="ltx_ref">2015</a>)</cite>.<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://machinelearning.apple.com/research/humanizing-wer" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://machinelearning.apple.com/research/humanizing-wer</a></span></span></span> For ASR, one native speaker per dialect rated the quality of 30 randomly sampled transcriptions from the test set produced by our best ASR models after finetuning. After listening to the source speech they assess fluency (how natural and grammatically correct the transcription sounds in their dialect) and adequacy (how accurately the transcription conveys the meaning of the source speech) using a Likert scale of (1–5), the higher the better. In <a href="#S6.T5" title="Table 5 ‣ 6 Human Evaluation ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 5</span></a> we show that human raters consider the transcriptions of standard Yorùbá and Ifẹ̀ to be moderately adequate and fluent on average, compared to Ìlàjẹ. These findings align with our observations from automatic metrics.</p>
</div>
<figure id="S6.T5" class="ltx_table">
<div id="S6.T5.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:325.2pt;height:137.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(77.4pt,-32.7pt) scale(1.90928215801758,1.90928215801758) ;">
<table id="S6.T5.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T5.2.2.2" class="ltx_tr">
<th id="S6.T5.2.2.2.3" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S6.T5.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Adequacy <math id="S6.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S6.T5.1.1.1.1.m1.1a"><mo stretchy="false" id="S6.T5.1.1.1.1.m1.1.1" xref="S6.T5.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T5.1.1.1.1.m1.1b"><ci id="S6.T5.1.1.1.1.m1.1.1.cmml" xref="S6.T5.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S6.T5.2.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Fluency <math id="S6.T5.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S6.T5.2.2.2.2.m1.1a"><mo stretchy="false" id="S6.T5.2.2.2.2.m1.1.1" xref="S6.T5.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T5.2.2.2.2.m1.1b"><ci id="S6.T5.2.2.2.2.m1.1.1.cmml" xref="S6.T5.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T5.2.2.3.1" class="ltx_tr">
<th id="S6.T5.2.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Standard</th>
<td id="S6.T5.2.2.3.1.2" class="ltx_td ltx_align_right ltx_border_t">3.37</td>
<td id="S6.T5.2.2.3.1.3" class="ltx_td ltx_align_right ltx_border_t">3.03</td>
</tr>
<tr id="S6.T5.2.2.4.2" class="ltx_tr">
<th id="S6.T5.2.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Ìlàjẹ</th>
<td id="S6.T5.2.2.4.2.2" class="ltx_td ltx_align_right">2.73</td>
<td id="S6.T5.2.2.4.2.3" class="ltx_td ltx_align_right">2.62</td>
</tr>
<tr id="S6.T5.2.2.5.3" class="ltx_tr">
<th id="S6.T5.2.2.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">Ifẹ̀</th>
<td id="S6.T5.2.2.5.3.2" class="ltx_td ltx_align_right ltx_border_bb">3.40</td>
<td id="S6.T5.2.2.5.3.3" class="ltx_td ltx_align_right ltx_border_bb">2.90</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Average human ratings of adequacy and fluency of transcriptions from the best ASR models after finetuning.</figcaption>
</figure>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">For MT, we ask human raters to compare the quality of translations from Google Translate with translations after finetuning NLLB, still focusing on fluency and adequacy still using a Likert scale (1–5). We provide the exact phrasings of instruction in the <a href="#A1.SS4" title="A.4 Human evaluation ‣ Appendix A Appendix ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">§A.4</span></a>. Our results, displayed in <a href="#S6.T6" title="Table 6 ‣ 6 Human Evaluation ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 6</span></a>, show that Google Translate is rated to be more fluent and accurate on Standard Yorùbá and Ìlàjẹ. However, our finetuned NLLB-600M model is rated to be more more fluent and accurate on Ifẹ̀ and Ìjẹ̀bú. The results on standard Yorùbá, Ifẹ̀ and Ìjẹ̀bú are very consistent with automatic evaluation results in <a href="#S5.F2" title="Figure 2 ‣ Results ‣ 5.1 Machine Translation ‣ 5 Finetuning Experiments ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>. This is not the case with Ìlàjẹ, as our ratings are lower compared to Google Translate, which contrasts with our automatic evaluation in <a href="#S5.F2" title="Figure 2 ‣ Results ‣ 5.1 Machine Translation ‣ 5 Finetuning Experiments ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>.</p>
</div>
<figure id="S6.T6" class="ltx_table">
<div id="S6.T6.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:424.9pt;height:194.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(94.5pt,-43.2pt) scale(1.80061451436852,1.80061451436852) ;">
<table id="S6.T6.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T6.2.2.2" class="ltx_tr">
<th id="S6.T6.2.2.2.3" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S6.T6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Adequacy <math id="S6.T6.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S6.T6.1.1.1.1.m1.1a"><mo stretchy="false" id="S6.T6.1.1.1.1.m1.1.1" xref="S6.T6.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T6.1.1.1.1.m1.1b"><ci id="S6.T6.1.1.1.1.m1.1.1.cmml" xref="S6.T6.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S6.T6.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Fluency <math id="S6.T6.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S6.T6.2.2.2.2.m1.1a"><mo stretchy="false" id="S6.T6.2.2.2.2.m1.1.1" xref="S6.T6.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T6.2.2.2.2.m1.1b"><ci id="S6.T6.2.2.2.2.m1.1.1.cmml" xref="S6.T6.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T6.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
<tr id="S6.T6.2.2.3.1" class="ltx_tr">
<th id="S6.T6.2.2.3.1.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S6.T6.2.2.3.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">GMNMT</th>
<th id="S6.T6.2.2.3.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">NLLB</th>
<th id="S6.T6.2.2.3.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">GMNMT</th>
<th id="S6.T6.2.2.3.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">NLLB</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T6.2.2.4.1" class="ltx_tr">
<th id="S6.T6.2.2.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Standard</th>
<td id="S6.T6.2.2.4.1.2" class="ltx_td ltx_align_right ltx_border_t">4.47</td>
<td id="S6.T6.2.2.4.1.3" class="ltx_td ltx_align_right ltx_border_t">4.13</td>
<td id="S6.T6.2.2.4.1.4" class="ltx_td ltx_align_right ltx_border_t">4.73</td>
<td id="S6.T6.2.2.4.1.5" class="ltx_td ltx_align_right ltx_border_t">4.60</td>
</tr>
<tr id="S6.T6.2.2.5.2" class="ltx_tr">
<th id="S6.T6.2.2.5.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Ìlàjẹ</th>
<td id="S6.T6.2.2.5.2.2" class="ltx_td ltx_align_right">2.73</td>
<td id="S6.T6.2.2.5.2.3" class="ltx_td ltx_align_right">2.63</td>
<td id="S6.T6.2.2.5.2.4" class="ltx_td ltx_align_right">2.10</td>
<td id="S6.T6.2.2.5.2.5" class="ltx_td ltx_align_right">1.83</td>
</tr>
<tr id="S6.T6.2.2.6.3" class="ltx_tr">
<th id="S6.T6.2.2.6.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">Ifẹ̀</th>
<td id="S6.T6.2.2.6.3.2" class="ltx_td ltx_align_right">2.90</td>
<td id="S6.T6.2.2.6.3.3" class="ltx_td ltx_align_right">3.67</td>
<td id="S6.T6.2.2.6.3.4" class="ltx_td ltx_align_right">2.73</td>
<td id="S6.T6.2.2.6.3.5" class="ltx_td ltx_align_right">3.57</td>
</tr>
<tr id="S6.T6.2.2.7.4" class="ltx_tr">
<th id="S6.T6.2.2.7.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">Ìjẹ̀bú</th>
<td id="S6.T6.2.2.7.4.2" class="ltx_td ltx_align_right ltx_border_bb">3.37</td>
<td id="S6.T6.2.2.7.4.3" class="ltx_td ltx_align_right ltx_border_bb">3.96</td>
<td id="S6.T6.2.2.7.4.4" class="ltx_td ltx_align_right ltx_border_bb">3.60</td>
<td id="S6.T6.2.2.7.4.5" class="ltx_td ltx_align_right ltx_border_bb">4.20</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Average human ratings of adequacy and fluency of test set translations comparing Google Translate with the best models after fine-tuning NLLB-600M</figcaption>
</figure>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Analysis and Discussion</h2>

<section id="S7.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Does edit distance explain performance gaps?</h4>

<div id="S7.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S7.SS0.SSS0.Px1.p1.7" class="ltx_p">In this analysis we aim to understand how dialectal similarity influences model adaptation during finetuning. Ideally, we expect dialects with higher similarity to Standard Yorùbá to perform better. Edit distance <cite class="ltx_cite ltx_citemacro_cite">Levenshtein (<a href="#bib.bib46" title="" class="ltx_ref">1966</a>)</cite> is a simple method commonly used in dialectometry to infer pronunciation differences between language dialects <cite class="ltx_cite ltx_citemacro_cite">Nerbonne et al. (<a href="#bib.bib52" title="" class="ltx_ref">2020</a>, <a href="#bib.bib53" title="" class="ltx_ref">1996</a>); Heeringa (<a href="#bib.bib38" title="" class="ltx_ref">2004</a>)</cite>. In our work, we use edit distance as a proxy for similarity between Standard Yorùbá and the other dialects in our corpus, expecting that dialects with a higher degree of similarity (lower edit distance) will perform better. We compute the average edit distance per dialect,
<math id="S7.SS0.SSS0.Px1.p1.1.m1.2" class="ltx_Math" alttext="\bar{d}=\frac{1}{N}\sum_{i=1}^{N}d(s_{i},t_{i})" display="inline"><semantics id="S7.SS0.SSS0.Px1.p1.1.m1.2a"><mrow id="S7.SS0.SSS0.Px1.p1.1.m1.2.2" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.cmml"><mover accent="true" id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.4" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.4.cmml"><mi id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.4.2" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.4.2.cmml">d</mi><mo id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.4.1" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.4.1.cmml">¯</mo></mover><mo id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.3" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.3.cmml">=</mo><mrow id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.cmml"><mfrac id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.4" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.4.cmml"><mn id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.4.2" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.4.2.cmml">1</mn><mi id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.4.3" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.4.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.3" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.3.cmml">​</mo><mrow id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.cmml"><msubsup id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.cmml"><mo id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.2" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.2.cmml">∑</mo><mrow id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.3" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.3.cmml"><mi id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.3.2" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.3.2.cmml">i</mi><mo id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.3.1" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.3.1.cmml">=</mo><mn id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.3.3" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.3.3.cmml">1</mn></mrow><mi id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.3" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.3.cmml">N</mi></msubsup><mrow id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.cmml"><mi id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.4" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.3" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.3.cmml">​</mo><mrow id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.2" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.2.3" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.3.cmml">(</mo><msub id="S7.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.1.1.1" xref="S7.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S7.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.1.1.1.2" xref="S7.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.1.1.1.2.cmml">s</mi><mi id="S7.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.1.1.1.3" xref="S7.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.2.4" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.3.cmml">,</mo><msub id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.2.2" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.2.2.cmml"><mi id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.2.2.2" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.2.2.2.cmml">t</mi><mi id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.2.2.3" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.2.5" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.SS0.SSS0.Px1.p1.1.m1.2b"><apply id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2"><eq id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.3.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.3"></eq><apply id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.4.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.4"><ci id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.4.1.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.4.1">¯</ci><ci id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.4.2.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.4.2">𝑑</ci></apply><apply id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2"><times id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.3.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.3"></times><apply id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.4.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.4"><divide id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.4.1.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.4"></divide><cn type="integer" id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.4.2.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.4.2">1</cn><ci id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.4.3.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.4.3">𝑁</ci></apply><apply id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2"><apply id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3"><csymbol cd="ambiguous" id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.1.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3">superscript</csymbol><apply id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3"><csymbol cd="ambiguous" id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.1.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3">subscript</csymbol><sum id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.2.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.2"></sum><apply id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.3.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.3"><eq id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.3.1.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.3.1"></eq><ci id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.3.2.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.3.2">𝑖</ci><cn type="integer" id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.3.3.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.2.3.3">1</cn></apply></apply><ci id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.3.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.3">𝑁</ci></apply><apply id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2"><times id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.3.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.3"></times><ci id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.4.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.4">𝑑</ci><interval closure="open" id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.3.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.2"><apply id="S7.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S7.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S7.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.1.1.1.2">𝑠</ci><ci id="S7.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.2.2.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.2.2.1.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.2.2">subscript</csymbol><ci id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.2.2.2.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.2.2.2">𝑡</ci><ci id="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.2.2.3.cmml" xref="S7.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.2.2.2.3">𝑖</ci></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS0.SSS0.Px1.p1.1.m1.2c">\bar{d}=\frac{1}{N}\sum_{i=1}^{N}d(s_{i},t_{i})</annotation></semantics></math>, where <math id="S7.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S7.SS0.SSS0.Px1.p1.2.m2.1a"><mi id="S7.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S7.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S7.SS0.SSS0.Px1.p1.2.m2.1b"><ci id="S7.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S7.SS0.SSS0.Px1.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS0.SSS0.Px1.p1.2.m2.1c">N</annotation></semantics></math> is the number of sentences in the test set of the dialect, <math id="S7.SS0.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S7.SS0.SSS0.Px1.p1.3.m3.1a"><mi id="S7.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S7.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S7.SS0.SSS0.Px1.p1.3.m3.1b"><ci id="S7.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S7.SS0.SSS0.Px1.p1.3.m3.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS0.SSS0.Px1.p1.3.m3.1c">s</annotation></semantics></math> is the sentence in Standard Yorùbá, <math id="S7.SS0.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S7.SS0.SSS0.Px1.p1.4.m4.1a"><mi id="S7.SS0.SSS0.Px1.p1.4.m4.1.1" xref="S7.SS0.SSS0.Px1.p1.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S7.SS0.SSS0.Px1.p1.4.m4.1b"><ci id="S7.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S7.SS0.SSS0.Px1.p1.4.m4.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS0.SSS0.Px1.p1.4.m4.1c">t</annotation></semantics></math> is the sentence in the corresponding dialect, and <math id="S7.SS0.SSS0.Px1.p1.5.m5.2" class="ltx_Math" alttext="d(s_{i},t_{i})" display="inline"><semantics id="S7.SS0.SSS0.Px1.p1.5.m5.2a"><mrow id="S7.SS0.SSS0.Px1.p1.5.m5.2.2" xref="S7.SS0.SSS0.Px1.p1.5.m5.2.2.cmml"><mi id="S7.SS0.SSS0.Px1.p1.5.m5.2.2.4" xref="S7.SS0.SSS0.Px1.p1.5.m5.2.2.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S7.SS0.SSS0.Px1.p1.5.m5.2.2.3" xref="S7.SS0.SSS0.Px1.p1.5.m5.2.2.3.cmml">​</mo><mrow id="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.2" xref="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.3.cmml"><mo stretchy="false" id="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.3" xref="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.3.cmml">(</mo><msub id="S7.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1" xref="S7.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.cmml"><mi id="S7.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.2" xref="S7.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.2.cmml">s</mi><mi id="S7.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.3" xref="S7.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.4" xref="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.3.cmml">,</mo><msub id="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2" xref="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.cmml"><mi id="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.2" xref="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.2.cmml">t</mi><mi id="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.3" xref="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.5" xref="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.SS0.SSS0.Px1.p1.5.m5.2b"><apply id="S7.SS0.SSS0.Px1.p1.5.m5.2.2.cmml" xref="S7.SS0.SSS0.Px1.p1.5.m5.2.2"><times id="S7.SS0.SSS0.Px1.p1.5.m5.2.2.3.cmml" xref="S7.SS0.SSS0.Px1.p1.5.m5.2.2.3"></times><ci id="S7.SS0.SSS0.Px1.p1.5.m5.2.2.4.cmml" xref="S7.SS0.SSS0.Px1.p1.5.m5.2.2.4">𝑑</ci><interval closure="open" id="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.3.cmml" xref="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.2"><apply id="S7.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.cmml" xref="S7.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="S7.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.cmml" xref="S7.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1">subscript</csymbol><ci id="S7.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.2.cmml" xref="S7.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.2">𝑠</ci><ci id="S7.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.3.cmml" xref="S7.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.3">𝑖</ci></apply><apply id="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.cmml" xref="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2"><csymbol cd="ambiguous" id="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.1.cmml" xref="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2">subscript</csymbol><ci id="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.2.cmml" xref="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.2">𝑡</ci><ci id="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.3.cmml" xref="S7.SS0.SSS0.Px1.p1.5.m5.2.2.2.2.2.3">𝑖</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS0.SSS0.Px1.p1.5.m5.2c">d(s_{i},t_{i})</annotation></semantics></math> is the edit distance between <math id="S7.SS0.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="s_{i}" display="inline"><semantics id="S7.SS0.SSS0.Px1.p1.6.m6.1a"><msub id="S7.SS0.SSS0.Px1.p1.6.m6.1.1" xref="S7.SS0.SSS0.Px1.p1.6.m6.1.1.cmml"><mi id="S7.SS0.SSS0.Px1.p1.6.m6.1.1.2" xref="S7.SS0.SSS0.Px1.p1.6.m6.1.1.2.cmml">s</mi><mi id="S7.SS0.SSS0.Px1.p1.6.m6.1.1.3" xref="S7.SS0.SSS0.Px1.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS0.SSS0.Px1.p1.6.m6.1b"><apply id="S7.SS0.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S7.SS0.SSS0.Px1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S7.SS0.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="S7.SS0.SSS0.Px1.p1.6.m6.1.1">subscript</csymbol><ci id="S7.SS0.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="S7.SS0.SSS0.Px1.p1.6.m6.1.1.2">𝑠</ci><ci id="S7.SS0.SSS0.Px1.p1.6.m6.1.1.3.cmml" xref="S7.SS0.SSS0.Px1.p1.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS0.SSS0.Px1.p1.6.m6.1c">s_{i}</annotation></semantics></math> and <math id="S7.SS0.SSS0.Px1.p1.7.m7.1" class="ltx_Math" alttext="t_{i}" display="inline"><semantics id="S7.SS0.SSS0.Px1.p1.7.m7.1a"><msub id="S7.SS0.SSS0.Px1.p1.7.m7.1.1" xref="S7.SS0.SSS0.Px1.p1.7.m7.1.1.cmml"><mi id="S7.SS0.SSS0.Px1.p1.7.m7.1.1.2" xref="S7.SS0.SSS0.Px1.p1.7.m7.1.1.2.cmml">t</mi><mi id="S7.SS0.SSS0.Px1.p1.7.m7.1.1.3" xref="S7.SS0.SSS0.Px1.p1.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S7.SS0.SSS0.Px1.p1.7.m7.1b"><apply id="S7.SS0.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S7.SS0.SSS0.Px1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S7.SS0.SSS0.Px1.p1.7.m7.1.1.1.cmml" xref="S7.SS0.SSS0.Px1.p1.7.m7.1.1">subscript</csymbol><ci id="S7.SS0.SSS0.Px1.p1.7.m7.1.1.2.cmml" xref="S7.SS0.SSS0.Px1.p1.7.m7.1.1.2">𝑡</ci><ci id="S7.SS0.SSS0.Px1.p1.7.m7.1.1.3.cmml" xref="S7.SS0.SSS0.Px1.p1.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS0.SSS0.Px1.p1.7.m7.1c">t_{i}</annotation></semantics></math> at the character-level.</p>
</div>
<div id="S7.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S7.SS0.SSS0.Px1.p2.1" class="ltx_p">We present the results of this analysis in MT in <a href="#S7.T7" title="Table 7 ‣ Does edit distance explain performance gaps? ‣ 7 Analysis and Discussion ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 7</span></a>. As expected, Ifẹ̀ has the smallest edit distance from Standard Yorùbá and respectively also the best performance after finetuning. However we surprisingly see that while Ìjẹ̀bú has a higher edit distance than Ìlàjẹ, the model performance is higher for Ìjẹ̀bú. We conclude that edit distance has a weak correlation with our MT metrics.</p>
</div>
<figure id="S7.T7" class="ltx_table">
<div id="S7.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:142.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(102.0pt,-35.3pt) scale(1.98020212084664,1.98020212084664) ;">
<table id="S7.T7.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S7.T7.1.1.1.1" class="ltx_tr">
<th id="S7.T7.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S7.T7.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dialect</span></th>
<th id="S7.T7.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S7.T7.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Avg. ED</span></th>
<th id="S7.T7.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S7.T7.1.1.1.1.3.1" class="ltx_text ltx_font_bold">BLEU</span></th>
<th id="S7.T7.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S7.T7.1.1.1.1.4.1" class="ltx_text ltx_font_bold">AfriCOMET</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S7.T7.1.1.2.1" class="ltx_tr">
<td id="S7.T7.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">Ifẹ̀</td>
<td id="S7.T7.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">24.66</td>
<td id="S7.T7.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">22.97</td>
<td id="S7.T7.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.59</td>
</tr>
<tr id="S7.T7.1.1.3.2" class="ltx_tr">
<td id="S7.T7.1.1.3.2.1" class="ltx_td ltx_align_center">Ìlàjẹ</td>
<td id="S7.T7.1.1.3.2.2" class="ltx_td ltx_align_center">38.07</td>
<td id="S7.T7.1.1.3.2.3" class="ltx_td ltx_align_center">18.64</td>
<td id="S7.T7.1.1.3.2.4" class="ltx_td ltx_align_center">0.55</td>
</tr>
<tr id="S7.T7.1.1.4.3" class="ltx_tr">
<td id="S7.T7.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_bb">Ìjẹ̀bú</td>
<td id="S7.T7.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">41.46</td>
<td id="S7.T7.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">21.98</td>
<td id="S7.T7.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">0.60</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Average edit distance and MT-Metrics comparison for MT across dialects.</figcaption>
</figure>
<div id="S7.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S7.SS0.SSS0.Px1.p3.1" class="ltx_p">For ASR, we compute edit distance on phonetic transcriptions using the PanPhon library developed by <cite class="ltx_cite ltx_citemacro_cite">Mortensen et al. (<a href="#bib.bib48" title="" class="ltx_ref">2016</a>)</cite>. The phonetic edit distance between standard Yorùbá to Ìlàjẹ and Ifẹ̀ is 34.99 and 44.4, respectively. Here again, we also see no correlations between edit distance and performance on dialect adaptation.</p>
</div>
</section>
<section id="S7.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Joint vs. dialect-specific finetuning.</h4>

<div id="S7.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S7.SS0.SSS0.Px2.p1.1" class="ltx_p">Dialects often exhibit rather subtle variations in text and speech. In data-constrained scenarios like ours, it is reasonable to expect that jointly finetuning on all dialects would result in better performance compared to fine-tuning on each dialect individually. In our earlier finetuning experiments detailed in <a href="#S5" title="5 Finetuning Experiments ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">§5</span></a>, we explored joint training. Now, we try to compare performance between joint training and individual training on MT and ASR tasks. We generally see that on both tasks, joint training is beneficial. In MT, <a href="#A1.T11" title="Table 11 ‣ A.4 Human evaluation ‣ Appendix A Appendix ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 11</span></a> in the Appendix shows a huge drop in performance across all dialects when we finetune on each dialect individually. This suggests that by jointly finetuning, the model leverages shared features across dialects for mutual benefit. However, in ASR, as shown in <a href="#A1.T8" title="Table 8 ‣ A.2 Finetuning setup ‣ Appendix A Appendix ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 8</span></a>, the drop in performance with individual finetuning is not as pronounced as with MT. We believe that in this case, the subtle variations in speech are sometimes significant, making it more challenging to greatly benefit from joint training. We however acknowledge that the data size of each individual dialect is one-fourth of the whole training set, so data paucity might also be influencing these results.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion </h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">We introduce <span id="S8.p1.1.1" class="ltx_text ltx_font_smallcaps">YorùLect</span>—the first high quality parallel text and speech corpus for four Yorùbá dialects sourced primarily from native speakers, to enable ASR, MT and S2TT tasks for widely-spoken varieties of Yorùbá. We have provided a detailed documentation of data curation process from standard text creation, to dialect localization and speech recording in communities where these dialects are spoken. Extensive experiments reveal that current models are not robust to dialectal variation, and improve significantly after our dialect-adaptive finetuning. Overall, our data collection methodology, new resources and improved models take a
step towards enhancing the quality and equity of NLP technologies for Yorùbá dialects and potentially other African languages.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Ethical Considerations</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Our datasets and models will be publicly released under an open license to foster research and continue to promote the development of NLP tools for African languages. Transcriptions, recordings and translations are carried out by paid native speakers who provided consent to use their voice to train our models. We acknowledge that the limited size of the corpus might not represent perfectly communities and speakers of the dialects. Further,
dialectal generations, particularly when erroneous, could be perceived as biased
or even microaggressions by some native speakers, as well as dialect-specific errors from the models <cite class="ltx_cite ltx_citemacro_cite">Wenzel and Kaufman (<a href="#bib.bib82" title="" class="ltx_ref">2024</a>)</cite>. While our work provides resources that aim to reduce dialectal biases and unfairness in multilingual NLP systems, future work should focus on careful human evaluation of how these resources are incorporated in end-user tools.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">A limitation of our work is the robustness of the metrics we use for evaluation. While all of these metrics are standard for all of the tasks, we acknowledge that model-based metrics like AfriCOMET <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib80" title="" class="ltx_ref">2024</a>)</cite> could be biased towards standard dialects that their models have been trained on. Exploring model-based metrics that facilitate robust evaluations on dialectal tasks remains a challenge for future work <cite class="ltx_cite ltx_citemacro_cite">Faisal et al. (<a href="#bib.bib36" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<div id="Sx2.p2" class="ltx_para">
<p id="Sx2.p2.1" class="ltx_p">Additionally, the text portion of our dataset is translated from the standard dialect into English and the non-standard dialects. We acknowledge that this could introduce translation artifacts known as translationese <cite class="ltx_cite ltx_citemacro_cite">Volansky et al. (<a href="#bib.bib79" title="" class="ltx_ref">2015</a>)</cite> that are not present in the source dialect. However, we believe that the benefits of our dataset outweighs the potential risks of these artifacts.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdul-Mageed et al. (2023)</span>
<span class="ltx_bibblock">
Muhammad Abdul-Mageed, AbdelRahim Elmadany, Chiyu Zhang, El Moatez Billah Nagoudi, Houda Bouamor, and Nizar Habash. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.arabicnlp-1.62" title="" class="ltx_ref ltx_href">NADI 2023: The fourth nuanced Arabic dialect identification shared task</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of ArabicNLP 2023</em>, pages 600–613, Singapore (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(2)</span>
<span class="ltx_bibblock">
Michael A Abiodun, Samuel O Akintoye, and Jelili Adewale Adeoye.

</span>
<span class="ltx_bibblock">A diachronic study of the loss of/w/in some lexical items in central yoruba dialect group.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adebara and Abdul-Mageed (2022)</span>
<span class="ltx_bibblock">
Ife Adebara and Muhammad Abdul-Mageed. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-long.265" title="" class="ltx_ref ltx_href">Towards afrocentric NLP for African languages: Where we are and where we can go</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3814–3841, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adebara et al. (2021)</span>
<span class="ltx_bibblock">
Ife Adebara, Muhammad Abdul-Mageed, and Miikka Silfverberg. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2103.04225" title="" class="ltx_ref ltx_href">Translating the unseen? yoruba-english mt in low-resource, morphologically-unmarked settings</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adebara et al. (2022)</span>
<span class="ltx_bibblock">
Ife Adebara, Muhammad Abdul-Mageed, and Miikka Silfverberg. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.coling-1.449" title="" class="ltx_ref ltx_href">Linguistically-motivated Yorùbá-English machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th International Conference on Computational Linguistics</em>, pages 5066–5075, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adelani et al. (2021a)</span>
<span class="ltx_bibblock">
David Adelani, Dana Ruiter, Jesujoba Alabi, Damilola Adebonojo, Adesina Ayeni, Mofe Adeyemi, Ayodele Esther Awokoya, and Cristina España-Bonet. 2021a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2021.mtsummit-research.6" title="" class="ltx_ref ltx_href">The effect of domain and diacritics in Yoruba–English neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Translation Summit XVIII: Research Track</em>, pages 61–75, Virtual. Association for Machine Translation in the Americas.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adelani et al. (2021b)</span>
<span class="ltx_bibblock">
David Ifeoluwa Adelani, Jade Abbott, Graham Neubig, Daniel D’souza, Julia Kreutzer, Constantine Lignos, Chester Palen-Michel, Happy Buzaaba, Shruti Rijhwani, Sebastian Ruder, Stephen Mayhew, Israel Abebe Azime, Shamsuddeen H. Muhammad, Chris Chinenye Emezue, Joyce Nakatumba-Nabende, Perez Ogayo, Aremu Anuoluwapo, Catherine Gitau, Derguene Mbaye, Jesujoba Alabi, Seid Muhie Yimam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani, Rubungo Andre Niyongabo, Jonathan Mukiibi, Verrah Otiende, Iroro Orife, Davis David, Samba Ngom, Tosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi, Gerald Muriuki, Emmanuel Anebi, Chiamaka Chukwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel Oyerinde, Clemencia Siro, Tobius Saul Bateesa, Temilola Oloyede, Yvonne Wambui, Victor Akinode, Deborah Nabagereka, Maurice Katusiime, Ayodele Awokoya, Mouhamadane MBOUP, Dibora Gebreyohannes, Henok Tilaye, Kelechi Nwaike, Degaga Wolde, Abdoulaye Faye, Blessing Sibanda, Orevaoghene Ahia, Bonaventure F. P. Dossou, Kelechi Ogueji, Thierno Ibrahima DIOP,
Abdoulaye Diallo, Adewale Akinfaderin, Tendai Marengereke, and Salomey Osei. 2021b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00416" title="" class="ltx_ref ltx_href">MasakhaNER: Named entity recognition for African languages</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, 9:1116–1131.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adelani et al. (2023)</span>
<span class="ltx_bibblock">
David Ifeoluwa Adelani, Marek Masiak, Israel Abebe Azime, Jesujoba Alabi, Atnafu Lambebo Tonja, Christine Mwase, Odunayo Ogundepo, Bonaventure F. P. Dossou, Akintunde Oladipo, Doreen Nixdorf, Chris Chinenye Emezue, Sana Al-azzawi, Blessing Sibanda, Davis David, Lolwethu Ndolela, Jonathan Mukiibi, Tunde Ajayi, Tatiana Moteu, Brian Odhiambo, Abraham Owodunni, Nnaemeka Obiefuna, Muhidin Mohamed, Shamsuddeen Hassan Muhammad, Teshome Mulugeta Ababu, Saheed Abdullahi Salahudeen, Mesay Gemeda Yigezu, Tajuddeen Gwadabe, Idris Abdulmumin, Mahlet Taye, Oluwabusayo Awoyomi, Iyanuoluwa Shode, Tolulope Adelani, Habiba Abdulganiyu, Abdul-Hakeem Omotayo, Adetola Adeeko, Abeeb Afolabi, Anuoluwapo Aremu, Olanrewaju Samuel, Clemencia Siro, Wangari Kimotho, Onyekachi Ogbu, Chinedu Mbonu, Chiamaka Chukwuneke, Samuel Fanijo, Jessica Ojo, Oyinkansola Awosan, Tadesse Kebede, Toadoum Sari Sakayo, Pamela Nyatsine, Freedmore Sidume, Oreen Yousuf, Mardiyyah Oduwole, Kanda Tshinu, Ussen Kimanuka, Thina Diko, Siyanda Nxakama, Sinodos
Nigusse, Abdulmejid Johar, Shafie Mohamed, Fuad Mire Hassan, Moges Ahmed Mehamed, Evrard Ngabire, Jules Jules, Ivan Ssenkungu, and Pontus Stenetorp. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.ijcnlp-main.10" title="" class="ltx_ref ltx_href">MasakhaNEWS: News topic classification for African languages</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 144–159, Nusa Dua, Bali. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adelani et al. (2024)</span>
<span class="ltx_bibblock">
David Ifeoluwa Adelani, Jessica Ojo, Israel Abebe Azime, Jian Yun Zhuang, Jesujoba O. Alabi, Xuanli He, Millicent Ochieng, Sara Hooker, Andiswa Bukula, En-Shiun Annie Lee, Chiamaka Chukwuneke, Happy Buzaaba, Blessing Sibanda, Godson Kalipe, Jonathan Mukiibi, Salomon Kabongo, Foutse Yuehgoh, Mmasibidi Setaka, Lolwethu Ndolela, Nkiruka Odu, Rooweither Mabuya, Shamsuddeen Hassan Muhammad, Salomey Osei, Sokhar Samb, Tadesse Kebede Guge, and Pontus Stenetorp. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2406.03368" title="" class="ltx_ref ltx_href">IrokoBench: A new benchmark for African languages in the age of large language models</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adeniyi (2021)</span>
<span class="ltx_bibblock">
Kolawole Adeniyi. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://raco.cat/index.php/Dialectologia/article/view/391682" title="" class="ltx_ref ltx_href">A tonal identification of yoruba dialects</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Dialectologia: revista electrònica</em>, (28):1–31.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adetugbo (1982)</span>
<span class="ltx_bibblock">
Abiodun Adetugbo. 1982.

</span>
<span class="ltx_bibblock">Towards a yoruba dialectology.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Yoruba language and literature</em>, pages 207–224.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahia et al. (2021)</span>
<span class="ltx_bibblock">
Orevaoghene Ahia, Julia Kreutzer, and Sara Hooker. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.findings-emnlp.282" title="" class="ltx_ref ltx_href">The low-resource double bind: An empirical study of pruning for low-resource machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2021</em>, pages 3316–3333, Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahmadi et al. (2024)</span>
<span class="ltx_bibblock">
Sina Ahmadi, Daban Q. Jaff, Md Mahfuz Ibn Alam, and Antonios Anastasopoulos. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2403.01983" title="" class="ltx_ref ltx_href">Language and speech technology for central kurdish varieties</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aji et al. (2022)</span>
<span class="ltx_bibblock">
Alham Fikri Aji, Genta Indra Winata, Fajri Koto, Samuel Cahyawijaya, Ade Romadhony, Rahmad Mahendra, Kemal Kurniawan, David Moeljadi, Radityo Eko Prasojo, Timothy Baldwin, Jey Han Lau, and Sebastian Ruder. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-long.500" title="" class="ltx_ref ltx_href">One country, 700+ languages: NLP challenges for underrepresented languages and dialects in Indonesia</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 7226–7249, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akinade et al. (2023)</span>
<span class="ltx_bibblock">
Idris Akinade, Jesujoba Alabi, David Adelani, Clement Odoje, and Dietrich Klakow. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.c3nlp-1.1" title="" class="ltx_ref ltx_href">Varepsilon kú mask: Integrating Yorùbá cultural greetings into machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP)</em>, pages 1–7, Dubrovnik, Croatia. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alabi et al. (2022)</span>
<span class="ltx_bibblock">
Jesujoba O. Alabi, David Ifeoluwa Adelani, Marius Mosbach, and Dietrich Klakow. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.coling-1.382" title="" class="ltx_ref ltx_href">Adapting pre-trained language models to African languages via multilingual adaptive fine-tuning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th International Conference on Computational Linguistics</em>, pages 4336–4349, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alam et al. (2024)</span>
<span class="ltx_bibblock">
Md Mahfuz Ibn Alam, Sina Ahmadi, and Antonios Anastasopoulos. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2024.findings-eacl.125" title="" class="ltx_ref ltx_href">CODET: A benchmark for contrastive dialectal evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EACL 2024</em>, pages 1790–1859, St. Julian’s, Malta. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aremu et al. (2023)</span>
<span class="ltx_bibblock">
Anuoluwapo Aremu, Jesujoba O. Alabi, and David Ifeoluwa Adelani. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.09768" title="" class="ltx_ref ltx_href">Yorc: Yoruba reading comprehension dataset</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arokoyo et al. (2019)</span>
<span class="ltx_bibblock">
Bolanle Elizabeth Arokoyo, Olamide Oluwaseun Lagunju, et al. 2019.

</span>
<span class="ltx_bibblock">A lexicostatistics comparison of standard yorùbá, àkúré and ìkáré àkókó dialects.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Journal of Universal Language</em>, 20(2):1–27.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et al. (2020)</span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf" title="" class="ltx_ref ltx_href">wav2vec 2.0: A framework for self-supervised learning of speech representations</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 33, pages 12449–12460. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ballard (1971)</span>
<span class="ltx_bibblock">
John A Ballard. 1971.

</span>
<span class="ltx_bibblock">Historical inferences from the linguistic geography of the nigerian middle belt.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Africa</em>, 41(4):294–305.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Batibo (2005)</span>
<span class="ltx_bibblock">
H. Batibo. 2005.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://books.google.com/books?id=yoZ_fU_B0KgC" title="" class="ltx_ref ltx_href"><em id="bib.bib22.1.1.1" class="ltx_emph ltx_font_italic">Language Decline and Death in Africa: Causes, Consequences, and Challenges</em></a>.

</span>
<span class="ltx_bibblock">Multilingual matters. Multilingual Matters.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belinkov et al. (2017)</span>
<span class="ltx_bibblock">
Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P17-1080" title="" class="ltx_ref ltx_href">What do neural machine translation models learn about morphology?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 861–872, Vancouver, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blaschke et al. (2023)</span>
<span class="ltx_bibblock">
Verena Blaschke, Hinrich Schuetze, and Barbara Plank. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2023.nodalida-1.41" title="" class="ltx_ref ltx_href">A survey of corpora for Germanic low-resource languages and dialects</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)</em>, pages 392–414, Tórshavn, Faroe Islands. University of Tartu Library.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bouamor et al. (2018)</span>
<span class="ltx_bibblock">
Houda Bouamor, Nizar Habash, Mohammad Salameh, Wajdi Zaghouani, Owen Rambow, Dana Abdulrahim, Ossama Obeid, Salam Khalifa, Fadhl Eryani, Alexander Erdmann, and Kemal Oflazer. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/L18-1535" title="" class="ltx_ref ltx_href">The MADAR Arabic dialect corpus and lexicon</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</em>, Miyazaki, Japan. European Language Resources Association (ELRA).

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chambers and Trudgill (1998)</span>
<span class="ltx_bibblock">
J. K. Chambers and Peter Trudgill. 1998.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Dialectology</em>, 2 edition.

</span>
<span class="ltx_bibblock">Cambridge Textbooks in Linguistics. Cambridge University Press.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chimoto and Bassett (2022)</span>
<span class="ltx_bibblock">
Everlyn Chimoto and Bruce Bassett. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.loresmt-1.1" title="" class="ltx_ref ltx_href">Very low resource sentence alignment: Luhya and Swahili</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fifth Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2022)</em>, pages 1–8, Gyeongju, Republic of Korea. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Communication et al. (2023)</span>
<span class="ltx_bibblock">
Seamless Communication, Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht, Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek, Ethan Ye, Bapi Akula, Peng-Jen Chen, Naji El Hachem, Brian Ellis, Gabriel Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes, Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi, Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh Ramakrishnan, Anna Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta R. Costa-Jussà, Onur Celebi, Maha Elbayad, Cynthia Gao, Francisco Guzmán, Justine Kao, Ann Lee, Alexandre Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff
Wang, and Skyler Wang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2308.11596" title="" class="ltx_ref ltx_href">Seamlessm4t: Massively multilingual &amp; multimodal machine translation</a>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costa-jussà et al. (2022)</span>
<span class="ltx_bibblock">
Marta R Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022.

</span>
<span class="ltx_bibblock">No language left behind: Scaling human-centered machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2207.04672</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Courtenay (1969)</span>
<span class="ltx_bibblock">
Karen Ruth Courtenay. 1969.

</span>
<span class="ltx_bibblock">A generative phonology of yorùbá.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diab (2016)</span>
<span class="ltx_bibblock">
Mona Diab. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/W16-4805" title="" class="ltx_ref ltx_href">Processing dialectal Arabic: Exploiting variability and similarity to overcome challenges and discover opportunities</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial3)</em>, page 42, Osaka, Japan. The COLING 2016 Organizing Committee.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Diab and Habash (2012)</span>
<span class="ltx_bibblock">
Mona Diab and Nizar Habash. 2012.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/N12-4003" title="" class="ltx_ref ltx_href">Arabic dialect processing tutorial</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Tutorial Abstracts at the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, Montréal, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dione et al. (2023)</span>
<span class="ltx_bibblock">
Cheikh M. Bamba Dione, David Ifeoluwa Adelani, Peter Nabende, Jesujoba Alabi, Thapelo Sindane, Happy Buzaaba, Shamsuddeen Hassan Muhammad, Chris Chinenye Emezue, Perez Ogayo, Anuoluwapo Aremu, Catherine Gitau, Derguene Mbaye, Jonathan Mukiibi, Blessing Sibanda, Bonaventure F. P. Dossou, Andiswa Bukula, Rooweither Mabuya, Allahsera Auguste Tapo, Edwin Munkoh-Buabeng, Victoire Memdjokam Koagne, Fatoumata Ouoba Kabore, Amelia Taylor, Godson Kalipe, Tebogo Macucwa, Vukosi Marivate, Tajuddeen Gwadabe, Mboning Tchiaze Elvis, Ikechukwu Onyenwe, Gratien Atindogbe, Tolulope Adelani, Idris Akinade, Olanrewaju Samuel, Marien Nahimana, Théogène Musabeyezu, Emile Niyomutabazi, Ester Chimhenga, Kudzai Gotosa, Patrick Mizha, Apelete Agbolo, Seydou Traore, Chinedu Uchechukwu, Aliyu Yusuf, Muhammad Abdullahi, and Dietrich Klakow. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-long.609" title="" class="ltx_ref ltx_href">MasakhaPOS: Part-of-speech tagging for typologically diverse African languages</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 10883–10900, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duh (2018)</span>
<span class="ltx_bibblock">
Kevin Duh. 2018.

</span>
<span class="ltx_bibblock">The multitarget ted talks task.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://www.cs.jhu.edu/~kevinduh/a/multitarget-tedtalks/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.cs.jhu.edu/~kevinduh/a/multitarget-tedtalks/</a>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Emezue et al. (2024)</span>
<span class="ltx_bibblock">
Chris Chinenye Emezue, Ifeoma Okoh, Chinedu Mbonu, Chiamaka Chukwuneke, Daisy Lal, Ignatius Ezeani, Paul Rayson, Ijemma Onwuzulike, Chukwuma Okeke, Gerald Nweya, Bright Ogbonna, Chukwuebuka Oraegbunam, Esther Chidinma Awo-Ndubuisi, Akudo Amarachukwu Osuagwu, and Obioha Nmezi. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2405.00997" title="" class="ltx_ref ltx_href">The igboapi dataset: Empowering igbo language technologies through multi-dialectal enrichment</a>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Faisal et al. (2024)</span>
<span class="ltx_bibblock">
Fahim Faisal, Orevaoghene Ahia, Aarohi Srivastava, Kabir Ahuja, David Chiang, Yulia Tsvetkov, and Antonios Anastasopoulos. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2403.11009" title="" class="ltx_ref ltx_href">Dialectbench: A nlp benchmark for dialects, varieties, and closely-related languages</a>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2020)</span>
<span class="ltx_bibblock">
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2010.11125" title="" class="ltx_ref ltx_href">Beyond english-centric multilingual machine translation</a>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heeringa (2004)</span>
<span class="ltx_bibblock">
Wilbert Heeringa. 2004.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:61144415" title="" class="ltx_ref ltx_href">Measuring dialect pronunciation differences using levenshtein distance</a>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heine and Nurse (2000)</span>
<span class="ltx_bibblock">
B. Heine and D. Nurse. 2000.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://books.google.com/books?id=C7XhcYoFxaQC" title="" class="ltx_ref ltx_href"><em id="bib.bib39.1.1.1" class="ltx_emph ltx_font_italic">African Languages: An Introduction</em></a>.

</span>
<span class="ltx_bibblock">African Languages: An Introduction. Cambridge University Press.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Houlsby et al. (2019)</span>
<span class="ltx_bibblock">
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v97/houlsby19a.html" title="" class="ltx_ref ltx_href">Parameter-efficient transfer learning for NLP</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 36th International Conference on Machine Learning</em>, volume 97 of <em id="bib.bib40.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 2790–2799. PMLR.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Itoh et al. (2015)</span>
<span class="ltx_bibblock">
Nobuyasu Itoh, Gakuto Kurata, Ryuki Tachibana, and Masafumi Nishimura. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:358037" title="" class="ltx_ref ltx_href">A metric for evaluating speech recognizer output based on human-perception model</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Interspeech</em>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al. (2024)</span>
<span class="ltx_bibblock">
Aditya Joshi, Raj Dabre, Diptesh Kanojia, Zhuang Li, Haolan Zhan, Gholamreza Haffari, and Doris Dippold. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:266933497" title="" class="ltx_ref ltx_href">Natural language processing for dialects of a language: A survey</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2401.05632.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kantharuban et al. (2023)</span>
<span class="ltx_bibblock">
Anjali Kantharuban, Ivan Vulić, and Anna Korhonen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-emnlp.481" title="" class="ltx_ref ltx_href">Quantifying the dialect gap and its correlates across languages</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 7226–7245, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kchaou et al. (2020)</span>
<span class="ltx_bibblock">
Saméh Kchaou, Rahma Boujelbane, and Lamia Hadrich-Belguith. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2020.wanlp-1.18" title="" class="ltx_ref ltx_href">Parallel resources for Tunisian Arabic dialect translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fifth Arabic Natural Language Processing Workshop</em>, pages 200–206, Barcelona, Spain (Online). Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2023)</span>
<span class="ltx_bibblock">
Jaechan Lee, Alisa Liu, Orevaoghene Ahia, Hila Gonen, and Noah Smith. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-emnlp.302" title="" class="ltx_ref ltx_href">That was the last straw, we need more: Are translation systems sensitive to disambiguating context?</a>

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 4555–4569, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levenshtein (1966)</span>
<span class="ltx_bibblock">
Vladimir I. Levenshtein. 1966.

</span>
<span class="ltx_bibblock">Binary codes capable of correcting deletions, insertions, and reversals.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Soviet physics doklady</em>, 10(8):707–710.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Milroy and Milroy (2012)</span>
<span class="ltx_bibblock">
J. Milroy and L. Milroy. 2012.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://books.google.com/books?id=w9jFBQAAQBAJ" title="" class="ltx_ref ltx_href"><em id="bib.bib47.1.1.1" class="ltx_emph ltx_font_italic">Authority in Language: Investigating Standard English</em></a>.

</span>
<span class="ltx_bibblock">Routledge Linguistics Classics. Taylor &amp; Francis.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mortensen et al. (2016)</span>
<span class="ltx_bibblock">
David R. Mortensen, Patrick Littell, Akash Bharadwaj, Kartik Goyal, Chris Dyer, and Lori S. Levin. 2016.

</span>
<span class="ltx_bibblock">Panphon: A resource for mapping IPA segments to articulatory feature vectors.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</em>, pages 3475–3484. ACL.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et al. (2023)</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-long.891" title="" class="ltx_ref ltx_href">Crosslingual generalization through multitask finetuning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 15991–16111, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muhammad et al. (2023)</span>
<span class="ltx_bibblock">
Shamsuddeen Muhammad, Idris Abdulmumin, Abinew Ayele, Nedjma Ousidhoum, David Adelani, Seid Yimam, Ibrahim Ahmad, Meriem Beloucif, Saif Mohammad, Sebastian Ruder, Oumaima Hourrane, Alipio Jorge, Pavel Brazdil, Felermino Ali, Davis David, Salomey Osei, Bello Shehu-Bello, Falalu Lawan, Tajuddeen Gwadabe, Samuel Rutunda, Tadesse Belay, Wendimu Messelle, Hailu Balcha, Sisay Chala, Hagos Gebremichael, Bernard Opoku, and Stephen Arthur. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.emnlp-main.862" title="" class="ltx_ref ltx_href">AfriSenti: A Twitter sentiment analysis benchmark for African languages</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 13968–13981, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nekoto et al. (2020)</span>
<span class="ltx_bibblock">
Wilhelmina Nekoto, Vukosi Marivate, Tshinondiwa Matsila, Timi Fasubaa, Tajudeen Kolawole, Taiwo Fagbohungbe, Solomon Oluwole Akinola, Shamsuddee Hassan Muhammad, Salomon Kabongo, Salomey Osei, et al. 2020.

</span>
<span class="ltx_bibblock">Participatory research for low-resourced machine translation: A case study in African languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Findings of EMNLP</em>.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nerbonne et al. (2020)</span>
<span class="ltx_bibblock">
John Nerbonne, Wilbert Heeringa, Jelena Proki´c, and Martijn Wieling. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:220632878" title="" class="ltx_ref ltx_href">5 dialectology for computational linguists</a>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nerbonne et al. (1996)</span>
<span class="ltx_bibblock">
John Nerbonne, Wilbert Heeringa, Erik van den Hout, Peter van der Kooi, Simone Otten, and Willem van de Vis. 1996.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:18288373" title="" class="ltx_ref ltx_href">Phonetic distance between dutch dialects</a>.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nigmatulina et al. (2020)</span>
<span class="ltx_bibblock">
Iuliia Nigmatulina, Tannon Kew, and Tanja Samardzic. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2020.vardial-1.2" title="" class="ltx_ref ltx_href">ASR for non-standardised languages with dialectal variation: the case of Swiss German</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects</em>, pages 15–24, Barcelona, Spain (Online). International Committee on Computational Linguistics (ICCL).

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ogayo et al. (2022)</span>
<span class="ltx_bibblock">
Perez Ogayo, Graham Neubig, and Alan W Black. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2207.00688" title="" class="ltx_ref ltx_href">Building african voices</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">23rd Annual Conference of the International Speech Communication Association (InterSpeech 2022)</em>, Incheon, Korea.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ogundepo et al. (2023)</span>
<span class="ltx_bibblock">
Odunayo Ogundepo, Tajuddeen Gwadabe, Clara Rivera, Jonathan Clark, Sebastian Ruder, David Adelani, Bonaventure Dossou, Abdou Diop, Claytone Sikasote, Gilles Hacheme, Happy Buzaaba, Ignatius Ezeani, Rooweither Mabuya, Salomey Osei, Chris Emezue, Albert Kahira, Shamsuddeen Muhammad, Akintunde Oladipo, Abraham Owodunni, Atnafu Tonja, Iyanuoluwa Shode, Akari Asai, Anuoluwapo Aremu, Ayodele Awokoya, Bernard Opoku, Chiamaka Chukwuneke, Christine Mwase, Clemencia Siro, Stephen Arthur, Tunde Ajayi, Verrah Otiende, Andre Rubungo, Boyd Sinkala, Daniel Ajisafe, Emeka Onwuegbuzia, Falalu Lawan, Ibrahim Ahmad, Jesujoba Alabi, Chinedu Mbonu, Mofetoluwa Adeyemi, Mofya Phiri, Orevaoghene Ahia, Ruqayya Iro, and Sonia Adhiambo. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-emnlp.997" title="" class="ltx_ref ltx_href">Cross-lingual open-retrieval question answering for African languages</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 14957–14972, Singapore. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ogunremi et al. (2024)</span>
<span class="ltx_bibblock">
Tolulope Ogunremi, Kola Tubosun, Anuoluwapo Aremu, Iroro Orife, and David Ifeoluwa Adelani. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2307.16071" title="" class="ltx_ref ltx_href">Ìròyìnspeech: A multi-purpose yorùbá speech corpus</a>.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ojo (1977)</span>
<span class="ltx_bibblock">
Valentine Ojo. 1977.

</span>
<span class="ltx_bibblock">English-yoruba language contact in nigeria.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">(No Title)</em>.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olánrewájú (2022)</span>
<span class="ltx_bibblock">
Emmanuel Omóníyì Olánrewájú. 2022.

</span>
<span class="ltx_bibblock">A contrastive analysis of interrogatives in standard yorùbà and central yorùbà dialects.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Hayatian Journal of Linguistics and Literature</em>, 6(1):24–46.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olumuyiwa (2009)</span>
<span class="ltx_bibblock">
Temitope Olumuyiwa. 2009.

</span>
<span class="ltx_bibblock">The high tone syllable in central yoruba dialects.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Nordic Journal of African Studies</em>, 18(2):9–9.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olumuyiwa (2016)</span>
<span class="ltx_bibblock">
Temitope Olumuyiwa. 2016.

</span>
<span class="ltx_bibblock">Vowel assimilation in èkìtì dialects of yorùbá language.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Lingue e Linguaggi</em>, 17:143–154.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oneata and Kamper (2024)</span>
<span class="ltx_bibblock">
Dan Oneata and Herman Kamper. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2406.07133" title="" class="ltx_ref ltx_href">Translating speech with just images</a>.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oyelaran and Watson (1991)</span>
<span class="ltx_bibblock">
Olasope O Oyelaran and RL Watson. 1991.

</span>
<span class="ltx_bibblock">Africanisms in American culture.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oyelaran (1971)</span>
<span class="ltx_bibblock">
Olasope Oyediji Oyelaran. 1971.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Yoruba phonology</em>.

</span>
<span class="ltx_bibblock">Stanford University.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oyetade (1988)</span>
<span class="ltx_bibblock">
Benjamin A Oyetade. 1988.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Issues in the analysis of Yoruba tone</em>.

</span>
<span class="ltx_bibblock">University of London, School of Oriental and African Studies (United Kingdom).

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ozburn (2023)</span>
<span class="ltx_bibblock">
Avery Ozburn. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://languageprofiles.ca/" title="" class="ltx_ref ltx_href">Language profiles project</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-06-13.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3115/1073083.1073135" title="" class="ltx_ref ltx_href">Bleu: a method for automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pratap et al. (2024)</span>
<span class="ltx_bibblock">
Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, et al. 2024.

</span>
<span class="ltx_bibblock">Scaling speech technology to 1,000+ languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 25(97):1–52.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Przezdziecki (2005)</span>
<span class="ltx_bibblock">
Marek A Przezdziecki. 2005.

</span>
<span class="ltx_bibblock"><em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">Vowel harmony and coarticulation in three dialects of Yoruba: phonetics determining phonology</em>.

</span>
<span class="ltx_bibblock">Cornell University.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2022)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2212.04356" title="" class="ltx_ref ltx_href">Robust speech recognition via large-scale weak supervision</a>.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rowlands (1967)</span>
<span class="ltx_bibblock">
E. C. Rowlands. 1967.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1017/S0041977X00132586" title="" class="ltx_ref ltx_href">Ayo bamgbose: A grammar of Yoruba. (west African language monograph series, 5.) xii, 175 pp. cambridge: University press in association with the west african languages survey and the institute of african studies, ibadan, 1966. 35s.</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">Bulletin of the School of Oriental and African Studies</em>, 30(3):736–737.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shode et al. (2023)</span>
<span class="ltx_bibblock">
Iyanuoluwa Shode, David Ifeoluwa Adelani, JIng Peng, and Anna Feldman. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-short.85" title="" class="ltx_ref ltx_href">NollySenti: Leveraging transfer learning and machine translation for Nigerian movie sentiment classification</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, pages 986–998, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sikasote and Anastasopoulos (2022)</span>
<span class="ltx_bibblock">
Claytone Sikasote and Antonios Anastasopoulos. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.lrec-1.790" title="" class="ltx_ref ltx_href">BembaSpeech: A speech recognition corpus for the Bemba language</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Thirteenth Language Resources and Evaluation Conference</em>, pages 7277–7283, Marseille, France. European Language Resources Association.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siminyu and Freshia (2020)</span>
<span class="ltx_bibblock">
Kathleen Siminyu and Sackey Freshia. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.winlp-1.18" title="" class="ltx_ref ltx_href">AI4D - African language dataset challenge</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fourth Widening Natural Language Processing Workshop</em>, pages 68–77, Seattle, USA. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siminyu et al. (2021)</span>
<span class="ltx_bibblock">
Kathleen Siminyu, Xinjian Li, Antonios Anastasopoulos, David Mortensen, Michael R. Marlo, and Graham Neubig. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/pdf/2104.01624.pdf" title="" class="ltx_ref ltx_href">Phoneme recognition through fine tuning of phonetic representations: a case study on luhya language varieties</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">Proceedings of Interspeech 2021</em>.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siminyu et al. (2022)</span>
<span class="ltx_bibblock">
Kathleen Siminyu, Kibibi Mohamed Amran, Abdulrahman Ndegwa Karatu, Mnata Resani, Mwimbi Makobo Junior, Rebecca Ryakitimbo, and Britone Mwasaru. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.computel-1.3" title="" class="ltx_ref ltx_href">Corpus development of kiswahili speech recognition test and evaluation sets, preemptively mitigating demographic bias through collaboration with linguists</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fifth Workshop on the Use of Computational Methods in the Study of Endangered Languages</em>, pages 13–19, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al. (2022)</span>
<span class="ltx_bibblock">
NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2207.04672" title="" class="ltx_ref ltx_href">No language left behind: Scaling human-centered machine translation</a>.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van Niekerk et al. (2017)</span>
<span class="ltx_bibblock">
Daniel van Niekerk, Charl van Heerden, Marelie Davel, Neil Kleynhans, Oddur Kjartansson, Martin Jansche, and Linne Ha. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://dx.doi.org/10.21437/Interspeech.2017-1139" title="" class="ltx_ref ltx_href">Rapid development of tts corpora for four South African languages</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2017</em>, pages 2178–2182.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Volansky et al. (2015)</span>
<span class="ltx_bibblock">
Vered Volansky, Noam Ordan, and Shuly Wintner. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:5133943" title="" class="ltx_ref ltx_href">On the features of translationese</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">Digit. Scholarsh. Humanit.</em>, 30:98–118.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024)</span>
<span class="ltx_bibblock">
Jiayi Wang, David Adelani, Sweta Agrawal, Marek Masiak, Ricardo Rei, Eleftheria Briakou, Marine Carpuat, Xuanli He, Sofia Bourhim, Andiswa Bukula, Muhidin Mohamed, Temitayo Olatoye, Tosin Adewumi, Hamam Mokayed, Christine Mwase, Wangui Kimotho, Foutse Yuehgoh, Anuoluwapo Aremu, Jessica Ojo, Shamsuddeen Muhammad, Salomey Osei, Abdul-Hakeem Omotayo, Chiamaka Chukwuneke, Perez Ogayo, Oumaima Hourrane, Salma El Anigri, Lolwethu Ndolela, Thabiso Mangwana, Shafie Mohamed, Hassan Ayinde, Oluwabusayo Awoyomi, Lama Alkhaled, Sana Al-azzawi, Naome Etori, Millicent Ochieng, Clemencia Siro, Njoroge Kiragu, Eric Muchiri, Wangari Kimotho, Toadoum Sari Sakayo, Lyse Naomi Wamba, Daud Abolade, Simbiat Ajao, Iyanuoluwa Shode, Ricky Macharm, Ruqayya Iro, Saheed Abdullahi, Stephen Moore, Bernard Opoku, Zainab Akinjobi, Abeeb Afolabi, Nnaemeka Obiefuna, Onyekachi Ogbu, Sam Ochieng’, Verrah Otiende, Chinedu Mbonu, Yao Lu, and Pontus Stenetorp. 2024.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2024.naacl-long.334" title="" class="ltx_ref ltx_href">AfriMTE and AfriCOMET: Enhancing COMET to embrace under-resourced African languages</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>, pages 5997–6023, Mexico City, Mexico. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Jiayi Wang, David Ifeoluwa Adelani, Sweta Agrawal, Ricardo Rei, Eleftheria Briakou, Marine Carpuat, Marek Masiak, Xuanli He, Sofia Bourhim, Andiswa Bukula, et al. 2023.

</span>
<span class="ltx_bibblock">Afrimte and africomet: Empowering COMET to embrace under-resourced African languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.09828</em>.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wenzel and Kaufman (2024)</span>
<span class="ltx_bibblock">
Kimi Wenzel and Geoff Kaufman. 2024.

</span>
<span class="ltx_bibblock">Designing for harm reduction: Communication repair for multicultural users’ voice interactions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">Proceedings of the CHI Conference on Human Factors in Computing Systems</em>, pages 1–17.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al. (2021)</span>
<span class="ltx_bibblock">
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.naacl-main.41" title="" class="ltx_ref ltx_href">mT5: A massively multilingual pre-trained text-to-text transformer</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 483–498, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ziems et al. (2023)</span>
<span class="ltx_bibblock">
Caleb Ziems, William Held, Jingfeng Yang, Jwala Dhamala, Rahul Gupta, and Diyi Yang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-long.44" title="" class="ltx_ref ltx_href">Multi-VALUE: A framework for cross-dialectal English NLP</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 744–768, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Üstün et al. (2024)</span>
<span class="ltx_bibblock">
Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D’souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. 2024.

</span>
<span class="ltx_bibblock">Aya model: An instruction finetuned open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.07827</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Related Work</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">Previous works that have developing technologies and resources for machine translation <cite class="ltx_cite ltx_citemacro_cite">Ahia et al. (<a href="#bib.bib12" title="" class="ltx_ref">2021</a>); Adebara et al. (<a href="#bib.bib5" title="" class="ltx_ref">2022</a>, <a href="#bib.bib4" title="" class="ltx_ref">2021</a>); Lee et al. (<a href="#bib.bib45" title="" class="ltx_ref">2023</a>); Akinade et al. (<a href="#bib.bib15" title="" class="ltx_ref">2023</a>); Adelani et al. (<a href="#bib.bib6" title="" class="ltx_ref">2021a</a>)</cite>, automatic speech recognition <cite class="ltx_cite ltx_citemacro_cite">Ogunremi et al. (<a href="#bib.bib57" title="" class="ltx_ref">2024</a>); Communication et al. (<a href="#bib.bib28" title="" class="ltx_ref">2023</a>); Baevski et al. (<a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite> and speech translation <cite class="ltx_cite ltx_citemacro_cite">Communication et al. (<a href="#bib.bib28" title="" class="ltx_ref">2023</a>); Oneata and Kamper (<a href="#bib.bib62" title="" class="ltx_ref">2024</a>)</cite> for Yorùbá have largely focused on the standard Yorùbá dialect. This is because, just like other African languages, standard Yorùbá is also very low-resourced, and all efforts have been directed there. Several works have shown that models often exhibit performance disparities between standard languages and their dialectal counterparts <cite class="ltx_cite ltx_citemacro_cite">Diab (<a href="#bib.bib31" title="" class="ltx_ref">2016</a>); Nigmatulina et al. (<a href="#bib.bib54" title="" class="ltx_ref">2020</a>); Kantharuban et al. (<a href="#bib.bib43" title="" class="ltx_ref">2023</a>); Ziems et al. (<a href="#bib.bib84" title="" class="ltx_ref">2023</a>); Faisal et al. (<a href="#bib.bib36" title="" class="ltx_ref">2024</a>); Ahmadi et al. (<a href="#bib.bib13" title="" class="ltx_ref">2024</a>); Joshi et al. (<a href="#bib.bib42" title="" class="ltx_ref">2024</a>); Blaschke et al. (<a href="#bib.bib24" title="" class="ltx_ref">2023</a>); Aji et al. (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>); Abdul-Mageed et al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>. Arabic language has roughly 30 regional dialects. Whilst majority of work has being done on Modern Standard Arabic (MSA), Arabic still has the widest coverage of tasks and datasets across several of its dialects <cite class="ltx_cite ltx_citemacro_cite">Faisal et al. (<a href="#bib.bib36" title="" class="ltx_ref">2024</a>); Diab and Habash (<a href="#bib.bib32" title="" class="ltx_ref">2012</a>); Bouamor et al. (<a href="#bib.bib25" title="" class="ltx_ref">2018</a>); Kchaou et al. (<a href="#bib.bib44" title="" class="ltx_ref">2020</a>)</cite>. Within African languages, some works that aim to build dialect-aware models have conducted their studies on Igbo <cite class="ltx_cite ltx_citemacro_cite">Emezue et al. (<a href="#bib.bib35" title="" class="ltx_ref">2024</a>)</cite>, Luhya <cite class="ltx_cite ltx_citemacro_cite">Siminyu et al. (<a href="#bib.bib75" title="" class="ltx_ref">2021</a>); Chimoto and Bassett (<a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>, Bemba <cite class="ltx_cite ltx_citemacro_cite">Sikasote and Anastasopoulos (<a href="#bib.bib73" title="" class="ltx_ref">2022</a>)</cite> and Kiswahili <cite class="ltx_cite ltx_citemacro_cite">Siminyu et al. (<a href="#bib.bib76" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Finetuning setup</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">For the MT, we fine-tuned in both directions with a learning-rate of 2e-5 and batch size of 16. We trained for four epochs, and kept the model with the best eval loss. We used a weight decay of 0.01, warmup ratio 0.1, and a cosine annealing scheduler for learning rate. While for ASR finetuning, we fine-tuned with a learning-rate of 1e-3 and batch size of 8 for 20 epochs, as the validation WER continued to drop after preliminary runs with 10 epochs. For S2TT, we fine-tuned for 10 epochs with an optimal learning rate of 3e-4. All training was done on two NVIDIA A40 GPUs.</p>
</div>
<figure id="A1.T8" class="ltx_table">
<table id="A1.T8.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T8.1.1.1" class="ltx_tr">
<th id="A1.T8.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Model</th>
<th id="A1.T8.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Standard</th>
<th id="A1.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Ife</th>
<th id="A1.T8.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Ilaje</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T8.1.2.1" class="ltx_tr">
<td id="A1.T8.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">Zero-Shot</td>
<td id="A1.T8.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">72.50</td>
<td id="A1.T8.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">85.38</td>
<td id="A1.T8.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">83.79</td>
</tr>
<tr id="A1.T8.1.3.2" class="ltx_tr">
<td id="A1.T8.1.3.2.1" class="ltx_td ltx_align_center">MMS-300m-Individual</td>
<td id="A1.T8.1.3.2.2" class="ltx_td ltx_align_center">74.67</td>
<td id="A1.T8.1.3.2.3" class="ltx_td ltx_align_center">93.20</td>
<td id="A1.T8.1.3.2.4" class="ltx_td ltx_align_center">78.24</td>
</tr>
<tr id="A1.T8.1.4.3" class="ltx_tr">
<td id="A1.T8.1.4.3.1" class="ltx_td ltx_align_center">MMS-1.3bn-Individual</td>
<td id="A1.T8.1.4.3.2" class="ltx_td ltx_align_center">55.43</td>
<td id="A1.T8.1.4.3.3" class="ltx_td ltx_align_center">72.00</td>
<td id="A1.T8.1.4.3.4" class="ltx_td ltx_align_center">61.80</td>
</tr>
<tr id="A1.T8.1.5.4" class="ltx_tr">
<td id="A1.T8.1.5.4.1" class="ltx_td ltx_align_center">XLSR-300m-Individual</td>
<td id="A1.T8.1.5.4.2" class="ltx_td ltx_align_center">56.26</td>
<td id="A1.T8.1.5.4.3" class="ltx_td ltx_align_center">81.23</td>
<td id="A1.T8.1.5.4.4" class="ltx_td ltx_align_center">64.22</td>
</tr>
<tr id="A1.T8.1.6.5" class="ltx_tr">
<td id="A1.T8.1.6.5.1" class="ltx_td ltx_align_center">XLSR-1.3bn-Individual</td>
<td id="A1.T8.1.6.5.2" class="ltx_td ltx_align_center">67.65</td>
<td id="A1.T8.1.6.5.3" class="ltx_td ltx_align_center">78.70</td>
<td id="A1.T8.1.6.5.4" class="ltx_td ltx_align_center">76.36</td>
</tr>
<tr id="A1.T8.1.7.6" class="ltx_tr">
<td id="A1.T8.1.7.6.1" class="ltx_td ltx_align_center">MMS-300m-Joint</td>
<td id="A1.T8.1.7.6.2" class="ltx_td ltx_align_center">58.11</td>
<td id="A1.T8.1.7.6.3" class="ltx_td ltx_align_center">76.58</td>
<td id="A1.T8.1.7.6.4" class="ltx_td ltx_align_center">67.17</td>
</tr>
<tr id="A1.T8.1.8.7" class="ltx_tr">
<td id="A1.T8.1.8.7.1" class="ltx_td ltx_align_center">MMS-1.3bn-Joint</td>
<td id="A1.T8.1.8.7.2" class="ltx_td ltx_align_center">55.73</td>
<td id="A1.T8.1.8.7.3" class="ltx_td ltx_align_center">73.95</td>
<td id="A1.T8.1.8.7.4" class="ltx_td ltx_align_center">63.94</td>
</tr>
<tr id="A1.T8.1.9.8" class="ltx_tr">
<td id="A1.T8.1.9.8.1" class="ltx_td ltx_align_center">XLSR-300m-Joint</td>
<td id="A1.T8.1.9.8.2" class="ltx_td ltx_align_center">54.55</td>
<td id="A1.T8.1.9.8.3" class="ltx_td ltx_align_center">73.72</td>
<td id="A1.T8.1.9.8.4" class="ltx_td ltx_align_center">61.03</td>
</tr>
<tr id="A1.T8.1.10.9" class="ltx_tr">
<td id="A1.T8.1.10.9.1" class="ltx_td ltx_align_center ltx_border_bb">XLSR-1.3bn-Joint</td>
<td id="A1.T8.1.10.9.2" class="ltx_td ltx_align_center ltx_border_bb">81.57</td>
<td id="A1.T8.1.10.9.3" class="ltx_td ltx_align_center ltx_border_bb">90.04</td>
<td id="A1.T8.1.10.9.4" class="ltx_td ltx_align_center ltx_border_bb">86.30</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>ASR Performance of across all models after fine-tuning individually and jointly</figcaption>
</figure>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Results from Joint vs Individual MT fine-tuning</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p id="A1.SS3.p1.1" class="ltx_p">We present tables comparing jointly fine-tuning to individual fine-tuning on MT across the two training directions in <a href="#A1.T12" title="Table 12 ‣ A.4 Human evaluation ‣ Appendix A Appendix ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 12</span></a> and <a href="#A1.T11" title="Table 11 ‣ A.4 Human evaluation ‣ Appendix A Appendix ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 11</span></a>.</p>
</div>
</section>
<section id="A1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Human evaluation</h3>

<div id="A1.SS4.p1" class="ltx_para">
<p id="A1.SS4.p1.1" class="ltx_p">We provide exact instructions given to human evaluators for our ASR and MT tasks in <a href="#S6.T5" title="Table 5 ‣ 6 Human Evaluation ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 5</span></a> and <a href="#S6.T6" title="Table 6 ‣ 6 Human Evaluation ‣ Voices Unheard: NLP Resources and Models for Yorùbá Regional Dialects" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 6</span></a></p>
</div>
<figure id="A1.T9" class="ltx_table">
<table id="A1.T9.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T9.1.1.1" class="ltx_tr">
<th id="A1.T9.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="A1.T9.1.1.1.1.1" class="ltx_text">
<span id="A1.T9.1.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:0.0pt;height:256pt;vertical-align:-128.0pt;"><span class="ltx_transformed_inner" style="width:256.1pt;transform:translate(-128.04pt,0pt) rotate(-90deg) ;">
</span></span></span></th>
<td id="A1.T9.1.1.1.2" class="ltx_td ltx_align_justify ltx_border_tt">
<span id="A1.T9.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.1.1.1.2.1.1" class="ltx_p">You are tasked to evaluate the performance of an Automatic Speech Recognition (ASR) system on your native Yoruba dialect. This task involves assessing the accuracy and quality of transcriptions produced by this system when transcribing audio from a folder that will be provided to you. Your evaluations will help us understand how well these systems handle linguistic variations. Each filename has a corresponding audio file with the same name in the audio folder. Listen to the audio first, then look at the transcription from the model. Next, evaluate the quality of the transcription compared to the audio you listened to and provide a score in the Excel sheet.</span>
</span>
</td>
</tr>
<tr id="A1.T9.1.2.2" class="ltx_tr">
<th id="A1.T9.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="A1.T9.1.2.2.1.1" class="ltx_text ltx_font_bold">Fluency</span></th>
<td id="A1.T9.1.2.2.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="A1.T9.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.1.2.2.2.1.1" class="ltx_p">Evaluate how natural and grammatically correct the transcription sounds in your dialect.</span>
</span>
</td>
</tr>
<tr id="A1.T9.1.3.3" class="ltx_tr">
<th id="A1.T9.1.3.3.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T9.1.3.3.2" class="ltx_td ltx_align_justify">
<span id="A1.T9.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.1.3.3.2.1.1" class="ltx_p">1. <span id="A1.T9.1.3.3.2.1.1.1" class="ltx_text ltx_font_bold">Incomprehensible</span>: The transcription is completely unintelligible and nonsensical. The text is difficult to understand.</span>
</span>
</td>
</tr>
<tr id="A1.T9.1.4.4" class="ltx_tr">
<th id="A1.T9.1.4.4.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T9.1.4.4.2" class="ltx_td ltx_align_justify">
<span id="A1.T9.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.1.4.4.2.1.1" class="ltx_p">2. <span id="A1.T9.1.4.4.2.1.1.1" class="ltx_text ltx_font_bold">Poor grammar and disfluent</span>: The transcription contains significant errors in grammar, syntax, and vocabulary that affect the clarity and naturalness of the text.</span>
</span>
</td>
</tr>
<tr id="A1.T9.1.5.5" class="ltx_tr">
<th id="A1.T9.1.5.5.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T9.1.5.5.2" class="ltx_td ltx_align_justify">
<span id="A1.T9.1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.1.5.5.2.1.1" class="ltx_p">3. <span id="A1.T9.1.5.5.2.1.1.1" class="ltx_text ltx_font_bold">Grammatically correct, potentially unnatural</span>: The transcription is grammatically correct but may have some errors in spelling, word choice, or syntax.</span>
</span>
</td>
</tr>
<tr id="A1.T9.1.6.6" class="ltx_tr">
<th id="A1.T9.1.6.6.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T9.1.6.6.2" class="ltx_td ltx_align_justify">
<span id="A1.T9.1.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.1.6.6.2.1.1" class="ltx_p">4. <span id="A1.T9.1.6.6.2.1.1.1" class="ltx_text ltx_font_bold">Fluent and natural</span>: The transcription contains no grammatical errors, and the text is somewhat easy to read and understand.</span>
</span>
</td>
</tr>
<tr id="A1.T9.1.7.7" class="ltx_tr">
<th id="A1.T9.1.7.7.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T9.1.7.7.2" class="ltx_td ltx_align_justify">
<span id="A1.T9.1.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.1.7.7.2.1.1" class="ltx_p">5. <span id="A1.T9.1.7.7.2.1.1.1" class="ltx_text ltx_font_bold">Perfectly fluent and natural</span>: The transcription is completely natural, grammatically flawless, reading as if written by a native speaker.</span>
</span>
</td>
</tr>
<tr id="A1.T9.1.8.8" class="ltx_tr">
<th id="A1.T9.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="A1.T9.1.8.8.1.1" class="ltx_text ltx_font_bold">Adequacy</span></th>
<td id="A1.T9.1.8.8.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="A1.T9.1.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.1.8.8.2.1.1" class="ltx_p">Assess how accurately the transcription conveys the meaning of the source speech.</span>
</span>
</td>
</tr>
<tr id="A1.T9.1.9.9" class="ltx_tr">
<th id="A1.T9.1.9.9.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T9.1.9.9.2" class="ltx_td ltx_align_justify">
<span id="A1.T9.1.9.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.1.9.9.2.1.1" class="ltx_p">1. <span id="A1.T9.1.9.9.2.1.1.1" class="ltx_text ltx_font_bold">Nonsense/No meaning preserved</span>: All information is lost between the transcription and the source.</span>
</span>
</td>
</tr>
<tr id="A1.T9.1.10.10" class="ltx_tr">
<th id="A1.T9.1.10.10.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T9.1.10.10.2" class="ltx_td ltx_align_justify">
<span id="A1.T9.1.10.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.1.10.10.2.1.1" class="ltx_p">2. <span id="A1.T9.1.10.10.2.1.1.1" class="ltx_text ltx_font_bold">Very poor meaning preservation</span>: The transcription preserves little meaning from the source.</span>
</span>
</td>
</tr>
<tr id="A1.T9.1.11.11" class="ltx_tr">
<th id="A1.T9.1.11.11.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T9.1.11.11.2" class="ltx_td ltx_align_justify">
<span id="A1.T9.1.11.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.1.11.11.2.1.1" class="ltx_p">3. <span id="A1.T9.1.11.11.2.1.1.1" class="ltx_text ltx_font_bold">Moderate meaning preservation</span>: The transcription retains some meaning but still misses important details.</span>
</span>
</td>
</tr>
<tr id="A1.T9.1.12.12" class="ltx_tr">
<th id="A1.T9.1.12.12.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T9.1.12.12.2" class="ltx_td ltx_align_justify">
<span id="A1.T9.1.12.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.1.12.12.2.1.1" class="ltx_p">4. <span id="A1.T9.1.12.12.2.1.1.1" class="ltx_text ltx_font_bold">Good meaning preservation</span>: The transcription retains most of the meaning of the source.</span>
</span>
</td>
</tr>
<tr id="A1.T9.1.13.13" class="ltx_tr">
<th id="A1.T9.1.13.13.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb"></th>
<td id="A1.T9.1.13.13.2" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="A1.T9.1.13.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T9.1.13.13.2.1.1" class="ltx_p">5. <span id="A1.T9.1.13.13.2.1.1.1" class="ltx_text ltx_font_bold">Perfect meaning preservation</span>: The meaning of the transcription is completely consistent with the source.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Automatic Speech Recognition (ASR) human evaluation guidelines</figcaption>
</figure>
<figure id="A1.T10" class="ltx_table">
<table id="A1.T10.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T10.1.1.1" class="ltx_tr">
<th id="A1.T10.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A1.T10.1.1.1.1.1" class="ltx_text">
<span id="A1.T10.1.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:0.0pt;height:199.2pt;vertical-align:-99.6pt;"><span class="ltx_transformed_inner" style="width:199.2pt;transform:translate(-99.58pt,0pt) rotate(-90deg) ;">
</span></span></span></th>
<th id="A1.T10.1.1.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span id="A1.T10.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T10.1.1.1.2.1.1" class="ltx_p">You are tasked to evaluate the performance of two Machine Translation systems on your native Yoruba dialect. This task involves assessing the accuracy and quality of translations produced by these systems, when translating from your dialect into English. Your evaluations will help us understand how well these systems handle linguistic variations.</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T10.1.2.1" class="ltx_tr">
<th id="A1.T10.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="A1.T10.1.2.1.1.1" class="ltx_text ltx_font_bold">Fluency</span></th>
<td id="A1.T10.1.2.1.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="A1.T10.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T10.1.2.1.2.1.1" class="ltx_p">Evaluate how natural and grammatically correct the translation sounds in the target language.</span>
</span>
</td>
</tr>
<tr id="A1.T10.1.3.2" class="ltx_tr">
<th id="A1.T10.1.3.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T10.1.3.2.2" class="ltx_td ltx_align_justify">
<span id="A1.T10.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T10.1.3.2.2.1.1" class="ltx_p">1. <span id="A1.T10.1.3.2.2.1.1.1" class="ltx_text ltx_font_bold">Incomprehensible</span>: The translation is completely unintelligible and nonsensical. The text is difficult to understand.</span>
</span>
</td>
</tr>
<tr id="A1.T10.1.4.3" class="ltx_tr">
<th id="A1.T10.1.4.3.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T10.1.4.3.2" class="ltx_td ltx_align_justify">
<span id="A1.T10.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T10.1.4.3.2.1.1" class="ltx_p">2. <span id="A1.T10.1.4.3.2.1.1.1" class="ltx_text ltx_font_bold">Poor grammar and disfluent</span>: The translation contains significant errors in grammar, syntax, and vocabulary that affect the clarity and naturalness of the text.</span>
</span>
</td>
</tr>
<tr id="A1.T10.1.5.4" class="ltx_tr">
<th id="A1.T10.1.5.4.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T10.1.5.4.2" class="ltx_td ltx_align_justify">
<span id="A1.T10.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T10.1.5.4.2.1.1" class="ltx_p">3. <span id="A1.T10.1.5.4.2.1.1.1" class="ltx_text ltx_font_bold">Mostly grammatically correct, potentially unnatural</span>: The translation has few grammatical errors and also has some errors in spellings, word choice, or syntax. The language may not be natural.</span>
</span>
</td>
</tr>
<tr id="A1.T10.1.6.5" class="ltx_tr">
<th id="A1.T10.1.6.5.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T10.1.6.5.2" class="ltx_td ltx_align_justify">
<span id="A1.T10.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T10.1.6.5.2.1.1" class="ltx_p">4. <span id="A1.T10.1.6.5.2.1.1.1" class="ltx_text ltx_font_bold">Grammatically correct and natural</span>: The translation contains few grammatical errors, the vocabulary is precise, and the text is easy to read and understand.</span>
</span>
</td>
</tr>
<tr id="A1.T10.1.7.6" class="ltx_tr">
<th id="A1.T10.1.7.6.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T10.1.7.6.2" class="ltx_td ltx_align_justify">
<span id="A1.T10.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T10.1.7.6.2.1.1" class="ltx_p">5. <span id="A1.T10.1.7.6.2.1.1.1" class="ltx_text ltx_font_bold">Perfectly fluent and natural</span>: The translation is completely fluent, sounds natural and is grammatically correct.</span>
</span>
</td>
</tr>
<tr id="A1.T10.1.8.7" class="ltx_tr">
<th id="A1.T10.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="A1.T10.1.8.7.1.1" class="ltx_text ltx_font_bold">Adequacy</span></th>
<td id="A1.T10.1.8.7.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="A1.T10.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T10.1.8.7.2.1.1" class="ltx_p">Assess how accurately the translation conveys the meaning of the source speech.</span>
</span>
</td>
</tr>
<tr id="A1.T10.1.9.8" class="ltx_tr">
<th id="A1.T10.1.9.8.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T10.1.9.8.2" class="ltx_td ltx_align_justify">
<span id="A1.T10.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T10.1.9.8.2.1.1" class="ltx_p">1. <span id="A1.T10.1.9.8.2.1.1.1" class="ltx_text ltx_font_bold">Nonsense/No meaning preserved</span>: All information is lost between the translation and the source.</span>
</span>
</td>
</tr>
<tr id="A1.T10.1.10.9" class="ltx_tr">
<th id="A1.T10.1.10.9.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T10.1.10.9.2" class="ltx_td ltx_align_justify">
<span id="A1.T10.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T10.1.10.9.2.1.1" class="ltx_p">2. <span id="A1.T10.1.10.9.2.1.1.1" class="ltx_text ltx_font_bold">Very poor meaning preservation</span>: The translation preserves little meaning from the source.</span>
</span>
</td>
</tr>
<tr id="A1.T10.1.11.10" class="ltx_tr">
<th id="A1.T10.1.11.10.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T10.1.11.10.2" class="ltx_td ltx_align_justify">
<span id="A1.T10.1.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T10.1.11.10.2.1.1" class="ltx_p">3. <span id="A1.T10.1.11.10.2.1.1.1" class="ltx_text ltx_font_bold">Moderate meaning preservation</span>: The translation retains some meaning but still misses important details.</span>
</span>
</td>
</tr>
<tr id="A1.T10.1.12.11" class="ltx_tr">
<th id="A1.T10.1.12.11.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="A1.T10.1.12.11.2" class="ltx_td ltx_align_justify">
<span id="A1.T10.1.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T10.1.12.11.2.1.1" class="ltx_p">4. <span id="A1.T10.1.12.11.2.1.1.1" class="ltx_text ltx_font_bold">Good meaning preservation</span>: The translation retains most of the meaning of the source.</span>
</span>
</td>
</tr>
<tr id="A1.T10.1.13.12" class="ltx_tr">
<th id="A1.T10.1.13.12.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb"></th>
<td id="A1.T10.1.13.12.2" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="A1.T10.1.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T10.1.13.12.2.1.1" class="ltx_p">5. <span id="A1.T10.1.13.12.2.1.1.1" class="ltx_text ltx_font_bold">Perfect meaning preservation</span>: The meaning of the translation is completely consistent with the source.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Machine Translation (MT) Human evaluation guidelines</figcaption>
</figure>
<figure id="A1.T11" class="ltx_table">
<div id="A1.T11.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:84.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-15.0pt,2.9pt) scale(0.935308456037963,0.935308456037963) ;">
<table id="A1.T11.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T11.2.2.2" class="ltx_tr">
<th id="A1.T11.2.2.2.3" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="A1.T11.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">
<span id="A1.T11.1.1.1.1.1" class="ltx_text ltx_font_bold">BLEU</span> <math id="A1.T11.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="A1.T11.1.1.1.1.m1.1a"><mo stretchy="false" id="A1.T11.1.1.1.1.m1.1.1" xref="A1.T11.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T11.1.1.1.1.m1.1b"><ci id="A1.T11.1.1.1.1.m1.1.1.cmml" xref="A1.T11.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T11.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="A1.T11.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">
<span id="A1.T11.2.2.2.2.1" class="ltx_text ltx_font_bold">AfriCOMET</span> <math id="A1.T11.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="A1.T11.2.2.2.2.m1.1a"><mo stretchy="false" id="A1.T11.2.2.2.2.m1.1.1" xref="A1.T11.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T11.2.2.2.2.m1.1b"><ci id="A1.T11.2.2.2.2.m1.1.1.cmml" xref="A1.T11.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T11.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
<tr id="A1.T11.2.2.3.1" class="ltx_tr">
<th id="A1.T11.2.2.3.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="A1.T11.2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Ìjẹ̀bú</th>
<th id="A1.T11.2.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Ifẹ̀</th>
<th id="A1.T11.2.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Ìlàjẹ</th>
<th id="A1.T11.2.2.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Standard</th>
<th id="A1.T11.2.2.3.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Ìjẹ̀bú</th>
<th id="A1.T11.2.2.3.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Ifẹ̀</th>
<th id="A1.T11.2.2.3.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Ìlàjẹ</th>
<th id="A1.T11.2.2.3.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Standard</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T11.2.2.4.1" class="ltx_tr">
<th id="A1.T11.2.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="A1.T11.2.2.4.1.1.1" class="ltx_text ltx_font_bold">Individual</span></th>
<td id="A1.T11.2.2.4.1.2" class="ltx_td ltx_align_center ltx_border_t">16.53</td>
<td id="A1.T11.2.2.4.1.3" class="ltx_td ltx_align_center ltx_border_t">16.04</td>
<td id="A1.T11.2.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t">12.98</td>
<td id="A1.T11.2.2.4.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">30.27</td>
<td id="A1.T11.2.2.4.1.6" class="ltx_td ltx_align_center ltx_border_t">0.57</td>
<td id="A1.T11.2.2.4.1.7" class="ltx_td ltx_align_center ltx_border_t">0.56</td>
<td id="A1.T11.2.2.4.1.8" class="ltx_td ltx_align_center ltx_border_t">0.52</td>
<td id="A1.T11.2.2.4.1.9" class="ltx_td ltx_align_center ltx_border_t">0.69</td>
</tr>
<tr id="A1.T11.2.2.5.2" class="ltx_tr">
<th id="A1.T11.2.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="A1.T11.2.2.5.2.1.1" class="ltx_text ltx_font_bold">Joint</span></th>
<td id="A1.T11.2.2.5.2.2" class="ltx_td ltx_align_center">21.98</td>
<td id="A1.T11.2.2.5.2.3" class="ltx_td ltx_align_center">22.97</td>
<td id="A1.T11.2.2.5.2.4" class="ltx_td ltx_align_center">18.64</td>
<td id="A1.T11.2.2.5.2.5" class="ltx_td ltx_align_center ltx_border_r">37.55</td>
<td id="A1.T11.2.2.5.2.6" class="ltx_td ltx_align_center">0.60</td>
<td id="A1.T11.2.2.5.2.7" class="ltx_td ltx_align_center">0.59</td>
<td id="A1.T11.2.2.5.2.8" class="ltx_td ltx_align_center">0.55</td>
<td id="A1.T11.2.2.5.2.9" class="ltx_td ltx_align_center">0.71</td>
</tr>
<tr id="A1.T11.2.2.6.3" class="ltx_tr">
<th id="A1.T11.2.2.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="A1.T11.2.2.6.3.1.1" class="ltx_text ltx_font_bold">Joint + MENYO-20k</span></th>
<td id="A1.T11.2.2.6.3.2" class="ltx_td ltx_align_center ltx_border_bb">19.80</td>
<td id="A1.T11.2.2.6.3.3" class="ltx_td ltx_align_center ltx_border_bb">20.77</td>
<td id="A1.T11.2.2.6.3.4" class="ltx_td ltx_align_center ltx_border_bb">17.21</td>
<td id="A1.T11.2.2.6.3.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">31.75</td>
<td id="A1.T11.2.2.6.3.6" class="ltx_td ltx_align_center ltx_border_bb">0.54</td>
<td id="A1.T11.2.2.6.3.7" class="ltx_td ltx_align_center ltx_border_bb">0.59</td>
<td id="A1.T11.2.2.6.3.8" class="ltx_td ltx_align_center ltx_border_bb">0.60</td>
<td id="A1.T11.2.2.6.3.9" class="ltx_td ltx_align_center ltx_border_bb">0.71</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>MT Finetuning Evaluation using NLLB-600M in the Yorùbá to English direction, training the dialects as individual languages, jointly under Yorùbá, and jointly along with MENYO-20k data.</figcaption>
</figure>
<figure id="A1.T12" class="ltx_table">
<div id="A1.T12.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:84.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-14.4pt,2.8pt) scale(0.937555539858727,0.937555539858727) ;">
<table id="A1.T12.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T12.2.2.2" class="ltx_tr">
<th id="A1.T12.2.2.2.3" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="A1.T12.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">
<span id="A1.T12.1.1.1.1.1" class="ltx_text ltx_font_bold">BLEU</span> <math id="A1.T12.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="A1.T12.1.1.1.1.m1.1a"><mo stretchy="false" id="A1.T12.1.1.1.1.m1.1.1" xref="A1.T12.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T12.1.1.1.1.m1.1b"><ci id="A1.T12.1.1.1.1.m1.1.1.cmml" xref="A1.T12.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T12.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="A1.T12.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">
<span id="A1.T12.2.2.2.2.1" class="ltx_text ltx_font_bold">AfriCOMET</span> <math id="A1.T12.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="A1.T12.2.2.2.2.m1.1a"><mo stretchy="false" id="A1.T12.2.2.2.2.m1.1.1" xref="A1.T12.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T12.2.2.2.2.m1.1b"><ci id="A1.T12.2.2.2.2.m1.1.1.cmml" xref="A1.T12.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T12.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
<tr id="A1.T12.2.2.3.1" class="ltx_tr">
<th id="A1.T12.2.2.3.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="A1.T12.2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Ìjẹ̀bú</th>
<th id="A1.T12.2.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Ifẹ̀</th>
<th id="A1.T12.2.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Ìlàjẹ</th>
<th id="A1.T12.2.2.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Standard</th>
<th id="A1.T12.2.2.3.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Ìjẹ̀bú</th>
<th id="A1.T12.2.2.3.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Ifẹ̀</th>
<th id="A1.T12.2.2.3.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Ìlàjẹ</th>
<th id="A1.T12.2.2.3.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Standard</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T12.2.2.4.1" class="ltx_tr">
<th id="A1.T12.2.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="A1.T12.2.2.4.1.1.1" class="ltx_text ltx_font_bold">Individual</span></th>
<td id="A1.T12.2.2.4.1.2" class="ltx_td ltx_align_center ltx_border_t">8.48</td>
<td id="A1.T12.2.2.4.1.3" class="ltx_td ltx_align_center ltx_border_t">8.74</td>
<td id="A1.T12.2.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t">5.78</td>
<td id="A1.T12.2.2.4.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">18.32</td>
<td id="A1.T12.2.2.4.1.6" class="ltx_td ltx_align_center ltx_border_t">0.52</td>
<td id="A1.T12.2.2.4.1.7" class="ltx_td ltx_align_center ltx_border_t">0.50</td>
<td id="A1.T12.2.2.4.1.8" class="ltx_td ltx_align_center ltx_border_t">0.47</td>
<td id="A1.T12.2.2.4.1.9" class="ltx_td ltx_align_center ltx_border_t">0.66</td>
</tr>
<tr id="A1.T12.2.2.5.2" class="ltx_tr">
<th id="A1.T12.2.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="A1.T12.2.2.5.2.1.1" class="ltx_text ltx_font_bold">Joint</span></th>
<td id="A1.T12.2.2.5.2.2" class="ltx_td ltx_align_center">8.71</td>
<td id="A1.T12.2.2.5.2.3" class="ltx_td ltx_align_center">8.93</td>
<td id="A1.T12.2.2.5.2.4" class="ltx_td ltx_align_center">6.48</td>
<td id="A1.T12.2.2.5.2.5" class="ltx_td ltx_align_center ltx_border_r">18.98</td>
<td id="A1.T12.2.2.5.2.6" class="ltx_td ltx_align_center">0.52</td>
<td id="A1.T12.2.2.5.2.7" class="ltx_td ltx_align_center">0.50</td>
<td id="A1.T12.2.2.5.2.8" class="ltx_td ltx_align_center">0.47</td>
<td id="A1.T12.2.2.5.2.9" class="ltx_td ltx_align_center">0.66</td>
</tr>
<tr id="A1.T12.2.2.6.3" class="ltx_tr">
<th id="A1.T12.2.2.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="A1.T12.2.2.6.3.1.1" class="ltx_text ltx_font_bold">Joint + MENYO-20k</span></th>
<td id="A1.T12.2.2.6.3.2" class="ltx_td ltx_align_center ltx_border_bb">7.23</td>
<td id="A1.T12.2.2.6.3.3" class="ltx_td ltx_align_center ltx_border_bb">7.25</td>
<td id="A1.T12.2.2.6.3.4" class="ltx_td ltx_align_center ltx_border_bb">5.29</td>
<td id="A1.T12.2.2.6.3.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">17.24</td>
<td id="A1.T12.2.2.6.3.6" class="ltx_td ltx_align_center ltx_border_bb">0.50</td>
<td id="A1.T12.2.2.6.3.7" class="ltx_td ltx_align_center ltx_border_bb">0.48</td>
<td id="A1.T12.2.2.6.3.8" class="ltx_td ltx_align_center ltx_border_bb">0.44</td>
<td id="A1.T12.2.2.6.3.9" class="ltx_td ltx_align_center ltx_border_bb">0.65</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span>MT Finetuning Evaluation using NLLB-600M in the English to Yorùbá direction.</figcaption>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.19563" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.19564" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.19564">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.19564" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.19565" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 23:40:10 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
