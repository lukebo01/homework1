<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.06484] Cross-Lingual Conversational Speech Summarization with Large Language Models</title><meta property="og:description" content="Cross-lingual conversational speech summarization is an important problem, but suffers from a dearth of resources.
While transcriptions exist for a number of languages, translated conversational speech is rare and data…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Cross-Lingual Conversational Speech Summarization with Large Language Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Cross-Lingual Conversational Speech Summarization with Large Language Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.06484">

<!--Generated on Thu Sep  5 15:15:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.3" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.4" class="ltx_ERROR undefined">\name</span>
<p id="p1.2" class="ltx_p">MaxNelson<sup id="p1.2.1" class="ltx_sup">∗</sup>
<span id="p1.2.2" class="ltx_ERROR undefined">\name</span>ShannonWotherspoon<sup id="p1.2.3" class="ltx_sup">∗</sup>
<span id="p1.2.4" class="ltx_ERROR undefined">\name</span>FrancisKeith
<span id="p1.2.5" class="ltx_ERROR undefined">\name</span>WilliamHartmann
<span id="p1.2.6" class="ltx_ERROR undefined">\name</span>MatthewSnover




</p>
</div>
<h1 class="ltx_title ltx_title_document">Cross-Lingual Conversational Speech Summarization with Large Language Models</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Cross-lingual conversational speech summarization is an important problem, but suffers from a dearth of resources.
While transcriptions exist for a number of languages, translated conversational speech is rare and datasets containing summaries are non-existent.
We build upon the existing Fisher and Callhome Spanish-English Speech Translation corpus by supplementing the translations with summaries.
The summaries are generated using GPT-4 from the reference translations and are treated as ground truth.
The task is to generate similar summaries in the presence of transcription and translation errors.
We build a baseline cascade-based system using open-source speech recognition and machine translation models.
We test a range of LLMs for summarization and analyze the impact of transcription and translation errors.
Adapting the Mistral-7B model for this task performs significantly better than off-the-shelf models and matches the performance of GPT-4.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>ASR, machine translation, summarization
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_note_type">footnotetext: </span>Equal Contribution</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Despite the advances in automatic speech recognition (ASR) since the advent of deep neural networks, conversational speech remains a significant challenge.
Due to the lack of data and challenging recording conditions, word error rates (WER) remain high.
Even when accurately transcribed, conversational speech is difficult to read; making it beneficial to build models of cross-lingual summarization that can generate more human-readable versions of conversations.
Faithful summarization captures the important information in the conversation without the distractions of hesitations and other speech disfluencies.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recent advances in large language models (LLMs) have allowed them to match or even surpass the capabilities of special purpose models for a number of tasks, including summarization.
For some datasets, LLMs have been found to not only surpass previous summarization models, but to meet the performance of humans <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
LLMs have the additional benefit of flexibility.
With prompting and finetuning, the model can be made to take advantage of additional information, or to provide a contextual summary based on additional instructions.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Summarization of speech has a long history <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
The majority of the work has focused on single-speaker audio with a clear goal (e.g., voicemail <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, broadcast news <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>).
Two-party conversations have also been a domain of interest.
Meeting summarization was explored in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> released a large corpus of interviews where a host interviews a guest.
Another major area is the summarization of call center interactions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
Given the audio from an interaction between a customer and a call center employee, the goal is to describe the nature of the call and whether and how a request was fulfilled.
To our knowledge, there is no prior work on the cross-lingual summarization of conversational speech outside of the call-center domain.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The classic approach to cross-lingual speech summarization has been a cascaded pipeline where audio is automatically transcribed and then fed to a summarization system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
A benefit of this approach is that the individual models can be trained independently, taking advantage of non-parallel data.
More recent work has explored direct summarization where a single model is used to directly summarize the audio <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
The model can be directly optimized for the task, as opposed to individual components being optimized for intermediate objectives.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Summarization can be either abstractive or extractive.
In a cross-lingual conversational speech domain, extractive summarization can be problematic.
Both transcription and translation errors are propagated, and the extracted utterances can be incomplete or incoherent <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
Because of these issues, we focus on abstractive summarization.
While direct approaches can be powerful, we focus on a cascaded approach which allows us to incorporate and compare open-source models.
We aim to establish an evaluation framework for conversational speech summarization and to evaluate the ability of LLMs to accomplish the task.
We leave comparisons against direct summarization to future work.
Our contributions are as follows:</p>
</div>
<div id="S1.p6" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We provide a first of its kind public conversational speech summarization dataset<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/hartmannw/spanish-cts-summarization</span></span></span> by building upon existing datasets.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We compare a range of LLMs and provide baseline performance using open-source tools and models.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We demonstrate that by fine-tuning a relatively small, quantized LLM, we achieve performance competitive with GPT-4.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Technical Approach and Dataset Creation</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Datasets</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">We focus on the Fisher and Callhome corpora of Spanish conversational telephone speech (CTS).
The audio and Spanish transcripts of which are available through the LDC<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://catalog.ldc.upenn.edu/{LDC2010T04, LDC2010S01, LDC96T17, LDC96S35}</span></span></span>.
The corpora contain crowd-sourced English translations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> of Spanish telephone calls.
The translations are publicly available<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/joshua-decoder/fisher-callhome-corpus</span></span></span>.
We focus on this dataset due to the large amount of transcribed audio compared to other CTS datasets and the availability of translations.
We are unaware of similar CTS datasets with English translations.
The existence of English reference translations are critical as they are used for reference summary generation.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The Callhome corpus comes with a predefined train/test split.
For the Fisher data, we adopt the data splits defined by Post et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> in order to align with their translations and results.
We report results across both Callhome test splits (Devtest, Evltest) and all three Fisher test splits (Dev, Dev2, Test).</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Summary Generation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">While we have human generated translations for this dataset, there are no existing summaries.
The collection of human summaries for this dataset would be expensive and time-consuming.
Instead, we generate summaries using GPT-4<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://openai.com/research/gpt-4</span></span></span>.
We justify this decision in two ways.
The first is that summaries generated by GPT-4's predecessors have been judged comparable to human summaries on some datasets<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
Given the difficulty of summarizing conversational speech, there is no guarantee that summaries generated by humans would be substantially better.
Second, GPT-4 is given access to reference translations when generating the summaries.
During evaluation, an LLM will be given input that contains both ASR and machine translation (MT) errors.
The goal is to generate a summary from errorful input that can match the reference summary.
Even if the reference summaries are deficient, obtaining a similar result in the presence of errors would still signify a significant achievement.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span> <span id="S2.T1.2.1" class="ltx_text ltx_font_italic">Breakdown of the number of conversations (Conv.), chunks, utterances, and hours of audio (Hrs) across the datasets. Summarization happens at the level of chunks.</span></figcaption>
<table id="S2.T1.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.3.1.1" class="ltx_tr">
<td id="S2.T1.3.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Dataset</td>
<td id="S2.T1.3.1.1.2" class="ltx_td ltx_align_right ltx_border_tt">Conv.</td>
<td id="S2.T1.3.1.1.3" class="ltx_td ltx_align_right ltx_border_tt">Chunks</td>
<td id="S2.T1.3.1.1.4" class="ltx_td ltx_align_right ltx_border_tt">Utterances</td>
<td id="S2.T1.3.1.1.5" class="ltx_td ltx_align_right ltx_border_tt">Hrs</td>
</tr>
<tr id="S2.T1.3.2.2" class="ltx_tr">
<td id="S2.T1.3.2.2.1" class="ltx_td ltx_align_left ltx_border_t">Callhome/Train</td>
<td id="S2.T1.3.2.2.2" class="ltx_td ltx_align_right ltx_border_t">80</td>
<td id="S2.T1.3.2.2.3" class="ltx_td ltx_align_right ltx_border_t">164</td>
<td id="S2.T1.3.2.2.4" class="ltx_td ltx_align_right ltx_border_t">14,996</td>
<td id="S2.T1.3.2.2.5" class="ltx_td ltx_align_right ltx_border_t">14</td>
</tr>
<tr id="S2.T1.3.3.3" class="ltx_tr">
<td id="S2.T1.3.3.3.1" class="ltx_td ltx_align_left">Fisher/Train</td>
<td id="S2.T1.3.3.3.2" class="ltx_td ltx_align_right">759</td>
<td id="S2.T1.3.3.3.3" class="ltx_td ltx_align_right">1,637</td>
<td id="S2.T1.3.3.3.4" class="ltx_td ltx_align_right">137,941</td>
<td id="S2.T1.3.3.3.5" class="ltx_td ltx_align_right">168</td>
</tr>
<tr id="S2.T1.3.4.4" class="ltx_tr">
<td id="S2.T1.3.4.4.1" class="ltx_td ltx_align_left ltx_border_t">Callhome/Devtest</td>
<td id="S2.T1.3.4.4.2" class="ltx_td ltx_align_right ltx_border_t">20</td>
<td id="S2.T1.3.4.4.3" class="ltx_td ltx_align_right ltx_border_t">41</td>
<td id="S2.T1.3.4.4.4" class="ltx_td ltx_align_right ltx_border_t">3,945</td>
<td id="S2.T1.3.4.4.5" class="ltx_td ltx_align_right ltx_border_t">4</td>
</tr>
<tr id="S2.T1.3.5.5" class="ltx_tr">
<td id="S2.T1.3.5.5.1" class="ltx_td ltx_align_left">Callhome/Evltest</td>
<td id="S2.T1.3.5.5.2" class="ltx_td ltx_align_right">20</td>
<td id="S2.T1.3.5.5.3" class="ltx_td ltx_align_right">23</td>
<td id="S2.T1.3.5.5.4" class="ltx_td ltx_align_right">1,826</td>
<td id="S2.T1.3.5.5.5" class="ltx_td ltx_align_right">2</td>
</tr>
<tr id="S2.T1.3.6.6" class="ltx_tr">
<td id="S2.T1.3.6.6.1" class="ltx_td ltx_align_left ltx_border_t">Fisher/Dev</td>
<td id="S2.T1.3.6.6.2" class="ltx_td ltx_align_right ltx_border_t">20</td>
<td id="S2.T1.3.6.6.3" class="ltx_td ltx_align_right ltx_border_t">44</td>
<td id="S2.T1.3.6.6.4" class="ltx_td ltx_align_right ltx_border_t">3,955</td>
<td id="S2.T1.3.6.6.5" class="ltx_td ltx_align_right ltx_border_t">5</td>
</tr>
<tr id="S2.T1.3.7.7" class="ltx_tr">
<td id="S2.T1.3.7.7.1" class="ltx_td ltx_align_left">Fisher/Dev2</td>
<td id="S2.T1.3.7.7.2" class="ltx_td ltx_align_right">20</td>
<td id="S2.T1.3.7.7.3" class="ltx_td ltx_align_right">44</td>
<td id="S2.T1.3.7.7.4" class="ltx_td ltx_align_right">3,937</td>
<td id="S2.T1.3.7.7.5" class="ltx_td ltx_align_right">5</td>
</tr>
<tr id="S2.T1.3.8.8" class="ltx_tr">
<td id="S2.T1.3.8.8.1" class="ltx_td ltx_align_left ltx_border_bb">Fisher/Test</td>
<td id="S2.T1.3.8.8.2" class="ltx_td ltx_align_right ltx_border_bb">20</td>
<td id="S2.T1.3.8.8.3" class="ltx_td ltx_align_right ltx_border_bb">43</td>
<td id="S2.T1.3.8.8.4" class="ltx_td ltx_align_right ltx_border_bb">3,618</td>
<td id="S2.T1.3.8.8.5" class="ltx_td ltx_align_right ltx_border_bb">4</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">For both the training and the test sets, we present GPT-4 with a conversation and ask it to generate a summary.
Since we know summaries capturing the same content can differ significantly in style, we generate four total summaries for each test conversation by sampling outputs with a temperature of 0.5.
We aim for our evaluation set to be useful even for evaluating models with limited context windows, so we set the maximum number of words in a conversation to 1200 words.
Given this limit, the context, prompt, and a reasonable length summary will all fit within a context window of 2048 tokens.
If a conversation in either set exceeds the 1200 word limit, we split it into equal-sized chunks and treat each individual chunk as a separate conversation.
Across both the train and test sets, the summaries range in size from 144 to 443 words, with a median size of 268 words.
We plan to release these reference summaries to the community.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">A breakdown of the number of conversations, chunks, utterances, and audio hours for each dataset is shown in Table <a href="#S4.T3" title="Table 3 ‣ 4.1 Zero-shot ‣ 4 Results ‣ Cross-Lingual Conversational Speech Summarization with Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
When building our summarization dataset, each chunk represents one training example or one datapoint for evaluation.
While any of the individual testsets would be sufficient for ASR or MT evaluation, they do not individually provide enough examples for summarization evaluation.
Aggregating all of the test sets gives a total 195 test examples from 100 conversations, a more appropriate number for summarization evaluation.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>LLM Adaptation</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Along with testing off-the-shelf LLMs, we also experiment with supervised fine-tuning for task adaptation in order to understand the potential for improvement and establish strong baselines for future work to compare against. We use LoRA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> finetuning to adapt the models. All fine-tuning experiments are run with 4-bit quantization and fp16 precision. A LoRA adaptor is learned for every linear layer in the model with <math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="r=64" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><mrow id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml"><mi id="S2.SS3.p1.1.m1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.2.cmml">r</mi><mo id="S2.SS3.p1.1.m1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.cmml">=</mo><mn id="S2.SS3.p1.1.m1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.3.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><apply id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1"><eq id="S2.SS3.p1.1.m1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1"></eq><ci id="S2.SS3.p1.1.m1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.2">𝑟</ci><cn type="integer" id="S2.SS3.p1.1.m1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">r=64</annotation></semantics></math>. The training data are GPT-4 reference summaries paired with either the reference English transcripts (Ref) or with the outputs of our Whisper-NLLB speech translation system (MT from ASR). In other words we we create two training samples for each GPT-4 summary that differ only in the input. We vary which inputs we use during fine-tuning to evaluate the extent to which domain-matched input improves summarization quality. For fine-tuning experiments that make use of both the reference English and MT from ASR inputs we train the model for a single epoch over all training data. When finetuning on only the reference English or MT from ASR transcripts we train for two epochs in order to keep the number of update steps constant across experiments.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>We also ran inference with these models at the one epoch mark and conclude that there is minimal difference when testing at one or two epochs. The ROUGE-L scores in Table <a href="#S4.T4" title="Table 4 ‣ 4.1 Zero-shot ‣ 4 Results ‣ Cross-Lingual Conversational Speech Summarization with Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> are the result of two epoch training, while those in the 182 hour condition of Table <a href="#S4.T5" title="Table 5 ‣ 4.1 Zero-shot ‣ 4 Results ‣ Cross-Lingual Conversational Speech Summarization with Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> are the result of one epoch of training. The difference in comparable values is less than 0.7 ROUGE.</span></span></span></p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>ASR Model</h3>

<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span> <span id="S3.T2.2.1" class="ltx_text ltx_font_italic">WER and BLEU scores for the Whisper ASR model and the NLLB MT model. The last two columns correspond to BLEU scores where the column header refers to the input to the MT system, either output from the Whisper model (ASR-Spanish) or reference transcriptions (Ref-Spanish).</span></figcaption>
<table id="S3.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.3.1.1" class="ltx_tr">
<th id="S3.T2.3.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt"></th>
<th id="S3.T2.3.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S3.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">BLEU</th>
</tr>
<tr id="S3.T2.3.2.2" class="ltx_tr">
<th id="S3.T2.3.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row">Test Set</th>
<th id="S3.T2.3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r">WER</th>
<th id="S3.T2.3.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">ASR-Spanish</th>
<th id="S3.T2.3.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Ref-Spanish</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.3.3.1" class="ltx_tr">
<th id="S3.T2.3.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Callhome/Devtest</th>
<th id="S3.T2.3.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">29.1</th>
<td id="S3.T2.3.3.1.3" class="ltx_td ltx_align_center ltx_border_t">21.8</td>
<td id="S3.T2.3.3.1.4" class="ltx_td ltx_align_center ltx_border_t">30.3</td>
</tr>
<tr id="S3.T2.3.4.2" class="ltx_tr">
<th id="S3.T2.3.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Callhome/Evltest</th>
<th id="S3.T2.3.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">26.6</th>
<td id="S3.T2.3.4.2.3" class="ltx_td ltx_align_center">23.0</td>
<td id="S3.T2.3.4.2.4" class="ltx_td ltx_align_center">31.2</td>
</tr>
<tr id="S3.T2.3.5.3" class="ltx_tr">
<th id="S3.T2.3.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Fisher/Dev</th>
<th id="S3.T2.3.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">31.9</th>
<td id="S3.T2.3.5.3.3" class="ltx_td ltx_align_center">22.6</td>
<td id="S3.T2.3.5.3.4" class="ltx_td ltx_align_center">30.3</td>
</tr>
<tr id="S3.T2.3.6.4" class="ltx_tr">
<th id="S3.T2.3.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Fisher/Dev2</th>
<th id="S3.T2.3.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">32.4</th>
<td id="S3.T2.3.6.4.3" class="ltx_td ltx_align_center">23.6</td>
<td id="S3.T2.3.6.4.4" class="ltx_td ltx_align_center">32.0</td>
</tr>
<tr id="S3.T2.3.7.5" class="ltx_tr">
<th id="S3.T2.3.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Fisher/Test</th>
<th id="S3.T2.3.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">25.4</th>
<td id="S3.T2.3.7.5.3" class="ltx_td ltx_align_center ltx_border_bb">23.4</td>
<td id="S3.T2.3.7.5.4" class="ltx_td ltx_align_center ltx_border_bb">30.7</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We use the Whisper-large-v3 model<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> for ASR.
While a dataset and language-specific model could likely outperform the Whisper model in the CTS domain, the Whisper model is publicly available and is chosen due to its wide use and reproducibility.
The WER of the Whisper model on each of the five test sets is shown in the second column of Table <a href="#S3.T2" title="Table 2 ‣ 3.1 ASR Model ‣ 3 Experimental Setup ‣ Cross-Lingual Conversational Speech Summarization with Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
We measure the WER after downcasing the the output and removing punctuation.
This postprocessing is not applied when used in the cascaded pipeline.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>MT Model</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We use the NLLB 1.3 Billion parameter dense model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> for machine translation.
As with Whisper for ASR, a domain-specific model would likely outperform the NLLB model on CTS, but we use NLLB for better reproducibility.
The BLEU scores for NLLB on the test sets are also shown in Table <a href="#S3.T2" title="Table 2 ‣ 3.1 ASR Model ‣ 3 Experimental Setup ‣ Cross-Lingual Conversational Speech Summarization with Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
We include punctuation when computing the reported BLEU scores, we also tested scoring without punctuation and found it to have negligible impact on the scores so we exclude those results for legibility.
The third column in Table <a href="#S3.T2" title="Table 2 ‣ 3.1 ASR Model ‣ 3 Experimental Setup ‣ Cross-Lingual Conversational Speech Summarization with Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> uses the Whisper ASR output as input to NLLB, while the last column uses the reference Spanish transcriptions.
On average the BLEU scores drop by about 8 points when using ASR output as opposed to reference transcriptions.
Note that while some of the test sets contain multiple translations, we only report BLEU scores using a single reference so that the numbers are comparable across test sets.
In the remaining sections we explore the impact of cascaded ASR and MT errors on downstream summarization.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>LLMs for Summarization</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">As described in Section <a href="#S2.SS2" title="2.2 Summary Generation ‣ 2 Technical Approach and Dataset Creation ‣ Cross-Lingual Conversational Speech Summarization with Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>, we use GPT-4 to generate the reference summaries.
We then evaluate a range of open-source and API-based models against the GPT-4 generated references.
The API-based models we consider are GPT-3.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and GPT-4.
The open-source models we consider are the 7 and 13 billion parameter versions of Llama 2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, the 7 billion parameter Mistral <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, and the 45 billion mixture-of-experts model Mixtral-8x7 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. We focus on these open-source models due to their low compute requirements which make them more amenable to real-world applications. For the same reason all inference is run with 4-bit quantization and fp16 precision.
All open-source models tested are the officially released chat or instruct tuned versions. It is well-known that the performance of LLMs can vary dramatically depending on the prompt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
We follow the guidelines for prompt structure released by the publishers of each of the individual models.
Our exact prompt structure will be released with the reference summaries.
In addition to applying these models off-the-shelf, we also run a set of further supervised fine-tuning experiments with the Mistral 7B model.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Zero-shot</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We evaluate the quality of summaries with ROUGE-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.
We explored a number of other metrics, but most gave a similar ordering in terms of model performance.
While we recognize the pitfalls of focusing on a single metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, we only report ROUGE-L due to space concerns.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span> <span id="S4.T3.2.1" class="ltx_text ltx_font_italic">ROUGE-L scores for summarization using LLMs. For each model, each row represents a different input condition. The first row is the reference translation. The second row is machine translation of the reference transcripts. The final row is the full pipeline, machine translation of the automatic transcripts.</span></figcaption>
<table id="S4.T3.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.3.1.1" class="ltx_tr">
<td id="S4.T3.3.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T3.3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2">Callhome</td>
<td id="S4.T3.3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3">Fisher</td>
<td id="S4.T3.3.1.1.4" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="S4.T3.3.2.2" class="ltx_tr">
<td id="S4.T3.3.2.2.1" class="ltx_td ltx_align_left">Model+Input</td>
<td id="S4.T3.3.2.2.2" class="ltx_td ltx_align_center">Dev</td>
<td id="S4.T3.3.2.2.3" class="ltx_td ltx_align_center ltx_border_r">Eval</td>
<td id="S4.T3.3.2.2.4" class="ltx_td ltx_align_center">Dev</td>
<td id="S4.T3.3.2.2.5" class="ltx_td ltx_align_center">Dev2</td>
<td id="S4.T3.3.2.2.6" class="ltx_td ltx_align_center ltx_border_r">Test</td>
<td id="S4.T3.3.2.2.7" class="ltx_td ltx_align_center">All</td>
</tr>
<tr id="S4.T3.3.3.3" class="ltx_tr">
<td id="S4.T3.3.3.3.1" class="ltx_td ltx_align_left ltx_border_t" colspan="7"><span id="S4.T3.3.3.3.1.1" class="ltx_text ltx_font_bold">Llama2-7B</span></td>
</tr>
<tr id="S4.T3.3.4.4" class="ltx_tr">
<td id="S4.T3.3.4.4.1" class="ltx_td ltx_align_left">- Ref.</td>
<td id="S4.T3.3.4.4.2" class="ltx_td ltx_align_center">23.8</td>
<td id="S4.T3.3.4.4.3" class="ltx_td ltx_align_center ltx_border_r">23.2</td>
<td id="S4.T3.3.4.4.4" class="ltx_td ltx_align_center">25.1</td>
<td id="S4.T3.3.4.4.5" class="ltx_td ltx_align_center">23.9</td>
<td id="S4.T3.3.4.4.6" class="ltx_td ltx_align_center ltx_border_r">24.5</td>
<td id="S4.T3.3.4.4.7" class="ltx_td ltx_align_center">24.2</td>
</tr>
<tr id="S4.T3.3.5.5" class="ltx_tr">
<td id="S4.T3.3.5.5.1" class="ltx_td ltx_align_left">- MT of Ref.</td>
<td id="S4.T3.3.5.5.2" class="ltx_td ltx_align_center">23.1</td>
<td id="S4.T3.3.5.5.3" class="ltx_td ltx_align_center ltx_border_r">22.4</td>
<td id="S4.T3.3.5.5.4" class="ltx_td ltx_align_center">23.3</td>
<td id="S4.T3.3.5.5.5" class="ltx_td ltx_align_center">22.8</td>
<td id="S4.T3.3.5.5.6" class="ltx_td ltx_align_center ltx_border_r">24.9</td>
<td id="S4.T3.3.5.5.7" class="ltx_td ltx_align_center">23.4</td>
</tr>
<tr id="S4.T3.3.6.6" class="ltx_tr">
<td id="S4.T3.3.6.6.1" class="ltx_td ltx_align_left">- MT of ASR</td>
<td id="S4.T3.3.6.6.2" class="ltx_td ltx_align_center">23.1</td>
<td id="S4.T3.3.6.6.3" class="ltx_td ltx_align_center ltx_border_r">23.6</td>
<td id="S4.T3.3.6.6.4" class="ltx_td ltx_align_center">23.3</td>
<td id="S4.T3.3.6.6.5" class="ltx_td ltx_align_center">22.9</td>
<td id="S4.T3.3.6.6.6" class="ltx_td ltx_align_center ltx_border_r">23.8</td>
<td id="S4.T3.3.6.6.7" class="ltx_td ltx_align_center">23.4</td>
</tr>
<tr id="S4.T3.3.7.7" class="ltx_tr">
<td id="S4.T3.3.7.7.1" class="ltx_td ltx_align_left ltx_border_t" colspan="7"><span id="S4.T3.3.7.7.1.1" class="ltx_text ltx_font_bold">Llama2-13B</span></td>
</tr>
<tr id="S4.T3.3.8.8" class="ltx_tr">
<td id="S4.T3.3.8.8.1" class="ltx_td ltx_align_left">- Ref.</td>
<td id="S4.T3.3.8.8.2" class="ltx_td ltx_align_center">23.6</td>
<td id="S4.T3.3.8.8.3" class="ltx_td ltx_align_center ltx_border_r">23.9</td>
<td id="S4.T3.3.8.8.4" class="ltx_td ltx_align_center">25.7</td>
<td id="S4.T3.3.8.8.5" class="ltx_td ltx_align_center">25.5</td>
<td id="S4.T3.3.8.8.6" class="ltx_td ltx_align_center ltx_border_r">26.4</td>
<td id="S4.T3.3.8.8.7" class="ltx_td ltx_align_center">25.2</td>
</tr>
<tr id="S4.T3.3.9.9" class="ltx_tr">
<td id="S4.T3.3.9.9.1" class="ltx_td ltx_align_left">- MT of Ref.</td>
<td id="S4.T3.3.9.9.2" class="ltx_td ltx_align_center">22.4</td>
<td id="S4.T3.3.9.9.3" class="ltx_td ltx_align_center ltx_border_r">22.3</td>
<td id="S4.T3.3.9.9.4" class="ltx_td ltx_align_center">24.1</td>
<td id="S4.T3.3.9.9.5" class="ltx_td ltx_align_center">22.7</td>
<td id="S4.T3.3.9.9.6" class="ltx_td ltx_align_center ltx_border_r">24.3</td>
<td id="S4.T3.3.9.9.7" class="ltx_td ltx_align_center">23.3</td>
</tr>
<tr id="S4.T3.3.10.10" class="ltx_tr">
<td id="S4.T3.3.10.10.1" class="ltx_td ltx_align_left">- MT of ASR</td>
<td id="S4.T3.3.10.10.2" class="ltx_td ltx_align_center">22.8</td>
<td id="S4.T3.3.10.10.3" class="ltx_td ltx_align_center ltx_border_r">21.9</td>
<td id="S4.T3.3.10.10.4" class="ltx_td ltx_align_center">23.9</td>
<td id="S4.T3.3.10.10.5" class="ltx_td ltx_align_center">23.6</td>
<td id="S4.T3.3.10.10.6" class="ltx_td ltx_align_center ltx_border_r">25.3</td>
<td id="S4.T3.3.10.10.7" class="ltx_td ltx_align_center">23.7</td>
</tr>
<tr id="S4.T3.3.11.11" class="ltx_tr">
<td id="S4.T3.3.11.11.1" class="ltx_td ltx_align_left ltx_border_t" colspan="7"><span id="S4.T3.3.11.11.1.1" class="ltx_text ltx_font_bold">Mixtral-8x7B</span></td>
</tr>
<tr id="S4.T3.3.12.12" class="ltx_tr">
<td id="S4.T3.3.12.12.1" class="ltx_td ltx_align_left">- Ref.</td>
<td id="S4.T3.3.12.12.2" class="ltx_td ltx_align_center">26.7</td>
<td id="S4.T3.3.12.12.3" class="ltx_td ltx_align_center ltx_border_r">27.0</td>
<td id="S4.T3.3.12.12.4" class="ltx_td ltx_align_center">28.4</td>
<td id="S4.T3.3.12.12.5" class="ltx_td ltx_align_center">27.7</td>
<td id="S4.T3.3.12.12.6" class="ltx_td ltx_align_center ltx_border_r">27.6</td>
<td id="S4.T3.3.12.12.7" class="ltx_td ltx_align_center">27.5</td>
</tr>
<tr id="S4.T3.3.13.13" class="ltx_tr">
<td id="S4.T3.3.13.13.1" class="ltx_td ltx_align_left">- MT of Ref.</td>
<td id="S4.T3.3.13.13.2" class="ltx_td ltx_align_center">27.1</td>
<td id="S4.T3.3.13.13.3" class="ltx_td ltx_align_center ltx_border_r">26.9</td>
<td id="S4.T3.3.13.13.4" class="ltx_td ltx_align_center">27.8</td>
<td id="S4.T3.3.13.13.5" class="ltx_td ltx_align_center">26.9</td>
<td id="S4.T3.3.13.13.6" class="ltx_td ltx_align_center ltx_border_r">27.2</td>
<td id="S4.T3.3.13.13.7" class="ltx_td ltx_align_center">27.2</td>
</tr>
<tr id="S4.T3.3.14.14" class="ltx_tr">
<td id="S4.T3.3.14.14.1" class="ltx_td ltx_align_left">- MT of ASR</td>
<td id="S4.T3.3.14.14.2" class="ltx_td ltx_align_center">26.7</td>
<td id="S4.T3.3.14.14.3" class="ltx_td ltx_align_center ltx_border_r">25.6</td>
<td id="S4.T3.3.14.14.4" class="ltx_td ltx_align_center">27.4</td>
<td id="S4.T3.3.14.14.5" class="ltx_td ltx_align_center">26.9</td>
<td id="S4.T3.3.14.14.6" class="ltx_td ltx_align_center ltx_border_r">27.1</td>
<td id="S4.T3.3.14.14.7" class="ltx_td ltx_align_center">26.9</td>
</tr>
<tr id="S4.T3.3.15.15" class="ltx_tr">
<td id="S4.T3.3.15.15.1" class="ltx_td ltx_align_left ltx_border_t" colspan="7"><span id="S4.T3.3.15.15.1.1" class="ltx_text ltx_font_bold">Mistral-7B</span></td>
</tr>
<tr id="S4.T3.3.16.16" class="ltx_tr">
<td id="S4.T3.3.16.16.1" class="ltx_td ltx_align_left">- Ref.</td>
<td id="S4.T3.3.16.16.2" class="ltx_td ltx_align_center">24.3</td>
<td id="S4.T3.3.16.16.3" class="ltx_td ltx_align_center ltx_border_r">22.6</td>
<td id="S4.T3.3.16.16.4" class="ltx_td ltx_align_center">26.1</td>
<td id="S4.T3.3.16.16.5" class="ltx_td ltx_align_center">25.6</td>
<td id="S4.T3.3.16.16.6" class="ltx_td ltx_align_center ltx_border_r">24.7</td>
<td id="S4.T3.3.16.16.7" class="ltx_td ltx_align_center">24.7</td>
</tr>
<tr id="S4.T3.3.17.17" class="ltx_tr">
<td id="S4.T3.3.17.17.1" class="ltx_td ltx_align_left">- MT of Ref.</td>
<td id="S4.T3.3.17.17.2" class="ltx_td ltx_align_center">22.5</td>
<td id="S4.T3.3.17.17.3" class="ltx_td ltx_align_center ltx_border_r">21.0</td>
<td id="S4.T3.3.17.17.4" class="ltx_td ltx_align_center">24.1</td>
<td id="S4.T3.3.17.17.5" class="ltx_td ltx_align_center">24.0</td>
<td id="S4.T3.3.17.17.6" class="ltx_td ltx_align_center ltx_border_r">25.0</td>
<td id="S4.T3.3.17.17.7" class="ltx_td ltx_align_center">23.3</td>
</tr>
<tr id="S4.T3.3.18.18" class="ltx_tr">
<td id="S4.T3.3.18.18.1" class="ltx_td ltx_align_left">- MT of ASR</td>
<td id="S4.T3.3.18.18.2" class="ltx_td ltx_align_center">22.3</td>
<td id="S4.T3.3.18.18.3" class="ltx_td ltx_align_center ltx_border_r">21.6</td>
<td id="S4.T3.3.18.18.4" class="ltx_td ltx_align_center">24.1</td>
<td id="S4.T3.3.18.18.5" class="ltx_td ltx_align_center">23.4</td>
<td id="S4.T3.3.18.18.6" class="ltx_td ltx_align_center ltx_border_r">23.9</td>
<td id="S4.T3.3.18.18.7" class="ltx_td ltx_align_center">23.1</td>
</tr>
<tr id="S4.T3.3.19.19" class="ltx_tr">
<td id="S4.T3.3.19.19.1" class="ltx_td ltx_align_left ltx_border_t" colspan="7"><span id="S4.T3.3.19.19.1.1" class="ltx_text ltx_font_bold">GPT-3.5</span></td>
</tr>
<tr id="S4.T3.3.20.20" class="ltx_tr">
<td id="S4.T3.3.20.20.1" class="ltx_td ltx_align_left">- Ref.</td>
<td id="S4.T3.3.20.20.2" class="ltx_td ltx_align_center">24.0</td>
<td id="S4.T3.3.20.20.3" class="ltx_td ltx_align_center ltx_border_r">23.7</td>
<td id="S4.T3.3.20.20.4" class="ltx_td ltx_align_center">28.0</td>
<td id="S4.T3.3.20.20.5" class="ltx_td ltx_align_center">27.0</td>
<td id="S4.T3.3.20.20.6" class="ltx_td ltx_align_center ltx_border_r">27.8</td>
<td id="S4.T3.3.20.20.7" class="ltx_td ltx_align_center">26.1</td>
</tr>
<tr id="S4.T3.3.21.21" class="ltx_tr">
<td id="S4.T3.3.21.21.1" class="ltx_td ltx_align_left">- MT of Ref.</td>
<td id="S4.T3.3.21.21.2" class="ltx_td ltx_align_center">23.5</td>
<td id="S4.T3.3.21.21.3" class="ltx_td ltx_align_center ltx_border_r">21.9</td>
<td id="S4.T3.3.21.21.4" class="ltx_td ltx_align_center">27.4</td>
<td id="S4.T3.3.21.21.5" class="ltx_td ltx_align_center">26.7</td>
<td id="S4.T3.3.21.21.6" class="ltx_td ltx_align_center ltx_border_r">26.0</td>
<td id="S4.T3.3.21.21.7" class="ltx_td ltx_align_center">25.1</td>
</tr>
<tr id="S4.T3.3.22.22" class="ltx_tr">
<td id="S4.T3.3.22.22.1" class="ltx_td ltx_align_left">- MT of ASR</td>
<td id="S4.T3.3.22.22.2" class="ltx_td ltx_align_center">20.9</td>
<td id="S4.T3.3.22.22.3" class="ltx_td ltx_align_center ltx_border_r">20.3</td>
<td id="S4.T3.3.22.22.4" class="ltx_td ltx_align_center">25.5</td>
<td id="S4.T3.3.22.22.5" class="ltx_td ltx_align_center">25.1</td>
<td id="S4.T3.3.22.22.6" class="ltx_td ltx_align_center ltx_border_r">25.5</td>
<td id="S4.T3.3.22.22.7" class="ltx_td ltx_align_center">23.5</td>
</tr>
<tr id="S4.T3.3.23.23" class="ltx_tr">
<td id="S4.T3.3.23.23.1" class="ltx_td ltx_align_left ltx_border_t" colspan="7"><span id="S4.T3.3.23.23.1.1" class="ltx_text ltx_font_bold">GPT-4</span></td>
</tr>
<tr id="S4.T3.3.24.24" class="ltx_tr">
<td id="S4.T3.3.24.24.1" class="ltx_td ltx_align_left">- Ref.</td>
<td id="S4.T3.3.24.24.2" class="ltx_td ltx_align_center">—</td>
<td id="S4.T3.3.24.24.3" class="ltx_td ltx_align_center ltx_border_r">—</td>
<td id="S4.T3.3.24.24.4" class="ltx_td ltx_align_center">—</td>
<td id="S4.T3.3.24.24.5" class="ltx_td ltx_align_center">—</td>
<td id="S4.T3.3.24.24.6" class="ltx_td ltx_align_center ltx_border_r">—</td>
<td id="S4.T3.3.24.24.7" class="ltx_td ltx_align_center">—</td>
</tr>
<tr id="S4.T3.3.25.25" class="ltx_tr">
<td id="S4.T3.3.25.25.1" class="ltx_td ltx_align_left">- MT of Ref.</td>
<td id="S4.T3.3.25.25.2" class="ltx_td ltx_align_center">32.0</td>
<td id="S4.T3.3.25.25.3" class="ltx_td ltx_align_center ltx_border_r">31.7</td>
<td id="S4.T3.3.25.25.4" class="ltx_td ltx_align_center">34.1</td>
<td id="S4.T3.3.25.25.5" class="ltx_td ltx_align_center">33.0</td>
<td id="S4.T3.3.25.25.6" class="ltx_td ltx_align_center ltx_border_r">33.7</td>
<td id="S4.T3.3.25.25.7" class="ltx_td ltx_align_center">32.9</td>
</tr>
<tr id="S4.T3.3.26.26" class="ltx_tr">
<td id="S4.T3.3.26.26.1" class="ltx_td ltx_align_left">- MT of ASR</td>
<td id="S4.T3.3.26.26.2" class="ltx_td ltx_align_center">30.9</td>
<td id="S4.T3.3.26.26.3" class="ltx_td ltx_align_center ltx_border_r">31.0</td>
<td id="S4.T3.3.26.26.4" class="ltx_td ltx_align_center">33.4</td>
<td id="S4.T3.3.26.26.5" class="ltx_td ltx_align_center">32.3</td>
<td id="S4.T3.3.26.26.6" class="ltx_td ltx_align_center ltx_border_r">32.6</td>
<td id="S4.T3.3.26.26.7" class="ltx_td ltx_align_center">32.0</td>
</tr>
<tr id="S4.T3.3.27.27" class="ltx_tr">
<td id="S4.T3.3.27.27.1" class="ltx_td ltx_align_left ltx_border_tt" colspan="7"><span id="S4.T3.3.27.27.1.1" class="ltx_text ltx_font_bold">FT Mistral-7B</span></td>
</tr>
<tr id="S4.T3.3.28.28" class="ltx_tr">
<td id="S4.T3.3.28.28.1" class="ltx_td ltx_align_left">- Ref.</td>
<td id="S4.T3.3.28.28.2" class="ltx_td ltx_align_center">33.1</td>
<td id="S4.T3.3.28.28.3" class="ltx_td ltx_align_center ltx_border_r">32.6</td>
<td id="S4.T3.3.28.28.4" class="ltx_td ltx_align_center">35.1</td>
<td id="S4.T3.3.28.28.5" class="ltx_td ltx_align_center">34.5</td>
<td id="S4.T3.3.28.28.6" class="ltx_td ltx_align_center ltx_border_r">35.2</td>
<td id="S4.T3.3.28.28.7" class="ltx_td ltx_align_center">34.3</td>
</tr>
<tr id="S4.T3.3.29.29" class="ltx_tr">
<td id="S4.T3.3.29.29.1" class="ltx_td ltx_align_left">- MT of Ref.</td>
<td id="S4.T3.3.29.29.2" class="ltx_td ltx_align_center">32.9</td>
<td id="S4.T3.3.29.29.3" class="ltx_td ltx_align_center ltx_border_r">32.0</td>
<td id="S4.T3.3.29.29.4" class="ltx_td ltx_align_center">34.6</td>
<td id="S4.T3.3.29.29.5" class="ltx_td ltx_align_center">33.5</td>
<td id="S4.T3.3.29.29.6" class="ltx_td ltx_align_center ltx_border_r">34.0</td>
<td id="S4.T3.3.29.29.7" class="ltx_td ltx_align_center">33.5</td>
</tr>
<tr id="S4.T3.3.30.30" class="ltx_tr">
<td id="S4.T3.3.30.30.1" class="ltx_td ltx_align_left ltx_border_bb">- MT of ASR</td>
<td id="S4.T3.3.30.30.2" class="ltx_td ltx_align_center ltx_border_bb">32.2</td>
<td id="S4.T3.3.30.30.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">31.0</td>
<td id="S4.T3.3.30.30.4" class="ltx_td ltx_align_center ltx_border_bb">33.3</td>
<td id="S4.T3.3.30.30.5" class="ltx_td ltx_align_center ltx_border_bb">33.0</td>
<td id="S4.T3.3.30.30.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">33.4</td>
<td id="S4.T3.3.30.30.7" class="ltx_td ltx_align_center ltx_border_bb">32.7</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span> <span id="S4.T4.2.1" class="ltx_text ltx_font_italic">ROUGE-L scores on the reference transcript and MT of ASR variants of the test set from models fine-tuned on reference transcripts, on MT of ASR transcripts, and on both. The source prompt version of the fine-tune set includes both reference and MT of ASR transcripts, with a prompt telling the LLM which is which.</span></figcaption>
<table id="S4.T4.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.3.1.1" class="ltx_tr">
<th id="S4.T4.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Finetune Data</th>
<th id="S4.T4.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Ref. Test</th>
<th id="S4.T4.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MT of ASR Test</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.3.2.1" class="ltx_tr">
<th id="S4.T4.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Ref.</th>
<td id="S4.T4.3.2.1.2" class="ltx_td ltx_align_center ltx_border_t">34.7</td>
<td id="S4.T4.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t">33.1</td>
</tr>
<tr id="S4.T4.3.3.2" class="ltx_tr">
<th id="S4.T4.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MT of ASR</th>
<td id="S4.T4.3.3.2.2" class="ltx_td ltx_align_center">33.9</td>
<td id="S4.T4.3.3.2.3" class="ltx_td ltx_align_center">32.9</td>
</tr>
<tr id="S4.T4.3.4.3" class="ltx_tr">
<th id="S4.T4.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ref. + MT of ASR</th>
<td id="S4.T4.3.4.3.2" class="ltx_td ltx_align_center">34.3</td>
<td id="S4.T4.3.4.3.3" class="ltx_td ltx_align_center">32.7</td>
</tr>
<tr id="S4.T4.3.5.4" class="ltx_tr">
<th id="S4.T4.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">+ Source prompt</th>
<td id="S4.T4.3.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">34.1</td>
<td id="S4.T4.3.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">33.0</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span> <span id="S4.T5.2.1" class="ltx_text ltx_font_italic">ROUGE-L scores for the Mistral-7B model on MT of ASR test data after fine-tuning. Each column adds progressively more training data into the finetuning.</span></figcaption>
<table id="S4.T5.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.3.1.1" class="ltx_tr">
<th id="S4.T5.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Train Data</th>
<th id="S4.T5.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">0 hrs</th>
<th id="S4.T5.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">32 hrs</th>
<th id="S4.T5.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">65 hrs</th>
<th id="S4.T5.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">182 hrs</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.3.2.1" class="ltx_tr">
<th id="S4.T5.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Ref. Translation</th>
<td id="S4.T5.3.2.1.2" class="ltx_td ltx_align_center ltx_border_t">24.7</td>
<td id="S4.T5.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t">28.1</td>
<td id="S4.T5.3.2.1.4" class="ltx_td ltx_align_center ltx_border_t">28.4</td>
<td id="S4.T5.3.2.1.5" class="ltx_td ltx_align_center ltx_border_t">32.4</td>
</tr>
<tr id="S4.T5.3.3.2" class="ltx_tr">
<th id="S4.T5.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">MT of ASR Transcript</th>
<td id="S4.T5.3.3.2.2" class="ltx_td ltx_align_center ltx_border_bb">23.9</td>
<td id="S4.T5.3.3.2.3" class="ltx_td ltx_align_center ltx_border_bb">27.2</td>
<td id="S4.T5.3.3.2.4" class="ltx_td ltx_align_center ltx_border_bb">29.1</td>
<td id="S4.T5.3.3.2.5" class="ltx_td ltx_align_center ltx_border_bb">32.3</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span> <span id="S4.T6.2.1" class="ltx_text ltx_font_italic">Example summaries highlighting the issues that can be introduced through ASR and MT errors. Both summaries are generated using GPT-4, but the second uses ASR+MT as input. Errors that can be traced to issues with ASR and MT are highlighted in red.</span></figcaption>
<table id="S4.T6.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T6.3.1.1" class="ltx_tr">
<td id="S4.T6.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T6.3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.1.1.1.1.1" class="ltx_p" style="width:56.9pt;">Summary from Reference Translation</span>
</span>
</td>
<td id="S4.T6.3.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T6.3.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.1.1.2.1.1" class="ltx_p" style="width:398.3pt;">The conversation revolves around family dynamics, work, health, and recent unfortunate events. The initial part of the dialogue discusses the complexities and challenges of working with family members, with both speakers agreeing that it can create discord and unhappiness on both sides. The topic then shifts to personal updates, with one speaker mentioning they and Estela are fine despite gaining weight, which humorously leads to a comment on financial struggles tied to their eating habits.</span>
</span>
</td>
</tr>
<tr id="S4.T6.3.2.2" class="ltx_tr">
<td id="S4.T6.3.2.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T6.3.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.2.2.1.1.1" class="ltx_p" style="width:56.9pt;">Summary from ASR+MT</span>
</span>
</td>
<td id="S4.T6.3.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S4.T6.3.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.3.2.2.2.1.1" class="ltx_p" style="width:398.3pt;">The conversation kicks off with Speaker A expressing frustration that a certain female party is unwilling to work with him, touching on a broader theme of children generally disliking working with their parents. Speaker B offers <span id="S4.T6.3.2.2.2.1.1.1" class="ltx_text" style="color:#FF0000;"> mundane responses</span>, possibly indicating detachment or an attempt to change the subject, mentioning the cold weather and previous rough experiences working with families. The dialogue then shifts to personal updates, revealing that Speaker A and his family are dealing with health and weight issues, alongside financial difficulties marked by the mention of someone being <span id="S4.T6.3.2.2.2.1.1.2" class="ltx_text" style="color:#FF0000;">"so poor" and possibly "comatose."</span>…Speaker B, meanwhile, maintains a pattern of responding with <span id="S4.T6.3.2.2.2.1.1.3" class="ltx_text" style="color:#FF0000;">"Thank you, I'm all right,"</span> perhaps to offer polite acknowledgment without engaging deeply with the troubles Speaker A shares.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Baseline results are shown in Table <a href="#S4.T3" title="Table 3 ‣ 4.1 Zero-shot ‣ 4 Results ‣ Cross-Lingual Conversational Speech Summarization with Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
Each row represents a different input condition.
While the performance of each model becomes progressively worse as more error is introduced through MT and ASR—as opposed to using reference transcripts and translations—the drop in performance is less than anticipated.
The difference between a summary generated from a reference translation and the cascade of AST and MT is no more than 10% relative across all models.
We believe there are two possible reasons for this result and they merit future investigation.
Either the errors from transcription and translation do not impact the model's ability to summarize the key information, or the metric is not able to measure the impact.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">The performance of the open-source models follows the expected ranking, with the larger MoE model Mixtral outperforming the smaller, dense Mistral and Llama 2 models. Despite the large size difference and the relatively large performance gaps between these models on other benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, the actual range of performance across open-source models before fine-tuning is relatively narrow. Mixtral, the best open-source model without fine-tuning, outperforms the worst, Llama2-7B, by only 3.5 BLEU when translating MT of ASR.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Similarly the models all seem robust to MT and ASR errors even without fine-tuning. The largest gap between ref. and MT of ASR transcript performance is only 1.5 ROUGE. This is in contrast to previous work on other types of cascading systems, like speech MT, where downstream models have been repeatedly shown to be highly sensitive to upstream errors.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">We also compare the open-source models against GPT-3.5. Mixtral outperforms GPT-3.5 across input types and the other models perform competitively. Given the difference in model size and training, the performance of the open-source models is impressive. As a top line for performance, we evaluate GPT-4 generated summaries using both MT of reference transcriptions and MT of ASR.
We do not evaluate GPT-4 on reference translations because it was used to generate the reference summaries.
Even when using the MT input, we expect the results to be biased as it is essentially using the same model to evaluate itself.
GPT-4 obtains a ROUGE-L score almost 50% higher than some of the competing open-source models.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Finetuning</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In addition to testing off-the-shelf API-based and open-source models, we also fine-tune Mistral-7B to provide a much stronger baseline. Table <a href="#S4.T3" title="Table 3 ‣ 4.1 Zero-shot ‣ 4 Results ‣ Cross-Lingual Conversational Speech Summarization with Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> includes results from a fine-tuned model that is trained with both reference and MT of ASR inputs.
Compared to the unadapted version, fine-tuning improves the performance of the Mistral-7B model by almost 10 ROUGE points.
After fine-tuning, the performance of the model is comparable to, or even outperforms, GPT-4.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">In order to determine the value of including both the reference transcript and MT of ASR inputs in fine-tuning, we run a set of experiments in which we vary our fine-tuning dataset. Table <a href="#S4.T4" title="Table 4 ‣ 4.1 Zero-shot ‣ 4 Results ‣ Cross-Lingual Conversational Speech Summarization with Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows performance, aggregated across datasets, when we fine-tune on only the reference transcripts, only the MT of ASR transcripts, and the combination of both. We also experiment with a fourth condition in which we include both the reference transcripts and MT of ASR transcripts, but use a separate <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_italic">source prompt</span> during fine-tuning and inference for the two. The intuition behind this experiment is that there are likely different error distributions between the two types of inputs and it might help the LLM to signal what type of input is being provided.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">We find that ROUGE scores are relatively flat across different fine-tuning sets. Fine-tuning on reference transcripts does result in the model that performs best at summarizing reference transcripts, although the differences are small. The same cannot be said for testing on MT of ASR outputs, where the inclusion of MT of ASR data in the train set does not reliably yield an improvement in ROUGE. This is potentially reflective of the fact that even before fine-tuning models seemed very robust to MT and ASR errors. In Table <a href="#S4.T5" title="Table 5 ‣ 4.1 Zero-shot ‣ 4 Results ‣ Cross-Lingual Conversational Speech Summarization with Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> we show how performance varies as we vary the amount of fine-tuning data. We see a roughly linear increase in ROUGE as the amount of data increases.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">In Table <a href="#S4.T6" title="Table 6 ‣ 4.1 Zero-shot ‣ 4 Results ‣ Cross-Lingual Conversational Speech Summarization with Large Language Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> we show an example summary using GPT-4 where the input comes either from reference translations or ASR+MT.
The second summary contains several phrases highlighted in red.
We can trace these back to errors in either the ASR or MT.
The statements ```so poor' and `comotose.'" likely arise from a combination of errors and the true statement should be, ``You eat all you earn."
When the summmary mentions Speaker B offering ``mundane responses" and saying, ``Thank you, I'm all right," it is a reflection of hallucinations in both the ASR and the MT.
The Whisper model tends to output ``Gracias" as a filler word and the NLLB model translates it as ``Thank you, I'm all right."
Interestingly, this demonstrates that errors in ASR+MT not only impact the factual information in the summary, but also the implied tone.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We have established an evaluation framework for CTS summarization.
Using GPT-4, we created reference summaries for a well-known Spanish CTS corpus with existing English translations.
Our experiments establish a baseline for a cascaded approach to summarization using publicly available models.While GPT-4 outperforms existing open-source models, we are able to match the performance of GPT-4 by fine-tuning the Mistral-7B model.
This demonstrates the efficacy of using large, API-based models like GPT-4 to generate evaluation and adaptation data for cross-lingual speech summarization.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">We plan to explore several extensions to this work in the future.
Moving beyond general summarization, we want to explore contextual summarization where the summary can be guided by input from the user to focus on specific information.
This presents further challenges, not just as a task, but also in terms of evaluation.
We also want to incorporate additional information to the summarization system based on alternative hypotheses for both transcription and translation.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. McKeown, and T. B. Hashimoto, ``Benchmarking large language models for news summarization,'' <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, vol. 12, pp. 39–57, 2024.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
R. Valenza, T. Robinson, M. Hickey, and R. Tucker, ``Summarisation of spoken audio through information extraction,'' in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">ESCA Tutorial and Research Workshop (ETRW) on Accessing Information in Spoken Audio</em>, 1999.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
K. Koumpis and S. Renals, ``Transcription and summarization of voicemail speech,'' in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">ICSLP</em>.   International Speech Communication Association, 2000.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
C. Hori, S. Furui, R. Malkin, H. Yu, and A. Waibel, ``Automatic summarization of english broadcast news speech,'' in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the second international conference on Human Language Technology Research</em>, 2002, pp. 241–246.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
G. Murray, S. Renals, and J. Carletta, ``Extractive summarization of meeting recordings,'' in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2005</em>, 2005, pp. 593–596.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
R. Sharma, W. Chen, T. Kano, R. Sharma, S. Arora, S. Watanabe, A. Ogawa, M. Delcroix, R. Singh, and B. Raj, ``Espnet-summ: Introducing a novel large dataset, toolkit, and a cross-corpora evaluation of speech summarization systems,'' in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.   IEEE, 2023, pp. 1–8.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y. Zou, L. Zhao, Y. Kang, J. Lin, M. Peng, Z. Jiang, C. Sun, Q. Zhang, X. Huang, and X. Liu, ``Topic-oriented spoken dialogue summarization for customer service with saliency-aware topic modeling,'' in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 35, no. 16, 2021, pp. 14 665–14 673.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S. Furui, T. Kikuchi, Y. Shinnaka, and C. Hori, ``Speech-to-text and speech-to-speech summarization of spontaneous speech,'' <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Speech and Audio Processing</em>, vol. 12, no. 4, pp. 401–408, 2004.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
T. Kano, A. Ogawa, M. Delcroix, K. Matsuura, T. Ashihara, W. Chen, and S. Watanabe, ``Summarize while translating: Universal model with parallel decoding for summarization and translation,'' in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.   IEEE, 2023, pp. 1–8.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
F. Liu and Y. Liu, ``Towards abstractive speech summarization: Exploring unsupervised and supervised approaches for spoken utterance compression,'' <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Audio, Speech, and Language Processing</em>, vol. 21, no. 7, pp. 1469–1480, 2013.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
M. Post, G. Kumar, A. Lopez, D. Karakos, C. Callison-Burch, and S. Khudanpur, ``Improved speech-to-text translation with the fisher and callhome spanish–english speech translation corpus,'' in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proc. IWSLT</em>, 2013.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, ``Lora: Low-rank adaptation of large language models,'' in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, ``Robust speech recognition via large-scale weak supervision,'' in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2023, pp. 28 492–28 518.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. R. Costa-jussà, J. Cross, O. Çelebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``No language left behind: Scaling human-centered machine translation,'' <em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2207.04672</em>, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Training language models to follow instructions with human feedback,'' 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Llama 2: Open foundation and fine-tuned chat models,'' <em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Mistral 7b,'' <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.06825</em>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Mixtral of experts,'' <em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.04088</em>, 2024.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Chain-of-thought prompting elicits reasoning in large language models,'' <em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 35, pp. 24 824–24 837, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
C.-Y. Lin and F. J. Och, ``Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics,'' in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 42nd annual meeting of the association for computational linguistics (ACL-04)</em>, 2004, pp. 605–612.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. R. Fabbri, W. Kryściński, B. McCann, C. Xiong, R. Socher, and D. Radev, ``Summeval: Re-evaluating summarization evaluation,'' <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>, vol. 9, pp. 391–409, 2021.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.06483" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.06484" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.06484">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.06484" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.06485" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 15:15:16 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
