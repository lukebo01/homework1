<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.05916] GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing</title><meta property="og:description" content="Multimodal language models (MLMs) are designed to process and integrate information from multiple sources, such as text, speech, images, and videos. Despite its success in language understanding, it is critical to eval…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.05916">

<!--Generated on Fri Apr  5 16:21:58 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hao LU<sup id="id26.26.id1" class="ltx_sup"><span id="id26.26.id1.1" class="ltx_text ltx_font_italic">1,2,</span></sup><math id="id2.2.m2.1" class="ltx_Math" alttext="{}^{\ ,}" display="inline"><semantics id="id2.2.m2.1a"><msup id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mi id="id2.2.m2.1.1a" xref="id2.2.m2.1.1.cmml"></mi><mo id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml">,</mo></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><ci id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1">,</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">{}^{\ ,}</annotation></semantics></math>  , Xuesong NIU<sup id="id27.27.id2" class="ltx_sup"><span id="id27.27.id2.1" class="ltx_text ltx_font_italic">3,∗,†</span></sup>, Jiyao WANG<sup id="id28.28.id3" class="ltx_sup"><span id="id28.28.id3.1" class="ltx_text ltx_font_italic">1,2,∗</span></sup>, Yin WANG<sup id="id29.29.id4" class="ltx_sup"><span id="id29.29.id4.1" class="ltx_text ltx_font_italic">4,∗</span></sup>, Qingyong HU<sup id="id30.30.id5" class="ltx_sup"><span id="id30.30.id5.1" class="ltx_text ltx_font_italic">2,∗</span></sup>, Jiaqi TANG<sup id="id31.31.id6" class="ltx_sup"><span id="id31.31.id6.1" class="ltx_text ltx_font_italic">1,2,∗</span></sup>, 
<br class="ltx_break">Yuting ZHANG<sup id="id32.32.id7" class="ltx_sup"><span id="id32.32.id7.1" class="ltx_text ltx_font_italic">1,2</span></sup>, Kaishen YUAN<sup id="id33.33.id8" class="ltx_sup"><span id="id33.33.id8.1" class="ltx_text ltx_font_italic">5</span></sup>, Bin HUANG<sup id="id34.34.id9" class="ltx_sup"><span id="id34.34.id9.1" class="ltx_text ltx_font_italic">6</span></sup>, Zitong YU<sup id="id35.35.id10" class="ltx_sup"><span id="id35.35.id10.1" class="ltx_text ltx_font_italic">5,</span></sup>  , Dengbo HE<sup id="id36.36.id11" class="ltx_sup"><span id="id36.36.id11.1" class="ltx_text ltx_font_italic">1,2</span></sup>,
Shuiguang DENG<sup id="id37.37.id12" class="ltx_sup"><span id="id37.37.id12.1" class="ltx_text ltx_font_italic">1,2</span></sup>,
<br class="ltx_break">Hao CHEN<sup id="id38.38.id13" class="ltx_sup"><span id="id38.38.id13.1" class="ltx_text ltx_font_italic">2</span></sup>, Yingcong CHEN<sup id="id39.39.id14" class="ltx_sup"><span id="id39.39.id14.1" class="ltx_text ltx_font_italic">1,2,‡</span></sup>, Shiguang SHAN<sup id="id40.40.id15" class="ltx_sup"><span id="id40.40.id15.1" class="ltx_text ltx_font_italic">7</span></sup>
<br class="ltx_break"><sup id="id41.41.id16" class="ltx_sup"><span id="id41.41.id16.1" class="ltx_text ltx_font_italic">1</span></sup>The Hong Kong University of Science &amp; Technology (Guangzhou), 
<br class="ltx_break"><sup id="id42.42.id17" class="ltx_sup"><span id="id42.42.id17.1" class="ltx_text ltx_font_italic">2</span></sup>The Hong Kong University of Science &amp; Technology,
<sup id="id43.43.id18" class="ltx_sup"><span id="id43.43.id18.1" class="ltx_text ltx_font_italic">3</span></sup>Beijing Institute for General Artificial Intelligence, 
<br class="ltx_break"><sup id="id44.44.id19" class="ltx_sup"><span id="id44.44.id19.1" class="ltx_text ltx_font_italic">4</span></sup>Zhejiang University, <sup id="id45.45.id20" class="ltx_sup"><span id="id45.45.id20.1" class="ltx_text ltx_font_italic">5</span></sup>Great Bay University, <sup id="id46.46.id21" class="ltx_sup"><span id="id46.46.id21.1" class="ltx_text ltx_font_italic">6</span></sup>Hangzhou Research Institute, Beihang University,
<br class="ltx_break"><sup id="id47.47.id22" class="ltx_sup"><span id="id47.47.id22.1" class="ltx_text ltx_font_italic">7</span></sup>Institute of Computing Technology, Chinese Academy of Sciences
</span><span class="ltx_author_notes">Equal contribution.Project Leader.Corresponding author.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id48.id1" class="ltx_p">Multimodal language models (MLMs) are designed to process and integrate information from multiple sources, such as text, speech, images, and videos. Despite its success in language understanding, it is critical to evaluate the performance of downstream tasks for better human-centric applications. This paper assesses the application of MLMs with 5 crucial abilities for affective computing, spanning from visual affective tasks and reasoning tasks. The results show that GPT4 has high accuracy in facial action unit recognition and micro-expression detection while its general facial expression recognition performance is not accurate. We also highlight the challenges of achieving fine-grained micro-expression recognition and the potential for further study and demonstrate the versatility and potential of GPT4 for handling advanced tasks in emotion recognition and related fields by integrating with task-related agents for more complex tasks, such as heart rate estimation through signal processing. In conclusion, this paper provides valuable insights into the potential applications and challenges of MLMs in human-centric computing. The interesting samples are available at <a target="_blank" href="https://github.com/LuPaoPao/GPT4Affectivity" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/LuPaoPao/GPT4Affectivity</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The development of multimodal language models (MLMs) has been a topic of growing interest in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>. MLMs are designed to process and integrate information from multiple modalities, such as text, speech, images, and videos. The development of these models has been driven by the need to improve the accuracy and efficiency of various tasks, such as affective computing, sentiment analysis, and natural language understanding.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">MLMs have shown great promise in improving the accuracy and robustness of affective computing systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>. These models can process and integrate information from multiple modalities, such as facial expressions, speech patterns, and physiological signals, to infer emotional states accurately <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> with significant implications for various applications, such as healthcare, education, and human-computer interaction.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2403.05916/assets/figure/Emotion.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="216" height="123" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.4.2" class="ltx_text" style="font-size:90%;">The propaganda image was generated by Dell2.</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Despite the rapid development of MLMs, the need for standardized evaluation metrics is highlighted for accurate assessment.
Different from the general language understanding evaluation benchmark that has been widely used to evaluate the performance of language models in NLP tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>, <a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite>, similar benchmarks to evaluate the performance of MLMs for affective computing tasks are lacked, which is of great benefit to advance this field.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">GPT4-V is a state-of-the-art MLM that has shown remarkable success in various natural language processing tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>. Its ability to process and integrate information from multiple modalities makes it an ideal candidate for evaluating the performance of MLMs in tasks for affective computing. Furthermore, it can invoke a variety of tools that benefit affective computing tasks, such as related program generation with self-correction. For example, GPT4-V can call Dell to generate high-quality visual affective images shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this paper, we evaluate GPT4-V with 5 typical human-centric tasks, spanning from visual affective tasks and reasoning tasks. We summarize our findings as follows:</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">(1) GPT4-V is highly accurate in recognizing facial action units. This accuracy can be attributed to its advanced understanding of facial movements and their corresponding emotions, which allows it to effectively identify and analyze facial action units.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">(2) GPT4-V is also precise in detecting micro-expressions. Its ability to process subtle and transient facial expressions enables it to accurately capture these fleeting emotional cues, which are often difficult for humans to perceive.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">(3) GPT4-V’s performance in general facial expression recognition is not as accurate. This limitation may be due to the complexity and variety of facial expressions, resulting in the challenges in capturing and analyzing them. Nevertheless, when GPT4-V is used to process thought chains, its accuracy in facial expression recognition improves significantly. This improvement suggests that incorporating additional contextual information is of great importance to recognize facial expressions.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">(4) Achieving high accuracy in micro-expression recognition remains a challenging task. This difficulty arises from the transient nature of micro-expressions and the need to detect and classify them within a very short time period. These challenges call for continuing research and development in this area for improving affective computing</p>
</div>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">(5) GPT4-V can also integrate with task-related agents to handle more complex tasks, such as detecting subtle facial changes and estimating heart rate with signal processing. By leveraging Python’s powerful libraries and tools, GPT4-V can effectively process and analyze intricate facial data to derive valuable insights, such as heart rate estimation, which can further enhance its applications in mental health monitoring and virtual human companion systems.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Visual Affective Evaluation</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Affective computing emerges as an interdisciplinary domain, leveraging computational technologies to discern, comprehend, and emulate human emotions. Its objective is to augment human-computer interaction, enhance user experiences, and facilitate improved communication and self-expression. Within the scope of computer vision, the analysis of human facial units <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>, expressions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>, micro-expressions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite>, micro-gestures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>, and deception detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>, alongside physiological measurements <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>, are pivotal to advancing emotional computing. Notably, large-scale pre-trained models, such as GPT-4V, have demonstrated substantial advancements in natural language processing, suggesting their considerable promise for application in affective computing. This study proposes to scrutinize the efficacy of GPT-4V across a variety of tasks, employing methodologies that include iterative conversations, open-ended inquiries, as well as multiple-choice and true/false questions.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Action Unit Detection</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The Facial Action Coding System (FACS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> offers an explainable and reliable framework for the analysis of human facial expressions. It systematically deconstructs facial expressions into discrete components, known as Action Units (AUs), which correspond to the activation of specific facial muscles or groups thereof. Through the identification and quantification of these AUs, researchers can conduct a methodical examination of facial expressions and the emotional states they signify. Our assessment of GPT-4V’s performance on the DISFA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, utilizing a gamut of question types, underscores its proficiency in accurately identifying AUs, thereby enabling precise emotion recognition from minimal interaction.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Remarkably, GPT-4V exhibits exceptional accuracy in AU identification, facilitating nearly flawless judgment across all AUs examined as shown in Tab. <a href="#S2.T1" title="Table 1 ‣ 2.1 Action Unit Detection ‣ 2 Visual Affective Evaluation ‣ GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Although our presentation includes a limited number of examples, our comprehensive evaluation reveals GPT-4V’s surprising efficacy in this domain. To quantitatively appraise this performance, we adopted a quantitative analysis approach, benchmarking against the F1 metrics as reported in related studies.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>, we report F1 metrics on DISFA. Specifically, we judge whether the recognition is successful by searching whether there is AU<math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mi id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">X</annotation></semantics></math> (such as AU1) keyword in the reply question. The results show that the performance of GPT-4V is stronger than that of later professional models. This shows that GPT-4V has learned the micro-characteristics of emotion in a large number of network data and achieved significant recognition accuracy. Our findings indicate that GPT-4V’s performance surpasses that of subsequent specialized models, underscoring its adeptness at learning the nuanced characteristics of emotion through extensive analysis of online data, thus achieving remarkable accuracy in emotion recognition.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2403.05916/assets/x1.png" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="390" height="167" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.4.2" class="ltx_text" style="font-size:90%;">Action Unit detection on DISFA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> dataset. We use the single round for the action unit. GPT-4V can accurately identify each AU.</span></figcaption>
</figure>
<figure id="S2.T1" class="ltx_table">
<div id="S2.T1.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:40.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-745.2pt,69.7pt) scale(0.225362325173362,0.225362325173362) ;">
<table id="S2.T1.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.2.1.1.1" class="ltx_tr">
<th id="S2.T1.2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">AU</th>
<th id="S2.T1.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">DRML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>
</th>
<th id="S2.T1.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">DSIN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>
</th>
<th id="S2.T1.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">LP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>
</th>
<th id="S2.T1.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">SRERL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>
</th>
<th id="S2.T1.2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">EAC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>
</th>
<th id="S2.T1.2.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">JAA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>
</th>
<th id="S2.T1.2.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ARL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>
</th>
<th id="S2.T1.2.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">FAUDT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>
</th>
<th id="S2.T1.2.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">PIAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite>
</th>
<th id="S2.T1.2.1.1.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ME-GraphAU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>
</th>
<th id="S2.T1.2.1.1.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">BG-AU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>
</th>
<th id="S2.T1.2.1.1.1.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MPSCL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>
</th>
<th id="S2.T1.2.1.1.1.14" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">GPT-4V</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.2.1.2.1" class="ltx_tr">
<th id="S2.T1.2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">1</th>
<td id="S2.T1.2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">17.3</td>
<td id="S2.T1.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">42.4</td>
<td id="S2.T1.2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">29.9</td>
<td id="S2.T1.2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">45.7</td>
<td id="S2.T1.2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">41.5</td>
<td id="S2.T1.2.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">43.7</td>
<td id="S2.T1.2.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t">43.9</td>
<td id="S2.T1.2.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t">46.1</td>
<td id="S2.T1.2.1.2.1.10" class="ltx_td ltx_align_center ltx_border_t">50.2</td>
<td id="S2.T1.2.1.2.1.11" class="ltx_td ltx_align_center ltx_border_t">52.5</td>
<td id="S2.T1.2.1.2.1.12" class="ltx_td ltx_align_center ltx_border_t">41.5</td>
<td id="S2.T1.2.1.2.1.13" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.2.1.2.1.13.1" class="ltx_text ltx_font_bold">62.0</span></td>
<td id="S2.T1.2.1.2.1.14" class="ltx_td ltx_align_center ltx_border_t">52.6</td>
</tr>
<tr id="S2.T1.2.1.3.2" class="ltx_tr">
<th id="S2.T1.2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">2</th>
<td id="S2.T1.2.1.3.2.2" class="ltx_td ltx_align_center">17.7</td>
<td id="S2.T1.2.1.3.2.3" class="ltx_td ltx_align_center">39.0</td>
<td id="S2.T1.2.1.3.2.4" class="ltx_td ltx_align_center">24.7</td>
<td id="S2.T1.2.1.3.2.5" class="ltx_td ltx_align_center">47.8</td>
<td id="S2.T1.2.1.3.2.6" class="ltx_td ltx_align_center">26.4</td>
<td id="S2.T1.2.1.3.2.7" class="ltx_td ltx_align_center">46.2</td>
<td id="S2.T1.2.1.3.2.8" class="ltx_td ltx_align_center">42.1</td>
<td id="S2.T1.2.1.3.2.9" class="ltx_td ltx_align_center">48.6</td>
<td id="S2.T1.2.1.3.2.10" class="ltx_td ltx_align_center">51.8</td>
<td id="S2.T1.2.1.3.2.11" class="ltx_td ltx_align_center">45.7</td>
<td id="S2.T1.2.1.3.2.12" class="ltx_td ltx_align_center">44.9</td>
<td id="S2.T1.2.1.3.2.13" class="ltx_td ltx_align_center"><span id="S2.T1.2.1.3.2.13.1" class="ltx_text ltx_font_bold">65.7</span></td>
<td id="S2.T1.2.1.3.2.14" class="ltx_td ltx_align_center">56.4</td>
</tr>
<tr id="S2.T1.2.1.4.3" class="ltx_tr">
<th id="S2.T1.2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">4</th>
<td id="S2.T1.2.1.4.3.2" class="ltx_td ltx_align_center">37.4</td>
<td id="S2.T1.2.1.4.3.3" class="ltx_td ltx_align_center">68.4</td>
<td id="S2.T1.2.1.4.3.4" class="ltx_td ltx_align_center">72.7</td>
<td id="S2.T1.2.1.4.3.5" class="ltx_td ltx_align_center">59.6</td>
<td id="S2.T1.2.1.4.3.6" class="ltx_td ltx_align_center">66.4</td>
<td id="S2.T1.2.1.4.3.7" class="ltx_td ltx_align_center">56.0</td>
<td id="S2.T1.2.1.4.3.8" class="ltx_td ltx_align_center">63.6</td>
<td id="S2.T1.2.1.4.3.9" class="ltx_td ltx_align_center">72.8</td>
<td id="S2.T1.2.1.4.3.10" class="ltx_td ltx_align_center">71.9</td>
<td id="S2.T1.2.1.4.3.11" class="ltx_td ltx_align_center">76.1</td>
<td id="S2.T1.2.1.4.3.12" class="ltx_td ltx_align_center">60.3</td>
<td id="S2.T1.2.1.4.3.13" class="ltx_td ltx_align_center">74.5</td>
<td id="S2.T1.2.1.4.3.14" class="ltx_td ltx_align_center"><span id="S2.T1.2.1.4.3.14.1" class="ltx_text ltx_font_bold">82.9</span></td>
</tr>
<tr id="S2.T1.2.1.5.4" class="ltx_tr">
<th id="S2.T1.2.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">6</th>
<td id="S2.T1.2.1.5.4.2" class="ltx_td ltx_align_center">29.0</td>
<td id="S2.T1.2.1.5.4.3" class="ltx_td ltx_align_center">28.6</td>
<td id="S2.T1.2.1.5.4.4" class="ltx_td ltx_align_center">46.8</td>
<td id="S2.T1.2.1.5.4.5" class="ltx_td ltx_align_center">47.1</td>
<td id="S2.T1.2.1.5.4.6" class="ltx_td ltx_align_center">50.7</td>
<td id="S2.T1.2.1.5.4.7" class="ltx_td ltx_align_center">41.4</td>
<td id="S2.T1.2.1.5.4.8" class="ltx_td ltx_align_center">41.8</td>
<td id="S2.T1.2.1.5.4.9" class="ltx_td ltx_align_center">56.7</td>
<td id="S2.T1.2.1.5.4.10" class="ltx_td ltx_align_center">50.6</td>
<td id="S2.T1.2.1.5.4.11" class="ltx_td ltx_align_center">51.8</td>
<td id="S2.T1.2.1.5.4.12" class="ltx_td ltx_align_center">51.5</td>
<td id="S2.T1.2.1.5.4.13" class="ltx_td ltx_align_center">53.2</td>
<td id="S2.T1.2.1.5.4.14" class="ltx_td ltx_align_center"><span id="S2.T1.2.1.5.4.14.1" class="ltx_text ltx_font_bold">64.3</span></td>
</tr>
<tr id="S2.T1.2.1.6.5" class="ltx_tr">
<th id="S2.T1.2.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">9</th>
<td id="S2.T1.2.1.6.5.2" class="ltx_td ltx_align_center">10.7</td>
<td id="S2.T1.2.1.6.5.3" class="ltx_td ltx_align_center">46.8</td>
<td id="S2.T1.2.1.6.5.4" class="ltx_td ltx_align_center">49.6</td>
<td id="S2.T1.2.1.6.5.5" class="ltx_td ltx_align_center">45.6</td>
<td id="S2.T1.2.1.6.5.6" class="ltx_td ltx_align_center"><span id="S2.T1.2.1.6.5.6.1" class="ltx_text ltx_font_bold">80.5</span></td>
<td id="S2.T1.2.1.6.5.7" class="ltx_td ltx_align_center">44.7</td>
<td id="S2.T1.2.1.6.5.8" class="ltx_td ltx_align_center">40.0</td>
<td id="S2.T1.2.1.6.5.9" class="ltx_td ltx_align_center">50.0</td>
<td id="S2.T1.2.1.6.5.10" class="ltx_td ltx_align_center">54.5</td>
<td id="S2.T1.2.1.6.5.11" class="ltx_td ltx_align_center">46.5</td>
<td id="S2.T1.2.1.6.5.12" class="ltx_td ltx_align_center">50.3</td>
<td id="S2.T1.2.1.6.5.13" class="ltx_td ltx_align_center">43.1</td>
<td id="S2.T1.2.1.6.5.14" class="ltx_td ltx_align_center">55.3</td>
</tr>
<tr id="S2.T1.2.1.7.6" class="ltx_tr">
<th id="S2.T1.2.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">12</th>
<td id="S2.T1.2.1.7.6.2" class="ltx_td ltx_align_center">37.7</td>
<td id="S2.T1.2.1.7.6.3" class="ltx_td ltx_align_center">70.8</td>
<td id="S2.T1.2.1.7.6.4" class="ltx_td ltx_align_center">72.9</td>
<td id="S2.T1.2.1.7.6.5" class="ltx_td ltx_align_center">73.5</td>
<td id="S2.T1.2.1.7.6.6" class="ltx_td ltx_align_center"><span id="S2.T1.2.1.7.6.6.1" class="ltx_text ltx_font_bold">89.3</span></td>
<td id="S2.T1.2.1.7.6.7" class="ltx_td ltx_align_center">69.6</td>
<td id="S2.T1.2.1.7.6.8" class="ltx_td ltx_align_center">76.2</td>
<td id="S2.T1.2.1.7.6.9" class="ltx_td ltx_align_center">72.1</td>
<td id="S2.T1.2.1.7.6.10" class="ltx_td ltx_align_center">79.7</td>
<td id="S2.T1.2.1.7.6.11" class="ltx_td ltx_align_center">76.1</td>
<td id="S2.T1.2.1.7.6.12" class="ltx_td ltx_align_center">70.4</td>
<td id="S2.T1.2.1.7.6.13" class="ltx_td ltx_align_center">76.9</td>
<td id="S2.T1.2.1.7.6.14" class="ltx_td ltx_align_center">75.4</td>
</tr>
<tr id="S2.T1.2.1.8.7" class="ltx_tr">
<th id="S2.T1.2.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">25</th>
<td id="S2.T1.2.1.8.7.2" class="ltx_td ltx_align_center">38.5</td>
<td id="S2.T1.2.1.8.7.3" class="ltx_td ltx_align_center">90.4</td>
<td id="S2.T1.2.1.8.7.4" class="ltx_td ltx_align_center">93.8</td>
<td id="S2.T1.2.1.8.7.5" class="ltx_td ltx_align_center">84.3</td>
<td id="S2.T1.2.1.8.7.6" class="ltx_td ltx_align_center">88.9</td>
<td id="S2.T1.2.1.8.7.7" class="ltx_td ltx_align_center">88.3</td>
<td id="S2.T1.2.1.8.7.8" class="ltx_td ltx_align_center"><span id="S2.T1.2.1.8.7.8.1" class="ltx_text ltx_font_bold">95.2</span></td>
<td id="S2.T1.2.1.8.7.9" class="ltx_td ltx_align_center">90.8</td>
<td id="S2.T1.2.1.8.7.10" class="ltx_td ltx_align_center">94.1</td>
<td id="S2.T1.2.1.8.7.11" class="ltx_td ltx_align_center">92.9</td>
<td id="S2.T1.2.1.8.7.12" class="ltx_td ltx_align_center">91.3</td>
<td id="S2.T1.2.1.8.7.13" class="ltx_td ltx_align_center">95.6</td>
<td id="S2.T1.2.1.8.7.14" class="ltx_td ltx_align_center">91.2</td>
</tr>
<tr id="S2.T1.2.1.9.8" class="ltx_tr">
<th id="S2.T1.2.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">26</th>
<td id="S2.T1.2.1.9.8.2" class="ltx_td ltx_align_center">20.1</td>
<td id="S2.T1.2.1.9.8.3" class="ltx_td ltx_align_center">42.2</td>
<td id="S2.T1.2.1.9.8.4" class="ltx_td ltx_align_center">65.0</td>
<td id="S2.T1.2.1.9.8.5" class="ltx_td ltx_align_center">43.6</td>
<td id="S2.T1.2.1.9.8.6" class="ltx_td ltx_align_center">15.6</td>
<td id="S2.T1.2.1.9.8.7" class="ltx_td ltx_align_center">58.4</td>
<td id="S2.T1.2.1.9.8.8" class="ltx_td ltx_align_center"><span id="S2.T1.2.1.9.8.8.1" class="ltx_text ltx_font_bold">66.8</span></td>
<td id="S2.T1.2.1.9.8.9" class="ltx_td ltx_align_center">55.4</td>
<td id="S2.T1.2.1.9.8.10" class="ltx_td ltx_align_center">57.2</td>
<td id="S2.T1.2.1.9.8.11" class="ltx_td ltx_align_center">57.6</td>
<td id="S2.T1.2.1.9.8.12" class="ltx_td ltx_align_center">55.3</td>
<td id="S2.T1.2.1.9.8.13" class="ltx_td ltx_align_center">53.1</td>
<td id="S2.T1.2.1.9.8.14" class="ltx_td ltx_align_center">66.4</td>
</tr>
<tr id="S2.T1.2.1.10.9" class="ltx_tr">
<th id="S2.T1.2.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t">Avg.</th>
<td id="S2.T1.2.1.10.9.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">26.7</td>
<td id="S2.T1.2.1.10.9.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">53.6</td>
<td id="S2.T1.2.1.10.9.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">56.9</td>
<td id="S2.T1.2.1.10.9.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">55.9</td>
<td id="S2.T1.2.1.10.9.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">48.5</td>
<td id="S2.T1.2.1.10.9.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">56.0</td>
<td id="S2.T1.2.1.10.9.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">58.7</td>
<td id="S2.T1.2.1.10.9.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">61.5</td>
<td id="S2.T1.2.1.10.9.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">63.8</td>
<td id="S2.T1.2.1.10.9.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">62.4</td>
<td id="S2.T1.2.1.10.9.12" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">58.2</td>
<td id="S2.T1.2.1.10.9.13" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">65.5</td>
<td id="S2.T1.2.1.10.9.14" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S2.T1.2.1.10.9.14.1" class="ltx_text ltx_font_bold">67.3</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.4.2" class="ltx_text" style="font-size:90%;">Comparison with state-of-the-art methods for AU detection on DISFA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> dataset using the F1-score metric (in %).</span></figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Expression Recognition</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The facial expression recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite> task involves identifying and analyzing human facial expressions to determine emotions. This task plays a crucial role in understanding human emotions, enhancing communication, and improving mental health monitoring and virtual human companion systems. It can be challenging due to the complexity and variety of facial expressions, as well as the need to detect and classify subtle and transient expressions accurately. For this reason, we qualitatively analyze the performance of GPT-4V for emotion recognition on RAF-DB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite> dataset. Our methodology encompassed a multifaceted approach, employing iterative dialogues, open-ended questions, multiple-choice queries, and true/false assessments, specifically utilizing the CASME2 dataset as a basis for evaluation. Contrary to expectations, preliminary results indicate that GPT-4V exhibits limitations in accurately responding to even basic true/false questions related to emotion recognition, as depicted in the referenced figure shown in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.2 Expression Recognition ‣ 2 Visual Affective Evaluation ‣ GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">As shown in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.2 Expression Recognition ‣ 2 Visual Affective Evaluation ‣ GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, natural emotions are thought to have no obvious characteristics, as soon as we pass a form of judgment question. For the emotion of Fear, GPT-4V thinks that the emotion is natural and cannot give the decision of fear. This is because emotions are inherently difficult to recognize without context, which is not considered an objective task. Therefore, GPT-4V cannot achieve good performance on these subjective tasks. This finding highlights a significant limitation in the application of advanced language models like GPT-4V for the nuanced task of emotion recognition. It suggests that while such models possess remarkable capabilities in various domains of natural language processing, their effectiveness in interpreting human emotions through facial expressions, especially in the absence of contextual information, remains constrained. The subjective nature of emotional expression, coupled with the subtleties and variations inherent in human facial expressions, necessitates a more sophisticated approach that incorporates contextual understanding and perhaps multimodal inputs that extend beyond textual analysis.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2403.05916/assets/x2.png" id="S2.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="392" height="168" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.4.2" class="ltx_text" style="font-size:90%;">Expression recognition on RAF-DB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite> dataset. GPT-4V cannot achieve good performance on the subjective task of emotion recognition.</span></figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Compound Emotion Recognition</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The task of compound emotion recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> extends beyond the scope of simple emotion recognition by necessitating the identification and analysis of multiple emotions simultaneously exhibited through human facial expressions. The complexity of this task is amplified by the requirement to accurately detect and classify a spectrum of emotions, which may often be overlapping or present ambiguous signals. It can be more challenging than simple emotion recognition due to the need to detect and classify multiple emotions accurately, as well as the potential for conflicting or ambiguous expressions. In our continued exploration of GPT-4V’s capabilities, we extend our assessment to include the recognition of compound emotions.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">As shown in Fig. <a href="#S2.F4" title="Figure 4 ‣ 2.3 Compound Emotion Recognition ‣ 2 Visual Affective Evaluation ‣ GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we qualitatively analyze the performance of GPT-4V for compound emotion recognition on RAF-DB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite> dataset and find that compound expressions can even be recognized. Even compound expressions are recognized more accurately than individual expressions. This does not mean GPT-4V is more accurate for compound than individual expressions. Instead, this is because the data of this compound expression is relatively more objective, and GPT-4V has an accurate judgment of this objective expression. This revelation underscores the importance of developing computational models that can navigate the intricacies of human emotions with a high degree of sensitivity and accuracy. For applications in mental health monitoring and virtual companionship, paving the way for innovations in emotional AI that can more closely mimic human empathetic and cognitive processes.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2403.05916/assets/x3.png" id="S2.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="393" height="164" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S2.F4.4.2" class="ltx_text" style="font-size:90%;">Compound emotion recognition on RAF-DB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite> dataset. GPT-4V can deduce objective compound expressions based on contextual information.</span></figcaption>
</figure>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Micro-expression Recognition</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">The domain of micro-expression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>]</cite> research within emotion recognition is characterized by the endeavor to identify and interpret subtle, fleeting expressions that manifest on the human face. These micro-expressions, often resulting from rapid emotional shifts or attempts to conceal emotions, are particularly ephemeral, lasting only between 1/25 to 1/5 of a second. This attribute renders micro-expressions both a fascinating and formidable area of study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>. However, the transient and elusive nature of micro-expressions presents significant challenges, notably in their detection and accurate interpretation. In our investigation, we meticulously crafted cue words and deployed a series of experimental setups involving judgment questions, multiple-choice inquiries, and iterative dialogues, all facilitated based on the CASME2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> through the GPT-4V platform. This approach aimed to explore the potential of GPT-4V in recognizing and interpreting micro-expressions within the constraints of textual communication.</p>
</div>
<figure id="S2.F5" class="ltx_figure"><img src="/html/2403.05916/assets/x4.png" id="S2.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="183" height="283" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S2.F5.4.2" class="ltx_text" style="font-size:90%;">Micro-expression recognition on the CASME2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> dataset. GPT-4V has difficulty understanding the small differences in the image directly, so it is difficult to understand the micro facial expressions accurately.</span></figcaption>
</figure>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">As shown in Fig. <a href="#S2.F5" title="Figure 5 ‣ 2.4 Micro-expression Recognition ‣ 2 Visual Affective Evaluation ‣ GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, GPT-4V did not answer the provided micro-expression test samples satisfactorily. GPT-4V cannot understand the difference between frames, and the difference is not visible to the human eye. We tried to amplify this difference, but GPT-4V thought the enlarged image was blurry, so GPT-4V was very weak on the microexpression task.</p>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Micro-gesture Recognition</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">Micro-gesture recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> tasks focus on recognizing and analyzing small, imperceptible body movements and facial expressions produced by people in specific scenarios, which usually represent an individual’s inner emotions, attitudes, or cognitive responses. Micro-gesture recognition techniques are valuable for many applications, such as emotion recognition, negotiation, police interrogation, and mental health assessment. The core challenge of this technology is to capture brief and subtle changes in movements that are difficult for individuals to control due to their association with the autonomic nervous system. Micro-gesture recognition improves social interactions and communication by helping people better understand others’ emotions and motivations. We carefully designed the cue words and tested several different micro-gesture sequences on judgment questions, multiple-choice questions, and multi-round conversations using GPT-4V.</p>
</div>
<figure id="S2.F6" class="ltx_figure"><img src="/html/2403.05916/assets/x5.png" id="S2.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="147" height="192" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S2.F6.4.2" class="ltx_text" style="font-size:90%;">Micro-gesture (rubbing eyes) recognition on iMiGUE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> dataset. GPT-4V can recognize obvious movements.</span></figcaption>
</figure>
<figure id="S2.F7" class="ltx_figure"><img src="/html/2403.05916/assets/x6.png" id="S2.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="183" height="261" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S2.F7.4.2" class="ltx_text" style="font-size:90%;">Micro-gesture (shaking double shoulders) recognition on iMiGUE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> dataset. GPT-4V cannot recognize subtle movements.</span></figcaption>
</figure>
<div id="S2.SS5.p2" class="ltx_para">
<p id="S2.SS5.p2.1" class="ltx_p">We qualitatively analyze the performance of GPT-4V for micro-gesture recognition on iMiGUE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> dataset. As shown in Fig. <a href="#S2.F6" title="Figure 6 ‣ 2.5 Micro-gesture Recognition ‣ 2 Visual Affective Evaluation ‣ GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, GPT-4V can give satisfactory answers to the micro-gesture test samples provided. It can give similar answers to even the most difficult questions (open-ended questions), such as rubbing the face (rubbing the eyes). As shown in Fig. <a href="#S2.F7" title="Figure 7 ‣ 2.5 Micro-gesture Recognition ‣ 2 Visual Affective Evaluation ‣ GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, GPT-4V doesn’t recognize the shoulder flutter. GPT-4V can’t recognize tiny movements. While GPT-4V marks a significant step forward in the application of AI in the field of emotion and behavior recognition, its current limitations in recognizing certain micro-gestures suggest that further refinement and development is needed.</p>
</div>
</section>
<section id="S2.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.6 </span>Deception Detection</h3>

<div id="S2.SS6.p1" class="ltx_para">
<p id="S2.SS6.p1.1" class="ltx_p">Deception detection is an important task for determining the authenticity of video content, which is very important for security. To verify the performance of GPT-4V for deception detection, we evaluated on Real-Life Trial dataset  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>.</p>
</div>
<div id="S2.SS6.p2" class="ltx_para">
<p id="S2.SS6.p2.1" class="ltx_p">As shown in <a href="#S2.F8" title="Figure 8 ‣ 2.6 Deception Detection ‣ 2 Visual Affective Evaluation ‣ GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, GPT-4V can’t tell if a person in a video is lying. In fact, such subjective tasks are difficult for even real people to accurately judge. In addition, we try to input some multimodal information such as the sound spectrum to guide the GPT-4V to produce the correct result. But such operations do not allow GPT to reason the correct result. This shows that GPT-4V is still challenging for subjective tasks.</p>
</div>
<figure id="S2.F8" class="ltx_figure"><img src="/html/2403.05916/assets/x7.png" id="S2.F8.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="194" height="213" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S2.F8.4.2" class="ltx_text" style="font-size:90%;">Deception Detection on Real-Life Trial dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>. GPT-4V can’t recognize a lie or not.</span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Advanced Capability of Reasoning</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Chain of thought</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The concept of Chain-of-Thought (CoT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> was first introduced in the seminal work by researchers at Google, titled Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. This innovative approach represents a significant advancement in cue strategies designed to enhance the performance of Large Language Models (LLMs) in executing complex reasoning tasks, encompassing arithmetic, common sense, and symbolic reasoning domains. Contrary to the Implicit Context Learning (ICL) approach, which primarily relies on input-output pairings, CoT incorporates a series of intermediate inference steps that scaffold toward the final solution, thereby enriching the model’s reasoning pathway. In essence, CoT facilitates discrete prompt learning by appending an example to the beginning of a given input, enabling the model to process these concatenated texts simultaneously and produce the desired output. This method, under including additional intermediate prompts for inference, represents a substantial improvement over traditional context learning approaches.GPT-4V has a hard time recognizing specific expressions without context. However, we asked GPT-4V to first recognize the specific AU representation and then deduce the emotion based on the relationship between AU and the expression, which allowed GPT-4V to give some possible outcomes of the expression.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Furthermore, the application of CoT in emotion recognition tasks reveals its potential to circumvent some of the limitations faced by models such as GPT-4V in interpreting ambiguous or neutral expressions. Despite GPT-4V’s proficiency in Action Unit (AU) recognition, its performance in emotion recognition from expressions remains suboptimal. By leveraging the correlation between CoT, AU, and facial expressions, we aim to enhance GPT-4V’s accuracy in this area. As evidenced in Fig. <a href="#S3.F9" title="Figure 9 ‣ 3.1 Chain of thought ‣ 3 Advanced Capability of Reasoning ‣ GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, the incorporation of CoT significantly improves GPT-4V’s capability to discern emotions, particularly in instances where expressions are ambiguous or lack clear contextual cues. This methodology enables GPT-4V to first accurately identify AUs, and subsequently infer the probable emotion based on the established relationship between AUs and facial expressions. The integration of CoT, as illustrated by the blue segments in the figure, thus facilitates a more nuanced understanding and recognition of emotional states by the model. Thus, the application of CoT in affective computing holds the potential to significantly improve the capability of visual language models in interpreting and predicting emotional states with greater accuracy, leveraging contextual information to bridge the gap between task-related cues and the corresponding emotional expressions.</p>
</div>
<figure id="S3.F9" class="ltx_figure"><img src="/html/2403.05916/assets/x8.png" id="S3.F9.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="198" height="393" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.3.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S3.F9.4.2" class="ltx_text" style="font-size:90%;">Improve the accuracy of expression recognition through COT. We use more accurate AU recognition and make GPT-4V recognition more accurate according to the relationship between AU and expression.</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Tool call and processing</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">GPT-4V is one of the state-of-the-art multimodal language models that has achieved remarkable success in various natural language processing tasks. However, it is not directly applicable to some complex tasks, such as remote photoplethysmography (rPPG) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>. rPPG is a non-invasive technique used to measure heart rate and respiratory rate from facial videos. It has a wide range of applications in healthcare, entertainment, and human-computer interaction.</p>
</div>
<figure id="S3.F10" class="ltx_figure"><img src="/html/2403.05916/assets/x9.png" id="S3.F10.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="183" height="283" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F10.3.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S3.F10.4.2" class="ltx_text" style="font-size:90%;">Tool call and processing for rPPG task. GPT-4V can directly write programs for signal processing on request and try to run them. If there is an error, GPT-4V can be further corrected according to the error prompt, and finally give the heart rate prediction and visualization.</span></figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Unfortunately, GPT-4V cannot read in long time-series videos and cannot discern subtle chromatic variations. To address this issue, a solution is for professional researchers to collaborate with GPT-4V. In this regard, we found that GPT-4V can call Python tools to run code and debug it. To demonstrate this process, we extracted facial video chromatic changes and used GPT-4V to process the signal. As shown in the figure, GPT-4V called Python to process and visualize the signal. During this process, there were several bugs, but GPT-4V was able to self-correct based on the bug information and ultimately provided an accurate heart rate result.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">This process has provided us with an insight that this can be turned into a framework for a human-large language multimodal model that can self-correct. In this process, any large model can self-correct. This framework has immense potential in enhancing the accuracy and efficiency of various tasks, including rPPG.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Further Discussion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">GPT4 is a powerful language model that has shown remarkable success in various natural language processing tasks. However, it faces several challenges in other domains, such as facial expression recognition, emotion recognition, complex emotion recognition, non-contact physiological measurement, and authenticity detection.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Emotion recognition is the process of identifying and classifying emotions based on physiological signals, facial expressions, and speech patterns. GPT4 has shown promising results in this task; however, it requires a large amount of training data and may not generalize well to new datasets. To overcome this limitation, future research can focus on developing transfer learning techniques that enable GPT4 to learn from smaller datasets and generalize to new datasets.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Non-contact physiological measurement involves measuring physiological signals, such as heart rate, respiratory rate, and blood pressure, without direct contact with the body <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>, <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>. GPT4 faces difficulty in this task due to its limited ability to process and interpret physiological signals accurately. To overcome this limitation, future research can focus on developing new technologies that can capture physiological signals accurately and integrate them with GPT4.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Deception detection involves identifying and verifying the authenticity of a person or an object. GPT4 faces difficulty in this task due to its limited ability to process and interpret visual and audio information accurately. To overcome this limitation, future research can explore ways to integrate GPT4 with computer vision and audio processing techniques to improve authenticity detection accuracy.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">In conclusion, GPT4 faces several challenges in non-language tasks, such as facial expression recognition, emotion recognition, complex emotion recognition, non-contact physiological measurement, and deception detection. Future research can focus on developing new techniques to enhance GPT4’s ability to process and integrate multimodal data and improve its accuracy and efficiency in these tasks.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we have discussed the challenges that GPT4 faces in non-language tasks, such as facial expression recognition, emotion recognition, complex emotion recognition, non-contact physiological measurement, and authenticity detection. While GPT4 has shown remarkable success in various natural language processing tasks, it faces limitations in these domains due to its limited ability to process and interpret visual and audio information accurately. To overcome these limitations, future research can focus on developing new techniques to enhance GPT4’s ability to process and integrate multimodal data and improve its accuracy and efficiency in these tasks. This may involve integrating GPT4 with computer vision and audio processing techniques, developing transfer learning techniques, exploring new sensor technologies, and improving the quality and quantity of training data. By addressing these challenges, GPT4 has the potential to significantly advance the fields of facial expression recognition, emotion recognition, complex emotion recognition, non-contact physiological measurement, and authenticity detection, and open up new avenues for research and application in these domains.


<span id="S5.p1.1.1" class="ltx_text" style="font-size:90%;"></span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Achiam et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Gpt-4 technical report.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib1.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.08774</em><span id="bib.bib1.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.4.4.1" class="ltx_text" style="font-size:90%;">Chen and McDuff [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.6.1" class="ltx_text" style="font-size:90%;">
Weixuan Chen and Daniel McDuff.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">Deepphys: Video-based physiological measurement using convolutional attention networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib2.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the european conference on computer vision (ECCV)</em><span id="bib.bib2.10.3" class="ltx_text" style="font-size:90%;">, pages 349–365, 2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Corneanu et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Ciprian Corneanu, Meysam Madadi, and Sergio Escalera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">Deep structure inference network for facial action unit recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib3.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the european conference on computer vision (ECCV)</em><span id="bib.bib3.11.3" class="ltx_text" style="font-size:90%;">, pages 298–313, 2018.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Cui et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Zijun Cui, Chenyi Kuang, Tian Gao, Kartik Talamadupula, and Qiang Ji.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">Biomechanics-guided facial action unit detection through force modeling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib4.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib4.11.3" class="ltx_text" style="font-size:90%;">, pages 8694–8703, 2023.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Du et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Shichuan Du, Yong Tao, and Aleix M Martinez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Compound facial expressions of emotion.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib5.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the national academy of sciences</em><span id="bib.bib5.10.2" class="ltx_text" style="font-size:90%;">, 111(15):E1454–E1462, 2014.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.4.4.1" class="ltx_text" style="font-size:90%;">Ekman and Friesen [1978]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.6.1" class="ltx_text" style="font-size:90%;">
Paul Ekman and Wallace V Friesen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">Facial action coding system.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib6.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Environmental Psychology &amp; Nonverbal Behavior</em><span id="bib.bib6.9.2" class="ltx_text" style="font-size:90%;">, 1978.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Feng et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">Towards revealing the mystery behind chain of thought: a theoretical perspective.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib7.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib7.10.2" class="ltx_text" style="font-size:90%;">, 36, 2024.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Guo et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Xiaobao Guo, Nithish Muthuchamy Selvaraj, Zitong Yu, Adams Wai-Kin Kong, Bingquan Shen, and Alex Kot.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">Audio-visual deception detection: Dolos dataset and parameter-efficient crossmodal learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib8.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib8.11.3" class="ltx_text" style="font-size:90%;">, pages 22135–22145, 2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Huang et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Bin Huang, Shen Hu, Zimeng Liu, Chun-Liang Lin, Junfeng Su, Changchen Zhao, Li Wang, and Wenjin Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">Challenges and prospects of visual contactless physiological monitoring in clinical study.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib9.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">NPJ Digital Medicine</em><span id="bib.bib9.10.2" class="ltx_text" style="font-size:90%;">, 6(1):231, 2023.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.4.4.1" class="ltx_text" style="font-size:90%;">Jacob and Stenger [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.6.1" class="ltx_text" style="font-size:90%;">
Geethu Miriam Jacob and Bjorn Stenger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">Facial action unit detection with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib10.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib10.10.3" class="ltx_text" style="font-size:90%;">, pages 7680–7689, 2021.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Guanbin Li, Xin Zhu, Yirui Zeng, Qing Wang, and Liang Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">Semantic relationships guided representation learning for facial action unit recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib11.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial Intelligence</em><span id="bib.bib11.11.3" class="ltx_text" style="font-size:90%;">, pages 8594–8601, 2019.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Videochat: Chat-centric video understanding.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib12.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.06355</em><span id="bib.bib12.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Wei Li, Farnaz Abtahi, Zhigang Zhu, and Lijun Yin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">Eac-net: Deep nets with enhancing and cropping for facial action unit detection.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib13.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</em><span id="bib.bib13.10.2" class="ltx_text" style="font-size:90%;">, 40(11):2583–2596, 2018.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2021a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">
Yante Li, Xiaohua Huang, and Guoying Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">Micro-expression action unit detection with spatial and channel attention.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib14.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Neurocomputing</em><span id="bib.bib14.10.2" class="ltx_text" style="font-size:90%;">, 436:221–231, 2021a.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2021b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
Yante Li, Wei Peng, and Guoying Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">Micro-expression action unit detection with dual-view attentive similarity-preserving knowledge distillation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib15.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)</em><span id="bib.bib15.11.3" class="ltx_text" style="font-size:90%;">, pages 01–08. IEEE, 2021b.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
Xin Liu, Henglin Shi, Haoyu Chen, Zitong Yu, Xiaobai Li, and Guoying Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">imigue: An identity-free video dataset for micro-gesture understanding and emotion analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib16.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span id="bib.bib16.11.3" class="ltx_text" style="font-size:90%;">, pages 10631–10642, 2021.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Xin Liu, Kaishen Yuan, Xuesong Niu, Jingang Shi, Zitong Yu, Huanjing Yue, and Jingyu Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Multi-scale promoted self-adjusting correlation learning for facial action unit detection.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib17.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2308.07770</em><span id="bib.bib17.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">Lu et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Hao Lu, Hu Han, and S Kevin Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">Dual-gan: Joint bvp and noise modeling for remote physiological measurement.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib18.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span id="bib.bib18.11.3" class="ltx_text" style="font-size:90%;">, pages 12404–12413, 2021.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Lu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Hao Lu, Zitong Yu, Xuesong Niu, and Ying-Cong Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Neuron structure modeling for generalizable remote physiological measurement.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib19.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib19.11.3" class="ltx_text" style="font-size:90%;">, pages 18589–18599, 2023.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Luo et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
Cheng Luo, Siyang Song, Weicheng Xie, Linlin Shen, and Hatice Gunes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">Learning multi-dimensional edge feature-based au relation graph for facial action unit recognition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib20.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">international joint conference on artificial intelligence</em><span id="bib.bib20.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Luo et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">Valley: Video assistant with large language model enhanced ability.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib21.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.07207</em><span id="bib.bib21.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.4.4.1" class="ltx_text" style="font-size:90%;">Lv and Sun [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.6.1" class="ltx_text" style="font-size:90%;">
Hui Lv and Qianru Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">Video anomaly detection and explanation via large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib22.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2401.05702</em><span id="bib.bib22.9.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Maaz et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">Video-chatgpt: Towards detailed video understanding via large vision and language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib23.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.05424</em><span id="bib.bib23.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Mavadati et al. [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
S Mohammad Mavadati, Mohammad H Mahoor, Kevin Bartlett, Philip Trinh, and Jeffrey F Cohn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Disfa: A spontaneous facial action intensity database.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib24.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Affective Computing</em><span id="bib.bib24.10.2" class="ltx_text" style="font-size:90%;">, 4(2):151–160, 2013.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="font-size:90%;">Niu et al. [2019a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">
Xuesong Niu, Hu Han, Songfan Yang, Yan Huang, and Shiguang Shan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">Local relationship learning with person-specific shape regularization for facial action unit detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib25.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition</em><span id="bib.bib25.11.3" class="ltx_text" style="font-size:90%;">, pages 11917–11926, 2019a.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Niu et al. [2019b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Xuesong Niu, Shiguang Shan, Hu Han, and Xilin Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Rhythmnet: End-to-end heart rate estimation from face via spatial-temporal representation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib26.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Image Processing</em><span id="bib.bib26.10.2" class="ltx_text" style="font-size:90%;">, 29:2409–2423, 2019b.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Poria et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Soujanya Poria, Erik Cambria, Rajiv Bajpai, and Amir Hussain.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">A review of affective computing: From unimodal analysis to multimodal fusion.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib27.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Information fusion</em><span id="bib.bib27.10.2" class="ltx_text" style="font-size:90%;">, 37:98–125, 2017.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Şen et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
M Umut Şen, Veronica Perez-Rosas, Berrin Yanikoglu, Mohamed Abouelenien, Mihai Burzo, and Rada Mihalcea.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">Multimodal deception detection using real-life trial data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib28.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Affective Computing</em><span id="bib.bib28.10.2" class="ltx_text" style="font-size:90%;">, 13(1):306–319, 2020.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.4.4.1" class="ltx_text" style="font-size:90%;">Shan and Deng [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.6.1" class="ltx_text" style="font-size:90%;">
Li Shan and Weihong Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">Reliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib29.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Image Processing</em><span id="bib.bib29.9.2" class="ltx_text" style="font-size:90%;">, 28(1):356–370, 2018.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="font-size:90%;">Shao et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">
Zhiwen Shao, Zhilei Liu, Jianfei Cai, and Lizhuang Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">Deep adaptive attention for joint facial action unit detection and face alignment.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib30.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the European conference on computer vision (ECCV)</em><span id="bib.bib30.11.3" class="ltx_text" style="font-size:90%;">, pages 705–720, 2018.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text" style="font-size:90%;">Shao et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">
Zhiwen Shao, Zhilei Liu, Jianfei Cai, Yunsheng Wu, and Lizhuang Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="font-size:90%;">Facial action unit detection using attention and relation learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib31.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE transactions on affective computing</em><span id="bib.bib31.10.2" class="ltx_text" style="font-size:90%;">, 13(3):1274–1289, 2019.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text" style="font-size:90%;">Tang et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">
Yang Tang, Wangding Zeng, Dafei Zhao, and Honggang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">Piap-df: Pixel-interested and anti person-specific facial action unit detection net with discrete feedback learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib32.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib32.11.3" class="ltx_text" style="font-size:90%;">, pages 12899–12908, 2021.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2022a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">
Hao Wang, Euijoon Ahn, and Jinman Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">Self-supervised representation learning framework for remote physiological measurement using spatiotemporal augmentation loss.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib33.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial Intelligence</em><span id="bib.bib33.11.3" class="ltx_text" style="font-size:90%;">, pages 2431–2439, 2022a.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2015a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
Wenjin Wang, Sander Stuijk, and Gerard De Haan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">A novel algorithm for remote photoplethysmography: Spatial subspace rotation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib34.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE transactions on biomedical engineering</em><span id="bib.bib34.10.2" class="ltx_text" style="font-size:90%;">, 63(9):1974–1984, 2015a.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">
Wenjin Wang, Albertus C Den Brinker, Sander Stuijk, and Gerard De Haan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">Algorithmic principles of remote ppg.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib35.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Biomedical Engineering</em><span id="bib.bib35.10.2" class="ltx_text" style="font-size:90%;">, 64(7):1479–1491, 2016.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2015b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">
Yandan Wang, John See, Raphael C-W Phan, and Yee-Hui Oh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="font-size:90%;">Lbp with six intersection points: Reducing redundant information in lbp-top for micro-expression recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib36.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision–ACCV 2014: 12th Asian Conference on Computer Vision, Singapore, Singapore, November 1-5, 2014, Revised Selected Papers, Part I 12</em><span id="bib.bib36.11.3" class="ltx_text" style="font-size:90%;">, pages 525–537. Springer, 2015b.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2022b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">
Yan Wang, Wei Song, Wei Tao, Antonio Liotta, Dawei Yang, Xinlei Li, Shuyong Gao, Yixuan Sun, Weifeng Ge, Wei Zhang, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">A systematic review on affective computing: Emotion models, databases, and recent advances.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib37.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Information Fusion</em><span id="bib.bib37.10.2" class="ltx_text" style="font-size:90%;">, 83:19–52, 2022b.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text" style="font-size:90%;">Wei et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">Chain-of-thought prompting elicits reasoning in large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib38.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib38.10.2" class="ltx_text" style="font-size:90%;">, 35:24824–24837, 2022.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.5.5.1" class="ltx_text" style="font-size:90%;">Wen et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">
Licheng Wen, Xuemeng Yang, Daocheng Fu, Xiaofeng Wang, Pinlong Cai, Xin Li, Tao Ma, Yingxuan Li, Linran Xu, Dengke Shang, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">On the road with gpt-4v (ision): Early explorations of visual-language model on autonomous driving.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib39.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2311.05332</em><span id="bib.bib39.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="font-size:90%;">Wilie et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">
Bryan Wilie, Karissa Vincentio, Genta Indra Winata, Samuel Cahyawijaya, Xiaohong Li, Zhi Yuan Lim, Sidik Soleman, Rahmad Mahendra, Pascale Fung, Syafri Bahar, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">Indonlu: Benchmark and resources for evaluating indonesian natural language understanding.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib40.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2009.05387</em><span id="bib.bib40.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Xu et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">Clue: A chinese language understanding evaluation benchmark.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib41.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2004.05986</em><span id="bib.bib41.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Yan et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
Wen-Jing Yan, Xiaobai Li, Su-Jing Wang, Guoying Zhao, Yong-Jin Liu, Yu-Hsin Chen, and Xiaolan Fu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">Casme ii: An improved spontaneous micro-expression database and the baseline evaluation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib42.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">PloS one</em><span id="bib.bib42.10.2" class="ltx_text" style="font-size:90%;">, 9(1):e86041, 2014.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text" style="font-size:90%;">Ye et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="font-size:90%;">mplug-owl: Modularization empowers large language models with multimodality.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib43.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2304.14178</em><span id="bib.bib43.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text" style="font-size:90%;">Yu et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">
Zitong Yu, Xiaobai Li, and Guoying Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text" style="font-size:90%;">Facial-video-based physiological signal measurement: Recent advances and affective applications.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib44.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Signal Processing Magazine</em><span id="bib.bib44.10.2" class="ltx_text" style="font-size:90%;">, 38(6):50–58, 2021.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text" style="font-size:90%;">Yu et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="font-size:90%;">
Zitong Yu, Yuming Shen, Jingang Shi, Hengshuang Zhao, Philip HS Torr, and Guoying Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="font-size:90%;">Physformer: Facial video-based physiological measurement with temporal difference transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib45.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span id="bib.bib45.11.3" class="ltx_text" style="font-size:90%;">, pages 4186–4196, 2022.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.5.5.1" class="ltx_text" style="font-size:90%;">Yu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="font-size:90%;">
Zitong Yu, Yuming Shen, Jingang Shi, Hengshuang Zhao, Yawen Cui, Jiehua Zhang, Philip Torr, and Guoying Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.8.1" class="ltx_text" style="font-size:90%;">Physformer++: Facial video-based physiological measurement with slowfast temporal difference transformer.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib46.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</em><span id="bib.bib46.10.2" class="ltx_text" style="font-size:90%;">, 131(6):1307–1330, 2023.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.5.5.1" class="ltx_text" style="font-size:90%;">Zhao et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text" style="font-size:90%;">
Guoying Zhao, Xiaobai Li, Yante Li, and Matti Pietikäinen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.8.1" class="ltx_text" style="font-size:90%;">Facial micro-expressions: An overview.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib47.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE</em><span id="bib.bib47.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text" style="font-size:90%;">Zhao et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text" style="font-size:90%;">
Kaili Zhao, Wen-Sheng Chu, and Honggang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text" style="font-size:90%;">Deep region and multi-label learning for facial action unit detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib48.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</em><span id="bib.bib48.11.3" class="ltx_text" style="font-size:90%;">, pages 3391–3399, 2016.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.05915" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.05916" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.05916">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.05916" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.05917" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 16:21:58 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
