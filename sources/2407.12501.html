<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.12501] EmoFace: Audio-driven Emotional 3D Face Animation</title><meta property="og:description" content="Audio-driven emotional 3D face animation aims to generate emotionally expressive talking heads with synchronized lip movements. However, previous research has often overlooked the influence of diverse emotions on facia…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="EmoFace: Audio-driven Emotional 3D Face Animation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="EmoFace: Audio-driven Emotional 3D Face Animation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.12501">

<!--Generated on Mon Aug  5 17:45:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.2" class="ltx_ERROR undefined">\onlineid</span>
<p id="p1.1" class="ltx_p">1973
<span id="p1.1.1" class="ltx_ERROR undefined">\vgtccategory</span>Research
<span id="p1.1.2" class="ltx_ERROR undefined">\vgtcinsertpkg</span>

<span id="p1.1.3" class="ltx_ERROR undefined">\teaser</span>
<img src="/html/2407.12501/assets/figures/teaser.png" id="p1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="265" alt="[Uncaptioned image]">
<span id="p1.1.4" class="ltx_text ltx_caption ltx_align_center">Given an audio clip and a target emotion, EmoFace can generate talking heads with fully controllable emotions. This figure shows generated facial animation using the same audio and different emotions.</span></p>
</div>
<h1 class="ltx_title ltx_title_document">EmoFace: Audio-driven Emotional 3D Face Animation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Chang Liu 
<br class="ltx_break"><span id="1.1.1" class="ltx_text" style="font-size:70%;">Shanghai Jiao Tong University</span>
</span><span class="ltx_author_notes">e-mail: frenkiedejong@sjtu.edu.cn</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qunfen Lin 
<br class="ltx_break"><span id="2.1.1" class="ltx_text" style="font-size:70%;">Tencent Games</span>
</span><span class="ltx_author_notes">e-mail: volleylin@tencent.com</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zijiao Zeng
<br class="ltx_break"><span id="3.1.1" class="ltx_text" style="font-size:70%;">Tencent Games
</span>
</span><span class="ltx_author_notes">e-mail: zijiaozeng@tencent.com</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ye Pan  
<br class="ltx_break"><span id="4.1.1" class="ltx_text" style="font-size:70%;">Shanghai Jiao Tong University
</span>
</span><span class="ltx_author_notes">e-mail: whitneypanye@sjtu.edu.cn (Corresponding author)</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="5.1" class="ltx_p">Audio-driven emotional 3D face animation aims to generate emotionally expressive talking heads with synchronized lip movements. However, previous research has often overlooked the influence of diverse emotions on facial expressions or proved unsuitable for driving MetaHuman models. In response to this deficiency, we introduce EmoFace, a novel audio-driven methodology for creating facial animations with vivid emotional dynamics. Our approach can generate facial expressions with multiple emotions, and has the ability to generate random yet natural blinks and eye movements, while maintaining accurate lip synchronization. We propose independent speech encoders and emotion encoders to learn the relationship between audio, emotion and corresponding facial controller rigs, and finally map into the sequence of controller values. Additionally, we introduce two post-processing techniques dedicated to enhancing the authenticity of the animation, particularly in blinks and eye movements. Furthermore, recognizing the scarcity of emotional audio-visual data suitable for MetaHuman model manipulation, we contribute an emotional audio-visual dataset and derive control parameters for each frames. Our proposed methodology can be applied in producing dialogues animations of non-playable characters (NPCs) in video games, and driving avatars in virtual reality environments. Our further quantitative and qualitative experiments, as well as an user study comparing with existing researches show that our approach demonstrates superior results in driving 3D facial models. The code and sample data are available at <a target="_blank" href="https://github.com/SJTU-Lucy/EmoFace" title="" class="ltx_ref ltx_href">https://github.com/SJTU-Lucy/EmoFace</a></p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\CCScatlist</span><span id="p2.2" class="ltx_ERROR undefined">\CCScatTwelve</span>
<p id="p2.3" class="ltx_p">Human-centered computingComputer graphicsGraphics systems and interfacesVirtual reality</p>
</div>
<div id="p3" class="ltx_para">
<p id="p3.1" class="ltx_p">Introduction</p>
</div>
<div id="p4" class="ltx_para">
<p id="p4.1" class="ltx_p">The ever-expanding development of virtual reality technology has led to a growing demand for the creation of virtual characters, and it has become an indispensable part in many domains. By creating avatars, we could put ourselves in a metaverse, and communicate via avatars. This mode of interaction empowers individuals to engage with others through avatar-mediated communication, thus bypassing the necessity of physical presence. It brings several advantages, including higher levels of anonymity and privacy, the opportunity to engage in virtual environments that might not be possible in the physical world. Furthermore, it has become a fundamental element of interactive technologies, finding applications in diverse fields. For instance, online virtual multiplayer games, social media platforms, virtual assistants, virtual meetings, and various other domains.</p>
</div>
<div id="p5" class="ltx_para">
<p id="p5.1" class="ltx_p">However, paradoxically, as the demand for realistic generated facial animations increases, people’s tolerance for imperfections in the results diminishes, even in the case of subtle facial nuances. Even the smallest imperfection can induce the uncanny valley effect in the animated avatar, substantially decreasing its audience acceptance.</p>
</div>
<div id="p6" class="ltx_para">
<p id="p6.1" class="ltx_p">Traditionally, avatars can be generated through vision-based methods like face tracking, which have highly realistic outcomes. But a significant challenge arises when the user wears a headset, making the capture of facial expressions unfeasible. Under such circumstances, employing audio input as the foundation for generating avatars becomes a more suitable approach. Generally speaking, existing researches on audio-driven facial animation generation can be mainly concluded into three types:</p>
</div>
<section id="S0.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Video-based generation</h5>

<div id="S0.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S0.SS0.SSS0.Px1.p1.1" class="ltx_p">Edits the video of the target character to achieve audio and video synchronization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>;</p>
</div>
</section>
<section id="S0.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Image-based generation</h5>

<div id="S0.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S0.SS0.SSS0.Px2.p1.1" class="ltx_p">Uses one or several facial images as prototype for generation, and edited as a frame in the animation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>;</p>
</div>
</section>
<section id="S0.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Model-based generation</h5>

<div id="S0.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S0.SS0.SSS0.Px3.p1.1" class="ltx_p">Uses controller rigs or facial mesh to drive the model or render facial animation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>;</p>
</div>
<div id="S0.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S0.SS0.SSS0.Px3.p2.1" class="ltx_p">Most previous studies focus on video-based and image-based generation, and few studies focus on model-based generation approaches. However, in terms of game production, it is more appropriate to use model-based approaches as the target characters appears in the form of 3D models.</p>
</div>
<div id="S0.SS0.SSS0.Px3.p3" class="ltx_para">
<p id="S0.SS0.SSS0.Px3.p3.1" class="ltx_p">The primary challenge in this task stems from the fact that speech audio includes more than just the phonemes of the spoken text. It also contains cues related to facial expressions. Consequently, a talking head should not only synchronize with the speech but also convey the speaker’s emotional state through its expressions. While there have been notable successes in the research on audio-driven facial animation, the domain of multi-emotional generation has seen relatively limited exploration. Moreover, a significant proportion of existing datasets, such as MEAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, are primarily based on English recordings, with an absence of datasets recorded in Chinese. Considering that we mainly use Chinese in application, and the substantial phonetic differences between Chinese and English, employing models trained on English data for Chinese audio clips can result in inaccurate facial animations. At the same time, the current datasets appear in the form of pairs of audio and video, and the complex mapping relationship between video and rig controller values to drive 3D models is hard to be learned. Consequently, the existing datasets cannot be directly utilized for model training. To address this issue, we propose an audio-visual dataset recorded in Chinese that contains seven different emotions. Through post-processing, we have extracted the controller values corresponding to each frames in the videos.</p>
</div>
<div id="S0.SS0.SSS0.Px3.p4" class="ltx_para">
<p id="S0.SS0.SSS0.Px3.p4.1" class="ltx_p">In addition to constructing the dataset, we also propose a fundamental face generation model tailored to this dataset, which can be used for the facial generation in multiple emotions. This model takes an audio clip and the desired emotion as inputs, producing corresponding controller values for each frame to drive the MetaHuman model. However, given the relatively short duration of each recording, the dataset contains few instances of blinks and eye movements. Consequently, it becomes challenging to learn a robust correlation between blinks, eye gaze and speech, potentially leading to unnatural details in the generated talking head. To address this issue, we introduce independent blink and eye gaze control module. The blink controller gains blinking frequency data from other datasets and learns stochastic rules governing blinking behavior. Additionally, the gaze controller generates subtle eye movements, enhancing the naturalness of the facial animation.</p>
</div>
<div id="S0.SS0.SSS0.Px3.p5" class="ltx_para">
<p id="S0.SS0.SSS0.Px3.p5.1" class="ltx_p">This paper introduces EmoFace, a technology for driving virtual characters using audio and emotion as input. The mainstream researches can achieve good synchronization between input audios and output lip motions. However, these animations typically lack emotional expressions, with neutral face even when inputting emotional audio clips. Furthermore, the generated images from these methods are not suitable for driving virtual character models. To address these limitations, we propose an approach that takes both audio and emotional information as input and produces controller values for driving MetaHuman models, thereby enhancing the precision of facial animation generation. The main contributions of this paper are as follows:</p>
</div>
<div id="S0.SS0.SSS0.Px3.p6" class="ltx_para">
<ul id="S0.I1" class="ltx_itemize">
<li id="S0.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S0.I1.i1.p1" class="ltx_para">
<p id="S0.I1.i1.p1.1" class="ltx_p">We construct a dataset recorded in Chinese with multiple emotions, and extract the controller values of each frames;</p>
</div>
</li>
<li id="S0.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S0.I1.i2.p1" class="ltx_para">
<p id="S0.I1.i2.p1.1" class="ltx_p">We propose a foundational model of audio-driven multi-emotional generation of MetaHuman controller rigs. This model offers the flexibility to control emotions and delivers high-quality facial animation;</p>
</div>
</li>
<li id="S0.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S0.I1.i3.p1" class="ltx_para">
<p id="S0.I1.i3.p1.1" class="ltx_p">We enhance the facial expression generation process by incorporating blink and gaze controllers, thereby achieving a more natural and realistic outcome;</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Related Work</h2>

<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Audio-Driven Talking Face Generation</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">The aim of audio-driven talking head generation is to produce an animation of the target character based on an audio clip while ensuring accurate lip synchronization. Existing research can be broadly categorized into three distinct types.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">Some research works use GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> to directly output talking head videos.
Song et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> introduced a novel conditional recurrent generation network. It incorporates reference images and audio signals into the recurrent unit to facilitate timing-dependent learning, thereby enhancing the temporal coherence of both images and audio signals. This enhancement ensures seamless transitions in lip and facial motion.
Vougioukas et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> employed a temporal generative adversarial network (TGAN). Within their approach, a generator features an encoder-decoder structure, combining raw audio and individual reference images, while a sequence discriminator is implemented to ensure the naturalness of the generated animations.
Wav2Lip <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> focuses on the synchronization of audio and mouth shape in face generation. It takes audio and a video with masked lower face as input. Then, a GAN is trained to fill the masked lower face, and a lip-sync loss, computed by SyncNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, is employed to guarantee the alignment of the generated faces with the audio.
The primary issue with GAN-based generation lies in its direct output of facial images for each frame, thus unable to be migrated to drive facial models.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">Some studies focus on extracting phoneme from audio and learning the mapping between phoneme and viseme.
JALI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> extracts phoneme sequence from text to animate the JALI rigs. Additionally, the system utilizes audio signal as an auxiliary input to predict the intensity of jaw and mouth movements based on audio features such as volume, pitch, and formant information.
Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> made enhancements to the JALI system by introducing a separation between the phoneme and landmark processes. Their approach involves the combination of phonemes, landmarks, and audio features to generate JA-LI parameter values and viseme information.
However, these approaches mainly focus on lip shapes and ignore the animation of other parts of the face.</p>
</div>
<div id="S1.SS1.p4" class="ltx_para">
<p id="S1.SS1.p4.1" class="ltx_p">Some researchers try to learn the mapping between audio and corresponding facial controller values.
Pham et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> employed spectrograms as audio features, with the outcome being unit intensities corresponding to distinct facial regions. This is achieved by employing separate convolution processes in both the frequency and time domains.
VOCA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> utilizes a pre-defined character model and audio features extracted by DeepSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> to generate facial meshes corresponding to the character.
FaceFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> encodes the long-term audio context using the pretrained wav2vec2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and employs a transformer decoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, with carefully designed attention masks. This allows for the automatic regression and prediction of facial meshes.
MeshTalk <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> aims to disentangle a latent space of facial animation through a classification process. In this disentangled space, audio-related information governs the lower face, while audio-irrelevant information influences the upper face.
While these methods have achieved a commendable level of authenticity in their results, they are constrained by the complex many-to-many mapping relationship that exists between audio and facial expressions. This direct mapping from audio to facial expressions may suffer from over-smoothing, indicating that the output tends to converge towards the mean value of similar examples within the dataset.</p>
</div>
<div id="S1.SS1.p5" class="ltx_para">
<p id="S1.SS1.p5.1" class="ltx_p">In the above researches, a notable limitation is the neglect of emotional expressions, resulting in outputs that lack emotion and being neutral.
Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> introduced the MEAD dataset, which aims to create emotional talking faces by independently segmenting the upper and lower parts of the face. Nevertheless, the animations generated using this approach did not have a high level of naturalness.
Building upon the groundwork by MEAD, Ji et al. propose EVP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, which extends the integration of emotions into the synthesis process. EVP utilizes time-aligned audio features in the form of MFCC (Mel-frequency cepstral coefficients) from the same text content under various emotional states during the training process. Moreover, EVP introduces a disentanglement module that effectively segregates content encoding from emotion encoding within the audio. EmoTalk <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> further implement wav2vec2.0 in the disentanglement module, making content and emotion further separated.</p>
</div>
<div id="S1.SS1.p6" class="ltx_para">
<p id="S1.SS1.p6.1" class="ltx_p">While most of the works focus on learning the mapping between audio and facial expressions, FLINT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> implements a VAE structure to learn facial motion priors. Although it could ease the problem of high-frequency jitter and other unnatural motions, details like blinks and gazes are ignored. Moreover, completing leaving aside audio could possibly cause lack of movements.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Audio-Visual Dataset</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">Currently there are some high-quality audio-visual datasets, but most of them do not consider emotional information.
The LRW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> is an automatically collected and processed compilation of data from British broadcast TV programs, offering a substantial diversity in its content. However, the dataset’s inclusion of various roles and distinct individuals may introduce substantial interference, potentially hindering the model’s ability to discern information that is irrelevant from individuals.
The VOCASET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> contains voice-face 4D scans from 6 female and 6 male subjects. 40 sentence fragments were collected for each subject, and the speech diversity was maximized.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">A small number of datasets consider audio-visual data under multi-emotional conditions.
The dataset of Fanelli et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> comprises a total of 1109 audio sequences, with an average duration of 4.67 seconds. Each participant contributed audio-4D facial scanning data of 40 spoken English sentences, and each sentence was recorded on two occasions: once with emotional expression and once without.
MEAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> is a multi-view, multi-emotional audio-visual dataset with different intensities. It is composed of 60 recorders with 8 different emotions. Each recorder within the dataset is associated with approximately 40 minutes of video content, providing a rich and diverse resource for research.
EmoTalk <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> proposes a large-scale 3D emotional talking face (3D-ETF) dataset including both blendshape coefficients and mesh vertices. The dataset was based on two 2D audio-visual datasets: RAVDESS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and HDTF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, and contains over 6.5 hours of data.</p>
</div>
<div id="S1.SS2.p3" class="ltx_para">
<p id="S1.SS2.p3.1" class="ltx_p">Sadly, existing audio-visual datasets have two problems. One is that the recorded audio is in English, which may have problems when extended to other languages. The other problem is that audio-visual data or blendshape coefficients are unsuitable for driving MetaHuman models.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2407.12501/assets/figures/EmoFace_Overview.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="1196" height="444" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Structure of EmoFace</figcaption>
</figure>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Overview</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The architecture of our proposed model is illustrated in <a href="#S1.F1" title="Figure 1 ‣ 1.2 Audio-Visual Dataset ‣ 1 Related Work ‣ EmoFace: Audio-driven Emotional 3D Face Animation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>. The primary objective is to create an emotional, audio-driven talking head, while also enabling users to control the emotion and other details of the output facial animation. The model takes as input an audio clip and desired emotion, and yields the controller values required to drive the MetaHuman model to render the facial animation. It is composed of three parts, audio encoder, emotion encoder and Audio2Rig module.</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Audio Encoder</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">Our audio encoder is constructed based on the self-supervised pre-trained speech model, wav2vec2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. The multi-layer convolutional feature extractor takes raw audio as input and output latent speech representations with a frequency of 50. That is, the length of extracted features for 1s of audio would be 50. The encoder consists of several blocks containing temporal convolutional networks (TCN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> followed by layer normalization and a GELU activation function. Then the output of the feature encoder is fed to a context network which follows the Transformer architecture to build representations from the entire sequence.</p>
</div>
<div id="S2.SS1.SSS1.p2" class="ltx_para">
<p id="S2.SS1.SSS1.p2.1" class="ltx_p">We initialize our audio encoder with a pre-trained wav2vec2.0 BASE model, which has been trained on a dataset of 960 hours of LibriSpeech data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> sampled at a 16 kHz frequency. In this paper, we use wav2vec2.0 to extract the general features of the audio, and freeze the weights of the feature extractor throughout the training process. But the hidden states after feature extraction have a frame rate of 50, which is incompatible with our dataset recorded in frame rate of 60. To solve this, we implement a simple linear interpolation after this to ensure frequency alignment. And according to huggingface, attention mask should not be passed to wav2vec2-base structure to avoid degraded performance.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Emotion Encoder</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">In a corresponding manner, the emotion encoder accepts the emotion category from 0 to 6 as input and transforms it into a vector with identical dimensions to the content encoding. The emotion encoder is composed of an embedding layer and two fully connected layers. The embedding layer generates codes for different emotions, while the two fully connected layers further process these emotion-specific codes to produce encoded content with the same dimension.</p>
</div>
<div id="S2.SS1.SSS2.p2" class="ltx_para">
<p id="S2.SS1.SSS2.p2.1" class="ltx_p">As for the choice of input emotion, the model does not directly extract emotions from the audio, primarily because audio contains only a limited portion of emotion-related features. The emotion contained in facial expressions and the speech text is ignored, which is likely to cause inaccurate facial expressions. The audio-based extraction of emotions can be challenging. EVP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> uses the MFCC of input audio clips to predict the emotion of input audio, but can only achieve an accuracy of 60%. Meanwhile, EmoTalk <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> applies a model based on XLSR-Wav2Vec2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> for prediction. After fine-tuning on our dataset, it can achieve an accuracy of about 90%. In addition, by inputting emotion label, users gain control over the emotional category for each frame, thus accurately obtain the required facial animation.</p>
</div>
</section>
<section id="S2.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Audio2Rig</h4>

<div id="S2.SS1.SSS3.p1" class="ltx_para">
<p id="S2.SS1.SSS3.p1.1" class="ltx_p">After collecting audio features and emotion encoding, they needs to be combined to form the input of Audio2Rig module. The audio features are first processed to produce content encoding. This content encoder comprises a fully connected layer and a positional encoding layer. The positional encoding layer serves to incorporate information regarding the relative position of tokens within the sequence window. The positional encoding has the same dimension as the model, and will be added to the input vector. In this regard, we use the original positional encoding in the transformer encoder, which has the form of sine and cosine functions of <a href="#S2.E1" title="1 ‣ 2.1.3 Audio2Rig ‣ 2.1 Overview ‣ 2 Method ‣ EmoFace: Audio-driven Emotional 3D Face Animation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Equation 1</span></a> and <a href="#S2.E2" title="2 ‣ 2.1.3 Audio2Rig ‣ 2.1 Overview ‣ 2 Method ‣ EmoFace: Audio-driven Emotional 3D Face Animation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Equation 2</span></a>. The <span id="S2.SS1.SSS3.p1.1.1" class="ltx_text ltx_font_italic">pos</span> is the position in the input vector, <span id="S2.SS1.SSS3.p1.1.2" class="ltx_text ltx_font_italic">i</span> is the dimension, <math id="S2.SS1.SSS3.p1.1.m1.1" class="ltx_Math" alttext="d_{model}" display="inline"><semantics id="S2.SS1.SSS3.p1.1.m1.1a"><msub id="S2.SS1.SSS3.p1.1.m1.1.1" xref="S2.SS1.SSS3.p1.1.m1.1.1.cmml"><mi id="S2.SS1.SSS3.p1.1.m1.1.1.2" xref="S2.SS1.SSS3.p1.1.m1.1.1.2.cmml">d</mi><mrow id="S2.SS1.SSS3.p1.1.m1.1.1.3" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.cmml"><mi id="S2.SS1.SSS3.p1.1.m1.1.1.3.2" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.1.m1.1.1.3.1" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.1.m1.1.1.3.3" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.1.m1.1.1.3.1a" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.1.m1.1.1.3.4" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.1.m1.1.1.3.1b" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.1.m1.1.1.3.5" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS1.SSS3.p1.1.m1.1.1.3.1c" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS1.SSS3.p1.1.m1.1.1.3.6" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS3.p1.1.m1.1b"><apply id="S2.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS3.p1.1.m1.1.1.1.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.SSS3.p1.1.m1.1.1.2.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.2">𝑑</ci><apply id="S2.SS1.SSS3.p1.1.m1.1.1.3.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.3"><times id="S2.SS1.SSS3.p1.1.m1.1.1.3.1.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.1"></times><ci id="S2.SS1.SSS3.p1.1.m1.1.1.3.2.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.2">𝑚</ci><ci id="S2.SS1.SSS3.p1.1.m1.1.1.3.3.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.3">𝑜</ci><ci id="S2.SS1.SSS3.p1.1.m1.1.1.3.4.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.4">𝑑</ci><ci id="S2.SS1.SSS3.p1.1.m1.1.1.3.5.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.5">𝑒</ci><ci id="S2.SS1.SSS3.p1.1.m1.1.1.3.6.cmml" xref="S2.SS1.SSS3.p1.1.m1.1.1.3.6">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS3.p1.1.m1.1c">d_{model}</annotation></semantics></math> is the feature dimension of the model, which is set to 512 in our model. By implementing sinusoidal version of positional encoding, model is able to extrapolate to sequence lengths longer than the ones encountered during training.</p>
</div>
<div id="S2.SS1.SSS3.p2" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.3" class="ltx_Math" alttext="PE_{(pos,2i)}=sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})" display="block"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.4" xref="S2.E1.m1.3.4.cmml"><mrow id="S2.E1.m1.3.4.2" xref="S2.E1.m1.3.4.2.cmml"><mi id="S2.E1.m1.3.4.2.2" xref="S2.E1.m1.3.4.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.4.2.1" xref="S2.E1.m1.3.4.2.1.cmml">​</mo><msub id="S2.E1.m1.3.4.2.3" xref="S2.E1.m1.3.4.2.3.cmml"><mi id="S2.E1.m1.3.4.2.3.2" xref="S2.E1.m1.3.4.2.3.2.cmml">E</mi><mrow id="S2.E1.m1.2.2.2.2" xref="S2.E1.m1.2.2.2.3.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.2.2.3" xref="S2.E1.m1.2.2.2.3.cmml">(</mo><mrow id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S2.E1.m1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1a" xref="S2.E1.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S2.E1.m1.1.1.1.1.1.4" xref="S2.E1.m1.1.1.1.1.1.4.cmml">s</mi></mrow><mo id="S2.E1.m1.2.2.2.2.4" xref="S2.E1.m1.2.2.2.3.cmml">,</mo><mrow id="S2.E1.m1.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.2.cmml"><mn id="S2.E1.m1.2.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.2.2.1" xref="S2.E1.m1.2.2.2.2.2.1.cmml">​</mo><mi id="S2.E1.m1.2.2.2.2.2.3" xref="S2.E1.m1.2.2.2.2.2.3.cmml">i</mi></mrow><mo stretchy="false" id="S2.E1.m1.2.2.2.2.5" xref="S2.E1.m1.2.2.2.3.cmml">)</mo></mrow></msub></mrow><mo id="S2.E1.m1.3.4.1" xref="S2.E1.m1.3.4.1.cmml">=</mo><mrow id="S2.E1.m1.3.4.3" xref="S2.E1.m1.3.4.3.cmml"><mi id="S2.E1.m1.3.4.3.2" xref="S2.E1.m1.3.4.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.4.3.1" xref="S2.E1.m1.3.4.3.1.cmml">​</mo><mi id="S2.E1.m1.3.4.3.3" xref="S2.E1.m1.3.4.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.4.3.1a" xref="S2.E1.m1.3.4.3.1.cmml">​</mo><mi id="S2.E1.m1.3.4.3.4" xref="S2.E1.m1.3.4.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.4.3.1b" xref="S2.E1.m1.3.4.3.1.cmml">​</mo><mrow id="S2.E1.m1.3.4.3.5.2" xref="S2.E1.m1.3.3.cmml"><mo stretchy="false" id="S2.E1.m1.3.4.3.5.2.1" xref="S2.E1.m1.3.3.cmml">(</mo><mfrac id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml"><mrow id="S2.E1.m1.3.3.2" xref="S2.E1.m1.3.3.2.cmml"><mi id="S2.E1.m1.3.3.2.2" xref="S2.E1.m1.3.3.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.2.1" xref="S2.E1.m1.3.3.2.1.cmml">​</mo><mi id="S2.E1.m1.3.3.2.3" xref="S2.E1.m1.3.3.2.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.2.1a" xref="S2.E1.m1.3.3.2.1.cmml">​</mo><mi id="S2.E1.m1.3.3.2.4" xref="S2.E1.m1.3.3.2.4.cmml">s</mi></mrow><msup id="S2.E1.m1.3.3.3" xref="S2.E1.m1.3.3.3.cmml"><mn id="S2.E1.m1.3.3.3.2" xref="S2.E1.m1.3.3.3.2.cmml">10000</mn><mfrac id="S2.E1.m1.3.3.3.3" xref="S2.E1.m1.3.3.3.3.cmml"><mrow id="S2.E1.m1.3.3.3.3.2" xref="S2.E1.m1.3.3.3.3.2.cmml"><mn id="S2.E1.m1.3.3.3.3.2.2" xref="S2.E1.m1.3.3.3.3.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.3.3.2.1" xref="S2.E1.m1.3.3.3.3.2.1.cmml">​</mo><mi id="S2.E1.m1.3.3.3.3.2.3" xref="S2.E1.m1.3.3.3.3.2.3.cmml">i</mi></mrow><msub id="S2.E1.m1.3.3.3.3.3" xref="S2.E1.m1.3.3.3.3.3.cmml"><mi id="S2.E1.m1.3.3.3.3.3.2" xref="S2.E1.m1.3.3.3.3.3.2.cmml">d</mi><mrow id="S2.E1.m1.3.3.3.3.3.3" xref="S2.E1.m1.3.3.3.3.3.3.cmml"><mi id="S2.E1.m1.3.3.3.3.3.3.2" xref="S2.E1.m1.3.3.3.3.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.3.3.3.3.1" xref="S2.E1.m1.3.3.3.3.3.3.1.cmml">​</mo><mi id="S2.E1.m1.3.3.3.3.3.3.3" xref="S2.E1.m1.3.3.3.3.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.3.3.3.3.1a" xref="S2.E1.m1.3.3.3.3.3.3.1.cmml">​</mo><mi id="S2.E1.m1.3.3.3.3.3.3.4" xref="S2.E1.m1.3.3.3.3.3.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.3.3.3.3.1b" xref="S2.E1.m1.3.3.3.3.3.3.1.cmml">​</mo><mi id="S2.E1.m1.3.3.3.3.3.3.5" xref="S2.E1.m1.3.3.3.3.3.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.3.3.3.3.1c" xref="S2.E1.m1.3.3.3.3.3.3.1.cmml">​</mo><mi id="S2.E1.m1.3.3.3.3.3.3.6" xref="S2.E1.m1.3.3.3.3.3.3.6.cmml">l</mi></mrow></msub></mfrac></msup></mfrac><mo stretchy="false" id="S2.E1.m1.3.4.3.5.2.2" xref="S2.E1.m1.3.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.4.cmml" xref="S2.E1.m1.3.4"><eq id="S2.E1.m1.3.4.1.cmml" xref="S2.E1.m1.3.4.1"></eq><apply id="S2.E1.m1.3.4.2.cmml" xref="S2.E1.m1.3.4.2"><times id="S2.E1.m1.3.4.2.1.cmml" xref="S2.E1.m1.3.4.2.1"></times><ci id="S2.E1.m1.3.4.2.2.cmml" xref="S2.E1.m1.3.4.2.2">𝑃</ci><apply id="S2.E1.m1.3.4.2.3.cmml" xref="S2.E1.m1.3.4.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.4.2.3.1.cmml" xref="S2.E1.m1.3.4.2.3">subscript</csymbol><ci id="S2.E1.m1.3.4.2.3.2.cmml" xref="S2.E1.m1.3.4.2.3.2">𝐸</ci><interval closure="open" id="S2.E1.m1.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2"><apply id="S2.E1.m1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"><times id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1"></times><ci id="S2.E1.m1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.2">𝑝</ci><ci id="S2.E1.m1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.3">𝑜</ci><ci id="S2.E1.m1.1.1.1.1.1.4.cmml" xref="S2.E1.m1.1.1.1.1.1.4">𝑠</ci></apply><apply id="S2.E1.m1.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.2"><times id="S2.E1.m1.2.2.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.2.2.1"></times><cn type="integer" id="S2.E1.m1.2.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.2.2">2</cn><ci id="S2.E1.m1.2.2.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2.2.3">𝑖</ci></apply></interval></apply></apply><apply id="S2.E1.m1.3.4.3.cmml" xref="S2.E1.m1.3.4.3"><times id="S2.E1.m1.3.4.3.1.cmml" xref="S2.E1.m1.3.4.3.1"></times><ci id="S2.E1.m1.3.4.3.2.cmml" xref="S2.E1.m1.3.4.3.2">𝑠</ci><ci id="S2.E1.m1.3.4.3.3.cmml" xref="S2.E1.m1.3.4.3.3">𝑖</ci><ci id="S2.E1.m1.3.4.3.4.cmml" xref="S2.E1.m1.3.4.3.4">𝑛</ci><apply id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.4.3.5.2"><divide id="S2.E1.m1.3.3.1.cmml" xref="S2.E1.m1.3.4.3.5.2"></divide><apply id="S2.E1.m1.3.3.2.cmml" xref="S2.E1.m1.3.3.2"><times id="S2.E1.m1.3.3.2.1.cmml" xref="S2.E1.m1.3.3.2.1"></times><ci id="S2.E1.m1.3.3.2.2.cmml" xref="S2.E1.m1.3.3.2.2">𝑝</ci><ci id="S2.E1.m1.3.3.2.3.cmml" xref="S2.E1.m1.3.3.2.3">𝑜</ci><ci id="S2.E1.m1.3.3.2.4.cmml" xref="S2.E1.m1.3.3.2.4">𝑠</ci></apply><apply id="S2.E1.m1.3.3.3.cmml" xref="S2.E1.m1.3.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.3.1.cmml" xref="S2.E1.m1.3.3.3">superscript</csymbol><cn type="integer" id="S2.E1.m1.3.3.3.2.cmml" xref="S2.E1.m1.3.3.3.2">10000</cn><apply id="S2.E1.m1.3.3.3.3.cmml" xref="S2.E1.m1.3.3.3.3"><divide id="S2.E1.m1.3.3.3.3.1.cmml" xref="S2.E1.m1.3.3.3.3"></divide><apply id="S2.E1.m1.3.3.3.3.2.cmml" xref="S2.E1.m1.3.3.3.3.2"><times id="S2.E1.m1.3.3.3.3.2.1.cmml" xref="S2.E1.m1.3.3.3.3.2.1"></times><cn type="integer" id="S2.E1.m1.3.3.3.3.2.2.cmml" xref="S2.E1.m1.3.3.3.3.2.2">2</cn><ci id="S2.E1.m1.3.3.3.3.2.3.cmml" xref="S2.E1.m1.3.3.3.3.2.3">𝑖</ci></apply><apply id="S2.E1.m1.3.3.3.3.3.cmml" xref="S2.E1.m1.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.3.3.3.1.cmml" xref="S2.E1.m1.3.3.3.3.3">subscript</csymbol><ci id="S2.E1.m1.3.3.3.3.3.2.cmml" xref="S2.E1.m1.3.3.3.3.3.2">𝑑</ci><apply id="S2.E1.m1.3.3.3.3.3.3.cmml" xref="S2.E1.m1.3.3.3.3.3.3"><times id="S2.E1.m1.3.3.3.3.3.3.1.cmml" xref="S2.E1.m1.3.3.3.3.3.3.1"></times><ci id="S2.E1.m1.3.3.3.3.3.3.2.cmml" xref="S2.E1.m1.3.3.3.3.3.3.2">𝑚</ci><ci id="S2.E1.m1.3.3.3.3.3.3.3.cmml" xref="S2.E1.m1.3.3.3.3.3.3.3">𝑜</ci><ci id="S2.E1.m1.3.3.3.3.3.3.4.cmml" xref="S2.E1.m1.3.3.3.3.3.3.4">𝑑</ci><ci id="S2.E1.m1.3.3.3.3.3.3.5.cmml" xref="S2.E1.m1.3.3.3.3.3.3.5">𝑒</ci><ci id="S2.E1.m1.3.3.3.3.3.3.6.cmml" xref="S2.E1.m1.3.3.3.3.3.3.6">𝑙</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">PE_{(pos,2i)}=sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.3" class="ltx_Math" alttext="PE_{(pos,2i+1)}=cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})" display="block"><semantics id="S2.E2.m1.3a"><mrow id="S2.E2.m1.3.4" xref="S2.E2.m1.3.4.cmml"><mrow id="S2.E2.m1.3.4.2" xref="S2.E2.m1.3.4.2.cmml"><mi id="S2.E2.m1.3.4.2.2" xref="S2.E2.m1.3.4.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.4.2.1" xref="S2.E2.m1.3.4.2.1.cmml">​</mo><msub id="S2.E2.m1.3.4.2.3" xref="S2.E2.m1.3.4.2.3.cmml"><mi id="S2.E2.m1.3.4.2.3.2" xref="S2.E2.m1.3.4.2.3.2.cmml">E</mi><mrow id="S2.E2.m1.2.2.2.2" xref="S2.E2.m1.2.2.2.3.cmml"><mo stretchy="false" id="S2.E2.m1.2.2.2.2.3" xref="S2.E2.m1.2.2.2.3.cmml">(</mo><mrow id="S2.E2.m1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S2.E2.m1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.1.1.1a" xref="S2.E2.m1.1.1.1.1.1.1.cmml">​</mo><mi id="S2.E2.m1.1.1.1.1.1.4" xref="S2.E2.m1.1.1.1.1.1.4.cmml">s</mi></mrow><mo id="S2.E2.m1.2.2.2.2.4" xref="S2.E2.m1.2.2.2.3.cmml">,</mo><mrow id="S2.E2.m1.2.2.2.2.2" xref="S2.E2.m1.2.2.2.2.2.cmml"><mrow id="S2.E2.m1.2.2.2.2.2.2" xref="S2.E2.m1.2.2.2.2.2.2.cmml"><mn id="S2.E2.m1.2.2.2.2.2.2.2" xref="S2.E2.m1.2.2.2.2.2.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S2.E2.m1.2.2.2.2.2.2.1" xref="S2.E2.m1.2.2.2.2.2.2.1.cmml">​</mo><mi id="S2.E2.m1.2.2.2.2.2.2.3" xref="S2.E2.m1.2.2.2.2.2.2.3.cmml">i</mi></mrow><mo id="S2.E2.m1.2.2.2.2.2.1" xref="S2.E2.m1.2.2.2.2.2.1.cmml">+</mo><mn id="S2.E2.m1.2.2.2.2.2.3" xref="S2.E2.m1.2.2.2.2.2.3.cmml">1</mn></mrow><mo stretchy="false" id="S2.E2.m1.2.2.2.2.5" xref="S2.E2.m1.2.2.2.3.cmml">)</mo></mrow></msub></mrow><mo id="S2.E2.m1.3.4.1" xref="S2.E2.m1.3.4.1.cmml">=</mo><mrow id="S2.E2.m1.3.4.3" xref="S2.E2.m1.3.4.3.cmml"><mi id="S2.E2.m1.3.4.3.2" xref="S2.E2.m1.3.4.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.4.3.1" xref="S2.E2.m1.3.4.3.1.cmml">​</mo><mi id="S2.E2.m1.3.4.3.3" xref="S2.E2.m1.3.4.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.4.3.1a" xref="S2.E2.m1.3.4.3.1.cmml">​</mo><mi id="S2.E2.m1.3.4.3.4" xref="S2.E2.m1.3.4.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.4.3.1b" xref="S2.E2.m1.3.4.3.1.cmml">​</mo><mrow id="S2.E2.m1.3.4.3.5.2" xref="S2.E2.m1.3.3.cmml"><mo stretchy="false" id="S2.E2.m1.3.4.3.5.2.1" xref="S2.E2.m1.3.3.cmml">(</mo><mfrac id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml"><mrow id="S2.E2.m1.3.3.2" xref="S2.E2.m1.3.3.2.cmml"><mi id="S2.E2.m1.3.3.2.2" xref="S2.E2.m1.3.3.2.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.2.1" xref="S2.E2.m1.3.3.2.1.cmml">​</mo><mi id="S2.E2.m1.3.3.2.3" xref="S2.E2.m1.3.3.2.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.2.1a" xref="S2.E2.m1.3.3.2.1.cmml">​</mo><mi id="S2.E2.m1.3.3.2.4" xref="S2.E2.m1.3.3.2.4.cmml">s</mi></mrow><msup id="S2.E2.m1.3.3.3" xref="S2.E2.m1.3.3.3.cmml"><mn id="S2.E2.m1.3.3.3.2" xref="S2.E2.m1.3.3.3.2.cmml">10000</mn><mfrac id="S2.E2.m1.3.3.3.3" xref="S2.E2.m1.3.3.3.3.cmml"><mrow id="S2.E2.m1.3.3.3.3.2" xref="S2.E2.m1.3.3.3.3.2.cmml"><mn id="S2.E2.m1.3.3.3.3.2.2" xref="S2.E2.m1.3.3.3.3.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.3.3.2.1" xref="S2.E2.m1.3.3.3.3.2.1.cmml">​</mo><mi id="S2.E2.m1.3.3.3.3.2.3" xref="S2.E2.m1.3.3.3.3.2.3.cmml">i</mi></mrow><msub id="S2.E2.m1.3.3.3.3.3" xref="S2.E2.m1.3.3.3.3.3.cmml"><mi id="S2.E2.m1.3.3.3.3.3.2" xref="S2.E2.m1.3.3.3.3.3.2.cmml">d</mi><mrow id="S2.E2.m1.3.3.3.3.3.3" xref="S2.E2.m1.3.3.3.3.3.3.cmml"><mi id="S2.E2.m1.3.3.3.3.3.3.2" xref="S2.E2.m1.3.3.3.3.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.3.3.3.3.1" xref="S2.E2.m1.3.3.3.3.3.3.1.cmml">​</mo><mi id="S2.E2.m1.3.3.3.3.3.3.3" xref="S2.E2.m1.3.3.3.3.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.3.3.3.3.1a" xref="S2.E2.m1.3.3.3.3.3.3.1.cmml">​</mo><mi id="S2.E2.m1.3.3.3.3.3.3.4" xref="S2.E2.m1.3.3.3.3.3.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.3.3.3.3.1b" xref="S2.E2.m1.3.3.3.3.3.3.1.cmml">​</mo><mi id="S2.E2.m1.3.3.3.3.3.3.5" xref="S2.E2.m1.3.3.3.3.3.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.3.3.3.3.3.3.1c" xref="S2.E2.m1.3.3.3.3.3.3.1.cmml">​</mo><mi id="S2.E2.m1.3.3.3.3.3.3.6" xref="S2.E2.m1.3.3.3.3.3.3.6.cmml">l</mi></mrow></msub></mfrac></msup></mfrac><mo stretchy="false" id="S2.E2.m1.3.4.3.5.2.2" xref="S2.E2.m1.3.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.3b"><apply id="S2.E2.m1.3.4.cmml" xref="S2.E2.m1.3.4"><eq id="S2.E2.m1.3.4.1.cmml" xref="S2.E2.m1.3.4.1"></eq><apply id="S2.E2.m1.3.4.2.cmml" xref="S2.E2.m1.3.4.2"><times id="S2.E2.m1.3.4.2.1.cmml" xref="S2.E2.m1.3.4.2.1"></times><ci id="S2.E2.m1.3.4.2.2.cmml" xref="S2.E2.m1.3.4.2.2">𝑃</ci><apply id="S2.E2.m1.3.4.2.3.cmml" xref="S2.E2.m1.3.4.2.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.4.2.3.1.cmml" xref="S2.E2.m1.3.4.2.3">subscript</csymbol><ci id="S2.E2.m1.3.4.2.3.2.cmml" xref="S2.E2.m1.3.4.2.3.2">𝐸</ci><interval closure="open" id="S2.E2.m1.2.2.2.3.cmml" xref="S2.E2.m1.2.2.2.2"><apply id="S2.E2.m1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1"><times id="S2.E2.m1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1"></times><ci id="S2.E2.m1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1.2">𝑝</ci><ci id="S2.E2.m1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.3">𝑜</ci><ci id="S2.E2.m1.1.1.1.1.1.4.cmml" xref="S2.E2.m1.1.1.1.1.1.4">𝑠</ci></apply><apply id="S2.E2.m1.2.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2.2"><plus id="S2.E2.m1.2.2.2.2.2.1.cmml" xref="S2.E2.m1.2.2.2.2.2.1"></plus><apply id="S2.E2.m1.2.2.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2.2.2"><times id="S2.E2.m1.2.2.2.2.2.2.1.cmml" xref="S2.E2.m1.2.2.2.2.2.2.1"></times><cn type="integer" id="S2.E2.m1.2.2.2.2.2.2.2.cmml" xref="S2.E2.m1.2.2.2.2.2.2.2">2</cn><ci id="S2.E2.m1.2.2.2.2.2.2.3.cmml" xref="S2.E2.m1.2.2.2.2.2.2.3">𝑖</ci></apply><cn type="integer" id="S2.E2.m1.2.2.2.2.2.3.cmml" xref="S2.E2.m1.2.2.2.2.2.3">1</cn></apply></interval></apply></apply><apply id="S2.E2.m1.3.4.3.cmml" xref="S2.E2.m1.3.4.3"><times id="S2.E2.m1.3.4.3.1.cmml" xref="S2.E2.m1.3.4.3.1"></times><ci id="S2.E2.m1.3.4.3.2.cmml" xref="S2.E2.m1.3.4.3.2">𝑐</ci><ci id="S2.E2.m1.3.4.3.3.cmml" xref="S2.E2.m1.3.4.3.3">𝑜</ci><ci id="S2.E2.m1.3.4.3.4.cmml" xref="S2.E2.m1.3.4.3.4">𝑠</ci><apply id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.4.3.5.2"><divide id="S2.E2.m1.3.3.1.cmml" xref="S2.E2.m1.3.4.3.5.2"></divide><apply id="S2.E2.m1.3.3.2.cmml" xref="S2.E2.m1.3.3.2"><times id="S2.E2.m1.3.3.2.1.cmml" xref="S2.E2.m1.3.3.2.1"></times><ci id="S2.E2.m1.3.3.2.2.cmml" xref="S2.E2.m1.3.3.2.2">𝑝</ci><ci id="S2.E2.m1.3.3.2.3.cmml" xref="S2.E2.m1.3.3.2.3">𝑜</ci><ci id="S2.E2.m1.3.3.2.4.cmml" xref="S2.E2.m1.3.3.2.4">𝑠</ci></apply><apply id="S2.E2.m1.3.3.3.cmml" xref="S2.E2.m1.3.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.3.1.cmml" xref="S2.E2.m1.3.3.3">superscript</csymbol><cn type="integer" id="S2.E2.m1.3.3.3.2.cmml" xref="S2.E2.m1.3.3.3.2">10000</cn><apply id="S2.E2.m1.3.3.3.3.cmml" xref="S2.E2.m1.3.3.3.3"><divide id="S2.E2.m1.3.3.3.3.1.cmml" xref="S2.E2.m1.3.3.3.3"></divide><apply id="S2.E2.m1.3.3.3.3.2.cmml" xref="S2.E2.m1.3.3.3.3.2"><times id="S2.E2.m1.3.3.3.3.2.1.cmml" xref="S2.E2.m1.3.3.3.3.2.1"></times><cn type="integer" id="S2.E2.m1.3.3.3.3.2.2.cmml" xref="S2.E2.m1.3.3.3.3.2.2">2</cn><ci id="S2.E2.m1.3.3.3.3.2.3.cmml" xref="S2.E2.m1.3.3.3.3.2.3">𝑖</ci></apply><apply id="S2.E2.m1.3.3.3.3.3.cmml" xref="S2.E2.m1.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.3.3.3.1.cmml" xref="S2.E2.m1.3.3.3.3.3">subscript</csymbol><ci id="S2.E2.m1.3.3.3.3.3.2.cmml" xref="S2.E2.m1.3.3.3.3.3.2">𝑑</ci><apply id="S2.E2.m1.3.3.3.3.3.3.cmml" xref="S2.E2.m1.3.3.3.3.3.3"><times id="S2.E2.m1.3.3.3.3.3.3.1.cmml" xref="S2.E2.m1.3.3.3.3.3.3.1"></times><ci id="S2.E2.m1.3.3.3.3.3.3.2.cmml" xref="S2.E2.m1.3.3.3.3.3.3.2">𝑚</ci><ci id="S2.E2.m1.3.3.3.3.3.3.3.cmml" xref="S2.E2.m1.3.3.3.3.3.3.3">𝑜</ci><ci id="S2.E2.m1.3.3.3.3.3.3.4.cmml" xref="S2.E2.m1.3.3.3.3.3.3.4">𝑑</ci><ci id="S2.E2.m1.3.3.3.3.3.3.5.cmml" xref="S2.E2.m1.3.3.3.3.3.3.5">𝑒</ci><ci id="S2.E2.m1.3.3.3.3.3.3.6.cmml" xref="S2.E2.m1.3.3.3.3.3.3.6">𝑙</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.3c">PE_{(pos,2i+1)}=cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS1.SSS3.p3" class="ltx_para">
<p id="S2.SS1.SSS3.p3.1" class="ltx_p">By combining the content encoding and emotion encoding, the result of 174-dimensional vector serves as input of Audio2Rig module. This module is composed of 10 transformer encoder layers and one fully connected layer to match dimensions for the output controller rigs. It is noteworthy that we depart from the approach employed in other studies, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, by deploying another Transformer encoder for prediction. Our choice to utilize a transformer encoder is mainly due to the following considerations:</p>
</div>
<div id="S2.SS1.SSS3.p4" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Transformer encoder significantly outperforms Transformer decoder in terms of inference speed. We conducted inference time tests on short audio clips using both CPU and GPU. When on GPU, the average inference time is 21.57ms for Transformer encoder and 432.29ms for Transformer decoder. On the CPU, the times are 254.49ms for Transformer encoder and 811.48ms for Transformer decoder. This clearly demonstrates that employing a transformer encoder is better suited to meet real-time requirements.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Transformer decoder traditionally incorporates all preceding values when predicting a new frame. However, in practice, only a few previous frames offer substantial insight for forecasting the next frame. Additionally, the wav2vec2 feature extractor already contains contextual information within the content encoding. Therefore, discarding previous frames has little impact on the result.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">To address potential jitter in the output sequence, we apply a Savitzky-Golay (savgol) filter with a window length of 15 frames and a polynomial order of 3. This filtering process ensures a smooth and visually pleasing facial animation.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Blink</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">While the above audio-driven model allows the generation of emotional talking heads with vivid expressions, a crucial issue remains: it lacks the action of blinks, which can significantly impact the user’s perception of the generated animation.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Several factors contribute to the absence of blinks in the generated animations. Firstly, the dataset recordings are mainly short sentences less than 5 seconds in duration. Consequently, blink actions are rarely captured. Secondly, blinks do not exhibit a straightforward association with audio content. Other factors, such as head motions and intonation, can also influence blink frequency. As a result, it becomes challenging for the model to learn the underlying patterns related to blinks.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Previous study on blink rate <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> highlights that blink rate is somewhat dependent on cognitive states, with tasks involving speaking or memory increasing blink rate and stable visual targets decreasing blink rate. Their results indicates that in the speaking state, the blink frequency, that is, the number of blinks in one minute, roughly conforms to the logarithmic normal distribution, with an average value of 26. Gender and age have no significant effect on blink frequency in this state.
The research of Le et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> indicates that blinking movements are mainly divided into involuntary blinking and voluntary eyelid movement. The former is usually completely closed, while the latter is usually related to head posture and emotional information. Their research also pointed out that the frequency of involuntary blinking basically conforms to the log-normal distribution law, in which the mean value of the frequency in the speaking mode is 21.1, and the standard deviation is 3.6.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">As for blink detection, we refer to the research of Cech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Existing research on predicting face landmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> typically utilize a single frame of static images as input and predict face landmarks using a set of 68 distinct points. Specifically, the left and right eyes are represented by 6 landmark points each, as illustrated in <a href="#S2.F2" title="Figure 2 ‣ 2.2 Blink ‣ 2 Method ‣ EmoFace: Audio-driven Emotional 3D Face Animation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>. Once these landmarks have been acquired, the eye aspect ratio (EAR) can be computed according to <a href="#S2.E3" title="3 ‣ 2.2 Blink ‣ 2 Method ‣ EmoFace: Audio-driven Emotional 3D Face Animation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Equation 3</span></a>. When the eyes are closed, the EAR value is approximately zero. The features of EAR includes qualities such as insensitivity to head pose, minimal variation between individuals, and resilience to uniform image scaling and rotation.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2407.12501/assets/figures/eye_landmark.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="169" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Positions of 6 landmarks for eye</figcaption>
</figure>
<div id="S2.SS2.p5" class="ltx_para">
<table id="S2.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E3.m1.3" class="ltx_Math" alttext="EAR=\frac{||p_{2}-p_{6}||+||p_{3}-p_{5}||}{2||p_{1}-p_{4}||}" display="block"><semantics id="S2.E3.m1.3a"><mrow id="S2.E3.m1.3.4" xref="S2.E3.m1.3.4.cmml"><mrow id="S2.E3.m1.3.4.2" xref="S2.E3.m1.3.4.2.cmml"><mi id="S2.E3.m1.3.4.2.2" xref="S2.E3.m1.3.4.2.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.4.2.1" xref="S2.E3.m1.3.4.2.1.cmml">​</mo><mi id="S2.E3.m1.3.4.2.3" xref="S2.E3.m1.3.4.2.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.4.2.1a" xref="S2.E3.m1.3.4.2.1.cmml">​</mo><mi id="S2.E3.m1.3.4.2.4" xref="S2.E3.m1.3.4.2.4.cmml">R</mi></mrow><mo id="S2.E3.m1.3.4.1" xref="S2.E3.m1.3.4.1.cmml">=</mo><mfrac id="S2.E3.m1.3.3" xref="S2.E3.m1.3.3.cmml"><mrow id="S2.E3.m1.2.2.2" xref="S2.E3.m1.2.2.2.cmml"><mrow id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E3.m1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><msub id="S2.E3.m1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.2.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.2.2" xref="S2.E3.m1.1.1.1.1.1.1.2.2.cmml">p</mi><mn id="S2.E3.m1.1.1.1.1.1.1.2.3" xref="S2.E3.m1.1.1.1.1.1.1.2.3.cmml">2</mn></msub><mo id="S2.E3.m1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S2.E3.m1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.1.1.1.3.2.cmml">p</mi><mn id="S2.E3.m1.1.1.1.1.1.1.3.3" xref="S2.E3.m1.1.1.1.1.1.1.3.3.cmml">6</mn></msub></mrow><mo stretchy="false" id="S2.E3.m1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.2.1.cmml">‖</mo></mrow><mo id="S2.E3.m1.2.2.2.3" xref="S2.E3.m1.2.2.2.3.cmml">+</mo><mrow id="S2.E3.m1.2.2.2.2.1" xref="S2.E3.m1.2.2.2.2.2.cmml"><mo stretchy="false" id="S2.E3.m1.2.2.2.2.1.2" xref="S2.E3.m1.2.2.2.2.2.1.cmml">‖</mo><mrow id="S2.E3.m1.2.2.2.2.1.1" xref="S2.E3.m1.2.2.2.2.1.1.cmml"><msub id="S2.E3.m1.2.2.2.2.1.1.2" xref="S2.E3.m1.2.2.2.2.1.1.2.cmml"><mi id="S2.E3.m1.2.2.2.2.1.1.2.2" xref="S2.E3.m1.2.2.2.2.1.1.2.2.cmml">p</mi><mn id="S2.E3.m1.2.2.2.2.1.1.2.3" xref="S2.E3.m1.2.2.2.2.1.1.2.3.cmml">3</mn></msub><mo id="S2.E3.m1.2.2.2.2.1.1.1" xref="S2.E3.m1.2.2.2.2.1.1.1.cmml">−</mo><msub id="S2.E3.m1.2.2.2.2.1.1.3" xref="S2.E3.m1.2.2.2.2.1.1.3.cmml"><mi id="S2.E3.m1.2.2.2.2.1.1.3.2" xref="S2.E3.m1.2.2.2.2.1.1.3.2.cmml">p</mi><mn id="S2.E3.m1.2.2.2.2.1.1.3.3" xref="S2.E3.m1.2.2.2.2.1.1.3.3.cmml">5</mn></msub></mrow><mo stretchy="false" id="S2.E3.m1.2.2.2.2.1.3" xref="S2.E3.m1.2.2.2.2.2.1.cmml">‖</mo></mrow></mrow><mrow id="S2.E3.m1.3.3.3" xref="S2.E3.m1.3.3.3.cmml"><mn id="S2.E3.m1.3.3.3.3" xref="S2.E3.m1.3.3.3.3.cmml">2</mn><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.3.2" xref="S2.E3.m1.3.3.3.2.cmml">​</mo><mrow id="S2.E3.m1.3.3.3.1.1" xref="S2.E3.m1.3.3.3.1.2.cmml"><mo stretchy="false" id="S2.E3.m1.3.3.3.1.1.2" xref="S2.E3.m1.3.3.3.1.2.1.cmml">‖</mo><mrow id="S2.E3.m1.3.3.3.1.1.1" xref="S2.E3.m1.3.3.3.1.1.1.cmml"><msub id="S2.E3.m1.3.3.3.1.1.1.2" xref="S2.E3.m1.3.3.3.1.1.1.2.cmml"><mi id="S2.E3.m1.3.3.3.1.1.1.2.2" xref="S2.E3.m1.3.3.3.1.1.1.2.2.cmml">p</mi><mn id="S2.E3.m1.3.3.3.1.1.1.2.3" xref="S2.E3.m1.3.3.3.1.1.1.2.3.cmml">1</mn></msub><mo id="S2.E3.m1.3.3.3.1.1.1.1" xref="S2.E3.m1.3.3.3.1.1.1.1.cmml">−</mo><msub id="S2.E3.m1.3.3.3.1.1.1.3" xref="S2.E3.m1.3.3.3.1.1.1.3.cmml"><mi id="S2.E3.m1.3.3.3.1.1.1.3.2" xref="S2.E3.m1.3.3.3.1.1.1.3.2.cmml">p</mi><mn id="S2.E3.m1.3.3.3.1.1.1.3.3" xref="S2.E3.m1.3.3.3.1.1.1.3.3.cmml">4</mn></msub></mrow><mo stretchy="false" id="S2.E3.m1.3.3.3.1.1.3" xref="S2.E3.m1.3.3.3.1.2.1.cmml">‖</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.3b"><apply id="S2.E3.m1.3.4.cmml" xref="S2.E3.m1.3.4"><eq id="S2.E3.m1.3.4.1.cmml" xref="S2.E3.m1.3.4.1"></eq><apply id="S2.E3.m1.3.4.2.cmml" xref="S2.E3.m1.3.4.2"><times id="S2.E3.m1.3.4.2.1.cmml" xref="S2.E3.m1.3.4.2.1"></times><ci id="S2.E3.m1.3.4.2.2.cmml" xref="S2.E3.m1.3.4.2.2">𝐸</ci><ci id="S2.E3.m1.3.4.2.3.cmml" xref="S2.E3.m1.3.4.2.3">𝐴</ci><ci id="S2.E3.m1.3.4.2.4.cmml" xref="S2.E3.m1.3.4.2.4">𝑅</ci></apply><apply id="S2.E3.m1.3.3.cmml" xref="S2.E3.m1.3.3"><divide id="S2.E3.m1.3.3.4.cmml" xref="S2.E3.m1.3.3"></divide><apply id="S2.E3.m1.2.2.2.cmml" xref="S2.E3.m1.2.2.2"><plus id="S2.E3.m1.2.2.2.3.cmml" xref="S2.E3.m1.2.2.2.3"></plus><apply id="S2.E3.m1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E3.m1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.2">norm</csymbol><apply id="S2.E3.m1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1"><minus id="S2.E3.m1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1"></minus><apply id="S2.E3.m1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.2">𝑝</ci><cn type="integer" id="S2.E3.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3">2</cn></apply><apply id="S2.E3.m1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3.2">𝑝</ci><cn type="integer" id="S2.E3.m1.1.1.1.1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3.3">6</cn></apply></apply></apply><apply id="S2.E3.m1.2.2.2.2.2.cmml" xref="S2.E3.m1.2.2.2.2.1"><csymbol cd="latexml" id="S2.E3.m1.2.2.2.2.2.1.cmml" xref="S2.E3.m1.2.2.2.2.1.2">norm</csymbol><apply id="S2.E3.m1.2.2.2.2.1.1.cmml" xref="S2.E3.m1.2.2.2.2.1.1"><minus id="S2.E3.m1.2.2.2.2.1.1.1.cmml" xref="S2.E3.m1.2.2.2.2.1.1.1"></minus><apply id="S2.E3.m1.2.2.2.2.1.1.2.cmml" xref="S2.E3.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.2.2.1.1.2.1.cmml" xref="S2.E3.m1.2.2.2.2.1.1.2">subscript</csymbol><ci id="S2.E3.m1.2.2.2.2.1.1.2.2.cmml" xref="S2.E3.m1.2.2.2.2.1.1.2.2">𝑝</ci><cn type="integer" id="S2.E3.m1.2.2.2.2.1.1.2.3.cmml" xref="S2.E3.m1.2.2.2.2.1.1.2.3">3</cn></apply><apply id="S2.E3.m1.2.2.2.2.1.1.3.cmml" xref="S2.E3.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.2.2.1.1.3.1.cmml" xref="S2.E3.m1.2.2.2.2.1.1.3">subscript</csymbol><ci id="S2.E3.m1.2.2.2.2.1.1.3.2.cmml" xref="S2.E3.m1.2.2.2.2.1.1.3.2">𝑝</ci><cn type="integer" id="S2.E3.m1.2.2.2.2.1.1.3.3.cmml" xref="S2.E3.m1.2.2.2.2.1.1.3.3">5</cn></apply></apply></apply></apply><apply id="S2.E3.m1.3.3.3.cmml" xref="S2.E3.m1.3.3.3"><times id="S2.E3.m1.3.3.3.2.cmml" xref="S2.E3.m1.3.3.3.2"></times><cn type="integer" id="S2.E3.m1.3.3.3.3.cmml" xref="S2.E3.m1.3.3.3.3">2</cn><apply id="S2.E3.m1.3.3.3.1.2.cmml" xref="S2.E3.m1.3.3.3.1.1"><csymbol cd="latexml" id="S2.E3.m1.3.3.3.1.2.1.cmml" xref="S2.E3.m1.3.3.3.1.1.2">norm</csymbol><apply id="S2.E3.m1.3.3.3.1.1.1.cmml" xref="S2.E3.m1.3.3.3.1.1.1"><minus id="S2.E3.m1.3.3.3.1.1.1.1.cmml" xref="S2.E3.m1.3.3.3.1.1.1.1"></minus><apply id="S2.E3.m1.3.3.3.1.1.1.2.cmml" xref="S2.E3.m1.3.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.3.1.1.1.2.1.cmml" xref="S2.E3.m1.3.3.3.1.1.1.2">subscript</csymbol><ci id="S2.E3.m1.3.3.3.1.1.1.2.2.cmml" xref="S2.E3.m1.3.3.3.1.1.1.2.2">𝑝</ci><cn type="integer" id="S2.E3.m1.3.3.3.1.1.1.2.3.cmml" xref="S2.E3.m1.3.3.3.1.1.1.2.3">1</cn></apply><apply id="S2.E3.m1.3.3.3.1.1.1.3.cmml" xref="S2.E3.m1.3.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.3.1.1.1.3.1.cmml" xref="S2.E3.m1.3.3.3.1.1.1.3">subscript</csymbol><ci id="S2.E3.m1.3.3.3.1.1.1.3.2.cmml" xref="S2.E3.m1.3.3.3.1.1.1.3.2">𝑝</ci><cn type="integer" id="S2.E3.m1.3.3.3.1.1.1.3.3.cmml" xref="S2.E3.m1.3.3.3.1.1.1.3.3">4</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.3c">EAR=\frac{||p_{2}-p_{6}||+||p_{3}-p_{5}||}{2||p_{1}-p_{4}||}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p">A traditional approach for detecting eye blinks involves setting a threshold and considering a number of consecutive frames that fall below this threshold as a blink event. While this method is straightforward to implement, it carries the risk of misjudgment. Specifically, a low EAR value does not necessarily mean a blink event, as emotional expressions or certain facial movements can also cause a reduction in the EAR value, potentially leading to the error judge of blinking events. To address this issue, we explored the training of a support vector machine (SVM) classifier for blink detection using EAR values within a temporal sliding window. Given that the duration of blinking typically falls within the range of 0.1 to 0.4 seconds, and considering a video with a frame rate of 30 fps, we employ a sliding window of seven frames. This window includes the current frame, as well as three frames before and after. We create the training dataset by frame-by-frame labeling of videos.</p>
</div>
<div id="S2.SS2.p7" class="ltx_para">
<p id="S2.SS2.p7.1" class="ltx_p">The method of using the EAR threshold to identify blinking is highly reliant on the precise selection of the threshold value. If the chosen value is too small, it may fail to recognize some blinks. Conversely, if set too high, actions induced by emotional expressions, such as squinting, may be classified as blinks, resulting in false recognition outcomes. <a href="#S2.F3" title="Figure 3 ‣ 2.2 Blink ‣ 2 Method ‣ EmoFace: Audio-driven Emotional 3D Face Animation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a> presents a comparison between the results of the SVM predictor and the threshold-based predictor. Although there is one false-predicted frame by SVM predictor, as only consecutive frames will be considered as blink, the SVM prediction identifies one blink, whereas the threshold-based prediction identifies two. By employing the SVM model, which does not depend on exquisitely set EAR threshold, the occurrence of false blink recognition is effectively avoided, making the frequency statistics more reliable.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2407.12501/assets/figures/svm.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Comparison between SVM and threshold predictor</figcaption>
</figure>
<div id="S2.SS2.p8" class="ltx_para">
<p id="S2.SS2.p8.1" class="ltx_p">Using the trained SVM model, we can collect the frequency of blinking from the dataset. Given that the videos are relatively short, we initially collect the time differences between two consecutive blinks and subsequently convert them into the number of blinks per minute for fitting purposes. <a href="#S2.F4" title="Figure 4 ‣ 2.2 Blink ‣ 2 Method ‣ EmoFace: Audio-driven Emotional 3D Face Animation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 4</span></a> shows the collected frequency, and its fitting result of the log-norm distribution curve. Values exceeding 100 are excluded from the analysis. It can be seen from the results that the blink frequency basically satisfies the log-norm distribution. The mean and standard deviation of the natural logarithm of this distribution are calculated as 3.518 and 0.532, respectively.</p>
</div>
<div id="S2.SS2.p9" class="ltx_para">
<p id="S2.SS2.p9.1" class="ltx_p">With the log-normal curve successfully fitted, the time intervals between blinks can be sampled from this distribution. When it is time for a blink, the blinking action is regulated by multiplying the blinking control parameters generated by the original eye values within a window of 13 frames, considering that the target frame rate is 60 frames per second. The incorporation of an independent blink controller effectively governs the blinking action, ensuring that the avatar blinks at a plausible and randomized frequency. Our approach also provides users with the flexibility to freely adjust the blink rate by manipulating the distribution parameters.</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2407.12501/assets/figures/log-norm.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="405" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Collected blinks in a minute and its log-norm fitting curve</figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Gaze</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">An additional issue related to upper face generation is the fixed eye gaze, which imparts a static and unnatural appearance to the talking head. This issue stems from the fact that the recording equipment is placed directly in front of the face. Consequently, during recording, the actor tends to maintain a straight-ahead gaze, leading to an absence of eye movement data in the dataset.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">To tackle this issue, our initial approach involved extracting eye dynamics to train a model for audio-driven gaze prediction. We employed OpenFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> to capture the eye gaze angles from the MEAD dataset, forming a corresponding audio-gaze dataset. Subsequently, we deployed a model that combines wav2vec2.0 and a DNN to learn the mapping between audio and gaze. However, the result of this training reveals that the gaze actions generated by the model tends to remain static, with little variation in gaze angles. This can be due to the concentration of gaze angle data within a limited range, which causes the model’s output towards the mean value of the training data. Then we doubled the gaze angles in the training dataset, which led to the emergence of eye darts. Nevertheless, a notable problem is that these eye darts demonstrate a high degree of consistency among different audio inputs and tend to favor a straight-ahead gaze over other directions.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Previous study has highlighted that eye movement is influenced by a multitude of factors, such as the accent of the spoken text and head posture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Therefore, it is difficult in principle to learn the irregular mapping relationship between audio and gaze. Consequently, we turned our attention to Nvidia’s Omniverse Audio2Face and observed its generated animations. While Audio2Face is capable of producing more natural blinks and eye movements, we observed a uniformity in facial movements across different audio inputs, similar to what we encountered with model-driven gaze generation. This uniformity, when applied to multiple audio inputs simultaneously, could result in a monotonous outcome.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">Therefore, we considered a random yet efficient way. Parameters in the random generator include the range for sampling frame intervals, gaze radius and gaze angle, as shown in <a href="#S2.F5" title="Figure 5 ‣ 2.3 Gaze ‣ 2 Method ‣ EmoFace: Audio-driven Emotional 3D Face Animation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>. In each cycle, the generator randomly samples the number of interval frames, radius and angle in the set range. And smoothly moves the eyeball to the angular position on the corresponding amplitude. To avoid constant movement, it has a 40% chance of returning to the center of eye. This value is set by comparing between results of different possibilities. If lower, the frequency of eye rolls can be too high. If higher, there are cases of staring for a long time. By fine-tuning the range settings for the random sampler, we can achieve a more natural result and ensure compatibility with audio inputs of varying lengths. In this model, we have set the interval frame range to be between 15 and 45 frames (equivalent to 0.25-0.75 seconds), with a radius range of 0.1-0.2. The transitions between different states follow a linear interpolation approach.</p>
</div>
<figure id="S2.F5" class="ltx_figure"><img src="/html/2407.12501/assets/figures/gaze.png" id="S2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="262" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Gaze sampling procedure</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiment</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Considering the absence of a suitable publicly available dataset, we undertook the task of recording and processing data to create our own audio-rig dataset. The data collection process involved gathering audio-visual data from a carefully chosen actor, who performed in seven different emotions. To ensure the accuracy and naturalness of the performances, we planned and executed the data collection process, including the choice of audio content, the method employed for data collection, and the post-processing of videos. Moreover, to guarantee the audio’s quality, we thoughtfully selected emotionally consistent text covering possible phonemes. As different people have different ways to show emotions, the data collected from only one actor may limit the creativity of the model. We will expand the dataset by collecting data from different actors in the future.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">For the design of the audio content, our aim was to maximize the phonemes within the speech material. The data for each emotion was divided into two segments: common and special. The common texts were devoid of explicit emotional cues and could be used across different emotions. In contrast, the special texts were tailored to convey specific emotional information and were exclusively employed in the dataset for that particular emotion. While for the emotion category, we defined seven different emotions including happy, sad, angry, surprised, fear, disgusted and neutral emotion states.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Upon recording the audio and videos, we proceeded to process this data to obtain the face controller values that serve as inputs for driving the MetaHuman model. A team of artists are responsible for converting video to rig sequence. They manually adjusted the controller values of the MetaHuman model to match the recorded video. By creating keyframes in this way, ground truth controller values can be obtained. The dataset consists of a total of 174 parameters. For each emotion, a distinct subset of these parameters is used, with any unused parameters set to 0. The 174 parameters separately control different parts of the face, mainly eye, jaw, mouth, teeth, tongue, brow, ear, nose and neck. Furthermore, the dataset was divided into a training set and a validation set, and the precise distribution is illustrated in <a href="#S3.T1" title="Table 1 ‣ 3.1 Dataset ‣ 3 Experiment ‣ EmoFace: Audio-driven Emotional 3D Face Animation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Data distribution of different emotions</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Emotion</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<div id="S3.T1.1.1.1.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:4.9pt;height:16.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:16.1pt;transform:translate(-5.64pt,-5.64pt) rotate(-90deg) ;">
<p id="S3.T1.1.1.1.2.1.1" class="ltx_p"><span id="S3.T1.1.1.1.2.1.1.1" class="ltx_text" style="font-size:70%;">Total</span></p>
</span></div>
</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<div id="S3.T1.1.1.1.3.1" class="ltx_inline-block ltx_transformed_outer" style="width:4.9pt;height:23.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:23.2pt;transform:translate(-9.15pt,-9.15pt) rotate(-90deg) ;">
<p id="S3.T1.1.1.1.3.1.1" class="ltx_p"><span id="S3.T1.1.1.1.3.1.1.1" class="ltx_text" style="font-size:70%;">Neutral</span></p>
</span></div>
</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<div id="S3.T1.1.1.1.4.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.1pt;height:20pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:20.0pt;transform:translate(-6.94pt,-6.26pt) rotate(-90deg) ;">
<p id="S3.T1.1.1.1.4.1.1" class="ltx_p"><span id="S3.T1.1.1.1.4.1.1.1" class="ltx_text" style="font-size:70%;">Happy</span></p>
</span></div>
</th>
<th id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<div id="S3.T1.1.1.1.5.1" class="ltx_inline-block ltx_transformed_outer" style="width:4.9pt;height:11.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:11.3pt;transform:translate(-3.21pt,-3.21pt) rotate(-90deg) ;">
<p id="S3.T1.1.1.1.5.1.1" class="ltx_p"><span id="S3.T1.1.1.1.5.1.1.1" class="ltx_text" style="font-size:70%;">Sad</span></p>
</span></div>
</th>
<th id="S3.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<div id="S3.T1.1.1.1.6.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.1pt;height:19.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:19.1pt;transform:translate(-6.47pt,-5.78pt) rotate(-90deg) ;">
<p id="S3.T1.1.1.1.6.1.1" class="ltx_p"><span id="S3.T1.1.1.1.6.1.1.1" class="ltx_text" style="font-size:70%;">Angry</span></p>
</span></div>
</th>
<th id="S3.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<div id="S3.T1.1.1.1.7.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.1pt;height:25pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:25.0pt;transform:translate(-9.41pt,-8.73pt) rotate(-90deg) ;">
<p id="S3.T1.1.1.1.7.1.1" class="ltx_p"><span id="S3.T1.1.1.1.7.1.1.1" class="ltx_text" style="font-size:70%;">Surprise</span></p>
</span></div>
</th>
<th id="S3.T1.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<div id="S3.T1.1.1.1.8.1" class="ltx_inline-block ltx_transformed_outer" style="width:4.8pt;height:13.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:13.3pt;transform:translate(-4.28pt,-4.28pt) rotate(-90deg) ;">
<p id="S3.T1.1.1.1.8.1.1" class="ltx_p"><span id="S3.T1.1.1.1.8.1.1.1" class="ltx_text" style="font-size:70%;">Fear</span></p>
</span></div>
</th>
<th id="S3.T1.1.1.1.9" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">
<div id="S3.T1.1.1.1.9.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.2pt;height:29.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:29.9pt;transform:translate(-11.85pt,-11.17pt) rotate(-90deg) ;">
<p id="S3.T1.1.1.1.9.1.1" class="ltx_p"><span id="S3.T1.1.1.1.9.1.1.1" class="ltx_text" style="font-size:70%;">Disgusted</span></p>
</span></div>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Train</span></td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.2.1.2.1" class="ltx_text" style="font-size:70%;">806</span></td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.2.1.3.1" class="ltx_text" style="font-size:70%;">141</span></td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.2.1.4.1" class="ltx_text" style="font-size:70%;">143</span></td>
<td id="S3.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.2.1.5.1" class="ltx_text" style="font-size:70%;">143</span></td>
<td id="S3.T1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.2.1.6.1" class="ltx_text" style="font-size:70%;">142</span></td>
<td id="S3.T1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.2.1.7.1" class="ltx_text" style="font-size:70%;">95</span></td>
<td id="S3.T1.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.2.1.8.1" class="ltx_text" style="font-size:70%;">95</span></td>
<td id="S3.T1.1.2.1.9" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T1.1.2.1.9.1" class="ltx_text" style="font-size:70%;">47</span></td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_right"><span id="S3.T1.1.3.2.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Validation</span></td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center"><span id="S3.T1.1.3.2.2.1" class="ltx_text" style="font-size:70%;">41</span></td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center"><span id="S3.T1.1.3.2.3.1" class="ltx_text" style="font-size:70%;">7</span></td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center"><span id="S3.T1.1.3.2.4.1" class="ltx_text" style="font-size:70%;">7</span></td>
<td id="S3.T1.1.3.2.5" class="ltx_td ltx_align_center"><span id="S3.T1.1.3.2.5.1" class="ltx_text" style="font-size:70%;">7</span></td>
<td id="S3.T1.1.3.2.6" class="ltx_td ltx_align_center"><span id="S3.T1.1.3.2.6.1" class="ltx_text" style="font-size:70%;">7</span></td>
<td id="S3.T1.1.3.2.7" class="ltx_td ltx_align_center"><span id="S3.T1.1.3.2.7.1" class="ltx_text" style="font-size:70%;">5</span></td>
<td id="S3.T1.1.3.2.8" class="ltx_td ltx_align_center"><span id="S3.T1.1.3.2.8.1" class="ltx_text" style="font-size:70%;">5</span></td>
<td id="S3.T1.1.3.2.9" class="ltx_td ltx_align_right"><span id="S3.T1.1.3.2.9.1" class="ltx_text" style="font-size:70%;">3</span></td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.4.3.1" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S3.T1.1.4.3.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Total</span></td>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.1.4.3.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">847</span></td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.1.4.3.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">148</span></td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.1.4.3.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">150</span></td>
<td id="S3.T1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.1.4.3.5.1" class="ltx_text ltx_font_bold" style="font-size:70%;">150</span></td>
<td id="S3.T1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.1.4.3.6.1" class="ltx_text ltx_font_bold" style="font-size:70%;">149</span></td>
<td id="S3.T1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.1.4.3.7.1" class="ltx_text ltx_font_bold" style="font-size:70%;">100</span></td>
<td id="S3.T1.1.4.3.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T1.1.4.3.8.1" class="ltx_text ltx_font_bold" style="font-size:70%;">100</span></td>
<td id="S3.T1.1.4.3.9" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S3.T1.1.4.3.9.1" class="ltx_text ltx_font_bold" style="font-size:70%;">50</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">We used phoneme toolkit (phkit) perform to phoneme decomposition of the text, enabling the analysis of speech text content. Chinese phonemes differ from those in English as they include initials, finals, and tones. Our focus remained on the initials and finals, which comprise 27 types of initials and 41 types of finals. We tracked the number of occurrences of different phonemes, and phkit can decompose it into a total of 65 distinct phonemes. An example of phoneme distribution of happy emotion is shown in <a href="#S3.F6" title="Figure 6 ‣ 3.1 Dataset ‣ 3 Experiment ‣ EmoFace: Audio-driven Emotional 3D Face Animation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>. It is worth noting that the corpus we devised comprehensively covers nearly all phonemes, ensuring that the training set incorporates a wide range of mouth animations.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2407.12501/assets/figures/happy_phonemes.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="316" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Phoneme distribution of happy emotion</figcaption>
</figure>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">Then, we conducted an analysis of the symmetry between the left and right part of the face. This involved the derivation of a correlation coefficient heat map that illustrates the relationships between parameters for the left and right sides of the face, as shown in <a href="#S3.F7" title="Figure 7 ‣ 3.1 Dataset ‣ 3 Experiment ‣ EmoFace: Audio-driven Emotional 3D Face Animation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>. As we can see, there are three diagonal lines in this heat map, which indicates that although the symmetric controller values are not strictly consistent, they do exhibit a high degree of correlation.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2407.12501/assets/figures/heatmap.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="572" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Correlation heat map of left and right face controller rigs</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Model Details</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In terms of the parameter settings, we employed the Adam optimizer. As for the learning rate strategy, training was conducted using a StepLR scheduler with a step size of 100 and a decay rate of 0.995. This means that every 100 rounds of training, the learning rate was reduced to 0.995 times its current value. In the selection of the loss function, we utilized the mean square error (MSE) to quantify the difference between the predicted 174 parameters for each frame and the ground truth. The training lasted a total of 3000 rounds.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Our model does not require audio pre-processing, the audio just needs to be loaded at a frequency of 16KHz. In the pre-training stage, we utilized the wav2vec2.0 BASE structure. Specifically, we employed the wav2vec2-base-960h pre-trained model. The weight of feature extractor in this pre-training model is frozen during training. The hidden states output by the pre-trained model is first mapped through a fully connected layer, and then the 512-dimensional vector is generated through the positional encoding layer. In the case of the emotion encoder, it comprises an embedding layer and two fully connected layers. We applied Leaky ReLU with a slope of 0.2 as the activation function between the fully connected layers. The emotion encoder also outputs a 512-dimensional encoding. The Audio2Rig module is structured as 10 transformer encoder layers and a fully connected layer. This component is responsible for mapping the 512-dimensional hidden states into a 174-dimensional controller rig sequence.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Comparison to state-of-the-art</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We conducted a comparative analysis between the results produced by EmoFace, FaceFormer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and EmoTalk <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. But the other two methods are designed for other datasets. FaceFormer uses VOCASET, an audio-mesh dataset. And EmoTalk proposed 3D-ETF based on RAVDESS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> and HDTF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, which is audio-blend shape correlated. To better compare them in our dataset, we modified FaceFormer and EmoTalk to fit the emotion label and output dimension of our dataset. We removed ”template” in FaceFormer, ”level” and ”person” in EmoTalk. And the output dimensions for both models are set to 174.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of MAE with state-of-the-art</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T2.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Model</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<div id="S4.T2.1.1.1.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:4.9pt;height:27.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:27.8pt;transform:translate(-11.47pt,-11.47pt) rotate(-90deg) ;">
<p id="S4.T2.1.1.1.2.1.1" class="ltx_p"><span id="S4.T2.1.1.1.2.1.1.1" class="ltx_text" style="font-size:70%;">Full-Face</span></p>
</span></div>
</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<div id="S4.T2.1.1.1.3.1" class="ltx_inline-block ltx_transformed_outer" style="width:4.9pt;height:37.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:37.4pt;transform:translate(-16.25pt,-16.25pt) rotate(-90deg) ;">
<p id="S4.T2.1.1.1.3.1.1" class="ltx_p"><span id="S4.T2.1.1.1.3.1.1.1" class="ltx_text" style="font-size:70%;">Mouth-Area</span></p>
</span></div>
</th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<div id="S4.T2.1.1.1.4.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.1pt;height:28.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:28.3pt;transform:translate(-11.08pt,-10.4pt) rotate(-90deg) ;">
<p id="S4.T2.1.1.1.4.1.1" class="ltx_p"><span id="S4.T2.1.1.1.4.1.1.1" class="ltx_text" style="font-size:70%;">Eye-Area</span></p>
</span></div>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t"><span id="S4.T2.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">EmoFace</span></th>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.1.2.1" class="ltx_text" style="font-size:70%;">0.04024</span></td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.1.3.1" class="ltx_text" style="font-size:70%;">0.03697</span></td>
<td id="S4.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.1.4.1" class="ltx_text" style="font-size:70%;">0.05698</span></td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<th id="S4.T2.1.3.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="S4.T2.1.3.2.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">FaceFormer</span></th>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.3.2.2.1" class="ltx_text" style="font-size:70%;">0.04114</span></td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.3.2.3.1" class="ltx_text" style="font-size:70%;">0.03919</span></td>
<td id="S4.T2.1.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.3.2.4.1" class="ltx_text" style="font-size:70%;">0.05389</span></td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<th id="S4.T2.1.4.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b"><span id="S4.T2.1.4.3.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">EmoTalk</span></th>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.4.3.2.1" class="ltx_text" style="font-size:70%;">0.04273</span></td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.4.3.3.1" class="ltx_text" style="font-size:70%;">0.03963</span></td>
<td id="S4.T2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.1.4.3.4.1" class="ltx_text" style="font-size:70%;">0.05956</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Quantitative analysis</h5>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">We use the mean absolute value (MAE) of the prediction results and the ground truth to evaluate the models. The comparison is shown in <a href="#S4.T2" title="Table 2 ‣ 4.1 Comparison to state-of-the-art ‣ 4 Evaluation ‣ EmoFace: Audio-driven Emotional 3D Face Animation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 2</span></a>. The table indicates that EmoFace exhibits a smaller MAE, and its predictions are closer to the ground truth. However, the difference between EmoFace and FaceFormer is not significant. As EmoTalk requires paired audios as inputs, special audios that contained only in certain emotions cannot be used, which may have negative impact on the performance. What’s more, EmoTalk does not directly use emotion label as input, but disentangles emotion embedding from the audio, any wrong prediction can seriously affect the predicted facial expression. To gain further insights, we conducted a separate analysis of the parameters in mouth area and eye area respectively. The controller rigs of the mouth region have a more significant impact on audio-lip synchronization. Conversely, the controller rigs affecting the eye area play a crucial role in conveying emotions. Our results demonstrate that EmoFace outperforms FaceFormer and EmoTalk in terms of mouth-related areas, although it slightly lags behind in other facial regions.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Qualitative analysis</h5>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">Considering the complex many-to-many mapping between upper face movements and audios, the quantitative evaluation metrics may not provide a completely accurate reflection of the prediction results. To address this, we rendered the predicted animations using a MetaHuman model, and the comparison is depicted in <a href="#S4.F8" title="Figure 8 ‣ Qualitative analysis ‣ 4.1 Comparison to state-of-the-art ‣ 4 Evaluation ‣ EmoFace: Audio-driven Emotional 3D Face Animation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 8</span></a>. In terms of conveying emotional expressions, EmoFace and FaceFormer excel at capturing emotional characteristics. But as EmoTalk first predicts emotion from the audio clip, a wrong prediction can cause severe error in facial expression. However, when examining mouth dynamics, the lip movement of FaceFormer is slightly insufficient, resulting in half-open mouth in many cases. In contrast, the lip movements generated by EmoFace and EmoTalk are able to maintain synchronization with the audio.</p>
</div>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2407.12501/assets/figures/sota.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="419" height="419" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Comparison of rendered animation with state-of-the-art</figcaption>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>User Study</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We designed a user study to compare with FaceFormer and EmoTalk to evaluate our model.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Participant and Design</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">We recruited 15 participants from Shanghai Jiao Tong University for the user study, the average age was 22, ranging from 18 to 25 years old; 10 were men. They were naive to the purpose of the experiment.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">We used the three trained models to render the two video clips for each emotion in a MetaHuman model for three different methods, as well as the extracted ground truth (GT).</p>
</div>
<div id="S4.SS2.SSS1.p3" class="ltx_para">
<p id="S4.SS2.SSS1.p3.1" class="ltx_p">The experiment involved 7 emotions (Angry, Disgust, Fear, Happy, Neutral, Sad &amp; Surprise), 4 methods (GT, EmoFace, FaceFormer &amp; EmoTalk) and 2 sentences (Common sentences, Emotion-related sentences) in a within-subject design regarding emotions, and methods. The video clips were presented to the participants in random order. Each participant took part in 56 trials to evaluate the human expression, there were 840 trials in total.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Procedure</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">Each participant were asked to answer the following questions for each rendered animation:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Emotion Recognition: “Which emotion is expressed?” Participants were asked to select one emotion from: Angry, Disgust, Fear, Happy, Neutral, Sad and Surprise.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Naturalness: “How natural is the generated face?” Participants rated naturalness from 1 to 7, where 1 represents ”not natural at all”, and 7 represents “very realistic”.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Lip Synchronization: “Is the lip motion in sync with audio?” Participants rated on quality of lip synchronization from 1 to 7, where 1 represents not synchronized at all, and 7 represents perfect synchronization.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">The whole experiment took about 15 minutes. The participants were paid 20 RMB amount. The experiment was approved by Shanghai Jiao Tong University Research Ethics Committee.</p>
</div>
<figure id="S4.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S4.F9.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:138.8pt;">
<img src="/html/2407.12501/assets/figures/recognition.png" id="S4.F9.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="409" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S4.F9.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:138.8pt;">
<img src="/html/2407.12501/assets/figures/naturalness.png" id="S4.F9.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="409" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div id="S4.F9.3" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:138.8pt;">
<img src="/html/2407.12501/assets/figures/sync.png" id="S4.F9.3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="409" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Results of User Study</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of ratings with 95% confidence interval</figcaption>
<table id="S4.T3.12" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.12.13.1" class="ltx_tr">
<th id="S4.T3.12.13.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T3.12.13.1.1.1" class="ltx_text" style="font-size:70%;">Model</span></th>
<th id="S4.T3.12.13.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<div id="S4.T3.12.13.1.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.1pt;height:36.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:36.3pt;transform:translate(-15.06pt,-14.38pt) rotate(-90deg) ;">
<p id="S4.T3.12.13.1.2.1.1" class="ltx_p"><span id="S4.T3.12.13.1.2.1.1.1" class="ltx_text" style="font-size:70%;">Recognition</span></p>
</span></div>
</th>
<th id="S4.T3.12.13.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<div id="S4.T3.12.13.1.3.1" class="ltx_inline-block ltx_transformed_outer" style="width:4.9pt;height:36.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:36.1pt;transform:translate(-15.6pt,-15.6pt) rotate(-90deg) ;">
<p id="S4.T3.12.13.1.3.1.1" class="ltx_p"><span id="S4.T3.12.13.1.3.1.1.1" class="ltx_text" style="font-size:70%;">Naturalness</span></p>
</span></div>
</th>
<th id="S4.T3.12.13.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<div id="S4.T3.12.13.1.4.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.1pt;height:27.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:27.1pt;transform:translate(-10.49pt,-9.81pt) rotate(-90deg) ;">
<p id="S4.T3.12.13.1.4.1.1" class="ltx_p"><span id="S4.T3.12.13.1.4.1.1.1" class="ltx_text" style="font-size:70%;">Lip-Sync</span></p>
</span></div>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.3.3" class="ltx_tr">
<th id="S4.T3.3.3.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.3.3.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">GT</span></th>
<td id="S4.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T3.1.1.1.m1.1" class="ltx_Math" alttext="0.833\pm 0.051" display="inline"><semantics id="S4.T3.1.1.1.m1.1a"><mrow id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml"><mn mathsize="70%" id="S4.T3.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.m1.1.1.2.cmml">0.833</mn><mo mathsize="70%" id="S4.T3.1.1.1.m1.1.1.1" xref="S4.T3.1.1.1.m1.1.1.1.cmml">±</mo><mn mathsize="70%" id="S4.T3.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.m1.1.1.3.cmml">0.051</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T3.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T3.1.1.1.m1.1.1.2.cmml" xref="S4.T3.1.1.1.m1.1.1.2">0.833</cn><cn type="float" id="S4.T3.1.1.1.m1.1.1.3.cmml" xref="S4.T3.1.1.1.m1.1.1.3">0.051</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">0.833\pm 0.051</annotation></semantics></math></td>
<td id="S4.T3.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T3.2.2.2.m1.1" class="ltx_Math" alttext="6.228\pm 0.098" display="inline"><semantics id="S4.T3.2.2.2.m1.1a"><mrow id="S4.T3.2.2.2.m1.1.1" xref="S4.T3.2.2.2.m1.1.1.cmml"><mn mathsize="70%" id="S4.T3.2.2.2.m1.1.1.2" xref="S4.T3.2.2.2.m1.1.1.2.cmml">6.228</mn><mo mathsize="70%" id="S4.T3.2.2.2.m1.1.1.1" xref="S4.T3.2.2.2.m1.1.1.1.cmml">±</mo><mn mathsize="70%" id="S4.T3.2.2.2.m1.1.1.3" xref="S4.T3.2.2.2.m1.1.1.3.cmml">0.098</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.m1.1b"><apply id="S4.T3.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.m1.1.1"><csymbol cd="latexml" id="S4.T3.2.2.2.m1.1.1.1.cmml" xref="S4.T3.2.2.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T3.2.2.2.m1.1.1.2.cmml" xref="S4.T3.2.2.2.m1.1.1.2">6.228</cn><cn type="float" id="S4.T3.2.2.2.m1.1.1.3.cmml" xref="S4.T3.2.2.2.m1.1.1.3">0.098</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.m1.1c">6.228\pm 0.098</annotation></semantics></math></td>
<td id="S4.T3.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T3.3.3.3.m1.1" class="ltx_Math" alttext="6.076\pm 0.083" display="inline"><semantics id="S4.T3.3.3.3.m1.1a"><mrow id="S4.T3.3.3.3.m1.1.1" xref="S4.T3.3.3.3.m1.1.1.cmml"><mn mathsize="70%" id="S4.T3.3.3.3.m1.1.1.2" xref="S4.T3.3.3.3.m1.1.1.2.cmml">6.076</mn><mo mathsize="70%" id="S4.T3.3.3.3.m1.1.1.1" xref="S4.T3.3.3.3.m1.1.1.1.cmml">±</mo><mn mathsize="70%" id="S4.T3.3.3.3.m1.1.1.3" xref="S4.T3.3.3.3.m1.1.1.3.cmml">0.083</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.m1.1b"><apply id="S4.T3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.m1.1.1"><csymbol cd="latexml" id="S4.T3.3.3.3.m1.1.1.1.cmml" xref="S4.T3.3.3.3.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T3.3.3.3.m1.1.1.2.cmml" xref="S4.T3.3.3.3.m1.1.1.2">6.076</cn><cn type="float" id="S4.T3.3.3.3.m1.1.1.3.cmml" xref="S4.T3.3.3.3.m1.1.1.3">0.083</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.m1.1c">6.076\pm 0.083</annotation></semantics></math></td>
</tr>
<tr id="S4.T3.6.6" class="ltx_tr">
<th id="S4.T3.6.6.4" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="S4.T3.6.6.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">EmoFace</span></th>
<td id="S4.T3.4.4.1" class="ltx_td ltx_align_center"><math id="S4.T3.4.4.1.m1.1" class="ltx_Math" alttext="0.805\pm 0.054" display="inline"><semantics id="S4.T3.4.4.1.m1.1a"><mrow id="S4.T3.4.4.1.m1.1.1" xref="S4.T3.4.4.1.m1.1.1.cmml"><mn mathsize="70%" id="S4.T3.4.4.1.m1.1.1.2" xref="S4.T3.4.4.1.m1.1.1.2.cmml">0.805</mn><mo mathsize="70%" id="S4.T3.4.4.1.m1.1.1.1" xref="S4.T3.4.4.1.m1.1.1.1.cmml">±</mo><mn mathsize="70%" id="S4.T3.4.4.1.m1.1.1.3" xref="S4.T3.4.4.1.m1.1.1.3.cmml">0.054</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.1.m1.1b"><apply id="S4.T3.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.1.m1.1.1"><csymbol cd="latexml" id="S4.T3.4.4.1.m1.1.1.1.cmml" xref="S4.T3.4.4.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T3.4.4.1.m1.1.1.2.cmml" xref="S4.T3.4.4.1.m1.1.1.2">0.805</cn><cn type="float" id="S4.T3.4.4.1.m1.1.1.3.cmml" xref="S4.T3.4.4.1.m1.1.1.3">0.054</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.1.m1.1c">0.805\pm 0.054</annotation></semantics></math></td>
<td id="S4.T3.5.5.2" class="ltx_td ltx_align_center"><math id="S4.T3.5.5.2.m1.1" class="ltx_Math" alttext="5.514\pm 0.141" display="inline"><semantics id="S4.T3.5.5.2.m1.1a"><mrow id="S4.T3.5.5.2.m1.1.1" xref="S4.T3.5.5.2.m1.1.1.cmml"><mn mathsize="70%" id="S4.T3.5.5.2.m1.1.1.2" xref="S4.T3.5.5.2.m1.1.1.2.cmml">5.514</mn><mo mathsize="70%" id="S4.T3.5.5.2.m1.1.1.1" xref="S4.T3.5.5.2.m1.1.1.1.cmml">±</mo><mn mathsize="70%" id="S4.T3.5.5.2.m1.1.1.3" xref="S4.T3.5.5.2.m1.1.1.3.cmml">0.141</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.2.m1.1b"><apply id="S4.T3.5.5.2.m1.1.1.cmml" xref="S4.T3.5.5.2.m1.1.1"><csymbol cd="latexml" id="S4.T3.5.5.2.m1.1.1.1.cmml" xref="S4.T3.5.5.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T3.5.5.2.m1.1.1.2.cmml" xref="S4.T3.5.5.2.m1.1.1.2">5.514</cn><cn type="float" id="S4.T3.5.5.2.m1.1.1.3.cmml" xref="S4.T3.5.5.2.m1.1.1.3">0.141</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.2.m1.1c">5.514\pm 0.141</annotation></semantics></math></td>
<td id="S4.T3.6.6.3" class="ltx_td ltx_align_center"><math id="S4.T3.6.6.3.m1.1" class="ltx_Math" alttext="5.509\pm 0.100" display="inline"><semantics id="S4.T3.6.6.3.m1.1a"><mrow id="S4.T3.6.6.3.m1.1.1" xref="S4.T3.6.6.3.m1.1.1.cmml"><mn mathsize="70%" id="S4.T3.6.6.3.m1.1.1.2" xref="S4.T3.6.6.3.m1.1.1.2.cmml">5.509</mn><mo mathsize="70%" id="S4.T3.6.6.3.m1.1.1.1" xref="S4.T3.6.6.3.m1.1.1.1.cmml">±</mo><mn mathsize="70%" id="S4.T3.6.6.3.m1.1.1.3" xref="S4.T3.6.6.3.m1.1.1.3.cmml">0.100</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.3.m1.1b"><apply id="S4.T3.6.6.3.m1.1.1.cmml" xref="S4.T3.6.6.3.m1.1.1"><csymbol cd="latexml" id="S4.T3.6.6.3.m1.1.1.1.cmml" xref="S4.T3.6.6.3.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T3.6.6.3.m1.1.1.2.cmml" xref="S4.T3.6.6.3.m1.1.1.2">5.509</cn><cn type="float" id="S4.T3.6.6.3.m1.1.1.3.cmml" xref="S4.T3.6.6.3.m1.1.1.3">0.100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.3.m1.1c">5.509\pm 0.100</annotation></semantics></math></td>
</tr>
<tr id="S4.T3.9.9" class="ltx_tr">
<th id="S4.T3.9.9.4" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="S4.T3.9.9.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">FaceFormer</span></th>
<td id="S4.T3.7.7.1" class="ltx_td ltx_align_center"><math id="S4.T3.7.7.1.m1.1" class="ltx_Math" alttext="0.776\pm 0.057" display="inline"><semantics id="S4.T3.7.7.1.m1.1a"><mrow id="S4.T3.7.7.1.m1.1.1" xref="S4.T3.7.7.1.m1.1.1.cmml"><mn mathsize="70%" id="S4.T3.7.7.1.m1.1.1.2" xref="S4.T3.7.7.1.m1.1.1.2.cmml">0.776</mn><mo mathsize="70%" id="S4.T3.7.7.1.m1.1.1.1" xref="S4.T3.7.7.1.m1.1.1.1.cmml">±</mo><mn mathsize="70%" id="S4.T3.7.7.1.m1.1.1.3" xref="S4.T3.7.7.1.m1.1.1.3.cmml">0.057</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.1.m1.1b"><apply id="S4.T3.7.7.1.m1.1.1.cmml" xref="S4.T3.7.7.1.m1.1.1"><csymbol cd="latexml" id="S4.T3.7.7.1.m1.1.1.1.cmml" xref="S4.T3.7.7.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T3.7.7.1.m1.1.1.2.cmml" xref="S4.T3.7.7.1.m1.1.1.2">0.776</cn><cn type="float" id="S4.T3.7.7.1.m1.1.1.3.cmml" xref="S4.T3.7.7.1.m1.1.1.3">0.057</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.1.m1.1c">0.776\pm 0.057</annotation></semantics></math></td>
<td id="S4.T3.8.8.2" class="ltx_td ltx_align_center"><math id="S4.T3.8.8.2.m1.1" class="ltx_Math" alttext="4.081\pm 0.227" display="inline"><semantics id="S4.T3.8.8.2.m1.1a"><mrow id="S4.T3.8.8.2.m1.1.1" xref="S4.T3.8.8.2.m1.1.1.cmml"><mn mathsize="70%" id="S4.T3.8.8.2.m1.1.1.2" xref="S4.T3.8.8.2.m1.1.1.2.cmml">4.081</mn><mo mathsize="70%" id="S4.T3.8.8.2.m1.1.1.1" xref="S4.T3.8.8.2.m1.1.1.1.cmml">±</mo><mn mathsize="70%" id="S4.T3.8.8.2.m1.1.1.3" xref="S4.T3.8.8.2.m1.1.1.3.cmml">0.227</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.8.8.2.m1.1b"><apply id="S4.T3.8.8.2.m1.1.1.cmml" xref="S4.T3.8.8.2.m1.1.1"><csymbol cd="latexml" id="S4.T3.8.8.2.m1.1.1.1.cmml" xref="S4.T3.8.8.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T3.8.8.2.m1.1.1.2.cmml" xref="S4.T3.8.8.2.m1.1.1.2">4.081</cn><cn type="float" id="S4.T3.8.8.2.m1.1.1.3.cmml" xref="S4.T3.8.8.2.m1.1.1.3">0.227</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.8.2.m1.1c">4.081\pm 0.227</annotation></semantics></math></td>
<td id="S4.T3.9.9.3" class="ltx_td ltx_align_center"><math id="S4.T3.9.9.3.m1.1" class="ltx_Math" alttext="4.609\pm 0.128" display="inline"><semantics id="S4.T3.9.9.3.m1.1a"><mrow id="S4.T3.9.9.3.m1.1.1" xref="S4.T3.9.9.3.m1.1.1.cmml"><mn mathsize="70%" id="S4.T3.9.9.3.m1.1.1.2" xref="S4.T3.9.9.3.m1.1.1.2.cmml">4.609</mn><mo mathsize="70%" id="S4.T3.9.9.3.m1.1.1.1" xref="S4.T3.9.9.3.m1.1.1.1.cmml">±</mo><mn mathsize="70%" id="S4.T3.9.9.3.m1.1.1.3" xref="S4.T3.9.9.3.m1.1.1.3.cmml">0.128</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.9.9.3.m1.1b"><apply id="S4.T3.9.9.3.m1.1.1.cmml" xref="S4.T3.9.9.3.m1.1.1"><csymbol cd="latexml" id="S4.T3.9.9.3.m1.1.1.1.cmml" xref="S4.T3.9.9.3.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T3.9.9.3.m1.1.1.2.cmml" xref="S4.T3.9.9.3.m1.1.1.2">4.609</cn><cn type="float" id="S4.T3.9.9.3.m1.1.1.3.cmml" xref="S4.T3.9.9.3.m1.1.1.3">0.128</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.9.3.m1.1c">4.609\pm 0.128</annotation></semantics></math></td>
</tr>
<tr id="S4.T3.12.12" class="ltx_tr">
<th id="S4.T3.12.12.4" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b"><span id="S4.T3.12.12.4.1" class="ltx_text ltx_font_bold" style="font-size:70%;">EmoTalk</span></th>
<td id="S4.T3.10.10.1" class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T3.10.10.1.m1.1" class="ltx_Math" alttext="0.671\pm 0.064" display="inline"><semantics id="S4.T3.10.10.1.m1.1a"><mrow id="S4.T3.10.10.1.m1.1.1" xref="S4.T3.10.10.1.m1.1.1.cmml"><mn mathsize="70%" id="S4.T3.10.10.1.m1.1.1.2" xref="S4.T3.10.10.1.m1.1.1.2.cmml">0.671</mn><mo mathsize="70%" id="S4.T3.10.10.1.m1.1.1.1" xref="S4.T3.10.10.1.m1.1.1.1.cmml">±</mo><mn mathsize="70%" id="S4.T3.10.10.1.m1.1.1.3" xref="S4.T3.10.10.1.m1.1.1.3.cmml">0.064</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.10.10.1.m1.1b"><apply id="S4.T3.10.10.1.m1.1.1.cmml" xref="S4.T3.10.10.1.m1.1.1"><csymbol cd="latexml" id="S4.T3.10.10.1.m1.1.1.1.cmml" xref="S4.T3.10.10.1.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T3.10.10.1.m1.1.1.2.cmml" xref="S4.T3.10.10.1.m1.1.1.2">0.671</cn><cn type="float" id="S4.T3.10.10.1.m1.1.1.3.cmml" xref="S4.T3.10.10.1.m1.1.1.3">0.064</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.10.10.1.m1.1c">0.671\pm 0.064</annotation></semantics></math></td>
<td id="S4.T3.11.11.2" class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T3.11.11.2.m1.1" class="ltx_Math" alttext="4.601\pm 0.212" display="inline"><semantics id="S4.T3.11.11.2.m1.1a"><mrow id="S4.T3.11.11.2.m1.1.1" xref="S4.T3.11.11.2.m1.1.1.cmml"><mn mathsize="70%" id="S4.T3.11.11.2.m1.1.1.2" xref="S4.T3.11.11.2.m1.1.1.2.cmml">4.601</mn><mo mathsize="70%" id="S4.T3.11.11.2.m1.1.1.1" xref="S4.T3.11.11.2.m1.1.1.1.cmml">±</mo><mn mathsize="70%" id="S4.T3.11.11.2.m1.1.1.3" xref="S4.T3.11.11.2.m1.1.1.3.cmml">0.212</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.11.11.2.m1.1b"><apply id="S4.T3.11.11.2.m1.1.1.cmml" xref="S4.T3.11.11.2.m1.1.1"><csymbol cd="latexml" id="S4.T3.11.11.2.m1.1.1.1.cmml" xref="S4.T3.11.11.2.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T3.11.11.2.m1.1.1.2.cmml" xref="S4.T3.11.11.2.m1.1.1.2">4.601</cn><cn type="float" id="S4.T3.11.11.2.m1.1.1.3.cmml" xref="S4.T3.11.11.2.m1.1.1.3">0.212</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.11.11.2.m1.1c">4.601\pm 0.212</annotation></semantics></math></td>
<td id="S4.T3.12.12.3" class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T3.12.12.3.m1.1" class="ltx_Math" alttext="5.295\pm 0.102" display="inline"><semantics id="S4.T3.12.12.3.m1.1a"><mrow id="S4.T3.12.12.3.m1.1.1" xref="S4.T3.12.12.3.m1.1.1.cmml"><mn mathsize="70%" id="S4.T3.12.12.3.m1.1.1.2" xref="S4.T3.12.12.3.m1.1.1.2.cmml">5.295</mn><mo mathsize="70%" id="S4.T3.12.12.3.m1.1.1.1" xref="S4.T3.12.12.3.m1.1.1.1.cmml">±</mo><mn mathsize="70%" id="S4.T3.12.12.3.m1.1.1.3" xref="S4.T3.12.12.3.m1.1.1.3.cmml">0.102</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.12.12.3.m1.1b"><apply id="S4.T3.12.12.3.m1.1.1.cmml" xref="S4.T3.12.12.3.m1.1.1"><csymbol cd="latexml" id="S4.T3.12.12.3.m1.1.1.1.cmml" xref="S4.T3.12.12.3.m1.1.1.1">plus-or-minus</csymbol><cn type="float" id="S4.T3.12.12.3.m1.1.1.2.cmml" xref="S4.T3.12.12.3.m1.1.1.2">5.295</cn><cn type="float" id="S4.T3.12.12.3.m1.1.1.3.cmml" xref="S4.T3.12.12.3.m1.1.1.3">0.102</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.12.12.3.m1.1c">5.295\pm 0.102</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Result</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">After gaining rating from participants, we conducted separate repeated measures Analysis of Variances (ANOVAs). We calculated the average score for each method with an error bar of 95% confidence interval, as shown in <a href="#S4.T3" title="Table 3 ‣ 4.2.2 Procedure ‣ 4.2 User Study ‣ 4 Evaluation ‣ EmoFace: Audio-driven Emotional 3D Face Animation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 3</span></a>. We ran Mauchly’s test for validating sphericity, and when it is significant, we will apply Greenhouse-Geisser correction and mark the corrected result with “*”. Post-hoc tests were conducted using the Tukey test for the comparison of means.</p>
</div>
<section id="S4.SS2.SSS3.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Emotion Recognition</h5>

<div id="S4.SS2.SSS3.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS3.Px1.p1.3" class="ltx_p">For the recognition of emotions, the choices of users are converted to 0 (incorrect) or 1 (correct). It can be seen that all three methods can well express emotion features. In contrast, our method achieves the best emotion recognition with average score of 0.809. FaceFormer also achieve 0.782, which is almost as good. However, the correctness for EmoTalk is significantly lower with 0.681. We used ANOVA to compare Faceformer and Emotalk respectively with our model. Results revealed the main effect of different models are significant with <math id="S4.SS2.SSS3.Px1.p1.1.m1.1" class="ltx_Math" alttext="p=0.004^{*}" display="inline"><semantics id="S4.SS2.SSS3.Px1.p1.1.m1.1a"><mrow id="S4.SS2.SSS3.Px1.p1.1.m1.1.1" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.2" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.1" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.1.cmml">=</mo><msup id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3.cmml"><mn id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3.2" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3.2.cmml">0.004</mn><mo id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3.3" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3.3.cmml">∗</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.Px1.p1.1.m1.1b"><apply id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1"><eq id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.1"></eq><ci id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.2">𝑝</ci><apply id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3">superscript</csymbol><cn type="float" id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3.2">0.004</cn><times id="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3.3.cmml" xref="S4.SS2.SSS3.Px1.p1.1.m1.1.1.3.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.Px1.p1.1.m1.1c">p=0.004^{*}</annotation></semantics></math>. The post-hoc result shows that our model is significantly better than EmoTalk with <math id="S4.SS2.SSS3.Px1.p1.2.m2.1" class="ltx_Math" alttext="p=0.0059" display="inline"><semantics id="S4.SS2.SSS3.Px1.p1.2.m2.1a"><mrow id="S4.SS2.SSS3.Px1.p1.2.m2.1.1" xref="S4.SS2.SSS3.Px1.p1.2.m2.1.1.cmml"><mi id="S4.SS2.SSS3.Px1.p1.2.m2.1.1.2" xref="S4.SS2.SSS3.Px1.p1.2.m2.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS3.Px1.p1.2.m2.1.1.1" xref="S4.SS2.SSS3.Px1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS3.Px1.p1.2.m2.1.1.3" xref="S4.SS2.SSS3.Px1.p1.2.m2.1.1.3.cmml">0.0059</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.Px1.p1.2.m2.1b"><apply id="S4.SS2.SSS3.Px1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS3.Px1.p1.2.m2.1.1"><eq id="S4.SS2.SSS3.Px1.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS3.Px1.p1.2.m2.1.1.1"></eq><ci id="S4.SS2.SSS3.Px1.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS3.Px1.p1.2.m2.1.1.2">𝑝</ci><cn type="float" id="S4.SS2.SSS3.Px1.p1.2.m2.1.1.3.cmml" xref="S4.SS2.SSS3.Px1.p1.2.m2.1.1.3">0.0059</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.Px1.p1.2.m2.1c">p=0.0059</annotation></semantics></math>, but shows no significance to FaceFormer with <math id="S4.SS2.SSS3.Px1.p1.3.m3.1" class="ltx_Math" alttext="p=0.8959" display="inline"><semantics id="S4.SS2.SSS3.Px1.p1.3.m3.1a"><mrow id="S4.SS2.SSS3.Px1.p1.3.m3.1.1" xref="S4.SS2.SSS3.Px1.p1.3.m3.1.1.cmml"><mi id="S4.SS2.SSS3.Px1.p1.3.m3.1.1.2" xref="S4.SS2.SSS3.Px1.p1.3.m3.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS3.Px1.p1.3.m3.1.1.1" xref="S4.SS2.SSS3.Px1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS2.SSS3.Px1.p1.3.m3.1.1.3" xref="S4.SS2.SSS3.Px1.p1.3.m3.1.1.3.cmml">0.8959</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.Px1.p1.3.m3.1b"><apply id="S4.SS2.SSS3.Px1.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS3.Px1.p1.3.m3.1.1"><eq id="S4.SS2.SSS3.Px1.p1.3.m3.1.1.1.cmml" xref="S4.SS2.SSS3.Px1.p1.3.m3.1.1.1"></eq><ci id="S4.SS2.SSS3.Px1.p1.3.m3.1.1.2.cmml" xref="S4.SS2.SSS3.Px1.p1.3.m3.1.1.2">𝑝</ci><cn type="float" id="S4.SS2.SSS3.Px1.p1.3.m3.1.1.3.cmml" xref="S4.SS2.SSS3.Px1.p1.3.m3.1.1.3">0.8959</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.Px1.p1.3.m3.1c">p=0.8959</annotation></semantics></math>. The result of emotion recognition shows that our method can precisely express emotion in the output facial animation.</p>
</div>
</section>
<section id="S4.SS2.SSS3.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Naturalness</h5>

<div id="S4.SS2.SSS3.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS3.Px2.p1.2" class="ltx_p">The score for naturalness reflects the overall result of the generated facial animation. It can seen that our model with added blink and gaze achieves a rating of 5.559, which significantly surpasses the other two. While FaceFormer achieves 4.127 and EmoTalk achieves 4.569. Our method can achieve a score close to GT of 6.235. As for significance, our model is significantly better than FaceFormer and EmoTalk, with <math id="S4.SS2.SSS3.Px2.p1.1.m1.1" class="ltx_Math" alttext="p&lt;0.001^{*}" display="inline"><semantics id="S4.SS2.SSS3.Px2.p1.1.m1.1a"><mrow id="S4.SS2.SSS3.Px2.p1.1.m1.1.1" xref="S4.SS2.SSS3.Px2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS3.Px2.p1.1.m1.1.1.2" xref="S4.SS2.SSS3.Px2.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS3.Px2.p1.1.m1.1.1.1" xref="S4.SS2.SSS3.Px2.p1.1.m1.1.1.1.cmml">&lt;</mo><msup id="S4.SS2.SSS3.Px2.p1.1.m1.1.1.3" xref="S4.SS2.SSS3.Px2.p1.1.m1.1.1.3.cmml"><mn id="S4.SS2.SSS3.Px2.p1.1.m1.1.1.3.2" xref="S4.SS2.SSS3.Px2.p1.1.m1.1.1.3.2.cmml">0.001</mn><mo id="S4.SS2.SSS3.Px2.p1.1.m1.1.1.3.3" xref="S4.SS2.SSS3.Px2.p1.1.m1.1.1.3.3.cmml">∗</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.Px2.p1.1.m1.1b"><apply id="S4.SS2.SSS3.Px2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS3.Px2.p1.1.m1.1.1"><lt id="S4.SS2.SSS3.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.Px2.p1.1.m1.1.1.1"></lt><ci id="S4.SS2.SSS3.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.Px2.p1.1.m1.1.1.2">𝑝</ci><apply id="S4.SS2.SSS3.Px2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS3.Px2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS3.Px2.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.SSS3.Px2.p1.1.m1.1.1.3">superscript</csymbol><cn type="float" id="S4.SS2.SSS3.Px2.p1.1.m1.1.1.3.2.cmml" xref="S4.SS2.SSS3.Px2.p1.1.m1.1.1.3.2">0.001</cn><times id="S4.SS2.SSS3.Px2.p1.1.m1.1.1.3.3.cmml" xref="S4.SS2.SSS3.Px2.p1.1.m1.1.1.3.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.Px2.p1.1.m1.1c">p&lt;0.001^{*}</annotation></semantics></math>. The post-hoc result shows that our model is significantly better than EmoTalk and EmoFace with <math id="S4.SS2.SSS3.Px2.p1.2.m2.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="S4.SS2.SSS3.Px2.p1.2.m2.1a"><mrow id="S4.SS2.SSS3.Px2.p1.2.m2.1.1" xref="S4.SS2.SSS3.Px2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.SSS3.Px2.p1.2.m2.1.1.2" xref="S4.SS2.SSS3.Px2.p1.2.m2.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS3.Px2.p1.2.m2.1.1.1" xref="S4.SS2.SSS3.Px2.p1.2.m2.1.1.1.cmml">&lt;</mo><mn id="S4.SS2.SSS3.Px2.p1.2.m2.1.1.3" xref="S4.SS2.SSS3.Px2.p1.2.m2.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.Px2.p1.2.m2.1b"><apply id="S4.SS2.SSS3.Px2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS3.Px2.p1.2.m2.1.1"><lt id="S4.SS2.SSS3.Px2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS3.Px2.p1.2.m2.1.1.1"></lt><ci id="S4.SS2.SSS3.Px2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS3.Px2.p1.2.m2.1.1.2">𝑝</ci><cn type="float" id="S4.SS2.SSS3.Px2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.SSS3.Px2.p1.2.m2.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.Px2.p1.2.m2.1c">p&lt;0.001</annotation></semantics></math> for both of them. It shows that our method can produce more natural facial expressions than the other two models.</p>
</div>
</section>
<section id="S4.SS2.SSS3.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Lip Synchronization</h5>

<div id="S4.SS2.SSS3.Px3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.Px3.p1.2" class="ltx_p">The rating for lip synchronization mainly reflects the accuracy of the lower half of the face. With pretrained wav2vec2 audio feature extractor, all three models can generate good lip synchronization with the audio. EmoFace (5.509) and EmoTalk (5.274) gain similar ratings, while FaceFormer (4.617) slightly lags behind. The main effect of the generation method is also significant, with <math id="S4.SS2.SSS3.Px3.p1.1.m1.1" class="ltx_Math" alttext="p&lt;0.001^{*}" display="inline"><semantics id="S4.SS2.SSS3.Px3.p1.1.m1.1a"><mrow id="S4.SS2.SSS3.Px3.p1.1.m1.1.1" xref="S4.SS2.SSS3.Px3.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS3.Px3.p1.1.m1.1.1.2" xref="S4.SS2.SSS3.Px3.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS3.Px3.p1.1.m1.1.1.1" xref="S4.SS2.SSS3.Px3.p1.1.m1.1.1.1.cmml">&lt;</mo><msup id="S4.SS2.SSS3.Px3.p1.1.m1.1.1.3" xref="S4.SS2.SSS3.Px3.p1.1.m1.1.1.3.cmml"><mn id="S4.SS2.SSS3.Px3.p1.1.m1.1.1.3.2" xref="S4.SS2.SSS3.Px3.p1.1.m1.1.1.3.2.cmml">0.001</mn><mo id="S4.SS2.SSS3.Px3.p1.1.m1.1.1.3.3" xref="S4.SS2.SSS3.Px3.p1.1.m1.1.1.3.3.cmml">∗</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.Px3.p1.1.m1.1b"><apply id="S4.SS2.SSS3.Px3.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS3.Px3.p1.1.m1.1.1"><lt id="S4.SS2.SSS3.Px3.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.Px3.p1.1.m1.1.1.1"></lt><ci id="S4.SS2.SSS3.Px3.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.Px3.p1.1.m1.1.1.2">𝑝</ci><apply id="S4.SS2.SSS3.Px3.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS3.Px3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS3.Px3.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.SSS3.Px3.p1.1.m1.1.1.3">superscript</csymbol><cn type="float" id="S4.SS2.SSS3.Px3.p1.1.m1.1.1.3.2.cmml" xref="S4.SS2.SSS3.Px3.p1.1.m1.1.1.3.2">0.001</cn><times id="S4.SS2.SSS3.Px3.p1.1.m1.1.1.3.3.cmml" xref="S4.SS2.SSS3.Px3.p1.1.m1.1.1.3.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.Px3.p1.1.m1.1c">p&lt;0.001^{*}</annotation></semantics></math>. The result of post-hoc shows that our model is significantly better than EmoTalk and FaceFormer with <math id="S4.SS2.SSS3.Px3.p1.2.m2.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="S4.SS2.SSS3.Px3.p1.2.m2.1a"><mrow id="S4.SS2.SSS3.Px3.p1.2.m2.1.1" xref="S4.SS2.SSS3.Px3.p1.2.m2.1.1.cmml"><mi id="S4.SS2.SSS3.Px3.p1.2.m2.1.1.2" xref="S4.SS2.SSS3.Px3.p1.2.m2.1.1.2.cmml">p</mi><mo id="S4.SS2.SSS3.Px3.p1.2.m2.1.1.1" xref="S4.SS2.SSS3.Px3.p1.2.m2.1.1.1.cmml">&lt;</mo><mn id="S4.SS2.SSS3.Px3.p1.2.m2.1.1.3" xref="S4.SS2.SSS3.Px3.p1.2.m2.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.Px3.p1.2.m2.1b"><apply id="S4.SS2.SSS3.Px3.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS3.Px3.p1.2.m2.1.1"><lt id="S4.SS2.SSS3.Px3.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS3.Px3.p1.2.m2.1.1.1"></lt><ci id="S4.SS2.SSS3.Px3.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS3.Px3.p1.2.m2.1.1.2">𝑝</ci><cn type="float" id="S4.SS2.SSS3.Px3.p1.2.m2.1.1.3.cmml" xref="S4.SS2.SSS3.Px3.p1.2.m2.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.Px3.p1.2.m2.1c">p&lt;0.001</annotation></semantics></math>. It demonstrates that our model could produce more precise lip movements.</p>
</div>
</section>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation study</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We conducted modifications to the EmoFace model to investigate the influence of different components on the prediction results. These modifications were divided into three categories: with and without weight initialization of the audio encoder, removing positional encoding, and the use of alternative structures to replace the transformer encoder in the Audio2Rig module. Each of the modified models was trained independently, and we subsequently performed quantitative analyses on the prediction results. We also apply MAE to evaluate these models. The outcomes of these analyses are presented in <a href="#S4.T4" title="Table 4 ‣ 4.3 Ablation study ‣ 4 Evaluation ‣ EmoFace: Audio-driven Emotional 3D Face Animation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 4</span></a>.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison between modified models</figcaption>
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T4.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Model</span></th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<div id="S4.T4.1.1.1.2.1" class="ltx_inline-block ltx_transformed_outer" style="width:4.9pt;height:27.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:27.8pt;transform:translate(-11.47pt,-11.47pt) rotate(-90deg) ;">
<p id="S4.T4.1.1.1.2.1.1" class="ltx_p"><span id="S4.T4.1.1.1.2.1.1.1" class="ltx_text" style="font-size:70%;">Full-Face</span></p>
</span></div>
</th>
<th id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<div id="S4.T4.1.1.1.3.1" class="ltx_inline-block ltx_transformed_outer" style="width:4.9pt;height:37.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:37.4pt;transform:translate(-16.25pt,-16.25pt) rotate(-90deg) ;">
<p id="S4.T4.1.1.1.3.1.1" class="ltx_p"><span id="S4.T4.1.1.1.3.1.1.1" class="ltx_text" style="font-size:70%;">Mouth-Area</span></p>
</span></div>
</th>
<th id="S4.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<div id="S4.T4.1.1.1.4.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.1pt;height:28.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:28.3pt;transform:translate(-11.08pt,-10.4pt) rotate(-90deg) ;">
<p id="S4.T4.1.1.1.4.1.1" class="ltx_p"><span id="S4.T4.1.1.1.4.1.1.1" class="ltx_text" style="font-size:70%;">Eye-Area</span></p>
</span></div>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.2.1" class="ltx_tr">
<th id="S4.T4.1.2.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t"><span id="S4.T4.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">EmoFace</span></th>
<td id="S4.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.2.1.2.1" class="ltx_text" style="font-size:70%;">0.04024</span></td>
<td id="S4.T4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.2.1.3.1" class="ltx_text" style="font-size:70%;">0.03697</span></td>
<td id="S4.T4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.2.1.4.1" class="ltx_text" style="font-size:70%;">0.05698</span></td>
</tr>
<tr id="S4.T4.1.3.2" class="ltx_tr">
<th id="S4.T4.1.3.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="S4.T4.1.3.2.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">wo weight initialize</span></th>
<td id="S4.T4.1.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.3.2.2.1" class="ltx_text" style="font-size:70%;">0.04706</span></td>
<td id="S4.T4.1.3.2.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.3.2.3.1" class="ltx_text" style="font-size:70%;">0.04641</span></td>
<td id="S4.T4.1.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.3.2.4.1" class="ltx_text" style="font-size:70%;">0.05826</span></td>
</tr>
<tr id="S4.T4.1.4.3" class="ltx_tr">
<th id="S4.T4.1.4.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="S4.T4.1.4.3.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">fc decoder</span></th>
<td id="S4.T4.1.4.3.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.4.3.2.1" class="ltx_text" style="font-size:70%;">0.04166</span></td>
<td id="S4.T4.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.4.3.3.1" class="ltx_text" style="font-size:70%;">0.03897</span></td>
<td id="S4.T4.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.4.3.4.1" class="ltx_text" style="font-size:70%;">0.05703</span></td>
</tr>
<tr id="S4.T4.1.5.4" class="ltx_tr">
<th id="S4.T4.1.5.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="S4.T4.1.5.4.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">lstm decoder</span></th>
<td id="S4.T4.1.5.4.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.5.4.2.1" class="ltx_text" style="font-size:70%;">0.04128</span></td>
<td id="S4.T4.1.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.5.4.3.1" class="ltx_text" style="font-size:70%;">0.03862</span></td>
<td id="S4.T4.1.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.5.4.4.1" class="ltx_text" style="font-size:70%;">0.05631</span></td>
</tr>
<tr id="S4.T4.1.6.5" class="ltx_tr">
<th id="S4.T4.1.6.5.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b"><span id="S4.T4.1.6.5.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">wo positional encoding</span></th>
<td id="S4.T4.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.1.6.5.2.1" class="ltx_text" style="font-size:70%;">0.04043</span></td>
<td id="S4.T4.1.6.5.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.1.6.5.3.1" class="ltx_text" style="font-size:70%;">0.03743</span></td>
<td id="S4.T4.1.6.5.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.1.6.5.4.1" class="ltx_text" style="font-size:70%;">0.05648</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Wav2vec2.0 initialization</h5>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p">We conducted a comparison between the predicted animations with and without weight initialization. The results shows a clear deterioration in the quality of facial motion when weight initialization is not employed. Furthermore, the MAE value of the predictions increases significantly. Relatively speaking, the decline in the model’s performance without the use of initialized weights, is mainly attributed to increased errors in mouth shape predictions. This observation highlights the importance of initializing with wav2vec2.0 pre-trained model. The prediction animations also exhibit issues such as unsynchronized audio and lip movements, as depicted in <a href="#S4.F10" title="Figure 10 ‣ Wav2vec2.0 initialization ‣ 4.3 Ablation study ‣ 4 Evaluation ‣ EmoFace: Audio-driven Emotional 3D Face Animation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 10</span></a>.</p>
</div>
<figure id="S4.F10" class="ltx_figure"><img src="/html/2407.12501/assets/figures/wo_init.png" id="S4.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Comparison to EmoFace without initialization</figcaption>
</figure>
</section>
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Decoder</h5>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p1.1" class="ltx_p">In the EmoFace model, we employed a 10-layer transformer encoder as the foundation for constructing the decoder. For comparison, we conducted additional training and testing with two distinct decoder architectures, one utilizing fully connected layers and the other implementing LSTM layers. In terms of quantitative analysis, when compared to the original model, the MAE of the fully connected decoder and the LSTM decoder exhibited relatively similar values for both the mouth shape and other parts of the face. However, the predicted animations reveal that when the audio reaches sections with lower volume, both decoders struggle to accurately represent the mouth movements, as illustrated in <a href="#S4.F11" title="Figure 11 ‣ Decoder ‣ 4.3 Ablation study ‣ 4 Evaluation ‣ EmoFace: Audio-driven Emotional 3D Face Animation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 11</span></a>. In comparison, the LSTM decoder, which can capture time-series information, yields slightly better performance, whereas the fully connected decoder tends to output mouth shapes resembling silence.</p>
</div>
<figure id="S4.F11" class="ltx_figure"><img src="/html/2407.12501/assets/figures/decoder.png" id="S4.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="196" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Comparison between different decoders</figcaption>
</figure>
</section>
<section id="S4.SS3.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Positional Encoding</h5>

<div id="S4.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px3.p1.1" class="ltx_p">The position encoding strategy we employ involves adding a temporal bias to the hidden feature and incorporating positional information into the hidden states. As a comparison, we attempted to remove the positional encoding component from the model to assess its impact on the model’s output results. Upon the removal of positional encoding, the MAE of the predictions did not show significant changes. Judging from the generated animation, it became evident that the removal of positional encoding had little overall impact. This is likely because contextual information is already embedded in the features extracted by the content encoder. However, the absence of relative position information from positional encoding can lead to some inaccuracies in predictions, especially when rapid changes occur in the mouth shape, as illustrated in <a href="#S4.F12" title="Figure 12 ‣ Positional Encoding ‣ 4.3 Ablation study ‣ 4 Evaluation ‣ EmoFace: Audio-driven Emotional 3D Face Animation" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 12</span></a>.</p>
</div>
<figure id="S4.F12" class="ltx_figure"><img src="/html/2407.12501/assets/figures/wo_pe.png" id="S4.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Comparison to EmoFace without positional encoding</figcaption>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">While our method can already generate realistic emotional facial animations, there are some limitations that need to be addressed. First, our method does not entirely resolve the challenge of multiple mappings between audio and facial expressions, potentially resulting in a lack of fine-grained detail in other facial areas. This issue is currently managed by introducing separate controllers for specific parts of the face, but it may become more obvious when working with larger datasets. Second, our model is built upon a large pre-trained language model, which leads to longer inference times and may not be suitable for real-time applications. Third, our dataset consists of data from a single actor in Chinese, which has relative small scale. Not only does it limit the diversity of generated expressions, but the subjectivity ability of the actor directly affects the performance of the model. Fourth, the generated expressions still suffer from the problem of lacking emotional intensity and lack of facial details comparing with motion captured ones, making it unable to meet the needs of cutscene-level applications. Our future work can be divided into two parts, expanding our dataset with data of different characters, emotion intensities and even languages, and exploring better face generation model using latest architectures such as diffusion models.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Our proposed method has a wide range of applications. It can be used in various fields, including game and movie production, where we can efficiently generate target animations from audio clips. In traditional production processes, creating facial expression videos for expression transfer or manually adjusting model parameters for each animation frame can be time-consuming. In comparison, audio-driven generation methods offer significant advantages in terms of speed and efficiency.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Moreover, in avatar-mediated communication, audio-driven face animation can play a pivotal role by synchronizing the avatar’s facial dynamics with the spoken words of its user. This method effectively tackles the challenge of missing facial expressions when wearing a VR headset. It capitalizes on voice recognition to detect subtle speech nuances, such as pitch, tone, and rhythm, and translates them into dynamic real-time facial animations. This innovation not only enhances the expressiveness and engagement in communication but also forges deeper emotional connections in virtual interactions, bridging the gap between the digital and physical worlds. As we continue to explore this remarkable technology, it holds the potential to elevate avatar-mediated communication into a transformative and indispensable tool within our ever-increasingly digitalized society.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work, we introduce a novel approach to generate multi-emotional 3D facial animations driven by audio input. Our model, EmoFace, employs a pre-trained audio encoder to extract essential audio features, which are then combined with emotion encoding to produce facial controller values through the Audio2Rig module. Additionally, we incorporate supplementary blink and eye gaze controllers into the system to ensure more lifelike results. To train this model, we propose an emotional audio-visual dataset and derive the controller rigs for each frame. In essence, EmoFace excels at the task of animating the MetaHuman model with emotional audio inputs, producing outcomes with superior lip synchronization and emotionally expressive facial expressions.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This work was supported by National Natural Science Foundation of China
(NSFC, NO. 62102255), the Open Project Program of the State Key Laboratory of CAD&amp;CG (Grant No. A2305), Zhejiang University, CCF-Tencent Open
Research Fund (RAGR20220128).


</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
B. Amos, B. Ludwiczuk, and M. Satyanarayanan.

</span>
<span class="ltx_bibblock">Openface: A general-purpose face recognition library with mobile
applications.

</span>
<span class="ltx_bibblock">Technical report, CMU-CS-16-118, CMU School of Computer Science,
2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Asthana, S. Zafeiriou, S. Cheng, and M. Pantic.

</span>
<span class="ltx_bibblock">Incremental face alignment in the wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pp. 1859–1866, 2014.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A. Baevski, Y. Zhou, A. Mohamed, and M. Auli.

</span>
<span class="ltx_bibblock">wav2vec 2.0: A framework for self-supervised learning of speech
representations.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>,
33:12449–12460, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. Bai, J. Z. Kolter, and V. Koltun.

</span>
<span class="ltx_bibblock">An empirical evaluation of generic convolutional and recurrent
networks for sequence modeling.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1803.01271</span>, 2018.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. R. Bentivoglio, S. B. Bressman, E. Cassetta, D. Carretta, P. Tonali, and
A. Albanese.

</span>
<span class="ltx_bibblock">Analysis of blink rate patterns in normal subjects.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Movement disorders</span>, 12(6):1028–1034, 1997.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J. Cech and T. Soukupova.

</span>
<span class="ltx_bibblock">Real-time eye blink detection using facial landmarks.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Cent. Mach. Perception, Dep. Cybern. Fac. Electr. Eng. Czech
Tech. Univ. Prague</span>, pp. 1–8, 2016.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
L. Chen, G. Cui, C. Liu, Z. Li, Z. Kou, Y. Xu, and C. Xu.

</span>
<span class="ltx_bibblock">Talking-head generation with rhythmic head motion.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, pp. 35–51.
Springer, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. S. Chung and A. Zisserman.

</span>
<span class="ltx_bibblock">Lip reading in the wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Computer Vision–ACCV 2016: 13th Asian Conference on Computer
Vision, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part
II 13</span>, pp. 87–103. Springer, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. S. Chung and A. Zisserman.

</span>
<span class="ltx_bibblock">Out of time: automated lip sync in the wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Computer Vision–ACCV 2016 Workshops: ACCV 2016 International
Workshops, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers,
Part II 13</span>, pp. 251–263. Springer, 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Conneau, A. Baevski, R. Collobert, A. Mohamed, and M. Auli.

</span>
<span class="ltx_bibblock">Unsupervised cross-lingual representation learning for speech
recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2006.13979</span>, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
D. Cudeiro, T. Bolkart, C. Laidlaw, A. Ranjan, and M. J. Black.

</span>
<span class="ltx_bibblock">Capture, learning, and synthesis of 3d speaking styles.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pp. 10101–10111, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
R. Daněček, K. Chhatre, S. Tripathi, Y. Wen, M. Black, and
T. Bolkart.

</span>
<span class="ltx_bibblock">Emotional speech-driven animation with content-emotion
disentanglement.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">SIGGRAPH Asia 2023 Conference Papers</span>, pp. 1–13, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
P. Edwards, C. Landreth, E. Fiume, and K. Singh.

</span>
<span class="ltx_bibblock">Jali: an animator-centric viseme model for expressive lip
synchronization.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">ACM Transactions on graphics (TOG)</span>, 35(4):1–11, 2016.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Y. Fan, Z. Lin, J. Saito, W. Wang, and T. Komura.

</span>
<span class="ltx_bibblock">Faceformer: Speech-driven 3d facial animation with transformers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pp. 18770–18780, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
G. Fanelli, J. Gall, H. Romsdorfer, T. Weise, and L. Van Gool.

</span>
<span class="ltx_bibblock">A 3-d audio-visual corpus of affective communication.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Multimedia</span>, 12(6):591–598, 2010.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, and Y. Bengio.

</span>
<span class="ltx_bibblock">Generative adversarial nets.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 27, 2014.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger,
S. Satheesh, S. Sengupta, A. Coates, et al.

</span>
<span class="ltx_bibblock">Deep speech: Scaling up end-to-end speech recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1412.5567</span>, 2014.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
X. Ji, H. Zhou, K. Wang, Q. Wu, W. Wu, F. Xu, and X. Cao.

</span>
<span class="ltx_bibblock">Eamm: One-shot emotional talking face via audio-based emotion-aware
motion model.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">ACM SIGGRAPH 2022 Conference Proceedings</span>, pp. 1–10, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
X. Ji, H. Zhou, K. Wang, W. Wu, C. C. Loy, X. Cao, and F. Xu.

</span>
<span class="ltx_bibblock">Audio-driven emotional video portraits.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span>, pp. 14080–14089, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
B. H. Le, X. Ma, and Z. Deng.

</span>
<span class="ltx_bibblock">Live speech driven head-and-eye motion generators.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">IEEE transactions on visualization and computer graphics</span>,
18(11):1902–1914, 2012.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
S. R. Livingstone and F. A. Russo.

</span>
<span class="ltx_bibblock">The ryerson audio-visual database of emotional speech and song
(ravdess): A dynamic, multimodal set of facial and vocal expressions in north
american english.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">PloS one</span>, 13(5):e0196391, 2018.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Y. Lu, J. Chai, and X. Cao.

</span>
<span class="ltx_bibblock">Live speech portraits: real-time photorealistic talking-head
animation.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Graphics (TOG)</span>, 40(6):1–17, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
V. Panayotov, G. Chen, D. Povey, and S. Khudanpur.

</span>
<span class="ltx_bibblock">Librispeech: an asr corpus based on public domain audio books.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">2015 IEEE international conference on acoustics, speech and
signal processing (ICASSP)</span>, pp. 5206–5210. IEEE, 2015.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Z. Peng, H. Wu, Z. Song, H. Xu, X. Zhu, J. He, H. Liu, and Z. Fan.

</span>
<span class="ltx_bibblock">Emotalk: Speech-driven emotional disentanglement for 3d face
animation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pp. 20687–20697, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
H. X. Pham, S. Cheung, and V. Pavlovic.

</span>
<span class="ltx_bibblock">Speech-driven 3d facial animation with implicit emotional awareness:
A deep learning approach.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition workshops</span>, pp. 80–88, 2017.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
H. X. Pham, Y. Wang, and V. Pavlovic.

</span>
<span class="ltx_bibblock">End-to-end learning for 3d facial animation from speech.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proceedings of the 20th ACM International Conference on
Multimodal Interaction</span>, pp. 361–365, 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
H. X. Pham, Y. Wang, and V. Pavlovic.

</span>
<span class="ltx_bibblock">Learning continuous facial actions from speech for real-time
animation.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Affective Computing</span>, 13(3):1567–1580,
2020.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
K. Prajwal, R. Mukhopadhyay, V. P. Namboodiri, and C. Jawahar.

</span>
<span class="ltx_bibblock">A lip sync expert is all you need for speech to lip generation in the
wild.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proceedings of the 28th ACM international conference on
multimedia</span>, pp. 484–492, 2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A. Richard, C. Lea, S. Ma, J. Gall, F. De la Torre, and Y. Sheikh.

</span>
<span class="ltx_bibblock">Audio-and gaze-driven facial animation of codec avatars.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF winter conference on applications
of computer vision</span>, pp. 41–50, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
A. Richard, M. Zollhöfer, Y. Wen, F. De la Torre, and Y. Sheikh.

</span>
<span class="ltx_bibblock">Meshtalk: 3d face animation from speech using cross-modality
disentanglement.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pp. 1173–1182, 2021.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
J. Saunders and V. Namboodiri.

</span>
<span class="ltx_bibblock">Read avatars: Realistic emotion-controllable audio driven avatars.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2303.00744</span>, 2023.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Y. Song, J. Zhu, D. Li, X. Wang, and H. Qi.

</span>
<span class="ltx_bibblock">Talking face generation by conditional recurrent adversarial network.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1804.04786</span>, 2018.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
S. Taylor, T. Kim, Y. Yue, M. Mahler, J. Krahe, A. G. Rodriguez, J. Hodgins,
and I. Matthews.

</span>
<span class="ltx_bibblock">A deep learning approach for generalized speech animation.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">ACM Transactions On Graphics (TOG)</span>, 36(4):1–11, 2017.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
M. Villanueva Aylagas, H. Anadon Leon, M. Teye, and K. Tollmar.

</span>
<span class="ltx_bibblock">Voice2face: Audio-driven facial and tongue rig animations with cvaes.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Computer Graphics Forum</span>, vol. 41, pp. 255–265. Wiley Online
Library, 2022.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
K. Vougioukas, S. Petridis, and M. Pantic.

</span>
<span class="ltx_bibblock">End-to-end speech-driven realistic facial animation with temporal
gans.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">CVPR Workshops</span>, pp. 37–40, 2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
K. Vougioukas, S. Petridis, and M. Pantic.

</span>
<span class="ltx_bibblock">Realistic speech-driven facial animation with gans.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, 128:1398–1413, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
K. Wang, Q. Wu, L. Song, Z. Yang, W. Wu, C. Qian, R. He, Y. Qiao, and C. C.
Loy.

</span>
<span class="ltx_bibblock">Mead: A large-scale audio-visual dataset for emotional talking-face
generation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, pp. 700–717.
Springer, 2020.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
S. Wang, L. Li, Y. Ding, C. Fan, and X. Yu.

</span>
<span class="ltx_bibblock">Audio2head: Audio-driven one-shot talking-head generation with
natural head motion.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2107.09293</span>, 2021.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
S. Wang, L. Li, Y. Ding, and X. Yu.

</span>
<span class="ltx_bibblock">One-shot talking face generation from single-speaker audio-visual
correlation learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</span>, vol. 36, pp. 2531–2539, 2022.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
X. Xiong and F. De la Torre.

</span>
<span class="ltx_bibblock">Supervised descent method and its applications to face alignment.

</span>
<span class="ltx_bibblock">In <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pp. 532–539, 2013.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Z. Zhang, L. Li, Y. Ding, and C. Fan.

</span>
<span class="ltx_bibblock">Flow-guided one-shot talking face generation with a high-resolution
audio-visual dataset.

</span>
<span class="ltx_bibblock">In <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pp. 3661–3670, 2021.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
H. Zhou, Y. Liu, Z. Liu, P. Luo, and X. Wang.

</span>
<span class="ltx_bibblock">Talking face generation by adversarially disentangled audio-visual
representation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI conference on artificial
intelligence</span>, vol. 33, pp. 9299–9306, 2019.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
H. Zhou, Y. Sun, W. Wu, C. C. Loy, X. Wang, and Z. Liu.

</span>
<span class="ltx_bibblock">Pose-controllable talking face generation by implicitly modularized
audio-visual representation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span>, pp. 4176–4186, 2021.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Y. Zhou, Z. Xu, C. Landreth, E. Kalogerakis, S. Maji, and K. Singh.

</span>
<span class="ltx_bibblock">Visemenet: Audio-driven animator-centric speech animation.

</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Graphics (TOG)</span>, 37(4):1–10, 2018.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.12500" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.12501" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.12501">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.12501" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.12502" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 17:45:43 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
