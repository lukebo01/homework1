<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.03495] Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations</title><meta property="og:description" content="Discrete speech representations have garnered recent attention for their efficacy in training transformer-based models for various speech-related tasks such as automatic speech recognition (ASR), translation, speaker vâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.03495">

<!--Generated on Mon Aug  5 17:58:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">KunalDhawan
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>Nithin RaoKoluguri
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>AnteJukiÄ‡
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>RyanLangman
<span id="p1.3.4" class="ltx_ERROR undefined">\name</span>JagadeeshBalam
<span id="p1.3.5" class="ltx_ERROR undefined">\name</span>BorisGinsburg





</p>
</div>
<h1 class="ltx_title ltx_title_document">Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Discrete speech representations have garnered recent attention for their efficacy in training transformer-based models for various speech-related tasks such as automatic speech recognition (ASR), translation, speaker verification, and joint speech-text foundational models. In this work, we present a comprehensive analysis on building ASR systems with discrete codes. We investigate different methods for codec training such as quantization schemes and time-domain vs spectral feature encodings. We further explore ASR training techniques aimed at enhancing performance, training efficiency, and noise robustness. Drawing upon our findings, we introduce a codec ASR pipeline that outperforms Encodec at similar bit-rate. Remarkably, it also surpasses the state-of-the-art results achieved by strong self-supervised models on the 143 languages ML-SUPERB benchmark despite being smaller in size and pretrained on significantly less data.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>discrete speech representation, automatic speech recognition, audio codecs, noise robustness 
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">A tremendous amount of progress has been achieved in the area of speech and audio technologies in recent years, in large part due to advances in deep learning and the availability of large-scale datasetsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
In particular, transformer-based models led to significant improvements in speech-related tasks such as automatic speech recognition (ASR)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and joint speech-text modelingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Typically, the input speech signal of an ASR model is represented using a <span id="S1.p2.1.1" class="ltx_text">mel-spectrogram</span>, resulting in a continuous representation of the speech signal.
Learnable alternatives have been explored for different applicationsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
However, using <span id="S1.p2.1.2" class="ltx_text">mel-spectrograms</span> is still a prevalent choice for ASR systems due to their effectivenessÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
Recently, the use of discrete speech representations has garnered attention for their efficacy in training transformer-based models for various speech-related tasksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and compatibility with language-modeling architecturesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Discrete speech representations are typically categorized as either acoustic or semantic.
The former capture the acoustic properties of the speech signal, such as pitch, tone, and rhythm. On the other hand, the latter capture the semantic properties of the speech signal, like the meaning and context conveyed by the speech, including words, phrases, and their associations.
Semantic codes are typically obtained by clustering the speech representation at the output of a pre-trained encoderÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, or using a codec modelÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.
Acoustic codes are typically obtained by compressing and quantizing the speech signal, e.g., using an audio codec, and aim to reconstruct the original signal from a compressed representation.
Several neural audio codecs (NACs) have been proposed recentlyÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
Typically, such codecs have an encoder-quantizer-decoder architecture, where the encoder compresses the input speech signal into a latent representation, quantizer approximates it using a discrete representation, and the decoder reconstructs the original signal from the discrete representation.
Acoustic codes are particularly relevant for multi-task foundational models, which aim to simultaneously understand the content in the input signal and generate high-quality output signals.
While acoustic tokens have been explored in the context of speech and audio synthesisÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and processingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, their use in ASR systems has been relatively underexploredÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To address the above gap, we perform a comprehensive analysis on building ASR systems with discrete codes.
Firstly, we train and evaluate codecs operating in either time or spectral domain with different quantizers.
Secondly, we explore different approaches to improve the ASR system performance, training efficiency and also evaluate approaches for improving their noise robustness.
Based on our findings, we present a pipeline for noise-robust ASR training with discrete representations generated using a neural audio codec.
Thirdly, to prove the generalizabilty of the proposed NAC+ASR pipeline, we further experiment with the ML-SUPERB datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> consisting of 143 languages.
The presented results give us a better understanding of the various components of the NAC+ASR pipeline.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We demonstrate that the proposed pipeline based on above learnings is very competitive, outperforming the prevalent Encodec<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>-based systems in comparable settings. Our system also achieves a CER of 21% on the hard ML-SUPERB 1h test set, beating previous state-of-the-art (SOTA) results. The trained NAC<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/audio_codec_16khz_small" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/audio_codec_16khz_small</a></span></span></span> and ASR models along with accompanying code will be released in the open-source NVIDIA NeMo<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/NVIDIA/NeMo" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/NVIDIA/NeMo</a></span></span></span> toolkit.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Speech recognition with audio codecs</h2>

<figure id="S2.F1" class="ltx_figure">
<div id="S2.F1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:325.2pt;height:248.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(51.2pt,-39.1pt) scale(1.46020944022527,1.46020944022527) ;"><svg id="S2.F1.1.pic1" class="ltx_picture" height="235.35" overflow="visible" version="1.1" width="298.49"><g transform="translate(0,235.35) matrix(1 0 0 -1 0 0) translate(151.32,0) translate(0,208.14)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" fill="#639C00" fill-opacity="0.1"><path d="M -49.21 -7.87 h 98.43 v 15.75 h -98.43 Z"></path></g><g stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" fill="#000000" fill-opacity="0.1" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -22.14 -4.05)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 4.745)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.11)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Filterbank</text></g></g></g></g><g fill="#FFFFFF"><path d="M 52.87 -7.53 h 43.59 v 15.07 h -43.59 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 56.1 -2.42)" fill="#000000" stroke="#000000"><foreignObject width="37.13" height="8.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F1.1.pic1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">Optional</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -148.1 17.26)" fill="#000000" stroke="#000000"><foreignObject width="79.1" height="8.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F1.1.pic1.2.2.2.1.1" class="ltx_text" style="font-size:70%;">time-domain input</span></foreignObject></g><path d="M -65.49 19.69 L 0 19.69 L 0 13.13" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 0 13.13)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><g fill="#639C00" fill-opacity="0.2"><path d="M -33.13 -47.81 L -49.2 -19.96 L 49.2 -19.96 L 33.13 -47.81 Z"></path></g><g fill-opacity="0.2" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -15.75 -37.27)"><foreignObject width="31.5" height="6.78" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S2.F1.1.pic1.3.3.3.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:22.8pt;">
<span id="S2.F1.1.pic1.3.3.3.1.1.1" class="ltx_p"></span>
<span id="S2.F1.1.pic1.3.3.3.1.1.2" class="ltx_p"><span id="S2.F1.1.pic1.3.3.3.1.1.2.1" class="ltx_text" style="font-size:70%;">Encoder</span></span>
</span></foreignObject></g><path d="M 0 -8.15 L 0 -14.98" style="fill:none"></path><g transform="matrix(-0.00002 -1.0 1.0 -0.00002 0 -14.98)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path d="M 0 -47.81 L 0 -55.25" style="fill:none"></path><g fill="#639C00" fill-opacity="0.2"><path d="M -32.48 -79.16 h 64.96 v 16.19 h -64.96 Z"></path></g><g fill-opacity="0.2" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -17.76 -75.93)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 5.565)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 7.85)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">quantize</text></g></g></g></g><path d="M 0 -55.25 L 0 -57.71" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 0 -57.71)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><g stroke-dasharray="0.4pt,2.0pt" stroke-dashoffset="0.0pt" fill="#FFFFFF"><path d="M -74.8 -119.08 h 149.61 v 23.62 h -149.61 Z"></path></g><path d="M 0 -79.44 L 0 -90.2" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 0 -90.2)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><g fill="#FFFFFF"><path d="M -108.67 -113.86 h 29.38 v 13.18 h -29.38 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -105.45 -110.63)" fill="#000000" stroke="#000000"><foreignObject width="22.92" height="6.73" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F1.1.pic1.4.4.4.1.1" class="ltx_text" style="font-size:70%;">codes</span></foreignObject></g><g fill="#666666"><path d="M 13.61 -107.27 C 13.61 -99.75 7.52 -93.66 0 -93.66 C -7.52 -93.66 -13.61 -99.75 -13.61 -107.27 C -13.61 -114.79 -7.52 -120.89 0 -120.89 C 7.52 -120.89 13.61 -114.79 13.61 -107.27 Z M 0 -107.27"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -8.21 -111.52)" fill="#000000" stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 4.25)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 6.62)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignObject width="16.41" height="8.5" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F1.1.pic1.5.5.5.1.1.1.1.1" class="ltx_ERROR undefined">\pgfmathresult</span><span id="S2.F1.1.pic1.6.6.6.2.2.2.2.2" class="ltx_text" style="font-size:70%;">pt</span></foreignObject></g></g></g></g><g fill="#CCCCCC"><path d="M -22.04 -107.27 C -22.04 -102.94 -25.55 -99.43 -29.88 -99.43 C -34.21 -99.43 -37.72 -102.94 -37.72 -107.27 C -37.72 -111.6 -34.21 -115.11 -29.88 -115.11 C -25.55 -115.11 -22.04 -111.6 -22.04 -107.27 Z M -29.88 -107.27"></path></g><g fill="#F2F2F2"><path d="M -46.15 -107.27 C -46.15 -102.94 -49.66 -99.43 -53.99 -99.43 C -58.32 -99.43 -61.82 -102.94 -61.82 -107.27 C -61.82 -111.6 -58.32 -115.11 -53.99 -115.11 C -49.66 -115.11 -46.15 -111.6 -46.15 -107.27 Z M -53.99 -107.27"></path></g><g fill="#050505"><path d="M -70.25 -107.27 C -70.25 -102.94 -73.76 -99.43 -78.09 -99.43 C -82.42 -99.43 -85.93 -102.94 -85.93 -107.27 C -85.93 -111.6 -82.42 -115.11 -78.09 -115.11 C -73.76 -115.11 -70.25 -111.6 -70.25 -107.27 Z M -78.09 -107.27"></path></g><g fill="#808080"><path d="M 37.72 -107.27 C 37.72 -102.94 34.21 -99.43 29.88 -99.43 C 25.55 -99.43 22.04 -102.94 22.04 -107.27 C 22.04 -111.6 25.55 -115.11 29.88 -115.11 C 34.21 -115.11 37.72 -111.6 37.72 -107.27 Z M 29.88 -107.27"></path></g><g fill="#1A1A1A"><path d="M 61.82 -107.27 C 61.82 -102.94 58.32 -99.43 53.99 -99.43 C 49.66 -99.43 46.15 -102.94 46.15 -107.27 C 46.15 -111.6 49.66 -115.11 53.99 -115.11 C 58.32 -115.11 61.82 -111.6 61.82 -107.27 Z M 53.99 -107.27"></path></g><g fill="#E6E6E6"><path d="M 85.93 -107.27 C 85.93 -102.94 82.42 -99.43 78.09 -99.43 C 73.76 -99.43 70.25 -102.94 70.25 -107.27 C 70.25 -111.6 73.76 -115.11 78.09 -115.11 C 82.42 -115.11 85.93 -111.6 85.93 -107.27 Z M 78.09 -107.27"></path></g><g fill="#639C00" fill-opacity="0.2"><path d="M -32.48 -151.83 h 64.96 v 16.45 h -64.96 Z"></path></g><g fill-opacity="0.2" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -22.6 -148.61)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 5.685)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.11)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">dequantize</text></g></g></g></g><path d="M 0 -119.36 L 0 -130.13" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 0 -130.13)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><path d="M 0 -152.11 L 0 -159.55" style="fill:none"></path><g fill="#639C00" fill-opacity="0.2"><path d="M -49.2 -194.84 L -33.13 -166.99 L 33.13 -166.99 L 49.2 -194.84 Z"></path></g><g fill-opacity="0.2" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -15.75 -184.31)"><foreignObject width="31.5" height="6.78" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S2.F1.1.pic1.7.7.7.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:22.8pt;">
<span id="S2.F1.1.pic1.7.7.7.1.1.1" class="ltx_p"></span>
<span id="S2.F1.1.pic1.7.7.7.1.1.2" class="ltx_p"><span id="S2.F1.1.pic1.7.7.7.1.1.2.1" class="ltx_text" style="font-size:70%;">Decoder</span></span>
</span></foreignObject></g><path d="M 0 -159.55 L 0 -162.01" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 0 -162.01)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 57.71 -203.02)" fill="#000000" stroke="#000000"><foreignObject width="85.02" height="8.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S2.F1.1.pic1.8.8.8.1.1" class="ltx_text" style="font-size:70%;">time-domain output</span></foreignObject></g><path d="M 0 -194.84 L 0 -200.6 L 49.22 -200.6" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 49.22 -200.6)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><g stroke="#808080" fill="#639C00" fill-opacity="0.02" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt"><path d="M -118.11 -158.45 h 236.22 v 102.36 h -236.22 Z"></path></g><g fill="#FFFFFF" fill-opacity="0.75"><path d="M 98.98 -115.44 h 48.19 v 16.34 h -48.19 Z" style="stroke:none"></path></g><g fill-opacity="0.75" fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 102.21 -112.21)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 5.64)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><text transform="matrix(1 0 0 -1 0 0)">Quantizer</text></g></g></g></g></g></svg>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Architecture of the considered neural audio codecs.</figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we discuss the various components of the proposed ASR pipeline that operates on discrete speech representations.
The block scheme of the complete pipeline is depicted in FigureÂ <a href="#S2.F2" title="Figure 2 â€£ 2.1.3 Spectral NAC â€£ 2.1 Audio codecs â€£ 2 Speech recognition with audio codecs â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Audio codecs</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Audio codecs capture details of the audio signal using discrete codes at a low bitrate, and are used for speech representation in various tasks, efficient data transmission, and general data compression.
Here we consider two types of NACs, operating either on the time-domain signal or on a spectral domain.
FigureÂ <a href="#S2.F1" title="Figure 1 â€£ 2 Speech recognition with audio codecs â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> depicts the general architecture of the considered codecs.</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Quantization schemes</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.5" class="ltx_p">Residual vector quantization (RVQ) is the common approach used for NAC, e.g., in SoundStreamÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, EncodecÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, and DACÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
The RVQ uses a series of codebooks with size <math id="S2.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="D_{\text{cb}}" display="inline"><semantics id="S2.SS1.SSS1.p1.1.m1.1a"><msub id="S2.SS1.SSS1.p1.1.m1.1.1" xref="S2.SS1.SSS1.p1.1.m1.1.1.cmml"><mi id="S2.SS1.SSS1.p1.1.m1.1.1.2" xref="S2.SS1.SSS1.p1.1.m1.1.1.2.cmml">D</mi><mtext id="S2.SS1.SSS1.p1.1.m1.1.1.3" xref="S2.SS1.SSS1.p1.1.m1.1.1.3a.cmml">cb</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.1.m1.1b"><apply id="S2.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.SSS1.p1.1.m1.1.1.2">ğ·</ci><ci id="S2.SS1.SSS1.p1.1.m1.1.1.3a.cmml" xref="S2.SS1.SSS1.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S2.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.SSS1.p1.1.m1.1.1.3">cb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.1.m1.1c">D_{\text{cb}}</annotation></semantics></math>, with the current codebook quantizing the residual from the previous quantization stepÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.
For each time step, RVQ produces <math id="S2.SS1.SSS1.p1.2.m2.1" class="ltx_Math" alttext="N_{\text{cb}}" display="inline"><semantics id="S2.SS1.SSS1.p1.2.m2.1a"><msub id="S2.SS1.SSS1.p1.2.m2.1.1" xref="S2.SS1.SSS1.p1.2.m2.1.1.cmml"><mi id="S2.SS1.SSS1.p1.2.m2.1.1.2" xref="S2.SS1.SSS1.p1.2.m2.1.1.2.cmml">N</mi><mtext id="S2.SS1.SSS1.p1.2.m2.1.1.3" xref="S2.SS1.SSS1.p1.2.m2.1.1.3a.cmml">cb</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.2.m2.1b"><apply id="S2.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.SSS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.SSS1.p1.2.m2.1.1.2">ğ‘</ci><ci id="S2.SS1.SSS1.p1.2.m2.1.1.3a.cmml" xref="S2.SS1.SSS1.p1.2.m2.1.1.3"><mtext mathsize="70%" id="S2.SS1.SSS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.SSS1.p1.2.m2.1.1.3">cb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.2.m2.1c">N_{\text{cb}}</annotation></semantics></math> codes, corresponding to the number of codebooks.
In this paper, RVQ is configured using <math id="S2.SS1.SSS1.p1.3.m3.1" class="ltx_Math" alttext="D_{\text{enc}}=128" display="inline"><semantics id="S2.SS1.SSS1.p1.3.m3.1a"><mrow id="S2.SS1.SSS1.p1.3.m3.1.1" xref="S2.SS1.SSS1.p1.3.m3.1.1.cmml"><msub id="S2.SS1.SSS1.p1.3.m3.1.1.2" xref="S2.SS1.SSS1.p1.3.m3.1.1.2.cmml"><mi id="S2.SS1.SSS1.p1.3.m3.1.1.2.2" xref="S2.SS1.SSS1.p1.3.m3.1.1.2.2.cmml">D</mi><mtext id="S2.SS1.SSS1.p1.3.m3.1.1.2.3" xref="S2.SS1.SSS1.p1.3.m3.1.1.2.3a.cmml">enc</mtext></msub><mo id="S2.SS1.SSS1.p1.3.m3.1.1.1" xref="S2.SS1.SSS1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S2.SS1.SSS1.p1.3.m3.1.1.3" xref="S2.SS1.SSS1.p1.3.m3.1.1.3.cmml">128</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.3.m3.1b"><apply id="S2.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS1.p1.3.m3.1.1"><eq id="S2.SS1.SSS1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.SSS1.p1.3.m3.1.1.1"></eq><apply id="S2.SS1.SSS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.SSS1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p1.3.m3.1.1.2.1.cmml" xref="S2.SS1.SSS1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS1.p1.3.m3.1.1.2.2.cmml" xref="S2.SS1.SSS1.p1.3.m3.1.1.2.2">ğ·</ci><ci id="S2.SS1.SSS1.p1.3.m3.1.1.2.3a.cmml" xref="S2.SS1.SSS1.p1.3.m3.1.1.2.3"><mtext mathsize="70%" id="S2.SS1.SSS1.p1.3.m3.1.1.2.3.cmml" xref="S2.SS1.SSS1.p1.3.m3.1.1.2.3">enc</mtext></ci></apply><cn type="integer" id="S2.SS1.SSS1.p1.3.m3.1.1.3.cmml" xref="S2.SS1.SSS1.p1.3.m3.1.1.3">128</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.3.m3.1c">D_{\text{enc}}=128</annotation></semantics></math>, <math id="S2.SS1.SSS1.p1.4.m4.1" class="ltx_Math" alttext="D_{\text{cb}}=1024" display="inline"><semantics id="S2.SS1.SSS1.p1.4.m4.1a"><mrow id="S2.SS1.SSS1.p1.4.m4.1.1" xref="S2.SS1.SSS1.p1.4.m4.1.1.cmml"><msub id="S2.SS1.SSS1.p1.4.m4.1.1.2" xref="S2.SS1.SSS1.p1.4.m4.1.1.2.cmml"><mi id="S2.SS1.SSS1.p1.4.m4.1.1.2.2" xref="S2.SS1.SSS1.p1.4.m4.1.1.2.2.cmml">D</mi><mtext id="S2.SS1.SSS1.p1.4.m4.1.1.2.3" xref="S2.SS1.SSS1.p1.4.m4.1.1.2.3a.cmml">cb</mtext></msub><mo id="S2.SS1.SSS1.p1.4.m4.1.1.1" xref="S2.SS1.SSS1.p1.4.m4.1.1.1.cmml">=</mo><mn id="S2.SS1.SSS1.p1.4.m4.1.1.3" xref="S2.SS1.SSS1.p1.4.m4.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.4.m4.1b"><apply id="S2.SS1.SSS1.p1.4.m4.1.1.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1"><eq id="S2.SS1.SSS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1.1"></eq><apply id="S2.SS1.SSS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p1.4.m4.1.1.2.1.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS1.p1.4.m4.1.1.2.2.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1.2.2">ğ·</ci><ci id="S2.SS1.SSS1.p1.4.m4.1.1.2.3a.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1.2.3"><mtext mathsize="70%" id="S2.SS1.SSS1.p1.4.m4.1.1.2.3.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1.2.3">cb</mtext></ci></apply><cn type="integer" id="S2.SS1.SSS1.p1.4.m4.1.1.3.cmml" xref="S2.SS1.SSS1.p1.4.m4.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.4.m4.1c">D_{\text{cb}}=1024</annotation></semantics></math> and <math id="S2.SS1.SSS1.p1.5.m5.1" class="ltx_Math" alttext="N_{\text{cb}}=8" display="inline"><semantics id="S2.SS1.SSS1.p1.5.m5.1a"><mrow id="S2.SS1.SSS1.p1.5.m5.1.1" xref="S2.SS1.SSS1.p1.5.m5.1.1.cmml"><msub id="S2.SS1.SSS1.p1.5.m5.1.1.2" xref="S2.SS1.SSS1.p1.5.m5.1.1.2.cmml"><mi id="S2.SS1.SSS1.p1.5.m5.1.1.2.2" xref="S2.SS1.SSS1.p1.5.m5.1.1.2.2.cmml">N</mi><mtext id="S2.SS1.SSS1.p1.5.m5.1.1.2.3" xref="S2.SS1.SSS1.p1.5.m5.1.1.2.3a.cmml">cb</mtext></msub><mo id="S2.SS1.SSS1.p1.5.m5.1.1.1" xref="S2.SS1.SSS1.p1.5.m5.1.1.1.cmml">=</mo><mn id="S2.SS1.SSS1.p1.5.m5.1.1.3" xref="S2.SS1.SSS1.p1.5.m5.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.5.m5.1b"><apply id="S2.SS1.SSS1.p1.5.m5.1.1.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1"><eq id="S2.SS1.SSS1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1.1"></eq><apply id="S2.SS1.SSS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p1.5.m5.1.1.2.1.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS1.p1.5.m5.1.1.2.2.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1.2.2">ğ‘</ci><ci id="S2.SS1.SSS1.p1.5.m5.1.1.2.3a.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1.2.3"><mtext mathsize="70%" id="S2.SS1.SSS1.p1.5.m5.1.1.2.3.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1.2.3">cb</mtext></ci></apply><cn type="integer" id="S2.SS1.SSS1.p1.5.m5.1.1.3.cmml" xref="S2.SS1.SSS1.p1.5.m5.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.5.m5.1c">N_{\text{cb}}=8</annotation></semantics></math>.</p>
</div>
<div id="S2.SS1.SSS1.p2" class="ltx_para">
<p id="S2.SS1.SSS1.p2.7" class="ltx_p">Finite scalar quantization (FSQ)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> typically uses a smaller latent dimension <math id="S2.SS1.SSS1.p2.1.m1.1" class="ltx_Math" alttext="D_{\text{enc}}" display="inline"><semantics id="S2.SS1.SSS1.p2.1.m1.1a"><msub id="S2.SS1.SSS1.p2.1.m1.1.1" xref="S2.SS1.SSS1.p2.1.m1.1.1.cmml"><mi id="S2.SS1.SSS1.p2.1.m1.1.1.2" xref="S2.SS1.SSS1.p2.1.m1.1.1.2.cmml">D</mi><mtext id="S2.SS1.SSS1.p2.1.m1.1.1.3" xref="S2.SS1.SSS1.p2.1.m1.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.1.m1.1b"><apply id="S2.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S2.SS1.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.SSS1.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.SSS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.SSS1.p2.1.m1.1.1.2">ğ·</ci><ci id="S2.SS1.SSS1.p2.1.m1.1.1.3a.cmml" xref="S2.SS1.SSS1.p2.1.m1.1.1.3"><mtext mathsize="70%" id="S2.SS1.SSS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.SSS1.p2.1.m1.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.1.m1.1c">D_{\text{enc}}</annotation></semantics></math> as compared to RVQ.
Each element of the latent vector is quantized independently into a number level, e.g., to <math id="S2.SS1.SSS1.p2.2.m2.3" class="ltx_Math" alttext="\{-1,0,1\}" display="inline"><semantics id="S2.SS1.SSS1.p2.2.m2.3a"><mrow id="S2.SS1.SSS1.p2.2.m2.3.3.1" xref="S2.SS1.SSS1.p2.2.m2.3.3.2.cmml"><mo stretchy="false" id="S2.SS1.SSS1.p2.2.m2.3.3.1.2" xref="S2.SS1.SSS1.p2.2.m2.3.3.2.cmml">{</mo><mrow id="S2.SS1.SSS1.p2.2.m2.3.3.1.1" xref="S2.SS1.SSS1.p2.2.m2.3.3.1.1.cmml"><mo id="S2.SS1.SSS1.p2.2.m2.3.3.1.1a" xref="S2.SS1.SSS1.p2.2.m2.3.3.1.1.cmml">âˆ’</mo><mn id="S2.SS1.SSS1.p2.2.m2.3.3.1.1.2" xref="S2.SS1.SSS1.p2.2.m2.3.3.1.1.2.cmml">1</mn></mrow><mo id="S2.SS1.SSS1.p2.2.m2.3.3.1.3" xref="S2.SS1.SSS1.p2.2.m2.3.3.2.cmml">,</mo><mn id="S2.SS1.SSS1.p2.2.m2.1.1" xref="S2.SS1.SSS1.p2.2.m2.1.1.cmml">0</mn><mo id="S2.SS1.SSS1.p2.2.m2.3.3.1.4" xref="S2.SS1.SSS1.p2.2.m2.3.3.2.cmml">,</mo><mn id="S2.SS1.SSS1.p2.2.m2.2.2" xref="S2.SS1.SSS1.p2.2.m2.2.2.cmml">1</mn><mo stretchy="false" id="S2.SS1.SSS1.p2.2.m2.3.3.1.5" xref="S2.SS1.SSS1.p2.2.m2.3.3.2.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.2.m2.3b"><set id="S2.SS1.SSS1.p2.2.m2.3.3.2.cmml" xref="S2.SS1.SSS1.p2.2.m2.3.3.1"><apply id="S2.SS1.SSS1.p2.2.m2.3.3.1.1.cmml" xref="S2.SS1.SSS1.p2.2.m2.3.3.1.1"><minus id="S2.SS1.SSS1.p2.2.m2.3.3.1.1.1.cmml" xref="S2.SS1.SSS1.p2.2.m2.3.3.1.1"></minus><cn type="integer" id="S2.SS1.SSS1.p2.2.m2.3.3.1.1.2.cmml" xref="S2.SS1.SSS1.p2.2.m2.3.3.1.1.2">1</cn></apply><cn type="integer" id="S2.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S2.SS1.SSS1.p2.2.m2.1.1">0</cn><cn type="integer" id="S2.SS1.SSS1.p2.2.m2.2.2.cmml" xref="S2.SS1.SSS1.p2.2.m2.2.2">1</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.2.m2.3c">\{-1,0,1\}</annotation></semantics></math> when using three levels.
As opposed to RVQ, FSQ results in a flat codebook, without a recursive relationship between individual codes.
In this paper, FSQ is configured using <math id="S2.SS1.SSS1.p2.3.m3.1" class="ltx_Math" alttext="D_{\text{enc}}=32" display="inline"><semantics id="S2.SS1.SSS1.p2.3.m3.1a"><mrow id="S2.SS1.SSS1.p2.3.m3.1.1" xref="S2.SS1.SSS1.p2.3.m3.1.1.cmml"><msub id="S2.SS1.SSS1.p2.3.m3.1.1.2" xref="S2.SS1.SSS1.p2.3.m3.1.1.2.cmml"><mi id="S2.SS1.SSS1.p2.3.m3.1.1.2.2" xref="S2.SS1.SSS1.p2.3.m3.1.1.2.2.cmml">D</mi><mtext id="S2.SS1.SSS1.p2.3.m3.1.1.2.3" xref="S2.SS1.SSS1.p2.3.m3.1.1.2.3a.cmml">enc</mtext></msub><mo id="S2.SS1.SSS1.p2.3.m3.1.1.1" xref="S2.SS1.SSS1.p2.3.m3.1.1.1.cmml">=</mo><mn id="S2.SS1.SSS1.p2.3.m3.1.1.3" xref="S2.SS1.SSS1.p2.3.m3.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.3.m3.1b"><apply id="S2.SS1.SSS1.p2.3.m3.1.1.cmml" xref="S2.SS1.SSS1.p2.3.m3.1.1"><eq id="S2.SS1.SSS1.p2.3.m3.1.1.1.cmml" xref="S2.SS1.SSS1.p2.3.m3.1.1.1"></eq><apply id="S2.SS1.SSS1.p2.3.m3.1.1.2.cmml" xref="S2.SS1.SSS1.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p2.3.m3.1.1.2.1.cmml" xref="S2.SS1.SSS1.p2.3.m3.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS1.p2.3.m3.1.1.2.2.cmml" xref="S2.SS1.SSS1.p2.3.m3.1.1.2.2">ğ·</ci><ci id="S2.SS1.SSS1.p2.3.m3.1.1.2.3a.cmml" xref="S2.SS1.SSS1.p2.3.m3.1.1.2.3"><mtext mathsize="70%" id="S2.SS1.SSS1.p2.3.m3.1.1.2.3.cmml" xref="S2.SS1.SSS1.p2.3.m3.1.1.2.3">enc</mtext></ci></apply><cn type="integer" id="S2.SS1.SSS1.p2.3.m3.1.1.3.cmml" xref="S2.SS1.SSS1.p2.3.m3.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.3.m3.1c">D_{\text{enc}}=32</annotation></semantics></math> and <math id="S2.SS1.SSS1.p2.4.m4.1" class="ltx_Math" alttext="N_{\text{cb}}=8" display="inline"><semantics id="S2.SS1.SSS1.p2.4.m4.1a"><mrow id="S2.SS1.SSS1.p2.4.m4.1.1" xref="S2.SS1.SSS1.p2.4.m4.1.1.cmml"><msub id="S2.SS1.SSS1.p2.4.m4.1.1.2" xref="S2.SS1.SSS1.p2.4.m4.1.1.2.cmml"><mi id="S2.SS1.SSS1.p2.4.m4.1.1.2.2" xref="S2.SS1.SSS1.p2.4.m4.1.1.2.2.cmml">N</mi><mtext id="S2.SS1.SSS1.p2.4.m4.1.1.2.3" xref="S2.SS1.SSS1.p2.4.m4.1.1.2.3a.cmml">cb</mtext></msub><mo id="S2.SS1.SSS1.p2.4.m4.1.1.1" xref="S2.SS1.SSS1.p2.4.m4.1.1.1.cmml">=</mo><mn id="S2.SS1.SSS1.p2.4.m4.1.1.3" xref="S2.SS1.SSS1.p2.4.m4.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.4.m4.1b"><apply id="S2.SS1.SSS1.p2.4.m4.1.1.cmml" xref="S2.SS1.SSS1.p2.4.m4.1.1"><eq id="S2.SS1.SSS1.p2.4.m4.1.1.1.cmml" xref="S2.SS1.SSS1.p2.4.m4.1.1.1"></eq><apply id="S2.SS1.SSS1.p2.4.m4.1.1.2.cmml" xref="S2.SS1.SSS1.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p2.4.m4.1.1.2.1.cmml" xref="S2.SS1.SSS1.p2.4.m4.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS1.p2.4.m4.1.1.2.2.cmml" xref="S2.SS1.SSS1.p2.4.m4.1.1.2.2">ğ‘</ci><ci id="S2.SS1.SSS1.p2.4.m4.1.1.2.3a.cmml" xref="S2.SS1.SSS1.p2.4.m4.1.1.2.3"><mtext mathsize="70%" id="S2.SS1.SSS1.p2.4.m4.1.1.2.3.cmml" xref="S2.SS1.SSS1.p2.4.m4.1.1.2.3">cb</mtext></ci></apply><cn type="integer" id="S2.SS1.SSS1.p2.4.m4.1.1.3.cmml" xref="S2.SS1.SSS1.p2.4.m4.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.4.m4.1c">N_{\text{cb}}=8</annotation></semantics></math>.
For convenience, each <math id="S2.SS1.SSS1.p2.5.m5.1" class="ltx_Math" alttext="D_{\text{enc}}/N_{\text{cb}}" display="inline"><semantics id="S2.SS1.SSS1.p2.5.m5.1a"><mrow id="S2.SS1.SSS1.p2.5.m5.1.1" xref="S2.SS1.SSS1.p2.5.m5.1.1.cmml"><msub id="S2.SS1.SSS1.p2.5.m5.1.1.2" xref="S2.SS1.SSS1.p2.5.m5.1.1.2.cmml"><mi id="S2.SS1.SSS1.p2.5.m5.1.1.2.2" xref="S2.SS1.SSS1.p2.5.m5.1.1.2.2.cmml">D</mi><mtext id="S2.SS1.SSS1.p2.5.m5.1.1.2.3" xref="S2.SS1.SSS1.p2.5.m5.1.1.2.3a.cmml">enc</mtext></msub><mo id="S2.SS1.SSS1.p2.5.m5.1.1.1" xref="S2.SS1.SSS1.p2.5.m5.1.1.1.cmml">/</mo><msub id="S2.SS1.SSS1.p2.5.m5.1.1.3" xref="S2.SS1.SSS1.p2.5.m5.1.1.3.cmml"><mi id="S2.SS1.SSS1.p2.5.m5.1.1.3.2" xref="S2.SS1.SSS1.p2.5.m5.1.1.3.2.cmml">N</mi><mtext id="S2.SS1.SSS1.p2.5.m5.1.1.3.3" xref="S2.SS1.SSS1.p2.5.m5.1.1.3.3a.cmml">cb</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.5.m5.1b"><apply id="S2.SS1.SSS1.p2.5.m5.1.1.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1"><divide id="S2.SS1.SSS1.p2.5.m5.1.1.1.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.1"></divide><apply id="S2.SS1.SSS1.p2.5.m5.1.1.2.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p2.5.m5.1.1.2.1.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS1.p2.5.m5.1.1.2.2.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.2.2">ğ·</ci><ci id="S2.SS1.SSS1.p2.5.m5.1.1.2.3a.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.2.3"><mtext mathsize="70%" id="S2.SS1.SSS1.p2.5.m5.1.1.2.3.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.2.3">enc</mtext></ci></apply><apply id="S2.SS1.SSS1.p2.5.m5.1.1.3.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p2.5.m5.1.1.3.1.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.3">subscript</csymbol><ci id="S2.SS1.SSS1.p2.5.m5.1.1.3.2.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.3.2">ğ‘</ci><ci id="S2.SS1.SSS1.p2.5.m5.1.1.3.3a.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.3.3"><mtext mathsize="70%" id="S2.SS1.SSS1.p2.5.m5.1.1.3.3.cmml" xref="S2.SS1.SSS1.p2.5.m5.1.1.3.3">cb</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.5.m5.1c">D_{\text{enc}}/N_{\text{cb}}</annotation></semantics></math>-dimensional subset of the embedding is seen as a separate group quantized with <math id="S2.SS1.SSS1.p2.6.m6.4" class="ltx_Math" alttext="\left[8,5,5,5\right]" display="inline"><semantics id="S2.SS1.SSS1.p2.6.m6.4a"><mrow id="S2.SS1.SSS1.p2.6.m6.4.5.2" xref="S2.SS1.SSS1.p2.6.m6.4.5.1.cmml"><mo id="S2.SS1.SSS1.p2.6.m6.4.5.2.1" xref="S2.SS1.SSS1.p2.6.m6.4.5.1.cmml">[</mo><mn id="S2.SS1.SSS1.p2.6.m6.1.1" xref="S2.SS1.SSS1.p2.6.m6.1.1.cmml">8</mn><mo id="S2.SS1.SSS1.p2.6.m6.4.5.2.2" xref="S2.SS1.SSS1.p2.6.m6.4.5.1.cmml">,</mo><mn id="S2.SS1.SSS1.p2.6.m6.2.2" xref="S2.SS1.SSS1.p2.6.m6.2.2.cmml">5</mn><mo id="S2.SS1.SSS1.p2.6.m6.4.5.2.3" xref="S2.SS1.SSS1.p2.6.m6.4.5.1.cmml">,</mo><mn id="S2.SS1.SSS1.p2.6.m6.3.3" xref="S2.SS1.SSS1.p2.6.m6.3.3.cmml">5</mn><mo id="S2.SS1.SSS1.p2.6.m6.4.5.2.4" xref="S2.SS1.SSS1.p2.6.m6.4.5.1.cmml">,</mo><mn id="S2.SS1.SSS1.p2.6.m6.4.4" xref="S2.SS1.SSS1.p2.6.m6.4.4.cmml">5</mn><mo id="S2.SS1.SSS1.p2.6.m6.4.5.2.5" xref="S2.SS1.SSS1.p2.6.m6.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.6.m6.4b"><list id="S2.SS1.SSS1.p2.6.m6.4.5.1.cmml" xref="S2.SS1.SSS1.p2.6.m6.4.5.2"><cn type="integer" id="S2.SS1.SSS1.p2.6.m6.1.1.cmml" xref="S2.SS1.SSS1.p2.6.m6.1.1">8</cn><cn type="integer" id="S2.SS1.SSS1.p2.6.m6.2.2.cmml" xref="S2.SS1.SSS1.p2.6.m6.2.2">5</cn><cn type="integer" id="S2.SS1.SSS1.p2.6.m6.3.3.cmml" xref="S2.SS1.SSS1.p2.6.m6.3.3">5</cn><cn type="integer" id="S2.SS1.SSS1.p2.6.m6.4.4.cmml" xref="S2.SS1.SSS1.p2.6.m6.4.4">5</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.6.m6.4c">\left[8,5,5,5\right]</annotation></semantics></math> levels, resulting in <math id="S2.SS1.SSS1.p2.7.m7.1" class="ltx_Math" alttext="D_{\text{cb}}=1000" display="inline"><semantics id="S2.SS1.SSS1.p2.7.m7.1a"><mrow id="S2.SS1.SSS1.p2.7.m7.1.1" xref="S2.SS1.SSS1.p2.7.m7.1.1.cmml"><msub id="S2.SS1.SSS1.p2.7.m7.1.1.2" xref="S2.SS1.SSS1.p2.7.m7.1.1.2.cmml"><mi id="S2.SS1.SSS1.p2.7.m7.1.1.2.2" xref="S2.SS1.SSS1.p2.7.m7.1.1.2.2.cmml">D</mi><mtext id="S2.SS1.SSS1.p2.7.m7.1.1.2.3" xref="S2.SS1.SSS1.p2.7.m7.1.1.2.3a.cmml">cb</mtext></msub><mo id="S2.SS1.SSS1.p2.7.m7.1.1.1" xref="S2.SS1.SSS1.p2.7.m7.1.1.1.cmml">=</mo><mn id="S2.SS1.SSS1.p2.7.m7.1.1.3" xref="S2.SS1.SSS1.p2.7.m7.1.1.3.cmml">1000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p2.7.m7.1b"><apply id="S2.SS1.SSS1.p2.7.m7.1.1.cmml" xref="S2.SS1.SSS1.p2.7.m7.1.1"><eq id="S2.SS1.SSS1.p2.7.m7.1.1.1.cmml" xref="S2.SS1.SSS1.p2.7.m7.1.1.1"></eq><apply id="S2.SS1.SSS1.p2.7.m7.1.1.2.cmml" xref="S2.SS1.SSS1.p2.7.m7.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS1.p2.7.m7.1.1.2.1.cmml" xref="S2.SS1.SSS1.p2.7.m7.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS1.p2.7.m7.1.1.2.2.cmml" xref="S2.SS1.SSS1.p2.7.m7.1.1.2.2">ğ·</ci><ci id="S2.SS1.SSS1.p2.7.m7.1.1.2.3a.cmml" xref="S2.SS1.SSS1.p2.7.m7.1.1.2.3"><mtext mathsize="70%" id="S2.SS1.SSS1.p2.7.m7.1.1.2.3.cmml" xref="S2.SS1.SSS1.p2.7.m7.1.1.2.3">cb</mtext></ci></apply><cn type="integer" id="S2.SS1.SSS1.p2.7.m7.1.1.3.cmml" xref="S2.SS1.SSS1.p2.7.m7.1.1.3">1000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p2.7.m7.1c">D_{\text{cb}}=1000</annotation></semantics></math>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Time-domain NAC</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.4" class="ltx_p">Time-domain NAC (TD-NAC) follows the architecture used in previous worksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
The encoder consists of a series of convolutional layers with downsampling applied directly on the time-domain signal at sample rate <math id="S2.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="f_{s}" display="inline"><semantics id="S2.SS1.SSS2.p1.1.m1.1a"><msub id="S2.SS1.SSS2.p1.1.m1.1.1" xref="S2.SS1.SSS2.p1.1.m1.1.1.cmml"><mi id="S2.SS1.SSS2.p1.1.m1.1.1.2" xref="S2.SS1.SSS2.p1.1.m1.1.1.2.cmml">f</mi><mi id="S2.SS1.SSS2.p1.1.m1.1.1.3" xref="S2.SS1.SSS2.p1.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.1.m1.1b"><apply id="S2.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1.2">ğ‘“</ci><ci id="S2.SS1.SSS2.p1.1.m1.1.1.3.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.1.m1.1c">f_{s}</annotation></semantics></math>, resulting in total downsampling factor <math id="S2.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="f_{\text{down}}" display="inline"><semantics id="S2.SS1.SSS2.p1.2.m2.1a"><msub id="S2.SS1.SSS2.p1.2.m2.1.1" xref="S2.SS1.SSS2.p1.2.m2.1.1.cmml"><mi id="S2.SS1.SSS2.p1.2.m2.1.1.2" xref="S2.SS1.SSS2.p1.2.m2.1.1.2.cmml">f</mi><mtext id="S2.SS1.SSS2.p1.2.m2.1.1.3" xref="S2.SS1.SSS2.p1.2.m2.1.1.3a.cmml">down</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.2.m2.1b"><apply id="S2.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.2">ğ‘“</ci><ci id="S2.SS1.SSS2.p1.2.m2.1.1.3a.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3"><mtext mathsize="70%" id="S2.SS1.SSS2.p1.2.m2.1.1.3.cmml" xref="S2.SS1.SSS2.p1.2.m2.1.1.3">down</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.2.m2.1c">f_{\text{down}}</annotation></semantics></math>.
For each time step, the encoder generates a latent representation of the input signal of dimension <math id="S2.SS1.SSS2.p1.3.m3.1" class="ltx_Math" alttext="D_{\text{enc}}" display="inline"><semantics id="S2.SS1.SSS2.p1.3.m3.1a"><msub id="S2.SS1.SSS2.p1.3.m3.1.1" xref="S2.SS1.SSS2.p1.3.m3.1.1.cmml"><mi id="S2.SS1.SSS2.p1.3.m3.1.1.2" xref="S2.SS1.SSS2.p1.3.m3.1.1.2.cmml">D</mi><mtext id="S2.SS1.SSS2.p1.3.m3.1.1.3" xref="S2.SS1.SSS2.p1.3.m3.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.3.m3.1b"><apply id="S2.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.3.m3.1.1.1.cmml" xref="S2.SS1.SSS2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.SSS2.p1.3.m3.1.1.2.cmml" xref="S2.SS1.SSS2.p1.3.m3.1.1.2">ğ·</ci><ci id="S2.SS1.SSS2.p1.3.m3.1.1.3a.cmml" xref="S2.SS1.SSS2.p1.3.m3.1.1.3"><mtext mathsize="70%" id="S2.SS1.SSS2.p1.3.m3.1.1.3.cmml" xref="S2.SS1.SSS2.p1.3.m3.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.3.m3.1c">D_{\text{enc}}</annotation></semantics></math> at rate <math id="S2.SS1.SSS2.p1.4.m4.1" class="ltx_Math" alttext="f_{\text{enc}}=f_{s}/f_{\text{down}}" display="inline"><semantics id="S2.SS1.SSS2.p1.4.m4.1a"><mrow id="S2.SS1.SSS2.p1.4.m4.1.1" xref="S2.SS1.SSS2.p1.4.m4.1.1.cmml"><msub id="S2.SS1.SSS2.p1.4.m4.1.1.2" xref="S2.SS1.SSS2.p1.4.m4.1.1.2.cmml"><mi id="S2.SS1.SSS2.p1.4.m4.1.1.2.2" xref="S2.SS1.SSS2.p1.4.m4.1.1.2.2.cmml">f</mi><mtext id="S2.SS1.SSS2.p1.4.m4.1.1.2.3" xref="S2.SS1.SSS2.p1.4.m4.1.1.2.3a.cmml">enc</mtext></msub><mo id="S2.SS1.SSS2.p1.4.m4.1.1.1" xref="S2.SS1.SSS2.p1.4.m4.1.1.1.cmml">=</mo><mrow id="S2.SS1.SSS2.p1.4.m4.1.1.3" xref="S2.SS1.SSS2.p1.4.m4.1.1.3.cmml"><msub id="S2.SS1.SSS2.p1.4.m4.1.1.3.2" xref="S2.SS1.SSS2.p1.4.m4.1.1.3.2.cmml"><mi id="S2.SS1.SSS2.p1.4.m4.1.1.3.2.2" xref="S2.SS1.SSS2.p1.4.m4.1.1.3.2.2.cmml">f</mi><mi id="S2.SS1.SSS2.p1.4.m4.1.1.3.2.3" xref="S2.SS1.SSS2.p1.4.m4.1.1.3.2.3.cmml">s</mi></msub><mo id="S2.SS1.SSS2.p1.4.m4.1.1.3.1" xref="S2.SS1.SSS2.p1.4.m4.1.1.3.1.cmml">/</mo><msub id="S2.SS1.SSS2.p1.4.m4.1.1.3.3" xref="S2.SS1.SSS2.p1.4.m4.1.1.3.3.cmml"><mi id="S2.SS1.SSS2.p1.4.m4.1.1.3.3.2" xref="S2.SS1.SSS2.p1.4.m4.1.1.3.3.2.cmml">f</mi><mtext id="S2.SS1.SSS2.p1.4.m4.1.1.3.3.3" xref="S2.SS1.SSS2.p1.4.m4.1.1.3.3.3a.cmml">down</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.4.m4.1b"><apply id="S2.SS1.SSS2.p1.4.m4.1.1.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1"><eq id="S2.SS1.SSS2.p1.4.m4.1.1.1.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1.1"></eq><apply id="S2.SS1.SSS2.p1.4.m4.1.1.2.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.4.m4.1.1.2.1.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1.2">subscript</csymbol><ci id="S2.SS1.SSS2.p1.4.m4.1.1.2.2.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1.2.2">ğ‘“</ci><ci id="S2.SS1.SSS2.p1.4.m4.1.1.2.3a.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1.2.3"><mtext mathsize="70%" id="S2.SS1.SSS2.p1.4.m4.1.1.2.3.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1.2.3">enc</mtext></ci></apply><apply id="S2.SS1.SSS2.p1.4.m4.1.1.3.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1.3"><divide id="S2.SS1.SSS2.p1.4.m4.1.1.3.1.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1.3.1"></divide><apply id="S2.SS1.SSS2.p1.4.m4.1.1.3.2.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1.3.2"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.4.m4.1.1.3.2.1.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1.3.2">subscript</csymbol><ci id="S2.SS1.SSS2.p1.4.m4.1.1.3.2.2.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1.3.2.2">ğ‘“</ci><ci id="S2.SS1.SSS2.p1.4.m4.1.1.3.2.3.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1.3.2.3">ğ‘ </ci></apply><apply id="S2.SS1.SSS2.p1.4.m4.1.1.3.3.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS1.SSS2.p1.4.m4.1.1.3.3.1.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1.3.3">subscript</csymbol><ci id="S2.SS1.SSS2.p1.4.m4.1.1.3.3.2.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1.3.3.2">ğ‘“</ci><ci id="S2.SS1.SSS2.p1.4.m4.1.1.3.3.3a.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1.3.3.3"><mtext mathsize="70%" id="S2.SS1.SSS2.p1.4.m4.1.1.3.3.3.cmml" xref="S2.SS1.SSS2.p1.4.m4.1.1.3.3.3">down</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.4.m4.1c">f_{\text{enc}}=f_{s}/f_{\text{down}}</annotation></semantics></math>, which is quantized to obtain discrete codes.
For reconstructions, discrete codes are dequantized into a latent representation, and a convolutional decoder is used to obtain a time-domain output signal.
Our encoder and decoder configuration is followingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
The encoder consists of 1D convolutions followed by residual convolution blocks with downsampling, with LSTM layers for sequence modeling and a final 1D convolution.
The decoder uses a reverse layer ordering with transposed convolutionsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Spectral NAC</h4>

<div id="S2.SS1.SSS3.p1" class="ltx_para">
<p id="S2.SS1.SSS3.p1.1" class="ltx_p">As opposed to the time-domain NAC, a spectral NACÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> applies the encoder on a spectral representation of the input signal obtained using a filterbank as depicted in FigureÂ <a href="#S2.F1" title="Figure 1 â€£ 2 Speech recognition with audio codecs â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
We use an 80-dimensional mel spectrogram obtained from a <span id="S2.SS1.SSS3.p1.1.1" class="ltx_text">mel-filterbank</span> and referred to the model as <span id="S2.SS1.SSS3.p1.1.2" class="ltx_text">Mel-NAC</span>.</p>
</div>
<div id="S2.SS1.SSS3.p2" class="ltx_para">
<p id="S2.SS1.SSS3.p2.1" class="ltx_p">With RVQ we encode the mel-spectrogram with a single residual network consisting of six <span id="S2.SS1.SSS3.p2.1.1" class="ltx_text">HiFi-GAN V1</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> residual blocks with a hidden dimension of 256 and 1024 residual channels.
With FSQ we divide the mel-spectrogram into 8 groups each containing 10 mel-bands.
Each group is encoded using separate residual encoders with hidden dimension of 128 and 256 residual channels.
The decoder is the <span id="S2.SS1.SSS3.p2.1.2" class="ltx_text">HiFi-GAN V1</span> generator with 1024 initial channels.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div id="S2.F2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:255.4pt;vertical-align:-0.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-483.5pt,284.4pt) scale(0.30959751849773,0.30959751849773) ;"><img src="/html/2407.03495/assets/images/asr_pipeline_new.png" id="S2.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="1938" height="1140" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The ASR with discrete codes pipeline.</figcaption>
</figure>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Speech recognition pipeline</h3>

<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Embedding layer and codebook initialization</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">The initial stage of the pipeline involves the mapping of codes to embeddings, which are subsequently forwarded to the ASR encoder for model training. Here we employ a standard neural embedding layer which maps the output of each codebook to a fixed dimensional embedding of size <math id="S2.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="D_{\text{emb}}" display="inline"><semantics id="S2.SS2.SSS1.p1.1.m1.1a"><msub id="S2.SS2.SSS1.p1.1.m1.1.1" xref="S2.SS2.SSS1.p1.1.m1.1.1.cmml"><mi id="S2.SS2.SSS1.p1.1.m1.1.1.2" xref="S2.SS2.SSS1.p1.1.m1.1.1.2.cmml">D</mi><mtext id="S2.SS2.SSS1.p1.1.m1.1.1.3" xref="S2.SS2.SSS1.p1.1.m1.1.1.3a.cmml">emb</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.1.m1.1b"><apply id="S2.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.2">ğ·</ci><ci id="S2.SS2.SSS1.p1.1.m1.1.1.3a.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S2.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1.3">emb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.1.m1.1c">D_{\text{emb}}</annotation></semantics></math>. The parameters of this neural embedding model are iteratively optimized during the end-to-end ASR system training. We can either initialize the weights of the embedding model randomly or use the learnt codebooks from the trained NAC model to provide a better starting point. We refer to the latter approach as codebook initialization of the embedding layer in the rest of the paper.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Code aggregation strategies</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.2" class="ltx_p">As discussed in SectionÂ <a href="#S2.SS1" title="2.1 Audio codecs â€£ 2 Speech recognition with audio codecs â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>, most NACs employ multiple codebooks to obtain reliable compressed discrete representation of the input signal. Consequently, this results in the presence of multiple codes per time step corresponding to each codebook. It becomes imperative to aggregate across codebooks for each timestep to enable their integration into standard ASR encoder-decoder architectures. This aggregation process can be executed through two distinct schemes, as illustrated in FigureÂ <a href="#S2.F2" title="Figure 2 â€£ 2.1.3 Spectral NAC â€£ 2.1 Audio codecs â€£ 2 Speech recognition with audio codecs â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>: stacking and averaging. In the stacking (stack) aggregation approach, embeddings from different codebooks are stacked atop one another, yielding an embedding size of <math id="S2.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="N_{cb}\times D_{\text{emb}}" display="inline"><semantics id="S2.SS2.SSS2.p1.1.m1.1a"><mrow id="S2.SS2.SSS2.p1.1.m1.1.1" xref="S2.SS2.SSS2.p1.1.m1.1.1.cmml"><msub id="S2.SS2.SSS2.p1.1.m1.1.1.2" xref="S2.SS2.SSS2.p1.1.m1.1.1.2.cmml"><mi id="S2.SS2.SSS2.p1.1.m1.1.1.2.2" xref="S2.SS2.SSS2.p1.1.m1.1.1.2.2.cmml">N</mi><mrow id="S2.SS2.SSS2.p1.1.m1.1.1.2.3" xref="S2.SS2.SSS2.p1.1.m1.1.1.2.3.cmml"><mi id="S2.SS2.SSS2.p1.1.m1.1.1.2.3.2" xref="S2.SS2.SSS2.p1.1.m1.1.1.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p1.1.m1.1.1.2.3.1" xref="S2.SS2.SSS2.p1.1.m1.1.1.2.3.1.cmml">â€‹</mo><mi id="S2.SS2.SSS2.p1.1.m1.1.1.2.3.3" xref="S2.SS2.SSS2.p1.1.m1.1.1.2.3.3.cmml">b</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.SSS2.p1.1.m1.1.1.1" xref="S2.SS2.SSS2.p1.1.m1.1.1.1.cmml">Ã—</mo><msub id="S2.SS2.SSS2.p1.1.m1.1.1.3" xref="S2.SS2.SSS2.p1.1.m1.1.1.3.cmml"><mi id="S2.SS2.SSS2.p1.1.m1.1.1.3.2" xref="S2.SS2.SSS2.p1.1.m1.1.1.3.2.cmml">D</mi><mtext id="S2.SS2.SSS2.p1.1.m1.1.1.3.3" xref="S2.SS2.SSS2.p1.1.m1.1.1.3.3a.cmml">emb</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.1.m1.1b"><apply id="S2.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1"><times id="S2.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.1"></times><apply id="S2.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.1.m1.1.1.2.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS2.p1.1.m1.1.1.2.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.2.2">ğ‘</ci><apply id="S2.SS2.SSS2.p1.1.m1.1.1.2.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.2.3"><times id="S2.SS2.SSS2.p1.1.m1.1.1.2.3.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.2.3.1"></times><ci id="S2.SS2.SSS2.p1.1.m1.1.1.2.3.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.2.3.2">ğ‘</ci><ci id="S2.SS2.SSS2.p1.1.m1.1.1.2.3.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.2.3.3">ğ‘</ci></apply></apply><apply id="S2.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.1.m1.1.1.3.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.3">subscript</csymbol><ci id="S2.SS2.SSS2.p1.1.m1.1.1.3.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.3.2">ğ·</ci><ci id="S2.SS2.SSS2.p1.1.m1.1.1.3.3a.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.3.3"><mtext mathsize="70%" id="S2.SS2.SSS2.p1.1.m1.1.1.3.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1.3.3">emb</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.1.m1.1c">N_{cb}\times D_{\text{emb}}</annotation></semantics></math>. Conversely, the averaging (avg) aggregation approach entails the computation of the average of embeddings from different codebooks at each timestamp, resulting in an embedding size of <math id="S2.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="D_{\text{emb}}" display="inline"><semantics id="S2.SS2.SSS2.p1.2.m2.1a"><msub id="S2.SS2.SSS2.p1.2.m2.1.1" xref="S2.SS2.SSS2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.SSS2.p1.2.m2.1.1.2" xref="S2.SS2.SSS2.p1.2.m2.1.1.2.cmml">D</mi><mtext id="S2.SS2.SSS2.p1.2.m2.1.1.3" xref="S2.SS2.SSS2.p1.2.m2.1.1.3a.cmml">emb</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.2.m2.1b"><apply id="S2.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.SSS2.p1.2.m2.1.1.2">ğ·</ci><ci id="S2.SS2.SSS2.p1.2.m2.1.1.3a.cmml" xref="S2.SS2.SSS2.p1.2.m2.1.1.3"><mtext mathsize="70%" id="S2.SS2.SSS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.SSS2.p1.2.m2.1.1.3">emb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.2.m2.1c">D_{\text{emb}}</annotation></semantics></math>. In this paper, the default codes aggregation strategy is averaging, unless otherwise specified.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Spectrogram augmentation</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">The technique of spectrogram augmentation (SpecAug) serves as a method for augmenting audio data, as introduced inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. This methodology transforms the augmentation task for audio signals into one resembling image augmentation by operating on the audio spectrogram. Though in this work we are training the ASR systems on discrete codes, we evaluate the impact of SpecAug on the ASR pipeline.</p>
</div>
</section>
<section id="S2.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.4 </span>Noisy embedding training</h4>

<div id="S2.SS2.SSS4.p1" class="ltx_para">
<p id="S2.SS2.SSS4.p1.1" class="ltx_p">Advancements in large language model (LLM) research has shown that model fine-tuning process can be improved by the simple augmentation technique of adding noise to the embedding vectors during trainingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. We evaluate the efficacy of this method by adding scaled uniform noise (parameterized by <math id="S2.SS2.SSS4.p1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S2.SS2.SSS4.p1.1.m1.1a"><mi id="S2.SS2.SSS4.p1.1.m1.1.1" xref="S2.SS2.SSS4.p1.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS4.p1.1.m1.1b"><ci id="S2.SS2.SSS4.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS4.p1.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS4.p1.1.m1.1c">\alpha</annotation></semantics></math> as introduced inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>) to the output of the embedding layer (SectionÂ <a href="#S2.SS2.SSS1" title="2.2.1 Embedding layer and codebook initialization â€£ 2.2 Speech recognition pipeline â€£ 2 Speech recognition with audio codecs â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.1</span></a>) during the training phase.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental setup</h2>

<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Configurations of the considered NACs.</figcaption>
<div id="S3.T1.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:355.6pt;height:115.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(39.5pt,-12.9pt) scale(1.28568092972168,1.28568092972168) ;">
<table id="S3.T1.3.3" class="ltx_tabular ltx_align_middle">
<tr id="S3.T1.3.3.3" class="ltx_tr">
<td id="S3.T1.3.3.3.4" class="ltx_td ltx_align_center ltx_border_tt">Codec</td>
<td id="S3.T1.3.3.3.5" class="ltx_td ltx_align_center ltx_border_tt">Quantizer</td>
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">Parameters / <math id="S3.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="10^{6}" display="inline"><semantics id="S3.T1.1.1.1.1.m1.1a"><msup id="S3.T1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.cmml"><mn id="S3.T1.1.1.1.1.m1.1.1.2" xref="S3.T1.1.1.1.1.m1.1.1.2.cmml">10</mn><mn id="S3.T1.1.1.1.1.m1.1.1.3" xref="S3.T1.1.1.1.1.m1.1.1.3.cmml">6</mn></msup><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><apply id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.T1.1.1.1.1.m1.1.1.2.cmml" xref="S3.T1.1.1.1.1.m1.1.1.2">10</cn><cn type="integer" id="S3.T1.1.1.1.1.m1.1.1.3.cmml" xref="S3.T1.1.1.1.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">10^{6}</annotation></semantics></math>
</td>
<td id="S3.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt"><math id="S3.T1.2.2.2.2.m1.1" class="ltx_Math" alttext="D_{\text{enc}}" display="inline"><semantics id="S3.T1.2.2.2.2.m1.1a"><msub id="S3.T1.2.2.2.2.m1.1.1" xref="S3.T1.2.2.2.2.m1.1.1.cmml"><mi id="S3.T1.2.2.2.2.m1.1.1.2" xref="S3.T1.2.2.2.2.m1.1.1.2.cmml">D</mi><mtext id="S3.T1.2.2.2.2.m1.1.1.3" xref="S3.T1.2.2.2.2.m1.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.m1.1b"><apply id="S3.T1.2.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.2.2.2.2.m1.1.1.1.cmml" xref="S3.T1.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S3.T1.2.2.2.2.m1.1.1.2.cmml" xref="S3.T1.2.2.2.2.m1.1.1.2">ğ·</ci><ci id="S3.T1.2.2.2.2.m1.1.1.3a.cmml" xref="S3.T1.2.2.2.2.m1.1.1.3"><mtext mathsize="70%" id="S3.T1.2.2.2.2.m1.1.1.3.cmml" xref="S3.T1.2.2.2.2.m1.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.m1.1c">D_{\text{enc}}</annotation></semantics></math></td>
<td id="S3.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">
<math id="S3.T1.3.3.3.3.m1.1" class="ltx_Math" alttext="f_{\text{enc}}" display="inline"><semantics id="S3.T1.3.3.3.3.m1.1a"><msub id="S3.T1.3.3.3.3.m1.1.1" xref="S3.T1.3.3.3.3.m1.1.1.cmml"><mi id="S3.T1.3.3.3.3.m1.1.1.2" xref="S3.T1.3.3.3.3.m1.1.1.2.cmml">f</mi><mtext id="S3.T1.3.3.3.3.m1.1.1.3" xref="S3.T1.3.3.3.3.m1.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.m1.1b"><apply id="S3.T1.3.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.3.3.3.3.m1.1.1.1.cmml" xref="S3.T1.3.3.3.3.m1.1.1">subscript</csymbol><ci id="S3.T1.3.3.3.3.m1.1.1.2.cmml" xref="S3.T1.3.3.3.3.m1.1.1.2">ğ‘“</ci><ci id="S3.T1.3.3.3.3.m1.1.1.3a.cmml" xref="S3.T1.3.3.3.3.m1.1.1.3"><mtext mathsize="70%" id="S3.T1.3.3.3.3.m1.1.1.3.cmml" xref="S3.T1.3.3.3.3.m1.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.m1.1c">f_{\text{enc}}</annotation></semantics></math> / Hz</td>
</tr>
<tr id="S3.T1.3.3.4" class="ltx_tr">
<td id="S3.T1.3.3.4.1" class="ltx_td ltx_align_center ltx_border_t">TD-NAC</td>
<td id="S3.T1.3.3.4.2" class="ltx_td ltx_align_center ltx_border_t">RVQ</td>
<td id="S3.T1.3.3.4.3" class="ltx_td ltx_align_center ltx_border_t">13.8</td>
<td id="S3.T1.3.3.4.4" class="ltx_td ltx_align_center ltx_border_t">128</td>
<td id="S3.T1.3.3.4.5" class="ltx_td ltx_align_center ltx_border_t">80</td>
</tr>
<tr id="S3.T1.3.3.5" class="ltx_tr">
<td id="S3.T1.3.3.5.1" class="ltx_td ltx_align_center ltx_border_t">TD-NAC</td>
<td id="S3.T1.3.3.5.2" class="ltx_td ltx_align_center ltx_border_t">FVQ</td>
<td id="S3.T1.3.3.5.3" class="ltx_td ltx_align_center ltx_border_t">13.1</td>
<td id="S3.T1.3.3.5.4" class="ltx_td ltx_align_center ltx_border_t">32</td>
<td id="S3.T1.3.3.5.5" class="ltx_td ltx_align_center ltx_border_t">80</td>
</tr>
<tr id="S3.T1.3.3.6" class="ltx_tr">
<td id="S3.T1.3.3.6.1" class="ltx_td ltx_align_center ltx_border_t">Mel-NAC</td>
<td id="S3.T1.3.3.6.2" class="ltx_td ltx_align_center ltx_border_t">RVQ</td>
<td id="S3.T1.3.3.6.3" class="ltx_td ltx_align_center ltx_border_t">105</td>
<td id="S3.T1.3.3.6.4" class="ltx_td ltx_align_center ltx_border_t">128</td>
<td id="S3.T1.3.3.6.5" class="ltx_td ltx_align_center ltx_border_t">62.5</td>
</tr>
<tr id="S3.T1.3.3.7" class="ltx_tr">
<td id="S3.T1.3.3.7.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Mel-NAC</td>
<td id="S3.T1.3.3.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">FVQ</td>
<td id="S3.T1.3.3.7.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">104</td>
<td id="S3.T1.3.3.7.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">32</td>
<td id="S3.T1.3.3.7.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">62.5</td>
</tr>
</table>
</span></div>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>NAC model training</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.9" class="ltx_p">Both <span id="S3.SS1.p1.9.1" class="ltx_text">TD-NAC</span> and <span id="S3.SS1.p1.9.2" class="ltx_text">Mel-NAC</span> are trained on the <span id="S3.SS1.p1.9.3" class="ltx_text">Libri-Light</span> datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> with sample rate 16â€‰kHz.
<span id="S3.SS1.p1.9.4" class="ltx_text">TD-NAC</span> models use an encoder with downsampling rates of <math id="S3.SS1.p1.1.m1.4" class="ltx_Math" alttext="\left\{2,4,5,5\right\}" display="inline"><semantics id="S3.SS1.p1.1.m1.4a"><mrow id="S3.SS1.p1.1.m1.4.5.2" xref="S3.SS1.p1.1.m1.4.5.1.cmml"><mo id="S3.SS1.p1.1.m1.4.5.2.1" xref="S3.SS1.p1.1.m1.4.5.1.cmml">{</mo><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">2</mn><mo id="S3.SS1.p1.1.m1.4.5.2.2" xref="S3.SS1.p1.1.m1.4.5.1.cmml">,</mo><mn id="S3.SS1.p1.1.m1.2.2" xref="S3.SS1.p1.1.m1.2.2.cmml">4</mn><mo id="S3.SS1.p1.1.m1.4.5.2.3" xref="S3.SS1.p1.1.m1.4.5.1.cmml">,</mo><mn id="S3.SS1.p1.1.m1.3.3" xref="S3.SS1.p1.1.m1.3.3.cmml">5</mn><mo id="S3.SS1.p1.1.m1.4.5.2.4" xref="S3.SS1.p1.1.m1.4.5.1.cmml">,</mo><mn id="S3.SS1.p1.1.m1.4.4" xref="S3.SS1.p1.1.m1.4.4.cmml">5</mn><mo id="S3.SS1.p1.1.m1.4.5.2.5" xref="S3.SS1.p1.1.m1.4.5.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.4b"><set id="S3.SS1.p1.1.m1.4.5.1.cmml" xref="S3.SS1.p1.1.m1.4.5.2"><cn type="integer" id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">2</cn><cn type="integer" id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2">4</cn><cn type="integer" id="S3.SS1.p1.1.m1.3.3.cmml" xref="S3.SS1.p1.1.m1.3.3">5</cn><cn type="integer" id="S3.SS1.p1.1.m1.4.4.cmml" xref="S3.SS1.p1.1.m1.4.4">5</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.4c">\left\{2,4,5,5\right\}</annotation></semantics></math>, resulting in <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="f_{\text{enc}}=80" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><msub id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2.2" xref="S3.SS1.p1.2.m2.1.1.2.2.cmml">f</mi><mtext id="S3.SS1.p1.2.m2.1.1.2.3" xref="S3.SS1.p1.2.m2.1.1.2.3a.cmml">enc</mtext></msub><mo id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">80</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><eq id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></eq><apply id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2.2">ğ‘“</ci><ci id="S3.SS1.p1.2.m2.1.1.2.3a.cmml" xref="S3.SS1.p1.2.m2.1.1.2.3"><mtext mathsize="70%" id="S3.SS1.p1.2.m2.1.1.2.3.cmml" xref="S3.SS1.p1.2.m2.1.1.2.3">enc</mtext></ci></apply><cn type="integer" id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">f_{\text{enc}}=80</annotation></semantics></math>â€‰Hz.
Both RVQ and FSQ quantizers use <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="N_{\text{cb}}=8" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><msub id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2.2" xref="S3.SS1.p1.3.m3.1.1.2.2.cmml">N</mi><mtext id="S3.SS1.p1.3.m3.1.1.2.3" xref="S3.SS1.p1.3.m3.1.1.2.3a.cmml">cb</mtext></msub><mo id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><eq id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></eq><apply id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2.2">ğ‘</ci><ci id="S3.SS1.p1.3.m3.1.1.2.3a.cmml" xref="S3.SS1.p1.3.m3.1.1.2.3"><mtext mathsize="70%" id="S3.SS1.p1.3.m3.1.1.2.3.cmml" xref="S3.SS1.p1.3.m3.1.1.2.3">cb</mtext></ci></apply><cn type="integer" id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">N_{\text{cb}}=8</annotation></semantics></math> codebooks with <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="D_{\text{cb}}\approx 2^{10}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mrow id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><msub id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2.2" xref="S3.SS1.p1.4.m4.1.1.2.2.cmml">D</mi><mtext id="S3.SS1.p1.4.m4.1.1.2.3" xref="S3.SS1.p1.4.m4.1.1.2.3a.cmml">cb</mtext></msub><mo id="S3.SS1.p1.4.m4.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.cmml">â‰ˆ</mo><msup id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml"><mn id="S3.SS1.p1.4.m4.1.1.3.2" xref="S3.SS1.p1.4.m4.1.1.3.2.cmml">2</mn><mn id="S3.SS1.p1.4.m4.1.1.3.3" xref="S3.SS1.p1.4.m4.1.1.3.3.cmml">10</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><approx id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1"></approx><apply id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.2.1.cmml" xref="S3.SS1.p1.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2.2">ğ·</ci><ci id="S3.SS1.p1.4.m4.1.1.2.3a.cmml" xref="S3.SS1.p1.4.m4.1.1.2.3"><mtext mathsize="70%" id="S3.SS1.p1.4.m4.1.1.2.3.cmml" xref="S3.SS1.p1.4.m4.1.1.2.3">cb</mtext></ci></apply><apply id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.3.1.cmml" xref="S3.SS1.p1.4.m4.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS1.p1.4.m4.1.1.3.2.cmml" xref="S3.SS1.p1.4.m4.1.1.3.2">2</cn><cn type="integer" id="S3.SS1.p1.4.m4.1.1.3.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3.3">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">D_{\text{cb}}\approx 2^{10}</annotation></semantics></math>, resulting in a bitrate of 6.4â€‰kbps.
The <span id="S3.SS1.p1.9.5" class="ltx_text">TD-NAC</span> decoder upsamples in the reverse order of <math id="S3.SS1.p1.5.m5.4" class="ltx_Math" alttext="\left\{5,5,4,2\right\}" display="inline"><semantics id="S3.SS1.p1.5.m5.4a"><mrow id="S3.SS1.p1.5.m5.4.5.2" xref="S3.SS1.p1.5.m5.4.5.1.cmml"><mo id="S3.SS1.p1.5.m5.4.5.2.1" xref="S3.SS1.p1.5.m5.4.5.1.cmml">{</mo><mn id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">5</mn><mo id="S3.SS1.p1.5.m5.4.5.2.2" xref="S3.SS1.p1.5.m5.4.5.1.cmml">,</mo><mn id="S3.SS1.p1.5.m5.2.2" xref="S3.SS1.p1.5.m5.2.2.cmml">5</mn><mo id="S3.SS1.p1.5.m5.4.5.2.3" xref="S3.SS1.p1.5.m5.4.5.1.cmml">,</mo><mn id="S3.SS1.p1.5.m5.3.3" xref="S3.SS1.p1.5.m5.3.3.cmml">4</mn><mo id="S3.SS1.p1.5.m5.4.5.2.4" xref="S3.SS1.p1.5.m5.4.5.1.cmml">,</mo><mn id="S3.SS1.p1.5.m5.4.4" xref="S3.SS1.p1.5.m5.4.4.cmml">2</mn><mo id="S3.SS1.p1.5.m5.4.5.2.5" xref="S3.SS1.p1.5.m5.4.5.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.4b"><set id="S3.SS1.p1.5.m5.4.5.1.cmml" xref="S3.SS1.p1.5.m5.4.5.2"><cn type="integer" id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">5</cn><cn type="integer" id="S3.SS1.p1.5.m5.2.2.cmml" xref="S3.SS1.p1.5.m5.2.2">5</cn><cn type="integer" id="S3.SS1.p1.5.m5.3.3.cmml" xref="S3.SS1.p1.5.m5.3.3">4</cn><cn type="integer" id="S3.SS1.p1.5.m5.4.4.cmml" xref="S3.SS1.p1.5.m5.4.4">2</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.4c">\left\{5,5,4,2\right\}</annotation></semantics></math> to obtain the reconstructed audio signal.
The model is trained on examples with one second of audio.
<span id="S3.SS1.p1.9.6" class="ltx_text">Mel-NAC</span> models use mel-filterbank with a frame length of 1024 samples and frame shift of 256 samples, resulting in <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="f_{\text{enc}}=62.5" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mrow id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><msub id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml"><mi id="S3.SS1.p1.6.m6.1.1.2.2" xref="S3.SS1.p1.6.m6.1.1.2.2.cmml">f</mi><mtext id="S3.SS1.p1.6.m6.1.1.2.3" xref="S3.SS1.p1.6.m6.1.1.2.3a.cmml">enc</mtext></msub><mo id="S3.SS1.p1.6.m6.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.cmml">=</mo><mn id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml">62.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><eq id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1"></eq><apply id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.1.1.2.1.cmml" xref="S3.SS1.p1.6.m6.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.6.m6.1.1.2.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2.2">ğ‘“</ci><ci id="S3.SS1.p1.6.m6.1.1.2.3a.cmml" xref="S3.SS1.p1.6.m6.1.1.2.3"><mtext mathsize="70%" id="S3.SS1.p1.6.m6.1.1.2.3.cmml" xref="S3.SS1.p1.6.m6.1.1.2.3">enc</mtext></ci></apply><cn type="float" id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3">62.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">f_{\text{enc}}=62.5</annotation></semantics></math>â€‰Hz.
Using the same quantizer setup as for <span id="S3.SS1.p1.9.7" class="ltx_text">TD-NAC</span>, this results in a bitrate of 5â€‰kbps.
The <span id="S3.SS1.p1.9.8" class="ltx_text">Mel-NAC</span> decoder upsamples at rates of <math id="S3.SS1.p1.7.m7.4" class="ltx_Math" alttext="\left\{8,4,4,2\right\}" display="inline"><semantics id="S3.SS1.p1.7.m7.4a"><mrow id="S3.SS1.p1.7.m7.4.5.2" xref="S3.SS1.p1.7.m7.4.5.1.cmml"><mo id="S3.SS1.p1.7.m7.4.5.2.1" xref="S3.SS1.p1.7.m7.4.5.1.cmml">{</mo><mn id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">8</mn><mo id="S3.SS1.p1.7.m7.4.5.2.2" xref="S3.SS1.p1.7.m7.4.5.1.cmml">,</mo><mn id="S3.SS1.p1.7.m7.2.2" xref="S3.SS1.p1.7.m7.2.2.cmml">4</mn><mo id="S3.SS1.p1.7.m7.4.5.2.3" xref="S3.SS1.p1.7.m7.4.5.1.cmml">,</mo><mn id="S3.SS1.p1.7.m7.3.3" xref="S3.SS1.p1.7.m7.3.3.cmml">4</mn><mo id="S3.SS1.p1.7.m7.4.5.2.4" xref="S3.SS1.p1.7.m7.4.5.1.cmml">,</mo><mn id="S3.SS1.p1.7.m7.4.4" xref="S3.SS1.p1.7.m7.4.4.cmml">2</mn><mo id="S3.SS1.p1.7.m7.4.5.2.5" xref="S3.SS1.p1.7.m7.4.5.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.4b"><set id="S3.SS1.p1.7.m7.4.5.1.cmml" xref="S3.SS1.p1.7.m7.4.5.2"><cn type="integer" id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">8</cn><cn type="integer" id="S3.SS1.p1.7.m7.2.2.cmml" xref="S3.SS1.p1.7.m7.2.2">4</cn><cn type="integer" id="S3.SS1.p1.7.m7.3.3.cmml" xref="S3.SS1.p1.7.m7.3.3">4</cn><cn type="integer" id="S3.SS1.p1.7.m7.4.4.cmml" xref="S3.SS1.p1.7.m7.4.4">2</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.4c">\left\{8,4,4,2\right\}</annotation></semantics></math> to obtain the reconstructed audio signal.
The model is trained on examples with 0.512 seconds of audio.
All NAC models are trained end-to-end using time-domain loss, discriminative loss, and frequency-domain loss, similar toÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> with equal weights for frequency and discriminative loss and <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><mn id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><cn type="float" id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">0.1</annotation></semantics></math> weight for time-domain loss.
Model sizes depending on the corresponding quantizer are provided in TableÂ <a href="#S3.T1" title="Table 1 â€£ 3 Experimental setup â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
The models are trained on eight NVIDIA V100 GPUs for 130k steps with the AdamW optimizer with a learning rate of <math id="S3.SS1.p1.9.m9.1" class="ltx_Math" alttext="10^{-4}" display="inline"><semantics id="S3.SS1.p1.9.m9.1a"><msup id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml"><mn id="S3.SS1.p1.9.m9.1.1.2" xref="S3.SS1.p1.9.m9.1.1.2.cmml">10</mn><mrow id="S3.SS1.p1.9.m9.1.1.3" xref="S3.SS1.p1.9.m9.1.1.3.cmml"><mo id="S3.SS1.p1.9.m9.1.1.3a" xref="S3.SS1.p1.9.m9.1.1.3.cmml">âˆ’</mo><mn id="S3.SS1.p1.9.m9.1.1.3.2" xref="S3.SS1.p1.9.m9.1.1.3.2.cmml">4</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><apply id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">superscript</csymbol><cn type="integer" id="S3.SS1.p1.9.m9.1.1.2.cmml" xref="S3.SS1.p1.9.m9.1.1.2">10</cn><apply id="S3.SS1.p1.9.m9.1.1.3.cmml" xref="S3.SS1.p1.9.m9.1.1.3"><minus id="S3.SS1.p1.9.m9.1.1.3.1.cmml" xref="S3.SS1.p1.9.m9.1.1.3"></minus><cn type="integer" id="S3.SS1.p1.9.m9.1.1.3.2.cmml" xref="S3.SS1.p1.9.m9.1.1.3.2">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">10^{-4}</annotation></semantics></math>.
A StepLR scheduler with a step size of 1 and gamma of 0.999996 is employed for learning rate decay.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>ASR model training</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.5" class="ltx_p">The ASR models presented in the paper adopt the FastConformer Transducer large architectureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> with 114â€‰M parameters. The encoder consists of 17 layers, with a model dimension of 512. We used 256 channels in sub-sampling module and a kernel size of 9 in convolution module. A single layer RNN-T with hidden dimension of 640 is used for decoder. We maintain the embedding layer output dimension <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="D_{\text{emb}}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msub id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">D</mi><mtext id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3a.cmml">emb</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">ğ·</ci><ci id="S3.SS2.p1.1.m1.1.1.3a.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">emb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">D_{\text{emb}}</annotation></semantics></math> (SectionÂ <a href="#S2.SS2.SSS1" title="2.2.1 Embedding layer and codebook initialization â€£ 2.2 Speech recognition pipeline â€£ 2 Speech recognition with audio codecs â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.1</span></a>) at 128 and set <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\alpha</annotation></semantics></math> (SectionÂ <a href="#S2.SS2.SSS4" title="2.2.4 Noisy embedding training â€£ 2.2 Speech recognition pipeline â€£ 2 Speech recognition with audio codecs â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.4</span></a>) to 5 across all experiments to ensure equitable comparison. The ASR models are trained on the LibriSpeech corpusÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, encompassing 960 hours of English speech data. Evaluation of ASR model performance is conducted using the standard 'clean' and 'other' sets of dev and test partitions from the LibriSpeech dataset. We use a Sentencepiece Byte Pair Encoding (BPE)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> tokenizer with a vocabulary size of 1024, trained on the text data from the LibriSpeech training set. All ASR models have been trained for 100k updates on two nodes with eight NVIDIA A100 80GB GPUs using a batch size of 32 on each GPU.
We use AdamW with a peak learning rate of <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="2\cdot 10^{-3}" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mrow id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mn id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.3.m3.1.1.1" xref="S3.SS2.p1.3.m3.1.1.1.cmml">â‹…</mo><msup id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml"><mn id="S3.SS2.p1.3.m3.1.1.3.2" xref="S3.SS2.p1.3.m3.1.1.3.2.cmml">10</mn><mrow id="S3.SS2.p1.3.m3.1.1.3.3" xref="S3.SS2.p1.3.m3.1.1.3.3.cmml"><mo id="S3.SS2.p1.3.m3.1.1.3.3a" xref="S3.SS2.p1.3.m3.1.1.3.3.cmml">âˆ’</mo><mn id="S3.SS2.p1.3.m3.1.1.3.3.2" xref="S3.SS2.p1.3.m3.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><ci id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1.1">â‹…</ci><cn type="integer" id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">2</cn><apply id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.p1.3.m3.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS2.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.p1.3.m3.1.1.3.2">10</cn><apply id="S3.SS2.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3"><minus id="S3.SS2.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3"></minus><cn type="integer" id="S3.SS2.p1.3.m3.1.1.3.3.2.cmml" xref="S3.SS2.p1.3.m3.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">2\cdot 10^{-3}</annotation></semantics></math>, 15k warmup steps with Cosine annealing, minimum learning rate of <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="10^{-6}" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><msup id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mn id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">10</mn><mrow id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml"><mo id="S3.SS2.p1.4.m4.1.1.3a" xref="S3.SS2.p1.4.m4.1.1.3.cmml">âˆ’</mo><mn id="S3.SS2.p1.4.m4.1.1.3.2" xref="S3.SS2.p1.4.m4.1.1.3.2.cmml">6</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">superscript</csymbol><cn type="integer" id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">10</cn><apply id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3"><minus id="S3.SS2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.p1.4.m4.1.1.3"></minus><cn type="integer" id="S3.SS2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.p1.4.m4.1.1.3.2">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">10^{-6}</annotation></semantics></math> and weight decay of <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="10^{-3}" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><msup id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mn id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">10</mn><mrow id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml"><mo id="S3.SS2.p1.5.m5.1.1.3a" xref="S3.SS2.p1.5.m5.1.1.3.cmml">âˆ’</mo><mn id="S3.SS2.p1.5.m5.1.1.3.2" xref="S3.SS2.p1.5.m5.1.1.3.2.cmml">3</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">superscript</csymbol><cn type="integer" id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2">10</cn><apply id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3"><minus id="S3.SS2.p1.5.m5.1.1.3.1.cmml" xref="S3.SS2.p1.5.m5.1.1.3"></minus><cn type="integer" id="S3.SS2.p1.5.m5.1.1.3.2.cmml" xref="S3.SS2.p1.5.m5.1.1.3.2">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">10^{-3}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Experiments and ablations</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The experiments are designed to study and understand four major components of the pipeline: (<span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">i</span>) role of the NAC type, i.e., <span id="S3.SS3.p1.1.2" class="ltx_text">TD-NAC</span> vs <span id="S3.SS3.p1.1.3" class="ltx_text">Mel-NAC</span>, (<span id="S3.SS3.p1.1.4" class="ltx_text ltx_font_italic">ii</span>) role of quantizers in NAC, i.e., RVQ vs FSQ, (<span id="S3.SS3.p1.1.5" class="ltx_text ltx_font_italic">iii</span>) effect of code aggregation strategies, (<span id="S3.SS3.p1.1.6" class="ltx_text ltx_font_italic">iv</span>) performance improvements of codec ASR systems with pipeline optimizations.
We also setup strong baselines in the form of the traditional Mel-Spectrogram features as well as the widely used Encodec audio codecÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
All other components like <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="D_{\text{emb}}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">D</mi><mtext id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3a.cmml">emb</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">ğ·</ci><ci id="S3.SS3.p1.1.m1.1.1.3a.cmml" xref="S3.SS3.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">emb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">D_{\text{emb}}</annotation></semantics></math>, ASR model size, ASR training data, and tokenizer have been kept constant to facilitate an unbiased study towards the role played by the above highlighted four components. Word error rate (WER) metric is used to evaluate the performance of the ASR models.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results and discussion</h2>

<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>ASR improvement on LibriSpeech eval sets contributed by the various components of the presented ASR pipeline.</figcaption>
<div id="S4.T2.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:217.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(39.0pt,-19.5pt) scale(1.21937569220248,1.21937569220248) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_border_tt"></td>
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">WER / % <math id="S4.T2.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T2.1.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span></td>
</tr>
<tr id="S4.T2.1.1.2" class="ltx_tr">
<td id="S4.T2.1.1.2.1" class="ltx_td ltx_align_left">Codec</td>
<td id="S4.T2.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.2.2.1" class="ltx_text ltx_font_bold">dev-clean</span></td>
<td id="S4.T2.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.2.3.1" class="ltx_text ltx_font_bold">dev-other</span></td>
<td id="S4.T2.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.2.4.1" class="ltx_text ltx_font_bold">test-clean</span></td>
<td id="S4.T2.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.1.2.5.1" class="ltx_text ltx_font_bold">test-other</span></td>
</tr>
<tr id="S4.T2.1.1.3" class="ltx_tr">
<td id="S4.T2.1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">TD-NAC-RVQ</td>
<td id="S4.T2.1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">17.58</td>
<td id="S4.T2.1.1.3.3" class="ltx_td ltx_align_center ltx_border_t">38.77</td>
<td id="S4.T2.1.1.3.4" class="ltx_td ltx_align_center ltx_border_t">17.18</td>
<td id="S4.T2.1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">41.55</td>
</tr>
<tr id="S4.T2.1.1.4" class="ltx_tr">
<td id="S4.T2.1.1.4.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S4.T2.1.1.4.1.1" class="ltx_text"></span><span id="S4.T2.1.1.4.1.2" class="ltx_text">
<span id="S4.T2.1.1.4.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.4.1.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.4.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">+Â codebook</span></span>
<span id="S4.T2.1.1.4.1.2.1.2" class="ltx_tr">
<span id="S4.T2.1.1.4.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">initialization</span></span>
</span></span><span id="S4.T2.1.1.4.1.3" class="ltx_text"></span></td>
<td id="S4.T2.1.1.4.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.1.1.4.2.1" class="ltx_text"></span> <span id="S4.T2.1.1.4.2.2" class="ltx_text">
<span id="S4.T2.1.1.4.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.4.2.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.4.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">3.87</span></span>
<span id="S4.T2.1.1.4.2.2.1.2" class="ltx_tr">
<span id="S4.T2.1.1.4.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(<span id="S4.T2.1.1.4.2.2.1.2.1.1" class="ltx_text" style="color:#008080;">-13.71</span>)</span></span>
</span></span><span id="S4.T2.1.1.4.2.3" class="ltx_text"></span></td>
<td id="S4.T2.1.1.4.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.1.1.4.3.1" class="ltx_text"></span> <span id="S4.T2.1.1.4.3.2" class="ltx_text">
<span id="S4.T2.1.1.4.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.4.3.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.4.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">12.17</span></span>
<span id="S4.T2.1.1.4.3.2.1.2" class="ltx_tr">
<span id="S4.T2.1.1.4.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(<span id="S4.T2.1.1.4.3.2.1.2.1.1" class="ltx_text" style="color:#008080;">-26.6</span>)</span></span>
</span></span><span id="S4.T2.1.1.4.3.3" class="ltx_text"></span></td>
<td id="S4.T2.1.1.4.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.1.1.4.4.1" class="ltx_text"></span> <span id="S4.T2.1.1.4.4.2" class="ltx_text">
<span id="S4.T2.1.1.4.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.4.4.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.4.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">3.84</span></span>
<span id="S4.T2.1.1.4.4.2.1.2" class="ltx_tr">
<span id="S4.T2.1.1.4.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(<span id="S4.T2.1.1.4.4.2.1.2.1.1" class="ltx_text" style="color:#008080;">-13.34</span>)</span></span>
</span></span><span id="S4.T2.1.1.4.4.3" class="ltx_text"></span></td>
<td id="S4.T2.1.1.4.5" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.1.1.4.5.1" class="ltx_text"></span> <span id="S4.T2.1.1.4.5.2" class="ltx_text">
<span id="S4.T2.1.1.4.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.4.5.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.4.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">12.28</span></span>
<span id="S4.T2.1.1.4.5.2.1.2" class="ltx_tr">
<span id="S4.T2.1.1.4.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(<span id="S4.T2.1.1.4.5.2.1.2.1.1" class="ltx_text" style="color:#008080;">-29.27</span>)</span></span>
</span></span><span id="S4.T2.1.1.4.5.3" class="ltx_text"></span></td>
</tr>
<tr id="S4.T2.1.1.5" class="ltx_tr">
<td id="S4.T2.1.1.5.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S4.T2.1.1.5.1.1" class="ltx_text"></span><span id="S4.T2.1.1.5.1.2" class="ltx_text">
<span id="S4.T2.1.1.5.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.5.1.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.5.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">+Â spectrogram</span></span>
<span id="S4.T2.1.1.5.1.2.1.2" class="ltx_tr">
<span id="S4.T2.1.1.5.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Â Â Â Â Â Â augmentation</span></span>
</span></span><span id="S4.T2.1.1.5.1.3" class="ltx_text"></span></td>
<td id="S4.T2.1.1.5.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.1.1.5.2.1" class="ltx_text"></span> <span id="S4.T2.1.1.5.2.2" class="ltx_text">
<span id="S4.T2.1.1.5.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.5.2.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.5.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">2.21</span></span>
<span id="S4.T2.1.1.5.2.2.1.2" class="ltx_tr">
<span id="S4.T2.1.1.5.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(<span id="S4.T2.1.1.5.2.2.1.2.1.1" class="ltx_text" style="color:#008080;">-1.66</span>)</span></span>
</span></span><span id="S4.T2.1.1.5.2.3" class="ltx_text"></span></td>
<td id="S4.T2.1.1.5.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.1.1.5.3.1" class="ltx_text"></span> <span id="S4.T2.1.1.5.3.2" class="ltx_text">
<span id="S4.T2.1.1.5.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.5.3.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.5.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">5.83</span></span>
<span id="S4.T2.1.1.5.3.2.1.2" class="ltx_tr">
<span id="S4.T2.1.1.5.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(<span id="S4.T2.1.1.5.3.2.1.2.1.1" class="ltx_text" style="color:#008080;">-6.34</span>)</span></span>
</span></span><span id="S4.T2.1.1.5.3.3" class="ltx_text"></span></td>
<td id="S4.T2.1.1.5.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.1.1.5.4.1" class="ltx_text"></span> <span id="S4.T2.1.1.5.4.2" class="ltx_text">
<span id="S4.T2.1.1.5.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.5.4.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.5.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">2.36</span></span>
<span id="S4.T2.1.1.5.4.2.1.2" class="ltx_tr">
<span id="S4.T2.1.1.5.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(<span id="S4.T2.1.1.5.4.2.1.2.1.1" class="ltx_text" style="color:#008080;">-1.48</span>)</span></span>
</span></span><span id="S4.T2.1.1.5.4.3" class="ltx_text"></span></td>
<td id="S4.T2.1.1.5.5" class="ltx_td ltx_align_center ltx_border_t">
<span id="S4.T2.1.1.5.5.1" class="ltx_text"></span> <span id="S4.T2.1.1.5.5.2" class="ltx_text">
<span id="S4.T2.1.1.5.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.5.5.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.5.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">5.84</span></span>
<span id="S4.T2.1.1.5.5.2.1.2" class="ltx_tr">
<span id="S4.T2.1.1.5.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(<span id="S4.T2.1.1.5.5.2.1.2.1.1" class="ltx_text" style="color:#008080;">-6.44</span>)</span></span>
</span></span><span id="S4.T2.1.1.5.5.3" class="ltx_text"></span></td>
</tr>
<tr id="S4.T2.1.1.6" class="ltx_tr">
<td id="S4.T2.1.1.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">
<span id="S4.T2.1.1.6.1.1" class="ltx_text"></span><span id="S4.T2.1.1.6.1.2" class="ltx_text">
<span id="S4.T2.1.1.6.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.6.1.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.6.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Â Â Â Â Â +Â noisy embedding</span></span>
<span id="S4.T2.1.1.6.1.2.1.2" class="ltx_tr">
<span id="S4.T2.1.1.6.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Â Â Â Â Â Â Â Â training</span></span>
</span></span><span id="S4.T2.1.1.6.1.3" class="ltx_text"></span></td>
<td id="S4.T2.1.1.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S4.T2.1.1.6.2.1" class="ltx_text"></span> <span id="S4.T2.1.1.6.2.2" class="ltx_text">
<span id="S4.T2.1.1.6.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.6.2.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.6.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">2.19</span></span>
<span id="S4.T2.1.1.6.2.2.1.2" class="ltx_tr">
<span id="S4.T2.1.1.6.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(<span id="S4.T2.1.1.6.2.2.1.2.1.1" class="ltx_text" style="color:#008080;">-0.02</span>)</span></span>
</span></span><span id="S4.T2.1.1.6.2.3" class="ltx_text"></span></td>
<td id="S4.T2.1.1.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S4.T2.1.1.6.3.1" class="ltx_text"></span> <span id="S4.T2.1.1.6.3.2" class="ltx_text">
<span id="S4.T2.1.1.6.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.6.3.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.6.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">5.72</span></span>
<span id="S4.T2.1.1.6.3.2.1.2" class="ltx_tr">
<span id="S4.T2.1.1.6.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(<span id="S4.T2.1.1.6.3.2.1.2.1.1" class="ltx_text" style="color:#008080;">-0.11</span>)</span></span>
</span></span><span id="S4.T2.1.1.6.3.3" class="ltx_text"></span></td>
<td id="S4.T2.1.1.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S4.T2.1.1.6.4.1" class="ltx_text"></span> <span id="S4.T2.1.1.6.4.2" class="ltx_text">
<span id="S4.T2.1.1.6.4.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.6.4.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.6.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">2.4</span></span>
<span id="S4.T2.1.1.6.4.2.1.2" class="ltx_tr">
<span id="S4.T2.1.1.6.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(<span id="S4.T2.1.1.6.4.2.1.2.1.1" class="ltx_text" style="color:#FF0000;">+0.04</span>)</span></span>
</span></span><span id="S4.T2.1.1.6.4.3" class="ltx_text"></span></td>
<td id="S4.T2.1.1.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S4.T2.1.1.6.5.1" class="ltx_text"></span> <span id="S4.T2.1.1.6.5.2" class="ltx_text">
<span id="S4.T2.1.1.6.5.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.6.5.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.6.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">5.76</span></span>
<span id="S4.T2.1.1.6.5.2.1.2" class="ltx_tr">
<span id="S4.T2.1.1.6.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">(<span id="S4.T2.1.1.6.5.2.1.2.1.1" class="ltx_text" style="color:#008080;">-0.08</span>)</span></span>
</span></span><span id="S4.T2.1.1.6.5.3" class="ltx_text"></span></td>
</tr>
</table>
</span></div>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>ASR performance on LibriSpeech evaluation sets for the considered pipeline configurations.</figcaption>
<div id="S4.T3.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:142.3pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-108.2pt,37.2pt) scale(0.655520193668854,0.655520193668854) ;">
<table id="S4.T3.5.5" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.5.5.5" class="ltx_tr">
<td id="S4.T3.5.5.5.6" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T3.5.5.5.6.1" class="ltx_text">
<span id="S4.T3.5.5.5.6.1.1" class="ltx_inline-block">
<span id="S4.T3.5.5.5.6.1.1.1" class="ltx_p">Input feature</span>
</span></span></td>
<td id="S4.T3.5.5.5.7" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T3.5.5.5.7.1" class="ltx_text">Quantizer</span></td>
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T3.1.1.1.1.1" class="ltx_text"><math id="S4.T3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="f_{s}" display="inline"><semantics id="S4.T3.1.1.1.1.1.m1.1a"><msub id="S4.T3.1.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T3.1.1.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.1.1.m1.1.1.2.cmml">f</mi><mi id="S4.T3.1.1.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.1.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S4.T3.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1.2">ğ‘“</ci><ci id="S4.T3.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T3.1.1.1.1.1.m1.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.1.m1.1c">f_{s}</annotation></semantics></math> / kHz</span></td>
<td id="S4.T3.5.5.5.8" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T3.5.5.5.8.1" class="ltx_text">Bitrate / kbps</span></td>
<td id="S4.T3.5.5.5.9" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T3.5.5.5.9.1" class="ltx_text">
<span id="S4.T3.5.5.5.9.1.1" class="ltx_inline-block">
<span id="S4.T3.5.5.5.9.1.1.1" class="ltx_p">Code</span>
<span id="S4.T3.5.5.5.9.1.1.2" class="ltx_p">aggregation</span>
</span></span></td>
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T3.2.2.2.2.1" class="ltx_text"><math id="S4.T3.2.2.2.2.1.m1.1" class="ltx_Math" alttext="N_{\text{cb}}" display="inline"><semantics id="S4.T3.2.2.2.2.1.m1.1a"><msub id="S4.T3.2.2.2.2.1.m1.1.1" xref="S4.T3.2.2.2.2.1.m1.1.1.cmml"><mi id="S4.T3.2.2.2.2.1.m1.1.1.2" xref="S4.T3.2.2.2.2.1.m1.1.1.2.cmml">N</mi><mtext id="S4.T3.2.2.2.2.1.m1.1.1.3" xref="S4.T3.2.2.2.2.1.m1.1.1.3a.cmml">cb</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.1.m1.1b"><apply id="S4.T3.2.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.2.2.2.2.1.m1.1.1.1.cmml" xref="S4.T3.2.2.2.2.1.m1.1.1">subscript</csymbol><ci id="S4.T3.2.2.2.2.1.m1.1.1.2.cmml" xref="S4.T3.2.2.2.2.1.m1.1.1.2">ğ‘</ci><ci id="S4.T3.2.2.2.2.1.m1.1.1.3a.cmml" xref="S4.T3.2.2.2.2.1.m1.1.1.3"><mtext mathsize="70%" id="S4.T3.2.2.2.2.1.m1.1.1.3.cmml" xref="S4.T3.2.2.2.2.1.m1.1.1.3">cb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.1.m1.1c">N_{\text{cb}}</annotation></semantics></math></span></td>
<td id="S4.T3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T3.3.3.3.3.1" class="ltx_text"><math id="S4.T3.3.3.3.3.1.m1.1" class="ltx_Math" alttext="D_{\text{cb}}" display="inline"><semantics id="S4.T3.3.3.3.3.1.m1.1a"><msub id="S4.T3.3.3.3.3.1.m1.1.1" xref="S4.T3.3.3.3.3.1.m1.1.1.cmml"><mi id="S4.T3.3.3.3.3.1.m1.1.1.2" xref="S4.T3.3.3.3.3.1.m1.1.1.2.cmml">D</mi><mtext id="S4.T3.3.3.3.3.1.m1.1.1.3" xref="S4.T3.3.3.3.3.1.m1.1.1.3a.cmml">cb</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.3.1.m1.1b"><apply id="S4.T3.3.3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.3.3.3.3.1.m1.1.1.1.cmml" xref="S4.T3.3.3.3.3.1.m1.1.1">subscript</csymbol><ci id="S4.T3.3.3.3.3.1.m1.1.1.2.cmml" xref="S4.T3.3.3.3.3.1.m1.1.1.2">ğ·</ci><ci id="S4.T3.3.3.3.3.1.m1.1.1.3a.cmml" xref="S4.T3.3.3.3.3.1.m1.1.1.3"><mtext mathsize="70%" id="S4.T3.3.3.3.3.1.m1.1.1.3.cmml" xref="S4.T3.3.3.3.3.1.m1.1.1.3">cb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.3.1.m1.1c">D_{\text{cb}}</annotation></semantics></math></span></td>
<td id="S4.T3.4.4.4.4" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T3.4.4.4.4.1" class="ltx_text"><math id="S4.T3.4.4.4.4.1.m1.1" class="ltx_Math" alttext="D_{\text{enc}}" display="inline"><semantics id="S4.T3.4.4.4.4.1.m1.1a"><msub id="S4.T3.4.4.4.4.1.m1.1.1" xref="S4.T3.4.4.4.4.1.m1.1.1.cmml"><mi id="S4.T3.4.4.4.4.1.m1.1.1.2" xref="S4.T3.4.4.4.4.1.m1.1.1.2.cmml">D</mi><mtext id="S4.T3.4.4.4.4.1.m1.1.1.3" xref="S4.T3.4.4.4.4.1.m1.1.1.3a.cmml">enc</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.4.1.m1.1b"><apply id="S4.T3.4.4.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.4.4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.T3.4.4.4.4.1.m1.1.1.1.cmml" xref="S4.T3.4.4.4.4.1.m1.1.1">subscript</csymbol><ci id="S4.T3.4.4.4.4.1.m1.1.1.2.cmml" xref="S4.T3.4.4.4.4.1.m1.1.1.2">ğ·</ci><ci id="S4.T3.4.4.4.4.1.m1.1.1.3a.cmml" xref="S4.T3.4.4.4.4.1.m1.1.1.3"><mtext mathsize="70%" id="S4.T3.4.4.4.4.1.m1.1.1.3.cmml" xref="S4.T3.4.4.4.4.1.m1.1.1.3">enc</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.4.1.m1.1c">D_{\text{enc}}</annotation></semantics></math></span></td>
<td id="S4.T3.5.5.5.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="4">WER / % <math id="S4.T3.5.5.5.5.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T3.5.5.5.5.m1.1a"><mo stretchy="false" id="S4.T3.5.5.5.5.m1.1.1" xref="S4.T3.5.5.5.5.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.5.m1.1b"><ci id="S4.T3.5.5.5.5.m1.1.1.cmml" xref="S4.T3.5.5.5.5.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.5.m1.1c">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T3.5.5.6" class="ltx_tr">
<td id="S4.T3.5.5.6.1" class="ltx_td ltx_align_center ltx_border_t">dev clean</td>
<td id="S4.T3.5.5.6.2" class="ltx_td ltx_align_center ltx_border_t">dev-other</td>
<td id="S4.T3.5.5.6.3" class="ltx_td ltx_align_center ltx_border_t">test clean</td>
<td id="S4.T3.5.5.6.4" class="ltx_td ltx_align_center ltx_border_t">test other</td>
</tr>
<tr id="S4.T3.5.5.7" class="ltx_tr">
<td id="S4.T3.5.5.7.1" class="ltx_td ltx_align_center ltx_border_t">mel-spectrogram</td>
<td id="S4.T3.5.5.7.2" class="ltx_td ltx_align_center ltx_border_t">â€“</td>
<td id="S4.T3.5.5.7.3" class="ltx_td ltx_align_center ltx_border_t">â€“</td>
<td id="S4.T3.5.5.7.4" class="ltx_td ltx_align_center ltx_border_t">â€“</td>
<td id="S4.T3.5.5.7.5" class="ltx_td ltx_align_center ltx_border_t">â€“</td>
<td id="S4.T3.5.5.7.6" class="ltx_td ltx_align_center ltx_border_t">â€“</td>
<td id="S4.T3.5.5.7.7" class="ltx_td ltx_align_center ltx_border_t">â€“</td>
<td id="S4.T3.5.5.7.8" class="ltx_td ltx_align_center ltx_border_t">â€“</td>
<td id="S4.T3.5.5.7.9" class="ltx_td ltx_align_center ltx_border_t">2.12</td>
<td id="S4.T3.5.5.7.10" class="ltx_td ltx_align_center ltx_border_t">4.88</td>
<td id="S4.T3.5.5.7.11" class="ltx_td ltx_align_center ltx_border_t">2.27</td>
<td id="S4.T3.5.5.7.12" class="ltx_td ltx_align_center ltx_border_t">5.03</td>
</tr>
<tr id="S4.T3.5.5.8" class="ltx_tr">
<td id="S4.T3.5.5.8.1" class="ltx_td ltx_align_center ltx_border_t">EnCodec</td>
<td id="S4.T3.5.5.8.2" class="ltx_td ltx_align_center ltx_border_t">RVQ</td>
<td id="S4.T3.5.5.8.3" class="ltx_td ltx_align_center ltx_border_t">24</td>
<td id="S4.T3.5.5.8.4" class="ltx_td ltx_align_center ltx_border_t">24</td>
<td id="S4.T3.5.5.8.5" class="ltx_td ltx_align_center ltx_border_t">avg</td>
<td id="S4.T3.5.5.8.6" class="ltx_td ltx_align_center ltx_border_t">32</td>
<td id="S4.T3.5.5.8.7" class="ltx_td ltx_align_center ltx_border_t">1024</td>
<td id="S4.T3.5.5.8.8" class="ltx_td ltx_align_center ltx_border_t">128</td>
<td id="S4.T3.5.5.8.9" class="ltx_td ltx_align_center ltx_border_t">2.16</td>
<td id="S4.T3.5.5.8.10" class="ltx_td ltx_align_center ltx_border_t">5.68</td>
<td id="S4.T3.5.5.8.11" class="ltx_td ltx_align_center ltx_border_t">2.3</td>
<td id="S4.T3.5.5.8.12" class="ltx_td ltx_align_center ltx_border_t">5.47</td>
</tr>
<tr id="S4.T3.5.5.9" class="ltx_tr">
<td id="S4.T3.5.5.9.1" class="ltx_td ltx_align_center">EnCodec</td>
<td id="S4.T3.5.5.9.2" class="ltx_td ltx_align_center">RVQ</td>
<td id="S4.T3.5.5.9.3" class="ltx_td ltx_align_center">24</td>
<td id="S4.T3.5.5.9.4" class="ltx_td ltx_align_center">12</td>
<td id="S4.T3.5.5.9.5" class="ltx_td ltx_align_center">avg</td>
<td id="S4.T3.5.5.9.6" class="ltx_td ltx_align_center">16</td>
<td id="S4.T3.5.5.9.7" class="ltx_td ltx_align_center">1024</td>
<td id="S4.T3.5.5.9.8" class="ltx_td ltx_align_center">128</td>
<td id="S4.T3.5.5.9.9" class="ltx_td ltx_align_center">2.26</td>
<td id="S4.T3.5.5.9.10" class="ltx_td ltx_align_center">5.77</td>
<td id="S4.T3.5.5.9.11" class="ltx_td ltx_align_center">2.45</td>
<td id="S4.T3.5.5.9.12" class="ltx_td ltx_align_center">5.8</td>
</tr>
<tr id="S4.T3.5.5.10" class="ltx_tr">
<td id="S4.T3.5.5.10.1" class="ltx_td ltx_align_center">EnCodec</td>
<td id="S4.T3.5.5.10.2" class="ltx_td ltx_align_center">RVQ</td>
<td id="S4.T3.5.5.10.3" class="ltx_td ltx_align_center">24</td>
<td id="S4.T3.5.5.10.4" class="ltx_td ltx_align_center">6</td>
<td id="S4.T3.5.5.10.5" class="ltx_td ltx_align_center">avg</td>
<td id="S4.T3.5.5.10.6" class="ltx_td ltx_align_center">8</td>
<td id="S4.T3.5.5.10.7" class="ltx_td ltx_align_center">1024</td>
<td id="S4.T3.5.5.10.8" class="ltx_td ltx_align_center">128</td>
<td id="S4.T3.5.5.10.9" class="ltx_td ltx_align_center">2.23</td>
<td id="S4.T3.5.5.10.10" class="ltx_td ltx_align_center">6.02</td>
<td id="S4.T3.5.5.10.11" class="ltx_td ltx_align_center">2.35</td>
<td id="S4.T3.5.5.10.12" class="ltx_td ltx_align_center">5.96</td>
</tr>
<tr id="S4.T3.5.5.11" class="ltx_tr">
<td id="S4.T3.5.5.11.1" class="ltx_td ltx_align_center">EnCodec</td>
<td id="S4.T3.5.5.11.2" class="ltx_td ltx_align_center">RVQ</td>
<td id="S4.T3.5.5.11.3" class="ltx_td ltx_align_center">24</td>
<td id="S4.T3.5.5.11.4" class="ltx_td ltx_align_center">3</td>
<td id="S4.T3.5.5.11.5" class="ltx_td ltx_align_center">avg</td>
<td id="S4.T3.5.5.11.6" class="ltx_td ltx_align_center">4</td>
<td id="S4.T3.5.5.11.7" class="ltx_td ltx_align_center">1024</td>
<td id="S4.T3.5.5.11.8" class="ltx_td ltx_align_center">128</td>
<td id="S4.T3.5.5.11.9" class="ltx_td ltx_align_center">2.44</td>
<td id="S4.T3.5.5.11.10" class="ltx_td ltx_align_center">7.13</td>
<td id="S4.T3.5.5.11.11" class="ltx_td ltx_align_center">2.6</td>
<td id="S4.T3.5.5.11.12" class="ltx_td ltx_align_center">7.13</td>
</tr>
<tr id="S4.T3.5.5.12" class="ltx_tr">
<td id="S4.T3.5.5.12.1" class="ltx_td ltx_align_center ltx_border_t">TD-NAC</td>
<td id="S4.T3.5.5.12.2" class="ltx_td ltx_align_center ltx_border_t">RVQ</td>
<td id="S4.T3.5.5.12.3" class="ltx_td ltx_align_center ltx_border_t">16</td>
<td id="S4.T3.5.5.12.4" class="ltx_td ltx_align_center ltx_border_t">6.4</td>
<td id="S4.T3.5.5.12.5" class="ltx_td ltx_align_center ltx_border_t">stack</td>
<td id="S4.T3.5.5.12.6" class="ltx_td ltx_align_center ltx_border_t">8</td>
<td id="S4.T3.5.5.12.7" class="ltx_td ltx_align_center ltx_border_t">1024</td>
<td id="S4.T3.5.5.12.8" class="ltx_td ltx_align_center ltx_border_t">128</td>
<td id="S4.T3.5.5.12.9" class="ltx_td ltx_align_center ltx_border_t">3.12</td>
<td id="S4.T3.5.5.12.10" class="ltx_td ltx_align_center ltx_border_t">10.17</td>
<td id="S4.T3.5.5.12.11" class="ltx_td ltx_align_center ltx_border_t">3.38</td>
<td id="S4.T3.5.5.12.12" class="ltx_td ltx_align_center ltx_border_t">10.17</td>
</tr>
<tr id="S4.T3.5.5.13" class="ltx_tr">
<td id="S4.T3.5.5.13.1" class="ltx_td ltx_align_center">TD-NAC</td>
<td id="S4.T3.5.5.13.2" class="ltx_td ltx_align_center">RVQ</td>
<td id="S4.T3.5.5.13.3" class="ltx_td ltx_align_center">16</td>
<td id="S4.T3.5.5.13.4" class="ltx_td ltx_align_center">6.4</td>
<td id="S4.T3.5.5.13.5" class="ltx_td ltx_align_center">avg</td>
<td id="S4.T3.5.5.13.6" class="ltx_td ltx_align_center">8</td>
<td id="S4.T3.5.5.13.7" class="ltx_td ltx_align_center">1024</td>
<td id="S4.T3.5.5.13.8" class="ltx_td ltx_align_center">128</td>
<td id="S4.T3.5.5.13.9" class="ltx_td ltx_align_center">2.19</td>
<td id="S4.T3.5.5.13.10" class="ltx_td ltx_align_center"><span id="S4.T3.5.5.13.10.1" class="ltx_text ltx_font_bold">5.72</span></td>
<td id="S4.T3.5.5.13.11" class="ltx_td ltx_align_center"><span id="S4.T3.5.5.13.11.1" class="ltx_text ltx_font_bold">2.40</span></td>
<td id="S4.T3.5.5.13.12" class="ltx_td ltx_align_center"><span id="S4.T3.5.5.13.12.1" class="ltx_text ltx_font_bold">5.76</span></td>
</tr>
<tr id="S4.T3.5.5.14" class="ltx_tr">
<td id="S4.T3.5.5.14.1" class="ltx_td ltx_align_center">TD-NAC</td>
<td id="S4.T3.5.5.14.2" class="ltx_td ltx_align_center">FSQ</td>
<td id="S4.T3.5.5.14.3" class="ltx_td ltx_align_center">16</td>
<td id="S4.T3.5.5.14.4" class="ltx_td ltx_align_center">6.4</td>
<td id="S4.T3.5.5.14.5" class="ltx_td ltx_align_center">stack</td>
<td id="S4.T3.5.5.14.6" class="ltx_td ltx_align_center">8</td>
<td id="S4.T3.5.5.14.7" class="ltx_td ltx_align_center">1000</td>
<td id="S4.T3.5.5.14.8" class="ltx_td ltx_align_center">32</td>
<td id="S4.T3.5.5.14.9" class="ltx_td ltx_align_center"><span id="S4.T3.5.5.14.9.1" class="ltx_text ltx_font_bold">2.18</span></td>
<td id="S4.T3.5.5.14.10" class="ltx_td ltx_align_center">6.08</td>
<td id="S4.T3.5.5.14.11" class="ltx_td ltx_align_center">2.42</td>
<td id="S4.T3.5.5.14.12" class="ltx_td ltx_align_center">5.92</td>
</tr>
<tr id="S4.T3.5.5.15" class="ltx_tr">
<td id="S4.T3.5.5.15.1" class="ltx_td ltx_align_center ltx_border_t">Mel-NAC</td>
<td id="S4.T3.5.5.15.2" class="ltx_td ltx_align_center ltx_border_t">RVQ</td>
<td id="S4.T3.5.5.15.3" class="ltx_td ltx_align_center ltx_border_t">16</td>
<td id="S4.T3.5.5.15.4" class="ltx_td ltx_align_center ltx_border_t">5</td>
<td id="S4.T3.5.5.15.5" class="ltx_td ltx_align_center ltx_border_t">avg</td>
<td id="S4.T3.5.5.15.6" class="ltx_td ltx_align_center ltx_border_t">8</td>
<td id="S4.T3.5.5.15.7" class="ltx_td ltx_align_center ltx_border_t">1024</td>
<td id="S4.T3.5.5.15.8" class="ltx_td ltx_align_center ltx_border_t">128</td>
<td id="S4.T3.5.5.15.9" class="ltx_td ltx_align_center ltx_border_t">2.23</td>
<td id="S4.T3.5.5.15.10" class="ltx_td ltx_align_center ltx_border_t">5.92</td>
<td id="S4.T3.5.5.15.11" class="ltx_td ltx_align_center ltx_border_t">2.40</td>
<td id="S4.T3.5.5.15.12" class="ltx_td ltx_align_center ltx_border_t">5.80</td>
</tr>
<tr id="S4.T3.5.5.16" class="ltx_tr">
<td id="S4.T3.5.5.16.1" class="ltx_td ltx_align_center ltx_border_bb">Mel-NAC</td>
<td id="S4.T3.5.5.16.2" class="ltx_td ltx_align_center ltx_border_bb">FSQ</td>
<td id="S4.T3.5.5.16.3" class="ltx_td ltx_align_center ltx_border_bb">16</td>
<td id="S4.T3.5.5.16.4" class="ltx_td ltx_align_center ltx_border_bb">5</td>
<td id="S4.T3.5.5.16.5" class="ltx_td ltx_align_center ltx_border_bb">stack</td>
<td id="S4.T3.5.5.16.6" class="ltx_td ltx_align_center ltx_border_bb">8</td>
<td id="S4.T3.5.5.16.7" class="ltx_td ltx_align_center ltx_border_bb">1000</td>
<td id="S4.T3.5.5.16.8" class="ltx_td ltx_align_center ltx_border_bb">32</td>
<td id="S4.T3.5.5.16.9" class="ltx_td ltx_align_center ltx_border_bb">2.33</td>
<td id="S4.T3.5.5.16.10" class="ltx_td ltx_align_center ltx_border_bb">6.18</td>
<td id="S4.T3.5.5.16.11" class="ltx_td ltx_align_center ltx_border_bb">2.45</td>
<td id="S4.T3.5.5.16.12" class="ltx_td ltx_align_center ltx_border_bb">6.09</td>
</tr>
</table>
</span></div>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Codebook initialization, spectrogram augmentation and noisy embedding training</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To investigate these components' effects, we first train a TD-NAC model with RVQ following the specifications outlined in SectionÂ <a href="#S3.SS1" title="3.1 NAC model training â€£ 3 Experimental setup â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. Utilizing features from this audio codec as input, we establish our baseline ASR pipeline, employing parameters detailed in SectionÂ <a href="#S3.SS2" title="3.2 ASR model training â€£ 3 Experimental setup â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, yielding the baseline performance noted in the first row of TableÂ <a href="#S4.T2" title="Table 2 â€£ 4 Results and discussion â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
Subsequently, we adapt the ASR pipeline to initialize the embedding layer with codebooks learned from the trained NAC (SectionÂ <a href="#S2.SS2.SSS1" title="2.2.1 Embedding layer and codebook initialization â€£ 2.2 Speech recognition pipeline â€£ 2 Speech recognition with audio codecs â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.1</span></a>), maintaining other pipeline components unchanged. With this setup, we train another ASR system and report it's performance in the second row of TableÂ <a href="#S4.T2" title="Table 2 â€£ 4 Results and discussion â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Likewise, we progressively integrate spectrogram augmentation and noisy embedding training into the pipeline. Notably, codebook initialization of the embedding layer significantly enhances the ASR model's performance, with more than 10% absolute WER improvement across all the evaluation sets. Spectrogram Augmentation aids in enhancing the model's noise robustness, as reflected by more than 6% absolute WER improvement on the noisy 'other' sets. Noisy embedding training is able to even further improve this noise robustness of the model. Consequently, for all subsequent experiments, we incorporate all three components - codebook initialization, spectrogram augmentation, and noisy embedding training - into the training pipeline.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Code aggregation strategy</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.3" class="ltx_p">To assess the influence of the code aggregation strategy on the ASR+NAC model pipeline, we build up on the baseline setting as motivated in SectionÂ <a href="#S4.SS1" title="4.1 Codebook initialization, spectrogram augmentation and noisy embedding training â€£ 4 Results and discussion â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>: TD-NAC model with RVQ, FastConformer-RNNT ASR model with embedding layer initialized with the learnt codebooks, SpecAug, and noisy embedding training. Two models are trained: one utilizing stacking for aggregating code embeddings and the other employing averaging (refer to SectionÂ <a href="#S2.SS2.SSS2" title="2.2.2 Code aggregation strategies â€£ 2.2 Speech recognition pipeline â€£ 2 Speech recognition with audio codecs â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.2</span></a>). The performance of these models are reported in rows <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn type="integer" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">6</annotation></semantics></math> and <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mn id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><cn type="integer" id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">7</annotation></semantics></math> of TableÂ <a href="#S4.T3" title="Table 3 â€£ 4 Results and discussion â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Notably, the averaging strategy yields significantly superior WER performance compared to stacking. It's worth noting that the embedding dimension <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="D_{\text{emb}}" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><msub id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mi id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml">D</mi><mtext id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3a.cmml">emb</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2">ğ·</ci><ci id="S4.SS2.p1.3.m3.1.1.3a.cmml" xref="S4.SS2.p1.3.m3.1.1.3"><mtext mathsize="70%" id="S4.SS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3">emb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">D_{\text{emb}}</annotation></semantics></math> (as discussed in SectionÂ <a href="#S2.SS2.SSS1" title="2.2.1 Embedding layer and codebook initialization â€£ 2.2 Speech recognition pipeline â€£ 2 Speech recognition with audio codecs â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.1</span></a>) remained fixed at 128 for both runs and the results might change with an increase in the embedding dimension. However, to ensure a fair comparison and assess the optimal configuration within the described setup, the embedding dimension was kept constant.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Despite the noted performance, stack remains the preferred aggregation scheme for all our NAC-FSQ systems. This choice is informed by the realization that different FSQ codebooks quantize distinct segments of the encoder output, whereas the RVQ codebooks encode residuals of the same vector.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Neural audio codec type</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We proceed to examine and compare TD-NAC with Mel-NAC, assessing their influence on downstream ASR tasks. Owing to the distinct down-sampling structures and rates outlined in SectionÂ <a href="#S2.SS1" title="2.1 Audio codecs â€£ 2 Speech recognition with audio codecs â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>, the compared TD-NAC operates at a bit-rate of 6.4â€‰kbps, whereas Mel-NAC operates at 5â€‰kbps.
The remainder of the ASR pipeline remains constant, incorporating insights from SectionÂ <a href="#S4.SS2" title="4.2 Code aggregation strategy â€£ 4 Results and discussion â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>, and we compare both RVQ and FSQ versions of the codecs. The results of these ablations are presented in the last four rows of TableÂ <a href="#S4.T3" title="Table 3 â€£ 4 Results and discussion â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Notably, TD-NAC demonstrates slightly better performance compared to Mel-NAC across all considered ASR eval sets. This finding is intriguing, given that Mel-NAC outperforms TD-NAC for TTS tasksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.
Hence, the selection of the NAC should consider the downstream task.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Furthermore, we observe that the presented TD-NAC with RVQ and only 8 codebooks outperforms Encodec with 4, 8, and even 16 codebooks, while maintaining all other parameters such as codebook size and ASR system parameter counts constant. The performance of the TD-NAC system with a bit-rate of only 6.4â€‰kbps closely matches that of Encodec with 24â€‰kbps (utilizing all 32 codebooks). We have open-sourced the weights (<span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_italic">audio_ codec_16khz_small</span>) and code<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/NVIDIA/NeMo/blob/main/tutorials/tts/Audio_Codec_Training.ipynb" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/NVIDIA/NeMo/blob/main/tutorials/tts/Audio_Codec_Training.ipynb</a></span></span></span> for this codec model so that it can be utilized by and be built upon by the community.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Quantization schemes</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Finally, we study the effect of quantization schemes on downstream ASR performance. Analysis of the last four rows of TableÂ <a href="#S4.T3" title="Table 3 â€£ 4 Results and discussion â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> reveals that FSQ detrimentally affects ASR performance, particularly on the noisy 'other' sets. We hypothesize this happens because of the fixed finite level encoding scheme utilized by FSQ, which poses challenges in modeling noisy data.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Multilingual extension</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">To demonstrate the generalization ability of the presented NAC+ASR pipeline, we performed a study using additional languages and broader corpora.
To this end, we participated in the ASR track of the Interspeech 2024 Speech Processing Using Discrete Speech Unit ChallengeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> that uses the ML-SUPERBÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> dataset comprising of 143 languages.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Model and data description</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Our pipeline uses TD-NAC model with RVQ, as detailed in SectionÂ <a href="#S3.SS1" title="3.1 NAC model training â€£ 3 Experimental setup â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, that obtained the best performance in the experiments summarized in TableÂ <a href="#S4.T3" title="Table 3 â€£ 4 Results and discussion â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
The NAC model was not retrained and we use the same setup as in SectionÂ <a href="#S3.SS1" title="3.1 NAC model training â€£ 3 Experimental setup â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
For ASR we use the FastConformer-RNNT model described in SectionÂ <a href="#S3.SS2" title="3.2 ASR model training â€£ 3 Experimental setup â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> along with avg code aggregation strategy, codebook initialization of the embedding layer, SpecAug and noisy embedding training, based on SectionÂ <a href="#S4" title="4 Results and discussion â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
As per the challenge requirements, the ASR model is trained on the LibriSpeech-clean-100 subset (100 hrs) along with the <span id="S5.SS1.p1.1.1" class="ltx_text">ML-SUPERB</span> 1h set (222 hrs) which contains data from 143 languages.
The combined data has 6280 unique characters.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Results</h3>

<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>CER on the ML-SUPERB 1h test set.</figcaption>
<div id="S5.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:303.5pt;height:54.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(50.9pt,-9.1pt) scale(1.50408251483549,1.50408251483549) ;">
<table id="S5.T4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<td id="S5.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">System</td>
<td id="S5.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Challenge baseline</td>
<td id="S5.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Our system</td>
</tr>
<tr id="S5.T4.1.1.2" class="ltx_tr">
<td id="S5.T4.1.1.2.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">CER</td>
<td id="S5.T4.1.1.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">72.6</td>
<td id="S5.T4.1.1.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">21.0</td>
</tr>
</table>
</span></div>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We compare the performance of our NAC+ASR pipeline with the baseline systemÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> on the <span id="S5.SS2.p1.1.1" class="ltx_text">ML-SUPERB</span> 1h test set which consists of 45 hours of unseen speech.
TableÂ <a href="#S5.T4" title="Table 4 â€£ 5.2 Results â€£ 5 Multilingual extension â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the Character Error Rate (CER) metric for both systems.
It can be observed that our system with 21% CER significantly outperforms the challenge baseline.
Moreover, our system surpasses the SOTA performance achieved by the XLSR-128 model, which reported a CER of 22%Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, despite being smaller in size and pretrained on significantly less data.
This competitive CER underscores the effectiveness of the proposed NAC+ASR pipeline not only in monolingual scenarios (cf. TableÂ <a href="#S4.T3" title="Table 3 â€£ 4 Results and discussion â€£ Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) but also in multilingual settings encompassing over 100 languages.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work, we presented a speech recognition pipeline working on discrete codes from an audio codec and performed a study of different components of the system.
We trained neural audio codecs with different quantizers and found that time-domain codec with RVQ resulted in the best performance on the considered data.
We investigated ASR pipeline optimizations and found that optimal code aggregation and codebook initialization resulted in large performance improvements.
Furthermore, we found that SpecAug and noisy embedding training in our pipeline lead to improved performance in clean conditions and superior robustness in noisy conditions.
Our best result outperforms EnCodec-based model at a comparable bit-rate.
Finally, we studied the performance on a large multi-lingual dataset.
The proposed model beats the SOTA performance of strong self-supervised models like XLSR-128 on the 143-language ML-SUPERB benchmark despite being smaller and trained on significantly less data.
All the trained NAC and ASR models along with accompanying code will be released in the NeMo toolkitÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
G.Â Hinton <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,'' <em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic">IEEE Signal Process. Mag.</em>, vol.Â 29, no.Â 6, pp. 82â€“97, 2012.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
W.Â Chan <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Speechstew: Simply mix all available speech recognition data to train one large neural network,'' <em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.02133</em>, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
T.Â J. Park <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``A review of speaker diarization: Recent advances with deep learning,'' <em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">Computer Speech &amp; Language</em>, vol.Â 72, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Q.Â Zhang <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss,'' in <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Proc. (ICASSP)</em>, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A.Â Gulati <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Conformer: Convolution-augmented transformer for speech recognition,'' in <em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
C.Â Wang <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Neural codec language models are zero-shot text to speech synthesizers,'' <em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.02111</em>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
X.Â Wang <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``SpeechX: Neural codec language model as a versatile speech transformer,'' <em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.06873</em>, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
T.Â N. Sainath <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Multichannel signal processing with deep neural networks for automatic speech recognition,'' <em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.Â 25, no.Â 5, pp. 965â€“979, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Y.Â Luo and N.Â Mesgarani, ``Tasnet: time-domain audio separation network for real-time, single-channel speech separation,'' in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Proc. (ICASSP)</em>, 2018, pp. 696â€“700.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M.Â Won <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Data-driven harmonic filters for audio representation learning,'' in <em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Proc. (ICASSP)</em>, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
N.Â Zeghidour <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``LEAF: A learnable frontend for audio classification,'' in <em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic">Proc. Int. Conf. Learning Representations (ICLR)</em>, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
G.Â Synnaeve <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``End-to-end ASR: from supervised to semi-supervised learning with modern architectures,'' in <em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic">Proc. ICML Workshop on Self-supervision in Audio and Speech</em>, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
R.Â Prabhavalkar <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``End-to-end speech recognition: A survey,'' <em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
K.Â C. Puvvada <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Discrete audio representation as an alternative to mel-spectrograms for speaker and speech recognition,'' in <em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Proc. (ICASSP)</em>, 2024.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
X.Â Chang <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Exploration of efficient end-to-end asr using discretized input from self-supervised learning,'' <em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.18108</em>, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
X.Â Chang, B.Â Yan <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Exploring speech recognition, translation, and understanding with discrete speech units: A comparative study,'' <em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.15800</em>, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
W.-N. Hsu <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``HuBERT: Self-supervised speech representation learning by masked prediction of hidden units,'' <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.Â 29, pp. 3451â€“3460, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Y.-A. Chung <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``w2v-BERT: Combining contrastive learning and masked language modeling for self-supervised speech pre-training,'' in <em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic">Proc. IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>, 2021, pp. 244â€“250.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
S.Â Chen <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``WavLM: Large-scale self-supervised pre-training for full stack speech processing,'' <em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, vol.Â 16, no.Â 6, pp. 1505â€“1518, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Z.Â Huang, C.Â Meng, and T.Â Ko, ``Repcodec: A speech representation codec for speech tokenization,'' <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2309.00169</em>, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
N.Â Zeghidour <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Soundstream: An end-to-end neural audio codec,'' <em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.Â 30, pp. 495â€“507, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A.Â DÃ©fossez <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``High fidelity neural audio compression,'' <em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
R.Â Kumar <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``High-fidelity audio compression with improved RVQGAN,'' in <em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic">Proc. Conf. on Neural Information Process. Systems (NeurIPS)</em>, 2023.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Y.-C. Wu <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``AudioDec: An open-source streaming high-fidelity neural audio codec,'' in <em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic">Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
X.Â Zhang <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Speechtokenizer: Unified speech tokenizer for speech large language models,'' <em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2308.16692</em>, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Z.Â Borsos <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``SoundStorm: Efficient parallel audio generation,'' <em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.09636</em>, 2023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J.Â Shi, D.Â Berrebbi, W.Â Chen, H.Â L. Chung <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Ml-superb: Multilingual speech universal performance benchmark,'' in <em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic">Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</em>, vol. 2023, 2023, pp. 884â€“888.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
F.Â Mentzer <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Finite scalar quantization: VQ-VAE made simple,'' in <em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic">Proc. International Conference on Learning Representations (ICLR)</em>, 2024.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
R.Â Langman <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Spectral Codecs: Spectrogram-based audio codecs for high quality speech synthesis,'' <em id="bib.bib29.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2406.05298</em>, 2024.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J.Â Kong, J.Â Kim, and J.Â Bae, ``HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis,'' in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proc. Conf. on Neural Information Process. Systems (NeurIPS)</em>, 2020.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
D.Â S. Park <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Specaugment: A simple data augmentation method for automatic speech recognition,'' <em id="bib.bib31.2.2" class="ltx_emph ltx_font_italic">Interspeech 2019</em>, 2019.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
N.Â Jain <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``NEFTune: Noisy embeddings improve instruction finetuning,'' in <em id="bib.bib32.2.2" class="ltx_emph ltx_font_italic">Proc. International Conference on Learning Representations (ICLR)</em>, 2023.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J.Â Kahn <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Libri-Light: A benchmark for asr with limited or no supervision,'' in <em id="bib.bib33.2.2" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Proc. (ICASSP)</em>, 2020.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
D.Â Rekesh <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Fast conformer with linearly scalable attention for efficient speech recognition,'' in <em id="bib.bib34.2.2" class="ltx_emph ltx_font_italic">Proc. IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>, 2023.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
V.Â Panayotov <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``LibriSpeech: an ASR corpus based on public domain audio books,'' in <em id="bib.bib35.2.2" class="ltx_emph ltx_font_italic">Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Proc. (ICASSP)</em>.Â Â Â IEEE, 2015, pp. 5206â€“5210.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
T.Â Kudo and J.Â Richardson, ``SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,'' in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proc. Conf. on Empirical Methods in Natural Language Processing: System Demonstrations</em>, 2018.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
X.Â Chang <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Interspeech 2024 speech processing using discrete speech unit challenge,'' <a target="_blank" href="https://www.wavlab.org/activities/2024/Interspeech2024-Discrete-Speech-Unit-Challenge" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.wavlab.org/activities/2024/Interspeech2024-Discrete-Speech-Unit-Challenge</a>, [Online].

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
NVIDIA, ``NeMo: a toolkit for conversational AI,'' <a target="_blank" href="https://github.com/NVIDIA/NeMo" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/NVIDIA/NeMo</a>, [Online; accessed May, 2024].

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.03494" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.03495" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.03495">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.03495" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.03496" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 17:58:45 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
