<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.09817] Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech Recognition System</title><meta property="og:description" content="Multi-talker speech recognition and target-talker speech recognition, both involve transcription in multi-talker contexts, remain significant challenges. However, existing methods rarely attempt to simultaneously addreâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech Recognition System">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech Recognition System">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.09817">

<!--Generated on Mon Aug  5 14:50:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">Lingwei Meng,
Jiawen Kang,
Yuejiao Wang,
Zengrui Jin,
Xixin Wu,
Xunying Liu,
Helen Meng






</p>
</div>
<h1 class="ltx_title ltx_title_document">Empowering Whisper as a Joint Multi-Talker and Target-Talker
<br class="ltx_break">Speech Recognition System</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Multi-talker speech recognition and target-talker speech recognition, both involve transcription in multi-talker contexts, remain significant challenges. However, existing methods rarely attempt to simultaneously address both tasks. In this study, we propose a pioneering approach to empower Whisper, which is a speech foundation model, to tackle joint multi-talker and target-talker speech recognition tasks. Specifically, (i) we freeze Whisper and plug a Sidecar separator into its encoder to separate mixed embedding for multiple talkers; (ii) a Target Talker Identifier is introduced to identify the embedding flow of the target talker on the fly, requiring only three-second enrollment speech as a cue; (iii) soft prompt tuning for decoder is explored for better task adaptation. Our method outperforms previous methods on two- and three-talker LibriMix and LibriSpeechMix datasets for both tasks, and delivers acceptable zero-shot performance on multi-talker ASR on AishellMix Mandarin dataset.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The code will be made public before INTERSPEECH 2024 opens.</span></span></span></p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>multi-talker speech recognition, target-talker speech recognition, prompt tuning, domain adaptation
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Driven by the rapid development of deep learning along with the availability of large-scale data, automatic speech recognition (ASR) has achieved significant progress in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. However, speech recognition in the multi-talker scenarios, where overlapping may exist, remains challenging and has attracted much attention.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To tackle the multi-talker speech recognition problem, various approaches have been explored.
Conventional cascaded systems employ a speech separation module as a front-end to separate mixed speech signals, which are then fed into a single-talker ASR system for transcription <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
However, these systems usually show limited performance due to the mismatch of their optimization objectives, and may need further joint training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
Recently, end-to-end models have garnered interest owing to their outstanding performance.
One primary challenge in training an end-to-end multi-talker ASR system is to associate prediction with the corresponding target labels for loss calculation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
Consequently, techniques such as Permutation Invariant Training (PIT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, Heuristic Error Assignment Training (HEAT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, and Serialized Output Training (SOT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> have emerged.
Although these methods have yielded impressive results, they often necessitate training from scratch or performing full fine-tuning on pre-trained models, which
does not fully capitalize on the existing advancements developed for standard single-talker ASR.
Enlightened by findings that the ASR encoder captures more acoustic information in its lower layers and more linguistic information in the upper layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, a recent study advocates for the use of a Conv-TasNet-like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> Sidecar separator to tackle multi-talker speech recognition, without distorting the parameters of a well-trained
single-talker ASR model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Target-talker ASR, which aims to efficiently recognize speech of a target talker under a multi-talker scenario, also holds substantial practical value. End-to-end approaches have been investigated and achieved substantial progress <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
However, these methods typically necessitate an external <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> or internal <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> module to derive the speaker embedding from the enrollment speech of target-talker, consequently increasing the model's computational burden. Moreover, they typically only output the transcripts of an assigned target talker, neglecting the speech of other talkers. This limitation hinders their applicability in situations where users may also be interested in obtaining the transcriptions of non-target talkers.
Although speaker-attributed ASR can transcribe multiple speakers in a speaker-aware manner, it typically necessitates the speaker embeddings of all involved individuals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
As far as we know, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> is the only study attempting to address joint multi-talker and target-talker ASR; however, it still requires an external speaker embedding extractor.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Nowadays, speech foundation models have emerged as a versatile solution for diverse speech tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. As an representative in this domain, Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> has demonstrated its potential across various tasks beyond ASR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> which motivated us to further extend Whisperâ€™s capabilities in tackling multi-
talker and target-talker speech recognition challenges.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this study, we empower Whisper as a joint multi-talker and target-talker system in a parameter-efficient style. Specifically, we freeze the weights of Whisper and incorporate a Sidecar separator into its encoder to endow it with multi-talker speech recognition capabilities. A Target Talker Identifier (TTI) module is introduced to distinguish the target speaker's embedding branch on the fly, requiring only three seconds of the target talker's enrollment speech as a cue. Moreover, soft prompt tuning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> for Whisper decoder is adopted to further adapt to the tasks. Our major contributions are threefold:</p>
</div>
<div id="S1.p6" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a pioneering framework to jointly transcribing multi-talker speech while highlighting the target talker's speech, without employing any speaker embedding extractor.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;padding-top:-0.6pt;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Leveraging the frozen Whisper as the foundation model, our framework only involves limited trainable parameters, making it a parameter-efficient and loosely-coupled system.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;padding-top:-0.6pt;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Extensive experiments reveal that the proposed approach achieves leading performance on two- and three-talker LibriMix and LibriSpeechMix datasets (English) on both tasks, and attains satisfactory zero-shot multi-talker ASR performance on AishellMix (Mandarin).</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2407.09817/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="442" height="167" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Take two-talker scenario as an example, the proposed system (a) take the concatenation of target enrollment speech and multi-talker speech as input. The embedding is separated by Sidecar separator. Target Talker Identifier (b) processes the prefix segments of encoder embeddings to identify the target talker branch. Optionally, non-target branchs can be discarded to accelerate inference.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The proposed method consists of four main components â€” Whisper serving as the foundation model, a Sidecar separator to separate mixed embedding for multiple talkers, a Target Talker Identifier to identify the embedding flow of the target talker, and a soft prompt embedding to facilitate task adaptation.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Whisper as the Speech Foundation Model</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Whisper is a speech recognition model featuring an attention-based encoder-decoder structure, which has been trained on massive amounts of web-scale labeled speech data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Nowadays, it is increasingly being utilized as a speech foundation model even beyond speech recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
In this study, we are inspired to extend its capability to handling joint multi-talker and target-talker ASR tasks.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Whisper takes log-Mel spectrogram as input, followed by transformer encoder and decoder modules to decode the output tokens in an auto-regressive manner.
Different from other ASR models, Whisper adopts several special tokens as the prefix of input sequences for decoder to specify tasks and condition information.
By default, the prefix tokens are <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_typewriter">"&lt;|PREV|&gt;</span>, <span id="S2.SS1.p2.1.2" class="ltx_text ltx_font_italic">text prompt</span>, <span id="S2.SS1.p2.1.3" class="ltx_text ltx_font_typewriter">&lt;|SOT|&gt;</span>, <span id="S2.SS1.p2.1.4" class="ltx_text ltx_font_typewriter">&lt;|LANGUAGE|&gt;</span>, <span id="S2.SS1.p2.1.5" class="ltx_text ltx_font_typewriter">&lt;|TRANSCRIBE|&gt;</span>, <span id="S2.SS1.p2.1.6" class="ltx_text ltx_font_typewriter">&lt;|NO_TIMESTAMP|&gt;"</span>, where <span id="S2.SS1.p2.1.7" class="ltx_text ltx_font_typewriter">&lt;|PREV|&gt;</span> and <span id="S2.SS1.p2.1.8" class="ltx_text ltx_font_italic">text prompt</span> are optional.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Empowering Whisper as a Multi-Talker ASR System</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Recently, the Sidecar separator (SS) has been introduced as a parameter-efficient module to convert a well-trained single-talker ASR model into a multi-talker one <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.
In this work, we incorporated the Sidecar separator with Whisper to harness its capability acquired from extensive training data.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The Sidecar separator is a temporal convolutional network inserted between the early layers of the ASR encoders.
It consists of stacked 1-D dilated convolutional blocks inspired by Conv-TasNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
As the shallower layers of the ASR encoder are believed to encode more acoustic information rather than the linguistic ones<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, the Sidecar separator is able to separate the mixed representation with talker-related masks, producing disentangled representation of different speakers.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">As depicted in Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the Sidecar separator accompanied by two 1-D convolutional layers is positioned after the second encoder block.
Talker-dependent masks are generated, which are element-wisely multiplied with the mixed embedding, yielding separated embeddings of each talker.
The subsequent encoder blocks and decoder process these branches, ultimately transcribing the corresponding text for each talker.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Target Talker Identifier</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">We introduce the Target Talker Identifier (TTI) module, which equips the system with the capability for target-talker ASR.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">During the forward process, as illustrated in Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (b), the encoder-output embeddings corresponding to different talkers will be segmented into two distinct segments:
a prefix segment aligning with the length of three-second enrollment speech, and a main segment that corresponds to the duration of the multi-talker speech.
The prefix segments are then fed into the TTI module, which determines the branch associated with the target talker, while only the main embedding segments are sent to the Whisper decoder for transcription.
</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.8" class="ltx_p">Specifically, the prefix segments hold a tensor shape of <math id="S2.SS3.p3.1.m1.3" class="ltx_Math" alttext="(B\times S,\textit{150},C)" display="inline"><semantics id="S2.SS3.p3.1.m1.3a"><mrow id="S2.SS3.p3.1.m1.3.3.1" xref="S2.SS3.p3.1.m1.3.3.2.cmml"><mo stretchy="false" id="S2.SS3.p3.1.m1.3.3.1.2" xref="S2.SS3.p3.1.m1.3.3.2.cmml">(</mo><mrow id="S2.SS3.p3.1.m1.3.3.1.1" xref="S2.SS3.p3.1.m1.3.3.1.1.cmml"><mi id="S2.SS3.p3.1.m1.3.3.1.1.2" xref="S2.SS3.p3.1.m1.3.3.1.1.2.cmml">B</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS3.p3.1.m1.3.3.1.1.1" xref="S2.SS3.p3.1.m1.3.3.1.1.1.cmml">Ã—</mo><mi id="S2.SS3.p3.1.m1.3.3.1.1.3" xref="S2.SS3.p3.1.m1.3.3.1.1.3.cmml">S</mi></mrow><mo id="S2.SS3.p3.1.m1.3.3.1.3" xref="S2.SS3.p3.1.m1.3.3.2.cmml">,</mo><mtext class="ltx_mathvariant_italic" id="S2.SS3.p3.1.m1.1.1" xref="S2.SS3.p3.1.m1.1.1a.cmml">150</mtext><mo id="S2.SS3.p3.1.m1.3.3.1.4" xref="S2.SS3.p3.1.m1.3.3.2.cmml">,</mo><mi id="S2.SS3.p3.1.m1.2.2" xref="S2.SS3.p3.1.m1.2.2.cmml">C</mi><mo stretchy="false" id="S2.SS3.p3.1.m1.3.3.1.5" xref="S2.SS3.p3.1.m1.3.3.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.1.m1.3b"><vector id="S2.SS3.p3.1.m1.3.3.2.cmml" xref="S2.SS3.p3.1.m1.3.3.1"><apply id="S2.SS3.p3.1.m1.3.3.1.1.cmml" xref="S2.SS3.p3.1.m1.3.3.1.1"><times id="S2.SS3.p3.1.m1.3.3.1.1.1.cmml" xref="S2.SS3.p3.1.m1.3.3.1.1.1"></times><ci id="S2.SS3.p3.1.m1.3.3.1.1.2.cmml" xref="S2.SS3.p3.1.m1.3.3.1.1.2">ğµ</ci><ci id="S2.SS3.p3.1.m1.3.3.1.1.3.cmml" xref="S2.SS3.p3.1.m1.3.3.1.1.3">ğ‘†</ci></apply><ci id="S2.SS3.p3.1.m1.1.1a.cmml" xref="S2.SS3.p3.1.m1.1.1"><mtext class="ltx_mathvariant_italic" id="S2.SS3.p3.1.m1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1">150</mtext></ci><ci id="S2.SS3.p3.1.m1.2.2.cmml" xref="S2.SS3.p3.1.m1.2.2">ğ¶</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.1.m1.3c">(B\times S,\textit{150},C)</annotation></semantics></math>, where <math id="S2.SS3.p3.2.m2.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S2.SS3.p3.2.m2.1a"><mi id="S2.SS3.p3.2.m2.1.1" xref="S2.SS3.p3.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.2.m2.1b"><ci id="S2.SS3.p3.2.m2.1.1.cmml" xref="S2.SS3.p3.2.m2.1.1">ğµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.2.m2.1c">B</annotation></semantics></math> denotes batch size, <math id="S2.SS3.p3.3.m3.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.SS3.p3.3.m3.1a"><mi id="S2.SS3.p3.3.m3.1.1" xref="S2.SS3.p3.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.3.m3.1b"><ci id="S2.SS3.p3.3.m3.1.1.cmml" xref="S2.SS3.p3.3.m3.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.3.m3.1c">S</annotation></semantics></math> denotes the number of talkers, <math id="S2.SS3.p3.4.m4.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S2.SS3.p3.4.m4.1a"><mi id="S2.SS3.p3.4.m4.1.1" xref="S2.SS3.p3.4.m4.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.4.m4.1b"><ci id="S2.SS3.p3.4.m4.1.1.cmml" xref="S2.SS3.p3.4.m4.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.4.m4.1c">C</annotation></semantics></math> denotes the number of channels, and <span id="S2.SS3.p3.8.1" class="ltx_text ltx_markedasmath ltx_font_italic">150</span> denotes the number of time frames. Given that each time frame spans a duration of 20 ms, <span id="S2.SS3.p3.8.2" class="ltx_text ltx_markedasmath ltx_font_italic">150</span> coincides with the three-second duration of the enrollment speech. As shown in Figure 1, within the TTI module, the prefix segments traverse a linear layer followed by the ReLU activation function, yielding a tensor with a shape of <math id="S2.SS3.p3.7.m7.3" class="ltx_Math" alttext="(B\times S,\textit{150},\textit{1})" display="inline"><semantics id="S2.SS3.p3.7.m7.3a"><mrow id="S2.SS3.p3.7.m7.3.3.1" xref="S2.SS3.p3.7.m7.3.3.2.cmml"><mo stretchy="false" id="S2.SS3.p3.7.m7.3.3.1.2" xref="S2.SS3.p3.7.m7.3.3.2.cmml">(</mo><mrow id="S2.SS3.p3.7.m7.3.3.1.1" xref="S2.SS3.p3.7.m7.3.3.1.1.cmml"><mi id="S2.SS3.p3.7.m7.3.3.1.1.2" xref="S2.SS3.p3.7.m7.3.3.1.1.2.cmml">B</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS3.p3.7.m7.3.3.1.1.1" xref="S2.SS3.p3.7.m7.3.3.1.1.1.cmml">Ã—</mo><mi id="S2.SS3.p3.7.m7.3.3.1.1.3" xref="S2.SS3.p3.7.m7.3.3.1.1.3.cmml">S</mi></mrow><mo id="S2.SS3.p3.7.m7.3.3.1.3" xref="S2.SS3.p3.7.m7.3.3.2.cmml">,</mo><mtext class="ltx_mathvariant_italic" id="S2.SS3.p3.7.m7.1.1" xref="S2.SS3.p3.7.m7.1.1a.cmml">150</mtext><mo id="S2.SS3.p3.7.m7.3.3.1.4" xref="S2.SS3.p3.7.m7.3.3.2.cmml">,</mo><mtext class="ltx_mathvariant_italic" id="S2.SS3.p3.7.m7.2.2" xref="S2.SS3.p3.7.m7.2.2a.cmml">1</mtext><mo stretchy="false" id="S2.SS3.p3.7.m7.3.3.1.5" xref="S2.SS3.p3.7.m7.3.3.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.7.m7.3b"><vector id="S2.SS3.p3.7.m7.3.3.2.cmml" xref="S2.SS3.p3.7.m7.3.3.1"><apply id="S2.SS3.p3.7.m7.3.3.1.1.cmml" xref="S2.SS3.p3.7.m7.3.3.1.1"><times id="S2.SS3.p3.7.m7.3.3.1.1.1.cmml" xref="S2.SS3.p3.7.m7.3.3.1.1.1"></times><ci id="S2.SS3.p3.7.m7.3.3.1.1.2.cmml" xref="S2.SS3.p3.7.m7.3.3.1.1.2">ğµ</ci><ci id="S2.SS3.p3.7.m7.3.3.1.1.3.cmml" xref="S2.SS3.p3.7.m7.3.3.1.1.3">ğ‘†</ci></apply><ci id="S2.SS3.p3.7.m7.1.1a.cmml" xref="S2.SS3.p3.7.m7.1.1"><mtext class="ltx_mathvariant_italic" id="S2.SS3.p3.7.m7.1.1.cmml" xref="S2.SS3.p3.7.m7.1.1">150</mtext></ci><ci id="S2.SS3.p3.7.m7.2.2a.cmml" xref="S2.SS3.p3.7.m7.2.2"><mtext class="ltx_mathvariant_italic" id="S2.SS3.p3.7.m7.2.2.cmml" xref="S2.SS3.p3.7.m7.2.2">1</mtext></ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.7.m7.3c">(B\times S,\textit{150},\textit{1})</annotation></semantics></math>. Upon squeezing and reshaping, the tensor proceeds through another linear layer and the softmax function to produce the probability <math id="S2.SS3.p3.8.m8.2" class="ltx_Math" alttext="(B,S)" display="inline"><semantics id="S2.SS3.p3.8.m8.2a"><mrow id="S2.SS3.p3.8.m8.2.3.2" xref="S2.SS3.p3.8.m8.2.3.1.cmml"><mo stretchy="false" id="S2.SS3.p3.8.m8.2.3.2.1" xref="S2.SS3.p3.8.m8.2.3.1.cmml">(</mo><mi id="S2.SS3.p3.8.m8.1.1" xref="S2.SS3.p3.8.m8.1.1.cmml">B</mi><mo id="S2.SS3.p3.8.m8.2.3.2.2" xref="S2.SS3.p3.8.m8.2.3.1.cmml">,</mo><mi id="S2.SS3.p3.8.m8.2.2" xref="S2.SS3.p3.8.m8.2.2.cmml">S</mi><mo stretchy="false" id="S2.SS3.p3.8.m8.2.3.2.3" xref="S2.SS3.p3.8.m8.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.8.m8.2b"><interval closure="open" id="S2.SS3.p3.8.m8.2.3.1.cmml" xref="S2.SS3.p3.8.m8.2.3.2"><ci id="S2.SS3.p3.8.m8.1.1.cmml" xref="S2.SS3.p3.8.m8.1.1">ğµ</ci><ci id="S2.SS3.p3.8.m8.2.2.cmml" xref="S2.SS3.p3.8.m8.2.2">ğ‘†</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.8.m8.2c">(B,S)</annotation></semantics></math> of each branch being the target talker.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">Consequently, the target talker branch is efficiently determined on the fly, introducing minimal computational overhead. Underpinned by the superior performance of the separation module, TTI can be considered as performing target-talker activity detection, which is a more economical task compared with methods necessitating speaker embedding extraction.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Soft Prompt Tuning</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">The original Whisper model allows for the inclusion of text prompt tokens as prefix to the decoder's input sequences, conditioned on which the model yields improved ASR performance on ambiguous audios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. In this context, by exploiting this inherent characteristic of Whisper combined with soft prompt tuning technique <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, we aim to adapt the model more efficiently to multi-talker and target-talker ASR tasks.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">Specifically, as shown in Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (a), we insert a learnable embedding as soft prompt between <span id="S2.SS4.p2.1.1" class="ltx_text ltx_font_typewriter">&lt;|PREV|&gt;</span> and <span id="S2.SS4.p2.1.2" class="ltx_text ltx_font_typewriter">&lt;|SOT|&gt;</span> tokens where hard prompt tokens were originally specified. Note that we mask the position of the soft prompt when calculating the training loss, since the model does not require learning to generate them. The soft prompt embedding will be updated as the model learns to transcribe the multi-talker speech.</p>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5 </span>Training objectives</h3>

<div id="S2.SS5.p1" class="ltx_para ltx_noindent">
<p id="S2.SS5.p1.11" class="ltx_p">At each training step, there's an 80% probability of undertaking multi-talker ASR training, while a 20% probability for appending a three-second enrollment speech for joint multi-talker and target-talker ASR training.
Both ASR loss and TTI's cross-entropy loss necessitate a permutation assignment for speaker order to address the label ambiguity issue <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. In this study, the permutation is determined by Permutation Invariant Training (PIT) based on ASR loss, and is then assigned for TTI's cross-entropy loss calculation. The permutation is derived as:


</p>
<table id="S6.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E1.m1.1" class="ltx_Math" alttext="\displaystyle\hat{\pi}" display="inline"><semantics id="S2.E1.m1.1a"><mover accent="true" id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><mi id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml">Ï€</mi><mo id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><ci id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1">^</ci><ci id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2">ğœ‹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\displaystyle\hat{\pi}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E1.m2.3" class="ltx_Math" alttext="\displaystyle=\operatorname*{arg\,min}_{\pi\in\mathcal{P}}\sum_{s=1}^{S}\,\text{Loss}_{\text{ASR}}(Y^{s},R^{\pi(s)})" display="inline"><semantics id="S2.E1.m2.3a"><mrow id="S2.E1.m2.3.3" xref="S2.E1.m2.3.3.cmml"><mi id="S2.E1.m2.3.3.4" xref="S2.E1.m2.3.3.4.cmml"></mi><mo id="S2.E1.m2.3.3.3" xref="S2.E1.m2.3.3.3.cmml">=</mo><mrow id="S2.E1.m2.3.3.2" xref="S2.E1.m2.3.3.2.cmml"><munder id="S2.E1.m2.3.3.2.4" xref="S2.E1.m2.3.3.2.4.cmml"><mrow id="S2.E1.m2.3.3.2.4.2" xref="S2.E1.m2.3.3.2.4.2.cmml"><mi id="S2.E1.m2.3.3.2.4.2.2" xref="S2.E1.m2.3.3.2.4.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em" id="S2.E1.m2.3.3.2.4.2.1" xref="S2.E1.m2.3.3.2.4.2.1.cmml">â€‹</mo><mi id="S2.E1.m2.3.3.2.4.2.3" xref="S2.E1.m2.3.3.2.4.2.3.cmml">min</mi></mrow><mrow id="S2.E1.m2.3.3.2.4.3" xref="S2.E1.m2.3.3.2.4.3.cmml"><mi id="S2.E1.m2.3.3.2.4.3.2" xref="S2.E1.m2.3.3.2.4.3.2.cmml">Ï€</mi><mo id="S2.E1.m2.3.3.2.4.3.1" xref="S2.E1.m2.3.3.2.4.3.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S2.E1.m2.3.3.2.4.3.3" xref="S2.E1.m2.3.3.2.4.3.3.cmml">ğ’«</mi></mrow></munder><mo lspace="0em" rspace="0em" id="S2.E1.m2.3.3.2.3" xref="S2.E1.m2.3.3.2.3.cmml">â€‹</mo><mrow id="S2.E1.m2.3.3.2.2" xref="S2.E1.m2.3.3.2.2.cmml"><mstyle displaystyle="true" id="S2.E1.m2.3.3.2.2.3" xref="S2.E1.m2.3.3.2.2.3.cmml"><munderover id="S2.E1.m2.3.3.2.2.3a" xref="S2.E1.m2.3.3.2.2.3.cmml"><mo movablelimits="false" id="S2.E1.m2.3.3.2.2.3.2.2" xref="S2.E1.m2.3.3.2.2.3.2.2.cmml">âˆ‘</mo><mrow id="S2.E1.m2.3.3.2.2.3.2.3" xref="S2.E1.m2.3.3.2.2.3.2.3.cmml"><mi id="S2.E1.m2.3.3.2.2.3.2.3.2" xref="S2.E1.m2.3.3.2.2.3.2.3.2.cmml">s</mi><mo id="S2.E1.m2.3.3.2.2.3.2.3.1" xref="S2.E1.m2.3.3.2.2.3.2.3.1.cmml">=</mo><mn id="S2.E1.m2.3.3.2.2.3.2.3.3" xref="S2.E1.m2.3.3.2.2.3.2.3.3.cmml">1</mn></mrow><mi id="S2.E1.m2.3.3.2.2.3.3" xref="S2.E1.m2.3.3.2.2.3.3.cmml">S</mi></munderover></mstyle><mrow id="S2.E1.m2.3.3.2.2.2" xref="S2.E1.m2.3.3.2.2.2.cmml"><msub id="S2.E1.m2.3.3.2.2.2.4" xref="S2.E1.m2.3.3.2.2.2.4.cmml"><mtext id="S2.E1.m2.3.3.2.2.2.4.2" xref="S2.E1.m2.3.3.2.2.2.4.2a.cmml">Loss</mtext><mtext id="S2.E1.m2.3.3.2.2.2.4.3" xref="S2.E1.m2.3.3.2.2.2.4.3a.cmml">ASR</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E1.m2.3.3.2.2.2.3" xref="S2.E1.m2.3.3.2.2.2.3.cmml">â€‹</mo><mrow id="S2.E1.m2.3.3.2.2.2.2.2" xref="S2.E1.m2.3.3.2.2.2.2.3.cmml"><mo stretchy="false" id="S2.E1.m2.3.3.2.2.2.2.2.3" xref="S2.E1.m2.3.3.2.2.2.2.3.cmml">(</mo><msup id="S2.E1.m2.2.2.1.1.1.1.1.1" xref="S2.E1.m2.2.2.1.1.1.1.1.1.cmml"><mi id="S2.E1.m2.2.2.1.1.1.1.1.1.2" xref="S2.E1.m2.2.2.1.1.1.1.1.1.2.cmml">Y</mi><mi id="S2.E1.m2.2.2.1.1.1.1.1.1.3" xref="S2.E1.m2.2.2.1.1.1.1.1.1.3.cmml">s</mi></msup><mo id="S2.E1.m2.3.3.2.2.2.2.2.4" xref="S2.E1.m2.3.3.2.2.2.2.3.cmml">,</mo><msup id="S2.E1.m2.3.3.2.2.2.2.2.2" xref="S2.E1.m2.3.3.2.2.2.2.2.2.cmml"><mi id="S2.E1.m2.3.3.2.2.2.2.2.2.2" xref="S2.E1.m2.3.3.2.2.2.2.2.2.2.cmml">R</mi><mrow id="S2.E1.m2.1.1.1" xref="S2.E1.m2.1.1.1.cmml"><mi id="S2.E1.m2.1.1.1.3" xref="S2.E1.m2.1.1.1.3.cmml">Ï€</mi><mo lspace="0em" rspace="0em" id="S2.E1.m2.1.1.1.2" xref="S2.E1.m2.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E1.m2.1.1.1.4.2" xref="S2.E1.m2.1.1.1.cmml"><mo stretchy="false" id="S2.E1.m2.1.1.1.4.2.1" xref="S2.E1.m2.1.1.1.cmml">(</mo><mi id="S2.E1.m2.1.1.1.1" xref="S2.E1.m2.1.1.1.1.cmml">s</mi><mo stretchy="false" id="S2.E1.m2.1.1.1.4.2.2" xref="S2.E1.m2.1.1.1.cmml">)</mo></mrow></mrow></msup><mo stretchy="false" id="S2.E1.m2.3.3.2.2.2.2.2.5" xref="S2.E1.m2.3.3.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m2.3b"><apply id="S2.E1.m2.3.3.cmml" xref="S2.E1.m2.3.3"><eq id="S2.E1.m2.3.3.3.cmml" xref="S2.E1.m2.3.3.3"></eq><csymbol cd="latexml" id="S2.E1.m2.3.3.4.cmml" xref="S2.E1.m2.3.3.4">absent</csymbol><apply id="S2.E1.m2.3.3.2.cmml" xref="S2.E1.m2.3.3.2"><times id="S2.E1.m2.3.3.2.3.cmml" xref="S2.E1.m2.3.3.2.3"></times><apply id="S2.E1.m2.3.3.2.4.cmml" xref="S2.E1.m2.3.3.2.4"><csymbol cd="ambiguous" id="S2.E1.m2.3.3.2.4.1.cmml" xref="S2.E1.m2.3.3.2.4">subscript</csymbol><apply id="S2.E1.m2.3.3.2.4.2.cmml" xref="S2.E1.m2.3.3.2.4.2"><times id="S2.E1.m2.3.3.2.4.2.1.cmml" xref="S2.E1.m2.3.3.2.4.2.1"></times><ci id="S2.E1.m2.3.3.2.4.2.2.cmml" xref="S2.E1.m2.3.3.2.4.2.2">arg</ci><ci id="S2.E1.m2.3.3.2.4.2.3.cmml" xref="S2.E1.m2.3.3.2.4.2.3">min</ci></apply><apply id="S2.E1.m2.3.3.2.4.3.cmml" xref="S2.E1.m2.3.3.2.4.3"><in id="S2.E1.m2.3.3.2.4.3.1.cmml" xref="S2.E1.m2.3.3.2.4.3.1"></in><ci id="S2.E1.m2.3.3.2.4.3.2.cmml" xref="S2.E1.m2.3.3.2.4.3.2">ğœ‹</ci><ci id="S2.E1.m2.3.3.2.4.3.3.cmml" xref="S2.E1.m2.3.3.2.4.3.3">ğ’«</ci></apply></apply><apply id="S2.E1.m2.3.3.2.2.cmml" xref="S2.E1.m2.3.3.2.2"><apply id="S2.E1.m2.3.3.2.2.3.cmml" xref="S2.E1.m2.3.3.2.2.3"><csymbol cd="ambiguous" id="S2.E1.m2.3.3.2.2.3.1.cmml" xref="S2.E1.m2.3.3.2.2.3">superscript</csymbol><apply id="S2.E1.m2.3.3.2.2.3.2.cmml" xref="S2.E1.m2.3.3.2.2.3"><csymbol cd="ambiguous" id="S2.E1.m2.3.3.2.2.3.2.1.cmml" xref="S2.E1.m2.3.3.2.2.3">subscript</csymbol><sum id="S2.E1.m2.3.3.2.2.3.2.2.cmml" xref="S2.E1.m2.3.3.2.2.3.2.2"></sum><apply id="S2.E1.m2.3.3.2.2.3.2.3.cmml" xref="S2.E1.m2.3.3.2.2.3.2.3"><eq id="S2.E1.m2.3.3.2.2.3.2.3.1.cmml" xref="S2.E1.m2.3.3.2.2.3.2.3.1"></eq><ci id="S2.E1.m2.3.3.2.2.3.2.3.2.cmml" xref="S2.E1.m2.3.3.2.2.3.2.3.2">ğ‘ </ci><cn type="integer" id="S2.E1.m2.3.3.2.2.3.2.3.3.cmml" xref="S2.E1.m2.3.3.2.2.3.2.3.3">1</cn></apply></apply><ci id="S2.E1.m2.3.3.2.2.3.3.cmml" xref="S2.E1.m2.3.3.2.2.3.3">ğ‘†</ci></apply><apply id="S2.E1.m2.3.3.2.2.2.cmml" xref="S2.E1.m2.3.3.2.2.2"><times id="S2.E1.m2.3.3.2.2.2.3.cmml" xref="S2.E1.m2.3.3.2.2.2.3"></times><apply id="S2.E1.m2.3.3.2.2.2.4.cmml" xref="S2.E1.m2.3.3.2.2.2.4"><csymbol cd="ambiguous" id="S2.E1.m2.3.3.2.2.2.4.1.cmml" xref="S2.E1.m2.3.3.2.2.2.4">subscript</csymbol><ci id="S2.E1.m2.3.3.2.2.2.4.2a.cmml" xref="S2.E1.m2.3.3.2.2.2.4.2"><mtext id="S2.E1.m2.3.3.2.2.2.4.2.cmml" xref="S2.E1.m2.3.3.2.2.2.4.2">Loss</mtext></ci><ci id="S2.E1.m2.3.3.2.2.2.4.3a.cmml" xref="S2.E1.m2.3.3.2.2.2.4.3"><mtext mathsize="70%" id="S2.E1.m2.3.3.2.2.2.4.3.cmml" xref="S2.E1.m2.3.3.2.2.2.4.3">ASR</mtext></ci></apply><interval closure="open" id="S2.E1.m2.3.3.2.2.2.2.3.cmml" xref="S2.E1.m2.3.3.2.2.2.2.2"><apply id="S2.E1.m2.2.2.1.1.1.1.1.1.cmml" xref="S2.E1.m2.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m2.2.2.1.1.1.1.1.1.1.cmml" xref="S2.E1.m2.2.2.1.1.1.1.1.1">superscript</csymbol><ci id="S2.E1.m2.2.2.1.1.1.1.1.1.2.cmml" xref="S2.E1.m2.2.2.1.1.1.1.1.1.2">ğ‘Œ</ci><ci id="S2.E1.m2.2.2.1.1.1.1.1.1.3.cmml" xref="S2.E1.m2.2.2.1.1.1.1.1.1.3">ğ‘ </ci></apply><apply id="S2.E1.m2.3.3.2.2.2.2.2.2.cmml" xref="S2.E1.m2.3.3.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m2.3.3.2.2.2.2.2.2.1.cmml" xref="S2.E1.m2.3.3.2.2.2.2.2.2">superscript</csymbol><ci id="S2.E1.m2.3.3.2.2.2.2.2.2.2.cmml" xref="S2.E1.m2.3.3.2.2.2.2.2.2.2">ğ‘…</ci><apply id="S2.E1.m2.1.1.1.cmml" xref="S2.E1.m2.1.1.1"><times id="S2.E1.m2.1.1.1.2.cmml" xref="S2.E1.m2.1.1.1.2"></times><ci id="S2.E1.m2.1.1.1.3.cmml" xref="S2.E1.m2.1.1.1.3">ğœ‹</ci><ci id="S2.E1.m2.1.1.1.1.cmml" xref="S2.E1.m2.1.1.1.1">ğ‘ </ci></apply></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m2.3c">\displaystyle=\operatorname*{arg\,min}_{\pi\in\mathcal{P}}\sum_{s=1}^{S}\,\text{Loss}_{\text{ASR}}(Y^{s},R^{\pi(s)})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS5.p1.10" class="ltx_p">where <math id="S2.SS5.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{P}" display="inline"><semantics id="S2.SS5.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS5.p1.1.m1.1.1" xref="S2.SS5.p1.1.m1.1.1.cmml">ğ’«</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p1.1.m1.1b"><ci id="S2.SS5.p1.1.m1.1.1.cmml" xref="S2.SS5.p1.1.m1.1.1">ğ’«</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p1.1.m1.1c">\mathcal{P}</annotation></semantics></math> denotes the set of all permutations on <math id="S2.SS5.p1.2.m2.3" class="ltx_Math" alttext="=\{1,...,S\}" display="inline"><semantics id="S2.SS5.p1.2.m2.3a"><mrow id="S2.SS5.p1.2.m2.3.4" xref="S2.SS5.p1.2.m2.3.4.cmml"><mi id="S2.SS5.p1.2.m2.3.4.2" xref="S2.SS5.p1.2.m2.3.4.2.cmml"></mi><mo id="S2.SS5.p1.2.m2.3.4.1" xref="S2.SS5.p1.2.m2.3.4.1.cmml">=</mo><mrow id="S2.SS5.p1.2.m2.3.4.3.2" xref="S2.SS5.p1.2.m2.3.4.3.1.cmml"><mo stretchy="false" id="S2.SS5.p1.2.m2.3.4.3.2.1" xref="S2.SS5.p1.2.m2.3.4.3.1.cmml">{</mo><mn id="S2.SS5.p1.2.m2.1.1" xref="S2.SS5.p1.2.m2.1.1.cmml">1</mn><mo id="S2.SS5.p1.2.m2.3.4.3.2.2" xref="S2.SS5.p1.2.m2.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.SS5.p1.2.m2.2.2" xref="S2.SS5.p1.2.m2.2.2.cmml">â€¦</mi><mo id="S2.SS5.p1.2.m2.3.4.3.2.3" xref="S2.SS5.p1.2.m2.3.4.3.1.cmml">,</mo><mi id="S2.SS5.p1.2.m2.3.3" xref="S2.SS5.p1.2.m2.3.3.cmml">S</mi><mo stretchy="false" id="S2.SS5.p1.2.m2.3.4.3.2.4" xref="S2.SS5.p1.2.m2.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS5.p1.2.m2.3b"><apply id="S2.SS5.p1.2.m2.3.4.cmml" xref="S2.SS5.p1.2.m2.3.4"><eq id="S2.SS5.p1.2.m2.3.4.1.cmml" xref="S2.SS5.p1.2.m2.3.4.1"></eq><csymbol cd="latexml" id="S2.SS5.p1.2.m2.3.4.2.cmml" xref="S2.SS5.p1.2.m2.3.4.2">absent</csymbol><set id="S2.SS5.p1.2.m2.3.4.3.1.cmml" xref="S2.SS5.p1.2.m2.3.4.3.2"><cn type="integer" id="S2.SS5.p1.2.m2.1.1.cmml" xref="S2.SS5.p1.2.m2.1.1">1</cn><ci id="S2.SS5.p1.2.m2.2.2.cmml" xref="S2.SS5.p1.2.m2.2.2">â€¦</ci><ci id="S2.SS5.p1.2.m2.3.3.cmml" xref="S2.SS5.p1.2.m2.3.3">ğ‘†</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p1.2.m2.3c">=\{1,...,S\}</annotation></semantics></math>, <math id="S2.SS5.p1.3.m3.1" class="ltx_Math" alttext="\pi(s)" display="inline"><semantics id="S2.SS5.p1.3.m3.1a"><mrow id="S2.SS5.p1.3.m3.1.2" xref="S2.SS5.p1.3.m3.1.2.cmml"><mi id="S2.SS5.p1.3.m3.1.2.2" xref="S2.SS5.p1.3.m3.1.2.2.cmml">Ï€</mi><mo lspace="0em" rspace="0em" id="S2.SS5.p1.3.m3.1.2.1" xref="S2.SS5.p1.3.m3.1.2.1.cmml">â€‹</mo><mrow id="S2.SS5.p1.3.m3.1.2.3.2" xref="S2.SS5.p1.3.m3.1.2.cmml"><mo stretchy="false" id="S2.SS5.p1.3.m3.1.2.3.2.1" xref="S2.SS5.p1.3.m3.1.2.cmml">(</mo><mi id="S2.SS5.p1.3.m3.1.1" xref="S2.SS5.p1.3.m3.1.1.cmml">s</mi><mo stretchy="false" id="S2.SS5.p1.3.m3.1.2.3.2.2" xref="S2.SS5.p1.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS5.p1.3.m3.1b"><apply id="S2.SS5.p1.3.m3.1.2.cmml" xref="S2.SS5.p1.3.m3.1.2"><times id="S2.SS5.p1.3.m3.1.2.1.cmml" xref="S2.SS5.p1.3.m3.1.2.1"></times><ci id="S2.SS5.p1.3.m3.1.2.2.cmml" xref="S2.SS5.p1.3.m3.1.2.2">ğœ‹</ci><ci id="S2.SS5.p1.3.m3.1.1.cmml" xref="S2.SS5.p1.3.m3.1.1">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p1.3.m3.1c">\pi(s)</annotation></semantics></math> denotes the <math id="S2.SS5.p1.4.m4.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S2.SS5.p1.4.m4.1a"><mi id="S2.SS5.p1.4.m4.1.1" xref="S2.SS5.p1.4.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p1.4.m4.1b"><ci id="S2.SS5.p1.4.m4.1.1.cmml" xref="S2.SS5.p1.4.m4.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p1.4.m4.1c">s</annotation></semantics></math>-th element in a permutation <math id="S2.SS5.p1.5.m5.1" class="ltx_Math" alttext="\pi" display="inline"><semantics id="S2.SS5.p1.5.m5.1a"><mi id="S2.SS5.p1.5.m5.1.1" xref="S2.SS5.p1.5.m5.1.1.cmml">Ï€</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p1.5.m5.1b"><ci id="S2.SS5.p1.5.m5.1.1.cmml" xref="S2.SS5.p1.5.m5.1.1">ğœ‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p1.5.m5.1c">\pi</annotation></semantics></math>, <math id="S2.SS5.p1.6.m6.1" class="ltx_Math" alttext="Y^{s}" display="inline"><semantics id="S2.SS5.p1.6.m6.1a"><msup id="S2.SS5.p1.6.m6.1.1" xref="S2.SS5.p1.6.m6.1.1.cmml"><mi id="S2.SS5.p1.6.m6.1.1.2" xref="S2.SS5.p1.6.m6.1.1.2.cmml">Y</mi><mi id="S2.SS5.p1.6.m6.1.1.3" xref="S2.SS5.p1.6.m6.1.1.3.cmml">s</mi></msup><annotation-xml encoding="MathML-Content" id="S2.SS5.p1.6.m6.1b"><apply id="S2.SS5.p1.6.m6.1.1.cmml" xref="S2.SS5.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS5.p1.6.m6.1.1.1.cmml" xref="S2.SS5.p1.6.m6.1.1">superscript</csymbol><ci id="S2.SS5.p1.6.m6.1.1.2.cmml" xref="S2.SS5.p1.6.m6.1.1.2">ğ‘Œ</ci><ci id="S2.SS5.p1.6.m6.1.1.3.cmml" xref="S2.SS5.p1.6.m6.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p1.6.m6.1c">Y^{s}</annotation></semantics></math> is the predicted token sequences of the <math id="S2.SS5.p1.7.m7.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S2.SS5.p1.7.m7.1a"><mi id="S2.SS5.p1.7.m7.1.1" xref="S2.SS5.p1.7.m7.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p1.7.m7.1b"><ci id="S2.SS5.p1.7.m7.1.1.cmml" xref="S2.SS5.p1.7.m7.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p1.7.m7.1c">s</annotation></semantics></math>-th branch, and <math id="S2.SS5.p1.8.m8.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S2.SS5.p1.8.m8.1a"><mi id="S2.SS5.p1.8.m8.1.1" xref="S2.SS5.p1.8.m8.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p1.8.m8.1b"><ci id="S2.SS5.p1.8.m8.1.1.cmml" xref="S2.SS5.p1.8.m8.1.1">ğ‘…</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p1.8.m8.1c">R</annotation></semantics></math> is the reference labels for <math id="S2.SS5.p1.9.m9.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.SS5.p1.9.m9.1a"><mi id="S2.SS5.p1.9.m9.1.1" xref="S2.SS5.p1.9.m9.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p1.9.m9.1b"><ci id="S2.SS5.p1.9.m9.1.1.cmml" xref="S2.SS5.p1.9.m9.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p1.9.m9.1c">S</annotation></semantics></math> talkers.
At last, the final objective function is the sum of PIT-ASR loss and corresponding TTI loss multiplied by a coefficient <math id="S2.SS5.p1.10.m10.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S2.SS5.p1.10.m10.1a"><mi id="S2.SS5.p1.10.m10.1.1" xref="S2.SS5.p1.10.m10.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p1.10.m10.1b"><ci id="S2.SS5.p1.10.m10.1.1.cmml" xref="S2.SS5.p1.10.m10.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p1.10.m10.1c">\lambda</annotation></semantics></math>. Therefore we have,</p>
<table id="S6.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E2.m1.1" class="ltx_Math" alttext="\displaystyle\mathcal{L}=\mathcal{L}_{\text{ASR}}+\lambda\,\mathcal{L}_{\text{TTI}}" display="inline"><semantics id="S2.E2.m1.1a"><mrow id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.1.1.2" xref="S2.E2.m1.1.1.2.cmml">â„’</mi><mo id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.cmml">=</mo><mrow id="S2.E2.m1.1.1.3" xref="S2.E2.m1.1.1.3.cmml"><msub id="S2.E2.m1.1.1.3.2" xref="S2.E2.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.1.1.3.2.2" xref="S2.E2.m1.1.1.3.2.2.cmml">â„’</mi><mtext id="S2.E2.m1.1.1.3.2.3" xref="S2.E2.m1.1.1.3.2.3a.cmml">ASR</mtext></msub><mo id="S2.E2.m1.1.1.3.1" xref="S2.E2.m1.1.1.3.1.cmml">+</mo><mrow id="S2.E2.m1.1.1.3.3" xref="S2.E2.m1.1.1.3.3.cmml"><mi id="S2.E2.m1.1.1.3.3.2" xref="S2.E2.m1.1.1.3.3.2.cmml">Î»</mi><mo lspace="0.170em" rspace="0em" id="S2.E2.m1.1.1.3.3.1" xref="S2.E2.m1.1.1.3.3.1.cmml">â€‹</mo><msub id="S2.E2.m1.1.1.3.3.3" xref="S2.E2.m1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.1.1.3.3.3.2" xref="S2.E2.m1.1.1.3.3.3.2.cmml">â„’</mi><mtext id="S2.E2.m1.1.1.3.3.3.3" xref="S2.E2.m1.1.1.3.3.3.3a.cmml">TTI</mtext></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1"><eq id="S2.E2.m1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"></eq><ci id="S2.E2.m1.1.1.2.cmml" xref="S2.E2.m1.1.1.2">â„’</ci><apply id="S2.E2.m1.1.1.3.cmml" xref="S2.E2.m1.1.1.3"><plus id="S2.E2.m1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.3.1"></plus><apply id="S2.E2.m1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.3.2.1.cmml" xref="S2.E2.m1.1.1.3.2">subscript</csymbol><ci id="S2.E2.m1.1.1.3.2.2.cmml" xref="S2.E2.m1.1.1.3.2.2">â„’</ci><ci id="S2.E2.m1.1.1.3.2.3a.cmml" xref="S2.E2.m1.1.1.3.2.3"><mtext mathsize="70%" id="S2.E2.m1.1.1.3.2.3.cmml" xref="S2.E2.m1.1.1.3.2.3">ASR</mtext></ci></apply><apply id="S2.E2.m1.1.1.3.3.cmml" xref="S2.E2.m1.1.1.3.3"><times id="S2.E2.m1.1.1.3.3.1.cmml" xref="S2.E2.m1.1.1.3.3.1"></times><ci id="S2.E2.m1.1.1.3.3.2.cmml" xref="S2.E2.m1.1.1.3.3.2">ğœ†</ci><apply id="S2.E2.m1.1.1.3.3.3.cmml" xref="S2.E2.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.3.3.3.1.cmml" xref="S2.E2.m1.1.1.3.3.3">subscript</csymbol><ci id="S2.E2.m1.1.1.3.3.3.2.cmml" xref="S2.E2.m1.1.1.3.3.3.2">â„’</ci><ci id="S2.E2.m1.1.1.3.3.3.3a.cmml" xref="S2.E2.m1.1.1.3.3.3.3"><mtext mathsize="70%" id="S2.E2.m1.1.1.3.3.3.3.cmml" xref="S2.E2.m1.1.1.3.3.3.3">TTI</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">\displaystyle\mathcal{L}=\mathcal{L}_{\text{ASR}}+\lambda\,\mathcal{L}_{\text{TTI}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S2.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E3.m1.3" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\text{ASR}}=\sum_{s}\,\text{Loss}_{\text{ASR}}(Y^{s},R^{\hat{\pi}(s)})" display="inline"><semantics id="S2.E3.m1.3a"><mrow id="S2.E3.m1.3.3" xref="S2.E3.m1.3.3.cmml"><msub id="S2.E3.m1.3.3.4" xref="S2.E3.m1.3.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.3.3.4.2" xref="S2.E3.m1.3.3.4.2.cmml">â„’</mi><mtext id="S2.E3.m1.3.3.4.3" xref="S2.E3.m1.3.3.4.3a.cmml">ASR</mtext></msub><mo id="S2.E3.m1.3.3.3" xref="S2.E3.m1.3.3.3.cmml">=</mo><mrow id="S2.E3.m1.3.3.2" xref="S2.E3.m1.3.3.2.cmml"><mstyle displaystyle="true" id="S2.E3.m1.3.3.2.3" xref="S2.E3.m1.3.3.2.3.cmml"><munder id="S2.E3.m1.3.3.2.3a" xref="S2.E3.m1.3.3.2.3.cmml"><mo movablelimits="false" id="S2.E3.m1.3.3.2.3.2" xref="S2.E3.m1.3.3.2.3.2.cmml">âˆ‘</mo><mi id="S2.E3.m1.3.3.2.3.3" xref="S2.E3.m1.3.3.2.3.3.cmml">s</mi></munder></mstyle><mrow id="S2.E3.m1.3.3.2.2" xref="S2.E3.m1.3.3.2.2.cmml"><msub id="S2.E3.m1.3.3.2.2.4" xref="S2.E3.m1.3.3.2.2.4.cmml"><mtext id="S2.E3.m1.3.3.2.2.4.2" xref="S2.E3.m1.3.3.2.2.4.2a.cmml">Loss</mtext><mtext id="S2.E3.m1.3.3.2.2.4.3" xref="S2.E3.m1.3.3.2.2.4.3a.cmml">ASR</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E3.m1.3.3.2.2.3" xref="S2.E3.m1.3.3.2.2.3.cmml">â€‹</mo><mrow id="S2.E3.m1.3.3.2.2.2.2" xref="S2.E3.m1.3.3.2.2.2.3.cmml"><mo stretchy="false" id="S2.E3.m1.3.3.2.2.2.2.3" xref="S2.E3.m1.3.3.2.2.2.3.cmml">(</mo><msup id="S2.E3.m1.2.2.1.1.1.1.1" xref="S2.E3.m1.2.2.1.1.1.1.1.cmml"><mi id="S2.E3.m1.2.2.1.1.1.1.1.2" xref="S2.E3.m1.2.2.1.1.1.1.1.2.cmml">Y</mi><mi id="S2.E3.m1.2.2.1.1.1.1.1.3" xref="S2.E3.m1.2.2.1.1.1.1.1.3.cmml">s</mi></msup><mo id="S2.E3.m1.3.3.2.2.2.2.4" xref="S2.E3.m1.3.3.2.2.2.3.cmml">,</mo><msup id="S2.E3.m1.3.3.2.2.2.2.2" xref="S2.E3.m1.3.3.2.2.2.2.2.cmml"><mi id="S2.E3.m1.3.3.2.2.2.2.2.2" xref="S2.E3.m1.3.3.2.2.2.2.2.2.cmml">R</mi><mrow id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.cmml"><mover accent="true" id="S2.E3.m1.1.1.1.3" xref="S2.E3.m1.1.1.1.3.cmml"><mi id="S2.E3.m1.1.1.1.3.2" xref="S2.E3.m1.1.1.1.3.2.cmml">Ï€</mi><mo id="S2.E3.m1.1.1.1.3.1" xref="S2.E3.m1.1.1.1.3.1.cmml">^</mo></mover><mo lspace="0em" rspace="0em" id="S2.E3.m1.1.1.1.2" xref="S2.E3.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E3.m1.1.1.1.4.2" xref="S2.E3.m1.1.1.1.cmml"><mo stretchy="false" id="S2.E3.m1.1.1.1.4.2.1" xref="S2.E3.m1.1.1.1.cmml">(</mo><mi id="S2.E3.m1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.cmml">s</mi><mo stretchy="false" id="S2.E3.m1.1.1.1.4.2.2" xref="S2.E3.m1.1.1.1.cmml">)</mo></mrow></mrow></msup><mo stretchy="false" id="S2.E3.m1.3.3.2.2.2.2.5" xref="S2.E3.m1.3.3.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.3b"><apply id="S2.E3.m1.3.3.cmml" xref="S2.E3.m1.3.3"><eq id="S2.E3.m1.3.3.3.cmml" xref="S2.E3.m1.3.3.3"></eq><apply id="S2.E3.m1.3.3.4.cmml" xref="S2.E3.m1.3.3.4"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.4.1.cmml" xref="S2.E3.m1.3.3.4">subscript</csymbol><ci id="S2.E3.m1.3.3.4.2.cmml" xref="S2.E3.m1.3.3.4.2">â„’</ci><ci id="S2.E3.m1.3.3.4.3a.cmml" xref="S2.E3.m1.3.3.4.3"><mtext mathsize="70%" id="S2.E3.m1.3.3.4.3.cmml" xref="S2.E3.m1.3.3.4.3">ASR</mtext></ci></apply><apply id="S2.E3.m1.3.3.2.cmml" xref="S2.E3.m1.3.3.2"><apply id="S2.E3.m1.3.3.2.3.cmml" xref="S2.E3.m1.3.3.2.3"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.2.3.1.cmml" xref="S2.E3.m1.3.3.2.3">subscript</csymbol><sum id="S2.E3.m1.3.3.2.3.2.cmml" xref="S2.E3.m1.3.3.2.3.2"></sum><ci id="S2.E3.m1.3.3.2.3.3.cmml" xref="S2.E3.m1.3.3.2.3.3">ğ‘ </ci></apply><apply id="S2.E3.m1.3.3.2.2.cmml" xref="S2.E3.m1.3.3.2.2"><times id="S2.E3.m1.3.3.2.2.3.cmml" xref="S2.E3.m1.3.3.2.2.3"></times><apply id="S2.E3.m1.3.3.2.2.4.cmml" xref="S2.E3.m1.3.3.2.2.4"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.2.2.4.1.cmml" xref="S2.E3.m1.3.3.2.2.4">subscript</csymbol><ci id="S2.E3.m1.3.3.2.2.4.2a.cmml" xref="S2.E3.m1.3.3.2.2.4.2"><mtext id="S2.E3.m1.3.3.2.2.4.2.cmml" xref="S2.E3.m1.3.3.2.2.4.2">Loss</mtext></ci><ci id="S2.E3.m1.3.3.2.2.4.3a.cmml" xref="S2.E3.m1.3.3.2.2.4.3"><mtext mathsize="70%" id="S2.E3.m1.3.3.2.2.4.3.cmml" xref="S2.E3.m1.3.3.2.2.4.3">ASR</mtext></ci></apply><interval closure="open" id="S2.E3.m1.3.3.2.2.2.3.cmml" xref="S2.E3.m1.3.3.2.2.2.2"><apply id="S2.E3.m1.2.2.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1">superscript</csymbol><ci id="S2.E3.m1.2.2.1.1.1.1.1.2.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.2">ğ‘Œ</ci><ci id="S2.E3.m1.2.2.1.1.1.1.1.3.cmml" xref="S2.E3.m1.2.2.1.1.1.1.1.3">ğ‘ </ci></apply><apply id="S2.E3.m1.3.3.2.2.2.2.2.cmml" xref="S2.E3.m1.3.3.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.3.3.2.2.2.2.2.1.cmml" xref="S2.E3.m1.3.3.2.2.2.2.2">superscript</csymbol><ci id="S2.E3.m1.3.3.2.2.2.2.2.2.cmml" xref="S2.E3.m1.3.3.2.2.2.2.2.2">ğ‘…</ci><apply id="S2.E3.m1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"><times id="S2.E3.m1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.2"></times><apply id="S2.E3.m1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.3"><ci id="S2.E3.m1.1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.1.3.1">^</ci><ci id="S2.E3.m1.1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.1.3.2">ğœ‹</ci></apply><ci id="S2.E3.m1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1">ğ‘ </ci></apply></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.3c">\displaystyle\mathcal{L}_{\text{ASR}}=\sum_{s}\,\text{Loss}_{\text{ASR}}(Y^{s},R^{\hat{\pi}(s)})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
<tbody id="S2.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.E4.m1.2" class="ltx_Math" alttext="\displaystyle\mathcal{L}_{\text{TTI}}=\text{Loss}_{\text{CE}}(Z,D^{\hat{\pi}})" display="inline"><semantics id="S2.E4.m1.2a"><mrow id="S2.E4.m1.2.2" xref="S2.E4.m1.2.2.cmml"><msub id="S2.E4.m1.2.2.3" xref="S2.E4.m1.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.2.2.3.2" xref="S2.E4.m1.2.2.3.2.cmml">â„’</mi><mtext id="S2.E4.m1.2.2.3.3" xref="S2.E4.m1.2.2.3.3a.cmml">TTI</mtext></msub><mo id="S2.E4.m1.2.2.2" xref="S2.E4.m1.2.2.2.cmml">=</mo><mrow id="S2.E4.m1.2.2.1" xref="S2.E4.m1.2.2.1.cmml"><msub id="S2.E4.m1.2.2.1.3" xref="S2.E4.m1.2.2.1.3.cmml"><mtext id="S2.E4.m1.2.2.1.3.2" xref="S2.E4.m1.2.2.1.3.2a.cmml">Loss</mtext><mtext id="S2.E4.m1.2.2.1.3.3" xref="S2.E4.m1.2.2.1.3.3a.cmml">CE</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E4.m1.2.2.1.2" xref="S2.E4.m1.2.2.1.2.cmml">â€‹</mo><mrow id="S2.E4.m1.2.2.1.1.1" xref="S2.E4.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S2.E4.m1.2.2.1.1.1.2" xref="S2.E4.m1.2.2.1.1.2.cmml">(</mo><mi id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml">Z</mi><mo id="S2.E4.m1.2.2.1.1.1.3" xref="S2.E4.m1.2.2.1.1.2.cmml">,</mo><msup id="S2.E4.m1.2.2.1.1.1.1" xref="S2.E4.m1.2.2.1.1.1.1.cmml"><mi id="S2.E4.m1.2.2.1.1.1.1.2" xref="S2.E4.m1.2.2.1.1.1.1.2.cmml">D</mi><mover accent="true" id="S2.E4.m1.2.2.1.1.1.1.3" xref="S2.E4.m1.2.2.1.1.1.1.3.cmml"><mi id="S2.E4.m1.2.2.1.1.1.1.3.2" xref="S2.E4.m1.2.2.1.1.1.1.3.2.cmml">Ï€</mi><mo id="S2.E4.m1.2.2.1.1.1.1.3.1" xref="S2.E4.m1.2.2.1.1.1.1.3.1.cmml">^</mo></mover></msup><mo stretchy="false" id="S2.E4.m1.2.2.1.1.1.4" xref="S2.E4.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.2b"><apply id="S2.E4.m1.2.2.cmml" xref="S2.E4.m1.2.2"><eq id="S2.E4.m1.2.2.2.cmml" xref="S2.E4.m1.2.2.2"></eq><apply id="S2.E4.m1.2.2.3.cmml" xref="S2.E4.m1.2.2.3"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.3.1.cmml" xref="S2.E4.m1.2.2.3">subscript</csymbol><ci id="S2.E4.m1.2.2.3.2.cmml" xref="S2.E4.m1.2.2.3.2">â„’</ci><ci id="S2.E4.m1.2.2.3.3a.cmml" xref="S2.E4.m1.2.2.3.3"><mtext mathsize="70%" id="S2.E4.m1.2.2.3.3.cmml" xref="S2.E4.m1.2.2.3.3">TTI</mtext></ci></apply><apply id="S2.E4.m1.2.2.1.cmml" xref="S2.E4.m1.2.2.1"><times id="S2.E4.m1.2.2.1.2.cmml" xref="S2.E4.m1.2.2.1.2"></times><apply id="S2.E4.m1.2.2.1.3.cmml" xref="S2.E4.m1.2.2.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.1.3.1.cmml" xref="S2.E4.m1.2.2.1.3">subscript</csymbol><ci id="S2.E4.m1.2.2.1.3.2a.cmml" xref="S2.E4.m1.2.2.1.3.2"><mtext id="S2.E4.m1.2.2.1.3.2.cmml" xref="S2.E4.m1.2.2.1.3.2">Loss</mtext></ci><ci id="S2.E4.m1.2.2.1.3.3a.cmml" xref="S2.E4.m1.2.2.1.3.3"><mtext mathsize="70%" id="S2.E4.m1.2.2.1.3.3.cmml" xref="S2.E4.m1.2.2.1.3.3">CE</mtext></ci></apply><interval closure="open" id="S2.E4.m1.2.2.1.1.2.cmml" xref="S2.E4.m1.2.2.1.1.1"><ci id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1">ğ‘</ci><apply id="S2.E4.m1.2.2.1.1.1.1.cmml" xref="S2.E4.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.2.2.1.1.1.1.1.cmml" xref="S2.E4.m1.2.2.1.1.1.1">superscript</csymbol><ci id="S2.E4.m1.2.2.1.1.1.1.2.cmml" xref="S2.E4.m1.2.2.1.1.1.1.2">ğ·</ci><apply id="S2.E4.m1.2.2.1.1.1.1.3.cmml" xref="S2.E4.m1.2.2.1.1.1.1.3"><ci id="S2.E4.m1.2.2.1.1.1.1.3.1.cmml" xref="S2.E4.m1.2.2.1.1.1.1.3.1">^</ci><ci id="S2.E4.m1.2.2.1.1.1.1.3.2.cmml" xref="S2.E4.m1.2.2.1.1.1.1.3.2">ğœ‹</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.2c">\displaystyle\mathcal{L}_{\text{TTI}}=\text{Loss}_{\text{CE}}(Z,D^{\hat{\pi}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS5.p2" class="ltx_para ltx_noindent">
<p id="S2.SS5.p2.2" class="ltx_p">where <math id="S2.SS5.p2.1.m1.1" class="ltx_Math" alttext="Z" display="inline"><semantics id="S2.SS5.p2.1.m1.1a"><mi id="S2.SS5.p2.1.m1.1.1" xref="S2.SS5.p2.1.m1.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S2.SS5.p2.1.m1.1b"><ci id="S2.SS5.p2.1.m1.1.1.cmml" xref="S2.SS5.p2.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p2.1.m1.1c">Z</annotation></semantics></math> is the probability of each branch is of the target talker, and <math id="S2.SS5.p2.2.m2.1" class="ltx_Math" alttext="D^{\hat{\pi}}" display="inline"><semantics id="S2.SS5.p2.2.m2.1a"><msup id="S2.SS5.p2.2.m2.1.1" xref="S2.SS5.p2.2.m2.1.1.cmml"><mi id="S2.SS5.p2.2.m2.1.1.2" xref="S2.SS5.p2.2.m2.1.1.2.cmml">D</mi><mover accent="true" id="S2.SS5.p2.2.m2.1.1.3" xref="S2.SS5.p2.2.m2.1.1.3.cmml"><mi id="S2.SS5.p2.2.m2.1.1.3.2" xref="S2.SS5.p2.2.m2.1.1.3.2.cmml">Ï€</mi><mo id="S2.SS5.p2.2.m2.1.1.3.1" xref="S2.SS5.p2.2.m2.1.1.3.1.cmml">^</mo></mover></msup><annotation-xml encoding="MathML-Content" id="S2.SS5.p2.2.m2.1b"><apply id="S2.SS5.p2.2.m2.1.1.cmml" xref="S2.SS5.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS5.p2.2.m2.1.1.1.cmml" xref="S2.SS5.p2.2.m2.1.1">superscript</csymbol><ci id="S2.SS5.p2.2.m2.1.1.2.cmml" xref="S2.SS5.p2.2.m2.1.1.2">ğ·</ci><apply id="S2.SS5.p2.2.m2.1.1.3.cmml" xref="S2.SS5.p2.2.m2.1.1.3"><ci id="S2.SS5.p2.2.m2.1.1.3.1.cmml" xref="S2.SS5.p2.2.m2.1.1.3.1">^</ci><ci id="S2.SS5.p2.2.m2.1.1.3.2.cmml" xref="S2.SS5.p2.2.m2.1.1.3.2">ğœ‹</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS5.p2.2.m2.1c">D^{\hat{\pi}}</annotation></semantics></math> is the ground truth after permutation.</p>
</div>
<div id="S2.SS5.p3" class="ltx_para">
<p id="S2.SS5.p3.1" class="ltx_p">Considering that the original Whisper was not trained using CTC loss, we refrain from employing an additional CTC loss for early permutation assignment as done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The experiments are conducted on three multi-talker public datasets, namely LibriMix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and LibriSpeechMix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> in English, and Aishell1Mix<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/huangzj421/Aishell1Mix</span></span></span> in Mandarin. Audio exceeding Whisper's maximum handling duration of 30 seconds are time-stretched to conform to this limit. For target-talker ASR on LibriMix and LibriSpeechMix, we randomly trim three-second clips from LibriSpeech as enrollment speech for each talker.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">LibriMix</span>. The dataset simulates audio mixtures in a left-aligned manner, involving two or three speakers from the LibriSpeech-clean corpus. Thus, the shorter source speech is entirely overlapped with the longer one from the start, presenting significant challenge in separating overlaps. We focus on its two-speaker-mixed and three-speaker-mixed clean subset, denoted as Libri2Mix and Libri3Mix in the following.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">LibriSpeechMix</span>. The utterances are simulated from LibriSpeech, comprising mixtures from two or three speakers. Unlike LibriMix, the delay time for each speaker is randomly sampled, resulting in partially overlapped mixtures. Since only official dev and test sets are released, we created our training set from the 960-hour LibriSpeech following the same protocol as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, except that the mixtures are kept under 30 seconds.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">Aishell1Mix</span>. It is a Mandarin multi-talker speech dataset, source from Aishell1 corpus. It simulate mixtures with a same protocol of the LibriMix. We focus on its two-speaker-clean subset for analysis, denoted as AishellMix in the following.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Model Settings and Evaluation Metrics</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Throughout this study, we employ Whisper-small, -medium, and -large-v3 as the foundation models, respectively. We freeze these models and only train the Sidecar separator, Target Talker Identifier, and soft prompt embedding. The number of trainable parameters for systems using different foundation models and for various numbers of talkers are listed in Table <a href="#S3.T1" title="Table 1 â€£ 3.2 Model Settings and Evaluation Metrics â€£ 3 Experimental Setup â€£ Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.2" class="ltx_p">The Conv-TasNet-like Sidecar separator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> comprises a series of <span id="S3.SS2.p2.2.1" class="ltx_text ltx_font_italic">K</span> temporal convolutional blocks with dilation rates ranging from 1 to <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="2^{\textit{K}-1}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msup id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mn id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">2</mn><mrow id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml"><mtext class="ltx_mathvariant_italic" id="S3.SS2.p2.1.m1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.3.2a.cmml">K</mtext><mo id="S3.SS2.p2.1.m1.1.1.3.1" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">âˆ’</mo><mn id="S3.SS2.p2.1.m1.1.1.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.cmml">1</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">2</cn><apply id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><minus id="S3.SS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.1"></minus><ci id="S3.SS2.p2.1.m1.1.1.3.2a.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S3.SS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2">K</mtext></ci><cn type="integer" id="S3.SS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">2^{\textit{K}-1}</annotation></semantics></math>, with each block repeats up to <span id="S3.SS2.p2.2.2" class="ltx_text ltx_font_italic">R</span> times. Consistent with the protocol in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, we use <span id="S3.SS2.p2.2.3" class="ltx_text ltx_font_italic">K</span> = 8 and <span id="S3.SS2.p2.2.4" class="ltx_text ltx_font_italic">R</span> = 3 and plug it between the second and third encoder blocks. The length for soft prompt embeddings are investigated through ablation experiments (Section <a href="#S4.SS4" title="4.4 Ablation Study â€£ 4 Results and Discussions â€£ Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>). As a result, we establish a length of <span id="S3.SS2.p2.2.5" class="ltx_text ltx_markedasmath ltx_font_italic">4</span>, which gives the best performance.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">For systems with the TTI module, at each training step, there's an 80% probability of undertaking multi-talker ASR training, while a 20% probability for joint multi-talker and target-talker ASR training. We set the coefficient of TTI loss <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\lambda</annotation></semantics></math> to 0.01. The systems are trained and evaluated on two- and three-talker subsets of LibriMix and LibriSpeechMix, respectively. Each training session lasts for a maximum of 200k steps on 8 NVIDIA V100 GPUs with a total batch size of 16, employing AdamW optimizer with an initial learning rate of 2e-4 that decreases linearly to 1e-4.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Permutations with minimum errors are used to compute word error rate (WER) or character error rate (CER) for multi-talker ASR as in prior studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. For target-talker ASR, we use standard WER for evaluation.
Both the model's predictions and the references are normalized following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>The amount of trainable parameters, with numbers in parentheses indicating their proportion in the total parameters.</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Foundation Model</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">2-speaker</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.8pt;padding-right:2.8pt;">3-speaker</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">Whisper-small</th>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">8.69 M (3.47%)</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.8pt;padding-right:2.8pt;">8.79 M (3.51%)</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.8pt;padding-right:2.8pt;">Whisper-medium</th>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">13.16 M (1.69%)</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center" style="padding-left:2.8pt;padding-right:2.8pt;">13.29 M (1.71%)</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">Whisper-large</th>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">18.41 M (1.18%)</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.8pt;padding-right:2.8pt;">18.58 M (1.19%)</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results and Discussions</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Multi-Talker ASR Results</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We compared the performance of various systems for multi-talker ASR on the two- and three-talker LibriMix and LibriSpeechMix test sets, as shown in Table <a href="#S4.T2" title="Table 2 â€£ 4.5 Limitations and Future Work â€£ 4 Results and Discussions â€£ Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Empowered by Sidecar separator (SS), TTI, and soft prompt, our systems (f)-(k) consistently illustrated improved performance across both the two- and three-speaker subsets. Even with Whisper-small-SS-TTI (g), owing to Whisper's extensive pre-training, our method has already surpassed the original Sidecar scheme<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. As the size of the Whisper model increases, we observed a steady improvement in performance, which aligns with our expectations. Ultimately, our systems outperformed previous approaches across all datasets except for LibriSpeechMix-3spk, demonstrating the superiority of the proposed approach.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Interestingly, we find that systems with the TTI module (g) (i) (k) outperform their counterparts without TTI (f) (h) (j) in multi-talker ASR task, even though the TTI is specifically designed to support target-talker ASR. This suggests that the training objective of learning to distinguish the target talker can benefit the Sidecar's ability to separate embeddings, thereby facilitating the task of multi-talker ASR.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Target-Talker ASR Results</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We evaluate target-talker ASR performance on two- and three-speaker subsets of LibriMix and LbriSpeechMix, as illustrated in Table <a href="#S4.T3" title="Table 3 â€£ 4.5 Limitations and Future Work â€£ 4 Results and Discussions â€£ Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We did not include results reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, which delivers better performance but undergoes about ten times training efforts as ours.</span></span></span> Our systems outperform previous state-of-the-art method on LibriMix dataset by a large margin, and we are the first to perform target-talker ASR task on Libri3Mix.
To guarantee a fair comparison, we further trained three additional systems with limited training data to ensure consistency with the data used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. These systems are denoted as "-limited".
The results demonstrate that our method still outperforms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> though under this restriction.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">For LibriSpeechMix, the speech signals are partially overlapped, which means the target talkerâ€™s speech can incur a considerable delay before it commences, resulting in a substantial time interval away between it and the enrollment speech.
Nevertheless, despite the existence of delays, our systems still demonstrate good performance on the LibriSpeechMix dataset, validating the effectiveness of the proposed method.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Zero-Shot Multi-Lingual Evaluation</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We investigated whether the multi-lingual characteristics of Whisper are retained after fine-tuning it on an English multi-talker dataset. Specifically, we conducted evaluations for systems (g) (i) (k) listed in Table 1 using the two-speaker AishellMix Mandarin dataset, which is the first time to be used on multi-talker ASR task.
The evaluations are performed using two schemes: zero-shot and one-batch-tuning. Zero-shot refers to directly evaluating the system on the AishellMix, while one-batch-tuning implies conducting an additional training epoch on the AishellMix training set prior to the evaluation.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">As Table 5 illustrates, the medium and large models demonstrated acceptable CER performance even under zero-shot conditions. With just one batch tuning, these models exhibited satisfactory results. This suggests that our method largely maintains the inherent multilingual capabilities of Whisper.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Study</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We investigated the optimal prompt length by examining the multi-talker ASR performance on the Libri2Mix dataset with Whisper-medium and Whisper-large models. As shown in Table <a href="#S4.T5" title="Table 5 â€£ 4.5 Limitations and Future Work â€£ 4 Results and Discussions â€£ Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech Recognition System" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, a soft prompt of length 4 yields the best performance. However, as the soft prompt length increases to 16, the systems see a decline in performance. This may be due to overly long sequence sequences, making the model difficult to optimize, given the original Whisper model is frozen.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Limitations and Future Work</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">This study has several limitations. Firstly, our method relies on PIT which requires pre-defining of the maximum number of speakers. Future efforts will integrate SOT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> or HEAT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> to address this issue and reduce training costs.
Secondly, when the target talker's speech undergoes excessive delay, there could be potential degradation in the target-talker ASR's performance. We anticipate future work to enable the TTI module synthesize information across the entire utterance duration rather than only the three-second enrollment speech.</p>
</div>
<figure id="S4.T2" class="ltx_table">

<figcaption class="ltx_caption ltx_align_left" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Multi-talker ASR on the test sets of LibriMix and LibriSpeechMix. Evaluated by WER (%). â€œSSâ€ denotes â€œSidecar Separatorâ€, â€œTTIâ€ denotes â€œTarget Talker Identifierâ€.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.T2.3" class="ltx_tabular ltx_align_left ltx_centering ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.3.1.1" class="ltx_tr">
<td id="S4.T2.3.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T2.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">LibriMix</span></td>
<td id="S4.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S4.T2.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">LibriSpeechMix</span></td>
</tr>
<tr id="S4.T2.3.2.2" class="ltx_tr">
<td id="S4.T2.3.2.2.1" class="ltx_td ltx_align_left"><span id="S4.T2.3.2.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">System</span></td>
<td id="S4.T2.3.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.2.2.2.1" class="ltx_text" style="font-size:80%;">2spk</span></td>
<td id="S4.T2.3.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.2.2.3.1" class="ltx_text" style="font-size:80%;">3spk</span></td>
<td id="S4.T2.3.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.2.2.4.1" class="ltx_text" style="font-size:80%;">2spk</span></td>
<td id="S4.T2.3.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.2.2.5.1" class="ltx_text" style="font-size:80%;">3spk</span></td>
</tr>
<tr id="S4.T2.3.3.3" class="ltx_tr">
<td id="S4.T2.3.3.3.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S4.T2.3.3.3.1.1" class="ltx_text" style="font-size:80%;">(a) WawLM Base+ PIT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.3.3.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S4.T2.3.3.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T2.3.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.3.3.2.1" class="ltx_text" style="font-size:80%;">18.45</span></td>
<td id="S4.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.3.3.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.3.3.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.3.3.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T2.3.4.4" class="ltx_tr">
<td id="S4.T2.3.4.4.1" class="ltx_td ltx_align_left">
<span id="S4.T2.3.4.4.1.1" class="ltx_text" style="font-size:80%;">(b) C-HuBERT-Large </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.3.4.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S4.T2.3.4.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T2.3.4.4.2" class="ltx_td ltx_align_center"><span id="S4.T2.3.4.4.2.1" class="ltx_text" style="font-size:80%;">7.80</span></td>
<td id="S4.T2.3.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T2.3.4.4.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.3.4.4.4" class="ltx_td ltx_align_center"><span id="S4.T2.3.4.4.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.3.4.4.5" class="ltx_td ltx_align_center"><span id="S4.T2.3.4.4.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T2.3.5.5" class="ltx_tr">
<td id="S4.T2.3.5.5.1" class="ltx_td ltx_align_left">
<span id="S4.T2.3.5.5.1.1" class="ltx_text" style="font-size:80%;">(c) SURT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.3.5.5.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.T2.3.5.5.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T2.3.5.5.2" class="ltx_td ltx_align_center"><span id="S4.T2.3.5.5.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.3.5.5.3" class="ltx_td ltx_align_center"><span id="S4.T2.3.5.5.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.3.5.5.4" class="ltx_td ltx_align_center"><span id="S4.T2.3.5.5.4.1" class="ltx_text" style="font-size:80%;">7.20</span></td>
<td id="S4.T2.3.5.5.5" class="ltx_td ltx_align_center"><span id="S4.T2.3.5.5.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T2.3.6.6" class="ltx_tr">
<td id="S4.T2.3.6.6.1" class="ltx_td ltx_align_left">
<span id="S4.T2.3.6.6.1.1" class="ltx_text" style="font-size:80%;">(d) SOT-Conformer </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.3.6.6.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib27" title="" class="ltx_ref">27</a><span id="S4.T2.3.6.6.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T2.3.6.6.2" class="ltx_td ltx_align_center"><span id="S4.T2.3.6.6.2.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.3.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T2.3.6.6.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.3.6.6.4" class="ltx_td ltx_align_center">
<span id="S4.T2.3.6.6.4.1" class="ltx_text" style="font-size:80%;">4.90</span><sup id="S4.T2.3.6.6.4.2" class="ltx_sup"><span id="S4.T2.3.6.6.4.2.1" class="ltx_text" style="font-size:80%;">â€ </span></sup>
</td>
<td id="S4.T2.3.6.6.5" class="ltx_td ltx_align_center"><span id="S4.T2.3.6.6.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">6.20<sup id="S4.T2.3.6.6.5.1.1" class="ltx_sup"><span id="S4.T2.3.6.6.5.1.1.1" class="ltx_text ltx_font_medium">â€ </span></sup></span></td>
</tr>
<tr id="S4.T2.3.7.7" class="ltx_tr">
<td id="S4.T2.3.7.7.1" class="ltx_td ltx_align_left">
<span id="S4.T2.3.7.7.1.1" class="ltx_text" style="font-size:80%;">(e) D2V-Sidecar-DB </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.3.7.7.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S4.T2.3.7.7.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T2.3.7.7.2" class="ltx_td ltx_align_center"><span id="S4.T2.3.7.7.2.1" class="ltx_text" style="font-size:80%;">9.69</span></td>
<td id="S4.T2.3.7.7.3" class="ltx_td ltx_align_center"><span id="S4.T2.3.7.7.3.1" class="ltx_text" style="font-size:80%;">33.91</span></td>
<td id="S4.T2.3.7.7.4" class="ltx_td ltx_align_center"><span id="S4.T2.3.7.7.4.1" class="ltx_text" style="font-size:80%;">7.49</span></td>
<td id="S4.T2.3.7.7.5" class="ltx_td ltx_align_center"><span id="S4.T2.3.7.7.5.1" class="ltx_text" style="font-size:80%;">11.94</span></td>
</tr>
<tr id="S4.T2.3.8.8" class="ltx_tr">
<td id="S4.T2.3.8.8.1" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T2.3.8.8.1.1" class="ltx_text" style="font-size:80%;">(f) Whisper-small-SS</span></td>
<td id="S4.T2.3.8.8.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.8.8.2.1" class="ltx_text" style="font-size:80%;">10.04</span></td>
<td id="S4.T2.3.8.8.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.8.8.3.1" class="ltx_text" style="font-size:80%;">29.20</span></td>
<td id="S4.T2.3.8.8.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.8.8.4.1" class="ltx_text" style="font-size:80%;">5.27</span></td>
<td id="S4.T2.3.8.8.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.3.8.8.5.1" class="ltx_text" style="font-size:80%;">9.85</span></td>
</tr>
<tr id="S4.T2.3.9.9" class="ltx_tr">
<td id="S4.T2.3.9.9.1" class="ltx_td ltx_align_left"><span id="S4.T2.3.9.9.1.1" class="ltx_text" style="font-size:80%;">(g) Whisper-small-SS-TTI</span></td>
<td id="S4.T2.3.9.9.2" class="ltx_td ltx_align_center"><span id="S4.T2.3.9.9.2.1" class="ltx_text" style="font-size:80%;">9.39</span></td>
<td id="S4.T2.3.9.9.3" class="ltx_td ltx_align_center"><span id="S4.T2.3.9.9.3.1" class="ltx_text" style="font-size:80%;">26.76</span></td>
<td id="S4.T2.3.9.9.4" class="ltx_td ltx_align_center"><span id="S4.T2.3.9.9.4.1" class="ltx_text" style="font-size:80%;">5.18</span></td>
<td id="S4.T2.3.9.9.5" class="ltx_td ltx_align_center"><span id="S4.T2.3.9.9.5.1" class="ltx_text" style="font-size:80%;">8.61</span></td>
</tr>
<tr id="S4.T2.3.10.10" class="ltx_tr">
<td id="S4.T2.3.10.10.1" class="ltx_td ltx_align_left"><span id="S4.T2.3.10.10.1.1" class="ltx_text" style="font-size:80%;">(h) Whisper-medium-SS</span></td>
<td id="S4.T2.3.10.10.2" class="ltx_td ltx_align_center"><span id="S4.T2.3.10.10.2.1" class="ltx_text" style="font-size:80%;">6.95</span></td>
<td id="S4.T2.3.10.10.3" class="ltx_td ltx_align_center"><span id="S4.T2.3.10.10.3.1" class="ltx_text" style="font-size:80%;">22.58</span></td>
<td id="S4.T2.3.10.10.4" class="ltx_td ltx_align_center"><span id="S4.T2.3.10.10.4.1" class="ltx_text" style="font-size:80%;">4.32</span></td>
<td id="S4.T2.3.10.10.5" class="ltx_td ltx_align_center"><span id="S4.T2.3.10.10.5.1" class="ltx_text" style="font-size:80%;">7.80</span></td>
</tr>
<tr id="S4.T2.3.11.11" class="ltx_tr">
<td id="S4.T2.3.11.11.1" class="ltx_td ltx_align_left"><span id="S4.T2.3.11.11.1.1" class="ltx_text" style="font-size:80%;">(i) Whisper-medium-SS-TTI</span></td>
<td id="S4.T2.3.11.11.2" class="ltx_td ltx_align_center"><span id="S4.T2.3.11.11.2.1" class="ltx_text" style="font-size:80%;">6.56</span></td>
<td id="S4.T2.3.11.11.3" class="ltx_td ltx_align_center"><span id="S4.T2.3.11.11.3.1" class="ltx_text" style="font-size:80%;">21.47</span></td>
<td id="S4.T2.3.11.11.4" class="ltx_td ltx_align_center"><span id="S4.T2.3.11.11.4.1" class="ltx_text" style="font-size:80%;">4.01</span></td>
<td id="S4.T2.3.11.11.5" class="ltx_td ltx_align_center"><span id="S4.T2.3.11.11.5.1" class="ltx_text" style="font-size:80%;">7.50</span></td>
</tr>
<tr id="S4.T2.3.12.12" class="ltx_tr">
<td id="S4.T2.3.12.12.1" class="ltx_td ltx_align_left"><span id="S4.T2.3.12.12.1.1" class="ltx_text" style="font-size:80%;">(j) Whisper-large-SS</span></td>
<td id="S4.T2.3.12.12.2" class="ltx_td ltx_align_center"><span id="S4.T2.3.12.12.2.1" class="ltx_text" style="font-size:80%;">4.98</span></td>
<td id="S4.T2.3.12.12.3" class="ltx_td ltx_align_center"><span id="S4.T2.3.12.12.3.1" class="ltx_text" style="font-size:80%;">17.55</span></td>
<td id="S4.T2.3.12.12.4" class="ltx_td ltx_align_center"><span id="S4.T2.3.12.12.4.1" class="ltx_text" style="font-size:80%;">3.81</span></td>
<td id="S4.T2.3.12.12.5" class="ltx_td ltx_align_center"><span id="S4.T2.3.12.12.5.1" class="ltx_text" style="font-size:80%;">7.13</span></td>
</tr>
<tr id="S4.T2.3.13.13" class="ltx_tr">
<td id="S4.T2.3.13.13.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T2.3.13.13.1.1" class="ltx_text" style="font-size:80%;">(k) Whisper-large-SS-TTI</span></td>
<td id="S4.T2.3.13.13.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.13.13.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">4.66</span></td>
<td id="S4.T2.3.13.13.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.13.13.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">16.79</span></td>
<td id="S4.T2.3.13.13.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.13.13.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">3.43</span></td>
<td id="S4.T2.3.13.13.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.13.13.5.1" class="ltx_text" style="font-size:80%;">6.80</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S4.I1" class="ltx_itemize ltx_align_left ltx_centering ltx_figure_panel">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><sup id="S4.I1.i1.p1.1.1" class="ltx_sup"><span id="S4.I1.i1.p1.1.1.1" class="ltx_text" style="font-size:80%;">â€ </span></sup><span id="S4.I1.i1.p1.1.2" class="ltx_text" style="font-size:80%;"> with extremely heavier training efforts.</span></p>
</div>
</li>
</ul>
</div>
</div>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Target-talker ASR on LibriMix and LibriSpeechMix. Evaluated by WER (%). "-limited" denotes using the same training data as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. </figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<th id="S4.T3.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"></th>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2"><span id="S4.T3.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">LibriMix</span></td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;" colspan="2"><span id="S4.T3.1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">LibriSpeechMix</span></td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<th id="S4.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.3.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">System</span></th>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.3.2.2.1" class="ltx_text" style="font-size:80%;">2spk</span></td>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.3.2.3.1" class="ltx_text" style="font-size:80%;">3spk</span></td>
<td id="S4.T3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.3.2.4.1" class="ltx_text" style="font-size:80%;">2spk</span></td>
<td id="S4.T3.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.3.2.5.1" class="ltx_text" style="font-size:80%;">3spk</span></td>
</tr>
<tr id="S4.T3.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T3.1.1.1.1" class="ltx_text" style="font-size:80%;">WavLM-Base</span><sup id="S4.T3.1.1.1.2" class="ltx_sup"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">+</span></sup><span id="S4.T3.1.1.1.3" class="ltx_text" style="font-size:80%;">-TSE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.1.1.4.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S4.T3.1.1.1.5.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.1.2.1" class="ltx_text" style="font-size:80%;">12.32</span></td>
<td id="S4.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.1.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.1.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.1.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<th id="S4.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;">
<span id="S4.T3.1.4.3.1.1" class="ltx_text" style="font-size:80%;">Whisper-TS-ASR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.4.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S4.T3.1.4.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</th>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.4.3.2.1" class="ltx_text" style="font-size:80%;">11.98</span></td>
<td id="S4.T3.1.4.3.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.4.3.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.1.4.3.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.4.3.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.1.4.3.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.4.3.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<th id="S4.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.5.4.1.1" class="ltx_text" style="font-size:80%;">Whisper-small-SS-TTI-limited</span></th>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.5.4.2.1" class="ltx_text" style="font-size:80%;">15.75</span></td>
<td id="S4.T3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.5.4.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.5.4.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.5.4.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T3.1.6.5" class="ltx_tr">
<th id="S4.T3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.6.5.1.1" class="ltx_text" style="font-size:80%;">Whisper-medium-SS-TTI-limited</span></th>
<td id="S4.T3.1.6.5.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.6.5.2.1" class="ltx_text" style="font-size:80%;">11.39</span></td>
<td id="S4.T3.1.6.5.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.6.5.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.1.6.5.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.6.5.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.1.6.5.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.6.5.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T3.1.7.6" class="ltx_tr">
<th id="S4.T3.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.7.6.1.1" class="ltx_text" style="font-size:80%;">Whisper-large-SS-TTI-limited</span></th>
<td id="S4.T3.1.7.6.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.7.6.2.1" class="ltx_text" style="font-size:80%;">10.79</span></td>
<td id="S4.T3.1.7.6.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.7.6.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.1.7.6.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.7.6.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T3.1.7.6.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.7.6.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T3.1.8.7" class="ltx_tr">
<th id="S4.T3.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.8.7.1.1" class="ltx_text" style="font-size:80%;">Whisper-small-SS-TTI</span></th>
<td id="S4.T3.1.8.7.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.8.7.2.1" class="ltx_text" style="font-size:80%;">11.81</span></td>
<td id="S4.T3.1.8.7.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.8.7.3.1" class="ltx_text" style="font-size:80%;">30.52</span></td>
<td id="S4.T3.1.8.7.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.8.7.4.1" class="ltx_text" style="font-size:80%;">8.89</span></td>
<td id="S4.T3.1.8.7.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.8.7.5.1" class="ltx_text" style="font-size:80%;">15.85</span></td>
</tr>
<tr id="S4.T3.1.9.8" class="ltx_tr">
<th id="S4.T3.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.9.8.1.1" class="ltx_text" style="font-size:80%;">Whisper-medium-SS-TTI</span></th>
<td id="S4.T3.1.9.8.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.9.8.2.1" class="ltx_text" style="font-size:80%;">9.14</span></td>
<td id="S4.T3.1.9.8.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.9.8.3.1" class="ltx_text" style="font-size:80%;">25.75</span></td>
<td id="S4.T3.1.9.8.4" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.9.8.4.1" class="ltx_text" style="font-size:80%;">7.58</span></td>
<td id="S4.T3.1.9.8.5" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.9.8.5.1" class="ltx_text" style="font-size:80%;">12.4</span></td>
</tr>
<tr id="S4.T3.1.10.9" class="ltx_tr">
<th id="S4.T3.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.10.9.1.1" class="ltx_text" style="font-size:80%;">Whisper-large-SS-TTI</span></th>
<td id="S4.T3.1.10.9.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.10.9.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">7.97</span></td>
<td id="S4.T3.1.10.9.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.10.9.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">21.97</span></td>
<td id="S4.T3.1.10.9.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.10.9.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">6.99</span></td>
<td id="S4.T3.1.10.9.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.0pt;padding-right:4.0pt;"><span id="S4.T3.1.10.9.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">11.4</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Zero-shot and one-batch-tuning multi-talker ASR on Aishell1Mix Mandarin dataset. Evaluated by CER (%).</figcaption>
<table id="S4.T4.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.3.1.1" class="ltx_tr">
<th id="S4.T4.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:10.2pt;padding-right:10.2pt;"><span id="S4.T4.3.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">System</span></th>
<th id="S4.T4.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:10.2pt;padding-right:10.2pt;"><span id="S4.T4.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">zero-shot</span></th>
<th id="S4.T4.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:10.2pt;padding-right:10.2pt;"><span id="S4.T4.3.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">one-batch-tuning</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.3.2.1" class="ltx_tr">
<th id="S4.T4.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:10.2pt;padding-right:10.2pt;"><span id="S4.T4.3.2.1.1.1" class="ltx_text" style="font-size:80%;">Whisper-small-SS-TTI</span></th>
<td id="S4.T4.3.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.2pt;padding-right:10.2pt;"><span id="S4.T4.3.2.1.2.1" class="ltx_text" style="font-size:80%;">55.87</span></td>
<td id="S4.T4.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:10.2pt;padding-right:10.2pt;"><span id="S4.T4.3.2.1.3.1" class="ltx_text" style="font-size:80%;">28.95</span></td>
</tr>
<tr id="S4.T4.3.3.2" class="ltx_tr">
<th id="S4.T4.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:10.2pt;padding-right:10.2pt;"><span id="S4.T4.3.3.2.1.1" class="ltx_text" style="font-size:80%;">Whisper-medium-SS-TTI</span></th>
<td id="S4.T4.3.3.2.2" class="ltx_td ltx_align_center" style="padding-left:10.2pt;padding-right:10.2pt;"><span id="S4.T4.3.3.2.2.1" class="ltx_text" style="font-size:80%;">36.28</span></td>
<td id="S4.T4.3.3.2.3" class="ltx_td ltx_align_center" style="padding-left:10.2pt;padding-right:10.2pt;"><span id="S4.T4.3.3.2.3.1" class="ltx_text" style="font-size:80%;">19.83</span></td>
</tr>
<tr id="S4.T4.3.4.3" class="ltx_tr">
<th id="S4.T4.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:10.2pt;padding-right:10.2pt;"><span id="S4.T4.3.4.3.1.1" class="ltx_text" style="font-size:80%;">Whisper-large-SS-TTI</span></th>
<td id="S4.T4.3.4.3.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.2pt;padding-right:10.2pt;"><span id="S4.T4.3.4.3.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">28.94</span></td>
<td id="S4.T4.3.4.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:10.2pt;padding-right:10.2pt;"><span id="S4.T4.3.4.3.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">17.81</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>Ablation study on soft prompt, evaluated by WER (%).</figcaption>
<table id="S4.T5.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.3.1.1" class="ltx_tr">
<th id="S4.T5.3.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:6.5pt;padding-right:6.5pt;"></th>
<th id="S4.T5.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:6.5pt;padding-right:6.5pt;" colspan="5"><span id="S4.T5.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Soft Prompt Length</span></th>
</tr>
<tr id="S4.T5.3.2.2" class="ltx_tr">
<th id="S4.T5.3.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" style="padding-left:6.5pt;padding-right:6.5pt;"><span id="S4.T5.3.2.2.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">System</span></th>
<th id="S4.T5.3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:6.5pt;padding-right:6.5pt;"><span id="S4.T5.3.2.2.2.1" class="ltx_text" style="font-size:80%;">0</span></th>
<th id="S4.T5.3.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:6.5pt;padding-right:6.5pt;"><span id="S4.T5.3.2.2.3.1" class="ltx_text" style="font-size:80%;">2</span></th>
<th id="S4.T5.3.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:6.5pt;padding-right:6.5pt;"><span id="S4.T5.3.2.2.4.1" class="ltx_text" style="font-size:80%;">4</span></th>
<th id="S4.T5.3.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:6.5pt;padding-right:6.5pt;"><span id="S4.T5.3.2.2.5.1" class="ltx_text" style="font-size:80%;">8</span></th>
<th id="S4.T5.3.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:6.5pt;padding-right:6.5pt;"><span id="S4.T5.3.2.2.6.1" class="ltx_text" style="font-size:80%;">16</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.3.3.1" class="ltx_tr">
<th id="S4.T5.3.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:6.5pt;padding-right:6.5pt;"><span id="S4.T5.3.3.1.1.1" class="ltx_text" style="font-size:80%;">Whisper-medium-SS-TTI</span></th>
<td id="S4.T5.3.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.5pt;padding-right:6.5pt;"><span id="S4.T5.3.3.1.2.1" class="ltx_text" style="font-size:80%;">7.21</span></td>
<td id="S4.T5.3.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.5pt;padding-right:6.5pt;"><span id="S4.T5.3.3.1.3.1" class="ltx_text" style="font-size:80%;">6.82</span></td>
<td id="S4.T5.3.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.5pt;padding-right:6.5pt;"><span id="S4.T5.3.3.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">6.56</span></td>
<td id="S4.T5.3.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.5pt;padding-right:6.5pt;"><span id="S4.T5.3.3.1.5.1" class="ltx_text" style="font-size:80%;">6.84</span></td>
<td id="S4.T5.3.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.5pt;padding-right:6.5pt;"><span id="S4.T5.3.3.1.6.1" class="ltx_text" style="font-size:80%;">7.5</span></td>
</tr>
<tr id="S4.T5.3.4.2" class="ltx_tr">
<th id="S4.T5.3.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:6.5pt;padding-right:6.5pt;"><span id="S4.T5.3.4.2.1.1" class="ltx_text" style="font-size:80%;">Whisper-large-SS-TTI</span></th>
<td id="S4.T5.3.4.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:6.5pt;padding-right:6.5pt;"><span id="S4.T5.3.4.2.2.1" class="ltx_text" style="font-size:80%;">5.27</span></td>
<td id="S4.T5.3.4.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:6.5pt;padding-right:6.5pt;"><span id="S4.T5.3.4.2.3.1" class="ltx_text" style="font-size:80%;">4.98</span></td>
<td id="S4.T5.3.4.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:6.5pt;padding-right:6.5pt;"><span id="S4.T5.3.4.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">4.66</span></td>
<td id="S4.T5.3.4.2.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:6.5pt;padding-right:6.5pt;"><span id="S4.T5.3.4.2.5.1" class="ltx_text" style="font-size:80%;">4.74</span></td>
<td id="S4.T5.3.4.2.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:6.5pt;padding-right:6.5pt;"><span id="S4.T5.3.4.2.6.1" class="ltx_text" style="font-size:80%;">5.43</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this study, we introduce a novel methodology that harnesses Whisper, a speech foundation model, to jointly transcribe multi-talker speech meanwhile highlighting the target talkerâ€™s
speech, without employing any speaker embedding extractor.
Specifically, we freeze whisper and insert a Sidecar separator into its encoder to separate mixed embedding for multiple talkers. Subsequently, a Target Talker Identifier module is introduced to identify the embedding flow of the target talker on the fly, requiring only three-second enrollment speech as a cue. The soft prompt tuning is further utilized to facilitate task adaptation.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Extensive experiments reveal that our approach outperforms previous methods on LibriMix and LibriSpeechMix on both tasks. Moreover, it achieves acceptable zero-shot performance on multi-talker ASR on AishellMix Mandarin dataset.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgements</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This research is partially supported by the HKSARG Research Grants Councilâ€™s Theme-based Research Grant Scheme (Project No. T45-407/19N) and by the CUHK Stanley Ho Big Data Decision Analytics Research Centre.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A.Â Radford, J.Â W. Kim <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Robust speech recognition via large-scale weak supervision,'' in <em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic">Proceedings of the 40th International Conference on Machine Learning</em>, vol. 202, 2023, pp. 28â€‰492â€“28â€‰518.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S.Â Settle, J.Â LeÂ Roux <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``End-to-end multi-speaker speech recognition,'' in <em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S.Â Li, B.Â Ouyang <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Real-time end-to-end monaural multi-speaker speech recognition,'' in <em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">Proc. Interspeech 2021</em>, 2021, pp. 3750â€“3754.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
J.Â Kang, L.Â Meng <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Cross-speaker encoding network for multi-talker speech recognition,'' in <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2024, pp. 1â€“5.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
H.Â Seki, T.Â Hori <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``A purely end-to-end system for multi-speaker speech recognition,'' in <em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic">ACL</em>, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
W.Â Zhang, X.Â Chang <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Improving end-to-end single-channel multi-talker speech recognition,'' <em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.Â 28, pp. 1385â€“1394, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
X.Â Chang, Y.Â Qian <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``End-to-end monaural multi-speaker ASR system without pretraining,'' in <em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
X.Â Chang, W.Â Zhang <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``End-to-end multi-speaker speech recognition with transformer,'' in <em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Z.Â Huang, D.Â Raj <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Adapting self-supervised models to multi-talker speech recognition using speaker embeddings,'' in <em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic">ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2023, pp. 1â€“5.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A.Â Tripathi, H.Â Lu <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``End-to-end multi-talker overlapping speech recognition,'' in <em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2020, pp. 6129â€“6133.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
L.Â Lu, N.Â Kanda <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Streaming end-to-end multi-talker speech recognition,'' <em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic">IEEE Signal Processing Letters</em>, vol.Â 28, pp. 803â€“807, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
D.Â Raj, D.Â Povey <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Surt 2.0: Advances in transducer-based multi-talker speech recognition,'' <em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.Â 31, pp. 3800â€“3813, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
N.Â Kanda, Y.Â Gaur <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Serialized output training for end-to-end overlapped speech recognition,'' in <em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic">Proc. Interspeech 2020</em>, 2020, pp. 2797â€“2801.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
N.Â Kanda, J.Â Wu <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Streaming Multi-Talker ASR with Token-Level Serialized Output Training,'' in <em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic">Proc. Interspeech 2022</em>, 2022, pp. 3774â€“3778.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
C.Â Li, Y.Â Qian <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Adapting Multi-Lingual ASR Models for Handling Multiple Talkers,'' in <em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH 2023</em>, 2023, pp. 1314â€“1318.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
A.Â Pasad, J.-C. Chou <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Layer-wise analysis of a self-supervised speech representation model,'' in <em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic">IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>, 2021, pp. 914â€“921.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
K.Â Shim, J.Â Choi <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Understanding the role of self attention for efficient speech recognition,'' in <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S.Â Chen, C.Â Wang <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Wavlm: Large-scale self-supervised pre-training for full stack speech processing,'' <em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, vol.Â 16, no.Â 6, pp. 1505â€“1518, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Y.Â Luo and N.Â Mesgarani, ``Conv-tasnet: Surpassing ideal timeâ€“frequency magnitude masking for speech separation,'' <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM transactions on audio, speech, and language processing</em>, vol.Â 27, no.Â 8, pp. 1256â€“1266, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
L.Â Meng, J.Â Kang <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``A sidecar separator can convert a single-talker speech recognition system to a multi-talker one,'' in <em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic">ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2023, pp. 1â€“5.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
L.Â Meng, J.Â Kang <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Unified Modeling of Multi-Talker Overlapped Speech Recognition and Diarization with a Sidecar Separator,'' in <em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH 2023</em>, 2023, pp. 3467â€“3471.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
W.Â Zhang and Y.Â Qian, ``Weakly-Supervised Speech Pre-training: A Case Study on Target Speech Recognition,'' in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH 2023</em>, 2023, pp. 3517â€“3521.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
T.Â Moriya, H.Â Sato <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Knowledge Distillation for Neural Transducer-based Target-Speaker ASR: Exploiting Parallel Mixture/Single-Talker Speech Data,'' in <em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH 2023</em>, 2023, pp. 899â€“903.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
H.Â Ma, Z.Â Peng <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Extending whisper with prompt tuning to target-speaker ASR,'' in <em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic">ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2024, pp. 1â€“5.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
R.Â Masumura, N.Â Makishima <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``End-to-End Joint Target and Non-Target Speakers ASR,'' in <em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH 2023</em>, 2023, pp. 2903â€“2907.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Y.Â Zhang, K.Â C. Puvvada <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Conformer-based target-speaker automatic speech recognition for single-channel audio,'' in <em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic">ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2023, pp. 1â€“5.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
N.Â Kanda, G.Â Ye <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``End-to-End Speaker-Attributed ASR with Transformer,'' in <em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic">Proc. Interspeech 2021</em>, 2021, pp. 4413â€“4417.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
A.Â Baevski, Y.Â Zhou <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``wav2vec 2.0: A framework for self-supervised learning of speech representations,'' in <em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol.Â 33, 2020, pp. 12â€‰449â€“12â€‰460.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
W.-N. Hsu, B.Â Bolte <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Hubert: Self-supervised speech representation learning by masked prediction of hidden units,'' <em id="bib.bib29.2.2" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.Â 29, pp. 3451â€“3460, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Y.Â Gong, S.Â Khurana <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers,'' in <em id="bib.bib30.2.2" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH 2023</em>, 2023, pp. 2798â€“2802.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
P.Â Peng, B.Â Yan <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization,'' in <em id="bib.bib31.2.2" class="ltx_emph ltx_font_italic">Proc. INTERSPEECH 2023</em>, 2023, pp. 396â€“400.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
B.Â Lester, R.Â Al-Rfou <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``The power of scale for parameter-efficient prompt tuning,'' in <em id="bib.bib32.2.2" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, Nov. 2021, pp. 3045â€“3059.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
D.Â Yu, M.Â KolbÃ¦k <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Permutation invariant training of deep models for speaker-independent multi-talker speech separation,'' in <em id="bib.bib33.2.2" class="ltx_emph ltx_font_italic">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2017, pp. 241â€“245.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
J.Â Cosentino, M.Â Pariente <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``LibriMix: An open-source dataset for generalizable speech separation,'' <em id="bib.bib34.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2005.11262</em>, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
M.Â Fazel-Zarandi and W.-N. Hsu, ``Cocktail hubert: Generalized self-supervised pre-training for mixture and single-source speech,'' in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2023, pp. 1â€“5.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.09816" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.09817" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.09817">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.09817" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.09818" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 14:50:56 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
