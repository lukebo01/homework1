<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.07930] Improving Multimodal Learning with Multi-Loss Gradient Modulation</title><meta property="og:description" content="Learning from multiple modalities, such as audio and video, offers opportunities for leveraging complementary information, enhancing robustness, and improving contextual understanding and performance. However, combininâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Improving Multimodal Learning with Multi-Loss Gradient Modulation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Improving Multimodal Learning with Multi-Loss Gradient Modulation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.07930">

<!--Generated on Wed Jun  5 16:06:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Improving Multimodal Learning with Multi-Loss Gradient Modulation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Konstantinos Kontras
<br class="ltx_break">ESAT, KU Leuven
<br class="ltx_break">Leuven, Belgium
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">konstantinos.kontras@kuleuven.be</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christos Chatzichristos
<br class="ltx_break">ESAT, KU Leuven
<br class="ltx_break">Leuven, Belgium
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">christos.chatzichristos@kuleuven.be</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Matthew Blaschko
<br class="ltx_break">ESAT, KU Leuven
<br class="ltx_break">Leuven, Belgium
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">matthew.blaschko@kuleuven.be</span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maarten De Vos
<br class="ltx_break">ESAT, KU Leuven
<br class="ltx_break">Leuven, Belgium
<br class="ltx_break"><span id="id4.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">maarten.devos@kuleuven.be</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Learning from multiple modalities, such as audio and video, offers opportunities for leveraging complementary information, enhancing robustness, and improving contextual understanding and performance. However, combining such modalities presents challenges, especially when modalities differ in data structure, predictive contribution, and the complexity of their learning processes. It has been observed that one modality can potentially dominate the learning process, hindering the effective utilization of information from other modalities and leading to sub-optimal model performance. To address this issue the vast majority of previous works suggest to assess the unimodal contributions and dynamically adjust the training to equalize them. We improve upon previous work by introducing a multi-loss objective and further refining the balancing process, allowing it to dynamically adjust the learning pace of each modality in both directions, acceleration and deceleration, with the ability to phase out balancing effects upon convergence. We achieve superior results across three audio-video datasets: on CREMA-D, models with ResNet backbone encoders surpass the previous best by 1.9% to 12.4%, and Conformer backbone models deliver improvements ranging from 2.8% to 14.1% across different fusion methods. On AVE, improvements range from 2.7% to 7.7%, while on UCF101, gains reach up to 6.1%.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Combining data from several modalities such as vision, text, audio, and time series has significantly improved performance across many tasks and has proven particularly advantageous in cases of noisy or unreliable sources <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">kontras2024core</span>]</cite>. However, studies show that the inclusion of a new modality doesnâ€™t always benefit, and can even impair, model performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>. As explained by Huang <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S1.p1.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>, different modalities compete with each other, resulting in underperforming modalities. These modalities exhibit inferior performance in their multimodal-trained encoders compared to their unimodal counterparts, assessing each modality encoder independently post-training, suggesting potential imbalances or inefficiencies in the integrated training process. This finding contradicts the assumption that more information necessarily improves task understanding.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To mitigate this effect, various balancing strategies have been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>. Previous methods typically use models with individual unimodal encoders and a fusion network that produces the multimodal output, generally falling into two main categories. Methods in the first category adjust the learning rate of unimodal encoders based on their estimated predictive performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>. Meanwhile, the methods of the second category employ additional loss functions derived from unimodal predictions and balance these losses during multimodal learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>. These losses enable accurate estimation of unimodal performance, however, these methods do not address the effects caused by the multimodal loss. We aim to bridge both approaches by incorporating additional losses and simultaneously adjusting the learning rates. Our methodology provides the following improvements and novelties:</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2405.07930/assets/Figures/Methods_BMVC_allmethods.png" id="S1.F1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="293" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.10.5.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.8.4" class="ltx_text" style="font-size:90%;">
Categorization of state-of-the-art balancing methodology: (a) Gradient Balancing methods use estimates of unimodal performance to calculate coefficients (<math id="S1.F1.5.1.m1.1" class="ltx_Math" alttext="k_{a}" display="inline"><semantics id="S1.F1.5.1.m1.1b"><msub id="S1.F1.5.1.m1.1.1" xref="S1.F1.5.1.m1.1.1.cmml"><mi id="S1.F1.5.1.m1.1.1.2" xref="S1.F1.5.1.m1.1.1.2.cmml">k</mi><mi id="S1.F1.5.1.m1.1.1.3" xref="S1.F1.5.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F1.5.1.m1.1c"><apply id="S1.F1.5.1.m1.1.1.cmml" xref="S1.F1.5.1.m1.1.1"><csymbol cd="ambiguous" id="S1.F1.5.1.m1.1.1.1.cmml" xref="S1.F1.5.1.m1.1.1">subscript</csymbol><ci id="S1.F1.5.1.m1.1.1.2.cmml" xref="S1.F1.5.1.m1.1.1.2">ğ‘˜</ci><ci id="S1.F1.5.1.m1.1.1.3.cmml" xref="S1.F1.5.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.5.1.m1.1d">k_{a}</annotation></semantics></math> and <math id="S1.F1.6.2.m2.1" class="ltx_Math" alttext="k_{v}" display="inline"><semantics id="S1.F1.6.2.m2.1b"><msub id="S1.F1.6.2.m2.1.1" xref="S1.F1.6.2.m2.1.1.cmml"><mi id="S1.F1.6.2.m2.1.1.2" xref="S1.F1.6.2.m2.1.1.2.cmml">k</mi><mi id="S1.F1.6.2.m2.1.1.3" xref="S1.F1.6.2.m2.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F1.6.2.m2.1c"><apply id="S1.F1.6.2.m2.1.1.cmml" xref="S1.F1.6.2.m2.1.1"><csymbol cd="ambiguous" id="S1.F1.6.2.m2.1.1.1.cmml" xref="S1.F1.6.2.m2.1.1">subscript</csymbol><ci id="S1.F1.6.2.m2.1.1.2.cmml" xref="S1.F1.6.2.m2.1.1.2">ğ‘˜</ci><ci id="S1.F1.6.2.m2.1.1.3.cmml" xref="S1.F1.6.2.m2.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.6.2.m2.1d">k_{v}</annotation></semantics></math>) and employ these to balance the multimodal loss. (b) Multi-Task methods incorporate unimodal classifiers into the model, each noted as CLS Head, to better estimate unimodal performance. The coefficients (<math id="S1.F1.7.3.m3.1" class="ltx_Math" alttext="k_{a}" display="inline"><semantics id="S1.F1.7.3.m3.1b"><msub id="S1.F1.7.3.m3.1.1" xref="S1.F1.7.3.m3.1.1.cmml"><mi id="S1.F1.7.3.m3.1.1.2" xref="S1.F1.7.3.m3.1.1.2.cmml">k</mi><mi id="S1.F1.7.3.m3.1.1.3" xref="S1.F1.7.3.m3.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F1.7.3.m3.1c"><apply id="S1.F1.7.3.m3.1.1.cmml" xref="S1.F1.7.3.m3.1.1"><csymbol cd="ambiguous" id="S1.F1.7.3.m3.1.1.1.cmml" xref="S1.F1.7.3.m3.1.1">subscript</csymbol><ci id="S1.F1.7.3.m3.1.1.2.cmml" xref="S1.F1.7.3.m3.1.1.2">ğ‘˜</ci><ci id="S1.F1.7.3.m3.1.1.3.cmml" xref="S1.F1.7.3.m3.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.7.3.m3.1d">k_{a}</annotation></semantics></math> and <math id="S1.F1.8.4.m4.1" class="ltx_Math" alttext="k_{v}" display="inline"><semantics id="S1.F1.8.4.m4.1b"><msub id="S1.F1.8.4.m4.1.1" xref="S1.F1.8.4.m4.1.1.cmml"><mi id="S1.F1.8.4.m4.1.1.2" xref="S1.F1.8.4.m4.1.1.2.cmml">k</mi><mi id="S1.F1.8.4.m4.1.1.3" xref="S1.F1.8.4.m4.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S1.F1.8.4.m4.1c"><apply id="S1.F1.8.4.m4.1.1.cmml" xref="S1.F1.8.4.m4.1.1"><csymbol cd="ambiguous" id="S1.F1.8.4.m4.1.1.1.cmml" xref="S1.F1.8.4.m4.1.1">subscript</csymbol><ci id="S1.F1.8.4.m4.1.1.2.cmml" xref="S1.F1.8.4.m4.1.1.2">ğ‘˜</ci><ci id="S1.F1.8.4.m4.1.1.3.cmml" xref="S1.F1.8.4.m4.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.8.4.m4.1d">k_{v}</annotation></semantics></math>) derived from comparing unimodal performance are used exclusively to balance the unimodal losses. (c) The proposed Multi-Loss Balanced method combines both strategies by incorporating unimodal classifiers for accurate unimodal performance estimation and balancing both multimodal and unimodal losses.
</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Throughout the entire training process, we employ a multi-objective loss that not only ensures each encoder converges close to its unimodal optimum, similarly to the methods described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, but also facilitates accurate unimodal performance estimation. We apply balancing for both multimodal and unimodal losses, distinguishing our method from any prior efforts in these categories.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">The proposed balancing technique, inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>, adaptively modifies the learning rates of encoders based on unimodal performance assessments. We enhance this strategy by enabling both acceleration and deceleration, tailored to the relative performance of the modalities.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">The balancing equations are designed such that when all unimodal encoders converge, the balancing naturally phases out, eliminating the need to pre-determine explicitly which epoch should mark the end of balancing.</p>
</div>
</li>
</ul>
<p id="S1.p3.1" class="ltx_p">Results conclusively show that the suggested method consistently outperforms balancing state-of-the-art methods across three video-audio datasets, employing either ResNet or Conformer backbone encoder models, and utilizing a range of fusion techniques.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>State-of-the-Art</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Previous methods addressing the challenge of modality competition in multimodal learning frameworks can mostly be broadly categorized into two primary groups: Gradient Balancing and Multi-Task methods. Those are illustrated in (a) and (b) of Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Improving Multimodal Learning with Multi-Loss Gradient Modulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Additionally, methods that donâ€™t fit into either category are described in SectionÂ <a href="#S2.SS3" title="2.3 Other Approaches â€£ 2 State-of-the-Art â€£ Improving Multimodal Learning with Multi-Loss Gradient Modulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Gradient Balancing Techniques</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In this category, methods address modality competition by adjusting the unimodal encodersâ€™ learning rates. The Modality-specific Learning Rates (<span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_bold">MSLR</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite> strategy introduces an approach for decision-level fusion models which dynamically adjust encoder gradients magnitude based on recent validation accuracy, independently for each modality. Building on this foundation, On-the-fly Gradient Modulation (<span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_bold">OGM</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> introduces an interactive framework, comparing performance improvements across modalities to tailor the adjustment of each encodersâ€™ learning rate. These methods are designed for late fusion models, allowing direct access to unimodal performance; however, they cannot directly be applied in more complex fusion schemes. Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite> partially belong to this category, as they propose a method that monitors the learning speed of each modality through gradient norms while using isolated phases of unimodal training to balance multimodal learning. However, measuring this learning speed requires a specialized model architecture, which prevents the use of a shared multimodal output, limiting its application.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Multi-task Learning</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The methods that belong in this category target the sub-optimal unimodal encoders by employing dedicated losses for each modality. It has been observed that incorporating unimodal classifiers enhances multimodal learning, as demonstrated in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">kontras2024core</span>]</cite>. Building on this foundation, Wang <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.p1.1.2" class="ltx_text"></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> suggest to dynamically adjust the weights of the unimodal losses based on an overfitting-to-generalization ratio, however accessing that information requires a separate validation set. Du <em id="S2.SS2.p1.1.3" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS2.p1.1.4" class="ltx_text"></span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> introduce a teacher-student schema with distillation losses for direct guidance of unimodal encoders. The Prototypical Modal Rebalance (<span id="S2.SS2.p1.1.5" class="ltx_text ltx_font_bold">PMR</span>) method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite> circumvents the use of additional parameters for classifiers by leveraging prototype-based classifiers and distance-based losses. This approach maintains a similar logic, wherein imbalanced performance is compensated for by the appropriate unimodal loss. However, balancing the unimodal losses alone disregards the effects caused by the multimodal loss.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Other Approaches</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Additional studies explore further approaches to multimodal learning, not fitting neatly into previously discussed categories. <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_bold">MMCosine</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> preconditions late fusion by standardizing both the feature vectors and the weights dedicated to each modality, equalizing their contribution to the final prediction. Gat <em id="S2.SS3.p1.1.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S2.SS3.p1.1.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> explore a regularization technique to enhance each modalityâ€™s contribution by maximizing functional entropy, estimated through prediction differences after input perturbations. This method, however, increases the modelâ€™s sensitivity to those perturbations. Adaptive Gradient Modulation (<span id="S2.SS3.p1.1.4" class="ltx_text ltx_font_bold">AGM</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> takes the contribution of each modality by utilizing zero-masking Shapley <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">shapley1953value</span>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> values. This allows for balancing on models of any structure, however at the cost of increased computational demands due to multiple forward passes.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Model</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We have endeavored to maintain our methodology as model-agnostic as possible, albeit with a few necessary assumptions. Our focus centers on multimodal models, wherein each modality is processed by a dedicated encoder. We aim to enhance the training efficiency of these encoders, thereby indirectly benefiting the entire networkâ€™s performance. To elucidate, our models conform to a unified structural framework.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.6" class="ltx_p">Given a dataset <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">D</annotation></semantics></math> consisting of <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">N</annotation></semantics></math> samples across <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">M</annotation></semantics></math> modalities, denoted as <math id="S3.SS1.p2.4.m4.1" class="ltx_math_unparsed" alttext="X=\{X_{1},..,X_{M}\}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1b"><mi id="S3.SS1.p2.4.m4.1.1">X</mi><mo id="S3.SS1.p2.4.m4.1.2">=</mo><mrow id="S3.SS1.p2.4.m4.1.3"><mo stretchy="false" id="S3.SS1.p2.4.m4.1.3.1">{</mo><msub id="S3.SS1.p2.4.m4.1.3.2"><mi id="S3.SS1.p2.4.m4.1.3.2.2">X</mi><mn id="S3.SS1.p2.4.m4.1.3.2.3">1</mn></msub><mo id="S3.SS1.p2.4.m4.1.3.3">,</mo><mo lspace="0em" rspace="0.0835em" id="S3.SS1.p2.4.m4.1.3.4">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S3.SS1.p2.4.m4.1.3.5">.</mo><mo id="S3.SS1.p2.4.m4.1.3.6">,</mo><msub id="S3.SS1.p2.4.m4.1.3.7"><mi id="S3.SS1.p2.4.m4.1.3.7.2">X</mi><mi id="S3.SS1.p2.4.m4.1.3.7.3">M</mi></msub><mo stretchy="false" id="S3.SS1.p2.4.m4.1.3.8">}</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">X=\{X_{1},..,X_{M}\}</annotation></semantics></math>, each sharing a common ground truth label <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mi id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">ğ‘Œ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">Y</annotation></semantics></math> with <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><mi id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><ci id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">C</annotation></semantics></math> distinct classes, we employ networks structured as follows:</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_math_unparsed" alttext="f(X;\theta)=f_{v}(f_{1}(X_{1};\theta_{1}),..,f_{M}(X_{M};\theta_{M});\theta_{v})," display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2b"><mi id="S3.E1.m1.2.3">f</mi><mrow id="S3.E1.m1.2.4"><mo stretchy="false" id="S3.E1.m1.2.4.1">(</mo><mi id="S3.E1.m1.1.1">X</mi><mo id="S3.E1.m1.2.4.2">;</mo><mi id="S3.E1.m1.2.2">Î¸</mi><mo stretchy="false" id="S3.E1.m1.2.4.3">)</mo></mrow><mo id="S3.E1.m1.2.5">=</mo><msub id="S3.E1.m1.2.6"><mi id="S3.E1.m1.2.6.2">f</mi><mi id="S3.E1.m1.2.6.3">v</mi></msub><mrow id="S3.E1.m1.2.7"><mo stretchy="false" id="S3.E1.m1.2.7.1">(</mo><msub id="S3.E1.m1.2.7.2"><mi id="S3.E1.m1.2.7.2.2">f</mi><mn id="S3.E1.m1.2.7.2.3">1</mn></msub><mrow id="S3.E1.m1.2.7.3"><mo stretchy="false" id="S3.E1.m1.2.7.3.1">(</mo><msub id="S3.E1.m1.2.7.3.2"><mi id="S3.E1.m1.2.7.3.2.2">X</mi><mn id="S3.E1.m1.2.7.3.2.3">1</mn></msub><mo id="S3.E1.m1.2.7.3.3">;</mo><msub id="S3.E1.m1.2.7.3.4"><mi id="S3.E1.m1.2.7.3.4.2">Î¸</mi><mn id="S3.E1.m1.2.7.3.4.3">1</mn></msub><mo stretchy="false" id="S3.E1.m1.2.7.3.5">)</mo></mrow><mo id="S3.E1.m1.2.7.4">,</mo><mo lspace="0em" rspace="0.0835em" id="S3.E1.m1.2.7.5">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S3.E1.m1.2.7.6">.</mo><mo id="S3.E1.m1.2.7.7">,</mo><msub id="S3.E1.m1.2.7.8"><mi id="S3.E1.m1.2.7.8.2">f</mi><mi id="S3.E1.m1.2.7.8.3">M</mi></msub><mrow id="S3.E1.m1.2.7.9"><mo stretchy="false" id="S3.E1.m1.2.7.9.1">(</mo><msub id="S3.E1.m1.2.7.9.2"><mi id="S3.E1.m1.2.7.9.2.2">X</mi><mi id="S3.E1.m1.2.7.9.2.3">M</mi></msub><mo id="S3.E1.m1.2.7.9.3">;</mo><msub id="S3.E1.m1.2.7.9.4"><mi id="S3.E1.m1.2.7.9.4.2">Î¸</mi><mi id="S3.E1.m1.2.7.9.4.3">M</mi></msub><mo stretchy="false" id="S3.E1.m1.2.7.9.5">)</mo></mrow><mo id="S3.E1.m1.2.7.10">;</mo><msub id="S3.E1.m1.2.7.11"><mi id="S3.E1.m1.2.7.11.2">Î¸</mi><mi id="S3.E1.m1.2.7.11.3">v</mi></msub><mo stretchy="false" id="S3.E1.m1.2.7.12">)</mo></mrow><mo id="S3.E1.m1.2.8">,</mo></mrow><annotation encoding="application/x-tex" id="S3.E1.m1.2c">f(X;\theta)=f_{v}(f_{1}(X_{1};\theta_{1}),..,f_{M}(X_{M};\theta_{M});\theta_{v}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.5" class="ltx_p">where <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mi id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">\theta</annotation></semantics></math> denotes all the parameters of the network, <math id="S3.SS1.p4.2.m2.1" class="ltx_math_unparsed" alttext="\theta_{1},..,\theta_{M}" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mrow id="S3.SS1.p4.2.m2.1b"><msub id="S3.SS1.p4.2.m2.1.1"><mi id="S3.SS1.p4.2.m2.1.1.2">Î¸</mi><mn id="S3.SS1.p4.2.m2.1.1.3">1</mn></msub><mo id="S3.SS1.p4.2.m2.1.2">,</mo><mo lspace="0em" rspace="0.0835em" id="S3.SS1.p4.2.m2.1.3">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S3.SS1.p4.2.m2.1.4">.</mo><mo id="S3.SS1.p4.2.m2.1.5">,</mo><msub id="S3.SS1.p4.2.m2.1.6"><mi id="S3.SS1.p4.2.m2.1.6.2">Î¸</mi><mi id="S3.SS1.p4.2.m2.1.6.3">M</mi></msub></mrow><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">\theta_{1},..,\theta_{M}</annotation></semantics></math> the unimodal encodersâ€™, <math id="S3.SS1.p4.3.m3.1" class="ltx_math_unparsed" alttext="f_{1},..,f_{M}" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mrow id="S3.SS1.p4.3.m3.1b"><msub id="S3.SS1.p4.3.m3.1.1"><mi id="S3.SS1.p4.3.m3.1.1.2">f</mi><mn id="S3.SS1.p4.3.m3.1.1.3">1</mn></msub><mo id="S3.SS1.p4.3.m3.1.2">,</mo><mo lspace="0em" rspace="0.0835em" id="S3.SS1.p4.3.m3.1.3">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S3.SS1.p4.3.m3.1.4">.</mo><mo id="S3.SS1.p4.3.m3.1.5">,</mo><msub id="S3.SS1.p4.3.m3.1.6"><mi id="S3.SS1.p4.3.m3.1.6.2">f</mi><mi id="S3.SS1.p4.3.m3.1.6.3">M</mi></msub></mrow><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">f_{1},..,f_{M}</annotation></semantics></math> parameters and <math id="S3.SS1.p4.4.m4.1" class="ltx_Math" alttext="\theta_{v}" display="inline"><semantics id="S3.SS1.p4.4.m4.1a"><msub id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml"><mi id="S3.SS1.p4.4.m4.1.1.2" xref="S3.SS1.p4.4.m4.1.1.2.cmml">Î¸</mi><mi id="S3.SS1.p4.4.m4.1.1.3" xref="S3.SS1.p4.4.m4.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><apply id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.1.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p4.4.m4.1.1.2.cmml" xref="S3.SS1.p4.4.m4.1.1.2">ğœƒ</ci><ci id="S3.SS1.p4.4.m4.1.1.3.cmml" xref="S3.SS1.p4.4.m4.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">\theta_{v}</annotation></semantics></math> the common parameters of the common fusion function <math id="S3.SS1.p4.5.m5.1" class="ltx_Math" alttext="f_{v}" display="inline"><semantics id="S3.SS1.p4.5.m5.1a"><msub id="S3.SS1.p4.5.m5.1.1" xref="S3.SS1.p4.5.m5.1.1.cmml"><mi id="S3.SS1.p4.5.m5.1.1.2" xref="S3.SS1.p4.5.m5.1.1.2.cmml">f</mi><mi id="S3.SS1.p4.5.m5.1.1.3" xref="S3.SS1.p4.5.m5.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.5.m5.1b"><apply id="S3.SS1.p4.5.m5.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.5.m5.1.1.1.cmml" xref="S3.SS1.p4.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p4.5.m5.1.1.2.cmml" xref="S3.SS1.p4.5.m5.1.1.2">ğ‘“</ci><ci id="S3.SS1.p4.5.m5.1.1.3.cmml" xref="S3.SS1.p4.5.m5.1.1.3">ğ‘£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.5.m5.1c">f_{v}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Balancing</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Our objective is to achieve synchronous convergence of the unimodal encoders, based on the hypothesis that this will prevent the model from overfitting to any single modality and may encourage the development of more synergistic behaviors.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.6" class="ltx_p">To achieve this, following prior studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>, we dynamically adjust the learning rates of each unimodal encoder based on the comparative analysis of unimodal performances. We estimate balancing coefficients for each modality <math id="S3.SS2.p2.1.m1.1" class="ltx_math_unparsed" alttext="i=1,..,M" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1b"><mi id="S3.SS2.p2.1.m1.1.1">i</mi><mo id="S3.SS2.p2.1.m1.1.2">=</mo><mn id="S3.SS2.p2.1.m1.1.3">1</mn><mo id="S3.SS2.p2.1.m1.1.4">,</mo><mo lspace="0em" rspace="0.0835em" id="S3.SS2.p2.1.m1.1.5">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S3.SS2.p2.1.m1.1.6">.</mo><mo id="S3.SS2.p2.1.m1.1.7">,</mo><mi id="S3.SS2.p2.1.m1.1.8">M</mi></mrow><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">i=1,..,M</annotation></semantics></math> as <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="k_{i}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><msub id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">k</mi><mi id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">ğ‘˜</ci><ci id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">k_{i}</annotation></semantics></math> that indicate how much each modality should change the learning pace of its encoder. When <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="k_{i}&gt;1" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><msub id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2.2" xref="S3.SS2.p2.3.m3.1.1.2.2.cmml">k</mi><mi id="S3.SS2.p2.3.m3.1.1.2.3" xref="S3.SS2.p2.3.m3.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">&gt;</mo><mn id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><gt id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></gt><apply id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.2.1.cmml" xref="S3.SS2.p2.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.2.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2.2">ğ‘˜</ci><ci id="S3.SS2.p2.3.m3.1.1.2.3.cmml" xref="S3.SS2.p2.3.m3.1.1.2.3">ğ‘–</ci></apply><cn type="integer" id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">k_{i}&gt;1</annotation></semantics></math>, we accelerate the learning of the modality, while when <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="k_{i}&lt;1" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mrow id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><msub id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2.2" xref="S3.SS2.p2.4.m4.1.1.2.2.cmml">k</mi><mi id="S3.SS2.p2.4.m4.1.1.2.3" xref="S3.SS2.p2.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS2.p2.4.m4.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.cmml">&lt;</mo><mn id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><lt id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1"></lt><apply id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.2.1.cmml" xref="S3.SS2.p2.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2.2">ğ‘˜</ci><ci id="S3.SS2.p2.4.m4.1.1.2.3.cmml" xref="S3.SS2.p2.4.m4.1.1.2.3">ğ‘–</ci></apply><cn type="integer" id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">k_{i}&lt;1</annotation></semantics></math>, we decelerate it. The update rule using stochastic gradient descent (SGD), the initial learning rate <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="lr_{\text{base}}" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mrow id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.5.m5.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.cmml">â€‹</mo><msub id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml"><mi id="S3.SS2.p2.5.m5.1.1.3.2" xref="S3.SS2.p2.5.m5.1.1.3.2.cmml">r</mi><mtext id="S3.SS2.p2.5.m5.1.1.3.3" xref="S3.SS2.p2.5.m5.1.1.3.3a.cmml">base</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><times id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1"></times><ci id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">ğ‘™</ci><apply id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.3.1.cmml" xref="S3.SS2.p2.5.m5.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.3.2.cmml" xref="S3.SS2.p2.5.m5.1.1.3.2">ğ‘Ÿ</ci><ci id="S3.SS2.p2.5.m5.1.1.3.3a.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3"><mtext mathsize="70%" id="S3.SS2.p2.5.m5.1.1.3.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3">base</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">lr_{\text{base}}</annotation></semantics></math> and the loss function <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><mi id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><ci id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">L</annotation></semantics></math> would then be:</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.4" class="ltx_Math" alttext="\Delta\theta_{i}=-lr_{\text{base}}\cdot k_{i}\cdot\nabla L(X,y,\theta)." display="block"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4.1" xref="S3.E2.m1.4.4.1.1.cmml"><mrow id="S3.E2.m1.4.4.1.1" xref="S3.E2.m1.4.4.1.1.cmml"><mrow id="S3.E2.m1.4.4.1.1.2" xref="S3.E2.m1.4.4.1.1.2.cmml"><mi mathvariant="normal" id="S3.E2.m1.4.4.1.1.2.2" xref="S3.E2.m1.4.4.1.1.2.2.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.1.1.2.1" xref="S3.E2.m1.4.4.1.1.2.1.cmml">â€‹</mo><msub id="S3.E2.m1.4.4.1.1.2.3" xref="S3.E2.m1.4.4.1.1.2.3.cmml"><mi id="S3.E2.m1.4.4.1.1.2.3.2" xref="S3.E2.m1.4.4.1.1.2.3.2.cmml">Î¸</mi><mi id="S3.E2.m1.4.4.1.1.2.3.3" xref="S3.E2.m1.4.4.1.1.2.3.3.cmml">i</mi></msub></mrow><mo id="S3.E2.m1.4.4.1.1.1" xref="S3.E2.m1.4.4.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.4.4.1.1.3" xref="S3.E2.m1.4.4.1.1.3.cmml"><mo id="S3.E2.m1.4.4.1.1.3a" xref="S3.E2.m1.4.4.1.1.3.cmml">âˆ’</mo><mrow id="S3.E2.m1.4.4.1.1.3.2" xref="S3.E2.m1.4.4.1.1.3.2.cmml"><mrow id="S3.E2.m1.4.4.1.1.3.2.2" xref="S3.E2.m1.4.4.1.1.3.2.2.cmml"><mrow id="S3.E2.m1.4.4.1.1.3.2.2.2" xref="S3.E2.m1.4.4.1.1.3.2.2.2.cmml"><mi id="S3.E2.m1.4.4.1.1.3.2.2.2.2" xref="S3.E2.m1.4.4.1.1.3.2.2.2.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.1.1.3.2.2.2.1" xref="S3.E2.m1.4.4.1.1.3.2.2.2.1.cmml">â€‹</mo><msub id="S3.E2.m1.4.4.1.1.3.2.2.2.3" xref="S3.E2.m1.4.4.1.1.3.2.2.2.3.cmml"><mi id="S3.E2.m1.4.4.1.1.3.2.2.2.3.2" xref="S3.E2.m1.4.4.1.1.3.2.2.2.3.2.cmml">r</mi><mtext id="S3.E2.m1.4.4.1.1.3.2.2.2.3.3" xref="S3.E2.m1.4.4.1.1.3.2.2.2.3.3a.cmml">base</mtext></msub></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.4.4.1.1.3.2.2.1" xref="S3.E2.m1.4.4.1.1.3.2.2.1.cmml">â‹…</mo><msub id="S3.E2.m1.4.4.1.1.3.2.2.3" xref="S3.E2.m1.4.4.1.1.3.2.2.3.cmml"><mi id="S3.E2.m1.4.4.1.1.3.2.2.3.2" xref="S3.E2.m1.4.4.1.1.3.2.2.3.2.cmml">k</mi><mi id="S3.E2.m1.4.4.1.1.3.2.2.3.3" xref="S3.E2.m1.4.4.1.1.3.2.2.3.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.4.4.1.1.3.2.2.1a" xref="S3.E2.m1.4.4.1.1.3.2.2.1.cmml">â‹…</mo><mrow id="S3.E2.m1.4.4.1.1.3.2.2.4" xref="S3.E2.m1.4.4.1.1.3.2.2.4.cmml"><mo rspace="0.167em" id="S3.E2.m1.4.4.1.1.3.2.2.4.1" xref="S3.E2.m1.4.4.1.1.3.2.2.4.1.cmml">âˆ‡</mo><mi id="S3.E2.m1.4.4.1.1.3.2.2.4.2" xref="S3.E2.m1.4.4.1.1.3.2.2.4.2.cmml">L</mi></mrow></mrow><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.1.1.3.2.1" xref="S3.E2.m1.4.4.1.1.3.2.1.cmml">â€‹</mo><mrow id="S3.E2.m1.4.4.1.1.3.2.3.2" xref="S3.E2.m1.4.4.1.1.3.2.3.1.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.1.1.3.2.3.2.1" xref="S3.E2.m1.4.4.1.1.3.2.3.1.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">X</mi><mo id="S3.E2.m1.4.4.1.1.3.2.3.2.2" xref="S3.E2.m1.4.4.1.1.3.2.3.1.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">y</mi><mo id="S3.E2.m1.4.4.1.1.3.2.3.2.3" xref="S3.E2.m1.4.4.1.1.3.2.3.1.cmml">,</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">Î¸</mi><mo stretchy="false" id="S3.E2.m1.4.4.1.1.3.2.3.2.4" xref="S3.E2.m1.4.4.1.1.3.2.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.E2.m1.4.4.1.2" xref="S3.E2.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.1.1.cmml" xref="S3.E2.m1.4.4.1"><eq id="S3.E2.m1.4.4.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1"></eq><apply id="S3.E2.m1.4.4.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.2"><times id="S3.E2.m1.4.4.1.1.2.1.cmml" xref="S3.E2.m1.4.4.1.1.2.1"></times><ci id="S3.E2.m1.4.4.1.1.2.2.cmml" xref="S3.E2.m1.4.4.1.1.2.2">Î”</ci><apply id="S3.E2.m1.4.4.1.1.2.3.cmml" xref="S3.E2.m1.4.4.1.1.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.2.3.1.cmml" xref="S3.E2.m1.4.4.1.1.2.3">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.2.3.2.cmml" xref="S3.E2.m1.4.4.1.1.2.3.2">ğœƒ</ci><ci id="S3.E2.m1.4.4.1.1.2.3.3.cmml" xref="S3.E2.m1.4.4.1.1.2.3.3">ğ‘–</ci></apply></apply><apply id="S3.E2.m1.4.4.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.3"><minus id="S3.E2.m1.4.4.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.3"></minus><apply id="S3.E2.m1.4.4.1.1.3.2.cmml" xref="S3.E2.m1.4.4.1.1.3.2"><times id="S3.E2.m1.4.4.1.1.3.2.1.cmml" xref="S3.E2.m1.4.4.1.1.3.2.1"></times><apply id="S3.E2.m1.4.4.1.1.3.2.2.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2"><ci id="S3.E2.m1.4.4.1.1.3.2.2.1.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.1">â‹…</ci><apply id="S3.E2.m1.4.4.1.1.3.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.2"><times id="S3.E2.m1.4.4.1.1.3.2.2.2.1.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.2.1"></times><ci id="S3.E2.m1.4.4.1.1.3.2.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.2.2">ğ‘™</ci><apply id="S3.E2.m1.4.4.1.1.3.2.2.2.3.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.3.2.2.2.3.1.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.2.3">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.3.2.2.2.3.2.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.2.3.2">ğ‘Ÿ</ci><ci id="S3.E2.m1.4.4.1.1.3.2.2.2.3.3a.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.2.3.3"><mtext mathsize="70%" id="S3.E2.m1.4.4.1.1.3.2.2.2.3.3.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.2.3.3">base</mtext></ci></apply></apply><apply id="S3.E2.m1.4.4.1.1.3.2.2.3.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.3.2.2.3.1.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.3">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.3.2.2.3.2.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.3.2">ğ‘˜</ci><ci id="S3.E2.m1.4.4.1.1.3.2.2.3.3.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.3.3">ğ‘–</ci></apply><apply id="S3.E2.m1.4.4.1.1.3.2.2.4.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.4"><ci id="S3.E2.m1.4.4.1.1.3.2.2.4.1.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.4.1">âˆ‡</ci><ci id="S3.E2.m1.4.4.1.1.3.2.2.4.2.cmml" xref="S3.E2.m1.4.4.1.1.3.2.2.4.2">ğ¿</ci></apply></apply><vector id="S3.E2.m1.4.4.1.1.3.2.3.1.cmml" xref="S3.E2.m1.4.4.1.1.3.2.3.2"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">ğ‘‹</ci><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">ğ‘¦</ci><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">ğœƒ</ci></vector></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">\Delta\theta_{i}=-lr_{\text{base}}\cdot k_{i}\cdot\nabla L(X,y,\theta).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">We improve upon previous works in three points. Firstly, we incorporate a multi-task objective with additional unimodal losses. Such objective ensures precise assessment of unimodal performance, while aiding the convergence of the unimodal classifiers. Secondly, our method allows for both the acceleration and deceleration of learning across the modalities, without ever entirely halting their progress. Finally, while previous methods imposed a hard limit to cease balancing after a predetermined number of epochs, we employ a function that naturally reduces balancing effects as the performances of both modalities converge.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.2" class="ltx_p">Our multi-loss objective <math id="S3.SS2.p5.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS2.p5.1.m1.1a"><mi id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><ci id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">L</annotation></semantics></math> is a summation of a cross-entropy (<math id="S3.SS2.p5.2.m2.1" class="ltx_Math" alttext="CE" display="inline"><semantics id="S3.SS2.p5.2.m2.1a"><mrow id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml"><mi id="S3.SS2.p5.2.m2.1.1.2" xref="S3.SS2.p5.2.m2.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.2.m2.1.1.1" xref="S3.SS2.p5.2.m2.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p5.2.m2.1.1.3" xref="S3.SS2.p5.2.m2.1.1.3.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><apply id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1"><times id="S3.SS2.p5.2.m2.1.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1.1"></times><ci id="S3.SS2.p5.2.m2.1.1.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2">ğ¶</ci><ci id="S3.SS2.p5.2.m2.1.1.3.cmml" xref="S3.SS2.p5.2.m2.1.1.3">ğ¸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">CE</annotation></semantics></math>) loss for the multimodal predictions and similar cross-entropy losses for each unimodal prediction. This can be expressed as:</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.5" class="ltx_Math" alttext="L=CE(f(X;\theta),y)+\sum^{M}_{i=1}{CE(f_{i}(X_{i};\theta_{i}),y)}." display="block"><semantics id="S3.E3.m1.5a"><mrow id="S3.E3.m1.5.5.1" xref="S3.E3.m1.5.5.1.1.cmml"><mrow id="S3.E3.m1.5.5.1.1" xref="S3.E3.m1.5.5.1.1.cmml"><mi id="S3.E3.m1.5.5.1.1.4" xref="S3.E3.m1.5.5.1.1.4.cmml">L</mi><mo id="S3.E3.m1.5.5.1.1.3" xref="S3.E3.m1.5.5.1.1.3.cmml">=</mo><mrow id="S3.E3.m1.5.5.1.1.2" xref="S3.E3.m1.5.5.1.1.2.cmml"><mrow id="S3.E3.m1.5.5.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.cmml"><mi id="S3.E3.m1.5.5.1.1.1.1.3" xref="S3.E3.m1.5.5.1.1.1.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.5.5.1.1.1.1.2" xref="S3.E3.m1.5.5.1.1.1.1.2.cmml">â€‹</mo><mi id="S3.E3.m1.5.5.1.1.1.1.4" xref="S3.E3.m1.5.5.1.1.1.1.4.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.5.5.1.1.1.1.2a" xref="S3.E3.m1.5.5.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E3.m1.5.5.1.1.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m1.5.5.1.1.1.1.1.1.2" xref="S3.E3.m1.5.5.1.1.1.1.1.2.cmml">(</mo><mrow id="S3.E3.m1.5.5.1.1.1.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.5.5.1.1.1.1.1.1.1.2" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml">â€‹</mo><mrow id="S3.E3.m1.5.5.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.3.1.cmml"><mo stretchy="false" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.3.2.1" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.3.1.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">X</mi><mo id="S3.E3.m1.5.5.1.1.1.1.1.1.1.3.2.2" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.3.1.cmml">;</mo><mi id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml">Î¸</mi><mo stretchy="false" id="S3.E3.m1.5.5.1.1.1.1.1.1.1.3.2.3" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.5.5.1.1.1.1.1.1.3" xref="S3.E3.m1.5.5.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E3.m1.3.3" xref="S3.E3.m1.3.3.cmml">y</mi><mo stretchy="false" id="S3.E3.m1.5.5.1.1.1.1.1.1.4" xref="S3.E3.m1.5.5.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo rspace="0.055em" id="S3.E3.m1.5.5.1.1.2.3" xref="S3.E3.m1.5.5.1.1.2.3.cmml">+</mo><mrow id="S3.E3.m1.5.5.1.1.2.2" xref="S3.E3.m1.5.5.1.1.2.2.cmml"><munderover id="S3.E3.m1.5.5.1.1.2.2.2" xref="S3.E3.m1.5.5.1.1.2.2.2.cmml"><mo movablelimits="false" id="S3.E3.m1.5.5.1.1.2.2.2.2.2" xref="S3.E3.m1.5.5.1.1.2.2.2.2.2.cmml">âˆ‘</mo><mrow id="S3.E3.m1.5.5.1.1.2.2.2.3" xref="S3.E3.m1.5.5.1.1.2.2.2.3.cmml"><mi id="S3.E3.m1.5.5.1.1.2.2.2.3.2" xref="S3.E3.m1.5.5.1.1.2.2.2.3.2.cmml">i</mi><mo id="S3.E3.m1.5.5.1.1.2.2.2.3.1" xref="S3.E3.m1.5.5.1.1.2.2.2.3.1.cmml">=</mo><mn id="S3.E3.m1.5.5.1.1.2.2.2.3.3" xref="S3.E3.m1.5.5.1.1.2.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E3.m1.5.5.1.1.2.2.2.2.3" xref="S3.E3.m1.5.5.1.1.2.2.2.2.3.cmml">M</mi></munderover><mrow id="S3.E3.m1.5.5.1.1.2.2.1" xref="S3.E3.m1.5.5.1.1.2.2.1.cmml"><mi id="S3.E3.m1.5.5.1.1.2.2.1.3" xref="S3.E3.m1.5.5.1.1.2.2.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.5.5.1.1.2.2.1.2" xref="S3.E3.m1.5.5.1.1.2.2.1.2.cmml">â€‹</mo><mi id="S3.E3.m1.5.5.1.1.2.2.1.4" xref="S3.E3.m1.5.5.1.1.2.2.1.4.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.5.5.1.1.2.2.1.2a" xref="S3.E3.m1.5.5.1.1.2.2.1.2.cmml">â€‹</mo><mrow id="S3.E3.m1.5.5.1.1.2.2.1.1.1" xref="S3.E3.m1.5.5.1.1.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.E3.m1.5.5.1.1.2.2.1.1.1.2" xref="S3.E3.m1.5.5.1.1.2.2.1.1.2.cmml">(</mo><mrow id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.cmml"><msub id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.4" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.4.cmml"><mi id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.4.2" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.4.2.cmml">f</mi><mi id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.4.3" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.4.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.3" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.3.cmml">â€‹</mo><mrow id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.2" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.2.3" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.3.cmml">(</mo><msub id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.1.1.1" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.2" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.2.cmml">X</mi><mi id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.3" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.2.4" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.3.cmml">;</mo><msub id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.2.2" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.2.2.cmml"><mi id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.2.2.2" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.2.2.2.cmml">Î¸</mi><mi id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.2.2.3" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.2.5" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.5.5.1.1.2.2.1.1.1.3" xref="S3.E3.m1.5.5.1.1.2.2.1.1.2.cmml">,</mo><mi id="S3.E3.m1.4.4" xref="S3.E3.m1.4.4.cmml">y</mi><mo stretchy="false" id="S3.E3.m1.5.5.1.1.2.2.1.1.1.4" xref="S3.E3.m1.5.5.1.1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.E3.m1.5.5.1.2" xref="S3.E3.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.5b"><apply id="S3.E3.m1.5.5.1.1.cmml" xref="S3.E3.m1.5.5.1"><eq id="S3.E3.m1.5.5.1.1.3.cmml" xref="S3.E3.m1.5.5.1.1.3"></eq><ci id="S3.E3.m1.5.5.1.1.4.cmml" xref="S3.E3.m1.5.5.1.1.4">ğ¿</ci><apply id="S3.E3.m1.5.5.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.2"><plus id="S3.E3.m1.5.5.1.1.2.3.cmml" xref="S3.E3.m1.5.5.1.1.2.3"></plus><apply id="S3.E3.m1.5.5.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1"><times id="S3.E3.m1.5.5.1.1.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.2"></times><ci id="S3.E3.m1.5.5.1.1.1.1.3.cmml" xref="S3.E3.m1.5.5.1.1.1.1.3">ğ¶</ci><ci id="S3.E3.m1.5.5.1.1.1.1.4.cmml" xref="S3.E3.m1.5.5.1.1.1.1.4">ğ¸</ci><interval closure="open" id="S3.E3.m1.5.5.1.1.1.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1"><apply id="S3.E3.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1"><times id="S3.E3.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.1"></times><ci id="S3.E3.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.2">ğ‘“</ci><list id="S3.E3.m1.5.5.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.5.5.1.1.1.1.1.1.1.3.2"><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">ğ‘‹</ci><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">ğœƒ</ci></list></apply><ci id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3">ğ‘¦</ci></interval></apply><apply id="S3.E3.m1.5.5.1.1.2.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2"><apply id="S3.E3.m1.5.5.1.1.2.2.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.2.2.2.1.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2">subscript</csymbol><apply id="S3.E3.m1.5.5.1.1.2.2.2.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.2.2.2.2.1.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2">superscript</csymbol><sum id="S3.E3.m1.5.5.1.1.2.2.2.2.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.2.2"></sum><ci id="S3.E3.m1.5.5.1.1.2.2.2.2.3.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.2.3">ğ‘€</ci></apply><apply id="S3.E3.m1.5.5.1.1.2.2.2.3.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.3"><eq id="S3.E3.m1.5.5.1.1.2.2.2.3.1.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.3.1"></eq><ci id="S3.E3.m1.5.5.1.1.2.2.2.3.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.3.2">ğ‘–</ci><cn type="integer" id="S3.E3.m1.5.5.1.1.2.2.2.3.3.cmml" xref="S3.E3.m1.5.5.1.1.2.2.2.3.3">1</cn></apply></apply><apply id="S3.E3.m1.5.5.1.1.2.2.1.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1"><times id="S3.E3.m1.5.5.1.1.2.2.1.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1.2"></times><ci id="S3.E3.m1.5.5.1.1.2.2.1.3.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1.3">ğ¶</ci><ci id="S3.E3.m1.5.5.1.1.2.2.1.4.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1.4">ğ¸</ci><interval closure="open" id="S3.E3.m1.5.5.1.1.2.2.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1"><apply id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1"><times id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.3.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.3"></times><apply id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.4.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.4.1.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.4">subscript</csymbol><ci id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.4.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.4.2">ğ‘“</ci><ci id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.4.3.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.4.3">ğ‘–</ci></apply><list id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.3.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.2"><apply id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.2">ğ‘‹</ci><ci id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.2.2.1.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.2.2.2.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.2.2.2">ğœƒ</ci><ci id="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.2.2.3.cmml" xref="S3.E3.m1.5.5.1.1.2.2.1.1.1.1.2.2.2.3">ğ‘–</ci></apply></list></apply><ci id="S3.E3.m1.4.4.cmml" xref="S3.E3.m1.4.4">ğ‘¦</ci></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.5c">L=CE(f(X;\theta),y)+\sum^{M}_{i=1}{CE(f_{i}(X_{i};\theta_{i}),y)}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<p id="S3.SS2.p7.1" class="ltx_p">To adaptively balance the learning of each unimodal encoder, we derive the coefficients <math id="S3.SS2.p7.1.m1.1" class="ltx_Math" alttext="k_{i}" display="inline"><semantics id="S3.SS2.p7.1.m1.1a"><msub id="S3.SS2.p7.1.m1.1.1" xref="S3.SS2.p7.1.m1.1.1.cmml"><mi id="S3.SS2.p7.1.m1.1.1.2" xref="S3.SS2.p7.1.m1.1.1.2.cmml">k</mi><mi id="S3.SS2.p7.1.m1.1.1.3" xref="S3.SS2.p7.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.1.m1.1b"><apply id="S3.SS2.p7.1.m1.1.1.cmml" xref="S3.SS2.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.1.m1.1.1.1.cmml" xref="S3.SS2.p7.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p7.1.m1.1.1.2.cmml" xref="S3.SS2.p7.1.m1.1.1.2">ğ‘˜</ci><ci id="S3.SS2.p7.1.m1.1.1.3.cmml" xref="S3.SS2.p7.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.1.m1.1c">k_{i}</annotation></semantics></math> of Eq. <a href="#S3.E2" title="Equation 2 â€£ 3.2 Balancing â€£ 3 Method â€£ Improving Multimodal Learning with Multi-Loss Gradient Modulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> based on modality performance. These coefficients are calculated as follows:</p>
</div>
<div id="S3.SS2.p8" class="ltx_para">
<table id="S6.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E4.m1.1" class="ltx_Math" alttext="\displaystyle s_{i}" display="inline"><semantics id="S3.E4.m1.1a"><msub id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><mi id="S3.E4.m1.1.1.2" xref="S3.E4.m1.1.1.2.cmml">s</mi><mi id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1">subscript</csymbol><ci id="S3.E4.m1.1.1.2.cmml" xref="S3.E4.m1.1.1.2">ğ‘ </ci><ci id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\displaystyle s_{i}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E4.m2.1" class="ltx_Math" alttext="\displaystyle=\sum_{i=1}^{M}\sum_{c=1}^{C}f_{i}(X_{i};\theta_{i})1_{k=y_{c}}," display="inline"><semantics id="S3.E4.m2.1a"><mrow id="S3.E4.m2.1.1.1" xref="S3.E4.m2.1.1.1.1.cmml"><mrow id="S3.E4.m2.1.1.1.1" xref="S3.E4.m2.1.1.1.1.cmml"><mi id="S3.E4.m2.1.1.1.1.4" xref="S3.E4.m2.1.1.1.1.4.cmml"></mi><mo id="S3.E4.m2.1.1.1.1.3" xref="S3.E4.m2.1.1.1.1.3.cmml">=</mo><mrow id="S3.E4.m2.1.1.1.1.2" xref="S3.E4.m2.1.1.1.1.2.cmml"><mstyle displaystyle="true" id="S3.E4.m2.1.1.1.1.2.3" xref="S3.E4.m2.1.1.1.1.2.3.cmml"><munderover id="S3.E4.m2.1.1.1.1.2.3a" xref="S3.E4.m2.1.1.1.1.2.3.cmml"><mo movablelimits="false" id="S3.E4.m2.1.1.1.1.2.3.2.2" xref="S3.E4.m2.1.1.1.1.2.3.2.2.cmml">âˆ‘</mo><mrow id="S3.E4.m2.1.1.1.1.2.3.2.3" xref="S3.E4.m2.1.1.1.1.2.3.2.3.cmml"><mi id="S3.E4.m2.1.1.1.1.2.3.2.3.2" xref="S3.E4.m2.1.1.1.1.2.3.2.3.2.cmml">i</mi><mo id="S3.E4.m2.1.1.1.1.2.3.2.3.1" xref="S3.E4.m2.1.1.1.1.2.3.2.3.1.cmml">=</mo><mn id="S3.E4.m2.1.1.1.1.2.3.2.3.3" xref="S3.E4.m2.1.1.1.1.2.3.2.3.3.cmml">1</mn></mrow><mi id="S3.E4.m2.1.1.1.1.2.3.3" xref="S3.E4.m2.1.1.1.1.2.3.3.cmml">M</mi></munderover></mstyle><mrow id="S3.E4.m2.1.1.1.1.2.2" xref="S3.E4.m2.1.1.1.1.2.2.cmml"><mstyle displaystyle="true" id="S3.E4.m2.1.1.1.1.2.2.3" xref="S3.E4.m2.1.1.1.1.2.2.3.cmml"><munderover id="S3.E4.m2.1.1.1.1.2.2.3a" xref="S3.E4.m2.1.1.1.1.2.2.3.cmml"><mo movablelimits="false" id="S3.E4.m2.1.1.1.1.2.2.3.2.2" xref="S3.E4.m2.1.1.1.1.2.2.3.2.2.cmml">âˆ‘</mo><mrow id="S3.E4.m2.1.1.1.1.2.2.3.2.3" xref="S3.E4.m2.1.1.1.1.2.2.3.2.3.cmml"><mi id="S3.E4.m2.1.1.1.1.2.2.3.2.3.2" xref="S3.E4.m2.1.1.1.1.2.2.3.2.3.2.cmml">c</mi><mo id="S3.E4.m2.1.1.1.1.2.2.3.2.3.1" xref="S3.E4.m2.1.1.1.1.2.2.3.2.3.1.cmml">=</mo><mn id="S3.E4.m2.1.1.1.1.2.2.3.2.3.3" xref="S3.E4.m2.1.1.1.1.2.2.3.2.3.3.cmml">1</mn></mrow><mi id="S3.E4.m2.1.1.1.1.2.2.3.3" xref="S3.E4.m2.1.1.1.1.2.2.3.3.cmml">C</mi></munderover></mstyle><mrow id="S3.E4.m2.1.1.1.1.2.2.2" xref="S3.E4.m2.1.1.1.1.2.2.2.cmml"><msub id="S3.E4.m2.1.1.1.1.2.2.2.4" xref="S3.E4.m2.1.1.1.1.2.2.2.4.cmml"><mi id="S3.E4.m2.1.1.1.1.2.2.2.4.2" xref="S3.E4.m2.1.1.1.1.2.2.2.4.2.cmml">f</mi><mi id="S3.E4.m2.1.1.1.1.2.2.2.4.3" xref="S3.E4.m2.1.1.1.1.2.2.2.4.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m2.1.1.1.1.2.2.2.3" xref="S3.E4.m2.1.1.1.1.2.2.2.3.cmml">â€‹</mo><mrow id="S3.E4.m2.1.1.1.1.2.2.2.2.2" xref="S3.E4.m2.1.1.1.1.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E4.m2.1.1.1.1.2.2.2.2.2.3" xref="S3.E4.m2.1.1.1.1.2.2.2.2.3.cmml">(</mo><msub id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.2.cmml">X</mi><mi id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E4.m2.1.1.1.1.2.2.2.2.2.4" xref="S3.E4.m2.1.1.1.1.2.2.2.2.3.cmml">;</mo><msub id="S3.E4.m2.1.1.1.1.2.2.2.2.2.2" xref="S3.E4.m2.1.1.1.1.2.2.2.2.2.2.cmml"><mi id="S3.E4.m2.1.1.1.1.2.2.2.2.2.2.2" xref="S3.E4.m2.1.1.1.1.2.2.2.2.2.2.2.cmml">Î¸</mi><mi id="S3.E4.m2.1.1.1.1.2.2.2.2.2.2.3" xref="S3.E4.m2.1.1.1.1.2.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E4.m2.1.1.1.1.2.2.2.2.2.5" xref="S3.E4.m2.1.1.1.1.2.2.2.2.3.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E4.m2.1.1.1.1.2.2.2.3a" xref="S3.E4.m2.1.1.1.1.2.2.2.3.cmml">â€‹</mo><msub id="S3.E4.m2.1.1.1.1.2.2.2.5" xref="S3.E4.m2.1.1.1.1.2.2.2.5.cmml"><mn id="S3.E4.m2.1.1.1.1.2.2.2.5.2" xref="S3.E4.m2.1.1.1.1.2.2.2.5.2.cmml">1</mn><mrow id="S3.E4.m2.1.1.1.1.2.2.2.5.3" xref="S3.E4.m2.1.1.1.1.2.2.2.5.3.cmml"><mi id="S3.E4.m2.1.1.1.1.2.2.2.5.3.2" xref="S3.E4.m2.1.1.1.1.2.2.2.5.3.2.cmml">k</mi><mo id="S3.E4.m2.1.1.1.1.2.2.2.5.3.1" xref="S3.E4.m2.1.1.1.1.2.2.2.5.3.1.cmml">=</mo><msub id="S3.E4.m2.1.1.1.1.2.2.2.5.3.3" xref="S3.E4.m2.1.1.1.1.2.2.2.5.3.3.cmml"><mi id="S3.E4.m2.1.1.1.1.2.2.2.5.3.3.2" xref="S3.E4.m2.1.1.1.1.2.2.2.5.3.3.2.cmml">y</mi><mi id="S3.E4.m2.1.1.1.1.2.2.2.5.3.3.3" xref="S3.E4.m2.1.1.1.1.2.2.2.5.3.3.3.cmml">c</mi></msub></mrow></msub></mrow></mrow></mrow></mrow><mo id="S3.E4.m2.1.1.1.2" xref="S3.E4.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m2.1b"><apply id="S3.E4.m2.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1"><eq id="S3.E4.m2.1.1.1.1.3.cmml" xref="S3.E4.m2.1.1.1.1.3"></eq><csymbol cd="latexml" id="S3.E4.m2.1.1.1.1.4.cmml" xref="S3.E4.m2.1.1.1.1.4">absent</csymbol><apply id="S3.E4.m2.1.1.1.1.2.cmml" xref="S3.E4.m2.1.1.1.1.2"><apply id="S3.E4.m2.1.1.1.1.2.3.cmml" xref="S3.E4.m2.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.2.3.1.cmml" xref="S3.E4.m2.1.1.1.1.2.3">superscript</csymbol><apply id="S3.E4.m2.1.1.1.1.2.3.2.cmml" xref="S3.E4.m2.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.2.3.2.1.cmml" xref="S3.E4.m2.1.1.1.1.2.3">subscript</csymbol><sum id="S3.E4.m2.1.1.1.1.2.3.2.2.cmml" xref="S3.E4.m2.1.1.1.1.2.3.2.2"></sum><apply id="S3.E4.m2.1.1.1.1.2.3.2.3.cmml" xref="S3.E4.m2.1.1.1.1.2.3.2.3"><eq id="S3.E4.m2.1.1.1.1.2.3.2.3.1.cmml" xref="S3.E4.m2.1.1.1.1.2.3.2.3.1"></eq><ci id="S3.E4.m2.1.1.1.1.2.3.2.3.2.cmml" xref="S3.E4.m2.1.1.1.1.2.3.2.3.2">ğ‘–</ci><cn type="integer" id="S3.E4.m2.1.1.1.1.2.3.2.3.3.cmml" xref="S3.E4.m2.1.1.1.1.2.3.2.3.3">1</cn></apply></apply><ci id="S3.E4.m2.1.1.1.1.2.3.3.cmml" xref="S3.E4.m2.1.1.1.1.2.3.3">ğ‘€</ci></apply><apply id="S3.E4.m2.1.1.1.1.2.2.cmml" xref="S3.E4.m2.1.1.1.1.2.2"><apply id="S3.E4.m2.1.1.1.1.2.2.3.cmml" xref="S3.E4.m2.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.2.2.3.1.cmml" xref="S3.E4.m2.1.1.1.1.2.2.3">superscript</csymbol><apply id="S3.E4.m2.1.1.1.1.2.2.3.2.cmml" xref="S3.E4.m2.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.2.2.3.2.1.cmml" xref="S3.E4.m2.1.1.1.1.2.2.3">subscript</csymbol><sum id="S3.E4.m2.1.1.1.1.2.2.3.2.2.cmml" xref="S3.E4.m2.1.1.1.1.2.2.3.2.2"></sum><apply id="S3.E4.m2.1.1.1.1.2.2.3.2.3.cmml" xref="S3.E4.m2.1.1.1.1.2.2.3.2.3"><eq id="S3.E4.m2.1.1.1.1.2.2.3.2.3.1.cmml" xref="S3.E4.m2.1.1.1.1.2.2.3.2.3.1"></eq><ci id="S3.E4.m2.1.1.1.1.2.2.3.2.3.2.cmml" xref="S3.E4.m2.1.1.1.1.2.2.3.2.3.2">ğ‘</ci><cn type="integer" id="S3.E4.m2.1.1.1.1.2.2.3.2.3.3.cmml" xref="S3.E4.m2.1.1.1.1.2.2.3.2.3.3">1</cn></apply></apply><ci id="S3.E4.m2.1.1.1.1.2.2.3.3.cmml" xref="S3.E4.m2.1.1.1.1.2.2.3.3">ğ¶</ci></apply><apply id="S3.E4.m2.1.1.1.1.2.2.2.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2"><times id="S3.E4.m2.1.1.1.1.2.2.2.3.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.3"></times><apply id="S3.E4.m2.1.1.1.1.2.2.2.4.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.4"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.2.2.2.4.1.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.4">subscript</csymbol><ci id="S3.E4.m2.1.1.1.1.2.2.2.4.2.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.4.2">ğ‘“</ci><ci id="S3.E4.m2.1.1.1.1.2.2.2.4.3.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.4.3">ğ‘–</ci></apply><list id="S3.E4.m2.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.2.2"><apply id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.2">ğ‘‹</ci><ci id="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m2.1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S3.E4.m2.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.2.2.2.2.2.2.1.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E4.m2.1.1.1.1.2.2.2.2.2.2.2.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.2.2.2.2">ğœƒ</ci><ci id="S3.E4.m2.1.1.1.1.2.2.2.2.2.2.3.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.2.2.2.3">ğ‘–</ci></apply></list><apply id="S3.E4.m2.1.1.1.1.2.2.2.5.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.5"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.2.2.2.5.1.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.5">subscript</csymbol><cn type="integer" id="S3.E4.m2.1.1.1.1.2.2.2.5.2.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.5.2">1</cn><apply id="S3.E4.m2.1.1.1.1.2.2.2.5.3.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.5.3"><eq id="S3.E4.m2.1.1.1.1.2.2.2.5.3.1.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.5.3.1"></eq><ci id="S3.E4.m2.1.1.1.1.2.2.2.5.3.2.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.5.3.2">ğ‘˜</ci><apply id="S3.E4.m2.1.1.1.1.2.2.2.5.3.3.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.5.3.3"><csymbol cd="ambiguous" id="S3.E4.m2.1.1.1.1.2.2.2.5.3.3.1.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.5.3.3">subscript</csymbol><ci id="S3.E4.m2.1.1.1.1.2.2.2.5.3.3.2.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.5.3.3.2">ğ‘¦</ci><ci id="S3.E4.m2.1.1.1.1.2.2.2.5.3.3.3.cmml" xref="S3.E4.m2.1.1.1.1.2.2.2.5.3.3.3">ğ‘</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m2.1c">\displaystyle=\sum_{i=1}^{M}\sum_{c=1}^{C}f_{i}(X_{i};\theta_{i})1_{k=y_{c}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E5.m1.1" class="ltx_Math" alttext="\displaystyle r_{i}" display="inline"><semantics id="S3.E5.m1.1a"><msub id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><mi id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml">r</mi><mi id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1">subscript</csymbol><ci id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2">ğ‘Ÿ</ci><ci id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\displaystyle r_{i}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E5.m2.3" class="ltx_Math" alttext="\displaystyle=\frac{\frac{1}{M-1}\sum_{j=1,j\neq i}^{M}s_{j}}{s_{i}}," display="inline"><semantics id="S3.E5.m2.3a"><mrow id="S3.E5.m2.3.3.1" xref="S3.E5.m2.3.3.1.1.cmml"><mrow id="S3.E5.m2.3.3.1.1" xref="S3.E5.m2.3.3.1.1.cmml"><mi id="S3.E5.m2.3.3.1.1.2" xref="S3.E5.m2.3.3.1.1.2.cmml"></mi><mo id="S3.E5.m2.3.3.1.1.1" xref="S3.E5.m2.3.3.1.1.1.cmml">=</mo><mstyle displaystyle="true" id="S3.E5.m2.2.2" xref="S3.E5.m2.2.2.cmml"><mfrac id="S3.E5.m2.2.2a" xref="S3.E5.m2.2.2.cmml"><mrow id="S3.E5.m2.2.2.2" xref="S3.E5.m2.2.2.2.cmml"><mfrac id="S3.E5.m2.2.2.2.4" xref="S3.E5.m2.2.2.2.4.cmml"><mn id="S3.E5.m2.2.2.2.4.2" xref="S3.E5.m2.2.2.2.4.2.cmml">1</mn><mrow id="S3.E5.m2.2.2.2.4.3" xref="S3.E5.m2.2.2.2.4.3.cmml"><mi id="S3.E5.m2.2.2.2.4.3.2" xref="S3.E5.m2.2.2.2.4.3.2.cmml">M</mi><mo id="S3.E5.m2.2.2.2.4.3.1" xref="S3.E5.m2.2.2.2.4.3.1.cmml">âˆ’</mo><mn id="S3.E5.m2.2.2.2.4.3.3" xref="S3.E5.m2.2.2.2.4.3.3.cmml">1</mn></mrow></mfrac><mo lspace="0em" rspace="0em" id="S3.E5.m2.2.2.2.3" xref="S3.E5.m2.2.2.2.3.cmml">â€‹</mo><mrow id="S3.E5.m2.2.2.2.5" xref="S3.E5.m2.2.2.2.5.cmml"><msubsup id="S3.E5.m2.2.2.2.5.1" xref="S3.E5.m2.2.2.2.5.1.cmml"><mo id="S3.E5.m2.2.2.2.5.1.2.2" xref="S3.E5.m2.2.2.2.5.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E5.m2.2.2.2.2.2.2" xref="S3.E5.m2.2.2.2.2.2.3.cmml"><mrow id="S3.E5.m2.1.1.1.1.1.1.1" xref="S3.E5.m2.1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m2.1.1.1.1.1.1.1.2" xref="S3.E5.m2.1.1.1.1.1.1.1.2.cmml">j</mi><mo id="S3.E5.m2.1.1.1.1.1.1.1.1" xref="S3.E5.m2.1.1.1.1.1.1.1.1.cmml">=</mo><mn id="S3.E5.m2.1.1.1.1.1.1.1.3" xref="S3.E5.m2.1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.E5.m2.2.2.2.2.2.2.3" xref="S3.E5.m2.2.2.2.2.2.3a.cmml">,</mo><mrow id="S3.E5.m2.2.2.2.2.2.2.2" xref="S3.E5.m2.2.2.2.2.2.2.2.cmml"><mi id="S3.E5.m2.2.2.2.2.2.2.2.2" xref="S3.E5.m2.2.2.2.2.2.2.2.2.cmml">j</mi><mo id="S3.E5.m2.2.2.2.2.2.2.2.1" xref="S3.E5.m2.2.2.2.2.2.2.2.1.cmml">â‰ </mo><mi id="S3.E5.m2.2.2.2.2.2.2.2.3" xref="S3.E5.m2.2.2.2.2.2.2.2.3.cmml">i</mi></mrow></mrow><mi id="S3.E5.m2.2.2.2.5.1.3" xref="S3.E5.m2.2.2.2.5.1.3.cmml">M</mi></msubsup><msub id="S3.E5.m2.2.2.2.5.2" xref="S3.E5.m2.2.2.2.5.2.cmml"><mi id="S3.E5.m2.2.2.2.5.2.2" xref="S3.E5.m2.2.2.2.5.2.2.cmml">s</mi><mi id="S3.E5.m2.2.2.2.5.2.3" xref="S3.E5.m2.2.2.2.5.2.3.cmml">j</mi></msub></mrow></mrow><msub id="S3.E5.m2.2.2.4" xref="S3.E5.m2.2.2.4.cmml"><mi id="S3.E5.m2.2.2.4.2" xref="S3.E5.m2.2.2.4.2.cmml">s</mi><mi id="S3.E5.m2.2.2.4.3" xref="S3.E5.m2.2.2.4.3.cmml">i</mi></msub></mfrac></mstyle></mrow><mo id="S3.E5.m2.3.3.1.2" xref="S3.E5.m2.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m2.3b"><apply id="S3.E5.m2.3.3.1.1.cmml" xref="S3.E5.m2.3.3.1"><eq id="S3.E5.m2.3.3.1.1.1.cmml" xref="S3.E5.m2.3.3.1.1.1"></eq><csymbol cd="latexml" id="S3.E5.m2.3.3.1.1.2.cmml" xref="S3.E5.m2.3.3.1.1.2">absent</csymbol><apply id="S3.E5.m2.2.2.cmml" xref="S3.E5.m2.2.2"><divide id="S3.E5.m2.2.2.3.cmml" xref="S3.E5.m2.2.2"></divide><apply id="S3.E5.m2.2.2.2.cmml" xref="S3.E5.m2.2.2.2"><times id="S3.E5.m2.2.2.2.3.cmml" xref="S3.E5.m2.2.2.2.3"></times><apply id="S3.E5.m2.2.2.2.4.cmml" xref="S3.E5.m2.2.2.2.4"><divide id="S3.E5.m2.2.2.2.4.1.cmml" xref="S3.E5.m2.2.2.2.4"></divide><cn type="integer" id="S3.E5.m2.2.2.2.4.2.cmml" xref="S3.E5.m2.2.2.2.4.2">1</cn><apply id="S3.E5.m2.2.2.2.4.3.cmml" xref="S3.E5.m2.2.2.2.4.3"><minus id="S3.E5.m2.2.2.2.4.3.1.cmml" xref="S3.E5.m2.2.2.2.4.3.1"></minus><ci id="S3.E5.m2.2.2.2.4.3.2.cmml" xref="S3.E5.m2.2.2.2.4.3.2">ğ‘€</ci><cn type="integer" id="S3.E5.m2.2.2.2.4.3.3.cmml" xref="S3.E5.m2.2.2.2.4.3.3">1</cn></apply></apply><apply id="S3.E5.m2.2.2.2.5.cmml" xref="S3.E5.m2.2.2.2.5"><apply id="S3.E5.m2.2.2.2.5.1.cmml" xref="S3.E5.m2.2.2.2.5.1"><csymbol cd="ambiguous" id="S3.E5.m2.2.2.2.5.1.1.cmml" xref="S3.E5.m2.2.2.2.5.1">superscript</csymbol><apply id="S3.E5.m2.2.2.2.5.1.2.cmml" xref="S3.E5.m2.2.2.2.5.1"><csymbol cd="ambiguous" id="S3.E5.m2.2.2.2.5.1.2.1.cmml" xref="S3.E5.m2.2.2.2.5.1">subscript</csymbol><sum id="S3.E5.m2.2.2.2.5.1.2.2.cmml" xref="S3.E5.m2.2.2.2.5.1.2.2"></sum><apply id="S3.E5.m2.2.2.2.2.2.3.cmml" xref="S3.E5.m2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E5.m2.2.2.2.2.2.3a.cmml" xref="S3.E5.m2.2.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.E5.m2.1.1.1.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1"><eq id="S3.E5.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.1"></eq><ci id="S3.E5.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.2">ğ‘—</ci><cn type="integer" id="S3.E5.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m2.1.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.E5.m2.2.2.2.2.2.2.2.cmml" xref="S3.E5.m2.2.2.2.2.2.2.2"><neq id="S3.E5.m2.2.2.2.2.2.2.2.1.cmml" xref="S3.E5.m2.2.2.2.2.2.2.2.1"></neq><ci id="S3.E5.m2.2.2.2.2.2.2.2.2.cmml" xref="S3.E5.m2.2.2.2.2.2.2.2.2">ğ‘—</ci><ci id="S3.E5.m2.2.2.2.2.2.2.2.3.cmml" xref="S3.E5.m2.2.2.2.2.2.2.2.3">ğ‘–</ci></apply></apply></apply><ci id="S3.E5.m2.2.2.2.5.1.3.cmml" xref="S3.E5.m2.2.2.2.5.1.3">ğ‘€</ci></apply><apply id="S3.E5.m2.2.2.2.5.2.cmml" xref="S3.E5.m2.2.2.2.5.2"><csymbol cd="ambiguous" id="S3.E5.m2.2.2.2.5.2.1.cmml" xref="S3.E5.m2.2.2.2.5.2">subscript</csymbol><ci id="S3.E5.m2.2.2.2.5.2.2.cmml" xref="S3.E5.m2.2.2.2.5.2.2">ğ‘ </ci><ci id="S3.E5.m2.2.2.2.5.2.3.cmml" xref="S3.E5.m2.2.2.2.5.2.3">ğ‘—</ci></apply></apply></apply><apply id="S3.E5.m2.2.2.4.cmml" xref="S3.E5.m2.2.2.4"><csymbol cd="ambiguous" id="S3.E5.m2.2.2.4.1.cmml" xref="S3.E5.m2.2.2.4">subscript</csymbol><ci id="S3.E5.m2.2.2.4.2.cmml" xref="S3.E5.m2.2.2.4.2">ğ‘ </ci><ci id="S3.E5.m2.2.2.4.3.cmml" xref="S3.E5.m2.2.2.4.3">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m2.3c">\displaystyle=\frac{\frac{1}{M-1}\sum_{j=1,j\neq i}^{M}s_{j}}{s_{i}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
<tbody id="S3.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E6.m1.1" class="ltx_Math" alttext="\displaystyle\beta_{i}" display="inline"><semantics id="S3.E6.m1.1a"><msub id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml"><mi id="S3.E6.m1.1.1.2" xref="S3.E6.m1.1.1.2.cmml">Î²</mi><mi id="S3.E6.m1.1.1.3" xref="S3.E6.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.cmml" xref="S3.E6.m1.1.1">subscript</csymbol><ci id="S3.E6.m1.1.1.2.cmml" xref="S3.E6.m1.1.1.2">ğ›½</ci><ci id="S3.E6.m1.1.1.3.cmml" xref="S3.E6.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">\displaystyle\beta_{i}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E6.m2.1" class="ltx_Math" alttext="\displaystyle=\begin{cases}\begin{aligned} &amp;\beta_{max}\quad\text{if }r_{i}&gt;1,\\
&amp;2\quad\text{otherwise},\end{aligned}\end{cases}" display="inline"><semantics id="S3.E6.m2.1a"><mrow id="S3.E6.m2.1.2" xref="S3.E6.m2.1.2.cmml"><mi id="S3.E6.m2.1.2.2" xref="S3.E6.m2.1.2.2.cmml"></mi><mo id="S3.E6.m2.1.2.1" xref="S3.E6.m2.1.2.1.cmml">=</mo><mrow id="S3.E6.m2.1.1a" xref="S3.E6.m2.1.2.3.1.cmml"><mo id="S3.E6.m2.1.1a.2" xref="S3.E6.m2.1.2.3.1.1.cmml">{</mo><mtable columnspacing="5pt" id="S3.E6.m2.1.1.1a" xref="S3.E6.m2.1.2.3.1.cmml"><mtr id="S3.E6.m2.1.1.1aa" xref="S3.E6.m2.1.2.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S3.E6.m2.1.1.1ab" xref="S3.E6.m2.1.2.3.1.cmml"><mtable columnspacing="0pt" rowspacing="0pt" id="S3.E6.m2.1.1.1.1.1.1" xref="S3.E6.m2.1.1.1.1.1.1.cmml"><mtr id="S3.E6.m2.1.1.1.1.1.1a" xref="S3.E6.m2.1.1.1.1.1.1.cmml"><mtd id="S3.E6.m2.1.1.1.1.1.1b" xref="S3.E6.m2.1.1.1.1.1.1.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E6.m2.1.1.1.1.1.1c" xref="S3.E6.m2.1.1.1.1.1.1.cmml"><mrow id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><msub id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">Î²</mi><mrow id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1a" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.4" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.4.cmml">x</mi></mrow></msub><mspace width="1em" id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"></mspace><mrow id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mtext id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2a.cmml">ifÂ </mtext><mo lspace="0em" rspace="0em" id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml">â€‹</mo><msub id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml"><mi id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.2" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml">r</mi><mi id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.3" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml">i</mi></msub></mrow></mrow><mo id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">&gt;</mo><mn id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.4" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.4.cmml">1</mn></mrow><mo id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">,</mo></mrow></mtd></mtr><mtr id="S3.E6.m2.1.1.1.1.1.1d" xref="S3.E6.m2.1.1.1.1.1.1.cmml"><mtd id="S3.E6.m2.1.1.1.1.1.1e" xref="S3.E6.m2.1.1.1.1.1.1.cmml"></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E6.m2.1.1.1.1.1.1f" xref="S3.E6.m2.1.1.1.1.1.1.cmml"><mrow id="S3.E6.m2.1.1.1.1.1.1.4.4.3.3.3" xref="S3.E6.m2.1.1.1.1.1.1.cmml"><mrow id="S3.E6.m2.1.1.1.1.1.1.4.4.3.3.3.1.2" xref="S3.E6.m2.1.1.1.1.1.1.4.4.3.3.3.1.1.cmml"><mn id="S3.E6.m2.1.1.1.1.1.1.2.2.1.1.1" xref="S3.E6.m2.1.1.1.1.1.1.2.2.1.1.1.cmml">2</mn><mspace width="1em" id="S3.E6.m2.1.1.1.1.1.1.4.4.3.3.3.1.2.1" xref="S3.E6.m2.1.1.1.1.1.1.4.4.3.3.3.1.1.cmml"></mspace><mtext id="S3.E6.m2.1.1.1.1.1.1.3.3.2.2.2" xref="S3.E6.m2.1.1.1.1.1.1.3.3.2.2.2a.cmml">otherwise</mtext></mrow><mo id="S3.E6.m2.1.1.1.1.1.1.4.4.3.3.3.2" xref="S3.E6.m2.1.1.1.1.1.1.cmml">,</mo></mrow></mtd></mtr></mtable></mtd><mtd id="S3.E6.m2.1.1.1ac" xref="S3.E6.m2.1.2.3.1.1.cmml"></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m2.1b"><apply id="S3.E6.m2.1.2.cmml" xref="S3.E6.m2.1.2"><eq id="S3.E6.m2.1.2.1.cmml" xref="S3.E6.m2.1.2.1"></eq><csymbol cd="latexml" id="S3.E6.m2.1.2.2.cmml" xref="S3.E6.m2.1.2.2">absent</csymbol><apply id="S3.E6.m2.1.2.3.1.cmml" xref="S3.E6.m2.1.1a"><csymbol cd="latexml" id="S3.E6.m2.1.2.3.1.1.cmml" xref="S3.E6.m2.1.1a.2">cases</csymbol><matrix id="S3.E6.m2.1.1.1.1.1.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1"><matrixrow id="S3.E6.m2.1.1.1.1.1.1a.cmml" xref="S3.E6.m2.1.1.1.1.1.1"><cerror id="S3.E6.m2.1.1.1.1.1.1b.cmml" xref="S3.E6.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.1.1c.cmml" xref="S3.E6.m2.1.1.1.1.1.1">missing-subexpression</csymbol></cerror><apply id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1"><gt id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.3"></gt><list id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2"><apply id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">ğ›½</ci><apply id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><times id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">ğ‘š</ci><ci id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">ğ‘</ci><ci id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.4.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.4">ğ‘¥</ci></apply></apply><apply id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2"><times id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.1"></times><ci id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2a.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2"><mtext id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.2">ifÂ </mtext></ci><apply id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3">subscript</csymbol><ci id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.2">ğ‘Ÿ</ci><ci id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.3.3">ğ‘–</ci></apply></apply></list><cn type="integer" id="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E6.m2.1.1.1.1.1.1.1.1.1.1.1.1.4">1</cn></apply></matrixrow><matrixrow id="S3.E6.m2.1.1.1.1.1.1d.cmml" xref="S3.E6.m2.1.1.1.1.1.1"><cerror id="S3.E6.m2.1.1.1.1.1.1e.cmml" xref="S3.E6.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m2.1.1.1.1.1.1f.cmml" xref="S3.E6.m2.1.1.1.1.1.1">missing-subexpression</csymbol></cerror><list id="S3.E6.m2.1.1.1.1.1.1.4.4.3.3.3.1.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.4.4.3.3.3.1.2"><cn type="integer" id="S3.E6.m2.1.1.1.1.1.1.2.2.1.1.1.cmml" xref="S3.E6.m2.1.1.1.1.1.1.2.2.1.1.1">2</cn><ci id="S3.E6.m2.1.1.1.1.1.1.3.3.2.2.2a.cmml" xref="S3.E6.m2.1.1.1.1.1.1.3.3.2.2.2"><mtext id="S3.E6.m2.1.1.1.1.1.1.3.3.2.2.2.cmml" xref="S3.E6.m2.1.1.1.1.1.1.3.3.2.2.2">otherwise</mtext></ci></list></matrixrow></matrix><ci id="S3.E6.m2.1.2.3.1.3a.cmml" xref="S3.E6.m2.1.1a"><mtext class="ltx_mathvariant_italic" id="S3.E6.m2.1.2.3.1.3.cmml" xref="S3.E6.m2.1.1a.2">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m2.1c">\displaystyle=\begin{cases}\begin{aligned} &amp;\beta_{max}\quad\text{if }r_{i}&gt;1,\\
&amp;2\quad\text{otherwise},\end{aligned}\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
<tbody id="S3.E7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E7.m1.1" class="ltx_Math" alttext="\displaystyle k_{i}" display="inline"><semantics id="S3.E7.m1.1a"><msub id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml"><mi id="S3.E7.m1.1.1.2" xref="S3.E7.m1.1.1.2.cmml">k</mi><mi id="S3.E7.m1.1.1.3" xref="S3.E7.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><apply id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.cmml" xref="S3.E7.m1.1.1">subscript</csymbol><ci id="S3.E7.m1.1.1.2.cmml" xref="S3.E7.m1.1.1.2">ğ‘˜</ci><ci id="S3.E7.m1.1.1.3.cmml" xref="S3.E7.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">\displaystyle k_{i}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E7.m2.3" class="ltx_Math" alttext="\displaystyle=1+(\beta_{i}-1)\cdot\tanh(\alpha\cdot(r_{i}-1))" display="inline"><semantics id="S3.E7.m2.3a"><mrow id="S3.E7.m2.3.3" xref="S3.E7.m2.3.3.cmml"><mi id="S3.E7.m2.3.3.4" xref="S3.E7.m2.3.3.4.cmml"></mi><mo id="S3.E7.m2.3.3.3" xref="S3.E7.m2.3.3.3.cmml">=</mo><mrow id="S3.E7.m2.3.3.2" xref="S3.E7.m2.3.3.2.cmml"><mn id="S3.E7.m2.3.3.2.4" xref="S3.E7.m2.3.3.2.4.cmml">1</mn><mo id="S3.E7.m2.3.3.2.3" xref="S3.E7.m2.3.3.2.3.cmml">+</mo><mrow id="S3.E7.m2.3.3.2.2" xref="S3.E7.m2.3.3.2.2.cmml"><mrow id="S3.E7.m2.2.2.1.1.1.1" xref="S3.E7.m2.2.2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E7.m2.2.2.1.1.1.1.2" xref="S3.E7.m2.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S3.E7.m2.2.2.1.1.1.1.1" xref="S3.E7.m2.2.2.1.1.1.1.1.cmml"><msub id="S3.E7.m2.2.2.1.1.1.1.1.2" xref="S3.E7.m2.2.2.1.1.1.1.1.2.cmml"><mi id="S3.E7.m2.2.2.1.1.1.1.1.2.2" xref="S3.E7.m2.2.2.1.1.1.1.1.2.2.cmml">Î²</mi><mi id="S3.E7.m2.2.2.1.1.1.1.1.2.3" xref="S3.E7.m2.2.2.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E7.m2.2.2.1.1.1.1.1.1" xref="S3.E7.m2.2.2.1.1.1.1.1.1.cmml">âˆ’</mo><mn id="S3.E7.m2.2.2.1.1.1.1.1.3" xref="S3.E7.m2.2.2.1.1.1.1.1.3.cmml">1</mn></mrow><mo rspace="0.055em" stretchy="false" id="S3.E7.m2.2.2.1.1.1.1.3" xref="S3.E7.m2.2.2.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.E7.m2.3.3.2.2.3" xref="S3.E7.m2.3.3.2.2.3.cmml">â‹…</mo><mrow id="S3.E7.m2.3.3.2.2.2.1" xref="S3.E7.m2.3.3.2.2.2.2.cmml"><mi id="S3.E7.m2.1.1" xref="S3.E7.m2.1.1.cmml">tanh</mi><mo id="S3.E7.m2.3.3.2.2.2.1a" xref="S3.E7.m2.3.3.2.2.2.2.cmml">â¡</mo><mrow id="S3.E7.m2.3.3.2.2.2.1.1" xref="S3.E7.m2.3.3.2.2.2.2.cmml"><mo stretchy="false" id="S3.E7.m2.3.3.2.2.2.1.1.2" xref="S3.E7.m2.3.3.2.2.2.2.cmml">(</mo><mrow id="S3.E7.m2.3.3.2.2.2.1.1.1" xref="S3.E7.m2.3.3.2.2.2.1.1.1.cmml"><mi id="S3.E7.m2.3.3.2.2.2.1.1.1.3" xref="S3.E7.m2.3.3.2.2.2.1.1.1.3.cmml">Î±</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E7.m2.3.3.2.2.2.1.1.1.2" xref="S3.E7.m2.3.3.2.2.2.1.1.1.2.cmml">â‹…</mo><mrow id="S3.E7.m2.3.3.2.2.2.1.1.1.1.1" xref="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.2" xref="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1" xref="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.cmml"><msub id="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.2" xref="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.2.cmml"><mi id="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.2.2" xref="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.2.2.cmml">r</mi><mi id="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.2.3" xref="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.1" xref="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.1.cmml">âˆ’</mo><mn id="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.3" xref="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.3" xref="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E7.m2.3.3.2.2.2.1.1.3" xref="S3.E7.m2.3.3.2.2.2.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m2.3b"><apply id="S3.E7.m2.3.3.cmml" xref="S3.E7.m2.3.3"><eq id="S3.E7.m2.3.3.3.cmml" xref="S3.E7.m2.3.3.3"></eq><csymbol cd="latexml" id="S3.E7.m2.3.3.4.cmml" xref="S3.E7.m2.3.3.4">absent</csymbol><apply id="S3.E7.m2.3.3.2.cmml" xref="S3.E7.m2.3.3.2"><plus id="S3.E7.m2.3.3.2.3.cmml" xref="S3.E7.m2.3.3.2.3"></plus><cn type="integer" id="S3.E7.m2.3.3.2.4.cmml" xref="S3.E7.m2.3.3.2.4">1</cn><apply id="S3.E7.m2.3.3.2.2.cmml" xref="S3.E7.m2.3.3.2.2"><ci id="S3.E7.m2.3.3.2.2.3.cmml" xref="S3.E7.m2.3.3.2.2.3">â‹…</ci><apply id="S3.E7.m2.2.2.1.1.1.1.1.cmml" xref="S3.E7.m2.2.2.1.1.1.1"><minus id="S3.E7.m2.2.2.1.1.1.1.1.1.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.1"></minus><apply id="S3.E7.m2.2.2.1.1.1.1.1.2.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E7.m2.2.2.1.1.1.1.1.2.1.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E7.m2.2.2.1.1.1.1.1.2.2.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.2.2">ğ›½</ci><ci id="S3.E7.m2.2.2.1.1.1.1.1.2.3.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.2.3">ğ‘–</ci></apply><cn type="integer" id="S3.E7.m2.2.2.1.1.1.1.1.3.cmml" xref="S3.E7.m2.2.2.1.1.1.1.1.3">1</cn></apply><apply id="S3.E7.m2.3.3.2.2.2.2.cmml" xref="S3.E7.m2.3.3.2.2.2.1"><tanh id="S3.E7.m2.1.1.cmml" xref="S3.E7.m2.1.1"></tanh><apply id="S3.E7.m2.3.3.2.2.2.1.1.1.cmml" xref="S3.E7.m2.3.3.2.2.2.1.1.1"><ci id="S3.E7.m2.3.3.2.2.2.1.1.1.2.cmml" xref="S3.E7.m2.3.3.2.2.2.1.1.1.2">â‹…</ci><ci id="S3.E7.m2.3.3.2.2.2.1.1.1.3.cmml" xref="S3.E7.m2.3.3.2.2.2.1.1.1.3">ğ›¼</ci><apply id="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.cmml" xref="S3.E7.m2.3.3.2.2.2.1.1.1.1.1"><minus id="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.1"></minus><apply id="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.2.2">ğ‘Ÿ</ci><ci id="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.2.3.cmml" xref="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.2.3">ğ‘–</ci></apply><cn type="integer" id="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E7.m2.3.3.2.2.2.1.1.1.1.1.1.3">1</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m2.3c">\displaystyle=1+(\beta_{i}-1)\cdot\tanh(\alpha\cdot(r_{i}-1))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p9" class="ltx_para ltx_noindent">
<p id="S3.SS2.p9.10" class="ltx_p"><math id="S3.SS2.p9.1.m1.1" class="ltx_Math" alttext="s_{i}" display="inline"><semantics id="S3.SS2.p9.1.m1.1a"><msub id="S3.SS2.p9.1.m1.1.1" xref="S3.SS2.p9.1.m1.1.1.cmml"><mi id="S3.SS2.p9.1.m1.1.1.2" xref="S3.SS2.p9.1.m1.1.1.2.cmml">s</mi><mi id="S3.SS2.p9.1.m1.1.1.3" xref="S3.SS2.p9.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.1.m1.1b"><apply id="S3.SS2.p9.1.m1.1.1.cmml" xref="S3.SS2.p9.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.1.m1.1.1.1.cmml" xref="S3.SS2.p9.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p9.1.m1.1.1.2.cmml" xref="S3.SS2.p9.1.m1.1.1.2">ğ‘ </ci><ci id="S3.SS2.p9.1.m1.1.1.3.cmml" xref="S3.SS2.p9.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.1.m1.1c">s_{i}</annotation></semantics></math> is the sum of correct class predictions by the unimodal encoder for modality <math id="S3.SS2.p9.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p9.2.m2.1a"><mi id="S3.SS2.p9.2.m2.1.1" xref="S3.SS2.p9.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.2.m2.1b"><ci id="S3.SS2.p9.2.m2.1.1.cmml" xref="S3.SS2.p9.2.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.2.m2.1c">i</annotation></semantics></math>, <math id="S3.SS2.p9.3.m3.1" class="ltx_Math" alttext="r_{i}" display="inline"><semantics id="S3.SS2.p9.3.m3.1a"><msub id="S3.SS2.p9.3.m3.1.1" xref="S3.SS2.p9.3.m3.1.1.cmml"><mi id="S3.SS2.p9.3.m3.1.1.2" xref="S3.SS2.p9.3.m3.1.1.2.cmml">r</mi><mi id="S3.SS2.p9.3.m3.1.1.3" xref="S3.SS2.p9.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.3.m3.1b"><apply id="S3.SS2.p9.3.m3.1.1.cmml" xref="S3.SS2.p9.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.3.m3.1.1.1.cmml" xref="S3.SS2.p9.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p9.3.m3.1.1.2.cmml" xref="S3.SS2.p9.3.m3.1.1.2">ğ‘Ÿ</ci><ci id="S3.SS2.p9.3.m3.1.1.3.cmml" xref="S3.SS2.p9.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.3.m3.1c">r_{i}</annotation></semantics></math> the relative performance of each encoder compared to the average performance of the others, <math id="S3.SS2.p9.4.m4.1" class="ltx_Math" alttext="\beta_{i}" display="inline"><semantics id="S3.SS2.p9.4.m4.1a"><msub id="S3.SS2.p9.4.m4.1.1" xref="S3.SS2.p9.4.m4.1.1.cmml"><mi id="S3.SS2.p9.4.m4.1.1.2" xref="S3.SS2.p9.4.m4.1.1.2.cmml">Î²</mi><mi id="S3.SS2.p9.4.m4.1.1.3" xref="S3.SS2.p9.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.4.m4.1b"><apply id="S3.SS2.p9.4.m4.1.1.cmml" xref="S3.SS2.p9.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.4.m4.1.1.1.cmml" xref="S3.SS2.p9.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p9.4.m4.1.1.2.cmml" xref="S3.SS2.p9.4.m4.1.1.2">ğ›½</ci><ci id="S3.SS2.p9.4.m4.1.1.3.cmml" xref="S3.SS2.p9.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.4.m4.1c">\beta_{i}</annotation></semantics></math> the max value of the coefficients and <math id="S3.SS2.p9.5.m5.1" class="ltx_Math" alttext="k_{i}" display="inline"><semantics id="S3.SS2.p9.5.m5.1a"><msub id="S3.SS2.p9.5.m5.1.1" xref="S3.SS2.p9.5.m5.1.1.cmml"><mi id="S3.SS2.p9.5.m5.1.1.2" xref="S3.SS2.p9.5.m5.1.1.2.cmml">k</mi><mi id="S3.SS2.p9.5.m5.1.1.3" xref="S3.SS2.p9.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.5.m5.1b"><apply id="S3.SS2.p9.5.m5.1.1.cmml" xref="S3.SS2.p9.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.5.m5.1.1.1.cmml" xref="S3.SS2.p9.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p9.5.m5.1.1.2.cmml" xref="S3.SS2.p9.5.m5.1.1.2">ğ‘˜</ci><ci id="S3.SS2.p9.5.m5.1.1.3.cmml" xref="S3.SS2.p9.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.5.m5.1c">k_{i}</annotation></semantics></math> the final balancing coefficient for each modality, where <math id="S3.SS2.p9.6.m6.1" class="ltx_Math" alttext="\alpha\in\mathbb{R}^{+}" display="inline"><semantics id="S3.SS2.p9.6.m6.1a"><mrow id="S3.SS2.p9.6.m6.1.1" xref="S3.SS2.p9.6.m6.1.1.cmml"><mi id="S3.SS2.p9.6.m6.1.1.2" xref="S3.SS2.p9.6.m6.1.1.2.cmml">Î±</mi><mo id="S3.SS2.p9.6.m6.1.1.1" xref="S3.SS2.p9.6.m6.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.p9.6.m6.1.1.3" xref="S3.SS2.p9.6.m6.1.1.3.cmml"><mi id="S3.SS2.p9.6.m6.1.1.3.2" xref="S3.SS2.p9.6.m6.1.1.3.2.cmml">â„</mi><mo id="S3.SS2.p9.6.m6.1.1.3.3" xref="S3.SS2.p9.6.m6.1.1.3.3.cmml">+</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.6.m6.1b"><apply id="S3.SS2.p9.6.m6.1.1.cmml" xref="S3.SS2.p9.6.m6.1.1"><in id="S3.SS2.p9.6.m6.1.1.1.cmml" xref="S3.SS2.p9.6.m6.1.1.1"></in><ci id="S3.SS2.p9.6.m6.1.1.2.cmml" xref="S3.SS2.p9.6.m6.1.1.2">ğ›¼</ci><apply id="S3.SS2.p9.6.m6.1.1.3.cmml" xref="S3.SS2.p9.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p9.6.m6.1.1.3.1.cmml" xref="S3.SS2.p9.6.m6.1.1.3">superscript</csymbol><ci id="S3.SS2.p9.6.m6.1.1.3.2.cmml" xref="S3.SS2.p9.6.m6.1.1.3.2">â„</ci><plus id="S3.SS2.p9.6.m6.1.1.3.3.cmml" xref="S3.SS2.p9.6.m6.1.1.3.3"></plus></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.6.m6.1c">\alpha\in\mathbb{R}^{+}</annotation></semantics></math> and <math id="S3.SS2.p9.7.m7.1" class="ltx_Math" alttext="\beta_{max}\in\mathbb{R}^{+}" display="inline"><semantics id="S3.SS2.p9.7.m7.1a"><mrow id="S3.SS2.p9.7.m7.1.1" xref="S3.SS2.p9.7.m7.1.1.cmml"><msub id="S3.SS2.p9.7.m7.1.1.2" xref="S3.SS2.p9.7.m7.1.1.2.cmml"><mi id="S3.SS2.p9.7.m7.1.1.2.2" xref="S3.SS2.p9.7.m7.1.1.2.2.cmml">Î²</mi><mrow id="S3.SS2.p9.7.m7.1.1.2.3" xref="S3.SS2.p9.7.m7.1.1.2.3.cmml"><mi id="S3.SS2.p9.7.m7.1.1.2.3.2" xref="S3.SS2.p9.7.m7.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p9.7.m7.1.1.2.3.1" xref="S3.SS2.p9.7.m7.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p9.7.m7.1.1.2.3.3" xref="S3.SS2.p9.7.m7.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p9.7.m7.1.1.2.3.1a" xref="S3.SS2.p9.7.m7.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p9.7.m7.1.1.2.3.4" xref="S3.SS2.p9.7.m7.1.1.2.3.4.cmml">x</mi></mrow></msub><mo id="S3.SS2.p9.7.m7.1.1.1" xref="S3.SS2.p9.7.m7.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.p9.7.m7.1.1.3" xref="S3.SS2.p9.7.m7.1.1.3.cmml"><mi id="S3.SS2.p9.7.m7.1.1.3.2" xref="S3.SS2.p9.7.m7.1.1.3.2.cmml">â„</mi><mo id="S3.SS2.p9.7.m7.1.1.3.3" xref="S3.SS2.p9.7.m7.1.1.3.3.cmml">+</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.7.m7.1b"><apply id="S3.SS2.p9.7.m7.1.1.cmml" xref="S3.SS2.p9.7.m7.1.1"><in id="S3.SS2.p9.7.m7.1.1.1.cmml" xref="S3.SS2.p9.7.m7.1.1.1"></in><apply id="S3.SS2.p9.7.m7.1.1.2.cmml" xref="S3.SS2.p9.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p9.7.m7.1.1.2.1.cmml" xref="S3.SS2.p9.7.m7.1.1.2">subscript</csymbol><ci id="S3.SS2.p9.7.m7.1.1.2.2.cmml" xref="S3.SS2.p9.7.m7.1.1.2.2">ğ›½</ci><apply id="S3.SS2.p9.7.m7.1.1.2.3.cmml" xref="S3.SS2.p9.7.m7.1.1.2.3"><times id="S3.SS2.p9.7.m7.1.1.2.3.1.cmml" xref="S3.SS2.p9.7.m7.1.1.2.3.1"></times><ci id="S3.SS2.p9.7.m7.1.1.2.3.2.cmml" xref="S3.SS2.p9.7.m7.1.1.2.3.2">ğ‘š</ci><ci id="S3.SS2.p9.7.m7.1.1.2.3.3.cmml" xref="S3.SS2.p9.7.m7.1.1.2.3.3">ğ‘</ci><ci id="S3.SS2.p9.7.m7.1.1.2.3.4.cmml" xref="S3.SS2.p9.7.m7.1.1.2.3.4">ğ‘¥</ci></apply></apply><apply id="S3.SS2.p9.7.m7.1.1.3.cmml" xref="S3.SS2.p9.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p9.7.m7.1.1.3.1.cmml" xref="S3.SS2.p9.7.m7.1.1.3">superscript</csymbol><ci id="S3.SS2.p9.7.m7.1.1.3.2.cmml" xref="S3.SS2.p9.7.m7.1.1.3.2">â„</ci><plus id="S3.SS2.p9.7.m7.1.1.3.3.cmml" xref="S3.SS2.p9.7.m7.1.1.3.3"></plus></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.7.m7.1c">\beta_{max}\in\mathbb{R}^{+}</annotation></semantics></math>, <math id="S3.SS2.p9.8.m8.1" class="ltx_Math" alttext="\beta_{max}\geq 1" display="inline"><semantics id="S3.SS2.p9.8.m8.1a"><mrow id="S3.SS2.p9.8.m8.1.1" xref="S3.SS2.p9.8.m8.1.1.cmml"><msub id="S3.SS2.p9.8.m8.1.1.2" xref="S3.SS2.p9.8.m8.1.1.2.cmml"><mi id="S3.SS2.p9.8.m8.1.1.2.2" xref="S3.SS2.p9.8.m8.1.1.2.2.cmml">Î²</mi><mrow id="S3.SS2.p9.8.m8.1.1.2.3" xref="S3.SS2.p9.8.m8.1.1.2.3.cmml"><mi id="S3.SS2.p9.8.m8.1.1.2.3.2" xref="S3.SS2.p9.8.m8.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p9.8.m8.1.1.2.3.1" xref="S3.SS2.p9.8.m8.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p9.8.m8.1.1.2.3.3" xref="S3.SS2.p9.8.m8.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p9.8.m8.1.1.2.3.1a" xref="S3.SS2.p9.8.m8.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p9.8.m8.1.1.2.3.4" xref="S3.SS2.p9.8.m8.1.1.2.3.4.cmml">x</mi></mrow></msub><mo id="S3.SS2.p9.8.m8.1.1.1" xref="S3.SS2.p9.8.m8.1.1.1.cmml">â‰¥</mo><mn id="S3.SS2.p9.8.m8.1.1.3" xref="S3.SS2.p9.8.m8.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.8.m8.1b"><apply id="S3.SS2.p9.8.m8.1.1.cmml" xref="S3.SS2.p9.8.m8.1.1"><geq id="S3.SS2.p9.8.m8.1.1.1.cmml" xref="S3.SS2.p9.8.m8.1.1.1"></geq><apply id="S3.SS2.p9.8.m8.1.1.2.cmml" xref="S3.SS2.p9.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p9.8.m8.1.1.2.1.cmml" xref="S3.SS2.p9.8.m8.1.1.2">subscript</csymbol><ci id="S3.SS2.p9.8.m8.1.1.2.2.cmml" xref="S3.SS2.p9.8.m8.1.1.2.2">ğ›½</ci><apply id="S3.SS2.p9.8.m8.1.1.2.3.cmml" xref="S3.SS2.p9.8.m8.1.1.2.3"><times id="S3.SS2.p9.8.m8.1.1.2.3.1.cmml" xref="S3.SS2.p9.8.m8.1.1.2.3.1"></times><ci id="S3.SS2.p9.8.m8.1.1.2.3.2.cmml" xref="S3.SS2.p9.8.m8.1.1.2.3.2">ğ‘š</ci><ci id="S3.SS2.p9.8.m8.1.1.2.3.3.cmml" xref="S3.SS2.p9.8.m8.1.1.2.3.3">ğ‘</ci><ci id="S3.SS2.p9.8.m8.1.1.2.3.4.cmml" xref="S3.SS2.p9.8.m8.1.1.2.3.4">ğ‘¥</ci></apply></apply><cn type="integer" id="S3.SS2.p9.8.m8.1.1.3.cmml" xref="S3.SS2.p9.8.m8.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.8.m8.1c">\beta_{max}\geq 1</annotation></semantics></math> are predetermined hyperparameters. Finally, <math id="S3.SS2.p9.9.m9.1" class="ltx_Math" alttext="tanh" display="inline"><semantics id="S3.SS2.p9.9.m9.1a"><mrow id="S3.SS2.p9.9.m9.1.1" xref="S3.SS2.p9.9.m9.1.1.cmml"><mi id="S3.SS2.p9.9.m9.1.1.2" xref="S3.SS2.p9.9.m9.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p9.9.m9.1.1.1" xref="S3.SS2.p9.9.m9.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p9.9.m9.1.1.3" xref="S3.SS2.p9.9.m9.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p9.9.m9.1.1.1a" xref="S3.SS2.p9.9.m9.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p9.9.m9.1.1.4" xref="S3.SS2.p9.9.m9.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p9.9.m9.1.1.1b" xref="S3.SS2.p9.9.m9.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p9.9.m9.1.1.5" xref="S3.SS2.p9.9.m9.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.9.m9.1b"><apply id="S3.SS2.p9.9.m9.1.1.cmml" xref="S3.SS2.p9.9.m9.1.1"><times id="S3.SS2.p9.9.m9.1.1.1.cmml" xref="S3.SS2.p9.9.m9.1.1.1"></times><ci id="S3.SS2.p9.9.m9.1.1.2.cmml" xref="S3.SS2.p9.9.m9.1.1.2">ğ‘¡</ci><ci id="S3.SS2.p9.9.m9.1.1.3.cmml" xref="S3.SS2.p9.9.m9.1.1.3">ğ‘</ci><ci id="S3.SS2.p9.9.m9.1.1.4.cmml" xref="S3.SS2.p9.9.m9.1.1.4">ğ‘›</ci><ci id="S3.SS2.p9.9.m9.1.1.5.cmml" xref="S3.SS2.p9.9.m9.1.1.5">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.9.m9.1c">tanh</annotation></semantics></math> is the hyperbolic tangent function, providing a smooth normalization of <math id="S3.SS2.p9.10.m10.1" class="ltx_Math" alttext="r_{i}" display="inline"><semantics id="S3.SS2.p9.10.m10.1a"><msub id="S3.SS2.p9.10.m10.1.1" xref="S3.SS2.p9.10.m10.1.1.cmml"><mi id="S3.SS2.p9.10.m10.1.1.2" xref="S3.SS2.p9.10.m10.1.1.2.cmml">r</mi><mi id="S3.SS2.p9.10.m10.1.1.3" xref="S3.SS2.p9.10.m10.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.10.m10.1b"><apply id="S3.SS2.p9.10.m10.1.1.cmml" xref="S3.SS2.p9.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.p9.10.m10.1.1.1.cmml" xref="S3.SS2.p9.10.m10.1.1">subscript</csymbol><ci id="S3.SS2.p9.10.m10.1.1.2.cmml" xref="S3.SS2.p9.10.m10.1.1.2">ğ‘Ÿ</ci><ci id="S3.SS2.p9.10.m10.1.1.3.cmml" xref="S3.SS2.p9.10.m10.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.10.m10.1c">r_{i}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.p10" class="ltx_para">
<p id="S3.SS2.p10.17" class="ltx_p">The utilization of two distinct cases for <math id="S3.SS2.p10.1.m1.1" class="ltx_Math" alttext="b_{i}" display="inline"><semantics id="S3.SS2.p10.1.m1.1a"><msub id="S3.SS2.p10.1.m1.1.1" xref="S3.SS2.p10.1.m1.1.1.cmml"><mi id="S3.SS2.p10.1.m1.1.1.2" xref="S3.SS2.p10.1.m1.1.1.2.cmml">b</mi><mi id="S3.SS2.p10.1.m1.1.1.3" xref="S3.SS2.p10.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.1.m1.1b"><apply id="S3.SS2.p10.1.m1.1.1.cmml" xref="S3.SS2.p10.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p10.1.m1.1.1.1.cmml" xref="S3.SS2.p10.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p10.1.m1.1.1.2.cmml" xref="S3.SS2.p10.1.m1.1.1.2">ğ‘</ci><ci id="S3.SS2.p10.1.m1.1.1.3.cmml" xref="S3.SS2.p10.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.1.m1.1c">b_{i}</annotation></semantics></math> arises from the varied behaviors we aim to achieve with the balancing method. When <math id="S3.SS2.p10.2.m2.1" class="ltx_Math" alttext="r_{i}&lt;1" display="inline"><semantics id="S3.SS2.p10.2.m2.1a"><mrow id="S3.SS2.p10.2.m2.1.1" xref="S3.SS2.p10.2.m2.1.1.cmml"><msub id="S3.SS2.p10.2.m2.1.1.2" xref="S3.SS2.p10.2.m2.1.1.2.cmml"><mi id="S3.SS2.p10.2.m2.1.1.2.2" xref="S3.SS2.p10.2.m2.1.1.2.2.cmml">r</mi><mi id="S3.SS2.p10.2.m2.1.1.2.3" xref="S3.SS2.p10.2.m2.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS2.p10.2.m2.1.1.1" xref="S3.SS2.p10.2.m2.1.1.1.cmml">&lt;</mo><mn id="S3.SS2.p10.2.m2.1.1.3" xref="S3.SS2.p10.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.2.m2.1b"><apply id="S3.SS2.p10.2.m2.1.1.cmml" xref="S3.SS2.p10.2.m2.1.1"><lt id="S3.SS2.p10.2.m2.1.1.1.cmml" xref="S3.SS2.p10.2.m2.1.1.1"></lt><apply id="S3.SS2.p10.2.m2.1.1.2.cmml" xref="S3.SS2.p10.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p10.2.m2.1.1.2.1.cmml" xref="S3.SS2.p10.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.p10.2.m2.1.1.2.2.cmml" xref="S3.SS2.p10.2.m2.1.1.2.2">ğ‘Ÿ</ci><ci id="S3.SS2.p10.2.m2.1.1.2.3.cmml" xref="S3.SS2.p10.2.m2.1.1.2.3">ğ‘–</ci></apply><cn type="integer" id="S3.SS2.p10.2.m2.1.1.3.cmml" xref="S3.SS2.p10.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.2.m2.1c">r_{i}&lt;1</annotation></semantics></math>, it indicates that other modalities perform, on average, worse than modality <math id="S3.SS2.p10.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p10.3.m3.1a"><mi id="S3.SS2.p10.3.m3.1.1" xref="S3.SS2.p10.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.3.m3.1b"><ci id="S3.SS2.p10.3.m3.1.1.cmml" xref="S3.SS2.p10.3.m3.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.3.m3.1c">i</annotation></semantics></math>, necessitating a reduction in the learning rate of modality <math id="S3.SS2.p10.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p10.4.m4.1a"><mi id="S3.SS2.p10.4.m4.1.1" xref="S3.SS2.p10.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.4.m4.1b"><ci id="S3.SS2.p10.4.m4.1.1.cmml" xref="S3.SS2.p10.4.m4.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.4.m4.1c">i</annotation></semantics></math> to slow its learning pace. To achieve this, we set <math id="S3.SS2.p10.5.m5.1" class="ltx_Math" alttext="b_{i}=2" display="inline"><semantics id="S3.SS2.p10.5.m5.1a"><mrow id="S3.SS2.p10.5.m5.1.1" xref="S3.SS2.p10.5.m5.1.1.cmml"><msub id="S3.SS2.p10.5.m5.1.1.2" xref="S3.SS2.p10.5.m5.1.1.2.cmml"><mi id="S3.SS2.p10.5.m5.1.1.2.2" xref="S3.SS2.p10.5.m5.1.1.2.2.cmml">b</mi><mi id="S3.SS2.p10.5.m5.1.1.2.3" xref="S3.SS2.p10.5.m5.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS2.p10.5.m5.1.1.1" xref="S3.SS2.p10.5.m5.1.1.1.cmml">=</mo><mn id="S3.SS2.p10.5.m5.1.1.3" xref="S3.SS2.p10.5.m5.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.5.m5.1b"><apply id="S3.SS2.p10.5.m5.1.1.cmml" xref="S3.SS2.p10.5.m5.1.1"><eq id="S3.SS2.p10.5.m5.1.1.1.cmml" xref="S3.SS2.p10.5.m5.1.1.1"></eq><apply id="S3.SS2.p10.5.m5.1.1.2.cmml" xref="S3.SS2.p10.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p10.5.m5.1.1.2.1.cmml" xref="S3.SS2.p10.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS2.p10.5.m5.1.1.2.2.cmml" xref="S3.SS2.p10.5.m5.1.1.2.2">ğ‘</ci><ci id="S3.SS2.p10.5.m5.1.1.2.3.cmml" xref="S3.SS2.p10.5.m5.1.1.2.3">ğ‘–</ci></apply><cn type="integer" id="S3.SS2.p10.5.m5.1.1.3.cmml" xref="S3.SS2.p10.5.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.5.m5.1c">b_{i}=2</annotation></semantics></math>, placing us in the portion of Figure <a href="#S3.F2" title="Figure 2 â€£ 3.2 Balancing â€£ 3 Method â€£ Improving Multimodal Learning with Multi-Loss Gradient Modulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> where <math id="S3.SS2.p10.6.m6.2" class="ltx_Math" alttext="r_{i}\in[0,1]" display="inline"><semantics id="S3.SS2.p10.6.m6.2a"><mrow id="S3.SS2.p10.6.m6.2.3" xref="S3.SS2.p10.6.m6.2.3.cmml"><msub id="S3.SS2.p10.6.m6.2.3.2" xref="S3.SS2.p10.6.m6.2.3.2.cmml"><mi id="S3.SS2.p10.6.m6.2.3.2.2" xref="S3.SS2.p10.6.m6.2.3.2.2.cmml">r</mi><mi id="S3.SS2.p10.6.m6.2.3.2.3" xref="S3.SS2.p10.6.m6.2.3.2.3.cmml">i</mi></msub><mo id="S3.SS2.p10.6.m6.2.3.1" xref="S3.SS2.p10.6.m6.2.3.1.cmml">âˆˆ</mo><mrow id="S3.SS2.p10.6.m6.2.3.3.2" xref="S3.SS2.p10.6.m6.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.p10.6.m6.2.3.3.2.1" xref="S3.SS2.p10.6.m6.2.3.3.1.cmml">[</mo><mn id="S3.SS2.p10.6.m6.1.1" xref="S3.SS2.p10.6.m6.1.1.cmml">0</mn><mo id="S3.SS2.p10.6.m6.2.3.3.2.2" xref="S3.SS2.p10.6.m6.2.3.3.1.cmml">,</mo><mn id="S3.SS2.p10.6.m6.2.2" xref="S3.SS2.p10.6.m6.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS2.p10.6.m6.2.3.3.2.3" xref="S3.SS2.p10.6.m6.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.6.m6.2b"><apply id="S3.SS2.p10.6.m6.2.3.cmml" xref="S3.SS2.p10.6.m6.2.3"><in id="S3.SS2.p10.6.m6.2.3.1.cmml" xref="S3.SS2.p10.6.m6.2.3.1"></in><apply id="S3.SS2.p10.6.m6.2.3.2.cmml" xref="S3.SS2.p10.6.m6.2.3.2"><csymbol cd="ambiguous" id="S3.SS2.p10.6.m6.2.3.2.1.cmml" xref="S3.SS2.p10.6.m6.2.3.2">subscript</csymbol><ci id="S3.SS2.p10.6.m6.2.3.2.2.cmml" xref="S3.SS2.p10.6.m6.2.3.2.2">ğ‘Ÿ</ci><ci id="S3.SS2.p10.6.m6.2.3.2.3.cmml" xref="S3.SS2.p10.6.m6.2.3.2.3">ğ‘–</ci></apply><interval closure="closed" id="S3.SS2.p10.6.m6.2.3.3.1.cmml" xref="S3.SS2.p10.6.m6.2.3.3.2"><cn type="integer" id="S3.SS2.p10.6.m6.1.1.cmml" xref="S3.SS2.p10.6.m6.1.1">0</cn><cn type="integer" id="S3.SS2.p10.6.m6.2.2.cmml" xref="S3.SS2.p10.6.m6.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.6.m6.2c">r_{i}\in[0,1]</annotation></semantics></math>. The extent of the learning deceleration can vary and is influenced by the slope of the <math id="S3.SS2.p10.7.m7.1" class="ltx_Math" alttext="tanh" display="inline"><semantics id="S3.SS2.p10.7.m7.1a"><mrow id="S3.SS2.p10.7.m7.1.1" xref="S3.SS2.p10.7.m7.1.1.cmml"><mi id="S3.SS2.p10.7.m7.1.1.2" xref="S3.SS2.p10.7.m7.1.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p10.7.m7.1.1.1" xref="S3.SS2.p10.7.m7.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p10.7.m7.1.1.3" xref="S3.SS2.p10.7.m7.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p10.7.m7.1.1.1a" xref="S3.SS2.p10.7.m7.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p10.7.m7.1.1.4" xref="S3.SS2.p10.7.m7.1.1.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p10.7.m7.1.1.1b" xref="S3.SS2.p10.7.m7.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p10.7.m7.1.1.5" xref="S3.SS2.p10.7.m7.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.7.m7.1b"><apply id="S3.SS2.p10.7.m7.1.1.cmml" xref="S3.SS2.p10.7.m7.1.1"><times id="S3.SS2.p10.7.m7.1.1.1.cmml" xref="S3.SS2.p10.7.m7.1.1.1"></times><ci id="S3.SS2.p10.7.m7.1.1.2.cmml" xref="S3.SS2.p10.7.m7.1.1.2">ğ‘¡</ci><ci id="S3.SS2.p10.7.m7.1.1.3.cmml" xref="S3.SS2.p10.7.m7.1.1.3">ğ‘</ci><ci id="S3.SS2.p10.7.m7.1.1.4.cmml" xref="S3.SS2.p10.7.m7.1.1.4">ğ‘›</ci><ci id="S3.SS2.p10.7.m7.1.1.5.cmml" xref="S3.SS2.p10.7.m7.1.1.5">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.7.m7.1c">tanh</annotation></semantics></math> function parameterized by <math id="S3.SS2.p10.8.m8.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS2.p10.8.m8.1a"><mi id="S3.SS2.p10.8.m8.1.1" xref="S3.SS2.p10.8.m8.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.8.m8.1b"><ci id="S3.SS2.p10.8.m8.1.1.cmml" xref="S3.SS2.p10.8.m8.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.8.m8.1c">a</annotation></semantics></math>; a larger <math id="S3.SS2.p10.9.m9.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS2.p10.9.m9.1a"><mi id="S3.SS2.p10.9.m9.1.1" xref="S3.SS2.p10.9.m9.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.9.m9.1b"><ci id="S3.SS2.p10.9.m9.1.1.cmml" xref="S3.SS2.p10.9.m9.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.9.m9.1c">a</annotation></semantics></math> value results in more pronounced changes in the learning rate. Conversely, when <math id="S3.SS2.p10.10.m10.1" class="ltx_Math" alttext="r_{i}&gt;1" display="inline"><semantics id="S3.SS2.p10.10.m10.1a"><mrow id="S3.SS2.p10.10.m10.1.1" xref="S3.SS2.p10.10.m10.1.1.cmml"><msub id="S3.SS2.p10.10.m10.1.1.2" xref="S3.SS2.p10.10.m10.1.1.2.cmml"><mi id="S3.SS2.p10.10.m10.1.1.2.2" xref="S3.SS2.p10.10.m10.1.1.2.2.cmml">r</mi><mi id="S3.SS2.p10.10.m10.1.1.2.3" xref="S3.SS2.p10.10.m10.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS2.p10.10.m10.1.1.1" xref="S3.SS2.p10.10.m10.1.1.1.cmml">&gt;</mo><mn id="S3.SS2.p10.10.m10.1.1.3" xref="S3.SS2.p10.10.m10.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.10.m10.1b"><apply id="S3.SS2.p10.10.m10.1.1.cmml" xref="S3.SS2.p10.10.m10.1.1"><gt id="S3.SS2.p10.10.m10.1.1.1.cmml" xref="S3.SS2.p10.10.m10.1.1.1"></gt><apply id="S3.SS2.p10.10.m10.1.1.2.cmml" xref="S3.SS2.p10.10.m10.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p10.10.m10.1.1.2.1.cmml" xref="S3.SS2.p10.10.m10.1.1.2">subscript</csymbol><ci id="S3.SS2.p10.10.m10.1.1.2.2.cmml" xref="S3.SS2.p10.10.m10.1.1.2.2">ğ‘Ÿ</ci><ci id="S3.SS2.p10.10.m10.1.1.2.3.cmml" xref="S3.SS2.p10.10.m10.1.1.2.3">ğ‘–</ci></apply><cn type="integer" id="S3.SS2.p10.10.m10.1.1.3.cmml" xref="S3.SS2.p10.10.m10.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.10.m10.1c">r_{i}&gt;1</annotation></semantics></math>, indicating that other modalities generally outperform modality <math id="S3.SS2.p10.11.m11.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p10.11.m11.1a"><mi id="S3.SS2.p10.11.m11.1.1" xref="S3.SS2.p10.11.m11.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.11.m11.1b"><ci id="S3.SS2.p10.11.m11.1.1.cmml" xref="S3.SS2.p10.11.m11.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.11.m11.1c">i</annotation></semantics></math>, we aim to increase the learning rate of modality <math id="S3.SS2.p10.12.m12.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.p10.12.m12.1a"><mi id="S3.SS2.p10.12.m12.1.1" xref="S3.SS2.p10.12.m12.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.12.m12.1b"><ci id="S3.SS2.p10.12.m12.1.1.cmml" xref="S3.SS2.p10.12.m12.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.12.m12.1c">i</annotation></semantics></math>, requiring <math id="S3.SS2.p10.13.m13.1" class="ltx_Math" alttext="k_{i}&gt;1" display="inline"><semantics id="S3.SS2.p10.13.m13.1a"><mrow id="S3.SS2.p10.13.m13.1.1" xref="S3.SS2.p10.13.m13.1.1.cmml"><msub id="S3.SS2.p10.13.m13.1.1.2" xref="S3.SS2.p10.13.m13.1.1.2.cmml"><mi id="S3.SS2.p10.13.m13.1.1.2.2" xref="S3.SS2.p10.13.m13.1.1.2.2.cmml">k</mi><mi id="S3.SS2.p10.13.m13.1.1.2.3" xref="S3.SS2.p10.13.m13.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS2.p10.13.m13.1.1.1" xref="S3.SS2.p10.13.m13.1.1.1.cmml">&gt;</mo><mn id="S3.SS2.p10.13.m13.1.1.3" xref="S3.SS2.p10.13.m13.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.13.m13.1b"><apply id="S3.SS2.p10.13.m13.1.1.cmml" xref="S3.SS2.p10.13.m13.1.1"><gt id="S3.SS2.p10.13.m13.1.1.1.cmml" xref="S3.SS2.p10.13.m13.1.1.1"></gt><apply id="S3.SS2.p10.13.m13.1.1.2.cmml" xref="S3.SS2.p10.13.m13.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p10.13.m13.1.1.2.1.cmml" xref="S3.SS2.p10.13.m13.1.1.2">subscript</csymbol><ci id="S3.SS2.p10.13.m13.1.1.2.2.cmml" xref="S3.SS2.p10.13.m13.1.1.2.2">ğ‘˜</ci><ci id="S3.SS2.p10.13.m13.1.1.2.3.cmml" xref="S3.SS2.p10.13.m13.1.1.2.3">ğ‘–</ci></apply><cn type="integer" id="S3.SS2.p10.13.m13.1.1.3.cmml" xref="S3.SS2.p10.13.m13.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.13.m13.1c">k_{i}&gt;1</annotation></semantics></math>. The extent of the acceleration is more sensitive, as large values may lead to model divergence. To control this, we introduce the hyperparameter <math id="S3.SS2.p10.14.m14.1" class="ltx_Math" alttext="b_{max}" display="inline"><semantics id="S3.SS2.p10.14.m14.1a"><msub id="S3.SS2.p10.14.m14.1.1" xref="S3.SS2.p10.14.m14.1.1.cmml"><mi id="S3.SS2.p10.14.m14.1.1.2" xref="S3.SS2.p10.14.m14.1.1.2.cmml">b</mi><mrow id="S3.SS2.p10.14.m14.1.1.3" xref="S3.SS2.p10.14.m14.1.1.3.cmml"><mi id="S3.SS2.p10.14.m14.1.1.3.2" xref="S3.SS2.p10.14.m14.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p10.14.m14.1.1.3.1" xref="S3.SS2.p10.14.m14.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p10.14.m14.1.1.3.3" xref="S3.SS2.p10.14.m14.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p10.14.m14.1.1.3.1a" xref="S3.SS2.p10.14.m14.1.1.3.1.cmml">â€‹</mo><mi id="S3.SS2.p10.14.m14.1.1.3.4" xref="S3.SS2.p10.14.m14.1.1.3.4.cmml">x</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.14.m14.1b"><apply id="S3.SS2.p10.14.m14.1.1.cmml" xref="S3.SS2.p10.14.m14.1.1"><csymbol cd="ambiguous" id="S3.SS2.p10.14.m14.1.1.1.cmml" xref="S3.SS2.p10.14.m14.1.1">subscript</csymbol><ci id="S3.SS2.p10.14.m14.1.1.2.cmml" xref="S3.SS2.p10.14.m14.1.1.2">ğ‘</ci><apply id="S3.SS2.p10.14.m14.1.1.3.cmml" xref="S3.SS2.p10.14.m14.1.1.3"><times id="S3.SS2.p10.14.m14.1.1.3.1.cmml" xref="S3.SS2.p10.14.m14.1.1.3.1"></times><ci id="S3.SS2.p10.14.m14.1.1.3.2.cmml" xref="S3.SS2.p10.14.m14.1.1.3.2">ğ‘š</ci><ci id="S3.SS2.p10.14.m14.1.1.3.3.cmml" xref="S3.SS2.p10.14.m14.1.1.3.3">ğ‘</ci><ci id="S3.SS2.p10.14.m14.1.1.3.4.cmml" xref="S3.SS2.p10.14.m14.1.1.3.4">ğ‘¥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.14.m14.1c">b_{max}</annotation></semantics></math>, which sets the maximum permissible increase. For instance, if <math id="S3.SS2.p10.15.m15.1" class="ltx_Math" alttext="b_{max}=10" display="inline"><semantics id="S3.SS2.p10.15.m15.1a"><mrow id="S3.SS2.p10.15.m15.1.1" xref="S3.SS2.p10.15.m15.1.1.cmml"><msub id="S3.SS2.p10.15.m15.1.1.2" xref="S3.SS2.p10.15.m15.1.1.2.cmml"><mi id="S3.SS2.p10.15.m15.1.1.2.2" xref="S3.SS2.p10.15.m15.1.1.2.2.cmml">b</mi><mrow id="S3.SS2.p10.15.m15.1.1.2.3" xref="S3.SS2.p10.15.m15.1.1.2.3.cmml"><mi id="S3.SS2.p10.15.m15.1.1.2.3.2" xref="S3.SS2.p10.15.m15.1.1.2.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p10.15.m15.1.1.2.3.1" xref="S3.SS2.p10.15.m15.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p10.15.m15.1.1.2.3.3" xref="S3.SS2.p10.15.m15.1.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p10.15.m15.1.1.2.3.1a" xref="S3.SS2.p10.15.m15.1.1.2.3.1.cmml">â€‹</mo><mi id="S3.SS2.p10.15.m15.1.1.2.3.4" xref="S3.SS2.p10.15.m15.1.1.2.3.4.cmml">x</mi></mrow></msub><mo id="S3.SS2.p10.15.m15.1.1.1" xref="S3.SS2.p10.15.m15.1.1.1.cmml">=</mo><mn id="S3.SS2.p10.15.m15.1.1.3" xref="S3.SS2.p10.15.m15.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.15.m15.1b"><apply id="S3.SS2.p10.15.m15.1.1.cmml" xref="S3.SS2.p10.15.m15.1.1"><eq id="S3.SS2.p10.15.m15.1.1.1.cmml" xref="S3.SS2.p10.15.m15.1.1.1"></eq><apply id="S3.SS2.p10.15.m15.1.1.2.cmml" xref="S3.SS2.p10.15.m15.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p10.15.m15.1.1.2.1.cmml" xref="S3.SS2.p10.15.m15.1.1.2">subscript</csymbol><ci id="S3.SS2.p10.15.m15.1.1.2.2.cmml" xref="S3.SS2.p10.15.m15.1.1.2.2">ğ‘</ci><apply id="S3.SS2.p10.15.m15.1.1.2.3.cmml" xref="S3.SS2.p10.15.m15.1.1.2.3"><times id="S3.SS2.p10.15.m15.1.1.2.3.1.cmml" xref="S3.SS2.p10.15.m15.1.1.2.3.1"></times><ci id="S3.SS2.p10.15.m15.1.1.2.3.2.cmml" xref="S3.SS2.p10.15.m15.1.1.2.3.2">ğ‘š</ci><ci id="S3.SS2.p10.15.m15.1.1.2.3.3.cmml" xref="S3.SS2.p10.15.m15.1.1.2.3.3">ğ‘</ci><ci id="S3.SS2.p10.15.m15.1.1.2.3.4.cmml" xref="S3.SS2.p10.15.m15.1.1.2.3.4">ğ‘¥</ci></apply></apply><cn type="integer" id="S3.SS2.p10.15.m15.1.1.3.cmml" xref="S3.SS2.p10.15.m15.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.15.m15.1c">b_{max}=10</annotation></semantics></math>, we allow the learning rate to increase by up to 10 times based on the performance ratio <math id="S3.SS2.p10.16.m16.1" class="ltx_Math" alttext="r_{i}" display="inline"><semantics id="S3.SS2.p10.16.m16.1a"><msub id="S3.SS2.p10.16.m16.1.1" xref="S3.SS2.p10.16.m16.1.1.cmml"><mi id="S3.SS2.p10.16.m16.1.1.2" xref="S3.SS2.p10.16.m16.1.1.2.cmml">r</mi><mi id="S3.SS2.p10.16.m16.1.1.3" xref="S3.SS2.p10.16.m16.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.16.m16.1b"><apply id="S3.SS2.p10.16.m16.1.1.cmml" xref="S3.SS2.p10.16.m16.1.1"><csymbol cd="ambiguous" id="S3.SS2.p10.16.m16.1.1.1.cmml" xref="S3.SS2.p10.16.m16.1.1">subscript</csymbol><ci id="S3.SS2.p10.16.m16.1.1.2.cmml" xref="S3.SS2.p10.16.m16.1.1.2">ğ‘Ÿ</ci><ci id="S3.SS2.p10.16.m16.1.1.3.cmml" xref="S3.SS2.p10.16.m16.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.16.m16.1c">r_{i}</annotation></semantics></math>, as shown in Figure <a href="#S3.F2" title="Figure 2 â€£ 3.2 Balancing â€£ 3 Method â€£ Improving Multimodal Learning with Multi-Loss Gradient Modulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> when <math id="S3.SS2.p10.17.m17.1" class="ltx_Math" alttext="r_{i}&gt;1" display="inline"><semantics id="S3.SS2.p10.17.m17.1a"><mrow id="S3.SS2.p10.17.m17.1.1" xref="S3.SS2.p10.17.m17.1.1.cmml"><msub id="S3.SS2.p10.17.m17.1.1.2" xref="S3.SS2.p10.17.m17.1.1.2.cmml"><mi id="S3.SS2.p10.17.m17.1.1.2.2" xref="S3.SS2.p10.17.m17.1.1.2.2.cmml">r</mi><mi id="S3.SS2.p10.17.m17.1.1.2.3" xref="S3.SS2.p10.17.m17.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS2.p10.17.m17.1.1.1" xref="S3.SS2.p10.17.m17.1.1.1.cmml">&gt;</mo><mn id="S3.SS2.p10.17.m17.1.1.3" xref="S3.SS2.p10.17.m17.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p10.17.m17.1b"><apply id="S3.SS2.p10.17.m17.1.1.cmml" xref="S3.SS2.p10.17.m17.1.1"><gt id="S3.SS2.p10.17.m17.1.1.1.cmml" xref="S3.SS2.p10.17.m17.1.1.1"></gt><apply id="S3.SS2.p10.17.m17.1.1.2.cmml" xref="S3.SS2.p10.17.m17.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p10.17.m17.1.1.2.1.cmml" xref="S3.SS2.p10.17.m17.1.1.2">subscript</csymbol><ci id="S3.SS2.p10.17.m17.1.1.2.2.cmml" xref="S3.SS2.p10.17.m17.1.1.2.2">ğ‘Ÿ</ci><ci id="S3.SS2.p10.17.m17.1.1.2.3.cmml" xref="S3.SS2.p10.17.m17.1.1.2.3">ğ‘–</ci></apply><cn type="integer" id="S3.SS2.p10.17.m17.1.1.3.cmml" xref="S3.SS2.p10.17.m17.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p10.17.m17.1c">r_{i}&gt;1</annotation></semantics></math>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2405.07930/assets/x1.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="249" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.10.5.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.8.4" class="ltx_text" style="font-size:90%;">Comparing Balancing Coefficients (<math id="S3.F2.5.1.m1.1" class="ltx_Math" alttext="k_{i}" display="inline"><semantics id="S3.F2.5.1.m1.1b"><msub id="S3.F2.5.1.m1.1.1" xref="S3.F2.5.1.m1.1.1.cmml"><mi id="S3.F2.5.1.m1.1.1.2" xref="S3.F2.5.1.m1.1.1.2.cmml">k</mi><mi id="S3.F2.5.1.m1.1.1.3" xref="S3.F2.5.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.5.1.m1.1c"><apply id="S3.F2.5.1.m1.1.1.cmml" xref="S3.F2.5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.F2.5.1.m1.1.1.1.cmml" xref="S3.F2.5.1.m1.1.1">subscript</csymbol><ci id="S3.F2.5.1.m1.1.1.2.cmml" xref="S3.F2.5.1.m1.1.1.2">ğ‘˜</ci><ci id="S3.F2.5.1.m1.1.1.3.cmml" xref="S3.F2.5.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.5.1.m1.1d">k_{i}</annotation></semantics></math>, Eq. <a href="#S3.E7" title="Equation 7 â€£ 3.2 Balancing â€£ 3 Method â€£ Improving Multimodal Learning with Multi-Loss Gradient Modulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>) and Performance Ratios (<math id="S3.F2.6.2.m2.1" class="ltx_Math" alttext="r_{i}" display="inline"><semantics id="S3.F2.6.2.m2.1b"><msub id="S3.F2.6.2.m2.1.1" xref="S3.F2.6.2.m2.1.1.cmml"><mi id="S3.F2.6.2.m2.1.1.2" xref="S3.F2.6.2.m2.1.1.2.cmml">r</mi><mi id="S3.F2.6.2.m2.1.1.3" xref="S3.F2.6.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.6.2.m2.1c"><apply id="S3.F2.6.2.m2.1.1.cmml" xref="S3.F2.6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.F2.6.2.m2.1.1.1.cmml" xref="S3.F2.6.2.m2.1.1">subscript</csymbol><ci id="S3.F2.6.2.m2.1.1.2.cmml" xref="S3.F2.6.2.m2.1.1.2">ğ‘Ÿ</ci><ci id="S3.F2.6.2.m2.1.1.3.cmml" xref="S3.F2.6.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.6.2.m2.1d">r_{i}</annotation></semantics></math>, Eq. <a href="#S3.E5" title="Equation 5 â€£ 3.2 Balancing â€£ 3 Method â€£ Improving Multimodal Learning with Multi-Loss Gradient Modulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) across different <math id="S3.F2.7.3.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.F2.7.3.m3.1b"><mi id="S3.F2.7.3.m3.1.1" xref="S3.F2.7.3.m3.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="S3.F2.7.3.m3.1c"><ci id="S3.F2.7.3.m3.1.1.cmml" xref="S3.F2.7.3.m3.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.7.3.m3.1d">\alpha</annotation></semantics></math> and <math id="S3.F2.8.4.m4.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S3.F2.8.4.m4.1b"><mi id="S3.F2.8.4.m4.1.1" xref="S3.F2.8.4.m4.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="S3.F2.8.4.m4.1c"><ci id="S3.F2.8.4.m4.1.1.cmml" xref="S3.F2.8.4.m4.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.8.4.m4.1d">\beta</annotation></semantics></math> settings.</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">CREMA-D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>:</span> is an emotion recognition dataset with audio and video modalities, featuring 91 actors expressing six emotions. Video frames are sampled at 1 frame-per-second (fps), selecting 3 consecutive frames, while audio segments are sampled at 22 kHz. Audio analysis employs Short-Time Fourier Transform (STFT) with a window size of 512 and a step size of 353 samples to create the log-Mel spectrograms. For advanced models, we adopt preprocessing steps suggested by Goncalves <em id="S4.SS1.p1.1.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S4.SS1.p1.1.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite>, utilizing the full audio recording at 16kHz without STFT and video data without subsampling. We also follow their dataset division, ensuring no actor overlap between training, validation, and test sets. Standard deviation (std) is reported across 3-folds.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">AVE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>:</span> spans 28 event categories of everyday human and animal activities, each with temporally labeled audio-video events lasting at least 2 seconds. Video segments with the event are sampled at 1 fps for 4 frames, with audio resampled at 16 kHz using CREMA-Dâ€™s STFT settings. AVE provides predefined training, validation, and test splits. Standard deviation (std) is calculated from three random seeds on the same test set.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">UCF101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>:</span> showcases real-life action YouTube videos across 101 categories, expanding UCF50. Our analysis focuses on 51 action categories featuring both video and audio modalities, following similar data preparation as the AVE dataset. Model evaluation utilizes the datasetâ€™s 3-fold split, with std reported across these folds.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.07930/assets/x2.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="422" height="169" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">CREMA-D</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.07930/assets/x3.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="211" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">AVE</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2405.07930/assets/x4.png" id="S4.F3.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="211" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F3.sf3.3.2" class="ltx_text" style="font-size:90%;">UCF-101</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">Accuracy of models that differentiate by the backbone encoders (colors), the fusion strategies (Late with a linear and Mid with a MLP classifier), and the balancing techniques (x-axis). Across all datasets, results demonstrate that employing unimodal losses within the Multi-Loss framework and balancing them consistently yields the best performance.</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Backbone Unimodal Encoders</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Our experimental framework aims to demonstrate the broad applicability of our findings across diverse unimodal encoders. We used two type of encoders for each modality: one randomly initialized ResNet and a Transformer-based with pretrained weights.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">In line with prior research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>, we employed ResNet-18 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>, initialized from scratch, as the unimodal encoder for handling both video and audio modalities across all datasets. We extend our analysis to include larger, pre-trained models. Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> on CREMA-D, we deploy the first 12 layers of the Wav2Vec2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> model with self-supervised pretrained weights for speech recognition. For the video modality we extract the facing bounding boxes and afterwards the facial features exploiting EfficientNet-B2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> as a frozen feature descriptor. Both the audio and video features are further refined using each a 5-layer Conformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>. Leveraging similar audio pre-trained encoders for the AVE and UCF-101 datasets did not yield significant improvement due to differences in data distribution, as these datasets lack speech-based audio. As a result, we focused exclusively on experimenting with ResNet for these two datasets.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Multimodal Fusion</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We are outlining five fusion models that combine unimodal features from the encoders to generate the multimodal output. Late Fusion combines the concatenated unimodal features using a linear layer, noted as <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_bold">Late-Linear</span>. Mid Fusion employs a 2-layer Multilayer Perceptron (MLP) to investigate the effects of nonlinear feature combinations on modality imbalances, noted as <span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_bold">Mid-MLP</span>. We align with prior research and experiment with advanced fusion strategies: Feature-wise Linear Modulation (<span id="S4.SS3.p1.1.3" class="ltx_text ltx_font_bold">FiLM</span>)<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> and <span id="S4.SS3.p1.1.4" class="ltx_text ltx_font_bold">Gated</span> mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>. Additionally, we explore integrating a 2-layer Conformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite> model as a Transformer-based (<span id="S4.SS3.p1.1.5" class="ltx_text ltx_font_bold">TF</span>) fusion method, utilizing both class and modality-independent tokens. Our objective is to showcase the impact of balancing methods by assessing their effectiveness across diverse fusion strategies.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we present results on the effectiveness of using different fusion strategies on multimodal training with several multimodal balancing techniques namely MMCosine, MSLR, OGM, AGM, PMR<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The PMR method, as per the provided code, exhibits several instabilities, resulting in exploding values of the classifier prototypes. This instability predominantly affects models with ResNet encoders and certain nonlinear fusion methods. Despite these issues, we opt not to modify their implementation to avoid inadvertently influencing the method incorrectly.</span></span></span>, Multi-Loss and Multi-Loss Balanced (<span id="S5.p1.1.1" class="ltx_text ltx_font_bold">MLB</span>). We show the models trained under the single multimodal objective without any balancing methods, named as <span id="S5.p1.1.2" class="ltx_text ltx_font_bold">Joint Training</span>. Hyperparameter tuning was conducted separately for each method, dataset, backbone encoder, and fusion strategy on the validation set. This ensured fairness with an equal number of trials for each configuration.</p>
</div>
<figure id="S5.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2405.07930/assets/x5.png" id="S5.F4.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="368" height="115" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S5.F4.1" class="ltx_p ltx_figure_panel">
<span id="S5.F4.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:496.9pt;">
<span id="S5.F4.1.1.1" class="ltx_p">(a) Conformer</span>
</span>
<img src="/html/2405.07930/assets/x6.png" id="S5.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="368" height="115" alt="Refer to caption"> 
<span id="S5.F4.1.2" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:496.9pt;">
<span id="S5.F4.1.2.1" class="ltx_p">(b) ResNet</span>
</span></p>
</div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S5.F4.4.2" class="ltx_text" style="font-size:90%;">Confusion Matrices between the unimodal models and the multimodal ones for both backbone encoders, (a) Conformer and (b) ResNet, trained under different balancing methods. Each column of the confusion matrix represents the cases where both unimodal predictions are incorrect, where only one is correct, and where both are correct. MLB consistently balances and improves performance across all categories.</span></figcaption>
</figure>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">In the bar plots presented in Figure <a href="#S4.F3" title="Figure 3 â€£ 4.1 Datasets â€£ 4 Experiments â€£ Improving Multimodal Learning with Multi-Loss Gradient Modulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we assess the previously mentioned models alongside suitable balancing methods across the three datasets. Results highlight that MLB consistently outperforms previous approaches, demonstrating its robustness across various settings. Our comparison with the simpler Multi-Loss strategy reveals that adding unimodal losses often suffices to adequately train the unimodal encoders. In most instances balancing provides a further incremental benefit.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Furthermore, we use the Expected Calibration Error (ECE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite> to assess whether multimodal training enhances model uncertainty awareness. Figure <a href="#S5.F5" title="Figure 5 â€£ 5 Results â€£ Improving Multimodal Learning with Multi-Loss Gradient Modulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> demonstrates that the MLB method, achieving the highest accuracy, also exhibits lower calibration error compared to other methods and to Multi-loss, highlighting the significance of balancing.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2405.07930/assets/x7.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="249" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.3.2" class="ltx_text" style="font-size:90%;">ECE Comparison on Conformer CREMA-D.</span></figcaption>
</figure>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">To verify that the issue of overfitting on one of the modalities is not prevalent to the fusion method, we incorporate alternative fusion strategies, as mentioned in Section <a href="#S4.SS3" title="4.3 Multimodal Fusion â€£ 4 Experiments â€£ Improving Multimodal Learning with Multi-Loss Gradient Modulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>, FiLM, Gated and TF. Both the Multi-Loss and Multi-Loss Balanced employ additional linear classifiers to derive unimodal predictions and calculate the unimodal losses. Figure <a href="#S5.F6" title="Figure 6 â€£ 5 Results â€£ Improving Multimodal Learning with Multi-Loss Gradient Modulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> supports our primary findings, indicating improvements in the learning process across models employing different fusion methods balanced with Multi-Loss and MLB. However, in some instances, we do not observe notable benefits from the addition of the balancing technique.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2405.07930/assets/x8.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="276" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.3.2" class="ltx_text" style="font-size:90%;">Accuracy of models using FiLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>, Gated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite> and TF fusion techniques on the ResNet and Conformer model applied to the CREMA-D dataset.</span></figcaption>
</figure>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">To ascertain whether the lack of training stems from an underperforming modality in the Joint Training schema and to assess the impact of different balancing techniques, we examine the confusion matrices. In Figure <a href="#S5.F4" title="Figure 4 â€£ 5 Results â€£ Improving Multimodal Learning with Multi-Loss Gradient Modulation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we illustrate the label agreement among the two unimodal models and the multimodal ones trained under various methods. Notably, Joint Training with the Conformer model tends to overfit to the video modality, exhibiting high agreement with it. Conversely, the same method applied to the ResNet model demonstrates a stronger reliance on the audio modality, resulting in increased errors on the Video True case. Interestingly, both the pretrained Conformer for video and the ResNet for audio required fewer optimization steps to converge compared to the other modality. This observation leads us to conclude that a modalityâ€™s dominance in the learning process is not solely determined by its predictive power, but also by its ability to reduce the training loss more rapidly.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p">From the same figure, MLB consistently enhances performance across all categories, including Both True, Both False, Audio True, and Video True, indicating across-the-board improvement. In the ResNet model results, we observe that AGM, the best previous method, slightly enhances the scenario where both modalities predict incorrectly (Both False), suggesting improved information discovery.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion and Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we tackle the issue of multimodal models overfitting on one modality, thereby hindering the effective utilization of the remaining modalities. We experiment with state-of-the-art balancing methods, revealing inconsistencies across different models and datasets. To address this, we introduce the Multi-Loss Balanced method. We demonstrate how additional losses derived from each modality can enhance the training of unimodal encoders and provide accurate estimations of their performance, enabling us to balance the learning of each encoder during multimodal training. Our approach incorporates coefficient estimation functions to support both acceleration and deceleration of each modality, while allowing the model to mute the balancing as it converges. We consistently observe improved results across three video-audio datasets, utilizing both ResNet and Transformer-based backbone encoders, along with a variety of fusion methods.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Baevski etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">wav2vec 2.0: A framework for self-supervised learning of speech representations.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib1.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib1.10.2" class="ltx_text" style="font-size:90%;">, 33:12449â€“12460, 2020.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Cao etÂ al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Houwei Cao, DavidÂ G Cooper, MichaelÂ K Keutmann, RubenÂ C Gur, Ani Nenkova, and Ragini Verma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">Crema-d: Crowd-sourced emotional multimodal actors dataset.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib2.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE transactions on affective computing</em><span id="bib.bib2.10.2" class="ltx_text" style="font-size:90%;">, 5(4):377â€“390, 2014.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Du etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Chenzhuang Du, Jiaye Teng, Tingle Li, Yichen Liu, Tianyuan Yuan, Yue Wang, Yang Yuan, and Hang Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">On uni-modal feature learning in supervised multi-modal learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib3.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.01233</em><span id="bib.bib3.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Fan etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Yunfeng Fan, Wenchao Xu, Haozhao Wang, Junxiao Wang, and Song Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">Pmr: Prototypical modal rebalance for multimodal learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib4.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib4.11.3" class="ltx_text" style="font-size:90%;">, pages 20029â€“20038, 2023.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Fujimori etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Naotsuna Fujimori, Rei Endo, Yoshihiko Kawai, and Takahiro Mochizuki.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Modality-specific learning rate control for multimodal classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib5.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Pattern Recognition: 5th Asian Conference, ACPR 2019, Auckland, New Zealand, November 26â€“29, 2019, Revised Selected Papers, Part II 5</em><span id="bib.bib5.11.3" class="ltx_text" style="font-size:90%;">, pages 412â€“422. Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Gat etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Itai Gat, Idan Schwartz, Alexander Schwing, and Tamir Hazan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">Removing bias in multi-modal classifiers: Regularization by maximizing functional entropies.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib6.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib6.10.2" class="ltx_text" style="font-size:90%;">, 33:3197â€“3208, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Goncalves etÂ al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
L. Goncalves, S.-G. Leem, W.-C. Lin, B. Sisman, and C. Busso.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">Versatile audiovisual learning for handling single and multi modalities in emotion regression and classification tasks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib7.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv e-prints (arXiv:2305.07216 )</em><span id="bib.bib7.10.2" class="ltx_text" style="font-size:90%;">, pages 1â€“14, 2023a.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Goncalves etÂ al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Lucas Goncalves, Seong-Gyun Leem, Wei-Cheng Lin, Berrak Sisman, and Carlos Busso.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">Versatile audio-visual learning for handling single and multi modalities in emotion regression and classification tasks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib8.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.07216</em><span id="bib.bib8.10.2" class="ltx_text" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Gulati etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">Conformer: Convolution-augmented transformer for speech recognition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib9.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2005.08100</em><span id="bib.bib9.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Guo etÂ al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
Chuan Guo, Geoff Pleiss, Yu Sun, and KilianÂ Q Weinberger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">On calibration of modern neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib10.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International conference on machine learning</em><span id="bib.bib10.11.3" class="ltx_text" style="font-size:90%;">, pages 1321â€“1330. PMLR, 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">He etÂ al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition, 2015.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Huang etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Yu Huang, Junyang Lin, Chang Zhou, Hongxia Yang, and Longbo Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Modality competition: What makes joint training of multi-modal network fail in deep learning?(provably).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib12.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib12.11.3" class="ltx_text" style="font-size:90%;">, pages 9226â€“9259. PMLR, 2022.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Kiela etÂ al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Douwe Kiela, Edouard Grave, Armand Joulin, and Tomas Mikolov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">Efficient large-scale multi-modal classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib13.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI conference on artificial intelligence</em><span id="bib.bib13.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text" style="font-size:90%;">Li etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">
Hong Li, Xingyu Li, Pengbo Hu, Yinuo Lei, Chunxiao Li, and Yi Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">Boosting multi-modal model performance with adaptive gradient modulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib14.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib14.11.3" class="ltx_text" style="font-size:90%;">, pages 22214â€“22224, 2023.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">Li etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib15.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib15.11.3" class="ltx_text" style="font-size:90%;">, pages 12888â€“12900. PMLR, 2022.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.4.4.1" class="ltx_text" style="font-size:90%;">Lundberg and Lee [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.6.1" class="ltx_text" style="font-size:90%;">
ScottÂ M Lundberg and Su-In Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">A unified approach to interpreting model predictions.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib16.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib16.9.2" class="ltx_text" style="font-size:90%;">, 30, 2017.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Peng etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Xiaokang Peng, Yake Wei, Andong Deng, Dong Wang, and Di Hu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Balanced multimodal learning via on-the-fly gradient modulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib17.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib17.11.3" class="ltx_text" style="font-size:90%;">, pages 8238â€“8247, 2022.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">Perez etÂ al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Ethan Perez, Florian Strub, Harm DeÂ Vries, Vincent Dumoulin, and Aaron Courville.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">Film: Visual reasoning with a general conditioning layer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib18.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI conference on artificial intelligence</em><span id="bib.bib18.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Phan etÂ al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Huy Phan, OliverÂ Y ChÃ©n, MinhÂ C Tran, Philipp Koch, Alfred Mertins, and Maarten DeÂ Vos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Xsleepnet: Multi-view sequential model for automatic sleep staging.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib19.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</em><span id="bib.bib19.10.2" class="ltx_text" style="font-size:90%;">, 44(9):5903â€“5915, 2021.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Soomro etÂ al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
Khurram Soomro, AmirÂ Roshan Zamir, and Mubarak Shah.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">Ucf101: A dataset of 101 human actions classes from videos in the wild.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib20.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1212.0402</em><span id="bib.bib20.10.2" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.4.4.1" class="ltx_text" style="font-size:90%;">Tan and Le [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.6.1" class="ltx_text" style="font-size:90%;">
Mingxing Tan and Quoc Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">Efficientnet: Rethinking model scaling for convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib21.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International conference on machine learning</em><span id="bib.bib21.10.3" class="ltx_text" style="font-size:90%;">, pages 6105â€“6114. PMLR, 2019.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Tian etÂ al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Audio-visual event localization in unconstrained videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib22.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the European conference on computer vision (ECCV)</em><span id="bib.bib22.11.3" class="ltx_text" style="font-size:90%;">, pages 247â€“263, 2018.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Vielzeuf etÂ al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Valentin Vielzeuf, Alexis Lechervy, StÃ©phane Pateux, and FrÃ©dÃ©ric Jurie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">Centralnet: a multilayer approach for multimodal fusion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib23.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</em><span id="bib.bib23.11.3" class="ltx_text" style="font-size:90%;">, pages 0â€“0, 2018.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Wang etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Weiyao Wang, Du Tran, and Matt Feiszli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">What makes training multi-modal classification networks hard?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib24.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em><span id="bib.bib24.11.3" class="ltx_text" style="font-size:90%;">, pages 12695â€“12705, 2020.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="font-size:90%;">Wang etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">
Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, OwaisÂ Khan Mohammed, Saksham Singhal, Subhojit Som, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">Image as a foreign language: Beit pretraining for all vision and vision-language tasks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib25.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2208.10442</em><span id="bib.bib25.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Wolf etÂ al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven LeÂ Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Transformers: State-of-the-art natural language processing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib26.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em><span id="bib.bib26.11.3" class="ltx_text" style="font-size:90%;">, pages 38â€“45, Online, 2020. Association for Computational Linguistics.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Wu etÂ al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Nan Wu, Stanislaw Jastrzebski, Kyunghyun Cho, and KrzysztofÂ J Geras.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">Characterizing and overcoming the greedy nature of learning in multi-modal deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib27.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib27.11.3" class="ltx_text" style="font-size:90%;">, pages 24043â€“24055. PMLR, 2022.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Xu etÂ al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
Ruize Xu, Ruoxuan Feng, Shi-Xiong Zhang, and Di Hu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">Mmcosine: Multi-modal cosine loss towards balanced audio-visual fine-grained learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib28.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em><span id="bib.bib28.11.3" class="ltx_text" style="font-size:90%;">, pages 1â€“5. IEEE, 2023.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.4.4.1" class="ltx_text" style="font-size:90%;">Yao and Mihalcea [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.6.1" class="ltx_text" style="font-size:90%;">
Yiqun Yao and Rada Mihalcea.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">Modality-specific learning rates for effective multimodal additive late-fusion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib29.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Findings of the Association for Computational Linguistics: ACL 2022</em><span id="bib.bib29.10.3" class="ltx_text" style="font-size:90%;">, pages 1824â€“1834. Association for Computational Linguistics, 2022.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.07929" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.07930" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.07930">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.07930" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.07931" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 16:06:22 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
