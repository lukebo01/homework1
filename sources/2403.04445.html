<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.04445] Classist Tools: Social Class Correlates with Performance in NLP</title><meta property="og:description" content="Since the foundational work of William Labov on the social stratification of language (Labov, 1964), linguistics has made concentrated efforts to explore the links between socio-demographic characteristics and language…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Classist Tools: Social Class Correlates with Performance in NLP">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Classist Tools: Social Class Correlates with Performance in NLP">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.04445">

<!--Generated on Fri Apr  5 13:28:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Classist Tools: Social Class Correlates with Performance in NLP</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amanda Cercas Curry
<br class="ltx_break">MilaNLP 
<br class="ltx_break">Bocconi University 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">amanda.cercas@unibocconi.it</span>

<br class="ltx_break"><span id="id2.2.id2" class="ltx_ERROR undefined">\And</span>Giuseppe Attanasio 
<br class="ltx_break">Instituto de Telecomunicações 
<br class="ltx_break">Lisbon, Portugal 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">giuseppeattanasio6@gmail.com</span> 
<br class="ltx_break"><span id="id4.4.id4" class="ltx_ERROR undefined">\AND</span>Zeerak Talat 
<br class="ltx_break">Mohamed Bin Zayed
<br class="ltx_break">University of Artificial Intelligence 
<br class="ltx_break"><span id="id5.5.id5" class="ltx_text ltx_font_typewriter">z@zeerak.org</span> 
<br class="ltx_break"><span id="id6.6.id6" class="ltx_ERROR undefined">\And</span>Dirk Hovy
<br class="ltx_break"> MilaNLP 
<br class="ltx_break">Bocconi University 
<br class="ltx_break"><span id="id7.7.id7" class="ltx_text ltx_font_typewriter">dirk.hovy@unibocconi.it</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">Since the foundational work of William Labov on the social stratification of language <cite class="ltx_cite ltx_citemacro_citep">(Labov, <a href="#bib.bib19" title="" class="ltx_ref">1964</a>)</cite>, linguistics has made concentrated efforts to explore the links between socio-demographic characteristics and language production and perception.
But while there is strong evidence for socio-demographic characteristics in language, they are infrequently used in Natural Language Processing (NLP). Age and gender are somewhat well represented, but Labov’s original target, socioeconomic status, is noticeably absent. And yet it matters. We show empirically that NLP disadvantages less-privileged socioeconomic groups.
We annotate a corpus of 95K utterances from movies with social class, ethnicity and geographical language variety and measure the performance of NLP systems on three tasks: language modelling, automatic speech recognition, and grammar error correction.
We find significant performance disparities that can be attributed to socioeconomic status as well as ethnicity and geographical differences. With NLP technologies becoming ever more ubiquitous and quotidian, they must accommodate all language varieties to avoid disadvantaging already marginalised groups.
We argue for the inclusion of socioeconomic class in future language technologies.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.1" class="ltx_block ltx_align_bottom">
<p id="p1.1.1" class="ltx_p"><span id="p1.1.1.1" class="ltx_text ltx_font_bold">Classist Tools: Social Class Correlates with Performance in NLP</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.2" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.2.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">

<span id="p1.1.2.1.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.2.1.1.1.1" class="ltx_tr">
<span id="p1.1.2.1.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Amanda Cercas Curry</span></span></span>
<span id="p1.1.2.1.1.2.2" class="ltx_tr">
<span id="p1.1.2.1.1.2.2.1" class="ltx_td ltx_align_center">MilaNLP</span></span>
<span id="p1.1.2.1.1.3.3" class="ltx_tr">
<span id="p1.1.2.1.1.3.3.1" class="ltx_td ltx_align_center">Bocconi University</span></span>
<span id="p1.1.2.1.1.4.4" class="ltx_tr">
<span id="p1.1.2.1.1.4.4.1" class="ltx_td ltx_align_center"><span id="p1.1.2.1.1.4.4.1.1" class="ltx_text ltx_font_typewriter">amanda.cercas@unibocconi.it</span></span></span>
</span>
</span></span>                      <span id="p1.1.2.2" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.2.2.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.2.2.1.1.1" class="ltx_tr">
<span id="p1.1.2.2.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.2.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Giuseppe Attanasio</span></span></span>
<span id="p1.1.2.2.1.2.2" class="ltx_tr">
<span id="p1.1.2.2.1.2.2.1" class="ltx_td ltx_align_center">Instituto de Telecomunicações</span></span>
<span id="p1.1.2.2.1.3.3" class="ltx_tr">
<span id="p1.1.2.2.1.3.3.1" class="ltx_td ltx_align_center">Lisbon, Portugal</span></span>
<span id="p1.1.2.2.1.4.4" class="ltx_tr">
<span id="p1.1.2.2.1.4.4.1" class="ltx_td ltx_align_center"><span id="p1.1.2.2.1.4.4.1.1" class="ltx_text ltx_font_typewriter">giuseppeattanasio6@gmail.com</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.3" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.3.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.3.1.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.3.1.1.1.1" class="ltx_tr">
<span id="p1.1.3.1.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Zeerak Talat</span></span></span>
<span id="p1.1.3.1.1.2.2" class="ltx_tr">
<span id="p1.1.3.1.1.2.2.1" class="ltx_td ltx_align_center">Mohamed Bin Zayed</span></span>
<span id="p1.1.3.1.1.3.3" class="ltx_tr">
<span id="p1.1.3.1.1.3.3.1" class="ltx_td ltx_align_center">University of Artificial Intelligence</span></span>
<span id="p1.1.3.1.1.4.4" class="ltx_tr">
<span id="p1.1.3.1.1.4.4.1" class="ltx_td ltx_align_center"><span id="p1.1.3.1.1.4.4.1.1" class="ltx_text ltx_font_typewriter">z@zeerak.org</span></span></span>
</span>
</span></span>                      <span id="p1.1.3.2" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.3.2.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.3.2.1.1.1" class="ltx_tr">
<span id="p1.1.3.2.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.3.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Dirk Hovy</span></span></span>
<span id="p1.1.3.2.1.2.2" class="ltx_tr">
<span id="p1.1.3.2.1.2.2.1" class="ltx_td ltx_align_center"> MilaNLP</span></span>
<span id="p1.1.3.2.1.3.3" class="ltx_tr">
<span id="p1.1.3.2.1.3.3.1" class="ltx_td ltx_align_center">Bocconi University</span></span>
<span id="p1.1.3.2.1.4.4" class="ltx_tr">
<span id="p1.1.3.2.1.4.4.1" class="ltx_td ltx_align_center"><span id="p1.1.3.2.1.4.4.1.1" class="ltx_text ltx_font_typewriter">dirk.hovy@unibocconi.it</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Class or socioeconomic status is an important part of identity formation <cite class="ltx_cite ltx_citemacro_cite">Rickford (<a href="#bib.bib28" title="" class="ltx_ref">1986</a>); Bucholtz and Hall (<a href="#bib.bib6" title="" class="ltx_ref">2005</a>); Eckert (<a href="#bib.bib10" title="" class="ltx_ref">2012</a>)</cite>.
Certain accents, phrases, and expressions, particularly in English, indicate upper, middle, or lower-class or other references to socioeconomic status.
This relationship was first examined by <cite class="ltx_cite ltx_citemacro_citet">Labov (<a href="#bib.bib19" title="" class="ltx_ref">1964</a>)</cite>. He discovered that New Yorkers with greater socioeconomic status pronounce the /R/ sound after vowels, whereas individuals with lower socioeconomic position drop it.
He quantified this observation by asking employees in various department shops (a proxy for socioeconomic rank) for things found on the <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">fourth floor</span>. Then he kept track of how many Rs were dropped in those two words.
He discovered a clear anti-correlation between the store’s (and presumably the speakers’) socioeconomic standing and the quantity of dropped Rs: the higher the status, the fewer dropped Rs.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Since <cite class="ltx_cite ltx_citemacro_citeauthor"><a href="#bib.bib19" title="" class="ltx_ref">Labov</a></cite>’s (<cite class="ltx_cite ltx_citemacro_citeyear"><a href="#bib.bib19" title="" class="ltx_ref">1964</a></cite>) analysis of social stratification and language, linguistics has made concerted efforts to understand how different socio-demographic factors influence language production and perception, and how speakers use them to create identity <cite class="ltx_cite ltx_citemacro_cite">Eckert (<a href="#bib.bib10" title="" class="ltx_ref">2012</a>)</cite>; using both naturally produced language and language from media like TV shows and films <cite class="ltx_cite ltx_citemacro_cite">Stamou (<a href="#bib.bib32" title="" class="ltx_ref">2014</a>)</cite> to study sociolects.
Despite a considerable body of evidence demonstrating links between language and demographic characteristics (the “first wave” of socio-linguistic variation studies), comparatively few socio-demographic factors have been studied in the context of natural language processing (NLP) technology. Given NLP’s role as a product and a tool for understanding other phenomena, it must represent all language varieties.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Existing research on socio-demographic characteristics has primarily concentrated on the signaling effect of certain linguistic variables like age, ethnicity, regional origin, and gender <cite class="ltx_cite ltx_citemacro_cite">Johannsen et al. (<a href="#bib.bib17" title="" class="ltx_ref">2015</a>)</cite>.
Most of these publications focus on any bias toward the particular variable they study.
However, NLP hardly interacts with the second and third waves of sociolinguistics, i.e., how variation 1) shapes local identity and 2) drives language development. To narrow this gap, we use NLP to focus on socioeconomic position.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Using a dataset of 95K utterances from English-language television episodes and movies, we empirically investigate performance as a function of socioeconomic class. We study its effect on vocabulary, automated speech recognition (ASR), language modelling, and grammatical error correction. In all applications, we see strong correlations.
Movie characters are usually typecast to present as a certain class, providing a representative sample without the privacy issues associated with regular subjects. The authenticity of these representations is backed by work in sociolinguistics such as <cite class="ltx_cite ltx_citemacro_citet">McHoul (<a href="#bib.bib21" title="" class="ltx_ref">1987</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Quaglio (<a href="#bib.bib25" title="" class="ltx_ref">2008</a>)</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We discover that the performance of NLP tools is associated with socioeconomic class, geographic variety (US vs. UK), and race across all tasks.
Our findings highlight an important lack of flexibility of NLP tools, and a more fundamental issue: What does it mean if NLP technologies only reliably work for a limited segment of society?</p>
</div>
<section id="S1.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Contributions</h4>

<div id="S1.SS0.SSS0.Px1.p1" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We construct a corpus of 95K utterances from television shows and movie scripts, coded for socioeconomic status, geography, and race.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We empirically show that socioeconomic status measurably impacts the performance of NLP systems across four measures.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Social Class and Its Impact on Language</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Social stratification is the grouping of people based on their socioeconomic status (SES), which is determined by factors such as income, education, wealth, and other characteristics. Different groups are distinguished in terms of power and prestige.
There are various social stratification systems, such as the Indian caste system, indigenous American clans or tribes, and the Western hierarchical class system.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The exact number of social strata is unknown and is likely to vary by country and culture.
However, at least three strata are commonly used to refer to different groups in a society: upper, middle, and lower-class people.
Other systems distinguish between blue collar and white collar jobs.
Recently, the Great British Class Survey <cite class="ltx_cite ltx_citemacro_citep">(GBCS, Savage et al., <a href="#bib.bib29" title="" class="ltx_ref">2013</a>)</cite> has taken an empirical approach to understanding the different social strata, and they propose a seven-level system for the United Kingdom.
Their stratification is based on economic, social, and cultural capital: elite, established middle-class, technical middle-class, new affluent workers, traditional working class, emergent service workers, and precariat.
They derive these classes from a survey conducted over British citizens that received more than <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="160" display="inline"><semantics id="S2.p2.1.m1.1a"><mn id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">160</mn><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><cn type="integer" id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">160</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">160</annotation></semantics></math>K responses.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Social class influences people’s daily lives by granting or limiting access to resources.
Beyond power and prestige, social stratification has a significant impact on people. For example, lower socioeconomic status has been linked to poorer health outcomes and higher mortality. <cite class="ltx_cite ltx_citemacro_cite">Saydah et al. (<a href="#bib.bib30" title="" class="ltx_ref">2013</a>)</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">SES also affects language use from the very early stages of development.
<cite class="ltx_cite ltx_citemacro_citet">Bernstein (<a href="#bib.bib5" title="" class="ltx_ref">1960</a>)</cite> posits that language takes on a different role in middle- and working-class families, where middle-class parents encourage language learning to describe more abstract thinking.
In working-class families, parents are <em id="S2.p4.1.1" class="ltx_emph ltx_font_italic">limited</em> to more concrete and descriptive concepts.
Parents from lower SES tend to interact less with their children, with fewer open-ended questions than parents from higher SES, which shapes language development <cite class="ltx_cite ltx_citemacro_cite">Clark and Casillas (<a href="#bib.bib7" title="" class="ltx_ref">2015</a>)</cite>.
While <cite class="ltx_cite ltx_citemacro_citet">Usategui Basozábal et al. (<a href="#bib.bib35" title="" class="ltx_ref">1992</a>)</cite> show that education reduces this language gap, education has traditionally been one of the most important factors in determining social class and potential for upward social mobility.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Given the well-documented effects of socioeconomic status on language development and use, it stands to reason that social class should be carefully considered as a variable in NLP.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Here, we want to assess the impact a user’s social class has on the performance of NLP systems. To the best of our knowledge, no datasets are available that would suit our purposes: a large enough sample that is (ethically) annotated with the speakers’ socieconomic background. We thus collect our own.
We identify an initial sample of English language television series by selecting the <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="250" display="inline"><semantics id="S3.p1.1.m1.1a"><mn id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">250</mn><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><cn type="integer" id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">250</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">250</annotation></semantics></math> highest-rated series on the International Movie Database (IMDB).<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://www.imdb.com/chart/toptv" title="" class="ltx_ref ltx_href">IMDB top 250 TV shows.</a></span></span></span>
We annotate each show according to attributes,<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Note that we are using <em id="footnote2.1" class="ltx_emph ltx_font_italic">ascribed</em> characteristics rather than <em id="footnote2.2" class="ltx_emph ltx_font_italic">self-reported</em> since we are dealing with fictional characters, however these two are found to be highly correlated based on linguistic features <cite class="ltx_cite ltx_citemacro_cite">Weirich and Simpson (<a href="#bib.bib36" title="" class="ltx_ref">2018</a>)</cite></span></span></span> i.e., class, race, gender, dialect, and geography of the main characters, as well as the genre and time period of the show.
Three annotators familiar with the shows agree on the labels unanimously.
We only consider shows that are scripted to ensure that the language production of each character is controlled by the script-writers and actors to produce a particular vernacular and to avoid collecting highly sensitive information about private individuals.
We therefore exclude cartoons, documentaries.
We further exclude science fiction and historical fiction, as these are likely to operate without strict adherence to existing social structures and language variation.After our filtering and selection, 63 shows remain in consideration.
Of those, 34 are predominantly about white men, and 29 feature a mixture of white women and men.
Only three predominantly feature female characters, and none black or lower-class female characters.
Based on this initial sample, we identified additional shows to augment gaps (e.g., the lack of Black stories) in class and dialect.
Because we want to measure the effect of class separately from gender, race and regional accent, we select the TV shows from the list representing each category.
</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">In our final selection of movies and television shows, we include a total of 19 shows that are based in the United States of America and the United Kingdom.
To ensure racial diversity, we emphasise movies and shows that predominantly represent Black characters in addition to movies and shows that represent white characters.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>At least one of the authors is familiar with every show and movie. We annotated them for race and class based on the setup and main characters.</span></span></span>
We include the U.S.-based shows <span id="S3.p2.1.1" class="ltx_text ltx_font_italic">The Wire</span> and <span id="S3.p2.1.2" class="ltx_text ltx_font_italic">When They See Us</span> (predominantly lower-class Black characters), <span id="S3.p2.1.3" class="ltx_text ltx_font_italic">Fargo</span> (middle-class Black characters), <span id="S3.p2.1.4" class="ltx_text ltx_font_italic">The Fresh Prince of Bel-Air</span> (upper-class Black characters).
For the shows’ centre white characters, we include <span id="S3.p2.1.5" class="ltx_text ltx_font_italic">Trailer Park Boys</span> (lower-class white), <span id="S3.p2.1.6" class="ltx_text ltx_font_italic">The Sopranos</span> (lower-class white), <span id="S3.p2.1.7" class="ltx_text ltx_font_italic">Breaking Bad</span> (middle-class white), and <span id="S3.p2.1.8" class="ltx_text ltx_font_italic">Arrested Development</span> (upper-class white).
The list of shows above, however, predominantly feature male characters. For this reason, we also include shows whose characters represent a wider array of genders.
The shows that we include are <span id="S3.p2.1.9" class="ltx_text ltx_font_italic">Pose</span> (lower-class Black and Latinx), <span id="S3.p2.1.10" class="ltx_text ltx_font_italic">Big Little Lies</span> (middle-class white), and <span id="S3.p2.1.11" class="ltx_text ltx_font_italic">Sex and the City</span> (upper middle-class white).
Finally, to compare with other regional varieties of English, we include movies and shows from different regions in the United Kingdom. The movies and shows that we include are <span id="S3.p2.1.12" class="ltx_text ltx_font_italic">Train Spotting</span> and <span id="S3.p2.1.13" class="ltx_text ltx_font_italic">T2</span> (lower-class white Scottish, predominantly male characters), <span id="S3.p2.1.14" class="ltx_text ltx_font_italic">Derry Girls</span> (lower-class white Irish, primarily female), <span id="S3.p2.1.15" class="ltx_text ltx_font_italic">Downton Abbey</span> (English), <span id="S3.p2.1.16" class="ltx_text ltx_font_italic">The IT Crowd</span> (white, middle-class) and <span id="S3.p2.1.17" class="ltx_text ltx_font_italic">The Crown</span> (upper-class white English). Full details in Table <a href="#A1.T8" title="Table 8 ‣ Appendix A Annotations per show ‣ Classist Tools: Social Class Correlates with Performance in NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> in the Appendix.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">We collect the first season for each of the shows on our list, except for Fargo and The Wire.
For Fargo, we collect the fourth season, which includes Black characters.
For The Wire, we collect seasons 1 and 3, which predominantly feature Black characters but have a significant minority representation of white characters.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">We deliberately chose this approach to data selection from “artificial sources” since collecting class information is highly sensitive and can introduce privacy risks.
Fictional characters, however, are often scripted into particular roles with explicit SES.
A drawback of this methodology is the question of authenticity - the extent to which actors are representative of particular sociolects. Actors reportedly undergo significant training to learn different lects.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>See for example, <a target="_blank" href="https://www.newyorker.com/magazine/2009/11/09/talk-this-way" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.newyorker.com/magazine/2009/11/09/talk-this-way</a></span></span></span> By selecting highly rated shows, we hope to capture good, realistic performances. Moreover, TV show and film data is increasingly used in sociolinguistics <cite class="ltx_cite ltx_citemacro_cite">Stamou (<a href="#bib.bib32" title="" class="ltx_ref">2014</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Quaglio (<a href="#bib.bib25" title="" class="ltx_ref">2008</a>)</cite> found important similarities between real and TV show dialogue. We therefore have reason to believe our dataset provides a fair representation of real speakers.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">Our dataset contains 95K utterances, text and speech, from 19 TV shows and movies (see <a href="#S3.T1" title="In 3 Dataset ‣ Classist Tools: Social Class Correlates with Performance in NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a> for dataset statistics).</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:342.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(91.3pt,-72.1pt) scale(1.72785393902655,1.72785393902655) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Geography</th>
<th id="S3.T1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Class</th>
<th id="S3.T1.1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Episodes</th>
<th id="S3.T1.1.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Utterances</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">USA</th>
<th id="S3.T1.1.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Low</th>
<td id="S3.T1.1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">13</td>
<td id="S3.T1.1.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t">20119</td>
</tr>
<tr id="S3.T1.1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.1.3.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Low, middle</th>
<td id="S3.T1.1.1.3.2.3" class="ltx_td ltx_align_right">13</td>
<td id="S3.T1.1.1.3.2.4" class="ltx_td ltx_align_right">22947</td>
</tr>
<tr id="S3.T1.1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.1.4.3.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Middle</th>
<td id="S3.T1.1.1.4.3.3" class="ltx_td ltx_align_right">7</td>
<td id="S3.T1.1.1.4.3.4" class="ltx_td ltx_align_right">4140</td>
</tr>
<tr id="S3.T1.1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.1.5.4.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Middle, Upper</th>
<td id="S3.T1.1.1.5.4.3" class="ltx_td ltx_align_right">25</td>
<td id="S3.T1.1.1.5.4.4" class="ltx_td ltx_align_right">13221</td>
</tr>
<tr id="S3.T1.1.1.6.5" class="ltx_tr">
<th id="S3.T1.1.1.6.5.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Upper</th>
<td id="S3.T1.1.1.6.5.3" class="ltx_td ltx_align_right">15</td>
<td id="S3.T1.1.1.6.5.4" class="ltx_td ltx_align_right">8561</td>
</tr>
<tr id="S3.T1.1.1.7.6" class="ltx_tr">
<th id="S3.T1.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">EN</th>
<th id="S3.T1.1.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Middle</th>
<td id="S3.T1.1.1.7.6.3" class="ltx_td ltx_align_right">6</td>
<td id="S3.T1.1.1.7.6.4" class="ltx_td ltx_align_right">2271</td>
</tr>
<tr id="S3.T1.1.1.8.7" class="ltx_tr">
<th id="S3.T1.1.1.8.7.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.8.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Upper</th>
<td id="S3.T1.1.1.8.7.3" class="ltx_td ltx_align_right">3</td>
<td id="S3.T1.1.1.8.7.4" class="ltx_td ltx_align_right">1515</td>
</tr>
<tr id="S3.T1.1.1.9.8" class="ltx_tr">
<th id="S3.T1.1.1.9.8.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T1.1.1.9.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Upper, Low</th>
<td id="S3.T1.1.1.9.8.3" class="ltx_td ltx_align_right">7</td>
<td id="S3.T1.1.1.9.8.4" class="ltx_td ltx_align_right">5214</td>
</tr>
<tr id="S3.T1.1.1.10.9" class="ltx_tr">
<th id="S3.T1.1.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">NE</th>
<th id="S3.T1.1.1.10.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Low</th>
<td id="S3.T1.1.1.10.9.3" class="ltx_td ltx_align_right">7</td>
<td id="S3.T1.1.1.10.9.4" class="ltx_td ltx_align_right">3929</td>
</tr>
<tr id="S3.T1.1.1.11.10" class="ltx_tr">
<th id="S3.T1.1.1.11.10.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb"></th>
<th id="S3.T1.1.1.11.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Middle</th>
<td id="S3.T1.1.1.11.10.3" class="ltx_td ltx_align_right ltx_border_bb">2</td>
<td id="S3.T1.1.1.11.10.4" class="ltx_td ltx_align_right ltx_border_bb">1238</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Summary of statistics by geographic variety and class. EN: refers to England, NE to non-English U.K.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Lexical Analysis</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Bernstein (<a href="#bib.bib5" title="" class="ltx_ref">1960</a>)</cite> showed that children from working-class families had significantly smaller vocabularies even when general IQ was controlled for, and <cite class="ltx_cite ltx_citemacro_citet">Flekova et al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite> showed that there is significant lexical and stylistic variation between different social strata, with lexical features being stronger predictors of income than even age.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Thus, we first assess whether our dataset has sufficient linguistic cues to capture lexical differences in language use.
We generally follow the methodology set out in <cite class="ltx_cite ltx_citemacro_citet">Flekova et al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite>. However, we model class as a categorical value. We calculate the following features:</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Surface:</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">Turn length, mean word length per turn, ratio of words longer than five letters, type-token ratio. <cite class="ltx_cite ltx_citemacro_citet">Flekova et al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite> also measure the use of emojis, but this is not relevant to our analysis.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Readability Metrics:</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">Readability metrics aim to measure the complexity of a text generally based on the number of syllables per word and the number of words per sentence. Using Textstat,<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/textstat/textstat" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/textstat/textstat</a></span></span></span> we calculate the Automatic Readability Index (ARI) <cite class="ltx_cite ltx_citemacro_cite">Senter and Smith (<a href="#bib.bib31" title="" class="ltx_ref">1967</a>)</cite>, the Flesch-Kincaid Grade Level (FKG) <cite class="ltx_cite ltx_citemacro_cite">Kincaid et al. (<a href="#bib.bib18" title="" class="ltx_ref">1975</a>)</cite>, the Coleman-Liau Index (CLI) <cite class="ltx_cite ltx_citemacro_cite">Coleman and Liau (<a href="#bib.bib8" title="" class="ltx_ref">1975</a>)</cite>, the Flesch Reading Ease (FRE) <cite class="ltx_cite ltx_citemacro_cite">Flesch (<a href="#bib.bib13" title="" class="ltx_ref">1948</a>)</cite>, the LIX Index (LIX) <cite class="ltx_cite ltx_citemacro_cite">Anderson (<a href="#bib.bib1" title="" class="ltx_ref">1983</a>)</cite>, and the Gunning-Fog Index (FOG) <cite class="ltx_cite ltx_citemacro_cite">Gunning (<a href="#bib.bib15" title="" class="ltx_ref">1968</a>)</cite>. We normalise the text before calculating these metrics by lower-casing and removing punctuating except for periods since these are used by CLI. Note that these were designed to evaluate long-form text.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Syntax:</h4>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">Texts with more nouns and articles as opposed to pronouns and adverbs are considered more formal. We measure the ratio of each POS per turn using Stanza <cite class="ltx_cite ltx_citemacro_cite">Qi et al. (<a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Style:</h4>

<div id="S3.SS1.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px4.p1.1" class="ltx_p">Finally, to capture some notion of speaker style, we measure the number of abstract words<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://onlymyenglish.com/list-of-abstract-nouns/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://onlymyenglish.com/list-of-abstract-nouns/</a></span></span></span>
, the ratio of hapax legomena (HL), and the number of named entities (NER).</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div id="S3.T2.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:744.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(101.5pt,-174.3pt) scale(1.88030557350878,1.88030557350878) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S3.T2.1.1.1.1.2" class="ltx_td ltx_align_right ltx_border_tt">Class</td>
<td id="S3.T2.1.1.1.1.3" class="ltx_td ltx_align_right ltx_border_tt">Gender</td>
<td id="S3.T2.1.1.1.1.4" class="ltx_td ltx_align_right ltx_border_tt">Race</td>
<td id="S3.T2.1.1.1.1.5" class="ltx_td ltx_align_right ltx_border_tt">Geography</td>
</tr>
<tr id="S3.T2.1.1.2.2" class="ltx_tr">
<th id="S3.T2.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Length</th>
<td id="S3.T2.1.1.2.2.2" class="ltx_td ltx_align_right ltx_border_t">-0.033</td>
<td id="S3.T2.1.1.2.2.3" class="ltx_td ltx_align_right ltx_border_t">-0.034</td>
<td id="S3.T2.1.1.2.2.4" class="ltx_td ltx_align_right ltx_border_t">-0.025</td>
<td id="S3.T2.1.1.2.2.5" class="ltx_td ltx_align_right ltx_border_t">0.052</td>
</tr>
<tr id="S3.T2.1.1.3.3" class="ltx_tr">
<th id="S3.T2.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Chars</th>
<td id="S3.T2.1.1.3.3.2" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.3.3.2.1" class="ltx_text ltx_font_bold">-0.437</span></td>
<td id="S3.T2.1.1.3.3.3" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.3.3.3.1" class="ltx_text ltx_font_bold">0.243</span></td>
<td id="S3.T2.1.1.3.3.4" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.3.3.4.1" class="ltx_text ltx_font_bold">0.203</span></td>
<td id="S3.T2.1.1.3.3.5" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.3.3.5.1" class="ltx_text ltx_font_bold">0.155</span></td>
</tr>
<tr id="S3.T2.1.1.4.4" class="ltx_tr">
<th id="S3.T2.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">&gt;5</th>
<td id="S3.T2.1.1.4.4.2" class="ltx_td ltx_align_right">0.073</td>
<td id="S3.T2.1.1.4.4.3" class="ltx_td ltx_align_right">-0.024</td>
<td id="S3.T2.1.1.4.4.4" class="ltx_td ltx_align_right">0.017</td>
<td id="S3.T2.1.1.4.4.5" class="ltx_td ltx_align_right">-0.022</td>
</tr>
<tr id="S3.T2.1.1.5.5" class="ltx_tr">
<th id="S3.T2.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TTR</th>
<td id="S3.T2.1.1.5.5.2" class="ltx_td ltx_align_right">-0.019</td>
<td id="S3.T2.1.1.5.5.3" class="ltx_td ltx_align_right">-0.047</td>
<td id="S3.T2.1.1.5.5.4" class="ltx_td ltx_align_right">-0.046</td>
<td id="S3.T2.1.1.5.5.5" class="ltx_td ltx_align_right">-0.085</td>
</tr>
<tr id="S3.T2.1.1.6.6" class="ltx_tr">
<th id="S3.T2.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">FRE</th>
<td id="S3.T2.1.1.6.6.2" class="ltx_td ltx_align_right ltx_border_t">-0.096</td>
<td id="S3.T2.1.1.6.6.3" class="ltx_td ltx_align_right ltx_border_t">0.033</td>
<td id="S3.T2.1.1.6.6.4" class="ltx_td ltx_align_right ltx_border_t">0.032</td>
<td id="S3.T2.1.1.6.6.5" class="ltx_td ltx_align_right ltx_border_t">0.074</td>
</tr>
<tr id="S3.T2.1.1.7.7" class="ltx_tr">
<th id="S3.T2.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ARI</th>
<td id="S3.T2.1.1.7.7.2" class="ltx_td ltx_align_right">-0.214</td>
<td id="S3.T2.1.1.7.7.3" class="ltx_td ltx_align_right">0.068</td>
<td id="S3.T2.1.1.7.7.4" class="ltx_td ltx_align_right">0.059</td>
<td id="S3.T2.1.1.7.7.5" class="ltx_td ltx_align_right">0.110</td>
</tr>
<tr id="S3.T2.1.1.8.8" class="ltx_tr">
<th id="S3.T2.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">CLI</th>
<td id="S3.T2.1.1.8.8.2" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.8.8.2.1" class="ltx_text ltx_font_bold">0.467</span></td>
<td id="S3.T2.1.1.8.8.3" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.8.8.3.1" class="ltx_text ltx_font_bold">-0.213</span></td>
<td id="S3.T2.1.1.8.8.4" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.8.8.4.1" class="ltx_text ltx_font_bold">-0.201</span></td>
<td id="S3.T2.1.1.8.8.5" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.8.8.5.1" class="ltx_text ltx_font_bold">-0.137</span></td>
</tr>
<tr id="S3.T2.1.1.9.9" class="ltx_tr">
<th id="S3.T2.1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FKG</th>
<td id="S3.T2.1.1.9.9.2" class="ltx_td ltx_align_right">0.007</td>
<td id="S3.T2.1.1.9.9.3" class="ltx_td ltx_align_right">-0.035</td>
<td id="S3.T2.1.1.9.9.4" class="ltx_td ltx_align_right">-0.029</td>
<td id="S3.T2.1.1.9.9.5" class="ltx_td ltx_align_right">-0.005</td>
</tr>
<tr id="S3.T2.1.1.10.10" class="ltx_tr">
<th id="S3.T2.1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FOG</th>
<td id="S3.T2.1.1.10.10.2" class="ltx_td ltx_align_right">0.137</td>
<td id="S3.T2.1.1.10.10.3" class="ltx_td ltx_align_right">-0.031</td>
<td id="S3.T2.1.1.10.10.4" class="ltx_td ltx_align_right">-0.040</td>
<td id="S3.T2.1.1.10.10.5" class="ltx_td ltx_align_right">-0.025</td>
</tr>
<tr id="S3.T2.1.1.11.11" class="ltx_tr">
<th id="S3.T2.1.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LIX</th>
<td id="S3.T2.1.1.11.11.2" class="ltx_td ltx_align_right">0.106</td>
<td id="S3.T2.1.1.11.11.3" class="ltx_td ltx_align_right">0.013</td>
<td id="S3.T2.1.1.11.11.4" class="ltx_td ltx_align_right">-0.007</td>
<td id="S3.T2.1.1.11.11.5" class="ltx_td ltx_align_right">-0.065</td>
</tr>
<tr id="S3.T2.1.1.12.12" class="ltx_tr">
<th id="S3.T2.1.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">NOUN</th>
<td id="S3.T2.1.1.12.12.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T2.1.1.12.12.2.1" class="ltx_text ltx_font_bold">-0.181</span></td>
<td id="S3.T2.1.1.12.12.3" class="ltx_td ltx_align_right ltx_border_t">0.076</td>
<td id="S3.T2.1.1.12.12.4" class="ltx_td ltx_align_right ltx_border_t">-0.035</td>
<td id="S3.T2.1.1.12.12.5" class="ltx_td ltx_align_right ltx_border_t">0.163</td>
</tr>
<tr id="S3.T2.1.1.13.13" class="ltx_tr">
<th id="S3.T2.1.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VERB</th>
<td id="S3.T2.1.1.13.13.2" class="ltx_td ltx_align_right">-0.059</td>
<td id="S3.T2.1.1.13.13.3" class="ltx_td ltx_align_right">0.039</td>
<td id="S3.T2.1.1.13.13.4" class="ltx_td ltx_align_right">-0.002</td>
<td id="S3.T2.1.1.13.13.5" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.13.13.5.1" class="ltx_text ltx_font_bold">0.082</span></td>
</tr>
<tr id="S3.T2.1.1.14.14" class="ltx_tr">
<th id="S3.T2.1.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">PROPN</th>
<td id="S3.T2.1.1.14.14.2" class="ltx_td ltx_align_right">-0.017</td>
<td id="S3.T2.1.1.14.14.3" class="ltx_td ltx_align_right">-0.064</td>
<td id="S3.T2.1.1.14.14.4" class="ltx_td ltx_align_right">-0.099</td>
<td id="S3.T2.1.1.14.14.5" class="ltx_td ltx_align_right">-0.067</td>
</tr>
<tr id="S3.T2.1.1.15.15" class="ltx_tr">
<th id="S3.T2.1.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ADV</th>
<td id="S3.T2.1.1.15.15.2" class="ltx_td ltx_align_right">-0.066</td>
<td id="S3.T2.1.1.15.15.3" class="ltx_td ltx_align_right">-0.068</td>
<td id="S3.T2.1.1.15.15.4" class="ltx_td ltx_align_right">-0.103</td>
<td id="S3.T2.1.1.15.15.5" class="ltx_td ltx_align_right">-0.037</td>
</tr>
<tr id="S3.T2.1.1.16.16" class="ltx_tr">
<th id="S3.T2.1.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ADJ</th>
<td id="S3.T2.1.1.16.16.2" class="ltx_td ltx_align_right">-0.051</td>
<td id="S3.T2.1.1.16.16.3" class="ltx_td ltx_align_right">0.044</td>
<td id="S3.T2.1.1.16.16.4" class="ltx_td ltx_align_right">0.021</td>
<td id="S3.T2.1.1.16.16.5" class="ltx_td ltx_align_right">0.076</td>
</tr>
<tr id="S3.T2.1.1.17.17" class="ltx_tr">
<th id="S3.T2.1.1.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">DET</th>
<td id="S3.T2.1.1.17.17.2" class="ltx_td ltx_align_right">0.037</td>
<td id="S3.T2.1.1.17.17.3" class="ltx_td ltx_align_right">0.059</td>
<td id="S3.T2.1.1.17.17.4" class="ltx_td ltx_align_right">0.024</td>
<td id="S3.T2.1.1.17.17.5" class="ltx_td ltx_align_right">0.026</td>
</tr>
<tr id="S3.T2.1.1.18.18" class="ltx_tr">
<th id="S3.T2.1.1.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">INTJ</th>
<td id="S3.T2.1.1.18.18.2" class="ltx_td ltx_align_right">0.121</td>
<td id="S3.T2.1.1.18.18.3" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.18.18.3.1" class="ltx_text ltx_font_bold">-0.079</span></td>
<td id="S3.T2.1.1.18.18.4" class="ltx_td ltx_align_right"><span id="S3.T2.1.1.18.18.4.1" class="ltx_text ltx_font_bold">-0.093</span></td>
<td id="S3.T2.1.1.18.18.5" class="ltx_td ltx_align_right">-0.044</td>
</tr>
<tr id="S3.T2.1.1.19.19" class="ltx_tr">
<th id="S3.T2.1.1.19.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">PRON</th>
<td id="S3.T2.1.1.19.19.2" class="ltx_td ltx_align_right">-0.004</td>
<td id="S3.T2.1.1.19.19.3" class="ltx_td ltx_align_right">0.014</td>
<td id="S3.T2.1.1.19.19.4" class="ltx_td ltx_align_right">0.008</td>
<td id="S3.T2.1.1.19.19.5" class="ltx_td ltx_align_right">0.020</td>
</tr>
<tr id="S3.T2.1.1.20.20" class="ltx_tr">
<th id="S3.T2.1.1.20.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">NER</th>
<td id="S3.T2.1.1.20.20.2" class="ltx_td ltx_align_right">0.078</td>
<td id="S3.T2.1.1.20.20.3" class="ltx_td ltx_align_right">-0.001</td>
<td id="S3.T2.1.1.20.20.4" class="ltx_td ltx_align_right">0.035</td>
<td id="S3.T2.1.1.20.20.5" class="ltx_td ltx_align_right">-0.010</td>
</tr>
<tr id="S3.T2.1.1.21.21" class="ltx_tr">
<th id="S3.T2.1.1.21.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">abst</th>
<td id="S3.T2.1.1.21.21.2" class="ltx_td ltx_align_right ltx_border_t"><span id="S3.T2.1.1.21.21.2.1" class="ltx_text ltx_font_bold">0.052</span></td>
<td id="S3.T2.1.1.21.21.3" class="ltx_td ltx_align_right ltx_border_t">-0.034</td>
<td id="S3.T2.1.1.21.21.4" class="ltx_td ltx_align_right ltx_border_t">-0.026</td>
<td id="S3.T2.1.1.21.21.5" class="ltx_td ltx_align_right ltx_border_t">-0.034</td>
</tr>
<tr id="S3.T2.1.1.22.22" class="ltx_tr">
<th id="S3.T2.1.1.22.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">HL</th>
<td id="S3.T2.1.1.22.22.2" class="ltx_td ltx_align_right ltx_border_bb">-0.034</td>
<td id="S3.T2.1.1.22.22.3" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T2.1.1.22.22.3.1" class="ltx_text ltx_font_bold">0.109</span></td>
<td id="S3.T2.1.1.22.22.4" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T2.1.1.22.22.4.1" class="ltx_text ltx_font_bold">0.142</span></td>
<td id="S3.T2.1.1.22.22.5" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T2.1.1.22.22.5.1" class="ltx_text ltx_font_bold">0.066</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Coefficients for the logistic regression model. For each class and type of feature, the strongest coefficient is in bold face.</figcaption>
</figure>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S3.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Val</th>
<th id="S3.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Test</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.1.2.1" class="ltx_tr">
<th id="S3.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Lexical</th>
<td id="S3.T3.1.2.1.2" class="ltx_td ltx_border_t"></td>
<td id="S3.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.290</td>
</tr>
<tr id="S3.T3.1.3.2" class="ltx_tr">
<th id="S3.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TF-IDF</th>
<td id="S3.T3.1.3.2.2" class="ltx_td ltx_align_center"><span id="S3.T3.1.3.2.2.1" class="ltx_text ltx_font_bold">0.528</span></td>
<td id="S3.T3.1.3.2.3" class="ltx_td ltx_align_center"><span id="S3.T3.1.3.2.3.1" class="ltx_text ltx_font_bold">0.524</span></td>
</tr>
<tr id="S3.T3.1.4.3" class="ltx_tr">
<th id="S3.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TF-IDF + Lexical</th>
<td id="S3.T3.1.4.3.2" class="ltx_td ltx_align_center">0.297</td>
<td id="S3.T3.1.4.3.3" class="ltx_td ltx_align_center">0.290</td>
</tr>
<tr id="S3.T3.1.5.4" class="ltx_tr">
<th id="S3.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Sentence Embed</th>
<td id="S3.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">0.457</td>
<td id="S3.T3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">0.454</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>F1 Macro for different representations on social class prediction. Best results in bold.</figcaption>
</figure>
<div id="S3.SS1.SSS0.Px4.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px4.p2.1" class="ltx_p">We run a multi-class logistic regression to understand whether the features are correlated to class.
After normalisation, we run a logistic regression with social class as the predicted class and the features above as the predictive features.
In contrast with <cite class="ltx_cite ltx_citemacro_citet">Flekova et al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite>, we find our set of features only mildly predictive in this dataset. Although CLI and the number of characters have strong coefficients, the overall performance is very low. Our best model achieves a macro F1 of 0.16, only slightly better than a most-frequent label baseline (macro-F1 of 0.07).
In addition, we run the same regression for the other sociodemographic aspects: gender, race, and geographical origin. Overall, we find these less predictive with F1 scores similar to the baseline, suggesting the features better capture differences caused by SES than other sociodemographic factors. Table <a href="#S3.T2" title="Table 2 ‣ Style: ‣ 3.1 Lexical Analysis ‣ 3 Dataset ‣ Classist Tools: Social Class Correlates with Performance in NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the coefficients for each sociodemographic factor.</p>
</div>
<div id="S3.SS1.SSS0.Px4.p3" class="ltx_para">
<p id="S3.SS1.SSS0.Px4.p3.1" class="ltx_p">Lexical features do not capture strong differences in SES indicating one of two things: the features are better suited for written text (for example, many of them capture formality), and/or our dataset does not accurately capture linguistic differences. To assess whether linguistic cues in our dataset might help us differentiate between speakers of SES which the previous study did not capture, we experiment with alternative text encodings such as TF-IDF, and sentence embeddings <cite class="ltx_cite ltx_citemacro_cite">Reimers and Gurevych (<a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite>.<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>We use the standard scikit-learn’s <a target="_blank" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" title="" class="ltx_ref ltx_href">TfidfVectorizer</a> collecting uni- and bi-grams for TF-IDF, and <a target="_blank" href="https://huggingface.co/sentence-transformers/all-mpnet-base-v2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/sentence-transformers/all-mpnet-base-v2</a>.</span></span></span>
The results are shown in Table <a href="#S3.T3" title="Table 3 ‣ Style: ‣ 3.1 Lexical Analysis ‣ 3 Dataset ‣ Classist Tools: Social Class Correlates with Performance in NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Logistic regression based on TF-IDF achieves an F1 of 0.52 (x1.8 lexical features).
This finding suggests there are linguistic differences, but the metrics do not capture them.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F1.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2403.04445/assets/figures/jona_WER.png" id="S3.F1.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="371" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Wav2Vec2</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F1.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2403.04445/assets/figures/whisper_WER.png" id="S3.F1.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="371" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Whisper</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Word error rate for the different models, grouped by the speakers’ SES and race, for US-based TV shows. The race in the chart refers to that of the majority of characters in the show.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Social Class and Speech</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Beyond written language, our dataset affords testing whether speech models understand and process accent unevenly.
Therefore, we test disparities in ASR word error rates across social classes.
We calculate the WER for all shows and movies to measure how well ASR technologies capture different variants of English in our sample.</p>
</div>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Models:</h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">We use Wav2Vec2 <cite class="ltx_cite ltx_citemacro_cite">Baevski et al. (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>, XLS-R <cite class="ltx_cite ltx_citemacro_cite">Babu et al. (<a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite>, and Whisper-medium <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite> as implemented in <span id="S4.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_typewriter">transformers</span> <cite class="ltx_cite ltx_citemacro_cite">Wolf et al. (<a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Processing Pipeline:</h4>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.1" class="ltx_p">We use movies and TV shows which are available online.
We first collect the audio and subtitles from OpenSubtitles<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="https://www.opensubtitles.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.opensubtitles.org/</a></span></span></span> and use them as gold reference.<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>Here, we manually checked a random sample to verify the quality.
We henceforth assume that the subtitles are an accurate transcription of what is being said.</span></span></span>
Then, we used one ASR model to transcribe the audio.</p>
</div>
<div id="S4.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p2.1" class="ltx_p">The timestamps from the model-generated transcription and the subtitles often do not match, and neither source provides information about which character is speaking.
We therefore group all utterances from a single episode or movie together into one long string and calculate the WER using Jiwer <cite class="ltx_cite ltx_citemacro_cite">Morris et al. (<a href="#bib.bib22" title="" class="ltx_ref">2004</a>)</cite>. Ideally, we would like to separate by character to get a more fine-grained analysis. However, this approach would require us to keep mappings from utterance to character consistent across episodes. Despite several approaches, we were not able to achieve this goal with the computational power we had. Our analysis is therefore group-based, rather than character-based.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results:</h4>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p1.1" class="ltx_p">Figure <a href="#S3.F1" title="Figure 1 ‣ Style: ‣ 3.1 Lexical Analysis ‣ 3 Dataset ‣ Classist Tools: Social Class Correlates with Performance in NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the differences in word-error-rate by race and class in US TV shows. We find clear trends across models, with effects from both race and class: <span id="S4.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_bold">lower ASR error rates are associated with higher SES and with whiteness.</span> These trends are stronger for the Wav2Vec2 model.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Language Modelling</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Language modelling plays an essential role in downstream applications in NLP, so it is imperative that language models can accurately and equally represent different speakers’ lects. Language models are evaluated through perplexity, a measure of how expected a given sentence is: the higher the perplexity, the more unexpected. Perplexity can give us an indication of how well a model might perform in downstream tasks: <cite class="ltx_cite ltx_citemacro_citet">Gonen et al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite> show that the lower the perplexity of a prompt, the better the prompt can perform the task.
Here, we calculate perplexity as a measure of linguistic acceptability, that is, how ‘expectable’ a particular language variant might be to a given language model. Should models display differing perplexities for different groups, they would put such groups at a disadvantage.</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Models:</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">We experiment with two state-of-the-art base models, Llama 2 <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib33" title="" class="ltx_ref">2023</a>)</cite> and Mistral-7B <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite>. Moreover, we include Zephyr-7B <cite class="ltx_cite ltx_citemacro_cite">Tunstall et al. (<a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>, a Mistral-7B optimized assistant model to test the effect of alignment.
<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>Hugging Face Hub IDs: <a target="_blank" href="https://huggingface.co/meta-llama/Llama-2-7b" title="" class="ltx_ref ltx_href">meta-llama/Llama-2-7b</a>, <a target="_blank" href="https://huggingface.co/mistralai/Mistral-7B-v0.1" title="" class="ltx_ref ltx_href">mistralai/Mistral-7B-v0.1</a>, and <a target="_blank" href="https://huggingface.co/HuggingFaceH4/zephyr-7b-beta" title="" class="ltx_ref ltx_href">HuggingFaceH4/zephyr-7b-beta</a></span></span></span></p>
</div>
<div id="S5.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p2.1" class="ltx_p">We calculate perplexity for each sentence in our dataset after lower-casing them and removing numbers and punctuation. We exclude turns shorter than five tokens as they generally do not differentiate between classes.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results:</h4>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">Table <a href="#S5.T4" title="Table 4 ‣ Results: ‣ 5 Language Modelling ‣ Classist Tools: Social Class Correlates with Performance in NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the mean perplexity and standard deviation for each model and each class for the entire dataset. We do not find significant differences based on geographical location (U.K. vs U.S.A) across the models. We see small differences across groups based on class alone, however, because language is affected by other socio-demographic besides class, we also consider the effect of race and geographic variety.</p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2403.04445/assets/figures/uk_perplexity.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="354" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Mean perplexity among U.K.-based shows by model.</figcaption>
</figure>
<div id="S5.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p2.1" class="ltx_p">We calculate mean perplexity by SES in the U.K. and the U.S.A. Figure <a href="#S5.F2" title="Figure 2 ‣ Results: ‣ 5 Language Modelling ‣ Classist Tools: Social Class Correlates with Performance in NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows that class and perplexity are correlated and we find similar patterns across geographical dialects: higher SES leads to lower perplexity also in U.S.A.-based shows.</p>
</div>
<div id="S5.SS0.SSS0.Px2.p3" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p3.5" class="ltx_p">Next, we consider U.S.-based shows. The breakdown of by class and race are shown in Table <a href="#S5.T5" title="Table 5 ‣ Results: ‣ 5 Language Modelling ‣ Classist Tools: Social Class Correlates with Performance in NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Once both factors are taken into account, a clearer picture emerges: <span id="S5.SS0.SSS0.Px2.p3.5.1" class="ltx_text ltx_font_bold">lower SES leads to higher perplexity.</span> To confirm these differences are significant, we run a one-tailed student’s t-test and find significant differences between classes, particularly for white speakers for both Mistral-7B and Zephyr-7B. In the case of Mistral, lower-class and lower-middle class speakers have significantly higher perplexities than Mid-Upper (<math id="S5.SS0.SSS0.Px2.p3.1.m1.1" class="ltx_Math" alttext="p&lt;0.05" display="inline"><semantics id="S5.SS0.SSS0.Px2.p3.1.m1.1a"><mrow id="S5.SS0.SSS0.Px2.p3.1.m1.1.1" xref="S5.SS0.SSS0.Px2.p3.1.m1.1.1.cmml"><mi id="S5.SS0.SSS0.Px2.p3.1.m1.1.1.2" xref="S5.SS0.SSS0.Px2.p3.1.m1.1.1.2.cmml">p</mi><mo id="S5.SS0.SSS0.Px2.p3.1.m1.1.1.1" xref="S5.SS0.SSS0.Px2.p3.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.SS0.SSS0.Px2.p3.1.m1.1.1.3" xref="S5.SS0.SSS0.Px2.p3.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p3.1.m1.1b"><apply id="S5.SS0.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p3.1.m1.1.1"><lt id="S5.SS0.SSS0.Px2.p3.1.m1.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p3.1.m1.1.1.1"></lt><ci id="S5.SS0.SSS0.Px2.p3.1.m1.1.1.2.cmml" xref="S5.SS0.SSS0.Px2.p3.1.m1.1.1.2">𝑝</ci><cn type="float" id="S5.SS0.SSS0.Px2.p3.1.m1.1.1.3.cmml" xref="S5.SS0.SSS0.Px2.p3.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p3.1.m1.1c">p&lt;0.05</annotation></semantics></math>) and Middle class peakers (<math id="S5.SS0.SSS0.Px2.p3.2.m2.1" class="ltx_Math" alttext="p&lt;0.05" display="inline"><semantics id="S5.SS0.SSS0.Px2.p3.2.m2.1a"><mrow id="S5.SS0.SSS0.Px2.p3.2.m2.1.1" xref="S5.SS0.SSS0.Px2.p3.2.m2.1.1.cmml"><mi id="S5.SS0.SSS0.Px2.p3.2.m2.1.1.2" xref="S5.SS0.SSS0.Px2.p3.2.m2.1.1.2.cmml">p</mi><mo id="S5.SS0.SSS0.Px2.p3.2.m2.1.1.1" xref="S5.SS0.SSS0.Px2.p3.2.m2.1.1.1.cmml">&lt;</mo><mn id="S5.SS0.SSS0.Px2.p3.2.m2.1.1.3" xref="S5.SS0.SSS0.Px2.p3.2.m2.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p3.2.m2.1b"><apply id="S5.SS0.SSS0.Px2.p3.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px2.p3.2.m2.1.1"><lt id="S5.SS0.SSS0.Px2.p3.2.m2.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p3.2.m2.1.1.1"></lt><ci id="S5.SS0.SSS0.Px2.p3.2.m2.1.1.2.cmml" xref="S5.SS0.SSS0.Px2.p3.2.m2.1.1.2">𝑝</ci><cn type="float" id="S5.SS0.SSS0.Px2.p3.2.m2.1.1.3.cmml" xref="S5.SS0.SSS0.Px2.p3.2.m2.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p3.2.m2.1c">p&lt;0.05</annotation></semantics></math>). In the case of Zephyr, the model shows higher perplexities for lower classes than it does for Middle (<math id="S5.SS0.SSS0.Px2.p3.3.m3.1" class="ltx_Math" alttext="p&lt;0.05" display="inline"><semantics id="S5.SS0.SSS0.Px2.p3.3.m3.1a"><mrow id="S5.SS0.SSS0.Px2.p3.3.m3.1.1" xref="S5.SS0.SSS0.Px2.p3.3.m3.1.1.cmml"><mi id="S5.SS0.SSS0.Px2.p3.3.m3.1.1.2" xref="S5.SS0.SSS0.Px2.p3.3.m3.1.1.2.cmml">p</mi><mo id="S5.SS0.SSS0.Px2.p3.3.m3.1.1.1" xref="S5.SS0.SSS0.Px2.p3.3.m3.1.1.1.cmml">&lt;</mo><mn id="S5.SS0.SSS0.Px2.p3.3.m3.1.1.3" xref="S5.SS0.SSS0.Px2.p3.3.m3.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p3.3.m3.1b"><apply id="S5.SS0.SSS0.Px2.p3.3.m3.1.1.cmml" xref="S5.SS0.SSS0.Px2.p3.3.m3.1.1"><lt id="S5.SS0.SSS0.Px2.p3.3.m3.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p3.3.m3.1.1.1"></lt><ci id="S5.SS0.SSS0.Px2.p3.3.m3.1.1.2.cmml" xref="S5.SS0.SSS0.Px2.p3.3.m3.1.1.2">𝑝</ci><cn type="float" id="S5.SS0.SSS0.Px2.p3.3.m3.1.1.3.cmml" xref="S5.SS0.SSS0.Px2.p3.3.m3.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p3.3.m3.1c">p&lt;0.05</annotation></semantics></math>), Mid-Upper (<math id="S5.SS0.SSS0.Px2.p3.4.m4.1" class="ltx_Math" alttext="p&lt;0.01" display="inline"><semantics id="S5.SS0.SSS0.Px2.p3.4.m4.1a"><mrow id="S5.SS0.SSS0.Px2.p3.4.m4.1.1" xref="S5.SS0.SSS0.Px2.p3.4.m4.1.1.cmml"><mi id="S5.SS0.SSS0.Px2.p3.4.m4.1.1.2" xref="S5.SS0.SSS0.Px2.p3.4.m4.1.1.2.cmml">p</mi><mo id="S5.SS0.SSS0.Px2.p3.4.m4.1.1.1" xref="S5.SS0.SSS0.Px2.p3.4.m4.1.1.1.cmml">&lt;</mo><mn id="S5.SS0.SSS0.Px2.p3.4.m4.1.1.3" xref="S5.SS0.SSS0.Px2.p3.4.m4.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p3.4.m4.1b"><apply id="S5.SS0.SSS0.Px2.p3.4.m4.1.1.cmml" xref="S5.SS0.SSS0.Px2.p3.4.m4.1.1"><lt id="S5.SS0.SSS0.Px2.p3.4.m4.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p3.4.m4.1.1.1"></lt><ci id="S5.SS0.SSS0.Px2.p3.4.m4.1.1.2.cmml" xref="S5.SS0.SSS0.Px2.p3.4.m4.1.1.2">𝑝</ci><cn type="float" id="S5.SS0.SSS0.Px2.p3.4.m4.1.1.3.cmml" xref="S5.SS0.SSS0.Px2.p3.4.m4.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p3.4.m4.1c">p&lt;0.01</annotation></semantics></math>) and Upper class speakers (<math id="S5.SS0.SSS0.Px2.p3.5.m5.1" class="ltx_Math" alttext="p&lt;0.01" display="inline"><semantics id="S5.SS0.SSS0.Px2.p3.5.m5.1a"><mrow id="S5.SS0.SSS0.Px2.p3.5.m5.1.1" xref="S5.SS0.SSS0.Px2.p3.5.m5.1.1.cmml"><mi id="S5.SS0.SSS0.Px2.p3.5.m5.1.1.2" xref="S5.SS0.SSS0.Px2.p3.5.m5.1.1.2.cmml">p</mi><mo id="S5.SS0.SSS0.Px2.p3.5.m5.1.1.1" xref="S5.SS0.SSS0.Px2.p3.5.m5.1.1.1.cmml">&lt;</mo><mn id="S5.SS0.SSS0.Px2.p3.5.m5.1.1.3" xref="S5.SS0.SSS0.Px2.p3.5.m5.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p3.5.m5.1b"><apply id="S5.SS0.SSS0.Px2.p3.5.m5.1.1.cmml" xref="S5.SS0.SSS0.Px2.p3.5.m5.1.1"><lt id="S5.SS0.SSS0.Px2.p3.5.m5.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p3.5.m5.1.1.1"></lt><ci id="S5.SS0.SSS0.Px2.p3.5.m5.1.1.2.cmml" xref="S5.SS0.SSS0.Px2.p3.5.m5.1.1.2">𝑝</ci><cn type="float" id="S5.SS0.SSS0.Px2.p3.5.m5.1.1.3.cmml" xref="S5.SS0.SSS0.Px2.p3.5.m5.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p3.5.m5.1c">p&lt;0.01</annotation></semantics></math>).</p>
</div>
<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S5.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Mistral-7B</th>
<th id="S5.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Zephyr-7B</th>
<th id="S5.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Llama 2</th>
</tr>
<tr id="S5.T4.1.2.2" class="ltx_tr">
<th id="S5.T4.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row">Class</th>
<th id="S5.T4.1.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_column">Mean</th>
<th id="S5.T4.1.2.2.3" class="ltx_td ltx_align_right ltx_th ltx_th_column">Std</th>
<th id="S5.T4.1.2.2.4" class="ltx_td ltx_align_right ltx_th ltx_th_column">Mean</th>
<th id="S5.T4.1.2.2.5" class="ltx_td ltx_align_right ltx_th ltx_th_column">Std</th>
<th id="S5.T4.1.2.2.6" class="ltx_td ltx_align_right ltx_th ltx_th_column">Mean</th>
<th id="S5.T4.1.2.2.7" class="ltx_td ltx_align_right ltx_th ltx_th_column">Std</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.3.1" class="ltx_tr">
<th id="S5.T4.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Low</th>
<td id="S5.T4.1.3.1.2" class="ltx_td ltx_align_right ltx_border_t">294.606</td>
<td id="S5.T4.1.3.1.3" class="ltx_td ltx_align_right ltx_border_t">690.361</td>
<td id="S5.T4.1.3.1.4" class="ltx_td ltx_align_right ltx_border_t">415.641</td>
<td id="S5.T4.1.3.1.5" class="ltx_td ltx_align_right ltx_border_t">989.066</td>
<td id="S5.T4.1.3.1.6" class="ltx_td ltx_align_right ltx_border_t">189.804</td>
<td id="S5.T4.1.3.1.7" class="ltx_td ltx_align_right ltx_border_t">361.815</td>
</tr>
<tr id="S5.T4.1.4.2" class="ltx_tr">
<th id="S5.T4.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Low, middle</th>
<td id="S5.T4.1.4.2.2" class="ltx_td ltx_align_right">442.756</td>
<td id="S5.T4.1.4.2.3" class="ltx_td ltx_align_right">911.300</td>
<td id="S5.T4.1.4.2.4" class="ltx_td ltx_align_right">649.462</td>
<td id="S5.T4.1.4.2.5" class="ltx_td ltx_align_right">1441.759</td>
<td id="S5.T4.1.4.2.6" class="ltx_td ltx_align_right">265.114</td>
<td id="S5.T4.1.4.2.7" class="ltx_td ltx_align_right">477.377</td>
</tr>
<tr id="S5.T4.1.5.3" class="ltx_tr">
<th id="S5.T4.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Middle</th>
<td id="S5.T4.1.5.3.2" class="ltx_td ltx_align_right">252.729</td>
<td id="S5.T4.1.5.3.3" class="ltx_td ltx_align_right">536.903</td>
<td id="S5.T4.1.5.3.4" class="ltx_td ltx_align_right">353.667</td>
<td id="S5.T4.1.5.3.5" class="ltx_td ltx_align_right">822.553</td>
<td id="S5.T4.1.5.3.6" class="ltx_td ltx_align_right">177.900</td>
<td id="S5.T4.1.5.3.7" class="ltx_td ltx_align_right">398.560</td>
</tr>
<tr id="S5.T4.1.6.4" class="ltx_tr">
<th id="S5.T4.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Middle, Upper</th>
<td id="S5.T4.1.6.4.2" class="ltx_td ltx_align_right"><span id="S5.T4.1.6.4.2.1" class="ltx_text ltx_font_bold">241.923</span></td>
<td id="S5.T4.1.6.4.3" class="ltx_td ltx_align_right">683.345</td>
<td id="S5.T4.1.6.4.4" class="ltx_td ltx_align_right"><span id="S5.T4.1.6.4.4.1" class="ltx_text ltx_font_bold">332.224</span></td>
<td id="S5.T4.1.6.4.5" class="ltx_td ltx_align_right">999.137</td>
<td id="S5.T4.1.6.4.6" class="ltx_td ltx_align_right"><span id="S5.T4.1.6.4.6.1" class="ltx_text ltx_font_bold">164.807</span></td>
<td id="S5.T4.1.6.4.7" class="ltx_td ltx_align_right">450.446</td>
</tr>
<tr id="S5.T4.1.7.5" class="ltx_tr">
<th id="S5.T4.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Upper</th>
<td id="S5.T4.1.7.5.2" class="ltx_td ltx_align_right">288.324</td>
<td id="S5.T4.1.7.5.3" class="ltx_td ltx_align_right">693.994</td>
<td id="S5.T4.1.7.5.4" class="ltx_td ltx_align_right">399.854</td>
<td id="S5.T4.1.7.5.5" class="ltx_td ltx_align_right">1063.942</td>
<td id="S5.T4.1.7.5.6" class="ltx_td ltx_align_right">190.536</td>
<td id="S5.T4.1.7.5.7" class="ltx_td ltx_align_right">370.958</td>
</tr>
<tr id="S5.T4.1.8.6" class="ltx_tr">
<th id="S5.T4.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Upper, Low</th>
<td id="S5.T4.1.8.6.2" class="ltx_td ltx_align_right ltx_border_bb">282.815</td>
<td id="S5.T4.1.8.6.3" class="ltx_td ltx_align_right ltx_border_bb">575.833</td>
<td id="S5.T4.1.8.6.4" class="ltx_td ltx_align_right ltx_border_bb">385.273</td>
<td id="S5.T4.1.8.6.5" class="ltx_td ltx_align_right ltx_border_bb">876.126</td>
<td id="S5.T4.1.8.6.6" class="ltx_td ltx_align_right ltx_border_bb">190.696</td>
<td id="S5.T4.1.8.6.7" class="ltx_td ltx_align_right ltx_border_bb">358.952</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Mean perplexity and standard deviation per class and model. Best mean result per model in bold.</figcaption>
</figure>
<figure id="S5.T5" class="ltx_table">
<table id="S5.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T5.1.1.1" class="ltx_tr">
<td id="S5.T5.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S5.T5.1.1.1.2" class="ltx_td ltx_border_tt"></td>
<td id="S5.T5.1.1.1.3" class="ltx_td ltx_align_right ltx_border_tt">Perplexity</td>
</tr>
<tr id="S5.T5.1.2.2" class="ltx_tr">
<td id="S5.T5.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">Black</td>
<td id="S5.T5.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">Low</td>
<td id="S5.T5.1.2.2.3" class="ltx_td ltx_align_right ltx_border_t">328.848</td>
</tr>
<tr id="S5.T5.1.3.3" class="ltx_tr">
<td id="S5.T5.1.3.3.1" class="ltx_td"></td>
<td id="S5.T5.1.3.3.2" class="ltx_td ltx_align_left">Middle, Upper</td>
<td id="S5.T5.1.3.3.3" class="ltx_td ltx_align_right"><span id="S5.T5.1.3.3.3.1" class="ltx_text ltx_font_bold">259.673</span></td>
</tr>
<tr id="S5.T5.1.4.4" class="ltx_tr">
<td id="S5.T5.1.4.4.1" class="ltx_td ltx_align_left ltx_border_t">White</td>
<td id="S5.T5.1.4.4.2" class="ltx_td ltx_align_left ltx_border_t">Low</td>
<td id="S5.T5.1.4.4.3" class="ltx_td ltx_align_right ltx_border_t">275.337</td>
</tr>
<tr id="S5.T5.1.5.5" class="ltx_tr">
<td id="S5.T5.1.5.5.1" class="ltx_td"></td>
<td id="S5.T5.1.5.5.2" class="ltx_td ltx_align_left">Low, middle</td>
<td id="S5.T5.1.5.5.3" class="ltx_td ltx_align_right">342.865</td>
</tr>
<tr id="S5.T5.1.6.6" class="ltx_tr">
<td id="S5.T5.1.6.6.1" class="ltx_td"></td>
<td id="S5.T5.1.6.6.2" class="ltx_td ltx_align_left">Middle</td>
<td id="S5.T5.1.6.6.3" class="ltx_td ltx_align_right">229.278</td>
</tr>
<tr id="S5.T5.1.7.7" class="ltx_tr">
<td id="S5.T5.1.7.7.1" class="ltx_td"></td>
<td id="S5.T5.1.7.7.2" class="ltx_td ltx_align_left">Middle, Upper</td>
<td id="S5.T5.1.7.7.3" class="ltx_td ltx_align_right"><span id="S5.T5.1.7.7.3.1" class="ltx_text ltx_font_bold">205.585</span></td>
</tr>
<tr id="S5.T5.1.8.8" class="ltx_tr">
<td id="S5.T5.1.8.8.1" class="ltx_td ltx_border_bb"></td>
<td id="S5.T5.1.8.8.2" class="ltx_td ltx_align_left ltx_border_bb">Upper</td>
<td id="S5.T5.1.8.8.3" class="ltx_td ltx_align_right ltx_border_bb">302.057</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Mean perplexity and standard deviation per class and race. Best results per race in bold.</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Grammar Correction</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Finally, we consider the grammaticality of each sentence. We suspect that grammar features correlate strongly with class, as “correct” grammar is typically a hallmark of signalling higher SES. We calculate the edit distance between the original sentence and the corrected one. Lower edit distance means higher grammaticality.</p>
</div>
<section id="S6.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Models:</h4>

<div id="S6.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px1.p1.1" class="ltx_p">Since we are concerned with potential end-user experience, we choose to evaluate the four most downloaded models on Huggingface since they will reach the largest audience. We use the following models for grammar correction:</p>
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p">T5 Grammar Correction<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a target="_blank" href="https://huggingface.co/vennify/t5-base-grammar-correction" title="" class="ltx_ref ltx_href">T5-based Grammar Correction</a></span></span></span>: A model based on the HappyTransformer<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span><a target="_blank" href="https://github.com/EricFillion/happy-transformer" title="" class="ltx_ref ltx_href">HappyTransformer</a></span></span></span> trained on the JFLEG dataset <cite class="ltx_cite ltx_citemacro_cite">Napoles et al. (<a href="#bib.bib23" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p">Gramformer:<span id="footnote13" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a target="_blank" href="https://github.com/PrithivirajDamodaran/Gramformer" title="" class="ltx_ref ltx_href">Gramformer</a></span></span></span> A seq2seq model fine-tuned on a dataset of WikiEdits.</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p">CoEdit-large<span id="footnote14" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span><a target="_blank" href="https://huggingface.co/grammarly/coedit-large" title="" class="ltx_ref ltx_href">CoEdit</a></span></span></span>: A flan-t5-large based model, finetuned on the CoEdit dataset <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">raheja-etal-2023-coedit</span></cite>.</p>
</div>
</li>
<li id="S6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i4.p1" class="ltx_para">
<p id="S6.I1.i4.p1.1" class="ltx_p">Grammar-synthesis-large<span id="footnote15" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span><a target="_blank" href="https://huggingface.co/pszemraj/flan-t5-large-grammar-synthesis" title="" class="ltx_ref ltx_href">Flan-T5 Correction</a></span></span></span>: A fine-tuned version of Google’s flan-t5-large for grammar correction, trained on an expanded version of the JFLEG dataset.</p>
</div>
</li>
</ul>
</div>
<figure id="S6.T6" class="ltx_table">
<div id="S6.T6.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:223.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(94.5pt,-48.7pt) scale(1.77278928166205,1.77278928166205) ;">
<table id="S6.T6.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T6.1.1.1.1" class="ltx_tr">
<th id="S6.T6.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S6.T6.1.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Happy</th>
<th id="S6.T6.1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">CoEdit</th>
<th id="S6.T6.1.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Flan-T5</th>
<th id="S6.T6.1.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Gramf.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T6.1.1.2.1" class="ltx_tr">
<th id="S6.T6.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Low</th>
<td id="S6.T6.1.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">2.455</td>
<td id="S6.T6.1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">1.963</td>
<td id="S6.T6.1.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t">6.424</td>
<td id="S6.T6.1.1.2.1.5" class="ltx_td ltx_align_right ltx_border_t">2.192</td>
</tr>
<tr id="S6.T6.1.1.3.2" class="ltx_tr">
<th id="S6.T6.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Mid-Low</th>
<td id="S6.T6.1.1.3.2.2" class="ltx_td ltx_align_right">2.590</td>
<td id="S6.T6.1.1.3.2.3" class="ltx_td ltx_align_right">2.138</td>
<td id="S6.T6.1.1.3.2.4" class="ltx_td ltx_align_right">12.050</td>
<td id="S6.T6.1.1.3.2.5" class="ltx_td ltx_align_right">1.553</td>
</tr>
<tr id="S6.T6.1.1.4.3" class="ltx_tr">
<th id="S6.T6.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Middle</th>
<td id="S6.T6.1.1.4.3.2" class="ltx_td ltx_align_right">2.068</td>
<td id="S6.T6.1.1.4.3.3" class="ltx_td ltx_align_right">1.785</td>
<td id="S6.T6.1.1.4.3.4" class="ltx_td ltx_align_right">6.117</td>
<td id="S6.T6.1.1.4.3.5" class="ltx_td ltx_align_right">1.653</td>
</tr>
<tr id="S6.T6.1.1.5.4" class="ltx_tr">
<th id="S6.T6.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Mid-Upper</th>
<td id="S6.T6.1.1.5.4.2" class="ltx_td ltx_align_right">2.866</td>
<td id="S6.T6.1.1.5.4.3" class="ltx_td ltx_align_right">1.944</td>
<td id="S6.T6.1.1.5.4.4" class="ltx_td ltx_align_right">7.952</td>
<td id="S6.T6.1.1.5.4.5" class="ltx_td ltx_align_right">2.091</td>
</tr>
<tr id="S6.T6.1.1.6.5" class="ltx_tr">
<th id="S6.T6.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Upper</th>
<td id="S6.T6.1.1.6.5.2" class="ltx_td ltx_align_right">1.122</td>
<td id="S6.T6.1.1.6.5.3" class="ltx_td ltx_align_right">1.212</td>
<td id="S6.T6.1.1.6.5.4" class="ltx_td ltx_align_right">5.518</td>
<td id="S6.T6.1.1.6.5.5" class="ltx_td ltx_align_right">0.808</td>
</tr>
<tr id="S6.T6.1.1.7.6" class="ltx_tr">
<th id="S6.T6.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">% corrected</th>
<td id="S6.T6.1.1.7.6.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">19.76</td>
<td id="S6.T6.1.1.7.6.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">35.94</td>
<td id="S6.T6.1.1.7.6.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">66.42</td>
<td id="S6.T6.1.1.7.6.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">19.11</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Mean edit distance between the subtitle reference and the grammar-corrected utterance generated by each model, and percentage of sentences with at least one correction.</figcaption>
</figure>
<figure id="S6.F3" class="ltx_figure"><img src="/html/2403.04445/assets/figures/proportion_gram.png" id="S6.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="359" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Proportion of utterances that are grammar-corrected per model and geographical language variety.</figcaption>
</figure>
</section>
<section id="S6.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results:</h4>

<div id="S6.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px2.p1.1" class="ltx_p">Figure <a href="#S6.F3" title="Figure 3 ‣ Models: ‣ 6 Grammar Correction ‣ Classist Tools: Social Class Correlates with Performance in NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the percentage of utterances that each model corrected, grouped by geographical variation. The Flan-T5-based model generates significantly more corrections than the other models, followed by CoEdit-large. Overall, U.K. utterances are corrected slightly more often by the models. Table <a href="#S6.T6" title="Table 6 ‣ Models: ‣ 6 Grammar Correction ‣ Classist Tools: Social Class Correlates with Performance in NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the mean edit distances between the original sentence extracted from the subtitles and the models’ proposed corrections. We find little difference in grammar error correction for most models, even when controlling for geographical and racial differences. However, on further analysis, we find that models generally produce corrections for relatively few utterances.</p>
</div>
<div id="S6.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S6.SS0.SSS0.Px2.p2.1" class="ltx_p">Instead, we consider whether models are more likely to produce edits for different classes. In Figure <a href="#S6.F4" title="Figure 4 ‣ Results: ‣ 6 Grammar Correction ‣ Classist Tools: Social Class Correlates with Performance in NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> we plot the count of utterances with corrections per model and per class. From this, <span id="S6.SS0.SSS0.Px2.p2.1.1" class="ltx_text ltx_font_bold">we can see a clear pattern where models produce corrections more frequently for those of lower SES.</span></p>
</div>
<figure id="S6.F4" class="ltx_figure"><img src="/html/2403.04445/assets/figures/grammar_distribution.png" id="S6.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="362" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Distribution by class of utterances with at least one correction.</figcaption>
</figure>
<div id="S6.SS0.SSS0.Px2.p3" class="ltx_para">
<p id="S6.SS0.SSS0.Px2.p3.1" class="ltx_p">In addition, we notice that often some of these corrections are performed on in-group slang or regional linguistic phenomena, see Table <a href="#S6.T7" title="Table 7 ‣ Results: ‣ 6 Grammar Correction ‣ Classist Tools: Social Class Correlates with Performance in NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. 
<br class="ltx_break"></p>
</div>
<figure id="S6.T7" class="ltx_table">
<table id="S6.T7.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T7.1.1.1" class="ltx_tr">
<th id="S6.T7.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column">
<span id="S6.T7.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T7.1.1.1.1.1.1" class="ltx_p" style="width:213.4pt;"><span id="S6.T7.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">The Crown</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T7.1.2.1" class="ltx_tr">
<td id="S6.T7.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S6.T7.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T7.1.2.1.1.1.1" class="ltx_p" style="width:213.4pt;"><span id="S6.T7.1.2.1.1.1.1.1" class="ltx_text ltx_font_italic">And what a bunch of ice-veined monsters <span id="S6.T7.1.2.1.1.1.1.1.1" class="ltx_text" style="background-color:#FFBFBF;">my family are.</span></span></span>
</span>
</td>
</tr>
<tr id="S6.T7.1.3.2" class="ltx_tr">
<td id="S6.T7.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T7.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T7.1.3.2.1.1.1" class="ltx_p" style="width:213.4pt;"><span id="S6.T7.1.3.2.1.1.1.1" class="ltx_text ltx_font_italic">And what a bunch of ice-veined monsters <span id="S6.T7.1.3.2.1.1.1.1.1" class="ltx_text" style="background-color:#FFBFBF;">my family is.</span></span></span>
</span>
</td>
</tr>
<tr id="S6.T7.1.4.3" class="ltx_tr">
<td id="S6.T7.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T7.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T7.1.4.3.1.1.1" class="ltx_p" style="width:213.4pt;"><span id="S6.T7.1.4.3.1.1.1.1" class="ltx_text ltx_font_bold">Pose</span></span>
</span>
</td>
</tr>
<tr id="S6.T7.1.5.4" class="ltx_tr">
<td id="S6.T7.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S6.T7.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T7.1.5.4.1.1.1" class="ltx_p" style="width:213.4pt;"><span id="S6.T7.1.5.4.1.1.1.1" class="ltx_text ltx_font_italic">The <span id="S6.T7.1.5.4.1.1.1.1.1" class="ltx_text" style="background-color:#FFBFBF;">tittiness</span> of it all.</span></span>
</span>
</td>
</tr>
<tr id="S6.T7.1.6.5" class="ltx_tr">
<td id="S6.T7.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T7.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T7.1.6.5.1.1.1" class="ltx_p" style="width:213.4pt;"><span id="S6.T7.1.6.5.1.1.1.1" class="ltx_text ltx_font_italic">The <span id="S6.T7.1.6.5.1.1.1.1.1" class="ltx_text" style="background-color:#FFBFBF;">quality</span> of it all.</span></span>
</span>
</td>
</tr>
<tr id="S6.T7.1.7.6" class="ltx_tr">
<td id="S6.T7.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T7.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T7.1.7.6.1.1.1" class="ltx_p" style="width:213.4pt;"><span id="S6.T7.1.7.6.1.1.1.1" class="ltx_text ltx_font_bold">Trainspotting</span></span>
</span>
</td>
</tr>
<tr id="S6.T7.1.8.7" class="ltx_tr">
<td id="S6.T7.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S6.T7.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T7.1.8.7.1.1.1" class="ltx_p" style="width:213.4pt;">So, I’m off <span id="S6.T7.1.8.7.1.1.1.1" class="ltx_text" style="background-color:#FFBFBF;">the skag</span>.<span id="footnote16" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span><span id="footnote16.1" class="ltx_text ltx_font_italic">Skag</span> refers to heroin in Scottish slang.</span></span></span></span>
</span>
</td>
</tr>
<tr id="S6.T7.1.9.8" class="ltx_tr">
<td id="S6.T7.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T7.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T7.1.9.8.1.1.1" class="ltx_p" style="width:213.4pt;">So, I’m off <span id="S6.T7.1.9.8.1.1.1.1" class="ltx_text" style="background-color:#FFBFBF;">the grid.</span></span>
</span>
</td>
</tr>
<tr id="S6.T7.1.10.9" class="ltx_tr">
<td id="S6.T7.1.10.9.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T7.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T7.1.10.9.1.1.1" class="ltx_p" style="width:213.4pt;"><span id="S6.T7.1.10.9.1.1.1.1" class="ltx_text ltx_font_bold">Trailer park Boys</span></span>
</span>
</td>
</tr>
<tr id="S6.T7.1.11.10" class="ltx_tr">
<td id="S6.T7.1.11.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S6.T7.1.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T7.1.11.10.1.1.1" class="ltx_p" style="width:213.4pt;"><span id="S6.T7.1.11.10.1.1.1.1" class="ltx_text ltx_font_italic">We <span id="S6.T7.1.11.10.1.1.1.1.1" class="ltx_text" style="background-color:#FFBFBF;">be dope</span> when the new NW A come out, know what <span id="S6.T7.1.11.10.1.1.1.1.2" class="ltx_text" style="background-color:#FFBFBF;">I’m sayin’</span></span></span>
</span>
</td>
</tr>
<tr id="S6.T7.1.12.11" class="ltx_tr">
<td id="S6.T7.1.12.11.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S6.T7.1.12.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S6.T7.1.12.11.1.1.1" class="ltx_p" style="width:213.4pt;"><span id="S6.T7.1.12.11.1.1.1.1" class="ltx_text ltx_font_italic">We <span id="S6.T7.1.12.11.1.1.1.1.1" class="ltx_text" style="background-color:#FFBFBF;">will be happy</span> when the new NWA comes out, know what <span id="S6.T7.1.12.11.1.1.1.1.2" class="ltx_text" style="background-color:#FFBFBF;">I’m saying?</span></span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Examples of grammar correction in different shows. The corrected examples are from the T5 Gramar correction model.</figcaption>
</figure>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Following established work in sociolinguistics, our results support our hypothesis that NLP systems display biases regarding different language varieties beyond geographical and ethnic differences.When it comes to evaluating the justice and fairness of NLP systems, <span id="S7.p1.1.1" class="ltx_text ltx_font_bold">all</span> factors affecting language should be considered.
By dint of their use, NLP systems are setting a standard of language: NLP systems are becoming more common, not only in social contexts calling for formal (i.e. standardised) language but in everyday scenarios. For example, one of the proposed use cases for the Gramformer include correcting text messages as a built-in feature for messaging apps, language models inform systems like automatic correction, and large language models like chatGPT now boast millions of users. This widespread interaction with language technologies reinforces the lect of middle class, white English speakers as the only valid English. Moreover, when we consider findings such as those by <cite class="ltx_cite ltx_citemacro_citet">Gonen et al. (<a href="#bib.bib14" title="" class="ltx_ref">2023</a>)</cite>, which showed that perplexity of a model and the accuracy of the response are negatively correlated, and certain parts of the population have significantly higher mean perplexities, we risk disadvantaging those groups.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">With our work, we are not encouraging more work toward social class prediction from text but to bring awareness to the NLP community about an essential factor of language use that has thus far been mostly ignored in our field. While reporting on the socioeconomic background of study participants and dataset contributors is more complicated than other factors such as age or ethnicity (as it is not a one-point measurement and it is culture-dependent), we encourage researchers to report on this to draw a more accurate picture of the language varieties NLP will capture.
Given the limitations of our study (most notably, restriction to English and group rather than individual scores, see below), we envision several possible avenues for future research in this area: (1) the findings of this study should be validated for other languages where such differences have been observed; (2) the collection of a (possibly larger) corpus of unaggregated speakers; and (3) while we find support from previous research such as <cite class="ltx_cite ltx_citemacro_citet">Flekova et al. (<a href="#bib.bib12" title="" class="ltx_ref">2016</a>)</cite>
for social media data, the findings should also be validated for other modalities of data such as long-form written texts.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Related Work</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">While there has been an uptick in the number of papers tackling gender and racial bias in NLP, work considering other under-privileged communities has lagged behind. <cite class="ltx_cite ltx_citemacro_citet">Curto et al. (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite> investigate bias against the poor in NLP but they do so in terms of the language that surrounds poor people, considering the associations of ‘poor’ and ‘rich’ in embeddings and language models. As a way to mitigate and document biases in NLP, <cite class="ltx_cite ltx_citemacro_citet">Bender and Friedman (<a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite> ask for the socioeconomic status of both the speakers and the annotators to be declared, however they do not suggest any standardised way to measure or report this.
<cite class="ltx_cite ltx_citemacro_citet">Field et al. (<a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite> conduct a survey focus on race but also call for more diversity in NLP in terms of the broader inclusion other underprivileged people such as those from lower socioeconomic status. To the best of our knowledge, we are the first to investigate how NLP systems fare when faced with different SES.</p>
</div>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Conclusion</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">Social class plays an important role in people’s identity construction, and is consequently strongly reflected in their language use. Despite its central status as a variable in sociolinguistics, there is so far little work in NLP engaging with social class and its impact.
Our contributions are twofold: following methods from sociolinguistics, we use a data set of 95K movie transcripts that we annotate for portrayed class, race, and geographical variety (UK vs. US). We use this data to show that linguistic class markers greatly affect the performance of various NLP tools.
Our findings suggest that speakers of low-prestige sociolects experience lower application performance on a range of tasks.
These results suggest that we should actively incorporate social class as a variable in NLP systems if we want to make them more equitable and fair. Our data set also provides a starting point for more explorations of social class in NLP.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Our study presents several limitations:
First, we are limited to the study of class, region, and race.
TV shows and films are biased in their representations of different identities—not all groups are (equally) represented and those that are may be
stereotyped into certain roles.
For example, in the initial top-200 TV shows list, we did not find
any TV shows about lower-class women, Latinxs, upper-class Non-English Britons, or non-white Britons. For this reason, we excluded a gender-based analysis.
Moreover, in this list, the majority of TV shows about Black Americans revolve around the drug trade.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">Second, characters are not necessarily written by, nor perfectly depict the group they represent and this is potentially reflected in their language.
To ensure high quality portrayals, we sourced highly rated and realistic shows, ensuring characters were not caricatures of the groups they portrayed.
Having said that, a sociolinguistics review found that TV show and film data is increasingly common as data for the study of sociolects, particularly with regards to race and class <cite class="ltx_cite ltx_citemacro_cite">Stamou (<a href="#bib.bib32" title="" class="ltx_ref">2014</a>)</cite>.
More specifically, work in sociolinguistics has also defended the use of film data in conversation analysis <cite class="ltx_cite ltx_citemacro_cite">McHoul (<a href="#bib.bib21" title="" class="ltx_ref">1987</a>); Quaglio (<a href="#bib.bib25" title="" class="ltx_ref">2008</a>)</cite>.
Furthermore, TV shows like <span id="Sx1.p2.1.1" class="ltx_text ltx_font_italic">The Wire</span> have been praised for their linguistic authenticity <cite class="ltx_cite ltx_citemacro_cite">Lopez and Bucholtz (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite>, reportedly primarily hiring Baltimore natives.
For these reasons, we believe our study sufficiently captures the relevant linguistic phenomena.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">Despite our best efforts, we were unable to reliably separate the utterances by character across episodes. Instead of introducing more sources of potential mix-ups, we therefore estimate the metrics over all characters in a show. Although this is a limitation in terms of the accuracy of the results, we expect that character-level annotations would strengthen our findings rather than negate them. Generally speaking, each TV show focuses on a single social stratum. A single character belonging to a different stratum would add noise and make the classes less clearly defined, so we suspect our results would be more marked with character-level annotations, not less.
We leave the character-based separation for future work.</p>
</div>
<div id="Sx1.p4" class="ltx_para">
<p id="Sx1.p4.1" class="ltx_p">Finally, though our dataset is large in terms of utterances, it is limited in the number of characters. However, we present a case study hoping to motivate further research that may validate our findings.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Ethical Considerations</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">Our paper is arguing for fairness in the services provided by NLP systems for people of all socioeconomic backgrounds. However, we note that by showing measurable differences in language use across groups, we run the risk of profiling speakers. In our work, we used fictional characters to avoid this, but in order to ensure fairness of service all peoples must be represented. Future work should ensure that this is done respectfully and with consent from any participants.
</p>
</div>
<div id="Sx2.p2" class="ltx_para">
<p id="Sx2.p2.1" class="ltx_p">Furthermore, measuring socioeconomic status is not a straightforward process. The class system used in this paper is fuzzy and tied to Western social structures that are not valid across the world. Future work should focus on measuring in the most appropriate way by following established metrics and guidelines from economics and other fields, such as  <cite class="ltx_cite ltx_citemacro_citet">Savage et al. (<a href="#bib.bib29" title="" class="ltx_ref">2013</a>)</cite>.
We urge any future work to follow ethical guidelines when it comes to dataset and system development.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson (1983)</span>
<span class="ltx_bibblock">
Jonathan Anderson. 1983.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.jstor.org/stable/40031755" title="" class="ltx_ref ltx_href">Lix and rix:
Variations on a little-known readability index</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Journal of Reading</em>, 26(6):490–496.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Babu et al. (2021)</span>
<span class="ltx_bibblock">
Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman
Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, et al.
2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.48550/arXiv.2111.09296" title="" class="ltx_ref ltx_href">XLS-R: Self-supervised cross-lingual speech representation learning at
scale</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.09296</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et al. (2020)</span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://dl.acm.org/doi/pdf/10.5555/3495724.3496768" title="" class="ltx_ref ltx_href">wav2vec
2.0: A framework for self-supervised learning of speech representations</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
33:12449–12460.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bender and Friedman (2018)</span>
<span class="ltx_bibblock">
Emily M. Bender and Batya Friedman. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00041" title="" class="ltx_ref ltx_href">Data statements for
natural language processing: Toward mitigating system bias and enabling
better science</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
6:587–604.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bernstein (1960)</span>
<span class="ltx_bibblock">
Basil Bernstein. 1960.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.2307/586750" title="" class="ltx_ref ltx_href">Language and social class</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">The British journal of sociology</em>, 11(3):271–276.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bucholtz and Hall (2005)</span>
<span class="ltx_bibblock">
Mary Bucholtz and Kira Hall. 2005.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1177/14614456050544" title="" class="ltx_ref ltx_href">Identity and interaction: A sociocultural linguistic approach</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Discourse studies</em>, 7(4-5):585–614.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark and Casillas (2015)</span>
<span class="ltx_bibblock">
Eve V Clark and Marisa Casillas. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1017/CBO9780511806698" title="" class="ltx_ref ltx_href">First language
acquisition</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">The Routledge handbook of linguistics</em>, pages 311–328.
Routledge.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coleman and Liau (1975)</span>
<span class="ltx_bibblock">
Meri Coleman and Ta Lin Liau. 1975.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1037/h0076540" title="" class="ltx_ref ltx_href">A computer
readability formula designed for machine scoring.</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Journal of Applied Psychology</em>, 60(2):283.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Curto et al. (2022)</span>
<span class="ltx_bibblock">
Georgina Curto, Mario Fernando Jojoa Acosta, Flavio Comim, and Begoña
Garcia-Zapirain. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/s00146-022-01494-z" title="" class="ltx_ref ltx_href">Are AI systems
biased against the poor? A machine learning analysis using Word2Vec and
GloVe embeddings</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">AI &amp; society</em>, pages 1–16.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eckert (2012)</span>
<span class="ltx_bibblock">
Penelope Eckert. 2012.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1146/annurev-anthro-092611-145828" title="" class="ltx_ref ltx_href">Three
waves of variation study: The emergence of meaning in the study of
sociolinguistic variation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Annual review of Anthropology</em>, 41(1):87–100.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Field et al. (2021)</span>
<span class="ltx_bibblock">
Anjalie Field, Su Lin Blodgett, Zeerak Waseem, and Yulia Tsvetkov. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.acl-long.149" title="" class="ltx_ref ltx_href">A survey of
race, racism, and anti-racism in NLP</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 1905–1925,
Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Flekova et al. (2016)</span>
<span class="ltx_bibblock">
Lucie Flekova, Daniel Preoţiuc-Pietro, and Lyle Ungar. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P16-2051" title="" class="ltx_ref ltx_href">Exploring stylistic
variation with age and income on Twitter</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers)</em>, pages 313–319,
Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Flesch (1948)</span>
<span class="ltx_bibblock">
Rudolph Flesch. 1948.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1037/h0057532" title="" class="ltx_ref ltx_href">A new
readability yardstick.</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Journal of applied psychology</em>, 32(3):221.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gonen et al. (2023)</span>
<span class="ltx_bibblock">
Hila Gonen, Srini Iyer, Terra Blevins, Noah Smith, and Luke Zettlemoyer. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.findings-emnlp.679" title="" class="ltx_ref ltx_href">Demystifying prompts in language models via perplexity estimation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2023</em>, pages 10136–10148, Singapore. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gunning (1968)</span>
<span class="ltx_bibblock">
Robert Gunning. 1968.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">The Technique of Clear Writing</em>.

</span>
<span class="ltx_bibblock">McGraw-Hill Book Company, New York.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
Guillaume Lample, Lucile Saulnier, et al. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.48550/arXiv.2310.06825" title="" class="ltx_ref ltx_href">Mistral 7b</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.06825</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johannsen et al. (2015)</span>
<span class="ltx_bibblock">
Anders Johannsen, Dirk Hovy, and Anders Søgaard. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/K15-1011" title="" class="ltx_ref ltx_href">Cross-lingual syntactic
variation over age and gender</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Nineteenth Conference on Computational
Natural Language Learning</em>, pages 103–112, Beijing, China. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kincaid et al. (1975)</span>
<span class="ltx_bibblock">
J Peter Kincaid, Robert P Fishburne Jr, Richard L Rogers, and Brad S Chissom.
1975.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://stars.library.ucf.edu/istlibrary/56" title="" class="ltx_ref ltx_href">Derivation of
new readability formulas (automated readability index, fog count and flesch
reading ease formula) for navy enlisted personnel</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Labov (1964)</span>
<span class="ltx_bibblock">
William Labov. 1964.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1017/CBO9780511618208" title="" class="ltx_ref ltx_href"><em id="bib.bib19.1.1.1" class="ltx_emph ltx_font_italic">The social stratification of English in New York city</em></a>.

</span>
<span class="ltx_bibblock">Ph.D. thesis, Columbia University.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lopez and Bucholtz (2017)</span>
<span class="ltx_bibblock">
Qiuana Lopez and Mary Bucholtz. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1075/jls.6.1.01lop" title="" class="ltx_ref ltx_href">“How my hair look?” Linguistic authenticity and racialized gender and
sexuality on The Wire</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Journal of Language and Sexuality</em>, 6(1):1–29.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McHoul (1987)</span>
<span class="ltx_bibblock">
Alec W McHoul. 1987.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1515/semi.1987.67.1-2.83" title="" class="ltx_ref ltx_href">An initial investigation of the usability of fictional conversation for
doing conversation analysis</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Semiotica</em>, 67(1-2):83–104.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morris et al. (2004)</span>
<span class="ltx_bibblock">
Andrew Cameron Morris, Viktoria Maier, and Phil Green. 2004.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.21437/Interspeech.2004-668" title="" class="ltx_ref ltx_href">From WER
and RIL to MER and WIL: Improved evaluation measures for
connected speech recognition</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Interspeech 2004</em>, pages 2765–2768. ISCA.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Napoles et al. (2017)</span>
<span class="ltx_bibblock">
Courtney Napoles, Keisuke Sakaguchi, and Joel Tetreault. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/E17-2037" title="" class="ltx_ref ltx_href">JFLEG: A fluency corpus
and benchmark for grammatical error correction</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 15th Conference of the European Chapter
of the Association for Computational Linguistics: Volume 2, Short Papers</em>,
pages 229–234, Valencia, Spain. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi et al. (2020)</span>
<span class="ltx_bibblock">
Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning.
2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-demos.14" title="" class="ltx_ref ltx_href">Stanza: A
python natural language processing toolkit for many human languages</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics: System Demonstrations</em>, pages 101–108,
Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Quaglio (2008)</span>
<span class="ltx_bibblock">
Paulo Quaglio. 2008.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1075/scl.36" title="" class="ltx_ref ltx_href">Television
dialogue and natural conversation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Corpora and discourse</em>, pages 189–210.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2023)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and
Ilya Sutskever. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.5555/3618408.3619590" title="" class="ltx_ref ltx_href">Robust speech
recognition via large-scale weak supervision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
28492–28518. PMLR.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1410" title="" class="ltx_ref ltx_href">Sentence-BERT:
Sentence embeddings using Siamese BERT-networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 3982–3992, Hong Kong,
China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rickford (1986)</span>
<span class="ltx_bibblock">
John R Rickford. 1986.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1016/0271-5309(86)90024-8" title="" class="ltx_ref ltx_href">The need for new approaches to social class analysis in sociolinguistics</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Language and communication</em>, 6(3):215–221.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Savage et al. (2013)</span>
<span class="ltx_bibblock">
Mike Savage, Fiona Devine, Niall Cunningham, Mark Taylor, Yaojun Li, Johs
Hjellbrekke, Brigitte Le Roux, Sam Friedman, and Andrew Miles. 2013.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1177/00380385134811" title="" class="ltx_ref ltx_href">A new
model of social class? findings from the bbc’s great british class survey
experiment</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Sociology</em>, 47(2):219–250.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saydah et al. (2013)</span>
<span class="ltx_bibblock">
Sharon H. Saydah, Giuseppina Imperatore, and Gloria L. Beckles. 2013.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.2337/dc11-1864" title="" class="ltx_ref ltx_href">Socioeconomic status and
mortality</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Diabetes Care</em>, 36(1):49–55.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Senter and Smith (1967)</span>
<span class="ltx_bibblock">
RJ Senter and Edgar A Smith. 1967.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://api.semanticscholar.org/CorpusID:38558516" title="" class="ltx_ref ltx_href">Automated
readability index.</a>

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">AMRL-TR. Aerospace Medical Research Laboratories</em>, pages 1–14.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stamou (2014)</span>
<span class="ltx_bibblock">
Anastasia G Stamou. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1177/0963947013519551" title="" class="ltx_ref ltx_href">A
literature review on the mediation of sociolinguistic style in television and
cinematic fiction: Sustaining the ideology of authenticity</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Language and Literature</em>, 23(2):118–140.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
et al. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.48550/arXiv.2307.09288" title="" class="ltx_ref ltx_href">Llama 2: Open foundation and fine-tuned chat models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tunstall et al. (2023)</span>
<span class="ltx_bibblock">
Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul,
Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier,
Nathan Habib, et al. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.48550/arXiv.2310.16944" title="" class="ltx_ref ltx_href">Zephyr: Direct distillation of lm alignment</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.16944</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Usategui Basozábal et al. (1992)</span>
<span class="ltx_bibblock">
Elisa Usategui Basozábal et al. 1992.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.4438/1988-592X-0034-8082-RE" title="" class="ltx_ref ltx_href">La
sociolingüística de basil bernstein y sus implicaciones en el
ámbito escolar</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Revista de educación</em>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weirich and Simpson (2018)</span>
<span class="ltx_bibblock">
Melanie Weirich and Adrian P Simpson. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1371/journal.pone.0209226" title="" class="ltx_ref ltx_href">Gender identity is indexed and perceived in speech</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">PLoS One</em>, 13(12):e0209226.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2020)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe
Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
and Alexander Rush. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-demos.6" title="" class="ltx_ref ltx_href">Transformers:
State-of-the-art natural language processing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</em>, pages 38–45, Online.
Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Annotations per show</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Table <a href="#A1.T8" title="Table 8 ‣ Appendix A Annotations per show ‣ Classist Tools: Social Class Correlates with Performance in NLP" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the annotated demographics for each season/film.</p>
</div>
<figure id="A1.T8" class="ltx_table">
<table id="A1.T8.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T8.1.1.1" class="ltx_tr">
<td id="A1.T8.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="A1.T8.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Class</th>
<th id="A1.T8.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Gender</th>
<th id="A1.T8.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Race</th>
<th id="A1.T8.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Geography</th>
</tr>
<tr id="A1.T8.1.2.2" class="ltx_tr">
<td id="A1.T8.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">Arrested Development</td>
<td id="A1.T8.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">Upper</td>
<td id="A1.T8.1.2.2.3" class="ltx_td ltx_align_left ltx_border_t">M, F</td>
<td id="A1.T8.1.2.2.4" class="ltx_td ltx_align_left ltx_border_t">White</td>
<td id="A1.T8.1.2.2.5" class="ltx_td ltx_align_left ltx_border_t">USA</td>
</tr>
<tr id="A1.T8.1.3.3" class="ltx_tr">
<td id="A1.T8.1.3.3.1" class="ltx_td ltx_align_left">Sex and the City</td>
<td id="A1.T8.1.3.3.2" class="ltx_td ltx_align_left">Middle, Upper</td>
<td id="A1.T8.1.3.3.3" class="ltx_td ltx_align_left">F</td>
<td id="A1.T8.1.3.3.4" class="ltx_td ltx_align_left">White</td>
<td id="A1.T8.1.3.3.5" class="ltx_td ltx_align_left">USA</td>
</tr>
<tr id="A1.T8.1.4.4" class="ltx_tr">
<td id="A1.T8.1.4.4.1" class="ltx_td ltx_align_left">The Fresh Prince of BelAir</td>
<td id="A1.T8.1.4.4.2" class="ltx_td ltx_align_left">Middle, Upper</td>
<td id="A1.T8.1.4.4.3" class="ltx_td ltx_align_left">M, F</td>
<td id="A1.T8.1.4.4.4" class="ltx_td ltx_align_left">Black</td>
<td id="A1.T8.1.4.4.5" class="ltx_td ltx_align_left">USA</td>
</tr>
<tr id="A1.T8.1.5.5" class="ltx_tr">
<td id="A1.T8.1.5.5.1" class="ltx_td ltx_align_left">Big Little Lies</td>
<td id="A1.T8.1.5.5.2" class="ltx_td ltx_align_left">Middle</td>
<td id="A1.T8.1.5.5.3" class="ltx_td ltx_align_left">F</td>
<td id="A1.T8.1.5.5.4" class="ltx_td ltx_align_left">White</td>
<td id="A1.T8.1.5.5.5" class="ltx_td ltx_align_left">USA</td>
</tr>
<tr id="A1.T8.1.6.6" class="ltx_tr">
<td id="A1.T8.1.6.6.1" class="ltx_td ltx_align_left">Breaking Bad</td>
<td id="A1.T8.1.6.6.2" class="ltx_td ltx_align_left">Low, middle</td>
<td id="A1.T8.1.6.6.3" class="ltx_td ltx_align_left">M</td>
<td id="A1.T8.1.6.6.4" class="ltx_td ltx_align_left">White</td>
<td id="A1.T8.1.6.6.5" class="ltx_td ltx_align_left">USA</td>
</tr>
<tr id="A1.T8.1.7.7" class="ltx_tr">
<td id="A1.T8.1.7.7.1" class="ltx_td ltx_align_left">The Wire (S01)</td>
<td id="A1.T8.1.7.7.2" class="ltx_td ltx_align_left">Low, middle</td>
<td id="A1.T8.1.7.7.3" class="ltx_td ltx_align_left">M</td>
<td id="A1.T8.1.7.7.4" class="ltx_td ltx_align_left">Black, white</td>
<td id="A1.T8.1.7.7.5" class="ltx_td ltx_align_left">USA</td>
</tr>
<tr id="A1.T8.1.8.8" class="ltx_tr">
<td id="A1.T8.1.8.8.1" class="ltx_td ltx_align_left">The Wire (S03)</td>
<td id="A1.T8.1.8.8.2" class="ltx_td ltx_align_left">Low, middle</td>
<td id="A1.T8.1.8.8.3" class="ltx_td ltx_align_left">M</td>
<td id="A1.T8.1.8.8.4" class="ltx_td ltx_align_left">Black, white</td>
<td id="A1.T8.1.8.8.5" class="ltx_td ltx_align_left">USA</td>
</tr>
<tr id="A1.T8.1.9.9" class="ltx_tr">
<td id="A1.T8.1.9.9.1" class="ltx_td ltx_align_left">Fargo (S04)</td>
<td id="A1.T8.1.9.9.2" class="ltx_td ltx_align_left">Low, middle</td>
<td id="A1.T8.1.9.9.3" class="ltx_td ltx_align_left">M, F</td>
<td id="A1.T8.1.9.9.4" class="ltx_td ltx_align_left">Black, white</td>
<td id="A1.T8.1.9.9.5" class="ltx_td ltx_align_left">USA</td>
</tr>
<tr id="A1.T8.1.10.10" class="ltx_tr">
<td id="A1.T8.1.10.10.1" class="ltx_td ltx_align_left">Trailer Park Boys</td>
<td id="A1.T8.1.10.10.2" class="ltx_td ltx_align_left">Low</td>
<td id="A1.T8.1.10.10.3" class="ltx_td ltx_align_left">M</td>
<td id="A1.T8.1.10.10.4" class="ltx_td ltx_align_left">White</td>
<td id="A1.T8.1.10.10.5" class="ltx_td ltx_align_left">USA</td>
</tr>
<tr id="A1.T8.1.11.11" class="ltx_tr">
<td id="A1.T8.1.11.11.1" class="ltx_td ltx_align_left">When They See Us</td>
<td id="A1.T8.1.11.11.2" class="ltx_td ltx_align_left">Low</td>
<td id="A1.T8.1.11.11.3" class="ltx_td ltx_align_left">M, F</td>
<td id="A1.T8.1.11.11.4" class="ltx_td ltx_align_left">Black</td>
<td id="A1.T8.1.11.11.5" class="ltx_td ltx_align_left">USA</td>
</tr>
<tr id="A1.T8.1.12.12" class="ltx_tr">
<td id="A1.T8.1.12.12.1" class="ltx_td ltx_align_left">The Sopranos</td>
<td id="A1.T8.1.12.12.2" class="ltx_td ltx_align_left">Low</td>
<td id="A1.T8.1.12.12.3" class="ltx_td ltx_align_left">M</td>
<td id="A1.T8.1.12.12.4" class="ltx_td ltx_align_left">White</td>
<td id="A1.T8.1.12.12.5" class="ltx_td ltx_align_left">USA</td>
</tr>
<tr id="A1.T8.1.13.13" class="ltx_tr">
<td id="A1.T8.1.13.13.1" class="ltx_td ltx_align_left">Pose</td>
<td id="A1.T8.1.13.13.2" class="ltx_td ltx_align_left">Low</td>
<td id="A1.T8.1.13.13.3" class="ltx_td ltx_align_left">F, Trans</td>
<td id="A1.T8.1.13.13.4" class="ltx_td ltx_align_left">Black,Latino</td>
<td id="A1.T8.1.13.13.5" class="ltx_td ltx_align_left">USA</td>
</tr>
<tr id="A1.T8.1.14.14" class="ltx_tr">
<td id="A1.T8.1.14.14.1" class="ltx_td ltx_align_left ltx_border_t">The Crown</td>
<td id="A1.T8.1.14.14.2" class="ltx_td ltx_align_left ltx_border_t">Upper</td>
<td id="A1.T8.1.14.14.3" class="ltx_td ltx_align_left ltx_border_t">M, F</td>
<td id="A1.T8.1.14.14.4" class="ltx_td ltx_align_left ltx_border_t">White</td>
<td id="A1.T8.1.14.14.5" class="ltx_td ltx_align_left ltx_border_t">UK</td>
</tr>
<tr id="A1.T8.1.15.15" class="ltx_tr">
<td id="A1.T8.1.15.15.1" class="ltx_td ltx_align_left">Downton Abbey</td>
<td id="A1.T8.1.15.15.2" class="ltx_td ltx_align_left">Upper, Low</td>
<td id="A1.T8.1.15.15.3" class="ltx_td ltx_align_left">M, F</td>
<td id="A1.T8.1.15.15.4" class="ltx_td ltx_align_left">White</td>
<td id="A1.T8.1.15.15.5" class="ltx_td ltx_align_left">UK</td>
</tr>
<tr id="A1.T8.1.16.16" class="ltx_tr">
<td id="A1.T8.1.16.16.1" class="ltx_td ltx_align_left">The IT Crowd</td>
<td id="A1.T8.1.16.16.2" class="ltx_td ltx_align_left">Middle</td>
<td id="A1.T8.1.16.16.3" class="ltx_td ltx_align_left">M</td>
<td id="A1.T8.1.16.16.4" class="ltx_td ltx_align_left">White</td>
<td id="A1.T8.1.16.16.5" class="ltx_td ltx_align_left">UK</td>
</tr>
<tr id="A1.T8.1.17.17" class="ltx_tr">
<td id="A1.T8.1.17.17.1" class="ltx_td ltx_align_left">Shetland</td>
<td id="A1.T8.1.17.17.2" class="ltx_td ltx_align_left">Middle</td>
<td id="A1.T8.1.17.17.3" class="ltx_td ltx_align_left">M, F</td>
<td id="A1.T8.1.17.17.4" class="ltx_td ltx_align_left">White</td>
<td id="A1.T8.1.17.17.5" class="ltx_td ltx_align_left">Scotland (UK)</td>
</tr>
<tr id="A1.T8.1.18.18" class="ltx_tr">
<td id="A1.T8.1.18.18.1" class="ltx_td ltx_align_left">T2.trainspotting</td>
<td id="A1.T8.1.18.18.2" class="ltx_td ltx_align_left">Low</td>
<td id="A1.T8.1.18.18.3" class="ltx_td ltx_align_left">M</td>
<td id="A1.T8.1.18.18.4" class="ltx_td ltx_align_left">White</td>
<td id="A1.T8.1.18.18.5" class="ltx_td ltx_align_left">Scotland (UK)</td>
</tr>
<tr id="A1.T8.1.19.19" class="ltx_tr">
<td id="A1.T8.1.19.19.1" class="ltx_td ltx_align_left">Trainspotting</td>
<td id="A1.T8.1.19.19.2" class="ltx_td ltx_align_left">Low</td>
<td id="A1.T8.1.19.19.3" class="ltx_td ltx_align_left">M</td>
<td id="A1.T8.1.19.19.4" class="ltx_td ltx_align_left">White</td>
<td id="A1.T8.1.19.19.5" class="ltx_td ltx_align_left">Scotland (UK)</td>
</tr>
<tr id="A1.T8.1.20.20" class="ltx_tr">
<td id="A1.T8.1.20.20.1" class="ltx_td ltx_align_left">Smother</td>
<td id="A1.T8.1.20.20.2" class="ltx_td ltx_align_left">Middle</td>
<td id="A1.T8.1.20.20.3" class="ltx_td ltx_align_left">M, F</td>
<td id="A1.T8.1.20.20.4" class="ltx_td ltx_align_left">White</td>
<td id="A1.T8.1.20.20.5" class="ltx_td ltx_align_left">Ireland (UK)</td>
</tr>
<tr id="A1.T8.1.21.21" class="ltx_tr">
<td id="A1.T8.1.21.21.1" class="ltx_td ltx_align_left ltx_border_bb">Derry Girls</td>
<td id="A1.T8.1.21.21.2" class="ltx_td ltx_align_left ltx_border_bb">Low</td>
<td id="A1.T8.1.21.21.3" class="ltx_td ltx_align_left ltx_border_bb">F</td>
<td id="A1.T8.1.21.21.4" class="ltx_td ltx_align_left ltx_border_bb">White</td>
<td id="A1.T8.1.21.21.5" class="ltx_td ltx_align_left ltx_border_bb">Ireland (NE)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Annotated demographics for each movie or TV show. M denotes Men and F, women. </figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.04444" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.04445" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.04445">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.04445" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.04446" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 13:28:16 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
