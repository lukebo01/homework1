<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.13924] Narrative Review of Support for Emotional Expressions in Virtual Reality: Psychophysiology of speech-to-text interfaces</title><meta property="og:description" content="Keywords: Emotional expression, VR, Psychophysiology, Speech-to-text, Empathic machine.">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Narrative Review of Support for Emotional Expressions in Virtual Reality: Psychophysiology of speech-to-text interfaces">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Narrative Review of Support for Emotional Expressions in Virtual Reality: Psychophysiology of speech-to-text interfaces">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.13924">

<!--Generated on Wed Jun  5 16:17:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Narrative Review of Support for Emotional Expressions in Virtual Reality: Psychophysiology of speech-to-text interfaces</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sunday D. Ubur, Denis Gracanin
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text ltx_font_bold">Keywords:</span> Emotional expression, VR, Psychophysiology, Speech-to-text, Empathic machine.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">This narrative review on emotional expression in Speech-to-Text (STT) interfaces with Virtual Reality (VR) aims to identify advancements, limitations, and research gaps in incorporating emotional expression into transcribed text generated by STT systems. Using a rigorous search strategy, relevant articles published between 2020 and 2024 are extracted and categorized into themes such as communication enhancement technologies, innovations in captioning, emotion recognition in AR and VR, and empathic machines. The findings reveal the evolution of tools and techniques to meet the needs of individuals with hearing impairments, showcasing innovations in live transcription, closed captioning, AR, VR, and emotion recognition technologies. Despite improvements in accessibility, the absence of emotional nuance in transcribed text remains a significant communication challenge. The study underscores the urgency for innovations in STT technology to capture emotional expressions. The research discusses integrating emotional expression into text through strategies like animated text captions, emojilization tools, and models associating emotions with animation properties. Extending these efforts into AR and VR environments opens new possibilities for immersive and emotionally resonant experiences, especially in educational contexts. The study also explores empathic applications in healthcare, education, and human-robot interactions, highlighting the potential for personalized and effective interactions. The multidisciplinary nature of the literature underscores the potential for collaborative and interdisciplinary research.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the ever-evolving landscape of communication technologies, Speech-to-Text (STT) interfaces play a pivotal role in enhancing accessibility and inclusivity, particularly for individuals with hearing impairments. The ability to convert spoken language into text facilitates communication in various contexts, ranging from online meetings to educational settings. However, amidst the strides made in improving accessibility, a critical aspect often overlooked is the incorporation of emotional expression into transcribed text generated by STT systems.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">This study embarks on a comprehensive exploration of the existing literature surrounding emotional expression in STT interfaces, with a specific emphasis on the psychophysiology aspect within Virtual Reality (VR). The objective is to identify advancements, limitations, and research gaps in the incorporation of emotional expression in transcribed text generated by STT systems. As communication technologies continue to advance, the ability to convey not only the semantic content but also the emotional nuances of speech becomes paramount for fostering richer and more meaningful interactions.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The ubiquity of STT applications, exemplified by tools like Live Transcribe, has significantly contributed to breaking communication barriers for the Deaf and Hard of Hearing (DHH) community. These applications offer real-time transcriptions that are invaluable in various scenarios, from professional communication to educational contexts. However, the inherent challenge lies in the loss of emotional nuance during the transcription process, posing a communication hurdle that this study seeks to address.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To unravel the complexities surrounding emotional expression in STT interfaces, our investigation spans various dimensions. From examining innovations in live transcription and closed captioning to delving into advancements in augmented reality (AR), emotive captioning, emotion recognition, and empathic machines, this study aims to provide a holistic understanding of the evolving tools and techniques. Additionally, the exploration extends into the realms of VR and AR, where unique opportunities arise to create immersive and emotionally resonant experiences, especially in the context of education and training.
In the subsequent sections, we detail the methodology employed in conducting the literature review, present the background encompassing communication enhancement technologies, and delve into innovations in captioning, emotion recognition, and empathic systems. The synthesis of these findings not only identifies current advancements but also points towards avenues for future research, emphasizing the ongoing need for inclusive and accessible communication technologies.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Search Strategy:</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">The search involved a comprehensive search of scholarly databases, including ACM Digital Library, IEEE Explore, Scopus, and Google Scholar, to identify relevant articles published between 2020 and 2024. The search terms used included (”Emotional Expressions” AND ”Virtual Reality” AND ”Psychophysiology” AND ”speech-to-text Interfaces” OR ”Captions”), and the inclusion criteria encompassed studies related to emotional expressions in psychophysiology of STT interfaces. We searched individual databases, however, much of the data came from the Publish or Perish software <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> as it extracts results across these databases mentioned.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Inclusion and Exclusion Criteria:</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">The rest of the methods process was conducted using Rayyan software <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
Most of the papers included are peer-reviewed articles, books, and conference proceedings that provided insights into the background and direction of implementing emotional expressions in STT. Studies with a focus on generating emotional expressions in text were prioritized. Non-English language publications and articles without full-text availability were excluded.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Study Selection:</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">The initial search yielded 1046 articles. After removing duplicates and conducting title and abstract screening, 31 articles were considered for full-text review. The final selection was based on the relevance of the content to the research question and the quality of the study design.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data Extraction:</h4>

<div id="S2.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px4.p1.1" class="ltx_p">Data extraction was performed to gather key information from the selected literature, including future work recommendations. This process involved summarizing findings, identifying common themes, and noting variations across studies.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Quality Assessment:</h4>

<div id="S2.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px5.p1.1" class="ltx_p">Given the narrative nature of this review, a formal quality assessment was not conducted. However, efforts were made to critically appraised each study’s methodology, sample size, and key findings to gauge the overall reliability of the evidence.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px6" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Synthesis:</h4>

<div id="S2.SS0.SSS0.Px6.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px6.p1.1" class="ltx_p">The synthesis involved organizing the selected literature into thematic categories and identifying overarching trends and patterns, and employed a narrative approach to present the key findings and critically discuss the implications of the literature on the research question.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Background</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section provides the narrative review of the literature, with study limitations and comparisions among related studies.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Communication Enhancement Technologies for DHH Individuals</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The exploration of communication enhancement technologies for DHH individuals has seen significant strides, particularly with the non-traditional applications of Live Transcribe, an automatic speech recognition (ASR) application <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. This tool proves invaluable in various scenarios, from technical support and communication with colleagues to note-taking during meetings, especially during the prevalence of online meetings amid COVID-19 lockdowns. The adaptability of Live Transcribe for immobile DHH individuals by mounting the tablet on a boom mic stand is a notable advancement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, emphasizing its pivotal role in promoting inclusivity and accessibility for this demographic.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Closed captioning is a technology that has seen minimal innovation since its inception in the 1970s, however recent studies are shedding light on potential enhancements. One such is the animated text in captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> which offers a promising avenue, providing improved access to emotive information often overlooked in traditional captions, such as music, sound effects, and intonation. This innovation, preferred by both hard of hearing and hearing participants, emphasizes the need to bridge the gap in conveying nonverbal nuances. This study and other related works was inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> which delved into the impact of captions on the affective reactions of hearing-impaired children to television programming, revealing the potential of captions in enhancing emotional perception.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Innovations in Captioning for Enhanced Emotional Communication</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Shifting focus to innovations in captioning for enhanced emotional communication, Rashid’s framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> introduces the concept of using animation and standard properties to express basic emotions. This framework, associating emotions with animation properties, establishes a consistent method for applying animation to text captions. Similarly, to enhance emotions in captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> incorporating graphics, color, and animation to illustrate sound and emotive information in television and film, a user study compared viewer reactions to video samples with emotive captions against conventional captioning, showcasing positive responses, particularly among hard of hearing viewers.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">In the realm of Voice User Interfaces (VUI), Hu’s paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> highlights the lack of emotional information in STT by proposing an emojilization tool. This tool automatically attaches emojis to generated text, compensating for emotional loss in the conversion process. The pilot study indicates that emojilized text enhances perceived emotions compared to plain text.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">We can also understand how to represent emotions in captions by analyzing how emotions are expressed in text. A study along this direction is the representation of emotions in text-based messaging applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> through visualizations in facial expression recognition in WhatsApp Web, and findings underscore users’ preference for maintaining control over their emotions in private settings, emphasizing the need for exclusive presentation and sharing of emotions in certain contexts. An important application of emotional expressions in text and imagery on social media during the COVID-19 pandemic <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> adds a temporal dimension to the narrative, shedding light on how emotions are expressed in different modalities during challenging times.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Emotion Recognition in AR and VR</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">VR has gained attention in various fields due to its unique advantages in manipulating perceived scenarios and providing controlled experiences <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
It has been found that VR can generate emotional reactions and experiences in users, as demonstrated by studies on 360 degree videos and VR educational tools <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
In fact, emotional responses in VR games have been observed to be more intense compared to desktop games, both psychologically and physiologically <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
This suggests that VR has the potential to enhance positive emotions, and it is worth to study if similar outcome can be experienced from STT in an AR or VR environment.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">More studies have discussed advancements in AR technologies for DHH individuals, particularly in the classroom setting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.
The AR visual-captions system, implemented using Unity, ARKit, and AR Foundation, aims to provide real-time STT and visual elements around the teacher, creating an immersive learning experience.
The study not only introduced the prototype but also outlined future research directions, especially on refining the design, conducting user studies, and extending the system’s capabilities for group conversations.
Similarly, Real-time augmented reality visual-captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> for DHH children in classrooms builds on the AR-based system, providing a real-time solution for STT and keyword extraction, addressing the challenges faced by DHH children in mainstream education.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">In higher education application, Pirker’s exploration into the potential of VR for computer science education <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> sheds light on the positive impact of VR environments on learning outcomes.
The user study reveals higher engagement, immersion, and positive emotions in the VR application compared to a web-based alternative, emphasizing VR’s potential to enhance computer science education.
Given the importance of AR and VR in education training, making the virtual environment convey emotional expression is essential.
Chen’s paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> focuses on facial expression recognition within immersive environments.
The proposed solution involves collecting real facial expression data using an infrared camera and light source, showcasing promising results for understanding human emotions in VR contexts.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Speech Emotion Recognition: Techniques and Challenges</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">An important technique found in the literature is the use of color coding and visualization of emotion in speech, which offer a unique perspective on conveying emotional information. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> utilizes haptic feedback vests and immersive virtual reality to quantify human emotion, providing insights for the development of emotionally relevant input devices. The use of bubble coloring in chat platforms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> explores the visual representation of speech emotion, considering color alterations and participant feedback for conveying specific emotions effectively.
Other techniques in machine learning are being employed to include emotional recognition in text. One is Human-robot interaction and perception of emotions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, demonstrating the effectiveness of animated text associated with emotional information in enhancing user interactions. The system’s potential applications in healthcare and education underscore its versatile impact.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Further, Schiano’s paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> delves into the perception of facial affect, bridging the gap between human facial expressions and prototype robot faces. The study informs the design of affective robot faces, contributing valuable insights into how emotions can be effectively communicated through facial expressions.In a related study,the analysis of humanoid avatar representations and emotions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> delves into the uncanniness factor, highlighting the importance of selecting appropriate avatar types for accurate expression communication. The study’s findings support the need for avatar representations that effectively convey emotional cues in interactive systems. Finally, gestures can also be generated directly from speech using GANs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> thereby opening up new possibilities in human-computer interaction. The user study using Turing test-inspired evaluation demonstrates the potential of the proposed technique in creating realistic gestures from speech.
In conclusion, the narrative literature review traverses a spectrum of communication enhancement technologies, showcasing the evolution of tools and techniques to address the unique needs of deaf and hard of hearing individuals. From innovations in live transcription and closed captioning to advancements in augmented reality, emotive captioning, and speech emotion recognition, the literature reflects a dynamic landscape of research and development. The exploration of humanoid avatars, voice user interfaces, virtual reality, human-robot interaction, and the color coding of speech emotions further emphasizes the interdisciplinary nature of this field. The review also underscores the significance of understanding emotional expressions in text, images, and gestures, offering a holistic perspective on communication technologies that cater to diverse sensory and cognitive needs. As we navigate the intricacies of enhancing communication for individuals with hearing impairments, these technological advancements pave the way for a more inclusive and accessible future.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Empathic Machine</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Emotions serve as an implicit communication channel among humans, conveyed through spoken words, facial expressions, behavior, and physiological responses. This empathic form of communication enables individuals to recognize cues and respond empathetically. Despite computing devices excelling in understanding user context, there are ongoing challenges in the scientific community to create emotion-sensing and empathic applications for human-computer interaction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. In this section we look at the literature contributions in tackling these challenges and support empathic ability in machines.</p>
</div>
<section id="S3.SS5.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Enhancing Empathic Abilities with Chatbots:</h4>

<div id="S3.SS5.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS5.SSS0.Px1.p1.1" class="ltx_p">The evolution of chatbots began with the creation of ELIZA in 1966 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, and today they are commonly used in customer service but have limitations in showing empathy. Empathy, defined as understanding and sharing another’s feelings, is deemed challenging for conversational agents. Recent research, however, indicates the feasibility of generating empathic responses in chatbots, particularly in customer service contexts, including in the healthcare, especially in providing physical health diagnosis through short text conversations.</p>
</div>
<div id="S3.SS5.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS5.SSS0.Px1.p2.1" class="ltx_p">Several studies available in the literature to compensate for the shortcomings of lack of empathic expressions in computer and AI employed the use of chatbots to convey emptional expressions. One study used chatbots to mediate social presence and trust in consumer emotions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, creating a sense of social presence using emoticons, appropriate language styles, and other cues that contribute to a more authentic and human-like interaction. Another study used empathic chatbot to understand users’ emotional states and generate responses that convey understanding and addressing challenges in handling multi-layered, context-sensitive, and implicitly expressed emotions in text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. It utilized advanced models and tools involving a benchmark bot and an empathic bot. To enhance empathic capabilities, they fine-tuned the language model on empathic conversations, incorporating the user’s emotional state into the input. The findings from the empathic bot include the effectiveness of transformer-based language models, the positive impact of training on empathic conversations, and the influence of emotional valence on perceived empathy.</p>
</div>
<div id="S3.SS5.SSS0.Px1.p3" class="ltx_para">
<p id="S3.SS5.SSS0.Px1.p3.1" class="ltx_p">Similarly, chatbot application is finding its way into gender voice discrimination. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> explores the tension between designing empathic agents and the gender assignment of chatbots and how they can relate to the design of the metaphor of the chatbots in conversational agents (Cas). Computers lack genuine emotions, however CAs can simulate and trigger empathy by adhering to human-social rules during interactions. This simulation involves techniques like sentiment analysis, emotion detection, and mimicking emotions to enhance user engagement. Notably, the paper explores whether CAs, specifically those perceived as having feminine qualities, evoke more empathy in human-chatbot interactions compared to other gender perceptions.</p>
</div>
</section>
<section id="S3.SS5.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Digitizing Human Emotions:</h4>

<div id="S3.SS5.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS5.SSS0.Px2.p1.1" class="ltx_p">As computing machines and AI need to be programmed to work, inculcating empathy into machines is through digitizing human emotions. One study used non-invasive techniques like electroencephalography (EEG), specifically the Emotive Epoc headset to digitize human emotions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and the primary goal was to conduct a proof-of-concept experiment enabling a humanoid robot’s control through digitized emotions, emphasizing potential applications in healthcare . Their contribution lies in adapting mature image recognition tools from Artificial Neural Networks (ANNs) for emotion recognition, and the resulting Brain-Computer Interface (BCI) system is designed to enable the robot to emulate empathy and interact with subjects based on predefined behavioral models. Similarly, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> explores the significance of recognizing emotions in face-to-face conversations and discussed the limitations of traditional cues like facial expressions and gestures, emphasizing the potential of analyzing brain activity and physiological signals for more reliable emotion recognition, especially in situations where emotions may be concealed. The study presents an experimental setup for inducing spontaneous emotions in conversations and creating a dataset incorporating EEG, Photoplethysmography (PPG), and Galvanic Skin Response (GSR) signals. The researchers developed an intelligent user interface for video conferencing and systems with conversational digital humans capable of recognizing and responding to emotions, using PEGCONV) dataset to train these systems to enhance their ability to understand and appropriately respond to human behavior.</p>
</div>
<div id="S3.SS5.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS5.SSS0.Px2.p2.1" class="ltx_p">Further, Robots can also respond to user emotional states. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> enhanced human-robot interactions (HRI) by proposing an Automatic Cognitive Empathy Model (ACEM) for humanoid robots. The goal was to achieve more extended and engaging interactions by having robots respond appropriately to users’ emotional states using ACEM model to continuously detects users’ affective states based on facial expressions, utilizing a stacked autoencoder network trained on the RAVDESS dataset. The model generates empathic behaviors adapted to users’ personalities, either in parallel or as reactive responses.</p>
</div>
</section>
<section id="S3.SS5.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Empathic Systems in AR/VR:</h4>

<div id="S3.SS5.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS5.SSS0.Px3.p1.1" class="ltx_p">Empathic machine is also finding applications into wearable. In exploring the use of smart glasses equipped with an emotion recognition system to enhance human-to-human communication, with a focus on doctors and autistic adults <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, this study identifies the potential benefits for doctors in improving patient-doctor communication and for autistic adults facing challenges in adhering to neurotypical communication norms. The proposed system combines emotion recognition, AI, and smart glasses to provide real-time feedback on the emotional state of conversation partners. User evaluations indicate positive responses from both doctors and autistic adults, highlighting the potential for improving empathy and communication. Design considerations include customizable output preferences and addressing privacy and social impact concerns. The authors emphasize the system’s potential for educational purposes and ongoing development in the context of social impact and user experience. In a related study, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> explores the use of virtual reality (VR) with wearable physiological sensors to examine how recalling autobiographical memories (AM) with emotional content affects an individual’s physiological state. By replicating the Autobiographical Memory Test (AMT) in VR, participants were presented with positive, negative, and neutral words to trigger memory recall. The study observed a positive influence of AM recall on electrodermal activity (EDA) peak amplitude, EDA peak number, and pupil diameter compared to situations without recall. However, emotional AM recall did not produce a significant impact. The article concludes by discussing the potential and limitations of utilizing autobiographical memory to enhance personalized mobile VR experiences in conjunction with physiological sensors. The findings reveal a significant effect of AM recall on EDA mean peak amplitude, EDA peak number, and pupil diameter compared to a no-recall condition. However, no significant differences were identified in emotional AM recall (positive, negative, neutral).</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Discussion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The purpose of this review is to explore existing literature on emotional expression in STT interfaces, with a specific focus on psychophysiology in VR.
As mentioned earlier in the introduction, this review aims to identify advancements, limitations, and research gaps in the incorporation of emotional expression in transcribed text generated by STT systems.
The overarching goal is to inspire further research in the field.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The review was conducted to address a critical aspect overlooked in traditional language transcription services—emotional expression.
While STT systems enhance accessibility for the DHH, the loss of emotional nuance in transcribed text poses a communication challenge.
The review seeks to analyze and synthesize available literature to understand the current state of research, advancements, and gaps in reintegrating emotional expression into transcribed text.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Through an extensive literature review spanning databases such as ACM Digital Library, IEEE Explore, Scopus, and Google Scholar, the study identified 31 relevant papers.
The review explored communication enhancement technologies, innovations in captioning, emotion recognition in AR and VR, and empathic machines.
The review successfully organized the literature into thematic categories, identified trends, and presented key findings.
It critically discussed the implications of the literature on the research objective.
The limited relevant studies found highlights the need for more research in this area.
Also, of the relevant papers found, there is no study that addressed incorporating emotional expression in STT in an AR or VR environment, thus further supporting more research in this area.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">The main findings of the narrative review highlight the evolution of tools and techniques aimed at addressing the unique needs of individuals with hearing impairments and other users of language transcription text.
This includes innovations in live transcription, closed captioning, AR, VR, and emotion recognition technologies.
The review underscores the interdisciplinary nature of the field, emphasizing the significance of understanding emotional expressions in diverse modalities such as text, images, and gestures.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">However, the notable absence of emotional expression in these applications, as highlighted by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, adds a layer of complexity to communication.
Despite their commendable impact on improving accessibility, the lack of emotional nuance in transcribed text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> limits the richness of communication.
The results emphasize the urgency for innovations in STT technology to capture emotional expressions from various sources, including music, sound effects, and speech emphasis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">The exploration of strategies to integrate emotional expression into text, such as Rashid’s framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, signifies a shift beyond traditional captioning methods. Animated text captions, enriched with graphics and color, not only show positive responses among hard of hearing viewers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> but also present a potential avenue for more inclusive and emotionally expressive communication.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p">The extension of these efforts into Augmented Reality (AR) and Virtual Reality (VR) environments, where technologies offer unique advantages <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, opens new possibilities for immersive and emotionally resonant experiences. While studies dedicated to enhancing STT captions in AR and VR environments are promising <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, the results underscores the need for continued research to infuse emotional expression into these advanced platforms, particularly in educational training contexts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.1" class="ltx_p">The implementation of the emojization tool <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, despite its limitations in covering a sufficient vocabulary, demonstrates a tangible effort to restore emotion to text in the STT transcription process. This example serves as a valuable step toward addressing the emotional gap in communication technologies, particularly within the AR/VR environment.</p>
</div>
<div id="S4.p9" class="ltx_para">
<p id="S4.p9.1" class="ltx_p">Furthermore, the application of emotional recognition through machine learning and robotics in healthcare and education <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> highlights the broader impact of these results. The effective communication of emotions in interactive systems has implications for personalized and empathetic interactions, potentially improving patient care and educational experiences.</p>
</div>
<div id="S4.p10" class="ltx_para">
<p id="S4.p10.1" class="ltx_p">The digitization of human emotions, as demonstrated in studies such as Roshdy et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, has direct implications for healthcare applications. The development of Brain-Computer Interface (BCI) systems capable of interpreting and responding to human emotions opens new possibilities for enhancing patient care and interaction with humanoid robots, suggesting a potential revolution in healthcare practices.</p>
</div>
<div id="S4.p11" class="ltx_para">
<p id="S4.p11.1" class="ltx_p">The exploration of models like the Automatic Cognitive Empathy Model (ACEM) for humanoid robots <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> contributes to more extended and engaging human-robot interactions. This has implications for various fields, including customer service, education, and entertainment, where robots can adapt their behavior based on users’ emotional states, leading to more personalized and effective interactions.</p>
</div>
<div id="S4.p12" class="ltx_para">
<p id="S4.p12.1" class="ltx_p">Wearable technologies, such as smart glasses with emotion recognition systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, have the potential to significantly improve communication in specific contexts, such as doctor-patient interactions and communication challenges faced by autistic adults. The real-time feedback on emotional states can lead to more empathetic and tailored communication, addressing specific needs in these domains.</p>
</div>
<div id="S4.p13" class="ltx_para">
<p id="S4.p13.1" class="ltx_p">The discussed systems, particularly those involving wearable technologies and intelligent interfaces, have implications for educational settings. The potential for improving empathy and communication, as well as considerations for customizable output preferences, privacy, and social impact, suggests that these technologies could play a role in educational environments, fostering better communication and understanding.</p>
</div>
<div id="S4.p14" class="ltx_para">
<p id="S4.p14.1" class="ltx_p">The exploration of virtual reality (VR) experiences with wearable physiological sensors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> provides insights into the potential and limitations of leveraging autobiographical memory to enhance personalized mobile VR experiences. This understanding is valuable for developers and researchers aiming to create immersive and emotionally resonant virtual experiences.</p>
</div>
<div id="S4.p15" class="ltx_para">
<p id="S4.p15.1" class="ltx_p">The multidisciplinary nature of these studies, incorporating fields such as neuroscience, artificial intelligence, and psychology, highlights the potential for collaborative and interdisciplinary research. This can lead to the development of more holistic and effective empathic systems that consider both technological and human factors.</p>
</div>
<div id="S4.p16" class="ltx_para">
<p id="S4.p16.1" class="ltx_p">The significance of this research lies in its contribution to advancing communication technologies for individuals with hearing impairments. By exploring emotional expression in STT interfaces, the study sheds light on the evolving landscape of tools and techniques. The research emphasizes the importance of inclusivity and accessibility in communication technologies, fostering a more empathetic and inclusive future.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Limitations</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The limitations of this study were: (i) it was a rapid review whose purpose is to survey existing work in emotional expressions, find existing gaps and inspire further research. (ii) the narrative review did not follow a strict rule expected of systematic literature review, hence could have missed some relevant papers. However, the papers that were found eligible and used in this review are the most relevant. (iii) the review is limited to peer-reviewed papers that were in English.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Gaps, Challenges and Opportunities</h3>

<div id="S4.SS2.p1" class="ltx_para">
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Innovations in emotive captioning and speech emotion recognition warrant further exploration, particularly in educational setting where DHH students, including other categories of students rely on STT as their main method of communication during lectures.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Explore the integration of cognitive empathy and improve emotion detection in short conversations. The study’s promising results underscore the potential for empathy-driven enhancements in medical chatbots <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Prioritize the development of an automatic metric for assessing empathy in text-based messages, eliminating reliance on human judges and enhancing result comparability. Refining the definition of empathy to encompass additional aspects like nationality and educational level is another crucial step. 2. Further research should incorporate finer emotion categories and measure emotion intensity. Improving the dialogue system’s adaptability to learn, unlearn, and re-learn from user interactions, monitoring emotional states and personal information, is essential. 3. Additionally, addressing ethical concerns related to enhanced empathy and human-like qualities in chatbots, including guidelines on handling user information and drawing conclusions, should be a focus of future investigations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Explore alternative markers for empathic responses, involve different character styles and modes of realism, and incorporate physiological or behavioral measures of empathy. 2. Additionally, replicating the study in a virtual reality context was suggested to further understand the role of immersive environments in influencing empathic experiences with virtual characters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p">Future work to incorporate video and audio data with EEG and physiological signals for a comprehensive approach to emotion recognition in conversations. The methods and dataset presented are anticipated to benefit various intelligent user interface applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
</li>
<li id="S4.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S4.I1.i6.p1" class="ltx_para">
<p id="S4.I1.i6.p1.1" class="ltx_p">Explore the system’s potential applications in educational settings, particularly for training and educating medical professionals. Assess how the system could be integrated into medical education curricula <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
</li>
<li id="S4.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="S4.I1.i7.p1" class="ltx_para">
<p id="S4.I1.i7.p1.1" class="ltx_p">Future research with a larger participant pool, the inclusion of higher emotional intensity words, and exploration of additional physiological signals like EEG and heart rate variability (HRV). The study’s implications extend to designing text-based VR interfaces, passive learning, exposure treatment, and personalized VR gaming experiences <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Next Steps</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">This study has revealed that research in this domain is greatly limited, and this is evident in the few papers deemed eligible for inclusion (only 31 papers).
Based on the available literature reviewed, there is limited study on the subject of incorporating emotional expression in STT in AR/VR for use in educational setting purposes.
Therefore, it is necessary to conduct further research to fill the identified gap. To do this, my next step will involve working with the following research questions as my Ph.D. research work:</p>
<ol id="S4.I2" class="ltx_enumerate">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">Can the integration of transcribed text in an Augmented Reality with corresponding visual elements such as pictures, emojis, and avatars enhance the conveyance of emotional expression in speech-to-text conversion?</p>
</div>
<div id="S4.I2.i1.p2" class="ltx_para">
<p id="S4.I2.i1.p2.1" class="ltx_p"><span id="S4.I2.i1.p2.1.1" class="ltx_text ltx_font_bold">Strategy</span>: Subsequently, conduct a user study to assess the effectiveness of this combined approach.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">What is the optimal channel and format for delivering speech-to-text transcriptions while preserving the speaker’s facial expressions?</p>
</div>
<div id="S4.I2.i2.p2" class="ltx_para">
<p id="S4.I2.i2.p2.1" class="ltx_p"><span id="S4.I2.i2.p2.1.1" class="ltx_text ltx_font_bold">Strategy</span>: Investigate different text formatting options, including presenting a few words per line, highlighting emphasized words, and summarizing content while maintaining eye contact, to determine the most effective approach.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p">How can the user experience in speech-to-text technology be enhanced to minimize cognitive load for both the speaker and the user?</p>
</div>
<div id="S4.I2.i3.p2" class="ltx_para">
<p id="S4.I2.i3.p2.1" class="ltx_p"><span id="S4.I2.i3.p2.1.1" class="ltx_text ltx_font_bold">Strategy</span>: Explore strategies and design elements that contribute to an improved user experience in speech-to-text interactions.</p>
</div>
</li>
<li id="S4.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S4.I2.i4.p1" class="ltx_para">
<p id="S4.I2.i4.p1.1" class="ltx_p">Can the substitution of emotionally recognizable words with equivalent emojis and avatars, introduced dynamically in a live transcription, effectively preserve and convey emotional expression?</p>
</div>
<div id="S4.I2.i4.p2" class="ltx_para">
<p id="S4.I2.i4.p2.1" class="ltx_p"><span id="S4.I2.i4.p2.1.1" class="ltx_text ltx_font_bold">Strategy</span>: Examine the impact of replacing words with visual elements on emotional expression in speech-to-text, particularly in a live transcription setting.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In conclusion, the narrative literature review traverses various communication enhancement technologies, showcasing advancements in tools and techniques. From live transcription and closed captioning to innovations in AR, emotive captioning, emotion recognition, and empathic machines, the review provides a holistic perspective on addressing the diverse sensory and cognitive needs of individuals with hearing impairments. The study underscores the ongoing challenges and potential avenues for future research in creating empathic applications for human-computer interaction and enhancing communication for individuals with hearing impairments.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">This narrative review explores communication enhancement technologies for DHH individuals, including anyone who relies on language transcription services to help with communication, with a focus on the psychophysiology of STT interfaces in VR.
The review highlights advancements in STT and closed captioning, emphasizing their relevance in enhancing communication accessibility, emphasizing how emotional expression is lost in the process of transcribing speech into text.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">The results from selected studies matter as they not only contribute to the ongoing efforts to make communication technologies more inclusive for the DHH community but also pave the way for emotionally rich interactions in various contexts, from virtual environments to healthcare and education.
The cited studies collectively suggest a growing recognition of the importance of emotional expression in human-computer interactions and the ongoing pursuit of innovative solutions to address this crucial aspect.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">The integration of empathy into machines has the potential to redefine how we interact with technology, making these interactions more intuitive, adaptive, and emotionally resonant.
However, ethical considerations, user preferences, and ongoing research are crucial in shaping the responsible and effective deployment of such technologies.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">The review has inspired future research recommendations based on the key findings uncovered from the study.
In conclusion, this synthesis provides a comprehensive overview of the evolution of communication enhancement technologies, spanning from live transcription and closed captioning to innovations in AR, VR, and speech emotion recognition.
The interdisciplinary nature of the field is underscored, emphasizing the need for continued research and development to create more inclusive and accessible communication technologies for individuals with hearing impairments and the general language transcription user.
As we navigate this dynamic landscape, the synthesis serves as a guide for future research directions and the development of impactful technologies that cater to diverse sensory and cognitive needs.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A.-W. Harzing, “Publish or perish.” https://harzing.com/resources/publish-or-perish, 2007.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
M. Ouzzani, H. Hammady, Z. Fedorowicz, and A. Elmagarmid, “Rayyan — a web and mobile app for systematic reviews,” <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Systematic Reviews</span>, vol. 5, p. 210, 2016.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
F. Loizides, S. Basson, D. Kanevsky, O. Prilepova, S. Savla, and S. Zaraysky, “Breaking boundaries with live transcribe: Expanding use cases beyond standard captioning scenarios,” in <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the 22nd International ACM SIGACCESS Conference on Computers and Accessibility</span>, pp. 1–6, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
D. Arnold and A. Tremblay, “Interaction of deaf and hearing preschool children,” <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Journal of Communication Disorders</span>, vol. 12, no. 3, pp. 245–251, 1979.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
R. Rashid, V. Quoc, R. Hunt, and D. I. Fels, “Dancing with words: Using animated text for captioning,” <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Intl. Journal of Human–Computer Interaction</span>, vol. 24, no. 5, pp. 505–519, 2008.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
V. Murphy-Berman and L. Whobrey, “The impact of captions on hearing-impaired children’s affective reactions to television,” <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">The Journal of Special Education</span>, vol. 17, no. 1, pp. 47–62, 1983.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
R. Rashid, J. Aitken, and D. I. Fels, “Expressing emotions using animated text captions,” in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Computers Helping People with Special Needs: 10th International Conference, ICCHP 2006, Linz, Austria, July 11-13, 2006. Proceedings 10</span>, pp. 24–31, Springer, 2006.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
D. G. Lee, D. I. Fels, and J. P. Udo, “Emotive captioning,” <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Computers in Entertainment (CIE)</span>, vol. 5, no. 2, p. 11, 2007.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Hu, Q. Xu, L. P. Fu, and Y. Xu, “Emojilization: An automated method for speech to emoji-labeled text,” in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems</span>, pp. 1–6, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
R. Poguntke, T. Mantz, M. Hassib, A. Schmidt, and S. Schneegass, “Smile to me: investigating emotions and their representation in text-based messaging in the wild,” in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of Mensch und Computer 2019</span>, pp. 373–385, ACM, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Q. Li, “Text vs. images: Understanding emotional expressions on social media during covid-19 pandemic,” <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Human Factors in Communication of Design</span>, vol. 49, p. 18, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
K. Kilteni, R. Groten, and M. Slater, “The sense of embodiment in virtual reality,” <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Presence: Virtual and Augmented Reality</span>, vol. 21, no. 4, pp. 373–387, 2012.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S. Lie, K. Røykenes, A. Sæheim, and K. Groven, “Developing a virtual reality educational tool to stimulate emotions for learning: focus group study,” <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">JMIR Formative Research</span>, vol. 7, p. e41829, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
F. Pallavicini and A. Pepe, “Virtual reality games and the role of body involvement in enhancing positive emotions and decreasing anxiety: within-subjects pilot study,” <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">JMIR Serious Games</span>, vol. 8, no. 2, p. e15635, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Alnafjan, A. Aljumaah, H. Alaskar, and R. Alshraihi, “Designing “najeeb”: Technology-enhanced learning for children with impaired hearing using arabic sign-language arsl applications,” in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">2017 International Conference on Computer and Applications (ICCA)</span>, pp. 238–273, IEEE, 2017.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J. Li, “Real-time augmented reality visual-captions for deaf and hard-of-hearing children in classrooms,” in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)</span>, pp. 641–642, IEEE, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J. Pirker, J. Kopf, A. Kainz, A. Dengel, and B. Buchbauer, “The potential of virtual reality for computer science education-engaging students through immersive visualizations,” in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)</span>, pp. 297–302, IEEE, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
X. Chen and H. Chen, “Emotion recognition using facial expressions in an immersive virtual reality application,” <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Virtual Reality</span>, vol. 27, no. 3, pp. 1717–1732, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
A. Elor, A. Song, and S. Kurniawan, “Understanding emotional expression with haptic feedback vest patterns and immersive virtual reality,” in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)</span>, pp. 183–188, IEEE, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
L. Bartram, A. Patra, and M. Stone, “Affective color in visualization,” in <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2017 CHI conference on human factors in computing systems</span>, pp. 1364–1374, 2017.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
D. Cernea, C. Weber, A. Ebert, and A. Kerren, “Emotion-prints: Interaction-driven emotion visualization on multi-touch interfaces,” in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Visualization and Data Analysis 2015</span>, vol. 9397, pp. 82–96, SPIE, 2015.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
H. Wang, H. Prendinger, and T. Igarashi, “Communicating emotions in online chat using physiological sensors and animated text,” in <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">CHI’04 extended abstracts on Human factors in computing systems</span>, pp. 1171–1174, 2004.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
D. J. Schiano, S. M. Ehrlich, K. Rahardja, and K. Sheridan, “Face to interface: facial affect in (hu) man and machine,” in <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proceedings of the SIGCHI conference on Human factors in computing systems</span>, pp. 193–200, 2000.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
D. Kumarapeli, S. Jung, and R. W. Lindeman, “Emotional avatars: Effect of uncanniness in identifying emotions using avatar expressions,” in <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)</span>, pp. 650–651, IEEE, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
M. Rebol, C. Güti, and K. Pietroszek, “Passing a non-verbal turing test: Evaluating gesture animations generated from speech,” in <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">2021 IEEE Virtual Reality and 3D User Interfaces (VR)</span>, pp. 573–581, IEEE, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
E. Bosch, D. Bethge, M. Klosterkamp, and T. Kosch, “Empathic technologies shaping innovative interaction: Future directions of affective computing,” in <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Adjunct Proceedings of the 2022 Nordic Human-Computer Interaction Conference</span>, pp. 1–3, 2022.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
K. Daher, J. Casas, O. A. Khaled, and E. Mugellini, “Empathic chatbot response for medical assistance,” in <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents</span>, pp. 1–3, 2020.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
X. Sun and H. Guan, “Research on empathic remediation mechanism of chatbots mediated by social presence and trust,” in <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering</span>, pp. 772–776, 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J. Casas, T. Spring, K. Daher, E. Mugellini, O. A. Khaled, and P. Cudré-Mauroux, “Enhancing conversational agents with empathic abilities,” in <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents</span>, pp. 41–47, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J.-Y. Jung and A. Bozzon, “Are female chatbots more empathic?-discussing gendered conversational agent through empathic design,” in <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2nd Empathy-Centric Design Workshop</span>, pp. 1–5, 2023.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. Roshdy, S. Al Kork, A. Karar, A. Al Sabi, Z. Al Barakeh, F. ElSayed, T. Beyrouthy, and A. Nait-Ali, “Machine empathy: Digitizing human emotions,” in <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">2021 International Symposium on Electrical, Electronics and Information Engineering</span>, pp. 307–311, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
N. Saffaryazdi, Y. Goonesekera, N. Saffaryazdi, N. D. Hailemariam, E. G. Temesgen, S. Nanayakkara, E. Broadbent, and M. Billinghurst, “Emotion recognition in conversations using brain and physiological signals,” in <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">27th International Conference on Intelligent User Interfaces</span>, pp. 229–242, 2022.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
E. Bagheri, P. G. Esteban, H.-L. Cao, A. D. Beir, D. Lefeber, and B. Vanderborght, “An autonomous cognitive empathy model responsive to users’ facial emotion expressions,” <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Interactive Intelligent Systems (TIIS)</span>, vol. 10, no. 3, pp. 1–23, 2020.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
T. Lin, L. Huang, B. Hannaford, C. Tran, J. Raiti, R. Zaragoza, T. Feng, L. Wagner, and J. James, “Empathics system: application of emotion analysis ai through smart glasses,” in <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Proceedings of the 13th ACM International Conference on PErvasive Technologies Related to Assistive Environments</span>, pp. 1–4, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
K. Gupta, S. W. Chan, Y. S. Pai, A. Sumich, S. Nanayakkara, and M. Billinghurst, “Towards understanding physiological responses to emotional autobiographical memory recall in mobile vr scenarios,” in <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Adjunct Publication of the 23rd International Conference on Mobile Human-Computer Interaction</span>, pp. 1–5, 2021.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
D. Higgins, Y. Zhan, B. R. Cowan, and R. McDonnell, “Investigating the effect of visual realism on empathic responses to emotionally expressive virtual humans,” in <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">ACM Symposium on Applied Perception 2023</span>, pp. 1–7, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.13923" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.13924" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.13924">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.13924" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.13925" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 16:17:09 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
