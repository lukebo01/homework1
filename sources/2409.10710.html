<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.10710] A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection with Children</title><meta property="og:description" content="Social-emotional learning (SEL) skills are essential for children to develop to provide a foundation for future relational and academic success. Using art as a medium for creation or as a topic to provoke conversation …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection with Children">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection with Children">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.10710">

<!--Generated on Sat Oct  5 18:37:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection with Children
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Isabella Pu<sup id="id10.10.id1" class="ltx_sup"><span id="id10.10.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Golda Nguyen<sup id="id11.11.id2" class="ltx_sup"><span id="id11.11.id2.1" class="ltx_text ltx_font_italic">2</span></sup>, Lama Alsultan<sup id="id12.12.id3" class="ltx_sup"><span id="id12.12.id3.1" class="ltx_text ltx_font_italic">1,3</span></sup>, Rosalind Picard<sup id="id13.13.id4" class="ltx_sup"><span id="id13.13.id4.1" class="ltx_text ltx_font_italic">1</span></sup>, Cynthia Breazeal<sup id="id14.14.id5" class="ltx_sup"><span id="id14.14.id5.1" class="ltx_text ltx_font_italic">1</span></sup>, Sharifa Alghowinem<sup id="id15.15.id6" class="ltx_sup"><span id="id15.15.id6.1" class="ltx_text ltx_font_italic">1</span></sup>
</span><span class="ltx_author_notes"><sup id="id16.16.id1" class="ltx_sup"><span id="id16.16.id1.1" class="ltx_text ltx_font_italic">1</span></sup> MIT Media Lab,
Massachusetts Institute of Technology, Cambridge, MA, United States,
<span id="id17.17.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">ipu@media.mit.edu</span><sup id="id18.18.id1" class="ltx_sup"><span id="id18.18.id1.1" class="ltx_text ltx_font_italic">2</span></sup> Department of Aeronautics and Astronautics,
Massachusetts Institute of Technology, Cambridge, MA, United States<sup id="id19.19.id1" class="ltx_sup"><span id="id19.19.id1.1" class="ltx_text ltx_font_italic">3</span></sup> Department of Computing and Information Systems,
Prince Sultan University, Riyadh, Saudi Arabia</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id20.id1" class="ltx_p">Social-emotional learning (SEL) skills are essential for children to develop to provide a foundation for future relational and academic success. Using art as a medium for creation or as a topic to provoke conversation is a well-known method of SEL learning. Similarly, social robots have been used to teach SEL competencies like empathy, but the combination of art and social robotics has been minimally explored. In this paper, we present a novel child-robot interaction designed to foster empathy and promote SEL competencies via a conversation about art scaffolded by a social robot. Participants (N=11, age range: 7-11) conversed with a social robot about emotional and neutral art. Analysis of video and speech data demonstrated that this interaction design successfully engaged children in the practice of SEL skills, like emotion recognition and self-awareness, and greater rates of empathetic reasoning were observed when children engaged with the robot about emotional art. This study demonstrated that art-based reflection with a social robot, particularly on emotional art, can foster empathy in children, and interactions with a social robot help alleviate discomfort when sharing deep or vulnerable emotions.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">INTRODUCTION &amp; BACKGROUND</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Social and emotional intelligence is an essential skill for individuals to effectively communicate, interact, and build relationships. This can be fostered through <span id="S1.p1.1.1" class="ltx_text ltx_font_bold">social-emotional learning</span>, which Elias et al. describes as the systematic acquisition of emotional intelligence by developing relevant skills, attitudes, and values <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The Collaborative for Academic, Social, and Emotional Learning (CASEL) names core SEL skills as self-awareness, self-management, social awareness, relationship skills, and responsible decision-making <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Early exposure to SEL provides significant short-term and long-term behavioral benefits, such as improved self-confidence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, reduced likelihood of emotional issues and conduct problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, and improved long-term academic and relational success <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Additionally, a 2013 survey showed 97% of teachers acknowledge the positive impact of SEL on students from all socioeconomic backgrounds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, and a study from the Aspen Institute found that SEL programs were particularly beneficial for fostering well-being in children from low-income communities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS1.4.1.1" class="ltx_text">I-A</span> </span><span id="S1.SS1.5.2" class="ltx_text ltx_font_italic">Social Robots for Social-Emotional Learning</span>
</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">To increase access to SEL programming, interactive technologies can deliver educational content inside and outside of the classroom. Social robots and artificial intelligence systems, through responsive engagement with interaction partners, are becoming increasingly prevalent in early childhood education <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. However, social robots have predominantly been used in childhood education for academic learning, focusing on areas like language <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, literacy, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, and computer science skills <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, while social robots for SEL practice have been relatively limited.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">Social robots are a promising method of delivering early SEL education to children given children’s high engagement with these technologies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Previous work has shown social robots can alleviate anxiety in children by offering reassurance, and that children felt comfortable sharing emotions with a social robot <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Another promising capability of social robots is their ability to convey empathy and to foster the development of empathy in children <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">Though fewer in number compared to studies on academic education with social robots, prior work has demonstrated success in using social robots to teach SEL competencies.
Embodied’s Moxie, a social robot designed to teach SEL skills to children with developmental disorders, significantly improved children’s SEL compentencies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
Several prior studies have focused on teaching emotion recognition and empathy to neurodivergent children (i.e., with autism spectrum disorder) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, but SEL training is impactful for both neurodivergent and neurotypical populations.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS2.4.1.1" class="ltx_text">I-B</span> </span><span id="S1.SS2.5.2" class="ltx_text ltx_font_italic">Social-Emotional Learning with Art</span>
</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">Using art as a medium for self-expression or to evoke thoughtful conversation is a widely recognized method of SEL instruction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Prior studies have demonstrated the use of both art education <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and artistic creation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> to build interpersonal and social skills, like self-awareness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, that contribute to overall emotional development in children <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">Different mediums of art have also been explored for SEL programming, including dance, music, and visual arts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. While most arts curricula for teaching SEL involve creating art, several examples have focused on discussing and observing art. Ebert et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> conducted a workshop where children observed emotions in subjects of different artworks to develop SEL skills like emotion recognition and self-awareness. The Metropolitan Museum of Art <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> developed a curriculum based on art observation and discussion to help students practice self-awareness, self-management, social awareness, empathy, and relationship skills—this curriculum was specifically designed for blind or partially sighted students, students with autism spectrum disorder (ASD), and students with developmental disabilities. In these curricula, children successfully developed skills like empathy and emotion recognition by discussing art in scaffolded settings.</p>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS3.4.1.1" class="ltx_text">I-C</span> </span><span id="S1.SS3.5.2" class="ltx_text ltx_font_italic">Study Objectives</span>
</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.1" class="ltx_p">The use of art has been minimally explored in tandem with interactive and responsive social agents for SEL instruction. Cooney et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> demonstrated the potential of social robots for art therapy to facilitate emotional expression via artistic creation. We expand on this connection by exploring robot-guided conversations on art, in which art is used as a conversational catalyst for children to practice emotion recognition, self-awareness, and empathy. Specifically, an experimental study was conducted to explore how different styles of art (explicitly emotive art and neutral art) affect behavioral responses in children (ages 7-11). Reflection on the artwork was scaffolded by guiding questions and responsive dialogue from a social robot, Jibo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, to investigate the following research questions:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">RQ1: </span>When interactively reflecting with a social robot, does emotional art foster empathy more successfully than neutral art in children?</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">RQ2: </span>Are there behavioral differences (in engagement, disclosure) when children reflect with a social robot on emotional art versus neutral art?</p>
</div>
</li>
</ul>
</div>
<div id="S1.SS3.p2" class="ltx_para ltx_noindent">
<p id="S1.SS3.p2.1" class="ltx_p">We propose the following hypotheses:</p>
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p"><span id="S1.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">H1: </span>Emotional art will foster more empathy than neutral art during interactive reflection with a social robot.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p"><span id="S1.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">H2: </span>Children will engage more deeply with emotional art than neutral art during the social robot interaction.</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p"><span id="S1.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">H3: </span>Children who are more open when sharing feelings with the social robot will engage more with the activity.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">INTERACTION DESIGN</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Robot Station Design</span>
</h3>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2409.10710/assets/Images/SmallJiboStation_v2.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="151" height="97" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">Jibo Robot Station</span></figcaption>
</figure>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2409.10710/assets/Images/SysArch.jpeg" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="129" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.3.2" class="ltx_text" style="font-size:90%;">Robot Station System Architecture</span></figcaption>
</figure>
<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">We designed an interaction where participants spoke with Jibo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, a social embodied agent, through a multi-device system (the “Robot Station”, shown in Figure <a href="#S2.F1" title="Figure 1 ‣ II-A Robot Station Design ‣ II INTERACTION DESIGN ‣ A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection with Children" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Jibo was used because of its ability to express emotional states with bodily animations, expressive screen-based face, child-friendly character design, and durability for autonomous conversational interactions. The Robot Station has a Jibo robot on the left side and a Samsung Galaxy tablet on the right. There is also a Logitech C930e camera housed above the tablet and a separate MXL AC-44 microphone located between Jibo and the tablet. An Ubuntu machine located inside the Robot Station behind the tablet controls Jibo’s movements and utterances.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The Robot Station design allows for Jibo to look between the participant and the tablet for social engagement. Jibo can also freely rotate around three axes while placed in the Robot Station, to provide emotive movement while speaking.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Jibo converses with the participant by asking scripted questions about artwork and replying with generations from GPT-4<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://openai.com/research/gpt-4</span></span></span>, in response to participant dialogue. The Ubuntu machine sends commands to Jibo through messaging a Firebase database, which Jibo reads from. The tablet also communicates with the same database, allowing Jibo, the Ubuntu machine, and the tablet to execute synchronously. The system architecture is shown in detail in Figure <a href="#S2.F2" title="Figure 2 ‣ II-A Robot Station Design ‣ II INTERACTION DESIGN ‣ A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection with Children" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Art Design for Robot Interaction</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Two categories of artwork were used to prompt empathy and disclosure in the child-robot interaction: 1) <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">emotional</span> and 2) <span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_italic">neutral</span> art. Artworks shown to participants were generated with DALL-E 3<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://openai.com/dall-e-3</span></span></span> and reviewed in advance by the research team to ensure their suitability for children.</p>
</div>
<figure id="S2.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2409.10710/assets/Art/HAPPY.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="70" height="70" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2409.10710/assets/Art/SAD.png" id="S2.F3.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="70" height="70" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2409.10710/assets/Art/ANGER.png" id="S2.F3.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="70" height="70" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">Emotional artworks</span></figcaption>
</figure>
<figure id="S2.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2409.10710/assets/Art/FISH.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="70" height="70" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2409.10710/assets/Art/LIVINGROOM.png" id="S2.F4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="70" height="70" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2409.10710/assets/Art/FARM.png" id="S2.F4.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="70" height="70" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S2.F4.3.2" class="ltx_text" style="font-size:90%;">Neutral artworks</span></figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_italic">Emotional</span> artworks (Figure <a href="#S2.F3" title="Figure 3 ‣ II-B Art Design for Robot Interaction ‣ II INTERACTION DESIGN ‣ A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection with Children" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) featured animal characters explicitly displaying an emotion: happiness, sadness, or anger. Different styles were used for variety but all included vibrant colors and animals to appeal to children. As an example, the prompt for the “anger” image was: <span id="S2.SS2.p2.1.2" class="ltx_text ltx_font_italic">Two angry teddy bears arguing in the street, kid-friendly comic style, hand-drawn, with vibrant colors</span>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_italic">Neutral</span> artworks (Figure <a href="#S2.F4" title="Figure 4 ‣ II-B Art Design for Robot Interaction ‣ II INTERACTION DESIGN ‣ A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection with Children" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) depicted abstract or landscape imagery to elicit emotions, but not any specific emotion, based on color, composition, or style. These pieces were also designed for children, with bright colors and familiar locations (the ocean, a living room, and a farm). For example, the “living room” image prompt was: <span id="S2.SS2.p3.1.2" class="ltx_text ltx_font_italic">Memphis Group style image of a colorful living room where the furniture is upside down, sideways, and backwards, with a blue couch that is right side up. 2D style with bold shapes</span>.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">We validated the emotional effects of these generated artworks during the experimental study (see Section IV).</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">EXPERIMENT &amp; ANALYSIS METHODOLOGY</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">An experimental study was conducted to explore how social robots can help children <span id="S3.p1.1.1" class="ltx_text ltx_font_bold">foster empathy</span> and <span id="S3.p1.1.2" class="ltx_text ltx_font_bold">practice SEL skills</span> via conversation on art. The study consisted of two sessions: 1) the participant discussed “neutral art” (without clear emotions) with a social robot (Jibo), and 2) the participant discussed “emotional art” (where characters in the art explicitly convey emotions) with the social robot. A within-subjects design was used, and session order was randomized to control for ordering effects. The study protocol was approved by our institution’s ethics review board.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Experimental Procedure</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Participants and their parents were invited to an in-person study in an enclosed space. Two members of the research team were also in the room. The study lasted approximately one hour with two robot interaction sessions (each about 15 minutes), with a break between sessions. Sessions were video- and audio-recorded using two cameras (the robot station and a wall-mounted GoPro) and the station microphone.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The first interaction session with Jibo began with a brief tutorial on how to record responses to Jibo’s questions and two neutral practice questions, asking for the participant’s name and age. After the tutorial, participants engaged in the first interaction session (one of the two categories described in <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">Art Design for Robot Interaction</span>). There was a short break after the first session before participants began the second session. After both sessions were completed, participants were interviewed on their experience.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Within each session, Jibo asked the following questions about each image:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Can you tell me a story about this picture or describe this picture to me?</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">What emotion does this picture make you feel?</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Why does this picture make you feel that emotion?</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Can you tell me about the last time you felt that emotion?</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">The questions were designed to guide participants to reflect on emotions they observed or related to within the art piece. Specifically, the questions targeted self-awareness (“identifying one’s emotions”, “linking feelings, values, and thoughts”) and relationship skills (“communicating effectively”) in all sessions, while additionally targeting social awareness (“demonstrating empathy and compassion”) in the emotional session <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Table <a href="#S3.T1" title="TABLE I ‣ III-A Experimental Procedure ‣ III EXPERIMENT &amp; ANALYSIS METHODOLOGY ‣ A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection with Children" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> provides an example of question prompts from Jibo and P-10’s responses to an emotional image.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.2.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S3.T1.3.2" class="ltx_text" style="font-size:90%;">Example Interaction between Jibo and P-10</span></figcaption>
<table id="S3.T1.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.4.1.1" class="ltx_tr">
<th id="S3.T1.4.1.1.1" class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.4.1.1.1.1" class="ltx_text ltx_font_bold">Jibo</span>:</th>
<th id="S3.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S3.T1.4.1.1.2.1" class="ltx_text ltx_font_bold">P-10</span>:</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.4.2.1" class="ltx_tr">
<td id="S3.T1.4.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.4.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.2.1.1.1.1" class="ltx_p" style="width:140.9pt;">Can you tell me a story about this picture or describe this picture to me?</span>
</span>
</td>
<td id="S3.T1.4.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.4.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.2.1.2.1.1" class="ltx_p" style="width:249.3pt;">The picture is about two bears that probably got in a fight. And they wanted… and they’re like, mad at each other.</span>
</span>
</td>
</tr>
<tr id="S3.T1.4.3.2" class="ltx_tr">
<td id="S3.T1.4.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.4.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.3.2.1.1.1" class="ltx_p" style="width:140.9pt;">What emotion does this picture make you feel?</span>
</span>
</td>
<td id="S3.T1.4.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.4.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.3.2.2.1.1" class="ltx_p" style="width:249.3pt;">It makes me feel sort of… It’s sort of silly. It makes me feel sort of silly, and scared. It makes me feel scared because I don’t like when people get in fights, and it makes me feel sort of silly because it’s funny that there are two bears that are in a fight.</span>
</span>
</td>
</tr>
<tr id="S3.T1.4.4.3" class="ltx_tr">
<td id="S3.T1.4.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.4.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.3.1.1.1" class="ltx_p" style="width:140.9pt;">Why does this picture make you feel that emotion?</span>
</span>
</td>
<td id="S3.T1.4.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.4.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.3.2.1.1" class="ltx_p" style="width:249.3pt;">Well, it’s because I don’t like when people get hurt and fighting usually means that you can get hurt. But it’s funny to watch. Because, I don’t know. I just feel this emotion.</span>
</span>
</td>
</tr>
<tr id="S3.T1.4.5.4" class="ltx_tr">
<td id="S3.T1.4.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.4.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.5.4.1.1.1" class="ltx_p" style="width:140.9pt;">Can you tell me about the last time you felt that emotion?</span>
</span>
</td>
<td id="S3.T1.4.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T1.4.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.5.4.2.1.1" class="ltx_p" style="width:249.3pt;">Well, one time I was with my brother and I got in a fight with him because I got mad at him. We didn’t get hurt, but I got scared. And I felt like it was sort of silly that we were both fighting though.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">Jibo’s utterances were generated by GPT-4 to adapt to participant responses for a more personalized interaction (GPT-4 responses are not shown in Table <a href="#S3.T1" title="TABLE I ‣ III-A Experimental Procedure ‣ III EXPERIMENT &amp; ANALYSIS METHODOLOGY ‣ A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection with Children" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> for brevity).
For example, a participant told Jibo about a new pet gecko, and Jibo’s response referenced the gecko and gave an encouraging comment on the bond between humans and pets.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">Though GPT-4 responses may vary between participants due to the AI’s nature, using the same strict and detailed prompts for all participants ensured that the responses maintained consistent vocabulary and were within the same vein.</p>
</div>
<div id="S3.SS1.p7" class="ltx_para">
<p id="S3.SS1.p7.1" class="ltx_p">The post-study interview was conducted without the Jibo robot present, to minimize potential effects of the robot’s presence effect on the child’s opinions. The child was asked questions on a tablet about what they liked about the interactions with Jibo, what they disliked, if they would change anything, their feelings of comfort with or trust in Jibo, and if they would want to interact with Jibo again.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Participants</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">This study was conducted with 11 children between age 7 and 11 (average age of 9.3), with 6 female and 5 male participants. 7 participants were white, 2 were Asian, and 2 were mixed race. Ethnicity was not considered as a factor in this study and was not analyzed any further than for demographic purposes. Legal guardians were consented, and participants provided their assent.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Participants were recruited via email advertisements to parents of children who had previously participated in community outreach programs or indicated interest in robot and AI studies. Participants and their family were not familiar with the specific researchers facilitating this study. Participants’ travel to the study location was reimbursed, but they were not otherwise compensated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Linguistic Analysis Methods</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Linguistic analysis of participant dialogue was performed to collect measures of conversational engagement and disclosure. Speech was first transcribed by Assembly AI Automatic Speech Recognition<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://www.assemblyai.com/</span></span></span>, then manually cleaned by the research team. Common themes were coded by question using thematic content analysis to examine emotional trends. Coding was performed by two independent raters, and inter-rater agreement was calculated using Cohen’s Kappa (<math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="\kappa" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">κ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝜅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\kappa</annotation></semantics></math>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Raters examined responses to question <math id="S3.SS3.p2.1.m1.1" class="ltx_math_unparsed" alttext="2)" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1b"><mn id="S3.SS3.p2.1.m1.1.1">2</mn><mo stretchy="false" id="S3.SS3.p2.1.m1.1.2">)</mo></mrow><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">2)</annotation></semantics></math> and listed all emotions participants cited in order to validate the success of the emotional artwork in eliciting the target emotion.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.2" class="ltx_p">Responses to question <math id="S3.SS3.p3.1.m1.1" class="ltx_math_unparsed" alttext="3)" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1b"><mn id="S3.SS3.p3.1.m1.1.1">3</mn><mo stretchy="false" id="S3.SS3.p3.1.m1.1.2">)</mo></mrow><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">3)</annotation></semantics></math> were coded as exhibiting “empathetic reasoning”, “visual reasoning”, or neither.
Examples of empathetic reasoning included <span id="S3.SS3.p3.2.1" class="ltx_text ltx_font_italic">“… she’s happy—just like I am—because she’s probably happy that she’s dancing because I’m happy when I dance.”</span> from P-10 and <span id="S3.SS3.p3.2.2" class="ltx_text ltx_font_italic">“It makes me feel upset because the hedgehog has dropped its ice cream, and it looks like really good ice cream.”</span> from P-06.
Examples of visual reasoning from the same participants included <span id="S3.SS3.p3.2.3" class="ltx_text ltx_font_italic">“It’s the theme of the artwork, and also the colors because it has rainbow colors on it and I like rainbow colors.”</span> from P-10 and <span id="S3.SS3.p3.2.4" class="ltx_text ltx_font_italic">“Because of the way it looks.”</span> from P-06.
Responses like <span id="S3.SS3.p3.2.5" class="ltx_text ltx_font_italic">“I don’t know”</span> or incoherent responses were labeled as exhibiting neither type of reasoning. This coding had very strong inter-rater agreement (<math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="\kappa=0.82" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mrow id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">κ</mi><mo id="S3.SS3.p3.2.m2.1.1.1" xref="S3.SS3.p3.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS3.p3.2.m2.1.1.3" xref="S3.SS3.p3.2.m2.1.1.3.cmml">0.82</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><eq id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1.1"></eq><ci id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2">𝜅</ci><cn type="float" id="S3.SS3.p3.2.m2.1.1.3.cmml" xref="S3.SS3.p3.2.m2.1.1.3">0.82</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">\kappa=0.82</annotation></semantics></math>).</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Video Analysis Methods</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">To conduct behavioral analysis, video data of the sessions was annotated by two independent annotators using ELAN 6.7, with a coding manual on three measurements: Comfort (ease, relaxation, and lack of anxiety), Engagement (attention, interest, and active participation), and Openness (open, honest, and vulnerable behavior). Each of the measurements included a negative (-1), neutral (0), or positive (1) rating. Annotators specifically marked events with negative (-1) or positive (1) behavior, leaving unmarked sections of video scored as neutral (0). Cohen’s Kappa was calculated on 30% of the video data to measure inter-rater reliability, revealing a substantial agreement score <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="(0.61&lt;\kappa&lt;0.80)" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mrow id="S3.SS4.p1.1.m1.1.1.1" xref="S3.SS4.p1.1.m1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS4.p1.1.m1.1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.1.1.cmml">(</mo><mrow id="S3.SS4.p1.1.m1.1.1.1.1" xref="S3.SS4.p1.1.m1.1.1.1.1.cmml"><mn id="S3.SS4.p1.1.m1.1.1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.1.1.2.cmml">0.61</mn><mo id="S3.SS4.p1.1.m1.1.1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.1.1.3.cmml">&lt;</mo><mi id="S3.SS4.p1.1.m1.1.1.1.1.4" xref="S3.SS4.p1.1.m1.1.1.1.1.4.cmml">κ</mi><mo id="S3.SS4.p1.1.m1.1.1.1.1.5" xref="S3.SS4.p1.1.m1.1.1.1.1.5.cmml">&lt;</mo><mn id="S3.SS4.p1.1.m1.1.1.1.1.6" xref="S3.SS4.p1.1.m1.1.1.1.1.6.cmml">0.80</mn></mrow><mo stretchy="false" id="S3.SS4.p1.1.m1.1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1.1"><and id="S3.SS4.p1.1.m1.1.1.1.1a.cmml" xref="S3.SS4.p1.1.m1.1.1.1"></and><apply id="S3.SS4.p1.1.m1.1.1.1.1b.cmml" xref="S3.SS4.p1.1.m1.1.1.1"><lt id="S3.SS4.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.3"></lt><cn type="float" id="S3.SS4.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.2">0.61</cn><ci id="S3.SS4.p1.1.m1.1.1.1.1.4.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.4">𝜅</ci></apply><apply id="S3.SS4.p1.1.m1.1.1.1.1c.cmml" xref="S3.SS4.p1.1.m1.1.1.1"><lt id="S3.SS4.p1.1.m1.1.1.1.1.5.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.5"></lt><share href="#S3.SS4.p1.1.m1.1.1.1.1.4.cmml" id="S3.SS4.p1.1.m1.1.1.1.1d.cmml" xref="S3.SS4.p1.1.m1.1.1.1"></share><cn type="float" id="S3.SS4.p1.1.m1.1.1.1.1.6.cmml" xref="S3.SS4.p1.1.m1.1.1.1.1.6">0.80</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">(0.61&lt;\kappa&lt;0.80)</annotation></semantics></math> across all three measures. Annotators also noted behavioral patterns that stood out or occurred often among participants.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">VALIDATION OF EMOTIONAL ARTWORK</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The emotional art used in this study was generated specifically for this experiment, with the goal of having participants empathize with that emotion. The three images used, as seen in Figure <a href="#S2.F3" title="Figure 3 ‣ II-B Art Design for Robot Interaction ‣ II INTERACTION DESIGN ‣ A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection with Children" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, sought to demonstrate happiness, sadness, and anger respectively (viewed from left to right).</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Most participants (8 out of 11) referenced some dimension of happiness (synonyms included “joyful” and “excited”) when viewing the image designed to elicit happiness. The other three participants referenced feeling “fine” (later elaborating that the image did not elicit a strong feeling), “weirded out” (describing that <span id="S4.p2.1.1" class="ltx_text ltx_font_italic">“a ballerina dress on an elephant seems weird”</span>), and “fear” (explaining that it reminded them of a dancing-related memory that induced fear when recollected).</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">All participants cited some dimension of sadness (synonyms included “feeling bad” and “upset”) when viewing the image designed to induce sadness.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">The picture designed to induce anger was the most polarizing, with 7 out of 11 participants describing it as making them feel either “angry” or “annoyed”. Of the other 4 participants, 3 expressed that they found the image “funny” or that it made them feel “silly”. When elaborating on their reasoning, these participants explained that they found watching others fight to be a funny or silly experience. However, 2 of these 3 participants still shared a related memory that involved a fight and described having negative emotions during that memory, like fear and confusion. One also mentioned that the image did not elicit anger but reminded them of anger, since the bears depicted in it were fighting. The last participant who did not mention anger instead expressed that they felt “tired”, likely due to it being the final image they were viewing over the entire study.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">Overall, a majority of participants experienced the intended emotion of each picture in the emotional session.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">In the neutral session, participants cited more varied emotions (8, 10, and 10 total emotions cited, respectively, for each neutral image). The most common feelings cited in the neutral session were “creative”, “energetic”, and “curious”.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">RESULTS</span>
</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Emotional Art Leads to Empathy and Vulnerability</span>
</h3>

<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.10710/assets/Figures/emo_reasoning.png" id="S5.F5.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="320" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">Emotional sessions</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2409.10710/assets/Figures/neutral_reasoning.png" id="S5.F5.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="318" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">Neutral sessions</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.3.2" class="ltx_text" style="font-size:90%;">Empathetic and visual reasoning between sessions</span></figcaption>
</figure>
<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Participant responses to question <math id="S5.SS1.p1.1.m1.1" class="ltx_math_unparsed" alttext="3)" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mrow id="S5.SS1.p1.1.m1.1b"><mn id="S5.SS1.p1.1.m1.1.1">3</mn><mo stretchy="false" id="S5.SS1.p1.1.m1.1.2">)</mo></mrow><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">3)</annotation></semantics></math> on why they felt a certain emotion after viewing an image were coded as containing either “empathetic reasoning”, “visual reasoning”, both, or neither. To investigate <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">H1</span>, counts were performed of how many instances each reasoning type (empathetic reasoning or visual reasoning) was used by each participant per session, and these counts are shown in Figure <a href="#S5.F5" title="Figure 5 ‣ V-A Emotional Art Leads to Empathy and Vulnerability ‣ V RESULTS ‣ A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection with Children" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">A distinct pattern quickly emerged, showing participants exhibited more empathetic reasoning when viewing emotional art, as opposed to exhibiting more visual reasoning when viewing neutral art. On average in the emotional sessions, participants used empathetic reasoning in 2.4 out of 3 images, while only using visual reasoning in 0.5 out of 3 images. On average in the neutral sessions, participants used empathetic reasoning in 0.4 out of 3 images while using visual reasoning in 2.5 out of 3 images.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">To compare between sessions, Wilcoxon signed-rank tests (non-parametric tests appropriate for small sample sizes) were performed. Significant differences were found, showing significantly more coded instances of empathetic reasoning in the emotional session and significantly more coded instances of visual reasoning in the neutral sessions (<math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="p&lt;.001" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mrow id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><mi id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml">p</mi><mo id="S5.SS1.p3.1.m1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.SS1.p3.1.m1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.3.cmml">.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><lt id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1"></lt><ci id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2">𝑝</ci><cn type="float" id="S5.SS1.p3.1.m1.1.1.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3">.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">p&lt;.001</annotation></semantics></math>), strongly supporting <span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold">H1</span>.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">Additionally, empathy felt by participants during the activity appeared to remain even past the end of the study. For example, during the post-activity interview, P-02 stated that their least favorite part of the activity was <span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_italic">“seeing the sad hedgehog with the dropped ice cream”</span>.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p">Participants also appreciated the chance to talk about their emotions, with P-13 stating <span id="S5.SS1.p5.1.1" class="ltx_text ltx_font_italic">“My favorite part of the activities today were the questions that most people wouldn’t have asked me, about your feelings…”</span> and P-10 responding that <span id="S5.SS1.p5.1.2" class="ltx_text ltx_font_italic">“Probably talking with Jibo and saying my emotions about the different artwork…”</span> was their favorite part of the activity.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Participant Verbosity</span>
</h3>

<figure id="S5.F6" class="ltx_figure"><img src="/html/2409.10710/assets/Figures/verbosity.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="301" height="199" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.3.2" class="ltx_text" style="font-size:90%;">Average words per participant</span></figcaption>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We noted a large difference in verbosity between the more verbose participants and the less verbose participants. Therefore, we divided the study sample into a more verbose group (<span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_bold">V+</span>) and a less verbose (<span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_bold">V-</span>) group, as distinguished in Figure <a href="#S5.F6" title="Figure 6 ‣ V-B Participant Verbosity ‣ V RESULTS ‣ A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection with Children" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Separating the study sample into <span id="S5.SS2.p1.1.3" class="ltx_text ltx_font_bold">V+</span> and <span id="S5.SS2.p1.1.4" class="ltx_text ltx_font_bold">V-</span> also helped to test hypothesis <span id="S5.SS2.p1.1.5" class="ltx_text ltx_font_bold">H3</span> on potential differences in behavior between open and closed participants.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Comparing verbosity, a Mann-Whitney U Test showed participants in the <span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">V+</span> group had significantly greater verbosity (average number of words spoken) than those in the <span id="S5.SS2.p2.1.2" class="ltx_text ltx_font_bold">V-</span> group (<math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="p&lt;0.01" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mrow id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mi id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml">p</mi><mo id="S5.SS2.p2.1.m1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><lt id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1.1"></lt><ci id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2">𝑝</ci><cn type="float" id="S5.SS2.p2.1.m1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">p&lt;0.01</annotation></semantics></math>). The <span id="S5.SS2.p2.1.3" class="ltx_text ltx_font_bold">V+</span> group spoke an average of 43.4 words per utterance with a standard deviation of 10.9 words, while the <span id="S5.SS2.p2.1.4" class="ltx_text ltx_font_bold">V-</span> group spoke an average of 10.3 words per utterance with a standard deviation of 6.7 words. The <span id="S5.SS2.p2.1.5" class="ltx_text ltx_font_bold">V+</span>/<span id="S5.SS2.p2.1.6" class="ltx_text ltx_font_bold">V-</span> split aligned with observations from video analysis of differences in noted extraversion and behavioral patterns. For example, participants in <span id="S5.SS2.p2.1.7" class="ltx_text ltx_font_bold">V+</span> tended to give more vulnerable and open responses compared to those in <span id="S5.SS2.p2.1.8" class="ltx_text ltx_font_bold">V-</span> (as noted by video annotators).</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Engagement and Discomfort</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Annotations from behavioral analysis demonstrated a positive average engagement in every session for every participant, even for sessions with participants who were mostly rated as displaying discomfort. During video analysis, annotators labeled events where participants appeared disengaged (-1) or engaged (1). Any portions of the video not marked as either were automatically classified as neutral behavior, with an engagement score of (0).</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2409.10710/assets/Figures/eng_verb.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="297" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S5.F7.3.2" class="ltx_text" style="font-size:90%;">Engagement per verbosity group and session type</span></figcaption>
</figure>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.4" class="ltx_p">On this scale, the <span id="S5.SS3.p2.4.1" class="ltx_text ltx_font_bold">V+</span> group had an average engagement score of 0.57 <math id="S5.SS3.p2.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS3.p2.1.m1.1a"><mo id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><csymbol cd="latexml" id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">\pm</annotation></semantics></math> 0.09 over emotional sessions and 0.41 <math id="S5.SS3.p2.2.m2.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS3.p2.2.m2.1a"><mo id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><csymbol cd="latexml" id="S5.SS3.p2.2.m2.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">\pm</annotation></semantics></math> 0.21 over neutral sessions, while the average engagement score of the <span id="S5.SS3.p2.4.2" class="ltx_text ltx_font_bold">V-</span> group was 0.20 <math id="S5.SS3.p2.3.m3.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS3.p2.3.m3.1a"><mo id="S5.SS3.p2.3.m3.1.1" xref="S5.SS3.p2.3.m3.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.3.m3.1b"><csymbol cd="latexml" id="S5.SS3.p2.3.m3.1.1.cmml" xref="S5.SS3.p2.3.m3.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.3.m3.1c">\pm</annotation></semantics></math> 0.09 over emotional sessions and 0.18 <math id="S5.SS3.p2.4.m4.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS3.p2.4.m4.1a"><mo id="S5.SS3.p2.4.m4.1.1" xref="S5.SS3.p2.4.m4.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.4.m4.1b"><csymbol cd="latexml" id="S5.SS3.p2.4.m4.1.1.cmml" xref="S5.SS3.p2.4.m4.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.4.m4.1c">\pm</annotation></semantics></math> 0.07 over neutral sessions, as shown in Figure <a href="#S5.F7" title="Figure 7 ‣ V-C Engagement and Discomfort ‣ V RESULTS ‣ A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection with Children" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">Though neither group saw a <span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_italic">significant</span> difference between engagement in emotional versus neutral sessions, we noted that both groups were on average more engaged in the emotional session, suggesting support for <span id="S5.SS3.p3.1.2" class="ltx_text ltx_font_bold">H2</span>. Both groups were also on average positively engaged in all sessions.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.2" class="ltx_p">A Mann-Whitney U Test was performed to examine differences in engagement by verbosity grouping. The <span id="S5.SS3.p4.2.1" class="ltx_text ltx_font_bold">V+</span> group’s engagement score is significantly higher (<math id="S5.SS3.p4.1.m1.1" class="ltx_Math" alttext="p&lt;0.01" display="inline"><semantics id="S5.SS3.p4.1.m1.1a"><mrow id="S5.SS3.p4.1.m1.1.1" xref="S5.SS3.p4.1.m1.1.1.cmml"><mi id="S5.SS3.p4.1.m1.1.1.2" xref="S5.SS3.p4.1.m1.1.1.2.cmml">p</mi><mo id="S5.SS3.p4.1.m1.1.1.1" xref="S5.SS3.p4.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.SS3.p4.1.m1.1.1.3" xref="S5.SS3.p4.1.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.1b"><apply id="S5.SS3.p4.1.m1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1"><lt id="S5.SS3.p4.1.m1.1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1.1"></lt><ci id="S5.SS3.p4.1.m1.1.1.2.cmml" xref="S5.SS3.p4.1.m1.1.1.2">𝑝</ci><cn type="float" id="S5.SS3.p4.1.m1.1.1.3.cmml" xref="S5.SS3.p4.1.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.1c">p&lt;0.01</annotation></semantics></math>) than the the <span id="S5.SS3.p4.2.2" class="ltx_text ltx_font_bold">V-</span> group score for average engagement in <span id="S5.SS3.p4.2.3" class="ltx_text ltx_font_italic">emotional</span> sessions. For neutral sessions, there was not a significant difference in engagement scores between groups (<math id="S5.SS3.p4.2.m2.1" class="ltx_Math" alttext="p&gt;0.05" display="inline"><semantics id="S5.SS3.p4.2.m2.1a"><mrow id="S5.SS3.p4.2.m2.1.1" xref="S5.SS3.p4.2.m2.1.1.cmml"><mi id="S5.SS3.p4.2.m2.1.1.2" xref="S5.SS3.p4.2.m2.1.1.2.cmml">p</mi><mo id="S5.SS3.p4.2.m2.1.1.1" xref="S5.SS3.p4.2.m2.1.1.1.cmml">&gt;</mo><mn id="S5.SS3.p4.2.m2.1.1.3" xref="S5.SS3.p4.2.m2.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.2.m2.1b"><apply id="S5.SS3.p4.2.m2.1.1.cmml" xref="S5.SS3.p4.2.m2.1.1"><gt id="S5.SS3.p4.2.m2.1.1.1.cmml" xref="S5.SS3.p4.2.m2.1.1.1"></gt><ci id="S5.SS3.p4.2.m2.1.1.2.cmml" xref="S5.SS3.p4.2.m2.1.1.2">𝑝</ci><cn type="float" id="S5.SS3.p4.2.m2.1.1.3.cmml" xref="S5.SS3.p4.2.m2.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.2.m2.1c">p&gt;0.05</annotation></semantics></math>).</p>
</div>
<div id="S5.SS3.p5" class="ltx_para">
<p id="S5.SS3.p5.1" class="ltx_p">We also considered that the novelty effect <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> could artificially inflate ratings for engagement, as participants who had never seen Jibo may have been excited to interact with a new stimulus and be more engaged than they would be with a familiar stimulus. However, more than half of the participants were already familiar with Jibo due to the nature of recruiting—participants familiar with Jibo were P-02, P-05, P-08, P-09, P-10, and P-11. Four of these participants were in the <span id="S5.SS3.p5.1.1" class="ltx_text ltx_font_bold">V+</span> group, and two were in the <span id="S5.SS3.p5.1.2" class="ltx_text ltx_font_bold">V-</span> group.</p>
</div>
<div id="S5.SS3.p6" class="ltx_para">
<p id="S5.SS3.p6.4" class="ltx_p">The average engagement score of participants who had previous experience with Jibo was 0.46 <math id="S5.SS3.p6.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS3.p6.1.m1.1a"><mo id="S5.SS3.p6.1.m1.1.1" xref="S5.SS3.p6.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.1.m1.1b"><csymbol cd="latexml" id="S5.SS3.p6.1.m1.1.1.cmml" xref="S5.SS3.p6.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.1.m1.1c">\pm</annotation></semantics></math> 0.22 in the emotional sessions and 0.40 <math id="S5.SS3.p6.2.m2.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS3.p6.2.m2.1a"><mo id="S5.SS3.p6.2.m2.1.1" xref="S5.SS3.p6.2.m2.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.2.m2.1b"><csymbol cd="latexml" id="S5.SS3.p6.2.m2.1.1.cmml" xref="S5.SS3.p6.2.m2.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.2.m2.1c">\pm</annotation></semantics></math> 0.18 in the neutral sessions, while the average engagement score of participants who were unfamiliar with Jibo was 0.34 <math id="S5.SS3.p6.3.m3.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS3.p6.3.m3.1a"><mo id="S5.SS3.p6.3.m3.1.1" xref="S5.SS3.p6.3.m3.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.3.m3.1b"><csymbol cd="latexml" id="S5.SS3.p6.3.m3.1.1.cmml" xref="S5.SS3.p6.3.m3.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.3.m3.1c">\pm</annotation></semantics></math> 0.19 in the emotional sessions and 0.19 <math id="S5.SS3.p6.4.m4.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S5.SS3.p6.4.m4.1a"><mo id="S5.SS3.p6.4.m4.1.1" xref="S5.SS3.p6.4.m4.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p6.4.m4.1b"><csymbol cd="latexml" id="S5.SS3.p6.4.m4.1.1.cmml" xref="S5.SS3.p6.4.m4.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p6.4.m4.1c">\pm</annotation></semantics></math> 0.16 in the neutral sessions. Participants who had never interacted with Jibo before were actually less engaged, on average, than participants who had previous familiarity with Jibo. Therefore, the novelty effect did not seem to have an effect on artificially increasing engagement.</p>
</div>
<div id="S5.SS3.p7" class="ltx_para">
<p id="S5.SS3.p7.1" class="ltx_p">Through qualitative analysis of participant responses and behavioral analysis, we observed that <span id="S5.SS3.p7.1.1" class="ltx_text ltx_font_bold">participant engagement remained high</span> even during moments <span id="S5.SS3.p7.1.2" class="ltx_text ltx_font_bold">when participants experienced discomfort</span> while sharing their feelings.</p>
</div>
<div id="S5.SS3.p8" class="ltx_para">
<p id="S5.SS3.p8.1" class="ltx_p">An example of a story that caused discomfort in a participant is from P-07, who recalled: <span id="S5.SS3.p8.1.1" class="ltx_text ltx_font_italic">“… I felt so bad, I almost started crying. Just like this porcupine. But I didn’t cry because men don’t cry. And I didn’t want to do it. I attract too much</span> [sic] <span id="S5.SS3.p8.1.2" class="ltx_text ltx_font_italic">people looking at me and staring and calling out so I just had to live with it.”</span> When sharing this emotional memory, P-07 exhibited behavior expressing discomfort (marked by annotators), like fidgeting and reduced eye contact. Despite their discomfort, they were sharing vulnerable feelings and engaging deeply with the interaction.</p>
</div>
<div id="S5.SS3.p9" class="ltx_para">
<p id="S5.SS3.p9.1" class="ltx_p">Another example was seen when examining utterances by P-02. They shared: <span id="S5.SS3.p9.1.1" class="ltx_text ltx_font_italic">“Okay, so the first day of school was a train wreck… I was like, how am I supposed to keep this up? For a year! I can’t even keep it up for like a day. Because, like, when I got home, I was very mad. I was very annoyed of what would happen that day</span> [sic]<span id="S5.SS3.p9.1.2" class="ltx_text ltx_font_italic">. And I was very sad. I was eating ice cream in my blankets on my bed.”</span> When sharing this anecdote, P-02 was noted by video annotators to exhibit behaviors suggesting discomfort such as nervous smiling, tense shoulders, and fidgeting. However, P-02 was also clearly engaging with the activity by sharing emotions and memories that were honest and vulnerable.</p>
</div>
<div id="S5.SS3.p10" class="ltx_para">
<p id="S5.SS3.p10.1" class="ltx_p">Additionally, in the post-activity interview, participants acknowledged that parts of the activity made them uncomfortable, with P-10 remarking <span id="S5.SS3.p10.1.1" class="ltx_text ltx_font_italic">“I felt a little uncomfortable talking with a robot but I also felt really excited to talk with Jibo,”</span> and P-09 sharing <span id="S5.SS3.p10.1.2" class="ltx_text ltx_font_italic">“I was feeling, like, a little shy.”</span></p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.4.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.5.2" class="ltx_text ltx_font_italic">Social Robot Mitigates Discomfort</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Video annotators noted across both session types that when participants gave a response that was particularly vulnerable, honest, or open (as noted by annotators in the Openness measurement), they would often exhibit discomfort. This discomfort would continue until Jibo responded to them with either an affirmative reassurance or compliment, whereupon participants’ discomfort would be reduced.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">For example, after sharing a particularly vulnerable memory, P-07 recalled that they were <span id="S5.SS4.p2.1.1" class="ltx_text ltx_font_italic">“…Feeling like I don’t really belong here, and why the hell am I even doing this?”</span> and immediately began exhibiting uncomfortable behaviors like flitting their eyes around the area and intense fidgeting. Once Jibo reassured P-07 by saying <span id="S5.SS4.p2.1.2" class="ltx_text ltx_font_italic">“I can see how this piece of art triggered some unique memories for you!”</span>, P-07 settled down and began making eye contact with Jibo again.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">This pattern occurred in both the <span id="S5.SS4.p3.1.1" class="ltx_text ltx_font_bold">V+</span> and <span id="S5.SS4.p3.1.2" class="ltx_text ltx_font_bold">V-</span> groups, but the <span id="S5.SS4.p3.1.3" class="ltx_text ltx_font_bold">V+</span> group shared more vulnerable, honest, or open answers than the <span id="S5.SS4.p3.1.4" class="ltx_text ltx_font_bold">V-</span> group. In total, the <span id="S5.SS4.p3.1.5" class="ltx_text ltx_font_bold">V+</span> group gave 97 total vulnerable, honest, or open answers (60 in emotional sessions and 37 in neutral sessions) and the <span id="S5.SS4.p3.1.6" class="ltx_text ltx_font_bold">V-</span> group only gave 14 (10 in emotional sessions and 4 in neutral sessions).</p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.1" class="ltx_p">A Mann-Whitney U Test was performed to test for significant differences between the number of vulnerable answers shared in the <span id="S5.SS4.p4.1.1" class="ltx_text ltx_font_bold">V+</span> group versus the <span id="S5.SS4.p4.1.2" class="ltx_text ltx_font_bold">V-</span> group. The <span id="S5.SS4.p4.1.3" class="ltx_text ltx_font_bold">V+</span> group shared significantly more vulnerable answers than the <span id="S5.SS4.p4.1.4" class="ltx_text ltx_font_bold">V-</span> group (<math id="S5.SS4.p4.1.m1.1" class="ltx_Math" alttext="p&lt;0.01" display="inline"><semantics id="S5.SS4.p4.1.m1.1a"><mrow id="S5.SS4.p4.1.m1.1.1" xref="S5.SS4.p4.1.m1.1.1.cmml"><mi id="S5.SS4.p4.1.m1.1.1.2" xref="S5.SS4.p4.1.m1.1.1.2.cmml">p</mi><mo id="S5.SS4.p4.1.m1.1.1.1" xref="S5.SS4.p4.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.SS4.p4.1.m1.1.1.3" xref="S5.SS4.p4.1.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p4.1.m1.1b"><apply id="S5.SS4.p4.1.m1.1.1.cmml" xref="S5.SS4.p4.1.m1.1.1"><lt id="S5.SS4.p4.1.m1.1.1.1.cmml" xref="S5.SS4.p4.1.m1.1.1.1"></lt><ci id="S5.SS4.p4.1.m1.1.1.2.cmml" xref="S5.SS4.p4.1.m1.1.1.2">𝑝</ci><cn type="float" id="S5.SS4.p4.1.m1.1.1.3.cmml" xref="S5.SS4.p4.1.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p4.1.m1.1c">p&lt;0.01</annotation></semantics></math>) in both sessions. This finding supports <span id="S5.SS4.p4.1.5" class="ltx_text ltx_font_bold">H3</span> for emotional art, as the <span id="S5.SS4.p4.1.6" class="ltx_text ltx_font_bold">V+</span> group was shown to both share significantly more vulnerable feelings and be significantly more engaged than the <span id="S5.SS4.p4.1.7" class="ltx_text ltx_font_bold">V-</span> group in emotional sessions.</p>
</div>
<div id="S5.SS4.p5" class="ltx_para">
<p id="S5.SS4.p5.1" class="ltx_p">Of 111 total vulnerable answers across all groups, 94 answers (or 84.7% of answers) were followed by a Jibo response that reduced the participant’s discomfort noticeably (as noted by the video annotators), while 15.3% of vulnerable answers followed by a Jibo response led to discomfort that either remained the same or increased.</p>
</div>
<div id="S5.SS4.p6" class="ltx_para">
<p id="S5.SS4.p6.1" class="ltx_p">When looking at the 97 vulnerable answers given by the <span id="S5.SS4.p6.1.1" class="ltx_text ltx_font_bold">V+</span> group, 83 (85.6%) were followed by a Jibo utterance that visibly reduced the participant’s discomfort. Though the <span id="S5.SS4.p6.1.2" class="ltx_text ltx_font_bold">V+</span> group provided more vulnerable answers, the proportion of times their discomfort was reduced by Jibo (after sharing vulnerably) was higher than that of the entire population.</p>
</div>
<div id="S5.SS4.p7" class="ltx_para">
<p id="S5.SS4.p7.1" class="ltx_p">Additionally, with 97 vulnerable answers across 6 members, the <span id="S5.SS4.p7.1.1" class="ltx_text ltx_font_bold">V+</span> group produced an average of 16.17 vulnerable answers per participant. Each participant was only asked 24 questions in total, leading to <span id="S5.SS4.p7.1.2" class="ltx_text ltx_font_bold">V+</span> group members sharing vulnerably in over two-thirds of their interactions with Jibo.</p>
</div>
<div id="S5.SS4.p8" class="ltx_para">
<p id="S5.SS4.p8.1" class="ltx_p">Examining the <span id="S5.SS4.p8.1.1" class="ltx_text ltx_font_bold">V+</span> group further, in every session, participants’ discomfort post-vulnerable response was reduced more often than not after Jibo’s next utterance. There was no significant difference in the proportion of vulnerable answers that led to reduced discomfort between emotional and neutral sessions, though participants in general tended to share more vulnerable, honest, or open answers during the emotional sessions (70 answers total, 60 from <span id="S5.SS4.p8.1.2" class="ltx_text ltx_font_bold">V+</span>) compared to the neutral sessions (41 answers total, 37 from <span id="S5.SS4.p8.1.3" class="ltx_text ltx_font_bold">V+</span>).</p>
</div>
<div id="S5.SS4.p9" class="ltx_para">
<p id="S5.SS4.p9.1" class="ltx_p">Qualitative analysis of the post-interaction interview showed that participants enjoyed Jibo’s responses and particularly appreciated that Jibo appeared to actively listen to them. P-07 remarked <span id="S5.SS4.p9.1.1" class="ltx_text ltx_font_italic">“I liked that when I say something, he really takes the time to think and he gives something corresponding to what I said”</span>, P-02 shared that <span id="S5.SS4.p9.1.2" class="ltx_text ltx_font_italic">“It was very fun to talk with something that was not human but also could probably hear me”</span>, and P-06 stated <span id="S5.SS4.p9.1.3" class="ltx_text ltx_font_italic">“I liked how Jibo responded to me and I think it understanded</span> [sic] <span id="S5.SS4.p9.1.4" class="ltx_text ltx_font_italic">me.”</span></p>
</div>
<div id="S5.SS4.p10" class="ltx_para">
<p id="S5.SS4.p10.1" class="ltx_p">Fisher’s Exact test was used to compare instances where Jibo’s responses did or did not reduce participant discomfort after vulnerable utterances. Jibo’s reassuring utterances significantly improved (<math id="S5.SS4.p10.1.m1.1" class="ltx_Math" alttext="p&lt;&lt;.001" display="inline"><semantics id="S5.SS4.p10.1.m1.1a"><mrow id="S5.SS4.p10.1.m1.1.1" xref="S5.SS4.p10.1.m1.1.1.cmml"><mi id="S5.SS4.p10.1.m1.1.1.2" xref="S5.SS4.p10.1.m1.1.1.2.cmml">p</mi><mo id="S5.SS4.p10.1.m1.1.1.1" xref="S5.SS4.p10.1.m1.1.1.1.cmml">&lt;&lt;</mo><mn id="S5.SS4.p10.1.m1.1.1.3" xref="S5.SS4.p10.1.m1.1.1.3.cmml">.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p10.1.m1.1b"><apply id="S5.SS4.p10.1.m1.1.1.cmml" xref="S5.SS4.p10.1.m1.1.1"><csymbol cd="latexml" id="S5.SS4.p10.1.m1.1.1.1.cmml" xref="S5.SS4.p10.1.m1.1.1.1">much-less-than</csymbol><ci id="S5.SS4.p10.1.m1.1.1.2.cmml" xref="S5.SS4.p10.1.m1.1.1.2">𝑝</ci><cn type="float" id="S5.SS4.p10.1.m1.1.1.3.cmml" xref="S5.SS4.p10.1.m1.1.1.3">.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p10.1.m1.1c">p&lt;&lt;.001</annotation></semantics></math>) participant comfort after they were vulnerable, honest, and open.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">DISCUSSION</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Art is a well-explored vehicle for helping children learn and practice SEL competencies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, though it has been minimally explored in tandem with social robots. Cooney et al. have examined social robots for art therapy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, and curricula have been designed to promote SEL skills in children when observing and reflecting on art <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. This study sought to expand on these prior works through a demonstration of scaffolding emotional conversations about art by using social robots for children to interactively develop and practice SEL competencies.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Conversations about emotional art scaffolded by social robots can foster empathy in children.</span> Significantly higher rates of empathetic reasoning were exhibited in emotional art versus neutral art sessions, strongly supporting <span id="S6.p2.1.2" class="ltx_text ltx_font_bold">H1</span>. Observing and reflecting on emotional art with Jibo promoted emotional transfer and empathetic thinking, which help build empathy as an SEL competency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The presence of Jibo and the interactive conversation facilitated by Jibo appeared to promote empathetic connection with the participant and supports the use of social robots to scaffold activities for building SEL skills. Scaffolding activities to reflect on art allowed for richer reflection on emotions observed in the art and how those emotions might connect to the participant’s life. These results expand the evidence base on using social robots for teaching emotion recognition and empathy, not just for neurodivergent children <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, but also to provide skill-building for neurotypical children.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p"><span id="S6.p3.1.1" class="ltx_text ltx_font_bold">Children are highly engaged in social robot-driven SEL practice, even when sharing vulnerable reflections and potentially experiencing discomfort.</span> On average, every participant was rated as having positive engagement (raters noted strong eye contact and deep, thoughtful contributions to the interaction) across sessions. This result demonstrated that interactions with Jibo successfully held children’s interest and is consistent with previous findings on how social robots can promote user engagement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Furthermore, participants who had previously interacted with Jibo had higher average engagement scores than those without prior experience, suggesting that engagement was not due to the novelty effect <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. This higher engagement from participants with past experience may suggest a self-selection bias in study participation but also points to the potential for leveraging social robots for longitudinal SEL programming, where continuous interaction may lead to higher engagement. From qualitative analysis of utterances, participant engagement remained high even during moments of discomfort, suggesting that participants felt it was a safe space to feel the discomfort that arises from vulnerability and could continue to engage with the robot. Results support <span id="S6.p3.1.2" class="ltx_text ltx_font_bold">H2</span> for the <span id="S6.p3.1.3" class="ltx_text ltx_font_bold">V+</span> group, as participants’ average engagement levels in the emotional sessions were higher than in neutral sessions. However, the difference was not statistically significant, and more data is needed to reach a conclusive result. Results also support <span id="S6.p3.1.4" class="ltx_text ltx_font_bold">H3</span> for emotional art, as the <span id="S6.p3.1.5" class="ltx_text ltx_font_bold">V+</span> group, who were significantly more open, had significantly higher levels of engagement than the <span id="S6.p3.1.6" class="ltx_text ltx_font_bold">V-</span> group in the emotional session.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p"><span id="S6.p4.1.1" class="ltx_text ltx_font_bold">A social robot can help mitigate the discomfort a child feels when sharing vulnerable feelings.</span> Discomfort that arose during and after participants shared vulnerable feelings decreased significantly after the robot offered a reassuring response. These findings demonstrated that interacting with Jibo was a comforting experience, consistent with previous findings showing that social robots can reduce children’s anxiety and promote comfort and disclosure <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Additionally, <span id="S6.p4.1.2" class="ltx_text ltx_font_bold">V+</span> group members on average shared deeply and openly 16.17 times out of 24 utterances total (approximately two-thirds of utterances). Participants appeared to share deeply as they felt comfortable around Jibo due to how his responses were personalized to their utterances, which may have helped the participants feel listened to and cared for. One change that could help the <span id="S6.p4.1.3" class="ltx_text ltx_font_bold">V-</span> group share more is using the robot to detect when further questioning is helpful—for example, when a participant said “I don’t know”, Jibo would move on and ask the participant what was confusing. The participant may have been able to share more if Jibo had instead prompted them to think again about their feelings.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">This study was limited by a small sample size, and future works will expand to a larger, more diverse population to validate findings. Behavioral analysis also suggested that a laboratory setting may have inadvertently heightened discomfort, as participants disclosed vulnerable information amidst strangers in an unfamiliar place. Conducting future studies in familiar areas could encourage more open and vulnerable responses. Future research would also benefit from exploring longitudinal interactions to better understand changes over time in participants’ SEL skills. This study only investigated reactions to three pieces of art representing three emotions. However, future research should include a broader spectrum of art and a wider range of emotions to facilitate more comprehensive exploration of emotional responses.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">CONCLUSION</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We explored how social robots can foster social-emotional learning (SEL) competencies in children through conversations about art. Our investigation involved 11 participants who engaged in two sessions discussing emotional and neutral artworks, facilitated by social robot scaffolding.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">Findings demonstrated that discussing emotional art with a social robot is an effective method for emotional self-awareness and empathy (key SEL skills). Reflecting on art prompted children to engage deeply and thoughtfully with the social robot, and it was able to alleviate discomfort to encourage continued engagement and emotional exploration.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">This work demonstrated a promising method of fostering social-emotional learning in children and provides an initial foundation for future inclusive, expansive, and longitudinal studies to validate and expand the capabilities for robot- and art-mediated interactions for building SEL skills.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
M. J. Elias, M. Elias, J. E. Zins, and R. P. Weissberg, <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Promoting social and emotional learning: Guidelines for educators</em>.   Ascd, 1997.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Skoog-Hoffman, C. Ackerman, A. Boyle, H. Schwartz, B. Williams, R. Jagers, L. Dusenbury, M. Greenberg, J. Mahoney, K. Schonert-Reichl, <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Evidence-based social and emotional learning programs: Casel criteria updates and rationale,” <em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">Retrieved February</em>, vol. 6, p. 2023, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
M. T. Greenberg, C. E. Domitrovich, R. P. Weissberg, and J. A. Durlak, “Social and emotional learning as a public health approach to education,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">The future of children</em>, pp. 13–32, 2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
C. F. Mondi, A. Giovanelli, and A. J. Reynolds, “Fostering socio-emotional learning through early childhood intervention,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International Journal of Child Care and Education Policy</em>, vol. 15, no. 1, pp. 1–43, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. D. Hawkins, R. Kosterman, R. F. Catalano, K. G. Hill, and R. D. Abbott, “Effects of social development intervention in childhood 15 years later,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Archives of pediatrics &amp; adolescent medicine</em>, vol. 162, no. 12, pp. 1133–1141, 2008.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
R. D. Taylor, E. Oberle, J. A. Durlak, and R. P. Weissberg, “Promoting Positive Youth Development Through School-Based Social and Emotional Learning Interventions: A Meta-Analysis of Follow-Up Effects,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Child Development</em>, vol. 88, no. 4, pp. 1156–1171, 2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J. A. Durlak, R. P. Weissberg, A. B. Dymnicki, R. D. Taylor, and K. B. Schellinger, “The Impact of Enhancing Students’ Social and Emotional Learning: A Meta-Analysis of School-Based Universal Interventions,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Child Development</em>, vol. 82, no. 1, pp. 405–432, 2011.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. Bridgeland, M. Bruce, and A. Hariharan, “The missing piece: A national teacher survey on how social and emotional learning can empower children and transform schools. a report for casel.” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Civic Enterprises</em>, 2013.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Aspen Institute National Commission on Social, Emotional, and Academic Development, “From a nation at risk to a nation at hope: Recommendations from the national commission on social, emotional, &amp; academic development.”   Aspen Institute Washington, DC, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
T. Kanda, M. Shimada, and S. Koizumi, “Children learning with a social robot,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 7th annual ACM/IEEE international conference on Human-Robot Interaction</em>, 2012, pp. 351–358.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. L. Thomaz, M. Cakmak, and K. Clark, “Active social learning in humans and robots,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Social learning theory: Phylogenetic considerations across animal, plant, and microbial taxa</em>, pp. 113–28, 2013.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. Kanero, V. Geçkin, C. Oranç, E. Mamus, A. C. Küntay, and T. Göksun, “Social robots for early language learning: Current evidence and future directions,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Child Development Perspectives</em>, vol. 12, no. 3, pp. 146–151, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
R. Van den Berghe, J. Verhagen, O. Oudgenoeg-Paz, S. Van der Ven, and P. Leseman, “Social robots for language learning: A review,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Review of Educational Research</em>, vol. 89, no. 2, pp. 259–295, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. M. Neumann, “Social robots and young children’s early language and literacy learning,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Early Childhood Education Journal</em>, vol. 48, no. 2, pp. 157–170, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
H. Chen, H. W. Park, and C. Breazeal, “Teaching and learning with children: Impact of reciprocal peer learning with a social robot on children’s learning and emotive engagement,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Computers &amp; Education</em>, vol. 150, p. 103836, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
R. Williams, “Popbots: leveraging social robots to aid preschool children’s artificial intelligence education,” Ph.D. dissertation, Massachusetts Institute of Technology, 2018.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
M. Rafique, M. A. Hassan, A. Jaleel, H. Khalid, and G. Bano, “A computation model for learning programming and emotional intelligence,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 8, pp. 149 616–149 629, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J. M. K. Westlund, L. Dickens, S. Jeong, P. L. Harris, D. DeSteno, and C. L. Breazeal, “Children use non-verbal cues to learn new words from robots as well as people,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">International Journal of Child-Computer Interaction</em>, vol. 13, pp. 1–9, 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
M. Fridin, “Storytelling by a kindergarten social assistive robot: A tool for constructive learning in preschool education,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Computers &amp; education</em>, vol. 70, pp. 53–64, 2014.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
J. A. Dosso, J. N. Kailley, S. E. Martin, and J. M. Robillard, ““a safe space for sharing feelings”: perspectives of children with lived experiences of anxiety on social robots,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Multimodal Technologies and Interaction</em>, vol. 7, no. 12, p. 118, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
E. Pashevich, “Can communication with social robots influence how children develop empathy? best-evidence synthesis,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">AI &amp; SOCIETY</em>, vol. 37, no. 2, pp. 579–589, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
M. Spitale, S. Okamoto, M. Gupta, H. Xi, and M. J. Matarić, “Socially assistive robots as storytellers that elicit empathy,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Human-Robot Interaction (THRI)</em>, vol. 11, no. 4, pp. 1–29, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
N. Hurst, C. Clabaugh, R. Baynes, J. Cohn, D. Mitroff, and S. Scherer, “Social and emotional skills training with embodied moxie,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.12962</em>, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
S. Kewalramani, K.-A. Allen, E. Leif, and A. Ng, “A scoping review of the use of robotics technologies for supporting social-emotional learning in children with autism,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Journal of Autism and Developmental Disorders</em>, pp. 1–15, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
F. Marino, P. Chilà, S. T. Sfrazzetto, C. Carrozza, I. Crimi, C. Failla, M. Busà, G. Bernava, G. Tartarisco, D. Vagni, <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Outcomes of a robot-assisted social-emotional understanding intervention for young children with autism spectrum disorders,” <em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic">Journal of autism and developmental disorders</em>, vol. 50, pp. 1973–1987, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
S.-S. Yun, J. Choi, S.-K. Park, G.-Y. Bong, and H. Yoo, “Social skills training for children with autism spectrum disorder using a robotic behavioral intervention system,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Autism Research</em>, vol. 10, no. 7, pp. 1306–1323, 2017.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
E. Wolfe, J. Weinberg, and S. Hupp, “Deploying a social robot to co-teach social emotional learning in the early childhood classroom,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 13th Annual ACM/IEEE International Conference on Human–Robot Interaction, Chicago, IL, USA</em>, 2018, pp. 5–8.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
E. W. Eisner, <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">The arts and the creation of mind</em>.   Yale University Press, 2002.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
L. Brouillette, “How the arts help children to create healthy social scripts: Exploring the perceptions of elementary teachers,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Arts Education Policy Review</em>, vol. 111, no. 1, pp. 16–24, 2009.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
C. A. Farrington, J. Maurer, M. R. A. McBride, J. Nagaoka, J. Puller, S. Shewfelt, E. M. Weiss, and L. Wright, “Arts education and social-emotional learning outcomes among k-12 students: Developing a theory of action.” <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">University of Chicago Consortium on School Research</em>, 2019.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
M. D. Cooney and M. L. R. Menezes, “Design for an art therapy robot: An explorative review of the theoretical foundations for engaging in emotional and creative painting with a robot,” <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Multimodal Technologies and Interaction</em>, vol. 2, no. 3, p. 52, 2018.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
M. Cooney, “Robot art, in the eye of the beholder?: Personalized metaphors facilitate communication of emotions and creativity,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Frontiers in Robotics and AI</em>, vol. 8, p. 668986, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
S. B. Heath, E. Soep, and A. Roach, <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Living the arts through language+ learning: A report on community-based youth organizations</em>.   Americans for the Arts, 1998.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
M. Eddy, C. Blatt-Gross, S. N. Edgar, A. Gohr, E. Halverson, K. Humphreys, and L. Smolin, “Local-level implementation of social emotional learning in arts education: Moving the heart through the arts,” <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Arts Education Policy Review</em>, vol. 122, no. 3, pp. 193–204, 2021.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
M. Ebert, J. D. Hoffmann, Z. Ivcevic, C. Phan, and M. A. Brackett, “Teaching emotion and creativity skills through art: A workshop for children,” <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">The International Journal of Creativity &amp; Problem Solving</em>, vol. 25, no. 2, pp. 23–35, 2015.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
“Social and emotional learning through art,” Online Curriculum, The Metropolitan Museum of Art, New York, New York, NY, USA, 2022.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
J. Inc., “Jibo,” https://jibo.com, [Online; accessed 01-August-2023].

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
J. R. Cordero, T. R. Groechel, and M. J. Matarić, “A review and recommendations on reporting recruitment and compensation information in hri research papers,” in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)</em>.   IEEE, 2022, pp. 1627–1633.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
M. L. McHugh, “Interrater reliability: the kappa statistic,” <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Biochemia medica</em>, vol. 22, no. 3, pp. 276–282, 2012.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
E. Shove and D. Southerton, “Defrosting the freezer: From novelty to convenience: A narrative of normalization,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Journal of Material Culture</em>, vol. 5, no. 3, pp. 301–319, 2000.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
E. M. Rogers, A. Singhal, and M. M. Quinlan, “Diffusion of innovations,” in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">An integrated approach to communication theory and research</em>.   Routledge, 2014, pp. 432–448.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.10709" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.10710" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.10710">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.10710" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.10711" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 18:37:47 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
