<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.09802] Hear Your Face: Face-based voice conversion with F0 estimation</title><meta property="og:description" content="This paper delves into the emerging field of face-based voice conversion, leveraging the unique relationship between an individual's facial features and their vocal characteristics.
We present a novel face-based voice â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hear Your Face: Face-based voice conversion with F0 estimation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Hear Your Face: Face-based voice conversion with F0 estimation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.09802">

<!--Generated on Thu Sep  5 16:13:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=1]JaejunLee
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=1]YooriOh
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=1]InjuneHwang
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>[affiliation=1,2,3]KyoguLee





</p>
</div>
<h1 class="ltx_title ltx_title_document">Hear Your Face: Face-based voice conversion with F0 estimation</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">This paper delves into the emerging field of face-based voice conversion, leveraging the unique relationship between an individual's facial features and their vocal characteristics.
We present a novel face-based voice conversion framework that particularly utilizes the average fundamental frequency of the target speaker, derived solely from their facial images.
Through extensive analysis, our framework demonstrates superior speech generation quality and the ability to align facial features with voice characteristics, including tracking of the target speaker's fundamental frequency.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>voice conversion, face/voice association, cross modal generation, speaker embedding
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Voice, the cornerstone of human speech, plays a crucial role in interpersonal communication. Beyond its communicative function, voice is a distinctive feature of an individual, reflecting personal identity.
Consequently, individuals who are unable to produce sound not only face a significant barrier to communication but also experience a loss of personal expression.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Conventional speech synthesis techniques, such as Text-to-SpeechÂ (TTS) and Voice ConversionÂ (VC), have made significant strides in emulating a target voice while retaining the non-verbal content elements. Yet, these techniques predominantly rely on the availability of the target voice's acoustic data to replicate its unique speech style effectively.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The human face represents another intrinsic aspect of individual identity, containing details such as biological gender, ethnicity, and age. More than just exploring visual information from face, recent studies have increasingly focused on understanding the relationship between facial features and vocal attributes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. This field of study may hold the key to a new form of speech synthesis, one that retains the target speaker's identity even in the absence of vocal information.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Recent advancements in face-based speech synthesis have experienced a notable surge, particularly through the integration of conventional TTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and VC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> techniques. While this growing interest and development show a promising view, the field remains in its formative phases. Specifically, identifying a `voice that matches the face' presents significant challenges, and metrics for evaluating it also remain a pivotal question.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.2" class="ltx_p">The fundamental frequency (<math id="S1.p5.1.m1.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S1.p5.1.m1.1a"><mrow id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml"><mi id="S1.p5.1.m1.1.1.2" xref="S1.p5.1.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S1.p5.1.m1.1.1.1" xref="S1.p5.1.m1.1.1.1.cmml">â€‹</mo><mn id="S1.p5.1.m1.1.1.3" xref="S1.p5.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><apply id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1"><times id="S1.p5.1.m1.1.1.1.cmml" xref="S1.p5.1.m1.1.1.1"></times><ci id="S1.p5.1.m1.1.1.2.cmml" xref="S1.p5.1.m1.1.1.2">ğ¹</ci><cn type="integer" id="S1.p5.1.m1.1.1.3.cmml" xref="S1.p5.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">F0</annotation></semantics></math>), one of the key components in voice conversion process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, not only serves as a pitch information of speech but also has an aspect of containing information of speakers identification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
It has been found that facial features have correlation with voice pitch information, even in cases where gender is controlled <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
It implies that voice pitch information, indicated by the <math id="S1.p5.2.m2.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S1.p5.2.m2.1a"><mrow id="S1.p5.2.m2.1.1" xref="S1.p5.2.m2.1.1.cmml"><mi id="S1.p5.2.m2.1.1.2" xref="S1.p5.2.m2.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S1.p5.2.m2.1.1.1" xref="S1.p5.2.m2.1.1.1.cmml">â€‹</mo><mn id="S1.p5.2.m2.1.1.3" xref="S1.p5.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p5.2.m2.1b"><apply id="S1.p5.2.m2.1.1.cmml" xref="S1.p5.2.m2.1.1"><times id="S1.p5.2.m2.1.1.1.cmml" xref="S1.p5.2.m2.1.1.1"></times><ci id="S1.p5.2.m2.1.1.2.cmml" xref="S1.p5.2.m2.1.1.2">ğ¹</ci><cn type="integer" id="S1.p5.2.m2.1.1.3.cmml" xref="S1.p5.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.2.m2.1c">F0</annotation></semantics></math>, could be derived from the speaker's facial images, and it is not merely from basic gender identification, but also from further biological associative information.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In this study, we propose a novel framework for face-based speech synthesis, focusing particularly on the voice conversion that imprints a face-based target voice's characteristics onto the original source audio. Our framework specifically utilize the <math id="S1.p6.1.m1.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S1.p6.1.m1.1a"><mrow id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml"><mi id="S1.p6.1.m1.1.1.2" xref="S1.p6.1.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S1.p6.1.m1.1.1.1" xref="S1.p6.1.m1.1.1.1.cmml">â€‹</mo><mn id="S1.p6.1.m1.1.1.3" xref="S1.p6.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><apply id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1"><times id="S1.p6.1.m1.1.1.1.cmml" xref="S1.p6.1.m1.1.1.1"></times><ci id="S1.p6.1.m1.1.1.2.cmml" xref="S1.p6.1.m1.1.1.2">ğ¹</ci><cn type="integer" id="S1.p6.1.m1.1.1.3.cmml" xref="S1.p6.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">F0</annotation></semantics></math> of target speaker, derived solely from facial images. This approach aims to enhance the face-based voice conversion process, generating speech that is well aligned with the target individual's vocal identity without using any acoustic data of the target speaker.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">To contextualize our research, we delineate our contributions as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We present a framework that sets a new benchmark in performance for face-based voice conversion, demonstrating state-of-the-art results.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose a novel approach for speech synthesis by estimating the <math id="S1.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S1.I1.i2.p1.1.m1.1a"><mrow id="S1.I1.i2.p1.1.m1.1.1" xref="S1.I1.i2.p1.1.m1.1.1.cmml"><mi id="S1.I1.i2.p1.1.m1.1.1.2" xref="S1.I1.i2.p1.1.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S1.I1.i2.p1.1.m1.1.1.1" xref="S1.I1.i2.p1.1.m1.1.1.1.cmml">â€‹</mo><mn id="S1.I1.i2.p1.1.m1.1.1.3" xref="S1.I1.i2.p1.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.1.m1.1b"><apply id="S1.I1.i2.p1.1.m1.1.1.cmml" xref="S1.I1.i2.p1.1.m1.1.1"><times id="S1.I1.i2.p1.1.m1.1.1.1.cmml" xref="S1.I1.i2.p1.1.m1.1.1.1"></times><ci id="S1.I1.i2.p1.1.m1.1.1.2.cmml" xref="S1.I1.i2.p1.1.m1.1.1.2">ğ¹</ci><cn type="integer" id="S1.I1.i2.p1.1.m1.1.1.3.cmml" xref="S1.I1.i2.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.1.m1.1c">F0</annotation></semantics></math> of the target speaker through their facial images.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Through extensive analysis and the introduction of a novel evaluation metric, we demonstrate that our framework not only produces high-quality synthetic speech but also suggests that the synthesized voice aligns reasonably well with the corresponding facial image.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">The demo is available on the link, <a target="_blank" href="https://jaejunL.github.io/HYFace_Demo/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://jaejunL.github.io/HYFace_Demo/</a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Voice conversion</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Voice conversion, a specialized subset of speech synthesis, is a process that automatically transforms speech from one source speaker into a voice resembling that of a target speaker, all the while maintaining the original linguistic content.
The challenge primarily arises in non-parallel voice conversion scenarios, where the lack of directly corresponding parallel data.
The disentanglement of linguistic content in speech and its acoustic voice, timbre is a crucial problem. To address this, various methodologies have been explored, including adversarial training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, vector quantization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, and information perturbation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S2.F1.1" class="ltx_p ltx_align_center ltx_figure_panel"><span id="S2.F1.1.1" class="ltx_text"><img src="/html/2408.09802/assets/x1.png" id="S2.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="977" height="379" alt="Refer to caption"></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S2.F1.11" class="ltx_p ltx_align_center ltx_figure_panel"><span id="S2.F1.11.1" class="ltx_text">Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â (a) Training phaseÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  (b) Inference phase</span></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.12.5.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.9.4" class="ltx_text" style="font-size:90%;">Overview of the proposed method, HYFace, conditional VAE based network that its speaker embedding is learned on face images only. In training phase, a predefiend speaker-wise average F0 (<math id="S2.F1.6.1.m1.2" class="ltx_Math" alttext="f_{0,\mathit{gt}}^{\mathit{avg}}" display="inline"><semantics id="S2.F1.6.1.m1.2b"><msubsup id="S2.F1.6.1.m1.2.3" xref="S2.F1.6.1.m1.2.3.cmml"><mi id="S2.F1.6.1.m1.2.3.2.2" xref="S2.F1.6.1.m1.2.3.2.2.cmml">f</mi><mrow id="S2.F1.6.1.m1.2.2.2.4" xref="S2.F1.6.1.m1.2.2.2.3.cmml"><mn id="S2.F1.6.1.m1.1.1.1.1" xref="S2.F1.6.1.m1.1.1.1.1.cmml">0</mn><mo id="S2.F1.6.1.m1.2.2.2.4.1" xref="S2.F1.6.1.m1.2.2.2.3.cmml">,</mo><mi id="S2.F1.6.1.m1.2.2.2.2" xref="S2.F1.6.1.m1.2.2.2.2.cmml">ğ‘”ğ‘¡</mi></mrow><mi id="S2.F1.6.1.m1.2.3.3" xref="S2.F1.6.1.m1.2.3.3.cmml">ğ‘ğ‘£ğ‘”</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.F1.6.1.m1.2c"><apply id="S2.F1.6.1.m1.2.3.cmml" xref="S2.F1.6.1.m1.2.3"><csymbol cd="ambiguous" id="S2.F1.6.1.m1.2.3.1.cmml" xref="S2.F1.6.1.m1.2.3">superscript</csymbol><apply id="S2.F1.6.1.m1.2.3.2.cmml" xref="S2.F1.6.1.m1.2.3"><csymbol cd="ambiguous" id="S2.F1.6.1.m1.2.3.2.1.cmml" xref="S2.F1.6.1.m1.2.3">subscript</csymbol><ci id="S2.F1.6.1.m1.2.3.2.2.cmml" xref="S2.F1.6.1.m1.2.3.2.2">ğ‘“</ci><list id="S2.F1.6.1.m1.2.2.2.3.cmml" xref="S2.F1.6.1.m1.2.2.2.4"><cn type="integer" id="S2.F1.6.1.m1.1.1.1.1.cmml" xref="S2.F1.6.1.m1.1.1.1.1">0</cn><ci id="S2.F1.6.1.m1.2.2.2.2.cmml" xref="S2.F1.6.1.m1.2.2.2.2">ğ‘”ğ‘¡</ci></list></apply><ci id="S2.F1.6.1.m1.2.3.3.cmml" xref="S2.F1.6.1.m1.2.3.3">ğ‘ğ‘£ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.6.1.m1.2d">f_{0,\mathit{gt}}^{\mathit{avg}}</annotation></semantics></math>) is used to estimate frame-wise <math id="S2.F1.7.2.m2.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S2.F1.7.2.m2.1b"><mrow id="S2.F1.7.2.m2.1.1" xref="S2.F1.7.2.m2.1.1.cmml"><mi id="S2.F1.7.2.m2.1.1.2" xref="S2.F1.7.2.m2.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S2.F1.7.2.m2.1.1.1" xref="S2.F1.7.2.m2.1.1.1.cmml">â€‹</mo><mn id="S2.F1.7.2.m2.1.1.3" xref="S2.F1.7.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.F1.7.2.m2.1c"><apply id="S2.F1.7.2.m2.1.1.cmml" xref="S2.F1.7.2.m2.1.1"><times id="S2.F1.7.2.m2.1.1.1.cmml" xref="S2.F1.7.2.m2.1.1.1"></times><ci id="S2.F1.7.2.m2.1.1.2.cmml" xref="S2.F1.7.2.m2.1.1.2">ğ¹</ci><cn type="integer" id="S2.F1.7.2.m2.1.1.3.cmml" xref="S2.F1.7.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.7.2.m2.1d">F0</annotation></semantics></math> values. However, as the <math id="S2.F1.8.3.m3.2" class="ltx_Math" alttext="f_{0,\mathit{gt}}^{\mathit{avg}}" display="inline"><semantics id="S2.F1.8.3.m3.2b"><msubsup id="S2.F1.8.3.m3.2.3" xref="S2.F1.8.3.m3.2.3.cmml"><mi id="S2.F1.8.3.m3.2.3.2.2" xref="S2.F1.8.3.m3.2.3.2.2.cmml">f</mi><mrow id="S2.F1.8.3.m3.2.2.2.4" xref="S2.F1.8.3.m3.2.2.2.3.cmml"><mn id="S2.F1.8.3.m3.1.1.1.1" xref="S2.F1.8.3.m3.1.1.1.1.cmml">0</mn><mo id="S2.F1.8.3.m3.2.2.2.4.1" xref="S2.F1.8.3.m3.2.2.2.3.cmml">,</mo><mi id="S2.F1.8.3.m3.2.2.2.2" xref="S2.F1.8.3.m3.2.2.2.2.cmml">ğ‘”ğ‘¡</mi></mrow><mi id="S2.F1.8.3.m3.2.3.3" xref="S2.F1.8.3.m3.2.3.3.cmml">ğ‘ğ‘£ğ‘”</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.F1.8.3.m3.2c"><apply id="S2.F1.8.3.m3.2.3.cmml" xref="S2.F1.8.3.m3.2.3"><csymbol cd="ambiguous" id="S2.F1.8.3.m3.2.3.1.cmml" xref="S2.F1.8.3.m3.2.3">superscript</csymbol><apply id="S2.F1.8.3.m3.2.3.2.cmml" xref="S2.F1.8.3.m3.2.3"><csymbol cd="ambiguous" id="S2.F1.8.3.m3.2.3.2.1.cmml" xref="S2.F1.8.3.m3.2.3">subscript</csymbol><ci id="S2.F1.8.3.m3.2.3.2.2.cmml" xref="S2.F1.8.3.m3.2.3.2.2">ğ‘“</ci><list id="S2.F1.8.3.m3.2.2.2.3.cmml" xref="S2.F1.8.3.m3.2.2.2.4"><cn type="integer" id="S2.F1.8.3.m3.1.1.1.1.cmml" xref="S2.F1.8.3.m3.1.1.1.1">0</cn><ci id="S2.F1.8.3.m3.2.2.2.2.cmml" xref="S2.F1.8.3.m3.2.2.2.2">ğ‘”ğ‘¡</ci></list></apply><ci id="S2.F1.8.3.m3.2.3.3.cmml" xref="S2.F1.8.3.m3.2.3.3">ğ‘ğ‘£ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.8.3.m3.2d">f_{0,\mathit{gt}}^{\mathit{avg}}</annotation></semantics></math> values for unseen target speakers are not available during the inference phase, we independently train an average <math id="S2.F1.9.4.m4.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S2.F1.9.4.m4.1b"><mrow id="S2.F1.9.4.m4.1.1" xref="S2.F1.9.4.m4.1.1.cmml"><mi id="S2.F1.9.4.m4.1.1.2" xref="S2.F1.9.4.m4.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S2.F1.9.4.m4.1.1.1" xref="S2.F1.9.4.m4.1.1.1.cmml">â€‹</mo><mn id="S2.F1.9.4.m4.1.1.3" xref="S2.F1.9.4.m4.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.F1.9.4.m4.1c"><apply id="S2.F1.9.4.m4.1.1.cmml" xref="S2.F1.9.4.m4.1.1"><times id="S2.F1.9.4.m4.1.1.1.cmml" xref="S2.F1.9.4.m4.1.1.1"></times><ci id="S2.F1.9.4.m4.1.1.2.cmml" xref="S2.F1.9.4.m4.1.1.2">ğ¹</ci><cn type="integer" id="S2.F1.9.4.m4.1.1.3.cmml" xref="S2.F1.9.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.9.4.m4.1d">F0</annotation></semantics></math> estimation network based solely on facial inputs. This module is then utilized in the inference phase.</span></figcaption>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Notably, recent advancements have been made with the advent of self-supervised learning (SSL) techniques. Pretrained representations trained on large data corpus exhibit a remarkable capacity for disentangling the contents information of speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, thereby significantly enhancing voice conversion processes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Recently, the collaborative voice conversion project, named Sovits-SVC<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/svc-develop-team/so-vits-svc</span></span></span> (SoftVC VITS Singing Voice Conversion), has demonstrated impressive outcomes in both standard voice conversion and singing voice conversion domains. It leverages SSL representations for content representations, and employs a neural-source filter vocoder, specifically designed to track the <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">â€‹</mo><mn id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><times id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1"></times><ci id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">ğ¹</ci><cn type="integer" id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">F0</annotation></semantics></math> of the source audio, which plays a significant role in its original intention for singing voice conversion.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Face-voice association</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Early studies, especially through human behavioral and neuroimaging approach, demonstrate that humans use both facial and vocal cues for identity recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Furthermore, similar studies reveal human ability to match faces with voices of unfamiliar individuals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Particularly, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> showed that humans can significantly match faces and voices under the controlled attributes such as gender, race, and age. Specifically, they also revealed that there is a correlation between the target speaker's voice pitch and facial features.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Building on these finding, interest has surged in learning based methods for associations between faces and voices. An application of such methods includes generating face from a given speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> or vice versa. Specifically, face-based speech synthesis, the focus of this paper, is categorized based on the type of input: text for TTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and source audio for VC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Recently, Sheng <span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> showed prominent result in zero-shot face-based voice conversion, employing memory based methods.
All these works tried to learn cross-modal speaker representations implicitly, without explicit voice characteristic such as <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mrow id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml"><mi id="S2.SS2.p2.1.m1.1.1.2" xref="S2.SS2.p2.1.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p2.1.m1.1.1.1" xref="S2.SS2.p2.1.m1.1.1.1.cmml">â€‹</mo><mn id="S2.SS2.p2.1.m1.1.1.3" xref="S2.SS2.p2.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><apply id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1"><times id="S2.SS2.p2.1.m1.1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1.1"></times><ci id="S2.SS2.p2.1.m1.1.1.2.cmml" xref="S2.SS2.p2.1.m1.1.1.2">ğ¹</ci><cn type="integer" id="S2.SS2.p2.1.m1.1.1.3.cmml" xref="S2.SS2.p2.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">F0</annotation></semantics></math>. Moreover, their evaluation primarily relied on metrics such as the mean opinion score (MOS) or speaker embedding similarities, rather than on assessments directly related to explicit voice characteristics.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.9.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S2.T1.10.2" class="ltx_text" style="font-size:90%;">Evaluation result. The definitions of all metrics are provided in Section <a href="#S4.SS3" title="4.3 Metrics â€£ 4 Experiments â€£ Hear Your Face: Face-based voice conversion with F0 estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.</span></figcaption>
<div id="S2.T1.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:601.3pt;height:91pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S2.T1.7.7" class="ltx_p"><span id="S2.T1.7.7.7" class="ltx_text">
<span id="S2.T1.7.7.7.7" class="ltx_tabular ltx_align_middle">
<span class="ltx_tbody">
<span id="S2.T1.7.7.7.7.7" class="ltx_tr">
<span id="S2.T1.7.7.7.7.7.8" class="ltx_td ltx_border_tt ltx_rowspan ltx_rowspan_2"></span>
<span id="S2.T1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2">Homogeneity<math id="S2.T1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S2.T1.1.1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math></span>
<span id="S2.T1.2.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2">Diversity<math id="S2.T1.2.2.2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S2.T1.2.2.2.2.2.2.m1.1a"><mo stretchy="false" id="S2.T1.2.2.2.2.2.2.m1.1.1" xref="S2.T1.2.2.2.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.2.2.2.m1.1b"><ci id="S2.T1.2.2.2.2.2.2.m1.1.1.cmml" xref="S2.T1.2.2.2.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.2.2.2.m1.1c">\downarrow</annotation></semantics></math></span>
<span id="S2.T1.3.3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2">Consistency(obj)<math id="S2.T1.3.3.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.3.3.3.3.3.3.m1.1a"><mo stretchy="false" id="S2.T1.3.3.3.3.3.3.m1.1.1" xref="S2.T1.3.3.3.3.3.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.3.3.3.m1.1b"><ci id="S2.T1.3.3.3.3.3.3.m1.1.1.cmml" xref="S2.T1.3.3.3.3.3.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math></span>
<span id="S2.T1.4.4.4.4.4.4" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2">Consistency(sub)<math id="S2.T1.4.4.4.4.4.4.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.4.4.4.4.4.4.m1.1a"><mo stretchy="false" id="S2.T1.4.4.4.4.4.4.m1.1.1" xref="S2.T1.4.4.4.4.4.4.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.4.4.4.4.m1.1b"><ci id="S2.T1.4.4.4.4.4.4.m1.1.1.cmml" xref="S2.T1.4.4.4.4.4.4.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.4.4.4.4.m1.1c">\uparrow</annotation></semantics></math></span>
<span id="S2.T1.5.5.5.5.5.5" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2">Naturalness<math id="S2.T1.5.5.5.5.5.5.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.5.5.5.5.5.5.m1.1a"><mo stretchy="false" id="S2.T1.5.5.5.5.5.5.m1.1.1" xref="S2.T1.5.5.5.5.5.5.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.5.5.5.5.m1.1b"><ci id="S2.T1.5.5.5.5.5.5.m1.1.1.cmml" xref="S2.T1.5.5.5.5.5.5.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.5.5.5.5.m1.1c">\uparrow</annotation></semantics></math></span>
<span id="S2.T1.7.7.7.7.7.7" class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2">ABX test(<math id="S2.T1.6.6.6.6.6.6.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S2.T1.6.6.6.6.6.6.m1.1a"><mo id="S2.T1.6.6.6.6.6.6.m1.1.1" xref="S2.T1.6.6.6.6.6.6.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S2.T1.6.6.6.6.6.6.m1.1b"><csymbol cd="latexml" id="S2.T1.6.6.6.6.6.6.m1.1.1.cmml" xref="S2.T1.6.6.6.6.6.6.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.6.6.6.6.6.m1.1c">\%</annotation></semantics></math>)<math id="S2.T1.7.7.7.7.7.7.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S2.T1.7.7.7.7.7.7.m2.1a"><mo stretchy="false" id="S2.T1.7.7.7.7.7.7.m2.1.1" xref="S2.T1.7.7.7.7.7.7.m2.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S2.T1.7.7.7.7.7.7.m2.1b"><ci id="S2.T1.7.7.7.7.7.7.m2.1.1.cmml" xref="S2.T1.7.7.7.7.7.7.m2.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.7.7.7.7.7.m2.1c">\uparrow</annotation></semantics></math></span></span>
<span id="S2.T1.7.7.7.7.8.1" class="ltx_tr">
<span id="S2.T1.7.7.7.7.8.1.1" class="ltx_td ltx_align_center ltx_border_t">HMG</span>
<span id="S2.T1.7.7.7.7.8.1.2" class="ltx_td ltx_align_center ltx_border_t">HTG</span>
<span id="S2.T1.7.7.7.7.8.1.3" class="ltx_td ltx_align_center ltx_border_t">HMG</span>
<span id="S2.T1.7.7.7.7.8.1.4" class="ltx_td ltx_align_center ltx_border_t">HTG</span>
<span id="S2.T1.7.7.7.7.8.1.5" class="ltx_td ltx_align_center ltx_border_t">HMG</span>
<span id="S2.T1.7.7.7.7.8.1.6" class="ltx_td ltx_align_center ltx_border_t">HTG</span>
<span id="S2.T1.7.7.7.7.8.1.7" class="ltx_td ltx_align_center ltx_border_t">HMG</span>
<span id="S2.T1.7.7.7.7.8.1.8" class="ltx_td ltx_align_center ltx_border_t">HTG</span>
<span id="S2.T1.7.7.7.7.8.1.9" class="ltx_td ltx_align_center ltx_border_t">HMG</span>
<span id="S2.T1.7.7.7.7.8.1.10" class="ltx_td ltx_align_center ltx_border_t">HTG</span>
<span id="S2.T1.7.7.7.7.8.1.11" class="ltx_td ltx_align_center ltx_border_t">HMG</span>
<span id="S2.T1.7.7.7.7.8.1.12" class="ltx_td ltx_align_center ltx_border_t">HTG</span></span>
<span id="S2.T1.7.7.7.7.9.2" class="ltx_tr">
<span id="S2.T1.7.7.7.7.9.2.1" class="ltx_td ltx_align_center ltx_border_t">GT</span>
<span id="S2.T1.7.7.7.7.9.2.2" class="ltx_td ltx_align_center ltx_border_t">0.7456</span>
<span id="S2.T1.7.7.7.7.9.2.3" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S2.T1.7.7.7.7.9.2.4" class="ltx_td ltx_align_center ltx_border_t">0.5418</span>
<span id="S2.T1.7.7.7.7.9.2.5" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S2.T1.7.7.7.7.9.2.6" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S2.T1.7.7.7.7.9.2.7" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S2.T1.7.7.7.7.9.2.8" class="ltx_td ltx_align_center ltx_border_t">3.9048</span>
<span id="S2.T1.7.7.7.7.9.2.9" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S2.T1.7.7.7.7.9.2.10" class="ltx_td ltx_align_center ltx_border_t">4.0469</span>
<span id="S2.T1.7.7.7.7.9.2.11" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S2.T1.7.7.7.7.9.2.12" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="S2.T1.7.7.7.7.9.2.13" class="ltx_td ltx_align_center ltx_border_t">-</span></span>
<span id="S2.T1.7.7.7.7.10.3" class="ltx_tr">
<span id="S2.T1.7.7.7.7.10.3.1" class="ltx_td ltx_align_center ltx_border_t">FVMVC<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite></span>
<span id="S2.T1.7.7.7.7.10.3.2" class="ltx_td ltx_align_center ltx_border_t">0.6391</span>
<span id="S2.T1.7.7.7.7.10.3.3" class="ltx_td ltx_align_center ltx_border_t">0.6401</span>
<span id="S2.T1.7.7.7.7.10.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.7.7.7.7.10.3.4.1" class="ltx_text ltx_font_bold">0.5942</span></span>
<span id="S2.T1.7.7.7.7.10.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.7.7.7.7.10.3.5.1" class="ltx_text ltx_font_bold">0.5976</span></span>
<span id="S2.T1.7.7.7.7.10.3.6" class="ltx_td ltx_align_center ltx_border_t">0.5105</span>
<span id="S2.T1.7.7.7.7.10.3.7" class="ltx_td ltx_align_center ltx_border_t">0.5086</span>
<span id="S2.T1.7.7.7.7.10.3.8" class="ltx_td ltx_align_center ltx_border_t">3.5705</span>
<span id="S2.T1.7.7.7.7.10.3.9" class="ltx_td ltx_align_center ltx_border_t">3.5009</span>
<span id="S2.T1.7.7.7.7.10.3.10" class="ltx_td ltx_align_center ltx_border_t">3.4096</span>
<span id="S2.T1.7.7.7.7.10.3.11" class="ltx_td ltx_align_center ltx_border_t">3.2470</span>
<span id="S2.T1.7.7.7.7.10.3.12" class="ltx_td ltx_align_center ltx_border_t">0.395</span>
<span id="S2.T1.7.7.7.7.10.3.13" class="ltx_td ltx_align_center ltx_border_t">0.420</span></span>
<span id="S2.T1.7.7.7.7.11.4" class="ltx_tr">
<span id="S2.T1.7.7.7.7.11.4.1" class="ltx_td ltx_align_center ltx_border_bb">HYFace</span>
<span id="S2.T1.7.7.7.7.11.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.7.7.7.7.11.4.2.1" class="ltx_text ltx_font_bold">0.6770</span></span>
<span id="S2.T1.7.7.7.7.11.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.7.7.7.7.11.4.3.1" class="ltx_text ltx_font_bold">0.6793</span></span>
<span id="S2.T1.7.7.7.7.11.4.4" class="ltx_td ltx_align_center ltx_border_bb">0.6072</span>
<span id="S2.T1.7.7.7.7.11.4.5" class="ltx_td ltx_align_center ltx_border_bb">0.6103</span>
<span id="S2.T1.7.7.7.7.11.4.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.7.7.7.7.11.4.6.1" class="ltx_text ltx_font_bold">0.5696</span></span>
<span id="S2.T1.7.7.7.7.11.4.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.7.7.7.7.11.4.7.1" class="ltx_text ltx_font_bold">0.5632</span></span>
<span id="S2.T1.7.7.7.7.11.4.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.7.7.7.7.11.4.8.1" class="ltx_text ltx_font_bold">3.8916</span></span>
<span id="S2.T1.7.7.7.7.11.4.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.7.7.7.7.11.4.9.1" class="ltx_text ltx_font_bold">3.8189</span></span>
<span id="S2.T1.7.7.7.7.11.4.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.7.7.7.7.11.4.10.1" class="ltx_text ltx_font_bold">3.8313</span></span>
<span id="S2.T1.7.7.7.7.11.4.11" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.7.7.7.7.11.4.11.1" class="ltx_text ltx_font_bold">3.7651</span></span>
<span id="S2.T1.7.7.7.7.11.4.12" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.7.7.7.7.11.4.12.1" class="ltx_text ltx_font_bold">0.605</span></span>
<span id="S2.T1.7.7.7.7.11.4.13" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.7.7.7.7.11.4.13.1" class="ltx_text ltx_font_bold">0.580</span></span></span>
</span>
</span></span></p>
</span></div>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we present our proposed method, HYFace (short for `Hear Your Face'), a novel approach to face-based voice conversion, it begins in Section <a href="#S3.SS1" title="3.1 HYFace â€£ 3 Methods â€£ Hear Your Face: Face-based voice conversion with F0 estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
Then, Section <a href="#S3.SS2" title="3.2 Model architecture â€£ 3 Methods â€£ Hear Your Face: Face-based voice conversion with F0 estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> provides detailed architectures of our proposed model.
Figures <a href="#S2.F1" title="Figure 1 â€£ 2.1 Voice conversion â€£ 2 Related work â€£ Hear Your Face: Face-based voice conversion with F0 estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(a) and <a href="#S2.F1" title="Figure 1 â€£ 2.1 Voice conversion â€£ 2 Related work â€£ Hear Your Face: Face-based voice conversion with F0 estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(b) illustrate the procedures of the training phase and the inference phase of our method, respectively.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>HYFace</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p">Our HYFace network is a voice conversion (VC) framework fundamentally inspired by Sovits-SVC, utilizing a conditional variational autoencoder architecture. It incorporates pretrained SSL representations as content input for the prior encoder.
However, distinct from traditional VC frameworks, HYFace uses the facial image of the target speaker to modify the style of the source audio, instead of using the target speaker's voice. In this system, the speaker embedding, which is learned from the facial images, conditions the prior encoder, posterior encoder, decoder and the frame-wise <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">â€‹</mo><mn id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ğ¹</ci><cn type="integer" id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">F0</annotation></semantics></math> decoder (<math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathit{FF}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">ğ¹ğ¹</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ğ¹ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mathit{FF}</annotation></semantics></math>).</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.12" class="ltx_p">Additionally, to enhance the model's capacity to incorporate target voice characteristics, frame-wise <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">â€‹</mo><mn id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><times id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"></times><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">ğ¹</ci><cn type="integer" id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">F0</annotation></semantics></math> values, <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="f_{0}^{i}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><msubsup id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2.2" xref="S3.SS1.p2.2.m2.1.1.2.2.cmml">f</mi><mn id="S3.SS1.p2.2.m2.1.1.2.3" xref="S3.SS1.p2.2.m2.1.1.2.3.cmml">0</mn><mi id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">superscript</csymbol><apply id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.2.1.cmml" xref="S3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2.2">ğ‘“</ci><cn type="integer" id="S3.SS1.p2.2.m2.1.1.2.3.cmml" xref="S3.SS1.p2.2.m2.1.1.2.3">0</cn></apply><ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">f_{0}^{i}</annotation></semantics></math> (<math id="S3.SS1.p2.3.m3.3" class="ltx_Math" alttext="i=1,...,n" display="inline"><semantics id="S3.SS1.p2.3.m3.3a"><mrow id="S3.SS1.p2.3.m3.3.4" xref="S3.SS1.p2.3.m3.3.4.cmml"><mi id="S3.SS1.p2.3.m3.3.4.2" xref="S3.SS1.p2.3.m3.3.4.2.cmml">i</mi><mo id="S3.SS1.p2.3.m3.3.4.1" xref="S3.SS1.p2.3.m3.3.4.1.cmml">=</mo><mrow id="S3.SS1.p2.3.m3.3.4.3.2" xref="S3.SS1.p2.3.m3.3.4.3.1.cmml"><mn id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">1</mn><mo id="S3.SS1.p2.3.m3.3.4.3.2.1" xref="S3.SS1.p2.3.m3.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="S3.SS1.p2.3.m3.2.2" xref="S3.SS1.p2.3.m3.2.2.cmml">â€¦</mi><mo id="S3.SS1.p2.3.m3.3.4.3.2.2" xref="S3.SS1.p2.3.m3.3.4.3.1.cmml">,</mo><mi id="S3.SS1.p2.3.m3.3.3" xref="S3.SS1.p2.3.m3.3.3.cmml">n</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.3b"><apply id="S3.SS1.p2.3.m3.3.4.cmml" xref="S3.SS1.p2.3.m3.3.4"><eq id="S3.SS1.p2.3.m3.3.4.1.cmml" xref="S3.SS1.p2.3.m3.3.4.1"></eq><ci id="S3.SS1.p2.3.m3.3.4.2.cmml" xref="S3.SS1.p2.3.m3.3.4.2">ğ‘–</ci><list id="S3.SS1.p2.3.m3.3.4.3.1.cmml" xref="S3.SS1.p2.3.m3.3.4.3.2"><cn type="integer" id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">1</cn><ci id="S3.SS1.p2.3.m3.2.2.cmml" xref="S3.SS1.p2.3.m3.2.2">â€¦</ci><ci id="S3.SS1.p2.3.m3.3.3.cmml" xref="S3.SS1.p2.3.m3.3.3">ğ‘›</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.3c">i=1,...,n</annotation></semantics></math>. <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">n</annotation></semantics></math> is the number of frames) conditions both the prior encoder and the decoder.
A speaker-wise average <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mrow id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.5.m5.1.1.1" xref="S3.SS1.p2.5.m5.1.1.1.cmml">â€‹</mo><mn id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><times id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1.1"></times><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">ğ¹</ci><cn type="integer" id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">F0</annotation></semantics></math>Â (<math id="S3.SS1.p2.6.m6.2" class="ltx_Math" alttext="f_{0,\mathit{gt}}^{\mathit{avg}}" display="inline"><semantics id="S3.SS1.p2.6.m6.2a"><msubsup id="S3.SS1.p2.6.m6.2.3" xref="S3.SS1.p2.6.m6.2.3.cmml"><mi id="S3.SS1.p2.6.m6.2.3.2.2" xref="S3.SS1.p2.6.m6.2.3.2.2.cmml">f</mi><mrow id="S3.SS1.p2.6.m6.2.2.2.4" xref="S3.SS1.p2.6.m6.2.2.2.3.cmml"><mn id="S3.SS1.p2.6.m6.1.1.1.1" xref="S3.SS1.p2.6.m6.1.1.1.1.cmml">0</mn><mo id="S3.SS1.p2.6.m6.2.2.2.4.1" xref="S3.SS1.p2.6.m6.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p2.6.m6.2.2.2.2" xref="S3.SS1.p2.6.m6.2.2.2.2.cmml">ğ‘”ğ‘¡</mi></mrow><mi id="S3.SS1.p2.6.m6.2.3.3" xref="S3.SS1.p2.6.m6.2.3.3.cmml">ğ‘ğ‘£ğ‘”</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.2b"><apply id="S3.SS1.p2.6.m6.2.3.cmml" xref="S3.SS1.p2.6.m6.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.2.3.1.cmml" xref="S3.SS1.p2.6.m6.2.3">superscript</csymbol><apply id="S3.SS1.p2.6.m6.2.3.2.cmml" xref="S3.SS1.p2.6.m6.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.2.3.2.1.cmml" xref="S3.SS1.p2.6.m6.2.3">subscript</csymbol><ci id="S3.SS1.p2.6.m6.2.3.2.2.cmml" xref="S3.SS1.p2.6.m6.2.3.2.2">ğ‘“</ci><list id="S3.SS1.p2.6.m6.2.2.2.3.cmml" xref="S3.SS1.p2.6.m6.2.2.2.4"><cn type="integer" id="S3.SS1.p2.6.m6.1.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1.1.1">0</cn><ci id="S3.SS1.p2.6.m6.2.2.2.2.cmml" xref="S3.SS1.p2.6.m6.2.2.2.2">ğ‘”ğ‘¡</ci></list></apply><ci id="S3.SS1.p2.6.m6.2.3.3.cmml" xref="S3.SS1.p2.6.m6.2.3.3">ğ‘ğ‘£ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.2c">f_{0,\mathit{gt}}^{\mathit{avg}}</annotation></semantics></math>)
is adjusted to <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="f_{0}^{i}" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><msubsup id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml"><mi id="S3.SS1.p2.7.m7.1.1.2.2" xref="S3.SS1.p2.7.m7.1.1.2.2.cmml">f</mi><mn id="S3.SS1.p2.7.m7.1.1.2.3" xref="S3.SS1.p2.7.m7.1.1.2.3.cmml">0</mn><mi id="S3.SS1.p2.7.m7.1.1.3" xref="S3.SS1.p2.7.m7.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">superscript</csymbol><apply id="S3.SS1.p2.7.m7.1.1.2.cmml" xref="S3.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.1.2.1.cmml" xref="S3.SS1.p2.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.p2.7.m7.1.1.2.2.cmml" xref="S3.SS1.p2.7.m7.1.1.2.2">ğ‘“</ci><cn type="integer" id="S3.SS1.p2.7.m7.1.1.2.3.cmml" xref="S3.SS1.p2.7.m7.1.1.2.3">0</cn></apply><ci id="S3.SS1.p2.7.m7.1.1.3.cmml" xref="S3.SS1.p2.7.m7.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">f_{0}^{i}</annotation></semantics></math> within the <math id="S3.SS1.p2.8.m8.1" class="ltx_Math" alttext="\mathit{FF}" display="inline"><semantics id="S3.SS1.p2.8.m8.1a"><mi id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml">ğ¹ğ¹</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><ci id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">ğ¹ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">\mathit{FF}</annotation></semantics></math>, in conjunction with the content embedding (<math id="S3.SS1.p2.9.m9.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS1.p2.9.m9.1a"><mi id="S3.SS1.p2.9.m9.1.1" xref="S3.SS1.p2.9.m9.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.1b"><ci id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.1c">c</annotation></semantics></math>) and face-based speaker embedding (<math id="S3.SS1.p2.10.m10.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.SS1.p2.10.m10.1a"><mi id="S3.SS1.p2.10.m10.1.1" xref="S3.SS1.p2.10.m10.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m10.1b"><ci id="S3.SS1.p2.10.m10.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m10.1c">s</annotation></semantics></math>).
The lossÂ <math id="S3.SS1.p2.11.m11.1" class="ltx_Math" alttext="\mathcal{L}_{\mathit{ff}}" display="inline"><semantics id="S3.SS1.p2.11.m11.1a"><msub id="S3.SS1.p2.11.m11.1.1" xref="S3.SS1.p2.11.m11.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.11.m11.1.1.2" xref="S3.SS1.p2.11.m11.1.1.2.cmml">â„’</mi><mi id="S3.SS1.p2.11.m11.1.1.3" xref="S3.SS1.p2.11.m11.1.1.3.cmml">ğ‘“ğ‘“</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m11.1b"><apply id="S3.SS1.p2.11.m11.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.11.m11.1.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1">subscript</csymbol><ci id="S3.SS1.p2.11.m11.1.1.2.cmml" xref="S3.SS1.p2.11.m11.1.1.2">â„’</ci><ci id="S3.SS1.p2.11.m11.1.1.3.cmml" xref="S3.SS1.p2.11.m11.1.1.3">ğ‘“ğ‘“</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m11.1c">\mathcal{L}_{\mathit{ff}}</annotation></semantics></math> for training <math id="S3.SS1.p2.12.m12.1" class="ltx_Math" alttext="\mathit{FF}" display="inline"><semantics id="S3.SS1.p2.12.m12.1a"><mi id="S3.SS1.p2.12.m12.1.1" xref="S3.SS1.p2.12.m12.1.1.cmml">ğ¹ğ¹</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.12.m12.1b"><ci id="S3.SS1.p2.12.m12.1.1.cmml" xref="S3.SS1.p2.12.m12.1.1">ğ¹ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.12.m12.1c">\mathit{FF}</annotation></semantics></math> is as follows:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.7" class="ltx_Math" alttext="\mathcal{L}_{\mathit{ff}}=\frac{1}{n}\sum_{i=1}^{n}(f_{0,\mathit{gt}}^{i}-\mathit{FF}(f_{0,\mathit{gt}}^{\mathit{avg}},c,s))^{2},\vspace{-2mm}" display="block"><semantics id="S3.E1.m1.7a"><mrow id="S3.E1.m1.7.7.1" xref="S3.E1.m1.7.7.1.1.cmml"><mrow id="S3.E1.m1.7.7.1.1" xref="S3.E1.m1.7.7.1.1.cmml"><msub id="S3.E1.m1.7.7.1.1.3" xref="S3.E1.m1.7.7.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.7.7.1.1.3.2" xref="S3.E1.m1.7.7.1.1.3.2.cmml">â„’</mi><mi id="S3.E1.m1.7.7.1.1.3.3" xref="S3.E1.m1.7.7.1.1.3.3.cmml">ğ‘“ğ‘“</mi></msub><mo id="S3.E1.m1.7.7.1.1.2" xref="S3.E1.m1.7.7.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.7.7.1.1.1" xref="S3.E1.m1.7.7.1.1.1.cmml"><mfrac id="S3.E1.m1.7.7.1.1.1.3" xref="S3.E1.m1.7.7.1.1.1.3.cmml"><mn id="S3.E1.m1.7.7.1.1.1.3.2" xref="S3.E1.m1.7.7.1.1.1.3.2.cmml">1</mn><mi id="S3.E1.m1.7.7.1.1.1.3.3" xref="S3.E1.m1.7.7.1.1.1.3.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.1.1.1.2" xref="S3.E1.m1.7.7.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E1.m1.7.7.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.cmml"><munderover id="S3.E1.m1.7.7.1.1.1.1.2" xref="S3.E1.m1.7.7.1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.E1.m1.7.7.1.1.1.1.2.2.2" xref="S3.E1.m1.7.7.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S3.E1.m1.7.7.1.1.1.1.2.2.3" xref="S3.E1.m1.7.7.1.1.1.1.2.2.3.cmml"><mi id="S3.E1.m1.7.7.1.1.1.1.2.2.3.2" xref="S3.E1.m1.7.7.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E1.m1.7.7.1.1.1.1.2.2.3.1" xref="S3.E1.m1.7.7.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E1.m1.7.7.1.1.1.1.2.2.3.3" xref="S3.E1.m1.7.7.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.7.7.1.1.1.1.2.3" xref="S3.E1.m1.7.7.1.1.1.1.2.3.cmml">n</mi></munderover><msup id="S3.E1.m1.7.7.1.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.7.7.1.1.1.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.2" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3.2.2.cmml">f</mi><mrow id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.3.cmml"><mn id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">0</mn><mo id="S3.E1.m1.2.2.2.4.1" xref="S3.E1.m1.2.2.2.3.cmml">,</mo><mi id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">ğ‘”ğ‘¡</mi></mrow><mi id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msubsup><mo id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.2.cmml">âˆ’</mo><mrow id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.3.cmml">ğ¹ğ¹</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><msubsup id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">f</mi><mrow id="S3.E1.m1.4.4.2.4" xref="S3.E1.m1.4.4.2.3.cmml"><mn id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml">0</mn><mo id="S3.E1.m1.4.4.2.4.1" xref="S3.E1.m1.4.4.2.3.cmml">,</mo><mi id="S3.E1.m1.4.4.2.2" xref="S3.E1.m1.4.4.2.2.cmml">ğ‘”ğ‘¡</mi></mrow><mi id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">ğ‘ğ‘£ğ‘”</mi></msubsup><mo id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml">c</mi><mo id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.4" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E1.m1.6.6" xref="S3.E1.m1.6.6.cmml">s</mi><mo stretchy="false" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.5" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.3" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S3.E1.m1.7.7.1.1.1.1.1.3" xref="S3.E1.m1.7.7.1.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow><mo id="S3.E1.m1.7.7.1.2" xref="S3.E1.m1.7.7.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.7b"><apply id="S3.E1.m1.7.7.1.1.cmml" xref="S3.E1.m1.7.7.1"><eq id="S3.E1.m1.7.7.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.2"></eq><apply id="S3.E1.m1.7.7.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.3.1.cmml" xref="S3.E1.m1.7.7.1.1.3">subscript</csymbol><ci id="S3.E1.m1.7.7.1.1.3.2.cmml" xref="S3.E1.m1.7.7.1.1.3.2">â„’</ci><ci id="S3.E1.m1.7.7.1.1.3.3.cmml" xref="S3.E1.m1.7.7.1.1.3.3">ğ‘“ğ‘“</ci></apply><apply id="S3.E1.m1.7.7.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1"><times id="S3.E1.m1.7.7.1.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.2"></times><apply id="S3.E1.m1.7.7.1.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.1.3"><divide id="S3.E1.m1.7.7.1.1.1.3.1.cmml" xref="S3.E1.m1.7.7.1.1.1.3"></divide><cn type="integer" id="S3.E1.m1.7.7.1.1.1.3.2.cmml" xref="S3.E1.m1.7.7.1.1.1.3.2">1</cn><ci id="S3.E1.m1.7.7.1.1.1.3.3.cmml" xref="S3.E1.m1.7.7.1.1.1.3.3">ğ‘›</ci></apply><apply id="S3.E1.m1.7.7.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1"><apply id="S3.E1.m1.7.7.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.1.1.2.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.2">superscript</csymbol><apply id="S3.E1.m1.7.7.1.1.1.1.2.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.2">subscript</csymbol><sum id="S3.E1.m1.7.7.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.2.2.2"></sum><apply id="S3.E1.m1.7.7.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.7.7.1.1.1.1.2.2.3"><eq id="S3.E1.m1.7.7.1.1.1.1.2.2.3.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.2.2.3.1"></eq><ci id="S3.E1.m1.7.7.1.1.1.1.2.2.3.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.2.2.3.2">ğ‘–</ci><cn type="integer" id="S3.E1.m1.7.7.1.1.1.1.2.2.3.3.cmml" xref="S3.E1.m1.7.7.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.7.7.1.1.1.1.2.3.cmml" xref="S3.E1.m1.7.7.1.1.1.1.2.3">ğ‘›</ci></apply><apply id="S3.E1.m1.7.7.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1"><minus id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.2"></minus><apply id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3.2.2">ğ‘“</ci><list id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.4"><cn type="integer" id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">0</cn><ci id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">ğ‘”ğ‘¡</ci></list></apply><ci id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply><apply id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1"><times id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.3">ğ¹ğ¹</ci><vector id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1"><apply id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.2.2">ğ‘“</ci><list id="S3.E1.m1.4.4.2.3.cmml" xref="S3.E1.m1.4.4.2.4"><cn type="integer" id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1">0</cn><ci id="S3.E1.m1.4.4.2.2.cmml" xref="S3.E1.m1.4.4.2.2">ğ‘”ğ‘¡</ci></list></apply><ci id="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.3">ğ‘ğ‘£ğ‘”</ci></apply><ci id="S3.E1.m1.5.5.cmml" xref="S3.E1.m1.5.5">ğ‘</ci><ci id="S3.E1.m1.6.6.cmml" xref="S3.E1.m1.6.6">ğ‘ </ci></vector></apply></apply><cn type="integer" id="S3.E1.m1.7.7.1.1.1.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.7c">\mathcal{L}_{\mathit{ff}}=\frac{1}{n}\sum_{i=1}^{n}(f_{0,\mathit{gt}}^{i}-\mathit{FF}(f_{0,\mathit{gt}}^{\mathit{avg}},c,s))^{2},\vspace{-2mm}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p2.22" class="ltx_p">where <math id="S3.SS1.p2.13.m1.2" class="ltx_Math" alttext="f_{0,\mathit{gt}}^{i}" display="inline"><semantics id="S3.SS1.p2.13.m1.2a"><msubsup id="S3.SS1.p2.13.m1.2.3" xref="S3.SS1.p2.13.m1.2.3.cmml"><mi id="S3.SS1.p2.13.m1.2.3.2.2" xref="S3.SS1.p2.13.m1.2.3.2.2.cmml">f</mi><mrow id="S3.SS1.p2.13.m1.2.2.2.4" xref="S3.SS1.p2.13.m1.2.2.2.3.cmml"><mn id="S3.SS1.p2.13.m1.1.1.1.1" xref="S3.SS1.p2.13.m1.1.1.1.1.cmml">0</mn><mo id="S3.SS1.p2.13.m1.2.2.2.4.1" xref="S3.SS1.p2.13.m1.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p2.13.m1.2.2.2.2" xref="S3.SS1.p2.13.m1.2.2.2.2.cmml">ğ‘”ğ‘¡</mi></mrow><mi id="S3.SS1.p2.13.m1.2.3.3" xref="S3.SS1.p2.13.m1.2.3.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.13.m1.2b"><apply id="S3.SS1.p2.13.m1.2.3.cmml" xref="S3.SS1.p2.13.m1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.13.m1.2.3.1.cmml" xref="S3.SS1.p2.13.m1.2.3">superscript</csymbol><apply id="S3.SS1.p2.13.m1.2.3.2.cmml" xref="S3.SS1.p2.13.m1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.13.m1.2.3.2.1.cmml" xref="S3.SS1.p2.13.m1.2.3">subscript</csymbol><ci id="S3.SS1.p2.13.m1.2.3.2.2.cmml" xref="S3.SS1.p2.13.m1.2.3.2.2">ğ‘“</ci><list id="S3.SS1.p2.13.m1.2.2.2.3.cmml" xref="S3.SS1.p2.13.m1.2.2.2.4"><cn type="integer" id="S3.SS1.p2.13.m1.1.1.1.1.cmml" xref="S3.SS1.p2.13.m1.1.1.1.1">0</cn><ci id="S3.SS1.p2.13.m1.2.2.2.2.cmml" xref="S3.SS1.p2.13.m1.2.2.2.2">ğ‘”ğ‘¡</ci></list></apply><ci id="S3.SS1.p2.13.m1.2.3.3.cmml" xref="S3.SS1.p2.13.m1.2.3.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.13.m1.2c">f_{0,\mathit{gt}}^{i}</annotation></semantics></math> refers to the ground-truth frame-wise <math id="S3.SS1.p2.14.m2.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S3.SS1.p2.14.m2.1a"><mrow id="S3.SS1.p2.14.m2.1.1" xref="S3.SS1.p2.14.m2.1.1.cmml"><mi id="S3.SS1.p2.14.m2.1.1.2" xref="S3.SS1.p2.14.m2.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.14.m2.1.1.1" xref="S3.SS1.p2.14.m2.1.1.1.cmml">â€‹</mo><mn id="S3.SS1.p2.14.m2.1.1.3" xref="S3.SS1.p2.14.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.14.m2.1b"><apply id="S3.SS1.p2.14.m2.1.1.cmml" xref="S3.SS1.p2.14.m2.1.1"><times id="S3.SS1.p2.14.m2.1.1.1.cmml" xref="S3.SS1.p2.14.m2.1.1.1"></times><ci id="S3.SS1.p2.14.m2.1.1.2.cmml" xref="S3.SS1.p2.14.m2.1.1.2">ğ¹</ci><cn type="integer" id="S3.SS1.p2.14.m2.1.1.3.cmml" xref="S3.SS1.p2.14.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.14.m2.1c">F0</annotation></semantics></math> values.
Note that <math id="S3.SS1.p2.15.m3.2" class="ltx_Math" alttext="f_{0,\mathit{gt}}^{\mathit{avg}}" display="inline"><semantics id="S3.SS1.p2.15.m3.2a"><msubsup id="S3.SS1.p2.15.m3.2.3" xref="S3.SS1.p2.15.m3.2.3.cmml"><mi id="S3.SS1.p2.15.m3.2.3.2.2" xref="S3.SS1.p2.15.m3.2.3.2.2.cmml">f</mi><mrow id="S3.SS1.p2.15.m3.2.2.2.4" xref="S3.SS1.p2.15.m3.2.2.2.3.cmml"><mn id="S3.SS1.p2.15.m3.1.1.1.1" xref="S3.SS1.p2.15.m3.1.1.1.1.cmml">0</mn><mo id="S3.SS1.p2.15.m3.2.2.2.4.1" xref="S3.SS1.p2.15.m3.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p2.15.m3.2.2.2.2" xref="S3.SS1.p2.15.m3.2.2.2.2.cmml">ğ‘”ğ‘¡</mi></mrow><mi id="S3.SS1.p2.15.m3.2.3.3" xref="S3.SS1.p2.15.m3.2.3.3.cmml">ğ‘ğ‘£ğ‘”</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.15.m3.2b"><apply id="S3.SS1.p2.15.m3.2.3.cmml" xref="S3.SS1.p2.15.m3.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.15.m3.2.3.1.cmml" xref="S3.SS1.p2.15.m3.2.3">superscript</csymbol><apply id="S3.SS1.p2.15.m3.2.3.2.cmml" xref="S3.SS1.p2.15.m3.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.15.m3.2.3.2.1.cmml" xref="S3.SS1.p2.15.m3.2.3">subscript</csymbol><ci id="S3.SS1.p2.15.m3.2.3.2.2.cmml" xref="S3.SS1.p2.15.m3.2.3.2.2">ğ‘“</ci><list id="S3.SS1.p2.15.m3.2.2.2.3.cmml" xref="S3.SS1.p2.15.m3.2.2.2.4"><cn type="integer" id="S3.SS1.p2.15.m3.1.1.1.1.cmml" xref="S3.SS1.p2.15.m3.1.1.1.1">0</cn><ci id="S3.SS1.p2.15.m3.2.2.2.2.cmml" xref="S3.SS1.p2.15.m3.2.2.2.2">ğ‘”ğ‘¡</ci></list></apply><ci id="S3.SS1.p2.15.m3.2.3.3.cmml" xref="S3.SS1.p2.15.m3.2.3.3">ğ‘ğ‘£ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.15.m3.2c">f_{0,\mathit{gt}}^{\mathit{avg}}</annotation></semantics></math> value represents the average of <math id="S3.SS1.p2.16.m4.2" class="ltx_Math" alttext="f_{0,\mathit{gt}}^{i}" display="inline"><semantics id="S3.SS1.p2.16.m4.2a"><msubsup id="S3.SS1.p2.16.m4.2.3" xref="S3.SS1.p2.16.m4.2.3.cmml"><mi id="S3.SS1.p2.16.m4.2.3.2.2" xref="S3.SS1.p2.16.m4.2.3.2.2.cmml">f</mi><mrow id="S3.SS1.p2.16.m4.2.2.2.4" xref="S3.SS1.p2.16.m4.2.2.2.3.cmml"><mn id="S3.SS1.p2.16.m4.1.1.1.1" xref="S3.SS1.p2.16.m4.1.1.1.1.cmml">0</mn><mo id="S3.SS1.p2.16.m4.2.2.2.4.1" xref="S3.SS1.p2.16.m4.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p2.16.m4.2.2.2.2" xref="S3.SS1.p2.16.m4.2.2.2.2.cmml">ğ‘”ğ‘¡</mi></mrow><mi id="S3.SS1.p2.16.m4.2.3.3" xref="S3.SS1.p2.16.m4.2.3.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.16.m4.2b"><apply id="S3.SS1.p2.16.m4.2.3.cmml" xref="S3.SS1.p2.16.m4.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.16.m4.2.3.1.cmml" xref="S3.SS1.p2.16.m4.2.3">superscript</csymbol><apply id="S3.SS1.p2.16.m4.2.3.2.cmml" xref="S3.SS1.p2.16.m4.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.16.m4.2.3.2.1.cmml" xref="S3.SS1.p2.16.m4.2.3">subscript</csymbol><ci id="S3.SS1.p2.16.m4.2.3.2.2.cmml" xref="S3.SS1.p2.16.m4.2.3.2.2">ğ‘“</ci><list id="S3.SS1.p2.16.m4.2.2.2.3.cmml" xref="S3.SS1.p2.16.m4.2.2.2.4"><cn type="integer" id="S3.SS1.p2.16.m4.1.1.1.1.cmml" xref="S3.SS1.p2.16.m4.1.1.1.1">0</cn><ci id="S3.SS1.p2.16.m4.2.2.2.2.cmml" xref="S3.SS1.p2.16.m4.2.2.2.2">ğ‘”ğ‘¡</ci></list></apply><ci id="S3.SS1.p2.16.m4.2.3.3.cmml" xref="S3.SS1.p2.16.m4.2.3.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.16.m4.2c">f_{0,\mathit{gt}}^{i}</annotation></semantics></math> values across all audio frames for each speaker in the training dataset.
Importantly, due to the unavailability of <math id="S3.SS1.p2.17.m5.2" class="ltx_Math" alttext="f_{0,\mathit{gt}}^{\mathit{avg}}" display="inline"><semantics id="S3.SS1.p2.17.m5.2a"><msubsup id="S3.SS1.p2.17.m5.2.3" xref="S3.SS1.p2.17.m5.2.3.cmml"><mi id="S3.SS1.p2.17.m5.2.3.2.2" xref="S3.SS1.p2.17.m5.2.3.2.2.cmml">f</mi><mrow id="S3.SS1.p2.17.m5.2.2.2.4" xref="S3.SS1.p2.17.m5.2.2.2.3.cmml"><mn id="S3.SS1.p2.17.m5.1.1.1.1" xref="S3.SS1.p2.17.m5.1.1.1.1.cmml">0</mn><mo id="S3.SS1.p2.17.m5.2.2.2.4.1" xref="S3.SS1.p2.17.m5.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p2.17.m5.2.2.2.2" xref="S3.SS1.p2.17.m5.2.2.2.2.cmml">ğ‘”ğ‘¡</mi></mrow><mi id="S3.SS1.p2.17.m5.2.3.3" xref="S3.SS1.p2.17.m5.2.3.3.cmml">ğ‘ğ‘£ğ‘”</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.17.m5.2b"><apply id="S3.SS1.p2.17.m5.2.3.cmml" xref="S3.SS1.p2.17.m5.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.17.m5.2.3.1.cmml" xref="S3.SS1.p2.17.m5.2.3">superscript</csymbol><apply id="S3.SS1.p2.17.m5.2.3.2.cmml" xref="S3.SS1.p2.17.m5.2.3"><csymbol cd="ambiguous" id="S3.SS1.p2.17.m5.2.3.2.1.cmml" xref="S3.SS1.p2.17.m5.2.3">subscript</csymbol><ci id="S3.SS1.p2.17.m5.2.3.2.2.cmml" xref="S3.SS1.p2.17.m5.2.3.2.2">ğ‘“</ci><list id="S3.SS1.p2.17.m5.2.2.2.3.cmml" xref="S3.SS1.p2.17.m5.2.2.2.4"><cn type="integer" id="S3.SS1.p2.17.m5.1.1.1.1.cmml" xref="S3.SS1.p2.17.m5.1.1.1.1">0</cn><ci id="S3.SS1.p2.17.m5.2.2.2.2.cmml" xref="S3.SS1.p2.17.m5.2.2.2.2">ğ‘”ğ‘¡</ci></list></apply><ci id="S3.SS1.p2.17.m5.2.3.3.cmml" xref="S3.SS1.p2.17.m5.2.3.3">ğ‘ğ‘£ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.17.m5.2c">f_{0,\mathit{gt}}^{\mathit{avg}}</annotation></semantics></math> information for unseen target speakers during the inference phase, we independently train a face-based average <math id="S3.SS1.p2.18.m6.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S3.SS1.p2.18.m6.1a"><mrow id="S3.SS1.p2.18.m6.1.1" xref="S3.SS1.p2.18.m6.1.1.cmml"><mi id="S3.SS1.p2.18.m6.1.1.2" xref="S3.SS1.p2.18.m6.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.18.m6.1.1.1" xref="S3.SS1.p2.18.m6.1.1.1.cmml">â€‹</mo><mn id="S3.SS1.p2.18.m6.1.1.3" xref="S3.SS1.p2.18.m6.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.18.m6.1b"><apply id="S3.SS1.p2.18.m6.1.1.cmml" xref="S3.SS1.p2.18.m6.1.1"><times id="S3.SS1.p2.18.m6.1.1.1.cmml" xref="S3.SS1.p2.18.m6.1.1.1"></times><ci id="S3.SS1.p2.18.m6.1.1.2.cmml" xref="S3.SS1.p2.18.m6.1.1.2">ğ¹</ci><cn type="integer" id="S3.SS1.p2.18.m6.1.1.3.cmml" xref="S3.SS1.p2.18.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.18.m6.1c">F0</annotation></semantics></math> estimation network (<math id="S3.SS1.p2.19.m7.1" class="ltx_Math" alttext="\mathit{AF}" display="inline"><semantics id="S3.SS1.p2.19.m7.1a"><mi id="S3.SS1.p2.19.m7.1.1" xref="S3.SS1.p2.19.m7.1.1.cmml">ğ´ğ¹</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.19.m7.1b"><ci id="S3.SS1.p2.19.m7.1.1.cmml" xref="S3.SS1.p2.19.m7.1.1">ğ´ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.19.m7.1c">\mathit{AF}</annotation></semantics></math>) solely on face image (<math id="S3.SS1.p2.20.m8.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S3.SS1.p2.20.m8.1a"><mi id="S3.SS1.p2.20.m8.1.1" xref="S3.SS1.p2.20.m8.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.20.m8.1b"><ci id="S3.SS1.p2.20.m8.1.1.cmml" xref="S3.SS1.p2.20.m8.1.1">ğ‘£</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.20.m8.1c">v</annotation></semantics></math>) of target speakers, which constitutes one of the key components of our proposed method. The lossÂ <math id="S3.SS1.p2.21.m9.1" class="ltx_Math" alttext="\mathcal{L}_{\mathit{af}}" display="inline"><semantics id="S3.SS1.p2.21.m9.1a"><msub id="S3.SS1.p2.21.m9.1.1" xref="S3.SS1.p2.21.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.21.m9.1.1.2" xref="S3.SS1.p2.21.m9.1.1.2.cmml">â„’</mi><mi id="S3.SS1.p2.21.m9.1.1.3" xref="S3.SS1.p2.21.m9.1.1.3.cmml">ğ‘ğ‘“</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.21.m9.1b"><apply id="S3.SS1.p2.21.m9.1.1.cmml" xref="S3.SS1.p2.21.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.21.m9.1.1.1.cmml" xref="S3.SS1.p2.21.m9.1.1">subscript</csymbol><ci id="S3.SS1.p2.21.m9.1.1.2.cmml" xref="S3.SS1.p2.21.m9.1.1.2">â„’</ci><ci id="S3.SS1.p2.21.m9.1.1.3.cmml" xref="S3.SS1.p2.21.m9.1.1.3">ğ‘ğ‘“</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.21.m9.1c">\mathcal{L}_{\mathit{af}}</annotation></semantics></math> for training <math id="S3.SS1.p2.22.m10.1" class="ltx_Math" alttext="AF" display="inline"><semantics id="S3.SS1.p2.22.m10.1a"><mrow id="S3.SS1.p2.22.m10.1.1" xref="S3.SS1.p2.22.m10.1.1.cmml"><mi id="S3.SS1.p2.22.m10.1.1.2" xref="S3.SS1.p2.22.m10.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p2.22.m10.1.1.1" xref="S3.SS1.p2.22.m10.1.1.1.cmml">â€‹</mo><mi id="S3.SS1.p2.22.m10.1.1.3" xref="S3.SS1.p2.22.m10.1.1.3.cmml">F</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.22.m10.1b"><apply id="S3.SS1.p2.22.m10.1.1.cmml" xref="S3.SS1.p2.22.m10.1.1"><times id="S3.SS1.p2.22.m10.1.1.1.cmml" xref="S3.SS1.p2.22.m10.1.1.1"></times><ci id="S3.SS1.p2.22.m10.1.1.2.cmml" xref="S3.SS1.p2.22.m10.1.1.2">ğ´</ci><ci id="S3.SS1.p2.22.m10.1.1.3.cmml" xref="S3.SS1.p2.22.m10.1.1.3">ğ¹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.22.m10.1c">AF</annotation></semantics></math> is as follows:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.4" class="ltx_Math" alttext="\mathcal{L}_{\mathit{af}}=(f_{0,\mathit{gt}}^{\mathit{avg}}-\mathit{AF}(v))^{2}." display="block"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4.1" xref="S3.E2.m1.4.4.1.1.cmml"><mrow id="S3.E2.m1.4.4.1.1" xref="S3.E2.m1.4.4.1.1.cmml"><msub id="S3.E2.m1.4.4.1.1.3" xref="S3.E2.m1.4.4.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.4.4.1.1.3.2" xref="S3.E2.m1.4.4.1.1.3.2.cmml">â„’</mi><mi id="S3.E2.m1.4.4.1.1.3.3" xref="S3.E2.m1.4.4.1.1.3.3.cmml">ğ‘ğ‘“</mi></msub><mo id="S3.E2.m1.4.4.1.1.2" xref="S3.E2.m1.4.4.1.1.2.cmml">=</mo><msup id="S3.E2.m1.4.4.1.1.1" xref="S3.E2.m1.4.4.1.1.1.cmml"><mrow id="S3.E2.m1.4.4.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.1.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.cmml"><msubsup id="S3.E2.m1.4.4.1.1.1.1.1.1.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.2.cmml">f</mi><mrow id="S3.E2.m1.2.2.2.4" xref="S3.E2.m1.2.2.2.3.cmml"><mn id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">0</mn><mo id="S3.E2.m1.2.2.2.4.1" xref="S3.E2.m1.2.2.2.3.cmml">,</mo><mi id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml">ğ‘”ğ‘¡</mi></mrow><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.2.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.3.cmml">ğ‘ğ‘£ğ‘”</mi></msubsup><mo id="S3.E2.m1.4.4.1.1.1.1.1.1.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1.cmml">âˆ’</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.4.4.1.1.1.1.1.1.3.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.2.cmml">ğ´ğ¹</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.1.1.1.1.1.1.3.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mrow id="S3.E2.m1.4.4.1.1.1.1.1.1.3.3.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.1.1.1.1.1.1.3.3.2.1" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.cmml">(</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">v</mi><mo stretchy="false" id="S3.E2.m1.4.4.1.1.1.1.1.1.3.3.2.2" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.E2.m1.4.4.1.1.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S3.E2.m1.4.4.1.1.1.3" xref="S3.E2.m1.4.4.1.1.1.3.cmml">2</mn></msup></mrow><mo lspace="0em" id="S3.E2.m1.4.4.1.2" xref="S3.E2.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.1.1.cmml" xref="S3.E2.m1.4.4.1"><eq id="S3.E2.m1.4.4.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.2"></eq><apply id="S3.E2.m1.4.4.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.3">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.3.2.cmml" xref="S3.E2.m1.4.4.1.1.3.2">â„’</ci><ci id="S3.E2.m1.4.4.1.1.3.3.cmml" xref="S3.E2.m1.4.4.1.1.3.3">ğ‘ğ‘“</ci></apply><apply id="S3.E2.m1.4.4.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1">superscript</csymbol><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1"><minus id="S3.E2.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.2.2">ğ‘“</ci><list id="S3.E2.m1.2.2.2.3.cmml" xref="S3.E2.m1.2.2.2.4"><cn type="integer" id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">0</cn><ci id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">ğ‘”ğ‘¡</ci></list></apply><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.2.3">ğ‘ğ‘£ğ‘”</ci></apply><apply id="S3.E2.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3"><times id="S3.E2.m1.4.4.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.1"></times><ci id="S3.E2.m1.4.4.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.4.4.1.1.1.1.1.1.3.2">ğ´ğ¹</ci><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">ğ‘£</ci></apply></apply><cn type="integer" id="S3.E2.m1.4.4.1.1.1.3.cmml" xref="S3.E2.m1.4.4.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">\mathcal{L}_{\mathit{af}}=(f_{0,\mathit{gt}}^{\mathit{avg}}-\mathit{AF}(v))^{2}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Then, <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="\mathit{AF}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">ğ´ğ¹</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">ğ´ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\mathit{AF}</annotation></semantics></math> is utilized during the inference phase, enhancing our face-based voice conversion network to produce speech that better aligns with the voice characteristics of the target speaker.
To clarify our HYFace training procedure, we describe our other loss functions, which include reconstruction loss, KLÂ (Kullback-Leibler) divergence loss, adversarial loss, and feature matching loss in Supplementary A.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Model architecture</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">This section details the architecture of the modules employed in our model. We note that all modules were trained from scratch, except for the contents encoder, which is based on pretrained models.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.7" class="ltx_p"><span id="S3.SS2.p2.7.4" class="ltx_text ltx_font_bold">Posterior Encoder</span>: It is consists of WaveNet-based residual blocks. To integrate face-based speaker embeddings, we employed global conditioning similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
<br class="ltx_break"><span id="S3.SS2.p2.7.5" class="ltx_text ltx_font_bold">Prior Encoder</span>: It has a transformer-based architecture similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, atop which is stacked a normalizing flow layer comprised of residual coupling blocks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
<br class="ltx_break"><span id="S3.SS2.p2.7.6" class="ltx_text ltx_font_bold">Face Encoder</span>: Vision Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> architectures with projection layer.
<br class="ltx_break"><span id="S3.SS2.p2.7.7" class="ltx_text ltx_font_bold">Contents Encoder</span>: We used ContentVec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, pretrained SSL represenations, especially hugging face version<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://huggingface.co/lengyue233/content-vec-best</span></span></span>
<br class="ltx_break"><span id="S3.SS2.p2.7.8" class="ltx_text ltx_font_bold">Decoder</span>: It basically has architecture similar to the generator of HiFi-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> so as to our discriminator network (<math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">D</annotation></semantics></math>), but for careful conditioning of <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">â€‹</mo><mn id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><times id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></times><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">ğ¹</ci><cn type="integer" id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">F0</annotation></semantics></math> information, we used a neural source filter method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> based conditioning similar to Sovits-SVC.
<br class="ltx_break"><span id="S3.SS2.p2.4.2" class="ltx_text ltx_font_bold">Frame-wise <span id="S3.SS2.p2.4.2.1" class="ltx_text ltx_markedasmath">F0</span> Decoder (<span id="S3.SS2.p2.4.2.2" class="ltx_text ltx_markedasmath">FF</span></span>): It is based on architecture with self-attention layers and feed forward layers conditioned with both content embedding and face-based speaker embedding. Fast Context-base Pitch Estimator<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/CNChTu/FCPE</span></span></span> (FCPE) is used to extract frame-wise <math id="S3.SS2.p2.5.m3.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S3.SS2.p2.5.m3.1a"><mrow id="S3.SS2.p2.5.m3.1.1" xref="S3.SS2.p2.5.m3.1.1.cmml"><mi id="S3.SS2.p2.5.m3.1.1.2" xref="S3.SS2.p2.5.m3.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.5.m3.1.1.1" xref="S3.SS2.p2.5.m3.1.1.1.cmml">â€‹</mo><mn id="S3.SS2.p2.5.m3.1.1.3" xref="S3.SS2.p2.5.m3.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m3.1b"><apply id="S3.SS2.p2.5.m3.1.1.cmml" xref="S3.SS2.p2.5.m3.1.1"><times id="S3.SS2.p2.5.m3.1.1.1.cmml" xref="S3.SS2.p2.5.m3.1.1.1"></times><ci id="S3.SS2.p2.5.m3.1.1.2.cmml" xref="S3.SS2.p2.5.m3.1.1.2">ğ¹</ci><cn type="integer" id="S3.SS2.p2.5.m3.1.1.3.cmml" xref="S3.SS2.p2.5.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m3.1c">F0</annotation></semantics></math> value and speaker-wise average <math id="S3.SS2.p2.6.m4.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S3.SS2.p2.6.m4.1a"><mrow id="S3.SS2.p2.6.m4.1.1" xref="S3.SS2.p2.6.m4.1.1.cmml"><mi id="S3.SS2.p2.6.m4.1.1.2" xref="S3.SS2.p2.6.m4.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.6.m4.1.1.1" xref="S3.SS2.p2.6.m4.1.1.1.cmml">â€‹</mo><mn id="S3.SS2.p2.6.m4.1.1.3" xref="S3.SS2.p2.6.m4.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m4.1b"><apply id="S3.SS2.p2.6.m4.1.1.cmml" xref="S3.SS2.p2.6.m4.1.1"><times id="S3.SS2.p2.6.m4.1.1.1.cmml" xref="S3.SS2.p2.6.m4.1.1.1"></times><ci id="S3.SS2.p2.6.m4.1.1.2.cmml" xref="S3.SS2.p2.6.m4.1.1.2">ğ¹</ci><cn type="integer" id="S3.SS2.p2.6.m4.1.1.3.cmml" xref="S3.SS2.p2.6.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m4.1c">F0</annotation></semantics></math> value.
<br class="ltx_break"><span id="S3.SS2.p2.7.3" class="ltx_text ltx_font_bold">Average <span id="S3.SS2.p2.7.3.1" class="ltx_text ltx_markedasmath">F0</span> estimation network (AF)</span>: Similar to face encoder, vision transformer based architectures with projection layer.
<br class="ltx_break"></p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We used LRS3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, the dataset consists of 5,502 videos from TED and TEDx which has more than 430 hours long. Each video is cropped on speaker's face and it has a resolution of 224Ã—224 with 25 frames per seconds images and 16kHz single channel audio. We used predefined <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">pretrain</span>, <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">trainval</span> and <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_italic">test</span> set for training, validation and evaluation, respectively. Expecting our proposed model to more carefully associate detailed face features with speaker's voice characteristics, we used only frontal images from the dataset. Especially, we employed OpenCV haarcascades<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://github.com/opencv/opencv/tree/master/data/haarcascades</span></span></span> for image selection, resulting in about 20% of image data being filtered out.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">For the evaluation, we picked 50 male and 50 female speakers on <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_italic">test</span> set, ranked by the amount of data available. On average, there are about 5.8 speech audio files and 280 facial images available per speaker.
We hypothesized that converting the voice to a target speaker of a different gender from the source is more challenging than converting to the same gender.
Therefore, we constructed two types of evaluation sets: Homogeneous Gender (HMG) set and Heterogeneous Gender (HTG) set. The HMG set pertains to face-based voice conversion scenarios in which the target speaker's gender is the same as the source speaker's (either male to male (M2M) or female to female (F2F)). In contrast, the HTG set applies to scenarios where the target speaker's gender is different from that of the source speaker (from male to female (M2F) or female to male (F2M)). Thus, technically we have four evaluation sets: M2M and F2F for HMG and M2F and F2M for HTG.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comparison systems</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Ground truth (GT)</span>: The original speech audio, which serves as the upperbound.
<br class="ltx_break"><span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_bold">FVMVC</span>: Face-based memory-based zero-shot Face Voice Conversion model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> which recently demonstrated state-of-the-art performance on LRS3 dataset.
<br class="ltx_break"><span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_bold">HYFace</span>: Our proposed method, detailed in Section <a href="#S3" title="3 Methods â€£ Hear Your Face: Face-based voice conversion with F0 estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Metrics</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Following Sheng <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_italic">et al.</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and other conventional VC studies, for objective evaluation, we assess the homogeneity, diversity, and objective consistency. For subjective evaluation, we examine subjective consistency, naturalness, and ABX tests. Furthermore, we propose a new evaluation metric: pitch deviation. For all objective evaluations, we randomly selected 10 source speakers for each of the 50 target speakers and repeated this process for 10 trials. This resulted in a total of 5,000 conversion pairs for each of the four evaluation sets. To measure cosine similarity, we utilized speaker embeddings generated by <span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_italic">Resemblyzer<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note"><span id="footnote5.1.1.1" class="ltx_text ltx_font_upright">5</span></span><span id="footnote5.5" class="ltx_text ltx_font_upright">https://github.com/resemble-ai/Resemblyzer</span></span></span></span></span>. For subjective evaluations, we use Mean Opinion Scores (MOS) collected via Amzon Mechanical Turk (MTurk). We described detailed procedure of MTurk on Supplementary B. The explanation of each metric is as follows.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.5" class="ltx_p"><span id="S4.SS3.p2.5.1" class="ltx_text ltx_font_bold">Homogeneity</span>: It measures cosine similarity of speaker embeddings in synthesized audio generated from different facial images of the same speaker.
Similarity value is expected to be high regardless of different view of face image on same speaker. We randomly select 10 face images from each target speaker.
<br class="ltx_break"><span id="S4.SS3.p2.5.2" class="ltx_text ltx_font_bold">Diversity</span>: It measures cosine similarity of speaker embeddings in synthesized audio generated from different speakers. In contrast from homogeneity, here the model aims to capture distinct speaker information for different target speakers.
<br class="ltx_break"><span id="S4.SS3.p2.5.3" class="ltx_text ltx_font_bold">Consistency(obj)</span>: It compares the speaker embedding similarity of the synthesized audio with that of the ground-truth audio from the same speaker. To ensure a robust comparison, we also assess this metric with ground-truth audio from a random speaker, referred to as `Consistency(rnd)'.
<br class="ltx_break"><span id="S4.SS3.p2.5.4" class="ltx_text ltx_font_bold">Consistency(sub)</span>: This metric measures consistency for subjective evaluation using a 5-point MOS scale (completely inconsistent to completely consistent). It assesses whether the synthesized audio aligns with the corresponding facial images.
<br class="ltx_break"><span id="S4.SS3.p2.5.5" class="ltx_text ltx_font_bold">Naturalness</span>: It assesses the sound quality of the synthesized audio using 5-point MOS scale (completely unnatural to completely natural).
<br class="ltx_break"><span id="S4.SS3.p2.5.6" class="ltx_text ltx_font_bold">ABX test</span>: This evaluates the subjective preference between two models. Participants are shown a face image and asked to decide which of two synthesized audio samples, one from HYFace and the other from FVMVC, more closely matches the face in the image.
<br class="ltx_break"><span id="S4.SS3.p2.5.7" class="ltx_text ltx_font_bold">Pitch deviations</span>: It is our newly proposed metric. Since the <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">â€‹</mo><mn id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><times id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></times><ci id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">ğ¹</ci><cn type="integer" id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">F0</annotation></semantics></math> is one of the key component of voice, we assess the deviation between the <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mrow id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mi id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.2.m2.1.1.1" xref="S4.SS3.p2.2.m2.1.1.1.cmml">â€‹</mo><mn id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><times id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1"></times><ci id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">ğ¹</ci><cn type="integer" id="S4.SS3.p2.2.m2.1.1.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">F0</annotation></semantics></math> of the synthesized audio and the average <math id="S4.SS3.p2.3.m3.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S4.SS3.p2.3.m3.1a"><mrow id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml"><mi id="S4.SS3.p2.3.m3.1.1.2" xref="S4.SS3.p2.3.m3.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.3.m3.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.cmml">â€‹</mo><mn id="S4.SS3.p2.3.m3.1.1.3" xref="S4.SS3.p2.3.m3.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1"><times id="S4.SS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1.1"></times><ci id="S4.SS3.p2.3.m3.1.1.2.cmml" xref="S4.SS3.p2.3.m3.1.1.2">ğ¹</ci><cn type="integer" id="S4.SS3.p2.3.m3.1.1.3.cmml" xref="S4.SS3.p2.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">F0</annotation></semantics></math> (represented as <math id="S4.SS3.p2.4.m4.2" class="ltx_Math" alttext="f_{0,\mathit{gt}}^{\mathit{avg}}" display="inline"><semantics id="S4.SS3.p2.4.m4.2a"><msubsup id="S4.SS3.p2.4.m4.2.3" xref="S4.SS3.p2.4.m4.2.3.cmml"><mi id="S4.SS3.p2.4.m4.2.3.2.2" xref="S4.SS3.p2.4.m4.2.3.2.2.cmml">f</mi><mrow id="S4.SS3.p2.4.m4.2.2.2.4" xref="S4.SS3.p2.4.m4.2.2.2.3.cmml"><mn id="S4.SS3.p2.4.m4.1.1.1.1" xref="S4.SS3.p2.4.m4.1.1.1.1.cmml">0</mn><mo id="S4.SS3.p2.4.m4.2.2.2.4.1" xref="S4.SS3.p2.4.m4.2.2.2.3.cmml">,</mo><mi id="S4.SS3.p2.4.m4.2.2.2.2" xref="S4.SS3.p2.4.m4.2.2.2.2.cmml">ğ‘”ğ‘¡</mi></mrow><mi id="S4.SS3.p2.4.m4.2.3.3" xref="S4.SS3.p2.4.m4.2.3.3.cmml">ğ‘ğ‘£ğ‘”</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.2b"><apply id="S4.SS3.p2.4.m4.2.3.cmml" xref="S4.SS3.p2.4.m4.2.3"><csymbol cd="ambiguous" id="S4.SS3.p2.4.m4.2.3.1.cmml" xref="S4.SS3.p2.4.m4.2.3">superscript</csymbol><apply id="S4.SS3.p2.4.m4.2.3.2.cmml" xref="S4.SS3.p2.4.m4.2.3"><csymbol cd="ambiguous" id="S4.SS3.p2.4.m4.2.3.2.1.cmml" xref="S4.SS3.p2.4.m4.2.3">subscript</csymbol><ci id="S4.SS3.p2.4.m4.2.3.2.2.cmml" xref="S4.SS3.p2.4.m4.2.3.2.2">ğ‘“</ci><list id="S4.SS3.p2.4.m4.2.2.2.3.cmml" xref="S4.SS3.p2.4.m4.2.2.2.4"><cn type="integer" id="S4.SS3.p2.4.m4.1.1.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1.1.1">0</cn><ci id="S4.SS3.p2.4.m4.2.2.2.2.cmml" xref="S4.SS3.p2.4.m4.2.2.2.2">ğ‘”ğ‘¡</ci></list></apply><ci id="S4.SS3.p2.4.m4.2.3.3.cmml" xref="S4.SS3.p2.4.m4.2.3.3">ğ‘ğ‘£ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.2c">f_{0,\mathit{gt}}^{\mathit{avg}}</annotation></semantics></math> in SectionÂ <a href="#S3" title="3 Methods â€£ Hear Your Face: Face-based voice conversion with F0 estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) of the ground-truth target speaker.
Note that the standard deviations (stdv) of the <math id="S4.SS3.p2.5.m5.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S4.SS3.p2.5.m5.1a"><mrow id="S4.SS3.p2.5.m5.1.1" xref="S4.SS3.p2.5.m5.1.1.cmml"><mi id="S4.SS3.p2.5.m5.1.1.2" xref="S4.SS3.p2.5.m5.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.SS3.p2.5.m5.1.1.1" xref="S4.SS3.p2.5.m5.1.1.1.cmml">â€‹</mo><mn id="S4.SS3.p2.5.m5.1.1.3" xref="S4.SS3.p2.5.m5.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.5.m5.1b"><apply id="S4.SS3.p2.5.m5.1.1.cmml" xref="S4.SS3.p2.5.m5.1.1"><times id="S4.SS3.p2.5.m5.1.1.1.cmml" xref="S4.SS3.p2.5.m5.1.1.1"></times><ci id="S4.SS3.p2.5.m5.1.1.2.cmml" xref="S4.SS3.p2.5.m5.1.1.2">ğ¹</ci><cn type="integer" id="S4.SS3.p2.5.m5.1.1.3.cmml" xref="S4.SS3.p2.5.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.5.m5.1c">F0</annotation></semantics></math> for all audio samples are 29.18 Hz for male speakers and 37.50 Hz for female speakers. These values serve as a baseline, reflecting the deviation is based solely on gender class. If the model captures the associations between facial features and voice characteristics within a gender-controlled set, then it should demonstrate a deviation lower than these baseline values.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Results</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The evaluation results for the metrics discussed in previous section can be found in Table <a href="#S2.T1" title="Table 1 â€£ 2.2 Face-voice association â€£ 2 Related work â€£ Hear Your Face: Face-based voice conversion with F0 estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
As mentioned in Section <a href="#S4.SS1" title="4.1 Dataset â€£ 4 Experiments â€£ Hear Your Face: Face-based voice conversion with F0 estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>, we have created four evaluations sets: HMG (M2M and F2F), HTG (M2F and F2M). However, due to space limitations, we present the averaged scores for both HMG and HTG. For detailed results from all four evaluation sets, please refer to Supplementary C. Note that GT refers to the ground-truth audio, in which pairing between source and target of different genders (heterogeneous gender pairing) is not possible. Therefore, only HMG scores are applicable. We have not reported Consistency(obj) scores for GT as it is conceptually identical to Homogeneity.</p>
</div>
<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Objective results</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.5" class="ltx_p">In Homogeneity, our proposed model HYFace presents high scores in both HMG and HTG (<math id="S4.SS4.SSS1.p1.1.m1.1" class="ltx_Math" alttext="p&lt;0.01" display="inline"><semantics id="S4.SS4.SSS1.p1.1.m1.1a"><mrow id="S4.SS4.SSS1.p1.1.m1.1.1" xref="S4.SS4.SSS1.p1.1.m1.1.1.cmml"><mi id="S4.SS4.SSS1.p1.1.m1.1.1.2" xref="S4.SS4.SSS1.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS4.SSS1.p1.1.m1.1.1.1" xref="S4.SS4.SSS1.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS4.SSS1.p1.1.m1.1.1.3" xref="S4.SS4.SSS1.p1.1.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.1.m1.1b"><apply id="S4.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS1.p1.1.m1.1.1"><lt id="S4.SS4.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS4.SSS1.p1.1.m1.1.1.1"></lt><ci id="S4.SS4.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS4.SSS1.p1.1.m1.1.1.2">ğ‘</ci><cn type="float" id="S4.SS4.SSS1.p1.1.m1.1.1.3.cmml" xref="S4.SS4.SSS1.p1.1.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.1.m1.1c">p&lt;0.01</annotation></semantics></math> in paired t-tests) than FVMVC. For Diversity, the FVMVC shows better scores both in HMG and HTG (<math id="S4.SS4.SSS1.p1.2.m2.1" class="ltx_Math" alttext="p&lt;0.01" display="inline"><semantics id="S4.SS4.SSS1.p1.2.m2.1a"><mrow id="S4.SS4.SSS1.p1.2.m2.1.1" xref="S4.SS4.SSS1.p1.2.m2.1.1.cmml"><mi id="S4.SS4.SSS1.p1.2.m2.1.1.2" xref="S4.SS4.SSS1.p1.2.m2.1.1.2.cmml">p</mi><mo id="S4.SS4.SSS1.p1.2.m2.1.1.1" xref="S4.SS4.SSS1.p1.2.m2.1.1.1.cmml">&lt;</mo><mn id="S4.SS4.SSS1.p1.2.m2.1.1.3" xref="S4.SS4.SSS1.p1.2.m2.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.2.m2.1b"><apply id="S4.SS4.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1"><lt id="S4.SS4.SSS1.p1.2.m2.1.1.1.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1.1"></lt><ci id="S4.SS4.SSS1.p1.2.m2.1.1.2.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1.2">ğ‘</ci><cn type="float" id="S4.SS4.SSS1.p1.2.m2.1.1.3.cmml" xref="S4.SS4.SSS1.p1.2.m2.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.2.m2.1c">p&lt;0.01</annotation></semantics></math> in paired t-tests). For Consistency(obj), HYFace scored higher, indicating closer similarity with the ground-truth audio.
For fair comparing, we measured the Consistency(rnd), which measures similarity between synthesized audio and the ground-truth audio of `different' speakers.
For HYFace, the Consistency(rnd) scores are 0.5577 for HMG and 0.5524 for HTG. The Consistency(obj) scores of HYFace are statistically higher than Consistency(rnd) (<math id="S4.SS4.SSS1.p1.3.m3.1" class="ltx_Math" alttext="p&lt;0.01" display="inline"><semantics id="S4.SS4.SSS1.p1.3.m3.1a"><mrow id="S4.SS4.SSS1.p1.3.m3.1.1" xref="S4.SS4.SSS1.p1.3.m3.1.1.cmml"><mi id="S4.SS4.SSS1.p1.3.m3.1.1.2" xref="S4.SS4.SSS1.p1.3.m3.1.1.2.cmml">p</mi><mo id="S4.SS4.SSS1.p1.3.m3.1.1.1" xref="S4.SS4.SSS1.p1.3.m3.1.1.1.cmml">&lt;</mo><mn id="S4.SS4.SSS1.p1.3.m3.1.1.3" xref="S4.SS4.SSS1.p1.3.m3.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.3.m3.1b"><apply id="S4.SS4.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS4.SSS1.p1.3.m3.1.1"><lt id="S4.SS4.SSS1.p1.3.m3.1.1.1.cmml" xref="S4.SS4.SSS1.p1.3.m3.1.1.1"></lt><ci id="S4.SS4.SSS1.p1.3.m3.1.1.2.cmml" xref="S4.SS4.SSS1.p1.3.m3.1.1.2">ğ‘</ci><cn type="float" id="S4.SS4.SSS1.p1.3.m3.1.1.3.cmml" xref="S4.SS4.SSS1.p1.3.m3.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.3.m3.1c">p&lt;0.01</annotation></semantics></math> in paired t-tests) both in HMG and HTG, suggesting HYFace has meaningful correlation with voice characteristics of ground-truth speaker. However, in case of FVMVC, the Consistency(rnd) scores are 0.5074 for HMG and 0.5064 for HTG, showing no significant difference from its Consistency(obj) scores (<math id="S4.SS4.SSS1.p1.4.m4.1" class="ltx_Math" alttext="p&gt;0.02" display="inline"><semantics id="S4.SS4.SSS1.p1.4.m4.1a"><mrow id="S4.SS4.SSS1.p1.4.m4.1.1" xref="S4.SS4.SSS1.p1.4.m4.1.1.cmml"><mi id="S4.SS4.SSS1.p1.4.m4.1.1.2" xref="S4.SS4.SSS1.p1.4.m4.1.1.2.cmml">p</mi><mo id="S4.SS4.SSS1.p1.4.m4.1.1.1" xref="S4.SS4.SSS1.p1.4.m4.1.1.1.cmml">&gt;</mo><mn id="S4.SS4.SSS1.p1.4.m4.1.1.3" xref="S4.SS4.SSS1.p1.4.m4.1.1.3.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.4.m4.1b"><apply id="S4.SS4.SSS1.p1.4.m4.1.1.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1"><gt id="S4.SS4.SSS1.p1.4.m4.1.1.1.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.1"></gt><ci id="S4.SS4.SSS1.p1.4.m4.1.1.2.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.2">ğ‘</ci><cn type="float" id="S4.SS4.SSS1.p1.4.m4.1.1.3.cmml" xref="S4.SS4.SSS1.p1.4.m4.1.1.3">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.4.m4.1c">p&gt;0.02</annotation></semantics></math> for HMG and <math id="S4.SS4.SSS1.p1.5.m5.1" class="ltx_Math" alttext="p&gt;0.1" display="inline"><semantics id="S4.SS4.SSS1.p1.5.m5.1a"><mrow id="S4.SS4.SSS1.p1.5.m5.1.1" xref="S4.SS4.SSS1.p1.5.m5.1.1.cmml"><mi id="S4.SS4.SSS1.p1.5.m5.1.1.2" xref="S4.SS4.SSS1.p1.5.m5.1.1.2.cmml">p</mi><mo id="S4.SS4.SSS1.p1.5.m5.1.1.1" xref="S4.SS4.SSS1.p1.5.m5.1.1.1.cmml">&gt;</mo><mn id="S4.SS4.SSS1.p1.5.m5.1.1.3" xref="S4.SS4.SSS1.p1.5.m5.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.5.m5.1b"><apply id="S4.SS4.SSS1.p1.5.m5.1.1.cmml" xref="S4.SS4.SSS1.p1.5.m5.1.1"><gt id="S4.SS4.SSS1.p1.5.m5.1.1.1.cmml" xref="S4.SS4.SSS1.p1.5.m5.1.1.1"></gt><ci id="S4.SS4.SSS1.p1.5.m5.1.1.2.cmml" xref="S4.SS4.SSS1.p1.5.m5.1.1.2">ğ‘</ci><cn type="float" id="S4.SS4.SSS1.p1.5.m5.1.1.3.cmml" xref="S4.SS4.SSS1.p1.5.m5.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.5.m5.1c">p&gt;0.1</annotation></semantics></math> for HTG in paired t-tests). It means that there is no correlation with speaker embedding of synthesized audio from FVMVC and that of ground-truth audio.
The objective results suggest that although HYFace may show slightly poorer Diversity compared to the benchmark, its speaker embeddings significantly align with those of the ground-truth speaker, a feature not observed in the benchmark model. Additionally, our model demonstrates higher Homogeneity scores.</p>
</div>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Subjective results</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.2" class="ltx_p">In all subjective evaluations, including Consistency(sub), Naturalness, and ABX test, our proposed HYFace model outperformed the benchmark for both HMG and HTG (<math id="S4.SS4.SSS2.p1.1.m1.1" class="ltx_Math" alttext="p&lt;0.01" display="inline"><semantics id="S4.SS4.SSS2.p1.1.m1.1a"><mrow id="S4.SS4.SSS2.p1.1.m1.1.1" xref="S4.SS4.SSS2.p1.1.m1.1.1.cmml"><mi id="S4.SS4.SSS2.p1.1.m1.1.1.2" xref="S4.SS4.SSS2.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS4.SSS2.p1.1.m1.1.1.1" xref="S4.SS4.SSS2.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.SS4.SSS2.p1.1.m1.1.1.3" xref="S4.SS4.SSS2.p1.1.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS2.p1.1.m1.1b"><apply id="S4.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS2.p1.1.m1.1.1"><lt id="S4.SS4.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS4.SSS2.p1.1.m1.1.1.1"></lt><ci id="S4.SS4.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS4.SSS2.p1.1.m1.1.1.2">ğ‘</ci><cn type="float" id="S4.SS4.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS4.SSS2.p1.1.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS2.p1.1.m1.1c">p&lt;0.01</annotation></semantics></math> in paired t-tests). Remarkably, in the Consistency(sub) metric, which assesses how well the synthesized audio matches the corresponding ground-truth facial image, HYFace achieved scores comparable to those of the ground-truth audio. Furthermore, HYFace's Naturalness score nearly approached that of the ground-truth audio.
In terms of performance differences between HMG and HTG sets, only the Naturalness scores in the FVMVC model showed a significant decrease in the HTG set compared to HMG (<math id="S4.SS4.SSS2.p1.2.m2.1" class="ltx_Math" alttext="p&lt;0.01" display="inline"><semantics id="S4.SS4.SSS2.p1.2.m2.1a"><mrow id="S4.SS4.SSS2.p1.2.m2.1.1" xref="S4.SS4.SSS2.p1.2.m2.1.1.cmml"><mi id="S4.SS4.SSS2.p1.2.m2.1.1.2" xref="S4.SS4.SSS2.p1.2.m2.1.1.2.cmml">p</mi><mo id="S4.SS4.SSS2.p1.2.m2.1.1.1" xref="S4.SS4.SSS2.p1.2.m2.1.1.1.cmml">&lt;</mo><mn id="S4.SS4.SSS2.p1.2.m2.1.1.3" xref="S4.SS4.SSS2.p1.2.m2.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS2.p1.2.m2.1b"><apply id="S4.SS4.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS2.p1.2.m2.1.1"><lt id="S4.SS4.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS4.SSS2.p1.2.m2.1.1.1"></lt><ci id="S4.SS4.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS4.SSS2.p1.2.m2.1.1.2">ğ‘</ci><cn type="float" id="S4.SS4.SSS2.p1.2.m2.1.1.3.cmml" xref="S4.SS4.SSS2.p1.2.m2.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS2.p1.2.m2.1c">p&lt;0.01</annotation></semantics></math> in paired t-tests).</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.6.2.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.2.1" class="ltx_text" style="font-size:90%;">Comparison of pitch deviation (in Hz) between synthesized audio and ground-truth average <math id="S4.T2.2.1.m1.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S4.T2.2.1.m1.1b"><mrow id="S4.T2.2.1.m1.1.1" xref="S4.T2.2.1.m1.1.1.cmml"><mi id="S4.T2.2.1.m1.1.1.2" xref="S4.T2.2.1.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.1.m1.1.1.1" xref="S4.T2.2.1.m1.1.1.1.cmml">â€‹</mo><mn id="S4.T2.2.1.m1.1.1.3" xref="S4.T2.2.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.1.m1.1c"><apply id="S4.T2.2.1.m1.1.1.cmml" xref="S4.T2.2.1.m1.1.1"><times id="S4.T2.2.1.m1.1.1.1.cmml" xref="S4.T2.2.1.m1.1.1.1"></times><ci id="S4.T2.2.1.m1.1.1.2.cmml" xref="S4.T2.2.1.m1.1.1.2">ğ¹</ci><cn type="integer" id="S4.T2.2.1.m1.1.1.3.cmml" xref="S4.T2.2.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.1.m1.1d">F0</annotation></semantics></math></span></figcaption>
<div id="S4.T2.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:275.2pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="S4.T2.4.2" class="ltx_p"><span id="S4.T2.4.2.2" class="ltx_text">
<span id="S4.T2.4.2.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_tbody">
<span id="S4.T2.4.2.2.2.2" class="ltx_tr">
<span id="S4.T2.4.2.2.2.2.3" class="ltx_td ltx_border_tt ltx_rowspan ltx_rowspan_2"></span>
<span id="S4.T2.3.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2">HMG<math id="S4.T2.3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.3.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T2.3.1.1.1.1.1.m1.1.1" xref="S4.T2.3.1.1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.1.1.1.1.1.m1.1b"><ci id="S4.T2.3.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.3.1.1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.1.1.1.1.1.m1.1c">\downarrow</annotation></semantics></math></span>
<span id="S4.T2.4.2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2">HTG<math id="S4.T2.4.2.2.2.2.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.4.2.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T2.4.2.2.2.2.2.m1.1.1" xref="S4.T2.4.2.2.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.2.2.2.2.2.m1.1b"><ci id="S4.T2.4.2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.4.2.2.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.2.2.2.2.2.m1.1c">\downarrow</annotation></semantics></math></span></span>
<span id="S4.T2.4.2.2.2.3.1" class="ltx_tr">
<span id="S4.T2.4.2.2.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">M2M</span>
<span id="S4.T2.4.2.2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">F2F</span>
<span id="S4.T2.4.2.2.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">M2F</span>
<span id="S4.T2.4.2.2.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">F2M</span></span>
<span id="S4.T2.4.2.2.2.4.2" class="ltx_tr">
<span id="S4.T2.4.2.2.2.4.2.1" class="ltx_td ltx_align_center ltx_border_t">FVMVC<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite></span>
<span id="S4.T2.4.2.2.2.4.2.2" class="ltx_td ltx_align_center ltx_border_t">29.55</span>
<span id="S4.T2.4.2.2.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t">34.15</span>
<span id="S4.T2.4.2.2.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t">34.75</span>
<span id="S4.T2.4.2.2.2.4.2.5" class="ltx_td ltx_align_center ltx_border_t">28.50</span></span>
<span id="S4.T2.4.2.2.2.5.3" class="ltx_tr">
<span id="S4.T2.4.2.2.2.5.3.1" class="ltx_td ltx_align_center ltx_border_bb">HYFace</span>
<span id="S4.T2.4.2.2.2.5.3.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.4.2.2.2.5.3.2.1" class="ltx_text ltx_font_bold">24.01</span></span>
<span id="S4.T2.4.2.2.2.5.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.4.2.2.2.5.3.3.1" class="ltx_text ltx_font_bold">29.58</span></span>
<span id="S4.T2.4.2.2.2.5.3.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.4.2.2.2.5.3.4.1" class="ltx_text ltx_font_bold">29.15</span></span>
<span id="S4.T2.4.2.2.2.5.3.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.4.2.2.2.5.3.5.1" class="ltx_text ltx_font_bold">24.31</span></span></span>
</span>
</span></span></p>
</span></div>
</figure>
</section>
<section id="S4.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3 </span>Pitch deviations</h4>

<div id="S4.SS4.SSS3.p1" class="ltx_para">
<p id="S4.SS4.SSS3.p1.3" class="ltx_p">Table <a href="#S4.T2" title="Table 2 â€£ 4.4.2 Subjective results â€£ 4.4 Results â€£ 4 Experiments â€£ Hear Your Face: Face-based voice conversion with F0 estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the <math id="S4.SS4.SSS3.p1.1.m1.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S4.SS4.SSS3.p1.1.m1.1a"><mrow id="S4.SS4.SSS3.p1.1.m1.1.1" xref="S4.SS4.SSS3.p1.1.m1.1.1.cmml"><mi id="S4.SS4.SSS3.p1.1.m1.1.1.2" xref="S4.SS4.SSS3.p1.1.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.p1.1.m1.1.1.1" xref="S4.SS4.SSS3.p1.1.m1.1.1.1.cmml">â€‹</mo><mn id="S4.SS4.SSS3.p1.1.m1.1.1.3" xref="S4.SS4.SSS3.p1.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.p1.1.m1.1b"><apply id="S4.SS4.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS3.p1.1.m1.1.1"><times id="S4.SS4.SSS3.p1.1.m1.1.1.1.cmml" xref="S4.SS4.SSS3.p1.1.m1.1.1.1"></times><ci id="S4.SS4.SSS3.p1.1.m1.1.1.2.cmml" xref="S4.SS4.SSS3.p1.1.m1.1.1.2">ğ¹</ci><cn type="integer" id="S4.SS4.SSS3.p1.1.m1.1.1.3.cmml" xref="S4.SS4.SSS3.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.p1.1.m1.1c">F0</annotation></semantics></math> deviation of two models, our proposed HYFace and FVMVC, the benchmark. Across evaluation sets of all gender pairings of source and target speakers (M2M, F2F, M2F, and F2M), the proposed HYFace exhibited superior <math id="S4.SS4.SSS3.p1.2.m2.1" class="ltx_Math" alttext="F0" display="inline"><semantics id="S4.SS4.SSS3.p1.2.m2.1a"><mrow id="S4.SS4.SSS3.p1.2.m2.1.1" xref="S4.SS4.SSS3.p1.2.m2.1.1.cmml"><mi id="S4.SS4.SSS3.p1.2.m2.1.1.2" xref="S4.SS4.SSS3.p1.2.m2.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.p1.2.m2.1.1.1" xref="S4.SS4.SSS3.p1.2.m2.1.1.1.cmml">â€‹</mo><mn id="S4.SS4.SSS3.p1.2.m2.1.1.3" xref="S4.SS4.SSS3.p1.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.p1.2.m2.1b"><apply id="S4.SS4.SSS3.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS3.p1.2.m2.1.1"><times id="S4.SS4.SSS3.p1.2.m2.1.1.1.cmml" xref="S4.SS4.SSS3.p1.2.m2.1.1.1"></times><ci id="S4.SS4.SSS3.p1.2.m2.1.1.2.cmml" xref="S4.SS4.SSS3.p1.2.m2.1.1.2">ğ¹</ci><cn type="integer" id="S4.SS4.SSS3.p1.2.m2.1.1.3.cmml" xref="S4.SS4.SSS3.p1.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.p1.2.m2.1c">F0</annotation></semantics></math> estimation performance compared to the benchmark (<math id="S4.SS4.SSS3.p1.3.m3.1" class="ltx_Math" alttext="p&lt;0.01" display="inline"><semantics id="S4.SS4.SSS3.p1.3.m3.1a"><mrow id="S4.SS4.SSS3.p1.3.m3.1.1" xref="S4.SS4.SSS3.p1.3.m3.1.1.cmml"><mi id="S4.SS4.SSS3.p1.3.m3.1.1.2" xref="S4.SS4.SSS3.p1.3.m3.1.1.2.cmml">p</mi><mo id="S4.SS4.SSS3.p1.3.m3.1.1.1" xref="S4.SS4.SSS3.p1.3.m3.1.1.1.cmml">&lt;</mo><mn id="S4.SS4.SSS3.p1.3.m3.1.1.3" xref="S4.SS4.SSS3.p1.3.m3.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.p1.3.m3.1b"><apply id="S4.SS4.SSS3.p1.3.m3.1.1.cmml" xref="S4.SS4.SSS3.p1.3.m3.1.1"><lt id="S4.SS4.SSS3.p1.3.m3.1.1.1.cmml" xref="S4.SS4.SSS3.p1.3.m3.1.1.1"></lt><ci id="S4.SS4.SSS3.p1.3.m3.1.1.2.cmml" xref="S4.SS4.SSS3.p1.3.m3.1.1.2">ğ‘</ci><cn type="float" id="S4.SS4.SSS3.p1.3.m3.1.1.3.cmml" xref="S4.SS4.SSS3.p1.3.m3.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.p1.3.m3.1c">p&lt;0.01</annotation></semantics></math> in paired t-tests).
Furthermore, HYFace consistently demonstrated significantly lower deviations compared to the stdv of the ground truth. In the male cases, the deviations are 24.01 for M2M and 24.31 for F2M, both below the GT male stdv of 29.18. In female cases, the deviations are 29.58 for F2F and 29.15 for M2F, each below the GT female stdv of 37.50. It suggests that our model can nearly estimate the pitch of the target speaker based solely on facial images, even under the controlled gender set.
To our knowledge, this marks the first instance of evaluating explicit voice characteristics, pitch, associating with facial features within the face-based voice conversion domain, and also yielding meaningful results.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we present a novel framework for face-based voice conversion, particularly utilizing fundamental frequency estimation module, which operates solely on facial images. Through comprehensive objective and subjective evaluations, our model has achieved state-of-the-art performance. Moreover, in our newly proposed metric, which explicitly assesses the association between facial features and voice characteristics, our method has yielded meaningful results.
We hope that this research will serve as stepping stones towards providing individuals without a voice with one that fits their identity.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgements</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We sincerely give thanks to <span id="S6.p1.1.1" class="ltx_text ltx_font_italic">Zheng-Yan, Sheng</span> who is the author of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> for dedication to the academy area and kind communication regarding model reproduction.
This work was supported by Institute of Information communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korean government(MSIT) [No. RS-2022-II220641, XVoice: Multi-Modal Voice Meta Learning].</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
C.Â Kim, H.Â V. Shin, T.-H. Oh, A.Â Kaspar, M.Â Elgharib, and W.Â Matusik, ``On learning associations of faces and voices,'' in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Computer Visionâ€“ACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 2â€“6, 2018, Revised Selected Papers, Part V 14</em>.Â Â Â Springer, 2019, pp. 276â€“292.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
T.-H. Oh, T.Â Dekel, C.Â Kim, I.Â Mosseri, W.Â T. Freeman, M.Â Rubinstein, and W.Â Matusik, ``Speech2face: Learning the face behind a voice,'' in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2019, pp. 7539â€“7548.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S.Â Goto, K.Â Onishi, Y.Â Saito, K.Â Tachibana, and K.Â Mori, ``Face2speech: Towards multi-speaker text-to-speech synthesis using an embedding vector predicted from a face image.'' in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, 2020, pp. 1321â€“1325.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
B.Â PlÃ¼ster, C.Â Weber, L.Â Qu, and S.Â Wermter, ``Hearing faces: Target speaker text-to-speech synthesis from a face,'' in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.Â Â Â IEEE, 2021, pp. 757â€“764.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J.Â Lee, J.Â S. Chung, and S.-W. Chung, ``Imaginary voice: Face-styled diffusion model for text-to-speech,'' in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2023, pp. 1â€“5.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
P.Â Wen, Q.Â Xu, Y.Â Jiang, Z.Â Yang, Y.Â He, and Q.Â Huang, ``Seeking the shape of sound: An adaptive framework for learning voice-face association,'' in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2021, pp. 16â€‰347â€“16â€‰356.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
H.-H. Lu, S.-E. Weng, Y.-F. Yen, H.-H. Shuai, and W.-H. Cheng, ``Face-based voice conversion: Learning the voice behind a face,'' in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th ACM International Conference on Multimedia</em>, 2021, pp. 496â€“505.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
N.Â Takahashi, M.Â K. Singh, and Y.Â Mitsufuji, ``Cross-modal face-and voice-style transfer,'' <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13838</em>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Z.-Y. Sheng, Y.Â Ai, Y.-N. Chen, and Z.-H. Ling, ``Face-driven zero-shot voice conversion with memory-based face-voice alignment,'' in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st ACM International Conference on Multimedia</em>, 2023, pp. 8443â€“8452.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
H.-S. Choi, J.Â Lee, W.Â Kim, J.Â Lee, H.Â Heo, and K.Â Lee, ``Neural analysis and synthesis: Reconstructing speech from self-supervised representations,'' <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol.Â 34, pp. 16â€‰251â€“16â€‰265, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
H.-S. Choi, J.Â Yang, J.Â Lee, and H.Â Kim, ``Nansy++: Unified voice synthesis with neural analysis and synthesis,'' in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations</em>, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
M.Â Xu, F.Â Homae, R.-i. Hashimoto, and H.Â Hagiwara, ``Acoustic cues for the recognition of self-voice and other-voice,'' <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Frontiers in psychology</em>, vol.Â 4, p. 735, 2013.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
C.-C. Hsu, H.-T. Hwang, Y.-C. Wu, Y.Â Tsao, and H.-M. Wang, ``Voice conversion from non-parallel corpora using variational auto-encoder,'' in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)</em>.Â Â Â IEEE, 2016, pp. 1â€“6.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
K.Â Qian, Y.Â Zhang, S.Â Chang, X.Â Yang, and M.Â Hasegawa-Johnson, ``Autovc: Zero-shot voice style transfer with only autoencoder loss,'' in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.Â Â Â PMLR, 2019, pp. 5210â€“5219.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A.Â T. Liu, P.-c. Hsu, and H.-y. Lee, ``Unsupervised end-to-end learning of discrete linguistic units for voice conversion,'' <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.11563</em>, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
D.Â Wang, L.Â Deng, Y.Â T. Yeung, X.Â Chen, X.Â Liu, and H.Â Meng, ``Vqmivc: Vector quantization and mutual information-based unsupervised speech representation disentanglement for one-shot voice conversion,'' <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.10132</em>, 2021.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
W.-N. Hsu, B.Â Bolte, Y.-H.Â H. Tsai, K.Â Lakhotia, R.Â Salakhutdinov, and A.Â Mohamed, ``Hubert: Self-supervised speech representation learning by masked prediction of hidden units,'' <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.Â 29, pp. 3451â€“3460, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
K.Â Qian, Y.Â Zhang, H.Â Gao, J.Â Ni, C.-I. Lai, D.Â Cox, M.Â Hasegawa-Johnson, and S.Â Chang, ``Contentvec: An improved self-supervised speech representation by disentangling speakers,'' in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.Â Â Â PMLR, 2022, pp. 18â€‰003â€“18â€‰017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
B.Â van Niekerk, M.-A. Carbonneau, J.Â ZaÃ¯di, M.Â Baas, H.Â SeutÃ©, and H.Â Kamper, ``A comparison of discrete and soft speech units for improved voice conversion,'' in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2022, pp. 6562â€“6566.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S.Â Campanella and P.Â Belin, ``Integrating face and voice in person perception,'' <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Trends in cognitive sciences</em>, vol.Â 11, no.Â 12, pp. 535â€“543, 2007.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
L.Â W. Mavica and E.Â Barenholtz, ``Matching voice and face identity from static images.'' <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Journal of Experimental Psychology: Human Perception and Performance</em>, vol.Â 39, no.Â 2, p. 307, 2013.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
H.Â M. Smith, A.Â K. Dunn, T.Â Baguley, and P.Â C. Stacey, ``Matching novel face and voice identity using static and dynamic facial images,'' <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Attention, Perception, &amp; Psychophysics</em>, vol.Â 78, pp. 868â€“879, 2016.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
H.-S. Choi, C.Â Park, and K.Â Lee, ``From inference to generation: End-to-end fully self-supervised generation of human face from speech,'' in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J.Â Wang, Z.Â Wang, X.Â Hu, X.Â Li, Q.Â Fang, and L.Â Liu, ``Residual-guided personalized speech synthesis based on face image,'' in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2022, pp. 4743â€“4747.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
J.Â Kim, J.Â Kong, and J.Â Son, ``Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech,'' in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.Â Â Â PMLR, 2021, pp. 5530â€“5540.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
P.Â Shaw, J.Â Uszkoreit, and A.Â Vaswani, ``Self-attention with relative position representations,'' <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.02155</em>, 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
L.Â Dinh, J.Â Sohl-Dickstein, and S.Â Bengio, ``Density estimation using real nvp,'' <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1605.08803</em>, 2016.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
A.Â Dosovitskiy, L.Â Beyer, A.Â Kolesnikov, D.Â Weissenborn, X.Â Zhai, T.Â Unterthiner, M.Â Dehghani, M.Â Minderer, G.Â Heigold, S.Â Gelly <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``An image is worth 16x16 words: Transformers for image recognition at scale,'' <em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.11929</em>, 2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J.Â Kong, J.Â Kim, and J.Â Bae, ``Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis,'' <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol.Â 33, pp. 17â€‰022â€“17â€‰033, 2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
X.Â Wang, S.Â Takaki, and J.Â Yamagishi, ``Neural source-filter-based waveform model for statistical parametric speech synthesis,'' in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2019, pp. 5916â€“5920.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
T.Â Afouras, J.Â S. Chung, and A.Â Zisserman, ``Lrs3-ted: a large-scale dataset for visual speech recognition,'' <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.00496</em>, 2018.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.09801" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.09802" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.09802">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.09802" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.09803" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 16:13:24 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
