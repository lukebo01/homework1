<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.17716] Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment</title><meta property="og:description" content="Speech emotion recognition (SER) systems often struggle in real-world environments, where ambient noise severely degrades their performance. This paper explores a novel approach that exploits prior knowledge of testing…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.17716">

<!--Generated on Mon Aug  5 17:40:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Seong-Gyun Leem 
<br class="ltx_break">Department of Electrical and Computer Engineering 
<br class="ltx_break">The University of Texas at Dallas
<br class="ltx_break">Richardson, TX 75080 USA 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">SeongGyun.Leem@utdallas.edu</span> 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_ERROR undefined">\And</span>Daniel Fulford 
<br class="ltx_break">Occupational Therapy and 
<br class="ltx_break">Psychological and Brain Sciences 
<br class="ltx_break">Boston University
<br class="ltx_break">MA 02215 USA
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">dfulford@bu.edu</span> 
<br class="ltx_break"><span id="id4.4.id4" class="ltx_ERROR undefined">\And</span>Jukka-Pekka Onnela 
<br class="ltx_break">Department of Biostatistics, 
<br class="ltx_break">Harvard T.H. Chan School of Public Health
<br class="ltx_break">Harvard University 
<br class="ltx_break">MA 02138 USA 
<br class="ltx_break"><span id="id5.5.id5" class="ltx_text ltx_font_typewriter">onnela@hsph.harvard.edu</span> 
<br class="ltx_break"><span id="id6.6.id6" class="ltx_ERROR undefined">\AND</span>David Gard 
<br class="ltx_break">Psychology Department 
<br class="ltx_break">San Francisco State University 
<br class="ltx_break">CA 94132 USA 
<br class="ltx_break"><span id="id7.7.id7" class="ltx_text ltx_font_italic">dgard@sfsu.edu</span> 
<br class="ltx_break"><span id="id8.8.id8" class="ltx_ERROR undefined">\And</span>Carlos Busso 
<br class="ltx_break">Department of Electrical and Computer Engineering 
<br class="ltx_break">The University of Texas at Dallas
<br class="ltx_break">Richardson, TX 75080 USA 
<br class="ltx_break"><span id="id9.9.id9" class="ltx_text ltx_font_typewriter">busso@utdallas.edu</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id10.id1" class="ltx_p"><em id="id10.id1.1" class="ltx_emph ltx_font_italic">Speech emotion recognition</em> (SER) systems often struggle in real-world environments, where ambient noise severely degrades their performance. This paper explores a novel approach that exploits prior knowledge of testing environments to maximize SER performance under noisy conditions. To address this task, we propose a text-guided, environment-aware training where an SER model is trained with contaminated speech samples and their paired noise description. We use a pre-trained text encoder to extract the text-based environment embedding and then fuse it to a transformer-based SER model during training and inference. We demonstrate the effectiveness of our approach through our experiment with the MSP-Podcast corpus and real-world additive noise samples collected from the Freesound repository. Our experiment indicates that the text-based environment descriptions processed by a <em id="id10.id1.2" class="ltx_emph ltx_font_italic">large language model</em> (LLM) produce representations that improve the noise-robustness of the SER system. In addition, our proposed approach with an LLM yields better performance than our environment-agnostic baselines, especially in low <em id="id10.id1.3" class="ltx_emph ltx_font_italic">signal-to-noise ratio</em> (SNR) conditions. When testing at -5dB SNR level, our proposed method shows better performance than our best baseline model by 31.8 % (arousal), 23.5% (dominance), and 9.5% (valence).</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.3" class="ltx_p"><em id="p1.3.1" class="ltx_emph ltx_font_bold ltx_font_italic">K</em><span id="p1.3.2" class="ltx_text ltx_font_bold">eywords</span> Speech emotion recognition  <math id="p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation></semantics></math>
noise-robustness  <math id="p1.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation></semantics></math>
text-guided training  <math id="p1.3.m3.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.3.m3.1a"><mo id="p1.3.m3.1.1" xref="p1.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.3.m3.1b"><ci id="p1.3.m3.1.1.cmml" xref="p1.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.3.m3.1c">\cdot</annotation></semantics></math>
multi-modal</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Speech <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">emotion recogntion</em> (SER) systems have highly improved with the help of pre-trained speech representation models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and the creation of larger emotional speech databases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Recently, there has been increased interest in deploying SER systems in real-world applications, opening opportunities across many domains, such as digital assistants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, health care applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, and security and defense. One important barrier in this direction is the degradation of SER performance in real-world environments caused by multiple types of non-stationary background noise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Several solutions have been proposed to improve the robustness of SER systems against acoustic noise. The solutions include data augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, feature enhancement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, feature selection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, and domain adaptation approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Since transformer-based speech representation models have been successfully used in speech problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, many studies have also worked on increasing the noise robustness of SER systems built with pre-trained speech representation models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. These approaches can increase the performance of transformer-based SER models in target noisy conditions. However, it is challenging to use these models in scenarios with multiple noisy environments since a transformer-based SER model requires important resources to adapt and store its parameters for each target environment. To address multiple noise types in a single SER model, Leem et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> proposed environment-agnostic and -specific adapters. Their work showed that leveraging the prior knowledge of the testing condition is important for an SER model’s adaptation to multiple noisy environments.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">This paper focuses on how to effectively use the prior knowledge of a testing condition for an SER model that is adapted to multiple environments. The prior knowledge is used as a mechanism for zero-shot learning in new environments with types of noises not considered while training the models. It also provides the mechanism to indirectly identify similar environmental conditions during training (e.g., noise in a bus station and a train station). Exploring this problem, we investigate using text-based environment descriptions as the prior knowledge for a noise-robust SER system. Using natural language prompts during training has shown potential in image classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, sound event classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, and several speech processing downstream tasks, including keyword spotting, and speaker counting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Natural language supervision is also applicable to SER tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. All these studies indicate that exploiting text information is a promising strategy to SER systems. We propose a <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">text-guided environment-aware training</em> (TG-EAT) strategy to improve the noise robustness of an SER model with text descriptions. We focus on the prediction of arousal (calm to active), valence (negative to positive), and dominance (weak to strong). TG-EAT uses noisy speech and its text-based environmental description to adapt the SER model. We use a pre-trained text encoder to extract the representation of text-based environment descriptions. This representation is combined with a transformer-based SER model. During adaptation, the SER model learns appropriate denoising functions with respect to the given environment description. During inference, we only need to change the template sentence to guide the SER model with testing environment information. We expect that the pre-trained text encoder can capture similar semantic information from environmental conditions included in the train set, allowing zero-shot environment learning for the SER model. This approach is expected to generalize the SER performance when tested in environmental conditions that are not included in the training process.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">Our experiment with the MSP-Podcast corpus shows that using text description of the testing environment can highly improve the SER performance, especially with <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">large language model</em> (LLM). In the -5dB <em id="S1.p4.1.2" class="ltx_emph ltx_font_italic">signal-to-noise ratio</em> (SNR) condition, our method improves the original SER model built with a <em id="S1.p4.1.3" class="ltx_emph ltx_font_italic">self-supervised learning</em> (SSL) representation by 163.6% for arousal, 200.0% for dominance, and 91.6% for valence. When we compare the proposed SER model with our best baseline, we observe improvements of 31.8 % for arousal, 23.5% for dominance, and 9.5% for valence (-5dB SNR level). With the text encoder from CLAP, pre-trained with paired audio, the SER model can achieve the best performance in the low SNR condition. Compared with freezing the text encoder, the fine-tuning approach improves performance by 72.2% for arousal, 91.6% for dominance, and 21.0% for valence under the -5dB SNR condition. Our solution is highly applicable to SER systems deployed in real-world applications. For example, systems can infer the testing environment from the <em id="S1.p4.1.4" class="ltx_emph ltx_font_italic">global positioning system</em> (GPS) information by using <em id="S1.p4.1.5" class="ltx_emph ltx_font_italic">geological information service</em> (GIS) mashups, such as OpenStreetMap <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. The main contributions of this study are:</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We explore using text embedding for an SER model to increase noise robustness in unseen conditions by explicitly leveraging the environment information.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We show the benefits of using LLM to improve SER performance under noisy conditions over using a pre-trained environment classifier, especially in a low SNR condition.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i3.p1.1" class="ltx_p">We show that fine-tuning the text encoder of CLAP can improve SER performance, leading to the possibility of using a paired audio encoder to deal with unknown testing environments.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">Our paper is organized as follows. Section <a href="#S2" title="2 Previous Work ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> describes studies relevant to SER in noisy conditions and text-guided training strategies. Section <a href="#S3" title="3 Proposed Method ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> describes the proposed approach, emphasizing the motivations and insights behind the TG-EAT framework. Section <a href="#S4" title="4 Experimental Settings ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> provides the experimental setting, including the database, baselines, and implementation details. Section <a href="#S5" title="5 Results ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents the results, discussing the clear benefits of the proposed strategy. Finally, Section <a href="#S6" title="6 Conclusions ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> concludes the paper, summarizing our study and providing future research directions inspired by the proposed approach.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Previous Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Speech Emotion Recognition under Noisy Environments</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">Increasing the noise robustness of an SER system is an essential task when deploying it in real-world applications. Previous studies have mainly focused on improving acoustic features for the SER model. Triantafyllopoulos et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> proposed to enhance noisy <em id="S2.SS1.p1.1.1" class="ltx_emph ltx_font_italic">low-level descriptors</em> (LLDs) for an SER model by using a <em id="S2.SS1.p1.1.2" class="ltx_emph ltx_font_italic">convolutional neural network</em> (CNN) with residual blocks. Pandharipande et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>proposed to discard noisy frames to increase the noise robustness of an SER model by using a voice activity detection module. Leem et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> proposed to select noise-robust LLDs by addressing the performance and robustness of each single LLD.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p">More recently, SER studies have mainly focused on using transformer-based speech representation models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, including Wav2Vec2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Such models have shown higher robustness against the small perturbation on the input speech than the traditional SER model with a Mel-spectrogram <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. Despite this trend, they still show performance differences from the ones tested in a clean environment. For this reason, studies are currently exploring strategies to improve the noise robustness of the pre-trained speech representation model. A common approach to address this issue is noise-aware training, where the clean training set is augmented with the noise sound during environment adaptation. Mitra et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> demonstrated that training a HuBERT-based SER model with noisy speech can highly improve the performance in low <em id="S2.SS1.p2.1.1" class="ltx_emph ltx_font_italic">signal-to-noise ratio</em> (SNR) conditions. Leem et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> proposed a contrastive teacher-student learning strategy to address the catastrophic forgetting issue when training a fine-tuned SER model with noisy speech. Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> proposed to dynamically change the distortion level of the augmented speech during adaptation based on the distortion metrics.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.1" class="ltx_p">The aforementioned methods focused on increasing the SER model’s robustness against a single target environment. They might not be the optimal solution for an SER model deployed on a real-world application since it is highly likely that this system will encounter multiple types of environmental noises. We focus on adapting a single transformer-based SER model to multiple noisy environments to efficiently deal with multiple types of environments. To address this issue, Leem et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> proposed to adapt the transformer-based SER model to multiple types of noises with skip connection adapters. They not only trained the SER model with multiple environments but also focused on leveraging the environmental information of the testing conditions to improve SER performance under noisy conditions. The results showed that using the environment-agnostic and -specific adapters with respect to the testing condition can improve the SER performance under noisy conditions. Such prior knowledge could be achieved using domain knowledge or <em id="S2.SS1.p3.1.1" class="ltx_emph ltx_font_italic">global positioning system</em> (GPS) information. Their result showed that using environmental information during inference is important for a SER model to perform well under noisy conditions. This work indicates that leveraging the prior knowledge of the testing condition is also important for a noise-robust SER model, as well as training it with multiple types of noises. This is beneficial for an SER model deployed on real-world applications where the system can exploit the domain knowledge of the testing environment and the <em id="S2.SS1.p3.1.2" class="ltx_emph ltx_font_italic">global positioning system</em> (GPS) information.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.p4.1" class="ltx_p">This paper also explores the multi-condition training approach where the fine-tuned SER model is adapted to multiple types of noise. Different from other methods, our strategy relies on a text embedding that describes the testing environment to deal with multiple unseen environments.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Text-Guided Training</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">As we discussed in Section <a href="#S2.SS1" title="2.1 Speech Emotion Recognition under Noisy Environments ‣ 2 Previous Work ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>, exploiting environmental information can improve SER performance in a noisy environment. This paper mainly focuses on using text prompts to infuse environmental information into an SER model. Using natural language prompts does not require the recognition model to use a fixed set of predetermined labels during training. <em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">Contrastive language-image pre-training</em> (CLIP) is a good example of this approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. It consists of an image encoder and a text encoder, trained with pairs of images and their corresponding text descriptions. These encoders are trained in a contrastive learning manner, which maximizes the similarity of both representations if the image and the description are paired and minimizes the similarity if they are unpaired. After training, these encoders can perform zero-shot classification by checking the similarity between the given image and the candidate prompts. The study of Radford et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> used the following prompt template: <span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_italic">“A photo of a </span>{<span id="S2.SS2.p1.1.3" class="ltx_text ltx_font_italic">label</span>}<span id="S2.SS2.p1.1.4" class="ltx_text ltx_font_italic">”</span>. They calculate the similarity between the representation from the given image and the representations from the prompts with different {<span id="S2.SS2.p1.1.5" class="ltx_text ltx_font_italic">label</span>}, selecting the {<span id="S2.SS2.p1.1.6" class="ltx_text ltx_font_italic">label</span>} that shows the maximum similarity.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p">The contrastive pre-training strategy with natural language supervision is also successful in universal audio and speech processing. Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> demonstrated that pre-training audio and text encoder with natural language guidance could improve audio classification performance. The study of Elizalde et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> showed that such natural language guidance can improve speech processing tasks, including keyword spotting, speaker counting, and SER tasks.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p">Previous studies have found that natural language supervision can apply to SER tasks. Stanley et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> used word embeddings to encode emotional labels for SER model. Gong et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> used <em id="S2.SS2.p3.1.1" class="ltx_emph ltx_font_italic">large language model</em> (LLM) to infer weak emotion labels for unlabeled data for weakly-supervised learning of an SER model. All these findings have shown that exploiting text information is highly applicable to SER systems. To the best knowledge of the authors, the use of natural language supervision to address SER robustness against unknown noisy environments is a novel research direction.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Method</h2>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2407.17716/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="342" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Our proposed text-guided environment-aware training framework. The environment representation is concatenated with the output of the convolutional feature encoder.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">This paper proposes <em id="S3.p1.1.1" class="ltx_emph ltx_font_italic">text-guided environment-aware training</em> (TG-EAT), which leverages environmental information to improve an SER model in noisy conditions. Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Proposed Method ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates our proposed TG-EAT framework, which uses a pair of noisy speech and its corresponding environmental description. The text embedding extracted from the environmental description is combined with the acoustic representation in the SER model, allowing it to denoise the representation for the given environmental description.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p">The key contribution of this study is how we use the text description from the target environment. We used prompts to generate the text description where the target environment is changed. As a preliminary experiment, we tested different prompts to describe the target environment such as <em id="S3.p2.1.1" class="ltx_emph ltx_font_italic">“The type of background noise is <span id="S3.p2.1.1.1" class="ltx_text ltx_framed ltx_framed_underline"><span id="S3.p2.1.1.1.1" class="ltx_text ltx_font_upright">{</span>environment<span id="S3.p2.1.1.1.2" class="ltx_text ltx_font_upright">}</span></span>,”</em> or <em id="S3.p2.1.2" class="ltx_emph ltx_font_italic">“The input is recorded with a sound of <span id="S3.p2.1.2.1" class="ltx_text ltx_framed ltx_framed_underline"><span id="S3.p2.1.2.1.1" class="ltx_text ltx_font_upright">{</span>environment<span id="S3.p2.1.2.1.2" class="ltx_text ltx_font_upright">}</span></span>.”</em> We change <em id="S3.p2.1.3" class="ltx_emph ltx_font_italic"><span id="S3.p2.1.3.1" class="ltx_text ltx_framed ltx_framed_underline"><span id="S3.p2.1.3.1.1" class="ltx_text ltx_font_upright">{</span>environment<span id="S3.p2.1.3.1.2" class="ltx_text ltx_font_upright">}</span></span></em> in the prompts according to the target environment during training and testing. We found that all the prompts showed similar emotion recognition performance for all the attributes. Therefore, we consistently use the following prompt in this study: <em id="S3.p2.1.4" class="ltx_emph ltx_font_italic">“This speech is recorded in <span id="S3.p2.1.4.1" class="ltx_text ltx_framed ltx_framed_underline"><span id="S3.p2.1.4.1.1" class="ltx_text ltx_font_upright">{</span>environment<span id="S3.p2.1.4.1.2" class="ltx_text ltx_font_upright">}</span></span>.”</em> We extract the text-based environment embedding from this text description using a pre-trained text encoder. We test two different text representations: <em id="S3.p2.1.5" class="ltx_emph ltx_font_italic">contrastive learning</em> (CL)-based representation and LLM-based representation. For the CL-based representation, we use the text encoder pre-trained with the <em id="S3.p2.1.6" class="ltx_emph ltx_font_italic">contrastive language audio pre-training</em> (CLAP) strategy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. CLAP consists of an audio encoder and a text encoder. It uses a pair of acoustic events and their text description during pre-training (e.g., <em id="S3.p2.1.7" class="ltx_emph ltx_font_italic">bird chirping sound</em> with the description, “Bird is chirping in the given audio”). With these audio-text pairs, the training objective is to maximize the similarity between the audio and text representation if they are from the same pair and minimize it if they are from a different pair. Since CLAP uses an audio-text pair during pre-training, we assume that its text encoder can generate an appropriate representation from the given environment description coherent with the target acoustic condition. This paper uses the pre-trained text encoder from the unfused CLAP model proposed in the study of Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. For the LLM-based representation, we use the encoder from the pre-trained RoBERTa model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. RoBERTa is pre-trained with <em id="S3.p2.1.8" class="ltx_emph ltx_font_italic">masked language modeling</em> (MLM) and <em id="S3.p2.1.9" class="ltx_emph ltx_font_italic">next sentence prediction</em> (NSP) tasks. RoBERTa has shown good performance in various benchmarks for evaluating natural language understanding systems, such as GLUE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. Although it is not pre-trained with audio data, we assume that its encoder can extract enriched semantic information from the given prompt. We use RoBERTa-large, which has 24 transformer layers. For each text encoder, we use the same tokenizer used in its pre-training to tokenize the text description of the environment. We extract token-level text embeddings from the tokenized prompt and then apply average pooling, resulting in a single representation vector for each prompt.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p">After the environmental representation is obtained, the next step is to introduce this information into the model. We mainly focus on a transformer-based SER model, which has shown good performance in SER tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. An important task is to fine-tune the model with clean and emotional speech data. We first fine-tune the SER model with clean speech to maximize the <em id="S3.p3.1.1" class="ltx_emph ltx_font_italic">concordance correlation coefficient</em> (CCC) between the predicted and the ground-truth emotional attribute scores of arousal, dominance, and valence. After fine-tuning with clean speech, the SER model is continuously updated with the training set contaminated with multiple types of noise and their corresponding text description. We insert the text representation from the given environment description into the fine-tuned transformer-based SER model. We achieve this goal by combining the text embedding with the acoustic representation, which is the output of the convolutional encoder. We apply trainable linear projection to the text embedding to match its dimension to the acoustic representation embeddings. We concatenate the projected text embedding to the acoustic representation embeddings along the time axis, then feed them into the transformer encoder. We update the transformer encoder and the downstream head with the concatenated embeddings. We use the same training objective as the one used when training with clean speech. From this framework, we want to evaluate if the SER model can learn the denoising function given a noisy acoustic representation with its text embedding.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Settings</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Data preparation</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">Our experiment uses the MSP-Podcast corpus, which consists of natural and diverse emotional speech samples from various podcast recordings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. The audios do not include background music or overlapped speech, and their predicted SNR is above 20 dB. We consider this corpus a clean emotion speech database for these reasons. This study focuses on predicting the emotional attributes of arousal (calm to active), dominance (weak to strong), and valence (negative to positive). Labels for these attributes were annotated by at least five raters using a seven-point Likert scale. We average the scores provided by raters for each sample to establish its ground truth values. This paper uses version 1.10 of the corpus, which consists of 104,267 annotated utterances. We use the train set to fine-tune the pre-trained speech representation model, using it as the original SER model. We use samples from the development set to select the best model during the fine-tuning process.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p">We simulate real-world noisy environments by collecting noise sounds from the Freesound repository <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, which contains publicly available ambient noise sounds. We use diverse queries related to each environment to collect noise sounds, including indoor, outdoor, and in-vehicle conditions. We use 20 noisy environments to contaminate the training and development set, consisting of {mall, restaurant, office, airport, station, city, park, street, traffic, home, kitchen, living room, bathroom, bedroom, metro, bus, car, construction site, pedestrian, beach}. For the evaluation, we use six environments, including {plaza, garden, school, tram, sea, boat}. Although these noise sounds are not used during adaptation, they have common characteristics with the noise sounds used during adaptation (e.g. indoor, outdoor, or in-vehicle conditions). We want to evaluate if our proposed method can capture this semantic similarity during the inference. We randomly pick the noise sounds to contaminate the Test1 set of the clean MSP-Podcast corpus. We repeat this process 10 times, creating 10 different sets for three different SNR levels, 5dB, 0dB, and -5dB.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Fine-Tuning Transformer-Based Architecture</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">We implement our proposed approach with two different pre-trained speech representation models: wav2vec2-large-robust <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> and the wavlm-base-plus models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. The wav2vec2-large-robust model has shown good performance in the emotional attribute prediction task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. The wavlm-base-plus model has shown good performance for emotion recognition in the <em id="S4.SS2.p1.1.1" class="ltx_emph ltx_font_italic">speech processing universal performance benchmark</em> (SUPERB) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. This model is pre-trained with noise, creating representations that are expected to be more robust to noise than other SSL representations. We fine-tune the transformer encoder of the pre-trained speech representation model and the downstream head with the clean version of the MSP-Podcast corpus. For wav2vec2-large-robust, we remove the top 12 transformer layers from the model to preserve the recognition performance with fewer parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. We import the pre-trained models from the HuggingFace library <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. We use two fully connected layers for the downstream head, where each layer has 512 nodes, layer normalization, and the <em id="S4.SS2.p1.1.2" class="ltx_emph ltx_font_italic">rectified linear unit</em> (ReLU) as the activation function. We use dropout in all the hidden layers to increase regularization, with a rate set to <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="p=0.5" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><eq id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></eq><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝑝</ci><cn type="float" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">p=0.5</annotation></semantics></math>. We use a linear output layer with three nodes to predict emotional attribute scores, where each node predicts the scores for arousal, dominance, and valence. We apply average pooling on top of the last transformer layer’s representation to feed it to the downstream head.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.3" class="ltx_p">During fine-tuning, we apply Z-normalization to the raw waveform by using the mean and standard deviation estimated over the training set and min-max normalization to the emotional labels, mapping them to the range of 0 to 1. We use 32 utterances per mini-batch and update the model for ten epochs. We use the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> with a learning rate warmup scheduling, which shows good performance when fine-tuning a pre-trained transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. For the first 1,000 mini-batches, we linearly increase the learning rate from <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="1e^{-8}" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mn id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">​</mo><msup id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml"><mi id="S4.SS2.p2.1.m1.1.1.3.2" xref="S4.SS2.p2.1.m1.1.1.3.2.cmml">e</mi><mrow id="S4.SS2.p2.1.m1.1.1.3.3" xref="S4.SS2.p2.1.m1.1.1.3.3.cmml"><mo id="S4.SS2.p2.1.m1.1.1.3.3a" xref="S4.SS2.p2.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS2.p2.1.m1.1.1.3.3.2" xref="S4.SS2.p2.1.m1.1.1.3.3.2.cmml">8</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><times id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">1</cn><apply id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.1.3.1.cmml" xref="S4.SS2.p2.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS2.p2.1.m1.1.1.3.2.cmml" xref="S4.SS2.p2.1.m1.1.1.3.2">𝑒</ci><apply id="S4.SS2.p2.1.m1.1.1.3.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3.3"><minus id="S4.SS2.p2.1.m1.1.1.3.3.1.cmml" xref="S4.SS2.p2.1.m1.1.1.3.3"></minus><cn type="integer" id="S4.SS2.p2.1.m1.1.1.3.3.2.cmml" xref="S4.SS2.p2.1.m1.1.1.3.3.2">8</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">1e^{-8}</annotation></semantics></math> to <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="1e^{-5}" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mn id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">​</mo><msup id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml"><mi id="S4.SS2.p2.2.m2.1.1.3.2" xref="S4.SS2.p2.2.m2.1.1.3.2.cmml">e</mi><mrow id="S4.SS2.p2.2.m2.1.1.3.3" xref="S4.SS2.p2.2.m2.1.1.3.3.cmml"><mo id="S4.SS2.p2.2.m2.1.1.3.3a" xref="S4.SS2.p2.2.m2.1.1.3.3.cmml">−</mo><mn id="S4.SS2.p2.2.m2.1.1.3.3.2" xref="S4.SS2.p2.2.m2.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><times id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">1</cn><apply id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m2.1.1.3.1.cmml" xref="S4.SS2.p2.2.m2.1.1.3">superscript</csymbol><ci id="S4.SS2.p2.2.m2.1.1.3.2.cmml" xref="S4.SS2.p2.2.m2.1.1.3.2">𝑒</ci><apply id="S4.SS2.p2.2.m2.1.1.3.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3.3"><minus id="S4.SS2.p2.2.m2.1.1.3.3.1.cmml" xref="S4.SS2.p2.2.m2.1.1.3.3"></minus><cn type="integer" id="S4.SS2.p2.2.m2.1.1.3.3.2.cmml" xref="S4.SS2.p2.2.m2.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">1e^{-5}</annotation></semantics></math>. After the 1,000 mini-batches, we fix the learning rate to <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="1e^{-5}" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mrow id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mn id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m3.1.1.1" xref="S4.SS2.p2.3.m3.1.1.1.cmml">​</mo><msup id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml"><mi id="S4.SS2.p2.3.m3.1.1.3.2" xref="S4.SS2.p2.3.m3.1.1.3.2.cmml">e</mi><mrow id="S4.SS2.p2.3.m3.1.1.3.3" xref="S4.SS2.p2.3.m3.1.1.3.3.cmml"><mo id="S4.SS2.p2.3.m3.1.1.3.3a" xref="S4.SS2.p2.3.m3.1.1.3.3.cmml">−</mo><mn id="S4.SS2.p2.3.m3.1.1.3.3.2" xref="S4.SS2.p2.3.m3.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><times id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1"></times><cn type="integer" id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">1</cn><apply id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p2.3.m3.1.1.3.1.cmml" xref="S4.SS2.p2.3.m3.1.1.3">superscript</csymbol><ci id="S4.SS2.p2.3.m3.1.1.3.2.cmml" xref="S4.SS2.p2.3.m3.1.1.3.2">𝑒</ci><apply id="S4.SS2.p2.3.m3.1.1.3.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3.3"><minus id="S4.SS2.p2.3.m3.1.1.3.3.1.cmml" xref="S4.SS2.p2.3.m3.1.1.3.3"></minus><cn type="integer" id="S4.SS2.p2.3.m3.1.1.3.3.2.cmml" xref="S4.SS2.p2.3.m3.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">1e^{-5}</annotation></semantics></math>. All of our experiments are conducted on a single NVIDIA GeForce RTX 3090.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Text-Guided Environment-Aware Training</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p">After fine-tuning with the clean speech, we adapt the SER model to the noisy environmental conditions. We randomly select one of the 20 noise conditions for each mini-batch during adaptation. We then use 32 different noise samples in the selected condition to contaminate 32 clean speech samples from the training set of the MSP-Podcast corpus. We build text prompts with respect to the picked environment for each mini-batch, as described in Section <a href="#S3" title="3 Proposed Method ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. In real-world applications, it is difficult to assume the exact SNR level of the testing condition. Therefore, we introduce an SER mismatch between our experiment’s adaptation and testing stages. We randomly select the SNR level for the adaptation of the models among these options: {2.5, 7.5, 12.5}dB. We use the same hyperparameters as the ones used for fine-tuning the SER model with clean speech during adaptation. We tested two variations of our proposed text-guided environment-aware training: the CL-based representation <span id="S4.SS3.p1.1.1" class="ltx_text ltx_framed ltx_framed_underline">TG-EAT-CL</span>, and the LLM-based representation <span id="S4.SS3.p1.1.2" class="ltx_text ltx_framed ltx_framed_underline">TG-EAT-LLM</span>.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Baselines</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p"><span id="S4.SS4.p1.1.1" class="ltx_text ltx_framed ltx_framed_underline">Original</span>: This model fine-tunes the model with clean emotional speech, with no adaptation to the noisy conditions.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.1" class="ltx_p"><span id="S4.SS4.p2.1.1" class="ltx_text ltx_framed ltx_framed_underline">Retrain the original model with noisy speech (RT)</span>: This baseline updates the transformer encoder and the downstream head of the Original model with noisy speech. It does not use environmental information during adaptation and inference. As described in Section <a href="#S4.SS1" title="4.1 Data preparation ‣ 4 Experimental Settings ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>, it uses 20 environmental conditions for adaptation. The evaluation uses six other environmental conditions.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para ltx_noindent">
<p id="S4.SS4.p3.1" class="ltx_p"><span id="S4.SS4.p3.1.1" class="ltx_text ltx_framed ltx_framed_underline">Domain adversarial training (DAT)</span>: Inspired by Huang et al. ’s work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, we test a domain adversarial training strategy to adapt an SER model to multiple noisy conditions. Along with the downstream head for the SER task, we attach an environment classifier on top of the average-pooled transformer representations. The environment classifier has the same architecture as the downstream head for the SER task. The environment classifier is trained to minimize the cross-entropy loss between the predicted and the ground-truth noise types. We applied a <em id="S4.SS4.p3.1.2" class="ltx_emph ltx_font_italic">gradient reversal layer</em> (GRL) between the environment classifier and the transformer encoder to train the transformer encoder to normalize the environment information in the resulting representations. Like the RT baseline, this baseline does not use environmental information during inference.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Average CCC of the ten experiments for the proposed text-guided environment-aware methods and the baselines. We report the performance with models implemented using either the wav2vec2-large-robust or wavlm-base-plus feature vectors. We denote with <sup id="S4.T1.88.1" class="ltx_sup"><span id="S4.T1.88.1.1" class="ltx_text ltx_font_italic">∗</span></sup>, <sup id="S4.T1.89.2" class="ltx_sup"><span id="S4.T1.89.2.1" class="ltx_text ltx_font_italic">†</span></sup>, and <sup id="S4.T1.90.3" class="ltx_sup"><span id="S4.T1.90.3.1" class="ltx_text ltx_font_italic">⋆</span></sup> when a model shows significantly better performance than the Original, RT, and DAT models, respectively. We highlight in bold the best performance per condition.</figcaption>
<table id="S4.T1.84" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.84.79.1" class="ltx_tr">
<th id="S4.T1.84.79.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SNR</th>
<th id="S4.T1.84.79.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Model</th>
<td id="S4.T1.84.79.1.3" class="ltx_td ltx_align_center ltx_border_t">Arousal</td>
<td id="S4.T1.84.79.1.4" class="ltx_td ltx_align_center ltx_border_t">Dominance</td>
<td id="S4.T1.84.79.1.5" class="ltx_td ltx_align_center ltx_border_t">Valence</td>
</tr>
<tr id="S4.T1.84.80.2" class="ltx_tr">
<th id="S4.T1.84.80.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T1.84.80.2.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt"></th>
<td id="S4.T1.84.80.2.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="3">wav2vec2-large-robust</td>
</tr>
<tr id="S4.T1.84.81.3" class="ltx_tr">
<th id="S4.T1.84.81.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="5"><span id="S4.T1.84.81.3.1.1" class="ltx_text">
<span id="S4.T1.84.81.3.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:17.6pt;vertical-align:-5.3pt;"><span class="ltx_transformed_inner" style="width:17.6pt;transform:translate(-5.35pt,0pt) rotate(-90deg) ;">
<span id="S4.T1.84.81.3.1.1.1.1" class="ltx_p">5dB</span>
</span></span></span></th>
<th id="S4.T1.84.81.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Original</th>
<td id="S4.T1.84.81.3.3" class="ltx_td ltx_align_center ltx_border_t">0.59</td>
<td id="S4.T1.84.81.3.4" class="ltx_td ltx_align_center ltx_border_t">0.51</td>
<td id="S4.T1.84.81.3.5" class="ltx_td ltx_align_center ltx_border_t">0.40</td>
</tr>
<tr id="S4.T1.7.1" class="ltx_tr">
<th id="S4.T1.7.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">RT</th>
<td id="S4.T1.7.1.3" class="ltx_td ltx_align_center">0.60</td>
<td id="S4.T1.7.1.4" class="ltx_td ltx_align_center">0.52</td>
<td id="S4.T1.7.1.1" class="ltx_td ltx_align_center">0.45<sup id="S4.T1.7.1.1.1" class="ltx_sup"><span id="S4.T1.7.1.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
</tr>
<tr id="S4.T1.8.2" class="ltx_tr">
<th id="S4.T1.8.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">DAT</th>
<td id="S4.T1.8.2.3" class="ltx_td ltx_align_center">0.61</td>
<td id="S4.T1.8.2.4" class="ltx_td ltx_align_center">0.5</td>
<td id="S4.T1.8.2.1" class="ltx_td ltx_align_center">0.45<sup id="S4.T1.8.2.1.1" class="ltx_sup"><span id="S4.T1.8.2.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
</tr>
<tr id="S4.T1.10.4" class="ltx_tr">
<th id="S4.T1.10.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-CL</th>
<td id="S4.T1.9.3.1" class="ltx_td ltx_align_center">0.62<sup id="S4.T1.9.3.1.1" class="ltx_sup"><span id="S4.T1.9.3.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T1.10.4.4" class="ltx_td ltx_align_center">0.52</td>
<td id="S4.T1.10.4.2" class="ltx_td ltx_align_center">0.46<sup id="S4.T1.10.4.2.1" class="ltx_sup"><span id="S4.T1.10.4.2.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
</tr>
<tr id="S4.T1.13.7" class="ltx_tr">
<th id="S4.T1.13.7.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-LLM</th>
<td id="S4.T1.11.5.1" class="ltx_td ltx_align_center"><span id="S4.T1.11.5.1.1" class="ltx_text ltx_font_bold">0.63<sup id="S4.T1.11.5.1.1.1" class="ltx_sup"><span id="S4.T1.11.5.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup></span></td>
<td id="S4.T1.12.6.2" class="ltx_td ltx_align_center"><span id="S4.T1.12.6.2.1" class="ltx_text ltx_font_bold">0.53<sup id="S4.T1.12.6.2.1.1" class="ltx_sup"><span id="S4.T1.12.6.2.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup></span></td>
<td id="S4.T1.13.7.3" class="ltx_td ltx_align_center"><span id="S4.T1.13.7.3.1" class="ltx_text ltx_font_bold">0.47<sup id="S4.T1.13.7.3.1.1" class="ltx_sup"><span id="S4.T1.13.7.3.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup></span></td>
</tr>
<tr id="S4.T1.84.82.4" class="ltx_tr">
<th id="S4.T1.84.82.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="5"><span id="S4.T1.84.82.4.1.1" class="ltx_text">
<span id="S4.T1.84.82.4.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:17.6pt;vertical-align:-5.3pt;"><span class="ltx_transformed_inner" style="width:17.6pt;transform:translate(-5.35pt,0pt) rotate(-90deg) ;">
<span id="S4.T1.84.82.4.1.1.1.1" class="ltx_p">0dB</span>
</span></span></span></th>
<th id="S4.T1.84.82.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Original</th>
<td id="S4.T1.84.82.4.3" class="ltx_td ltx_align_center ltx_border_t">0.53</td>
<td id="S4.T1.84.82.4.4" class="ltx_td ltx_align_center ltx_border_t">0.47</td>
<td id="S4.T1.84.82.4.5" class="ltx_td ltx_align_center ltx_border_t">0.33</td>
</tr>
<tr id="S4.T1.15.9" class="ltx_tr">
<th id="S4.T1.15.9.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">RT</th>
<td id="S4.T1.14.8.1" class="ltx_td ltx_align_center">0.56<sup id="S4.T1.14.8.1.1" class="ltx_sup"><span id="S4.T1.14.8.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T1.15.9.4" class="ltx_td ltx_align_center">0.46</td>
<td id="S4.T1.15.9.2" class="ltx_td ltx_align_center">0.4<sup id="S4.T1.15.9.2.1" class="ltx_sup"><span id="S4.T1.15.9.2.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
</tr>
<tr id="S4.T1.16.10" class="ltx_tr">
<th id="S4.T1.16.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">DAT</th>
<td id="S4.T1.16.10.3" class="ltx_td ltx_align_center">0.54</td>
<td id="S4.T1.16.10.4" class="ltx_td ltx_align_center">0.44</td>
<td id="S4.T1.16.10.1" class="ltx_td ltx_align_center">0.4<sup id="S4.T1.16.10.1.1" class="ltx_sup"><span id="S4.T1.16.10.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
</tr>
<tr id="S4.T1.17.11" class="ltx_tr">
<th id="S4.T1.17.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-CL</th>
<td id="S4.T1.17.11.3" class="ltx_td ltx_align_center">0.52</td>
<td id="S4.T1.17.11.4" class="ltx_td ltx_align_center">0.42</td>
<td id="S4.T1.17.11.1" class="ltx_td ltx_align_center">0.4<sup id="S4.T1.17.11.1.1" class="ltx_sup"><span id="S4.T1.17.11.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
</tr>
<tr id="S4.T1.22.16" class="ltx_tr">
<th id="S4.T1.22.16.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-LLM</th>
<td id="S4.T1.19.13.2" class="ltx_td ltx_align_center"><span id="S4.T1.19.13.2.2" class="ltx_text ltx_font_bold">0.57<sup id="S4.T1.19.13.2.2.1" class="ltx_sup"><span id="S4.T1.19.13.2.2.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup><sup id="S4.T1.19.13.2.2.2" class="ltx_sup"><span id="S4.T1.19.13.2.2.2.1" class="ltx_text ltx_font_medium ltx_font_italic">⋆</span></sup></span></td>
<td id="S4.T1.21.15.4" class="ltx_td ltx_align_center"><span id="S4.T1.21.15.4.2" class="ltx_text ltx_font_bold">0.48<sup id="S4.T1.21.15.4.2.1" class="ltx_sup"><span id="S4.T1.21.15.4.2.1.1" class="ltx_text ltx_font_medium ltx_font_italic">†</span></sup><sup id="S4.T1.21.15.4.2.2" class="ltx_sup"><span id="S4.T1.21.15.4.2.2.1" class="ltx_text ltx_font_medium ltx_font_italic">⋆</span></sup></span></td>
<td id="S4.T1.22.16.5" class="ltx_td ltx_align_center"><span id="S4.T1.22.16.5.1" class="ltx_text ltx_font_bold">0.41<sup id="S4.T1.22.16.5.1.1" class="ltx_sup"><span id="S4.T1.22.16.5.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup></span></td>
</tr>
<tr id="S4.T1.84.83.5" class="ltx_tr">
<th id="S4.T1.84.83.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="5"><span id="S4.T1.84.83.5.1.1" class="ltx_text">
<span id="S4.T1.84.83.5.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:21pt;vertical-align:-7.0pt;"><span class="ltx_transformed_inner" style="width:21.0pt;transform:translate(-7.01pt,0pt) rotate(-90deg) ;">
<span id="S4.T1.84.83.5.1.1.1.1" class="ltx_p">-5dB</span>
</span></span></span></th>
<th id="S4.T1.84.83.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Original</th>
<td id="S4.T1.84.83.5.3" class="ltx_td ltx_align_center ltx_border_t">0.27</td>
<td id="S4.T1.84.83.5.4" class="ltx_td ltx_align_center ltx_border_t">0.25</td>
<td id="S4.T1.84.83.5.5" class="ltx_td ltx_align_center ltx_border_t">0.14</td>
</tr>
<tr id="S4.T1.23.17" class="ltx_tr">
<th id="S4.T1.23.17.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">RT</th>
<td id="S4.T1.23.17.3" class="ltx_td ltx_align_center">0.24</td>
<td id="S4.T1.23.17.4" class="ltx_td ltx_align_center">0.22</td>
<td id="S4.T1.23.17.1" class="ltx_td ltx_align_center">0.19<sup id="S4.T1.23.17.1.1" class="ltx_sup"><span id="S4.T1.23.17.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
</tr>
<tr id="S4.T1.24.18" class="ltx_tr">
<th id="S4.T1.24.18.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">DAT</th>
<td id="S4.T1.24.18.3" class="ltx_td ltx_align_center">0.24</td>
<td id="S4.T1.24.18.4" class="ltx_td ltx_align_center">0.22</td>
<td id="S4.T1.24.18.1" class="ltx_td ltx_align_center">0.16<sup id="S4.T1.24.18.1.1" class="ltx_sup"><span id="S4.T1.24.18.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
</tr>
<tr id="S4.T1.25.19" class="ltx_tr">
<th id="S4.T1.25.19.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-CL</th>
<td id="S4.T1.25.19.3" class="ltx_td ltx_align_center">0.21</td>
<td id="S4.T1.25.19.4" class="ltx_td ltx_align_center">0.2</td>
<td id="S4.T1.25.19.1" class="ltx_td ltx_align_center">0.18 <sup id="S4.T1.25.19.1.1" class="ltx_sup"><span id="S4.T1.25.19.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
</tr>
<tr id="S4.T1.34.28" class="ltx_tr">
<th id="S4.T1.34.28.10" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-LLM</th>
<td id="S4.T1.28.22.3" class="ltx_td ltx_align_center"><span id="S4.T1.28.22.3.3" class="ltx_text ltx_font_bold">0.29<sup id="S4.T1.28.22.3.3.1" class="ltx_sup"><span id="S4.T1.28.22.3.3.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup><sup id="S4.T1.28.22.3.3.2" class="ltx_sup"><span id="S4.T1.28.22.3.3.2.1" class="ltx_text ltx_font_medium ltx_font_italic">†</span></sup><sup id="S4.T1.28.22.3.3.3" class="ltx_sup"><span id="S4.T1.28.22.3.3.3.1" class="ltx_text ltx_font_medium ltx_font_italic">⋆</span></sup></span></td>
<td id="S4.T1.31.25.6" class="ltx_td ltx_align_center"><span id="S4.T1.31.25.6.3" class="ltx_text ltx_font_bold">0.27<sup id="S4.T1.31.25.6.3.1" class="ltx_sup"><span id="S4.T1.31.25.6.3.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup><sup id="S4.T1.31.25.6.3.2" class="ltx_sup"><span id="S4.T1.31.25.6.3.2.1" class="ltx_text ltx_font_medium ltx_font_italic">†</span></sup><sup id="S4.T1.31.25.6.3.3" class="ltx_sup"><span id="S4.T1.31.25.6.3.3.1" class="ltx_text ltx_font_medium ltx_font_italic">⋆</span></sup></span></td>
<td id="S4.T1.34.28.9" class="ltx_td ltx_align_center"><span id="S4.T1.34.28.9.3" class="ltx_text ltx_font_bold">0.21<sup id="S4.T1.34.28.9.3.1" class="ltx_sup"><span id="S4.T1.34.28.9.3.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup><sup id="S4.T1.34.28.9.3.2" class="ltx_sup"><span id="S4.T1.34.28.9.3.2.1" class="ltx_text ltx_font_medium ltx_font_italic">†</span></sup><sup id="S4.T1.34.28.9.3.3" class="ltx_sup"><span id="S4.T1.34.28.9.3.3.1" class="ltx_text ltx_font_medium ltx_font_italic">⋆</span></sup></span></td>
</tr>
<tr id="S4.T1.84.84.6" class="ltx_tr">
<th id="S4.T1.84.84.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_t"></th>
<th id="S4.T1.84.84.6.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S4.T1.84.84.6.3" class="ltx_td ltx_align_center ltx_border_t" colspan="3">wavlm-base-plus</td>
</tr>
<tr id="S4.T1.84.85.7" class="ltx_tr">
<th id="S4.T1.84.85.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="5"><span id="S4.T1.84.85.7.1.1" class="ltx_text">
<span id="S4.T1.84.85.7.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:17.6pt;vertical-align:-5.3pt;"><span class="ltx_transformed_inner" style="width:17.6pt;transform:translate(-5.35pt,0pt) rotate(-90deg) ;">
<span id="S4.T1.84.85.7.1.1.1.1" class="ltx_p">5dB</span>
</span></span></span></th>
<th id="S4.T1.84.85.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Original</th>
<td id="S4.T1.84.85.7.3" class="ltx_td ltx_align_center ltx_border_t">0.53</td>
<td id="S4.T1.84.85.7.4" class="ltx_td ltx_align_center ltx_border_t">0.46</td>
<td id="S4.T1.84.85.7.5" class="ltx_td ltx_align_center ltx_border_t">0.45</td>
</tr>
<tr id="S4.T1.36.30" class="ltx_tr">
<th id="S4.T1.36.30.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">RT</th>
<td id="S4.T1.35.29.1" class="ltx_td ltx_align_center">0.57<sup id="S4.T1.35.29.1.1" class="ltx_sup"><span id="S4.T1.35.29.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T1.36.30.2" class="ltx_td ltx_align_center">0.48<sup id="S4.T1.36.30.2.1" class="ltx_sup"><span id="S4.T1.36.30.2.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T1.36.30.4" class="ltx_td ltx_align_center">0.41</td>
</tr>
<tr id="S4.T1.40.34" class="ltx_tr">
<th id="S4.T1.40.34.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">DAT</th>
<td id="S4.T1.37.31.1" class="ltx_td ltx_align_center"><span id="S4.T1.37.31.1.1" class="ltx_text ltx_font_bold">0.59<sup id="S4.T1.37.31.1.1.1" class="ltx_sup"><span id="S4.T1.37.31.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup></span></td>
<td id="S4.T1.38.32.2" class="ltx_td ltx_align_center"><span id="S4.T1.38.32.2.1" class="ltx_text ltx_font_bold">0.49<sup id="S4.T1.38.32.2.1.1" class="ltx_sup"><span id="S4.T1.38.32.2.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup></span></td>
<td id="S4.T1.40.34.4" class="ltx_td ltx_align_center"><span id="S4.T1.40.34.4.2" class="ltx_text ltx_font_bold">0.48<sup id="S4.T1.40.34.4.2.1" class="ltx_sup"><span id="S4.T1.40.34.4.2.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup><sup id="S4.T1.40.34.4.2.2" class="ltx_sup"><span id="S4.T1.40.34.4.2.2.1" class="ltx_text ltx_font_medium ltx_font_italic">†</span></sup></span></td>
</tr>
<tr id="S4.T1.43.37" class="ltx_tr">
<th id="S4.T1.43.37.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-CL</th>
<td id="S4.T1.41.35.1" class="ltx_td ltx_align_center">0.57<sup id="S4.T1.41.35.1.1" class="ltx_sup"><span id="S4.T1.41.35.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T1.43.37.5" class="ltx_td ltx_align_center">0.47</td>
<td id="S4.T1.43.37.3" class="ltx_td ltx_align_center">0.47<sup id="S4.T1.43.37.3.1" class="ltx_sup"><span id="S4.T1.43.37.3.1.1" class="ltx_text ltx_font_italic">∗</span></sup><sup id="S4.T1.43.37.3.2" class="ltx_sup"><span id="S4.T1.43.37.3.2.1" class="ltx_text ltx_font_italic">†</span></sup>
</td>
</tr>
<tr id="S4.T1.46.40" class="ltx_tr">
<th id="S4.T1.46.40.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-LLM</th>
<td id="S4.T1.44.38.1" class="ltx_td ltx_align_center">0.58<sup id="S4.T1.44.38.1.1" class="ltx_sup"><span id="S4.T1.44.38.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T1.45.39.2" class="ltx_td ltx_align_center">0.48<sup id="S4.T1.45.39.2.1" class="ltx_sup"><span id="S4.T1.45.39.2.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T1.46.40.3" class="ltx_td ltx_align_center">0.45<sup id="S4.T1.46.40.3.1" class="ltx_sup"><span id="S4.T1.46.40.3.1.1" class="ltx_text ltx_font_italic">†</span></sup>
</td>
</tr>
<tr id="S4.T1.84.86.8" class="ltx_tr">
<th id="S4.T1.84.86.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="5"><span id="S4.T1.84.86.8.1.1" class="ltx_text">
<span id="S4.T1.84.86.8.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:17.6pt;vertical-align:-5.3pt;"><span class="ltx_transformed_inner" style="width:17.6pt;transform:translate(-5.35pt,0pt) rotate(-90deg) ;">
<span id="S4.T1.84.86.8.1.1.1.1" class="ltx_p">0dB</span>
</span></span></span></th>
<th id="S4.T1.84.86.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Original</th>
<td id="S4.T1.84.86.8.3" class="ltx_td ltx_align_center ltx_border_t">0.40</td>
<td id="S4.T1.84.86.8.4" class="ltx_td ltx_align_center ltx_border_t">0.32</td>
<td id="S4.T1.84.86.8.5" class="ltx_td ltx_align_center ltx_border_t">0.35</td>
</tr>
<tr id="S4.T1.48.42" class="ltx_tr">
<th id="S4.T1.48.42.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">RT</th>
<td id="S4.T1.47.41.1" class="ltx_td ltx_align_center">0.53<sup id="S4.T1.47.41.1.1" class="ltx_sup"><span id="S4.T1.47.41.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T1.48.42.2" class="ltx_td ltx_align_center">0.43<sup id="S4.T1.48.42.2.1" class="ltx_sup"><span id="S4.T1.48.42.2.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T1.48.42.4" class="ltx_td ltx_align_center">0.34</td>
</tr>
<tr id="S4.T1.52.46" class="ltx_tr">
<th id="S4.T1.52.46.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">DAT</th>
<td id="S4.T1.49.43.1" class="ltx_td ltx_align_center">0.53<sup id="S4.T1.49.43.1.1" class="ltx_sup"><span id="S4.T1.49.43.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T1.50.44.2" class="ltx_td ltx_align_center"><span id="S4.T1.50.44.2.1" class="ltx_text ltx_font_bold">0.45<sup id="S4.T1.50.44.2.1.1" class="ltx_sup"><span id="S4.T1.50.44.2.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup></span></td>
<td id="S4.T1.52.46.4" class="ltx_td ltx_align_center"><span id="S4.T1.52.46.4.2" class="ltx_text ltx_font_bold">0.42<sup id="S4.T1.52.46.4.2.1" class="ltx_sup"><span id="S4.T1.52.46.4.2.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup><sup id="S4.T1.52.46.4.2.2" class="ltx_sup"><span id="S4.T1.52.46.4.2.2.1" class="ltx_text ltx_font_medium ltx_font_italic">†</span></sup></span></td>
</tr>
<tr id="S4.T1.56.50" class="ltx_tr">
<th id="S4.T1.56.50.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-CL</th>
<td id="S4.T1.53.47.1" class="ltx_td ltx_align_center">0.52<sup id="S4.T1.53.47.1.1" class="ltx_sup"><span id="S4.T1.53.47.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T1.54.48.2" class="ltx_td ltx_align_center">0.43<sup id="S4.T1.54.48.2.1" class="ltx_sup"><span id="S4.T1.54.48.2.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T1.56.50.4" class="ltx_td ltx_align_center"><span id="S4.T1.56.50.4.2" class="ltx_text ltx_font_bold">0.42<sup id="S4.T1.56.50.4.2.1" class="ltx_sup"><span id="S4.T1.56.50.4.2.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup><sup id="S4.T1.56.50.4.2.2" class="ltx_sup"><span id="S4.T1.56.50.4.2.2.1" class="ltx_text ltx_font_medium ltx_font_italic">†</span></sup></span></td>
</tr>
<tr id="S4.T1.63.57" class="ltx_tr">
<th id="S4.T1.63.57.8" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-LLM</th>
<td id="S4.T1.59.53.3" class="ltx_td ltx_align_center"><span id="S4.T1.59.53.3.3" class="ltx_text ltx_font_bold">0.55<sup id="S4.T1.59.53.3.3.1" class="ltx_sup"><span id="S4.T1.59.53.3.3.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup><sup id="S4.T1.59.53.3.3.2" class="ltx_sup"><span id="S4.T1.59.53.3.3.2.1" class="ltx_text ltx_font_medium ltx_font_italic">†</span></sup><sup id="S4.T1.59.53.3.3.3" class="ltx_sup"><span id="S4.T1.59.53.3.3.3.1" class="ltx_text ltx_font_medium ltx_font_italic">⋆</span></sup></span></td>
<td id="S4.T1.61.55.5" class="ltx_td ltx_align_center"><span id="S4.T1.61.55.5.2" class="ltx_text ltx_font_bold">0.45<sup id="S4.T1.61.55.5.2.1" class="ltx_sup"><span id="S4.T1.61.55.5.2.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup><sup id="S4.T1.61.55.5.2.2" class="ltx_sup"><span id="S4.T1.61.55.5.2.2.1" class="ltx_text ltx_font_medium ltx_font_italic">†</span></sup></span></td>
<td id="S4.T1.63.57.7" class="ltx_td ltx_align_center">0.40<sup id="S4.T1.63.57.7.1" class="ltx_sup"><span id="S4.T1.63.57.7.1.1" class="ltx_text ltx_font_italic">∗</span></sup><sup id="S4.T1.63.57.7.2" class="ltx_sup"><span id="S4.T1.63.57.7.2.1" class="ltx_text ltx_font_italic">†</span></sup>
</td>
</tr>
<tr id="S4.T1.84.87.9" class="ltx_tr">
<th id="S4.T1.84.87.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" rowspan="5"><span id="S4.T1.84.87.9.1.1" class="ltx_text">
<span id="S4.T1.84.87.9.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:21pt;vertical-align:-7.0pt;"><span class="ltx_transformed_inner" style="width:21.0pt;transform:translate(-7.01pt,0pt) rotate(-90deg) ;">
<span id="S4.T1.84.87.9.1.1.1.1" class="ltx_p">-5dB</span>
</span></span></span></th>
<th id="S4.T1.84.87.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Original</th>
<td id="S4.T1.84.87.9.3" class="ltx_td ltx_align_center ltx_border_t">0.11</td>
<td id="S4.T1.84.87.9.4" class="ltx_td ltx_align_center ltx_border_t">0.07</td>
<td id="S4.T1.84.87.9.5" class="ltx_td ltx_align_center ltx_border_t">0.12</td>
</tr>
<tr id="S4.T1.65.59" class="ltx_tr">
<th id="S4.T1.65.59.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">RT</th>
<td id="S4.T1.64.58.1" class="ltx_td ltx_align_center">0.19<sup id="S4.T1.64.58.1.1" class="ltx_sup"><span id="S4.T1.64.58.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T1.65.59.2" class="ltx_td ltx_align_center">0.12<sup id="S4.T1.65.59.2.1" class="ltx_sup"><span id="S4.T1.65.59.2.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T1.65.59.4" class="ltx_td ltx_align_center">0.13</td>
</tr>
<tr id="S4.T1.71.65" class="ltx_tr">
<th id="S4.T1.71.65.7" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">DAT</th>
<td id="S4.T1.67.61.2" class="ltx_td ltx_align_center">0.22<sup id="S4.T1.67.61.2.1" class="ltx_sup"><span id="S4.T1.67.61.2.1.1" class="ltx_text ltx_font_italic">∗</span></sup><sup id="S4.T1.67.61.2.2" class="ltx_sup"><span id="S4.T1.67.61.2.2.1" class="ltx_text ltx_font_italic">†</span></sup>
</td>
<td id="S4.T1.69.63.4" class="ltx_td ltx_align_center">0.17<sup id="S4.T1.69.63.4.1" class="ltx_sup"><span id="S4.T1.69.63.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup><sup id="S4.T1.69.63.4.2" class="ltx_sup"><span id="S4.T1.69.63.4.2.1" class="ltx_text ltx_font_italic">†</span></sup>
</td>
<td id="S4.T1.71.65.6" class="ltx_td ltx_align_center">0.21<sup id="S4.T1.71.65.6.1" class="ltx_sup"><span id="S4.T1.71.65.6.1.1" class="ltx_text ltx_font_italic">∗</span></sup><sup id="S4.T1.71.65.6.2" class="ltx_sup"><span id="S4.T1.71.65.6.2.1" class="ltx_text ltx_font_italic">†</span></sup>
</td>
</tr>
<tr id="S4.T1.75.69" class="ltx_tr">
<th id="S4.T1.75.69.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-CL</th>
<td id="S4.T1.72.66.1" class="ltx_td ltx_align_center">0.18<sup id="S4.T1.72.66.1.1" class="ltx_sup"><span id="S4.T1.72.66.1.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T1.73.67.2" class="ltx_td ltx_align_center">0.12<sup id="S4.T1.73.67.2.1" class="ltx_sup"><span id="S4.T1.73.67.2.1.1" class="ltx_text ltx_font_italic">∗</span></sup>
</td>
<td id="S4.T1.75.69.4" class="ltx_td ltx_align_center">0.19<sup id="S4.T1.75.69.4.1" class="ltx_sup"><span id="S4.T1.75.69.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup><sup id="S4.T1.75.69.4.2" class="ltx_sup"><span id="S4.T1.75.69.4.2.1" class="ltx_text ltx_font_italic">†</span></sup>
</td>
</tr>
<tr id="S4.T1.84.78" class="ltx_tr">
<th id="S4.T1.84.78.10" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">TG-EAT-LLM</th>
<td id="S4.T1.78.72.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.78.72.3.3" class="ltx_text ltx_font_bold">0.29<sup id="S4.T1.78.72.3.3.1" class="ltx_sup"><span id="S4.T1.78.72.3.3.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup><sup id="S4.T1.78.72.3.3.2" class="ltx_sup"><span id="S4.T1.78.72.3.3.2.1" class="ltx_text ltx_font_medium ltx_font_italic">†</span></sup><sup id="S4.T1.78.72.3.3.3" class="ltx_sup"><span id="S4.T1.78.72.3.3.3.1" class="ltx_text ltx_font_medium ltx_font_italic">⋆</span></sup></span></td>
<td id="S4.T1.81.75.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.81.75.6.3" class="ltx_text ltx_font_bold">0.21<sup id="S4.T1.81.75.6.3.1" class="ltx_sup"><span id="S4.T1.81.75.6.3.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup><sup id="S4.T1.81.75.6.3.2" class="ltx_sup"><span id="S4.T1.81.75.6.3.2.1" class="ltx_text ltx_font_medium ltx_font_italic">†</span></sup><sup id="S4.T1.81.75.6.3.3" class="ltx_sup"><span id="S4.T1.81.75.6.3.3.1" class="ltx_text ltx_font_medium ltx_font_italic">⋆</span></sup></span></td>
<td id="S4.T1.84.78.9" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T1.84.78.9.3" class="ltx_text ltx_font_bold">0.23<sup id="S4.T1.84.78.9.3.1" class="ltx_sup"><span id="S4.T1.84.78.9.3.1.1" class="ltx_text ltx_font_medium ltx_font_italic">∗</span></sup><sup id="S4.T1.84.78.9.3.2" class="ltx_sup"><span id="S4.T1.84.78.9.3.2.1" class="ltx_text ltx_font_medium ltx_font_italic">†</span></sup><sup id="S4.T1.84.78.9.3.3" class="ltx_sup"><span id="S4.T1.84.78.9.3.3.1" class="ltx_text ltx_font_medium ltx_font_italic">⋆</span></sup></span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Emotion recognition performance</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.2" class="ltx_p">We report the SER performance of our text-guided environment-aware training with our baselines. As described in Section <a href="#S4.SS1" title="4.1 Data preparation ‣ 4 Experimental Settings ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>, we use ten different evaluation sets for three SNR levels. We report the average CCC of ten experiments for each SNR level. We conduct a one-tailed Welch’s t-test between the baselines and our proposed models to assess if the training strategy shows significantly better SER performance in noisy conditions. We assert significance at <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mi id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">p</annotation></semantics></math>-value <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mo id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><lt id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">&lt;</annotation></semantics></math> 0.05.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‣ 4.4 Baselines ‣ 4 Experimental Settings ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the SER performance of each model in noisy testing environments. When comparing our baselines (RT, DAT) with the original model, they do not consistently yield performance improvement for all the attributes. RT does not improve the performance neither for arousal and dominance with the wav2vec2-large-robust feature vector, nor for valence with the wavlm-base-plus feature vector. Although the DAT shows significant performance improvement with wavlm-base-plus feature vector, it fails to improve arousal and dominance prediction performance with wav2vec2-large-robust feature vector. Since these baselines do not use environmental information, we can observe the importance of using environmental information when adapting the SER model to multiple noisy environments.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p">Compared with the baselines, our proposed TG-EAT-LLM performs best when using the wav2vec2-large-robust feature vector. In the 5dB condition, TG-EAT-LLM improves the original model’s performance by 6.7 % (arousal), 3.9% (dominance), and 17.5% (valence). It does not yield the best performance with the wavlm-base-plus feature vector in the 5dB conditions. However, as the SNR level decreases, TG-EAT-LLM shows higher performance than the baselines. In -5dB condition, TG-EAT-LLM shows performance gains of 31.8% (arousal), 23.5% (dominance), and 9.5% (valence) compared to the best baseline, DAT. In spite of having a mismatch in SNR and environment conditions, TG-EAT-LLM shows robust results under all the conditions. These results indicate that guiding the SER model with LLM-based representation can improve the noise-robustness for the SER task. It shows good generalization to unknown environments. Although the DAT approach is effective when using the wavlm-base-plus model for noise conditions above 0dB SNR, using LLM-based representation is more helpful when dealing with low SNR conditions.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.p4.1" class="ltx_p">When we compare the TG-EAT-CL and TG-EAT-LLM models, we conclude that the CL-based representation does not show a performance improvement over the original SER model, especially with the wav2vec2-large-robust feature vector. We can clearly see that the TG-EAT-CL model does not improve the performance for arousal and dominance in the 0dB and -5dB conditions. This result indicates that pre-training the text encoder to have enriched semantic information is more helpful for the noise-robust SER model than pre-training the text encoder with a audio-text pair.</p>
</div>
<figure id="S5.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2407.17716/assets/x2.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="22" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S5.F2.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:195.1pt;">
<p id="S5.F2.1.1" class="ltx_p ltx_align_center"><span id="S5.F2.1.1.1" class="ltx_text"><img src="/html/2407.17716/assets/x3.png" id="S5.F2.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="334" alt="Refer to caption"></span></p>
<p id="S5.F2.1.2" class="ltx_p ltx_align_center ltx_align_center"><span id="S5.F2.1.2.1" class="ltx_text">(a)</span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S5.F2.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:195.1pt;">
<p id="S5.F2.2.1" class="ltx_p ltx_align_center"><span id="S5.F2.2.1.1" class="ltx_text"><img src="/html/2407.17716/assets/x4.png" id="S5.F2.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="334" alt="Refer to caption"></span></p>
<p id="S5.F2.2.2" class="ltx_p ltx_align_center ltx_align_center"><span id="S5.F2.2.2.1" class="ltx_text">(b)</span></p>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Embedding differences in the first and the last transformer encoder layers using clean and noisy speech in the -5dB condition. We use the wavlm-base-plus feature vector in this analysis. (a) it extracts and compares clean and noisy representations from each of the models. (b) it extracts and compares the clean representation from the Original model and the noisy representation from each of the models.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Embedding analysis</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p">Section <a href="#S5.SS1" title="5.1 Emotion recognition performance ‣ 5 Results ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a> demonstrated that the TG-EAT-LLM approach shows better performance than the environment-agnostic baselines and the TG-EAT-CL approach. Our initial assumption is that the proposed TG-EAT-LLM can learn appropriate denoising functions for the transformer encoder. To verify this assumption, we analyze the difference between the clean and noisy representations (Fig. <a href="#S5.F2" title="Figure 2 ‣ 5.1 Emotion recognition performance ‣ 5 Results ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a)). We use the wavlm-base-plus feature vector and the noisy speech from the -5dB condition for this analysis. The first analysis compares the clean and noisy representation extracted from each model. We want to assess with this analysis if the model has robustness between clean and noisy speech. The second analysis compares the clean representation from the Original framework and the noisy representation from each of the models (Fig. <a href="#S5.F2" title="Figure 2 ‣ 5.1 Emotion recognition performance ‣ 5 Results ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(b)). In this analysis, we want to assess if the model can keep the knowledge of the original SER model. We extract the representations from the first and the last transformer encoder layers and then calculate the mean square difference between clean and noisy representations for each layer.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p">Figure <a href="#S5.F2" title="Figure 2 ‣ 5.1 Emotion recognition performance ‣ 5 Results ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates our analysis results. When extracting clean and noisy representations from the same model, we can first see that DAT shows the lowest difference in the last transformer layer. On the contrary, it shows the highest difference when extracting the clean representation from the original model. This result demonstrates the risk of catastrophic forgetting when using the DAT method. Although it can normalize the environmental difference in the adapted model, its representation can deviate from the original SER model’s representation. However, our TG-EAT method does not highly increase the difference compared to the original model’s clean representation. This result indicates that TG-EAT can minimize the risk of catastrophic forgetting during adaptation by introducing environmental information about the speech.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p">Compared with the TG-EAT-LLM method, TG-EAT-CL shows a higher representation difference in the first layer. When comparing the clean and noisy representations from the same model, TG-EAT-LLM shows 7.7% less representation difference than the TG-EAT-CL method in the first transformer layer. However, TG-EAT-CL shows less representation difference than the TG-EAT-LLM in the last layer. Even though the downstream head uses the representation from the last transformer layer, TG-EAT-CL shows worse performance than the TG-EAT-LLM approach. LLM-based representation can better denoise the acoustic representation than the CL-based representation. In addition, we speculate that the embedding difference in the lower transformer layer might be the crucial factor for increasing the noise-robustness of the SER system.</p>
</div>
<figure id="S5.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S5.F3.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:195.1pt;">
<p id="S5.F3.1.1" class="ltx_p ltx_align_center"><span id="S5.F3.1.1.1" class="ltx_text"><img src="/html/2407.17716/assets/x5.png" id="S5.F3.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="345" alt="Refer to caption"></span></p>
<p id="S5.F3.1.2" class="ltx_p ltx_align_center ltx_align_center"><span id="S5.F3.1.2.1" class="ltx_text">(a) TG-EAT-CL</span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S5.F3.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:195.1pt;">
<p id="S5.F3.2.1" class="ltx_p ltx_align_center"><span id="S5.F3.2.1.1" class="ltx_text"><img src="/html/2407.17716/assets/x6.png" id="S5.F3.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="345" alt="Refer to caption"></span></p>
<p id="S5.F3.2.2" class="ltx_p ltx_align_center ltx_align_center"><span id="S5.F3.2.2.1" class="ltx_text">(b) TG-EAT-LLM</span></p>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Visualization of text-based environment embeddings. We use UMAP to project text embeddings into 2D space.</figcaption>
</figure>
<div id="S5.SS2.p4" class="ltx_para ltx_noindent">
<p id="S5.SS2.p4.1" class="ltx_p">We also investigate if the proposed text-based environment embedding clusters similar environments together, which it the key premise of the proposed approach to deal with unseen environments. First, we extract the 26 different text embeddings using the different templates to describe each environmental condition. We project these embeddings into the 2D space to visualize the embedding space using the <em id="S5.SS2.p4.1.1" class="ltx_emph ltx_font_italic">uniform manifold approximation and projection</em> (UMAP) method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. Figure <a href="#S5.F3" title="Figure 3 ‣ 5.2 Embedding analysis ‣ 5 Results ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the text embedding space of TG-EAT-CL and TG-EAT-LLM. The figure shows that both frameworks cluster environmental conditions that are semantically similar together. For example, we observe the embeddings for "boat" and “sea,” together. We also observe the ones for "subway" and "station" clustered together. Both encoders cluster the house environments (house, home, kitchen) and the vehicle environments (bus, taxi, car), which indicates that the text encoder can cluster acoustically similar environments. This analysis implies that our proposed frameworks can deal with unseen environments by clustering acoustically and semantically similar environments.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Average CCC of the ten experiments for the seen environment. The environmental conditions for the train set and the test set are the same. We compare the proposed method with the baselines by using the wavlm-base-plus model.</figcaption>
<table id="S5.T2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">SNR</th>
<th id="S5.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Model</th>
<th id="S5.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Arousal</th>
<th id="S5.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Dominance</th>
<th id="S5.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Valence</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.2.1" class="ltx_tr">
<th id="S5.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="3"><span id="S5.T2.1.2.1.1.1" class="ltx_text">
<span id="S5.T2.1.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:17.6pt;vertical-align:-5.3pt;"><span class="ltx_transformed_inner" style="width:17.6pt;transform:translate(-5.35pt,0pt) rotate(-90deg) ;">
<span id="S5.T2.1.2.1.1.1.1.1" class="ltx_p">5dB</span>
</span></span></span></th>
<th id="S5.T2.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">Original</th>
<td id="S5.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">0.54</td>
<td id="S5.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">0.46</td>
<td id="S5.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">0.45</td>
</tr>
<tr id="S5.T2.1.3.2" class="ltx_tr">
<th id="S5.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">One-hot</th>
<td id="S5.T2.1.3.2.2" class="ltx_td ltx_align_center">0.59</td>
<td id="S5.T2.1.3.2.3" class="ltx_td ltx_align_center">0.48</td>
<td id="S5.T2.1.3.2.4" class="ltx_td ltx_align_center">0.47</td>
</tr>
<tr id="S5.T2.1.4.3" class="ltx_tr">
<th id="S5.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-LLM</th>
<td id="S5.T2.1.4.3.2" class="ltx_td ltx_align_center">0.59</td>
<td id="S5.T2.1.4.3.3" class="ltx_td ltx_align_center">0.48</td>
<td id="S5.T2.1.4.3.4" class="ltx_td ltx_align_center">0.47</td>
</tr>
<tr id="S5.T2.1.5.4" class="ltx_tr">
<th id="S5.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="S5.T2.1.5.4.1.1" class="ltx_text">
<span id="S5.T2.1.5.4.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:17.6pt;vertical-align:-5.3pt;"><span class="ltx_transformed_inner" style="width:17.6pt;transform:translate(-5.35pt,0pt) rotate(-90deg) ;">
<span id="S5.T2.1.5.4.1.1.1.1" class="ltx_p">0dB</span>
</span></span></span></th>
<th id="S5.T2.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Original</th>
<td id="S5.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">0.40</td>
<td id="S5.T2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t">0.32</td>
<td id="S5.T2.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t">0.35</td>
</tr>
<tr id="S5.T2.1.6.5" class="ltx_tr">
<th id="S5.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">One-hot</th>
<td id="S5.T2.1.6.5.2" class="ltx_td ltx_align_center">0.56</td>
<td id="S5.T2.1.6.5.3" class="ltx_td ltx_align_center">0.45</td>
<td id="S5.T2.1.6.5.4" class="ltx_td ltx_align_center">0.42</td>
</tr>
<tr id="S5.T2.1.7.6" class="ltx_tr">
<th id="S5.T2.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-LLM</th>
<td id="S5.T2.1.7.6.2" class="ltx_td ltx_align_center">0.56</td>
<td id="S5.T2.1.7.6.3" class="ltx_td ltx_align_center">0.46</td>
<td id="S5.T2.1.7.6.4" class="ltx_td ltx_align_center">0.40</td>
</tr>
<tr id="S5.T2.1.8.7" class="ltx_tr">
<th id="S5.T2.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" rowspan="3"><span id="S5.T2.1.8.7.1.1" class="ltx_text">
<span id="S5.T2.1.8.7.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:21pt;vertical-align:-7.0pt;"><span class="ltx_transformed_inner" style="width:21.0pt;transform:translate(-7.01pt,0pt) rotate(-90deg) ;">
<span id="S5.T2.1.8.7.1.1.1.1" class="ltx_p">-5dB</span>
</span></span></span></th>
<th id="S5.T2.1.8.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Original</th>
<td id="S5.T2.1.8.7.3" class="ltx_td ltx_align_center ltx_border_t">0.09</td>
<td id="S5.T2.1.8.7.4" class="ltx_td ltx_align_center ltx_border_t">0.06</td>
<td id="S5.T2.1.8.7.5" class="ltx_td ltx_align_center ltx_border_t">0.10</td>
</tr>
<tr id="S5.T2.1.9.8" class="ltx_tr">
<th id="S5.T2.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">One-hot</th>
<td id="S5.T2.1.9.8.2" class="ltx_td ltx_align_center">0.29</td>
<td id="S5.T2.1.9.8.3" class="ltx_td ltx_align_center">0.20</td>
<td id="S5.T2.1.9.8.4" class="ltx_td ltx_align_center">0.21</td>
</tr>
<tr id="S5.T2.1.10.9" class="ltx_tr">
<th id="S5.T2.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">TG-EAT-LLM</th>
<td id="S5.T2.1.10.9.2" class="ltx_td ltx_align_center ltx_border_b">0.27</td>
<td id="S5.T2.1.10.9.3" class="ltx_td ltx_align_center ltx_border_b">0.18</td>
<td id="S5.T2.1.10.9.4" class="ltx_td ltx_align_center ltx_border_b">0.21</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Average CCC of the ten experiments for the unseen environment. We compare the proposed method with the baselines by using the wavlm-base-plus model.</figcaption>
<table id="S5.T3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">SNR</th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Model</th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Arousal</th>
<th id="S5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Dominance</th>
<th id="S5.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Valence</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<th id="S5.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="5"><span id="S5.T3.1.2.1.1.1" class="ltx_text">
<span id="S5.T3.1.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:17.6pt;vertical-align:-5.3pt;"><span class="ltx_transformed_inner" style="width:17.6pt;transform:translate(-5.35pt,0pt) rotate(-90deg) ;">
<span id="S5.T3.1.2.1.1.1.1.1" class="ltx_p">5dB</span>
</span></span></span></th>
<th id="S5.T3.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">Original</th>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">0.53</td>
<td id="S5.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">0.46</td>
<td id="S5.T3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">0.45</td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<th id="S5.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">RT</th>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_center">0.57</td>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_align_center">0.48</td>
<td id="S5.T3.1.3.2.4" class="ltx_td ltx_align_center">0.41</td>
</tr>
<tr id="S5.T3.1.4.3" class="ltx_tr">
<th id="S5.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">GloVe</th>
<td id="S5.T3.1.4.3.2" class="ltx_td ltx_align_center">0.58</td>
<td id="S5.T3.1.4.3.3" class="ltx_td ltx_align_center">0.47</td>
<td id="S5.T3.1.4.3.4" class="ltx_td ltx_align_center">0.42</td>
</tr>
<tr id="S5.T3.1.5.4" class="ltx_tr">
<th id="S5.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">AST</th>
<td id="S5.T3.1.5.4.2" class="ltx_td ltx_align_center"><span id="S5.T3.1.5.4.2.1" class="ltx_text ltx_font_bold">0.60</span></td>
<td id="S5.T3.1.5.4.3" class="ltx_td ltx_align_center"><span id="S5.T3.1.5.4.3.1" class="ltx_text ltx_font_bold">0.49</span></td>
<td id="S5.T3.1.5.4.4" class="ltx_td ltx_align_center"><span id="S5.T3.1.5.4.4.1" class="ltx_text ltx_font_bold">0.48</span></td>
</tr>
<tr id="S5.T3.1.6.5" class="ltx_tr">
<th id="S5.T3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-LLM</th>
<td id="S5.T3.1.6.5.2" class="ltx_td ltx_align_center">0.58</td>
<td id="S5.T3.1.6.5.3" class="ltx_td ltx_align_center">0.48</td>
<td id="S5.T3.1.6.5.4" class="ltx_td ltx_align_center">0.45</td>
</tr>
<tr id="S5.T3.1.7.6" class="ltx_tr">
<th id="S5.T3.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="5"><span id="S5.T3.1.7.6.1.1" class="ltx_text">
<span id="S5.T3.1.7.6.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:17.6pt;vertical-align:-5.3pt;"><span class="ltx_transformed_inner" style="width:17.6pt;transform:translate(-5.35pt,0pt) rotate(-90deg) ;">
<span id="S5.T3.1.7.6.1.1.1.1" class="ltx_p">0dB</span>
</span></span></span></th>
<th id="S5.T3.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Original</th>
<td id="S5.T3.1.7.6.3" class="ltx_td ltx_align_center ltx_border_t">0.40</td>
<td id="S5.T3.1.7.6.4" class="ltx_td ltx_align_center ltx_border_t">0.32</td>
<td id="S5.T3.1.7.6.5" class="ltx_td ltx_align_center ltx_border_t">0.35</td>
</tr>
<tr id="S5.T3.1.8.7" class="ltx_tr">
<th id="S5.T3.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">RT</th>
<td id="S5.T3.1.8.7.2" class="ltx_td ltx_align_center">0.53</td>
<td id="S5.T3.1.8.7.3" class="ltx_td ltx_align_center">0.43</td>
<td id="S5.T3.1.8.7.4" class="ltx_td ltx_align_center">0.34</td>
</tr>
<tr id="S5.T3.1.9.8" class="ltx_tr">
<th id="S5.T3.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">GloVe</th>
<td id="S5.T3.1.9.8.2" class="ltx_td ltx_align_center">0.53</td>
<td id="S5.T3.1.9.8.3" class="ltx_td ltx_align_center">0.43</td>
<td id="S5.T3.1.9.8.4" class="ltx_td ltx_align_center">0.37</td>
</tr>
<tr id="S5.T3.1.10.9" class="ltx_tr">
<th id="S5.T3.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">AST</th>
<td id="S5.T3.1.10.9.2" class="ltx_td ltx_align_center"><span id="S5.T3.1.10.9.2.1" class="ltx_text ltx_font_bold">0.56</span></td>
<td id="S5.T3.1.10.9.3" class="ltx_td ltx_align_center"><span id="S5.T3.1.10.9.3.1" class="ltx_text ltx_font_bold">0.46</span></td>
<td id="S5.T3.1.10.9.4" class="ltx_td ltx_align_center"><span id="S5.T3.1.10.9.4.1" class="ltx_text ltx_font_bold">0.43</span></td>
</tr>
<tr id="S5.T3.1.11.10" class="ltx_tr">
<th id="S5.T3.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-LLM</th>
<td id="S5.T3.1.11.10.2" class="ltx_td ltx_align_center">0.55</td>
<td id="S5.T3.1.11.10.3" class="ltx_td ltx_align_center">0.45</td>
<td id="S5.T3.1.11.10.4" class="ltx_td ltx_align_center">0.40</td>
</tr>
<tr id="S5.T3.1.12.11" class="ltx_tr">
<th id="S5.T3.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" rowspan="5"><span id="S5.T3.1.12.11.1.1" class="ltx_text">
<span id="S5.T3.1.12.11.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:21pt;vertical-align:-7.0pt;"><span class="ltx_transformed_inner" style="width:21.0pt;transform:translate(-7.01pt,0pt) rotate(-90deg) ;">
<span id="S5.T3.1.12.11.1.1.1.1" class="ltx_p">-5dB</span>
</span></span></span></th>
<th id="S5.T3.1.12.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Original</th>
<td id="S5.T3.1.12.11.3" class="ltx_td ltx_align_center ltx_border_t">0.11</td>
<td id="S5.T3.1.12.11.4" class="ltx_td ltx_align_center ltx_border_t">0.07</td>
<td id="S5.T3.1.12.11.5" class="ltx_td ltx_align_center ltx_border_t">0.12</td>
</tr>
<tr id="S5.T3.1.13.12" class="ltx_tr">
<th id="S5.T3.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">RT</th>
<td id="S5.T3.1.13.12.2" class="ltx_td ltx_align_center">0.19</td>
<td id="S5.T3.1.13.12.3" class="ltx_td ltx_align_center">0.12</td>
<td id="S5.T3.1.13.12.4" class="ltx_td ltx_align_center">0.13</td>
</tr>
<tr id="S5.T3.1.14.13" class="ltx_tr">
<th id="S5.T3.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">GloVe</th>
<td id="S5.T3.1.14.13.2" class="ltx_td ltx_align_center">0.25</td>
<td id="S5.T3.1.14.13.3" class="ltx_td ltx_align_center">0.17</td>
<td id="S5.T3.1.14.13.4" class="ltx_td ltx_align_center">0.20</td>
</tr>
<tr id="S5.T3.1.15.14" class="ltx_tr">
<th id="S5.T3.1.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">AST</th>
<td id="S5.T3.1.15.14.2" class="ltx_td ltx_align_center">0.25</td>
<td id="S5.T3.1.15.14.3" class="ltx_td ltx_align_center">0.18</td>
<td id="S5.T3.1.15.14.4" class="ltx_td ltx_align_center">0.21</td>
</tr>
<tr id="S5.T3.1.16.15" class="ltx_tr">
<th id="S5.T3.1.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">TG-EAT-LLM</th>
<td id="S5.T3.1.16.15.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T3.1.16.15.2.1" class="ltx_text ltx_font_bold">0.29</span></td>
<td id="S5.T3.1.16.15.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T3.1.16.15.3.1" class="ltx_text ltx_font_bold">0.21</span></td>
<td id="S5.T3.1.16.15.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T3.1.16.15.4.1" class="ltx_text ltx_font_bold">0.23</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Evaluation of Different Types of Environmental Embedding</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.p1.1" class="ltx_p">Our proposed method uses the embedding extracted from the text encoder to represent the testing environmental condition. To verify the benefits of using a text-based environmental embedding, we compare it with three different types of environmental embedding: <em id="S5.SS3.p1.1.1" class="ltx_emph ltx_font_italic">one-hot encoding</em> (One-hot), <em id="S5.SS3.p1.1.2" class="ltx_emph ltx_font_italic">global vectors for word representation</em> (GloVe) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, and <em id="S5.SS3.p1.1.3" class="ltx_emph ltx_font_italic">audio spectrogram transformer representation</em> (AST) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. One-hot uses 20-dimension binary vectors, where 1 represents the target environment condition, and 0 represents the others. Each dimension corresponds to the environmental condition of the training set. This embedding fully represents a seen environment with a simple vector; however, it cannot represent unseen environments, which is inappropriate for real-world services. GloVe is a word-level vector representation extracted from the regression model that considers the co-occurrences of words. We import the pre-trained GloVe vector collections consisting of 2.2 million vocabularies. We select the word vector representation that corresponds to the target noisy environment. The resulting representation is a 300-dimension vector. This representation can deal with unseen environments using text description, but it is semantically limited compared to our proposed text encoders. AST uses a transformer architecture to map the spectrogram patches into an audio-level representation. The model is fine-tuned with sound event classification tasks by using AudioSet, which is the same noise sound corpus for our training set. This model can automatically capture the acoustic characteristics from the audio-only input. However, it cannot explicitly use the semantic information of the testing environment.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para ltx_noindent">
<p id="S5.SS3.p2.1" class="ltx_p">We compare our proposed method with One-hot in the seen environment scenario (Table <a href="#S5.T2" title="Table 2 ‣ 5.2 Embedding analysis ‣ 5 Results ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) and with the other baselines in the unseen environment scenario (Table <a href="#S5.T3" title="Table 3 ‣ 5.2 Embedding analysis ‣ 5 Results ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). For the seen environment scenario, we used the same environmental condition as the train set to contaminate the clean test set but with different audio samples. We use ten different test sets and report the average CCC for both cases. Table <a href="#S5.T2" title="Table 2 ‣ 5.2 Embedding analysis ‣ 5 Results ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S5.T3" title="Table 3 ‣ 5.2 Embedding analysis ‣ 5 Results ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> report the results for the seen and unseen environments, respectively. In the seen environment, our proposed method and the one-hot environment encoding model improve the original SER performance for all the conditions and attributes. Both models show similar performances in the seen environments. However, the one-hot encoding cannot cover unseen environments. This result demonstrates that the proposed text embedding can deal with both seen and unseen environments. Compared to the model that uses GloVe embeddings, our proposed method shows better SER performances in 0dB and -5dB conditions. It also shows a better performance for valence in the 10dB condition. The GloVe model only considers word co-occurrence to get a word embedding, while our proposed text encoder model is pre-trained to understand the semantic information of a sentence. This result implies the importance of pre-training the text encoder with language modeling to get a robust environment embedding for performance improvement. The AST strategy shows better performance for valence than our proposed model under the 5dB and 0dB conditions. In comparison, our proposed model performs better for all the emotional attributes under the -5dB conditions. AST does not use semantic information from the testing environment to get environmental embedding; instead, it extracts the environmental information from the given audio. Considering that the -5dB SNR level is not presented while training the model, the result demonstrates that the AST works well for the seen SNR level but not for the unseen SNR level. In contrast, our proposed method works well for the unseen SNR level since the text description is independent of the SNR level. This result demonstrates that our proposed method is robust against the unseen SNR level, which is practical for real-world scenarios.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Benefit of Fine-Tuning the Text Encoder</h3>

<div id="S5.SS4.p1" class="ltx_para ltx_noindent">
<p id="S5.SS4.p1.1" class="ltx_p">Our results demonstrate that using the text encoder pre-trained with the CLAP strategy shows worse SER performance than using the pre-trained LLM. Despite this observation, we assume that this type of text encoder should have the potential to improve since the text encoder is pre-trained with the audio modality. Our assumption is that jointly fine-tuning the text encoder with the SER model could further improve the performance. Therefore, we compare the performance of an SER model by either freezing the text encoder or updating the encoder while adapting the SER model with the text-based environment embedding. We refer to the models that fine-tune the text encoder of the <em id="S5.SS4.p1.1.1" class="ltx_emph ltx_font_italic">TG-EAT-CL</em> and <em id="S5.SS4.p1.1.2" class="ltx_emph ltx_font_italic">TG-EAT-LLM</em> approaches during adaptation as <em id="S5.SS4.p1.1.3" class="ltx_emph ltx_font_italic">TG-EAT-CL-FT</em> and <em id="S5.SS4.p1.1.4" class="ltx_emph ltx_font_italic">TG-EAT-LLM-FT</em>, respectively.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para ltx_noindent">
<p id="S5.SS4.p2.1" class="ltx_p">Table <a href="#S5.T4" title="Table 4 ‣ 5.4 Benefit of Fine-Tuning the Text Encoder ‣ 5 Results ‣ Describe Where You Are: Improving Noise-Robustness for Speech Emotion Recognition with Text Description of the Environment" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> reports the average CCC of ten different test sets for each model. When comparing the <em id="S5.SS4.p2.1.1" class="ltx_emph ltx_font_italic">TG-EAT-LLM</em> and <em id="S5.SS4.p2.1.2" class="ltx_emph ltx_font_italic">TG-EAT-LLM-FT</em> implementations, they do not show significantly different performance. However, the <em id="S5.SS4.p2.1.3" class="ltx_emph ltx_font_italic">TG-EAT-CL-FT</em> approach shows meaningful performance improvement over the <em id="S5.SS4.p2.1.4" class="ltx_emph ltx_font_italic">TG-EAT-CL</em> implementation. For the -5dB conditions, it even reaches the best performance among all the models. This observation illustrates the importance of compensating the embedding space gap between the pre-trained text encoder space and the acoustic embedding. Although jointly fine-tuning the text encoder and the SER model can cost more memory space and computation time for the adaptation, this strategy can fully utilize the potential of the text encoder pre-trained with the audio modality.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of freezing the text encoder and updating it while adapting the SER model for the TG-EAT-CL and the TG-EAT-LLM models. We report the average CCC of the ten experiments for all the methods. We implement all the approaches with wavlm-base-plus feature vectors. We highlight in bold the best performance per condition.</figcaption>
<table id="S5.T4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">SNR</th>
<th id="S5.T4.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Model</th>
<th id="S5.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Arousal</th>
<th id="S5.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Dominance</th>
<th id="S5.T4.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Valence</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.2.1" class="ltx_tr">
<th id="S5.T4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="4"><span id="S5.T4.1.2.1.1.1" class="ltx_text">
<span id="S5.T4.1.2.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:17.6pt;vertical-align:-5.3pt;"><span class="ltx_transformed_inner" style="width:17.6pt;transform:translate(-5.35pt,0pt) rotate(-90deg) ;">
<span id="S5.T4.1.2.1.1.1.1.1" class="ltx_p">5dB</span>
</span></span></span></th>
<th id="S5.T4.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt">TG-EAT-CL</th>
<td id="S5.T4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">0.57</td>
<td id="S5.T4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">0.47</td>
<td id="S5.T4.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">0.47</td>
</tr>
<tr id="S5.T4.1.3.2" class="ltx_tr">
<th id="S5.T4.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-CL-FT</th>
<td id="S5.T4.1.3.2.2" class="ltx_td ltx_align_center">0.58</td>
<td id="S5.T4.1.3.2.3" class="ltx_td ltx_align_center">0.48</td>
<td id="S5.T4.1.3.2.4" class="ltx_td ltx_align_center"><span id="S5.T4.1.3.2.4.1" class="ltx_text ltx_font_bold">0.49</span></td>
</tr>
<tr id="S5.T4.1.4.3" class="ltx_tr">
<th id="S5.T4.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-LLM</th>
<td id="S5.T4.1.4.3.2" class="ltx_td ltx_align_center">0.58</td>
<td id="S5.T4.1.4.3.3" class="ltx_td ltx_align_center">0.48</td>
<td id="S5.T4.1.4.3.4" class="ltx_td ltx_align_center">0.45</td>
</tr>
<tr id="S5.T4.1.5.4" class="ltx_tr">
<th id="S5.T4.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-LLM-FT</th>
<td id="S5.T4.1.5.4.2" class="ltx_td ltx_align_center">0.58</td>
<td id="S5.T4.1.5.4.3" class="ltx_td ltx_align_center">0.48</td>
<td id="S5.T4.1.5.4.4" class="ltx_td ltx_align_center">0.46</td>
</tr>
<tr id="S5.T4.1.6.5" class="ltx_tr">
<th id="S5.T4.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="S5.T4.1.6.5.1.1" class="ltx_text">
<span id="S5.T4.1.6.5.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:17.6pt;vertical-align:-5.3pt;"><span class="ltx_transformed_inner" style="width:17.6pt;transform:translate(-5.35pt,0pt) rotate(-90deg) ;">
<span id="S5.T4.1.6.5.1.1.1.1" class="ltx_p">0dB</span>
</span></span></span></th>
<th id="S5.T4.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">TG-EAT-CL</th>
<td id="S5.T4.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t">0.52</td>
<td id="S5.T4.1.6.5.4" class="ltx_td ltx_align_center ltx_border_t">0.43</td>
<td id="S5.T4.1.6.5.5" class="ltx_td ltx_align_center ltx_border_t">0.42</td>
</tr>
<tr id="S5.T4.1.7.6" class="ltx_tr">
<th id="S5.T4.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-CL-FT</th>
<td id="S5.T4.1.7.6.2" class="ltx_td ltx_align_center">0.56</td>
<td id="S5.T4.1.7.6.3" class="ltx_td ltx_align_center">0.46</td>
<td id="S5.T4.1.7.6.4" class="ltx_td ltx_align_center"><span id="S5.T4.1.7.6.4.1" class="ltx_text ltx_font_bold">0.45</span></td>
</tr>
<tr id="S5.T4.1.8.7" class="ltx_tr">
<th id="S5.T4.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-LLM</th>
<td id="S5.T4.1.8.7.2" class="ltx_td ltx_align_center">0.55</td>
<td id="S5.T4.1.8.7.3" class="ltx_td ltx_align_center">0.45</td>
<td id="S5.T4.1.8.7.4" class="ltx_td ltx_align_center">0.40</td>
</tr>
<tr id="S5.T4.1.9.8" class="ltx_tr">
<th id="S5.T4.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-LLM-FT</th>
<td id="S5.T4.1.9.8.2" class="ltx_td ltx_align_center">0.55</td>
<td id="S5.T4.1.9.8.3" class="ltx_td ltx_align_center">0.45</td>
<td id="S5.T4.1.9.8.4" class="ltx_td ltx_align_center">0.41</td>
</tr>
<tr id="S5.T4.1.10.9" class="ltx_tr">
<th id="S5.T4.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" rowspan="4"><span id="S5.T4.1.10.9.1.1" class="ltx_text">
<span id="S5.T4.1.10.9.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.9pt;height:21pt;vertical-align:-7.0pt;"><span class="ltx_transformed_inner" style="width:21.0pt;transform:translate(-7.01pt,0pt) rotate(-90deg) ;">
<span id="S5.T4.1.10.9.1.1.1.1" class="ltx_p">-5dB</span>
</span></span></span></th>
<th id="S5.T4.1.10.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">TG-EAT-CL</th>
<td id="S5.T4.1.10.9.3" class="ltx_td ltx_align_center ltx_border_t">0.18</td>
<td id="S5.T4.1.10.9.4" class="ltx_td ltx_align_center ltx_border_t">0.12</td>
<td id="S5.T4.1.10.9.5" class="ltx_td ltx_align_center ltx_border_t">0.19</td>
</tr>
<tr id="S5.T4.1.11.10" class="ltx_tr">
<th id="S5.T4.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-CL-FT</th>
<td id="S5.T4.1.11.10.2" class="ltx_td ltx_align_center"><span id="S5.T4.1.11.10.2.1" class="ltx_text ltx_font_bold">0.31</span></td>
<td id="S5.T4.1.11.10.3" class="ltx_td ltx_align_center"><span id="S5.T4.1.11.10.3.1" class="ltx_text ltx_font_bold">0.23</span></td>
<td id="S5.T4.1.11.10.4" class="ltx_td ltx_align_center"><span id="S5.T4.1.11.10.4.1" class="ltx_text ltx_font_bold">0.23</span></td>
</tr>
<tr id="S5.T4.1.12.11" class="ltx_tr">
<th id="S5.T4.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TG-EAT-LLM</th>
<td id="S5.T4.1.12.11.2" class="ltx_td ltx_align_center">0.29</td>
<td id="S5.T4.1.12.11.3" class="ltx_td ltx_align_center">0.21</td>
<td id="S5.T4.1.12.11.4" class="ltx_td ltx_align_center"><span id="S5.T4.1.12.11.4.1" class="ltx_text ltx_font_bold">0.23</span></td>
</tr>
<tr id="S5.T4.1.13.12" class="ltx_tr">
<th id="S5.T4.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">TG-EAT-LLM-FT</th>
<td id="S5.T4.1.13.12.2" class="ltx_td ltx_align_center ltx_border_b">0.27</td>
<td id="S5.T4.1.13.12.3" class="ltx_td ltx_align_center ltx_border_b">0.19</td>
<td id="S5.T4.1.13.12.4" class="ltx_td ltx_align_center ltx_border_b">0.21</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">We proposed the TG-EAT method, which uses a text description of the testing environment for noise-robust SER. This approach inserts a text-based environment representation into an SER model, leading it to denoise the speech representation with respect to the given environmental information. Our experiment demonstrated that the LLM-based representation can improve SER performance under noisy conditions, especially when dealing with low SNR conditions. Our analysis indicates that the pre-trained text encoder can cluster acoustically and semantically similar environments into the same embedding, which is crucial for generalizing the models for unseen environments. Our result also shows that the CLAP-based text encoder can be highly improved by updating the text encoder. This result demonstrates the importance of minimizing the embedding space gap between the text encoder and the acoustic embedding.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p">We plan to expand this approach to cases where we cannot obtain information on the testing environment. We assume that the CL-based representation can address the scenario when the noise information is not provided by introducing its audio encoder. CLAP trains the audio encoder to have a similar representation to the ones from the text encoder, which could be useful for extracting environmental information from the audio. For this reason, we plan to investigate how we can improve the noise-robustness of the SER model with a CLAP encoder.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A framework for self-supervised learning of speech representations,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems (NeurIPS 2020)</em>, vol. 33, Virtual, December 2020, pp. 12 449–12 460.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
W.-N. Hsu, Y.-H. H. T. B. Bolte, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, “HuBERT: Self-supervised speech representation learning by masked prediction of hidden units,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 29, pp. 3451–3460, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao, J. Wu, L. Zhou, S. Ren, Y. Qian, Y. Qian, J. Wu, M. Zeng, X. Yu, and F. Wei, “WavLM: Large-scale self-supervised pre-training for full stack speech processing,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, vol. 16, no. 6, pp. 1505–1518, October 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
L. Martinez-Lucas, M. Abdelwahab, and C. Busso, “The MSP-conversation corpus,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Interspeech 2020</em>, Shanghai, China, October 2020, pp. 1823–1827.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
l. Kondratenko, N. Karpov, A. Sokolov, N. Savushkin, O. Kutuzov, and F. Minkin, “Hybrid dataset for speech emotion recognition in Russian language,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ISCA Interspeech 2023</em>, Dublin, Ireland, August 2023, pp. 2958–1796.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
R. Lotfian and C. Busso, “Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em>, vol. 10, no. 4, pp. 471–483, October-December 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S. Upadhyay, W.-S. Chien, B.-H. Su, L. Goncalves, Y.-T. Wu, A. Salman, C. Busso, and C.-C. Lee, “An intelligent infrastructure toward large scale naturalistic affective speech corpora collection,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">International Conference on Affective Computing and Intelligent Interaction (ACII 2023)</em>, Cambridge, MA, USA, September 2023, pp. 1–8.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
R. Chatterjee, S. Mazumdar, R. S. Sherratt, R. Halder, T. Maitra, and D. Giri, “Real-time speech emotion analysis for smart home assistants,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Consumer Electronics</em>, vol. 67, no. 1, pp. 68–76, February 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
D. Fulford, J. Mote, R. Gonzalez, S. Abplanalp, Y. Zhang, J. Luckenbaugh, J.-P. Onnela, C. Busso, and D. Gard, “Smartphone sensing of social interactions in people with and without schizophrenia,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Journal of Psychiatric Research</em>, vol. 137, pp. 613–620, May 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
C.-C. Lee, K. Sridhar, J.-L. Li, W.-C.Lin, B.-H. Su, and C. Busso, “Deep representation learning for affective speech signal analysis and processing: Preventing unwanted signal disparities,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>, vol. 38, no. 6, pp. 22–38, November 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
E. Lakomkin, M. A. Zamani, C. Weber, S. Magg, and S. Wermter, “On the robustness of speech emotion recognition for human-robot interaction with deep neural networks,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2018)</em>, Madrid, Spain, October 2018, pp. 854–860.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y.-T. Wu and C.-C. Lee, “MetricAug: A distortion metric-lead augmentation strategy for training noise-robust speech emotion recognizer,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ISCA Interspeech 2023</em>, Dublin, Ireland, August 2023, pp. 3587–3591.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
<span id="bib.bib13.1.1" class="ltx_text ltx_font_caligraphic">L</span>. Juszkiewicz, “Improving noise robustness of speech emotion recognition system,” in <em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic">Intelligent Distributed Computing VII</em>, ser. International Symposium on Intelligent Distributed Computing (IDC 2013), F. Zavoral, J. Jung, and C. Badica, Eds.   Prague, Czech Republic: Springer International Publishing, 2014, vol. 511, pp. 223–232.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A. Triantafyllopoulos, G. Keren, J. Wagner, I. Steiner, and B. Schuller, “Towards robust speech emotion recognition using deep residual networks for speech enhancement,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Interspeech 2019</em>, Graz, Austria, September 2019, pp. 1691–1695.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
B. Schuller, D. Arsic, F. Wallhoff, and G. Rigoll, “Emotion recognition in the noise applying large acoustic feature sets,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ISCA Speech Prosody</em>.   Dresden, Germany: ISCA, May 2006.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S.-G. Leem, D. Fulford, J.-P. Onnela, D. Gard, and C. Busso, “Not all features are equal: Selection of robust features for speech emotion recognition in noisy environments,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2022)</em>, Singapore, May 2022, pp. 6447–6451.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A. Wilf and E. Mower Provost, “Towards noise robust speech emotion recognition using dynamic layer customization,” in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">International Conference on Affective Computing and Intelligent Interaction (ACII 2021)</em>, Nara, Japan, September-October 2021, pp. 1–8.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S.-G. Leem, D. Fulford, J.-P. Onnela, D. Gard, and C. Busso, “Separation of emotional and reconstruction embeddings on ladder network to improve speech emotion recognition robustness in noisy conditions,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Interspeech 2021</em>, Brno, Czech Republic, August-September 2021, pp. 2871–2875.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
V. Mitra, V. Kowtha, H.-Y. S. Chien, E. Azemi, and C. Avendano, “Pre-trained model representations and their robustness against noise for speech emotion analysis,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023)</em>, Rhodes Island, Greece, June 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S.-G. Leem, D. Fulford, J.-P. Onnela, D. Gard, and C. Busso, “Adapting a self-supervised speech representation for noisy speech emotion recognition by using contrastive teacher-student learning,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023)</em>, Rhodes island, Greece, June 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
——, “Computation and memory efficient noise adaptation of Wav2Vec2.0 for noisy speech emotion recognition with skip connection adapters,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Interspeech 2023</em>, Dublin, Ireland, August 2023, pp. 1888–1892.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A. Radford, J. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, “Learning transferable visual models from natural language supervision,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning (ICML 2021)</em>, M. Meila, , and T. Zhang, Eds.   Virtual: Proceedings of Machine Learning Research (PMLR), July 2021, vol. 139, pp. 8748–8763.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnov, “Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023)</em>, Rhodes Island, Greece, June 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
B. Elizalde, S. Deshmukh, M. A. Ismail, and H. Wang, “CLAP learning audio concepts from natural language supervision,” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023)</em>, Rhodes Island, Greece, June 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
E. Stanley, E. DeMattos, A. Klementiev, P. Ozimek, G. Clarke, M. Berger, and D. Palaz, “Emotion label encoding using word embeddings for speech emotion recognition,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">ISCA Interspeech 2023</em>, Dublin, Ireland, August 2023, pp. 2418–2422.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
T. Gong, J. Belanich, K. Somandepalli, A. Nagrani, B. Eoff, and B. Jou, “LanSER: Language-model supported speech emotion recognition,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">ISCA Interspeech 2023</em>, Dublin, Ireland, August 2023, pp. 2408–2412.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J. E. Vargas-Munoz, S. Srivastava, D. Tuia, and A. X. F. ao, “OpenStreetMap: Challenges and opportunities in machine learning and remote sensing,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">IEEE Geoscience and Remote Sensing Magazine</em>, vol. 9, no. 1, pp. 184–199, March 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
M. Pandharipande, R. Chakraborty, A. Panda, and S. K. Kopparapu, “An unsupervised frame selection technique for robust emotion recognition in noisy speech,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">European Signal Processing Conference (EUSIPCO 2018)</em>, Rome, Italy, September 2018, pp. 2055–2059.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
S.-G. Leem, D. Fulford, J.-P. Onnela, D. Gard, and C. Busso, “Selective acoustic feature enhancement for speech emotion recognition with noisy speech,” <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech and Language Processing</em>, vol. 32, pp. 917–929, 2024.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
L. Goncalves, A. Salman, A. Reddy Naini, L. Moro-Velazquez, T. Thebaud, P. Garcia, N. Dehak, B. Sisman, and C. Busso, “Odyssey 2024 - speech emotion recognition challenge: Dataset, baseline framework, and results,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">The Speaker and Language Recognition Workshop (Odyssey 2024)</em>, Quebec, Canada, June 2024, pp. 247–254.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
L. Pepino, P. Riera, and L. Ferrer, “Emotion recognition from speech using Wav2vec 2.0 embeddings,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Interspeech 2021</em>, Brno, Czech Republic, August-September 2021, pp. 3400–3404.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
J. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt, F. Eyben, and B. Schuller, “Dawn of the transformer era in speech emotion recognition: Closing the valence gap,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 45, no. 9, pp. 10 745–10 759, September 2023.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
P. Mote, B. Sisman, and C. Busso, “Unsupervised domain adaptation for speech emotion recognition using K-Nearest neighbors voice conversion,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Interspeech 2024</em>, Kos Island, Greece, September 2024.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
L. Goncalves, D. Robinson, E. Richerson, and C. Busso, “Bridging emotions across languages: Low rank adaptation for multilingual speech emotion recognition,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Interspeech 2024</em>, Kos Island, Greece, September 2024.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
S. Upadhyay, C. Busso, and C.-C. Lee, “A layer-anchoring strategy for enhancing cross-lingual speech emotion recognition,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Interspeech 2024</em>, Kos Island, Greece, September 2024.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, “RoBERTa: A robustly optimized BERT pretraining approach,” <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">ArXiv e-prints (arXiv:1907.11692)</em>, pp. 1–12, July 2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, “GLUE: A multi-task benchmark and analysis platform for natural language understanding,” in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations (ICLR 2019)</em>, New Orleans, LA, USA, May 2019, pp. 1–20.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
A. Keesing, Y. Koh, and M. Witbrock, “Acoustic features and neural representations for categorical emotion recognition from speech,” in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Interspeech 2021</em>, Brno, Czech Republic, August-September 2021, pp. 3415–3419.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
E. Fonseca, J. Pons Puig, X. Favory, F. Font Corbera, D. Bogdanov, A. Ferraro, S. Oramas, A. Porter, and X. Serra, “Freesound datasets: a platform for the creation of open audio datasets,” in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">International Society for Music Information Retrieval (ISMIR 2017)</em>, Suzhou, China, October 2017, pp. 486–493.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
W.-N. Hsu, A. Sriram, A. Baevski, T. Likhomanenko, Q. Xu, V. Pratap, J. Kahn, A. Lee, R. Collobert, G. Synnaeve, and M. Auli, “Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">ArXiv e-prints (arXiv:2104.01027)</em>, pp. 1–9, April 2021.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
S.-W. Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Lin, A. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K.-T. Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H.-Y. Lee, “SUPERB: Speech Processing Universal PERformance Benchmark,” in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Interspeech 2021</em>, Brno, Czech Republic, August-September 2021, pp. 1194–1198.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame, and Q. L. amd A.M. Rush, “HuggingFace’s transformers: State-of-the-art natural language processing,” <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">ArXiv e-prints (arXiv:1910.03771v5)</em>, pp. 1–8, October 2019.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
D. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, San Diego, CA, USA, May 2015, pp. 1–13.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
M. M. Popel and O. Bojar, “Training tips for the transformer model,” <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">The Prague Bulletin of Mathematical Linguistics</em>, vol. 110, pp. 43–70, April 2018.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
K. Huang, Y.-K. Fu, Y. Zhang, and H.-Y. Lee, “Improving distortion robustness of self-supervised speech processing tasks with domain adaptation,” in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">ISCA Interspeech 2022</em>, Incheon, Korea, September 2022, pp. 2193–2197.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
L. McInnes, J. Healy, N. Saul, and L. Großberger, “UMAP: Uniform manifold approximation and projection,” <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Journal of Open Source Software,</em>, vol. 3, no. 29, p. 861, September 2018.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
J. Pennington, R. Socher, and C. Manning, “GloVe: Global vectors for word representation,” in <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Conference on Empirical Methods in Natural Language Processing (EMNLP 2014)</em>, Doha, Qatar, October 2014, pp. 1532–1543.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Y. Gong, Y.-A. Chung, and J. Glass, “AST: Audio spectrogram transformer,” in <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">ISCA Interspeech 2021</em>, Brno, Czechia, August-September 2021, pp. 571–575.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.17715" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.17716" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.17716">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.17716" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.17717" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 17:40:34 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
