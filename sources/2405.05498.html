<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2405.05498] The RoyalFlush Automatic Speech Diarization and Recognition System for In-Car Multi-Channel Automatic Speech Recognition Challenge</title><meta property="og:description" content="This paper presents our system submission for the In-Car Multi-Channel Automatic Speech Recognition (ICMC-ASR) Challenge, which focuses on speaker diarization and speech recognition in complex multi-speaker scenarios. …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The RoyalFlush Automatic Speech Diarization and Recognition System for In-Car Multi-Channel Automatic Speech Recognition Challenge">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="The RoyalFlush Automatic Speech Diarization and Recognition System for In-Car Multi-Channel Automatic Speech Recognition Challenge">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2405.05498">

<!--Generated on Wed Jun  5 15:14:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">The RoyalFlush Automatic Speech Diarization and Recognition System for In-Car Multi-Channel Automatic Speech Recognition Challenge</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text" style="font-size:90%;">This paper presents our system submission for the In-Car Multi-Channel Automatic Speech Recognition (ICMC-ASR) Challenge, which focuses on speaker diarization and speech recognition in complex multi-speaker scenarios. To address these challenges, we develop end-to-end speaker diarization models that notably decrease the diarization error rate (DER) by 49.58% compared to the official baseline on the development set. For speech recognition, we utilize self-supervised learning representations to train end-to-end ASR models. By integrating these models, we achieve a character error rate (CER) of 16.93% on the track 1 evaluation set, and a concatenated minimum permutation character error rate (cpCER) of 25.88% on the track 2 evaluation set.</span></p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_note_type">footnotetext: </span><span id="footnotex1.1" class="ltx_text" style="font-size:90%;">Equal contribution.</span></span></span></span>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">— <span id="p1.1.1.1.1" class="ltx_text ltx_font_medium">
ICMC-ASR, ASDR, TS-VAD, speaker diarization, speech recognition</span></span></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text" style="font-size:90%;">As cars have become an essential part of daily life, speech-based interaction in cars, a natural method of human-computer interaction, is gaining prominence. Unlike ASR systems used at home or in meetings, in-car systems face unique challenges. These challenges stem from the complex acoustic environment of the cockpit, various noises from both inside and outside the car, and different driving conditions. This has led to the establishment of the ICMC-ASR competition </span><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://icmcasr.org/</span></span></span><span id="S1.p1.1.2" class="ltx_text" style="font-size:90%;">. The challenge is split into two tracks: track 1 focuses on single-speaker ASR with oracle segmentation, while track 2 concentrates on multi-speaker ASR, requiring participants to engineer systems for speaker diarization and transcription.</span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">2 </span>System Description</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text" style="font-size:90%;">Fig. </span><a href="#S2.F1" title="Figure 1 ‣ 2.1 Speaker Diarization ‣ 2 System Description ‣ The RoyalFlush Automatic Speech Diarization and Recognition System for In-Car Multi-Channel Automatic Speech Recognition Challenge" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S2.p1.1.2" class="ltx_text" style="font-size:90%;"> (a) illustrates our modular automatic speech diarization and recognition (ASDR) system for the ICMC-ASR challenge. Multi-channel speech signals undergo processing by acoustic echo cancellation (AEC) and independent vector analysis (IVA) to yield enhanced audio. The speaker diarization module primarily relies on target-speaker voice activity detection (TS-VAD). Guided source separation (GSS) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S2.p1.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.5" class="ltx_text" style="font-size:90%;"> utilizes speaker activity information provided by TS-VAD and enhanced audio to perform front-end enhancement of overlapped speech signals. Following GSS, we partition the audio based on the diarization output and feed it into a single-speaker HuBERT-based ASR module.</span></p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Speaker Diarization</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p"><span id="S2.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">TS-VAD was introduced as a method to identify the vocal activities of a specific group of speakers within an input audio signal, using the known profiles of these speakers </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S2.SS1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS1.p1.1.4" class="ltx_text" style="font-size:90%;">. Typically, these speaker profiles are acquired in advance through a process known as clustering-based speaker diarization. Our model </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS1.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S2.SS1.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS1.p1.1.7" class="ltx_text" style="font-size:90%;"> for this process consists of TCN-based VAD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS1.p1.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="S2.SS1.p1.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS1.p1.1.10" class="ltx_text" style="font-size:90%;">, a Resnet34-TSDP speaker embedding extractor, and NMESC. In contrast to the original TS-VAD, which uses i-vector for target-speaker embedding, we employ neural speaker embedding extracted by Resnet34-TSDP for target-speaker detection. The TS-VAD model is composed of three modules: 1) a frame-level speaker embedding encoder, which has the same architecture as the target-speaker embedding extractor, 2) a speaker detection module based on either Transformer or Conformer, 3) a combining module that utilizes either BLSTM or DPRNN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS1.p1.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="S2.SS1.p1.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS1.p1.1.13" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<figure id="S2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S2.F1.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:173.4pt;">
<p id="S2.F1.1.1" class="ltx_p ltx_align_center"><span id="S2.F1.1.1.1" class="ltx_text" style="font-size:90%;"><img src="/html/2405.05498/assets/Picture1.png" id="S2.F1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="138" height="132" alt="Refer to caption"></span></p>
<p id="S2.F1.1.2" class="ltx_p ltx_align_center ltx_align_center"><span id="S2.F1.1.2.1" class="ltx_text" style="font-size:90%;">(a)</span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S2.F1.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:260.2pt;">
<p id="S2.F1.2.1" class="ltx_p ltx_align_center"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;"><img src="/html/2405.05498/assets/Picture2.png" id="S2.F1.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="181" height="129" alt="Refer to caption"></span></p>
<p id="S2.F1.2.2" class="ltx_p ltx_align_center ltx_align_center"><span id="S2.F1.2.2.1" class="ltx_text" style="font-size:90%;">(b)</span></p>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S2.F1.3" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:433.6pt;">
<p id="S2.F1.3.1" class="ltx_p ltx_align_center"><span id="S2.F1.3.1.1" class="ltx_text" style="font-size:90%;"><img src="/html/2405.05498/assets/Picture3.png" id="S2.F1.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="236" height="99" alt="Refer to caption"></span></p>
<p id="S2.F1.3.2" class="ltx_p ltx_align_center ltx_align_center"><span id="S2.F1.3.2.1" class="ltx_text" style="font-size:90%;">(c)</span></p>
</div>
</div>
</div>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.7.1.1" class="ltx_text ltx_font_bold">Fig. 1</span>: </span>(a) The overview of the ASDR system; (b) Data flow for speaker diarization training; (c) Data flow for ASR training.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Speech Recognition</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p"><span id="S2.SS2.p1.1.1" class="ltx_text" style="font-size:90%;">Drawing inspiration from IRIS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS2.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S2.SS2.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS2.p1.1.4" class="ltx_text" style="font-size:90%;">, we developed an end-to-end speech recognition system. This system comprises three key components: a DCCRN-VAE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS2.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S2.SS2.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS2.p1.1.7" class="ltx_text" style="font-size:90%;"> for speech enhancement (SE), a HuBERT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS2.p1.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S2.SS2.p1.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.SS2.p1.1.10" class="ltx_text" style="font-size:90%;"> for self-supervised learning representation (SSLR), and a joint CTC/attention-based encoder-decoder for ASR. The acoustic model of the ASR mirrors the official baseline, utilizing an E-Branchformer-based encoder and a transformer-based decoder. Additionally, we incorporated a transformer-based language model (LM) into our system.</span></p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.4.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>The results of the ASDR system on the ICMC-ASR development and evaluation set.</figcaption>
<table id="S2.T1.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.5.1.1" class="ltx_tr">
<td id="S2.T1.5.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="4"><span id="S2.T1.5.1.1.1.1" class="ltx_text" style="font-size:90%;">Development Set</span></td>
<td id="S2.T1.5.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="3"><span id="S2.T1.5.1.1.2.1" class="ltx_text" style="font-size:90%;">Evaluation set</span></td>
</tr>
<tr id="S2.T1.5.2.2" class="ltx_tr">
<td id="S2.T1.5.2.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.2.2.1.1" class="ltx_text" style="font-size:90%;">Speaker Diarization Model</span></td>
<td id="S2.T1.5.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.2.2.2.1" class="ltx_text" style="font-size:90%;">DER</span></td>
<td id="S2.T1.5.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.2.2.3.1" class="ltx_text" style="font-size:90%;">ASR Model</span></td>
<td id="S2.T1.5.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.2.2.4.1" class="ltx_text" style="font-size:90%;">CER</span></td>
<td id="S2.T1.5.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.2.2.5.1" class="ltx_text" style="font-size:90%;">ASDR Model</span></td>
<td id="S2.T1.5.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.2.2.6.1" class="ltx_text" style="font-size:90%;">Track 1 (CER)</span></td>
<td id="S2.T1.5.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.2.2.7.1" class="ltx_text" style="font-size:90%;">Track 2 (cpCER)</span></td>
</tr>
<tr id="S2.T1.5.3.3" class="ltx_tr">
<td id="S2.T1.5.3.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.3.3.1.1" class="ltx_text" style="font-size:90%;">Baseline</span></td>
<td id="S2.T1.5.3.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.3.3.2.1" class="ltx_text" style="font-size:90%;">54.79%</span></td>
<td id="S2.T1.5.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.3.3.3.1" class="ltx_text" style="font-size:90%;">Baseline</span></td>
<td id="S2.T1.5.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.3.3.4.1" class="ltx_text" style="font-size:90%;">32.92%</span></td>
<td id="S2.T1.5.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.3.3.5.1" class="ltx_text" style="font-size:90%;">Baseline</span></td>
<td id="S2.T1.5.3.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.3.3.6.1" class="ltx_text" style="font-size:90%;">26.24%</span></td>
<td id="S2.T1.5.3.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.3.3.7.1" class="ltx_text" style="font-size:90%;">72.88%</span></td>
</tr>
<tr id="S2.T1.5.4.4" class="ltx_tr">
<td id="S2.T1.5.4.4.1" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.4.4.1.1" class="ltx_text" style="font-size:90%;">Transformer-BLSTM</span></td>
<td id="S2.T1.5.4.4.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.4.4.2.1" class="ltx_text" style="font-size:90%;">5.90%</span></td>
<td id="S2.T1.5.4.4.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.4.4.3.1" class="ltx_text" style="font-size:90%;">HuBERT-ASR</span></td>
<td id="S2.T1.5.4.4.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.4.4.4.1" class="ltx_text" style="font-size:90%;">23.56%</span></td>
<td id="S2.T1.5.4.4.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.4.4.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S2.T1.5.4.4.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.4.4.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S2.T1.5.4.4.7" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.4.4.7.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S2.T1.5.5.5" class="ltx_tr">
<td id="S2.T1.5.5.5.1" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.5.5.1.1" class="ltx_text" style="font-size:90%;">Transformer-DPRNN</span></td>
<td id="S2.T1.5.5.5.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.5.5.2.1" class="ltx_text" style="font-size:90%;">5.67%</span></td>
<td id="S2.T1.5.5.5.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.5.5.3.1" class="ltx_text" style="font-size:90%;">SE-HuBERT-ASR-Part</span></td>
<td id="S2.T1.5.5.5.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.5.5.4.1" class="ltx_text" style="font-size:90%;">23.07%</span></td>
<td id="S2.T1.5.5.5.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.5.5.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S2.T1.5.5.5.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.5.5.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S2.T1.5.5.5.7" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.5.5.7.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S2.T1.5.6.6" class="ltx_tr">
<td id="S2.T1.5.6.6.1" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.6.6.1.1" class="ltx_text" style="font-size:90%;">Conformer-BLSTM</span></td>
<td id="S2.T1.5.6.6.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.6.6.2.1" class="ltx_text" style="font-size:90%;">5.95%</span></td>
<td id="S2.T1.5.6.6.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.6.6.3.1" class="ltx_text" style="font-size:90%;">SE-HuBERT-ASR-All</span></td>
<td id="S2.T1.5.6.6.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.6.6.4.1" class="ltx_text" style="font-size:90%;">22.85%</span></td>
<td id="S2.T1.5.6.6.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.6.6.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S2.T1.5.6.6.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.6.6.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S2.T1.5.6.6.7" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.6.6.7.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S2.T1.5.7.7" class="ltx_tr">
<td id="S2.T1.5.7.7.1" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.7.7.1.1" class="ltx_text" style="font-size:90%;">Conformer-DPRNN</span></td>
<td id="S2.T1.5.7.7.2" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.7.7.2.1" class="ltx_text" style="font-size:90%;">5.69%</span></td>
<td id="S2.T1.5.7.7.3" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.7.7.3.1" class="ltx_text" style="font-size:90%;">SE-HuBERT-ASR-All-LM</span></td>
<td id="S2.T1.5.7.7.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.7.7.4.1" class="ltx_text" style="font-size:90%;">22.84%</span></td>
<td id="S2.T1.5.7.7.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.7.7.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S2.T1.5.7.7.6" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.7.7.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S2.T1.5.7.7.7" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.7.7.7.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S2.T1.5.8.8" class="ltx_tr">
<td id="S2.T1.5.8.8.1" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.8.8.1.1" class="ltx_text" style="font-size:90%;">Fusion</span></td>
<td id="S2.T1.5.8.8.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S2.T1.5.8.8.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">5.21</span><span id="S2.T1.5.8.8.2.2" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="S2.T1.5.8.8.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.8.8.3.1" class="ltx_text" style="font-size:90%;">Fusion</span></td>
<td id="S2.T1.5.8.8.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S2.T1.5.8.8.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">21.77</span><span id="S2.T1.5.8.8.4.2" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="S2.T1.5.8.8.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S2.T1.5.8.8.5.1" class="ltx_text" style="font-size:90%;">Fusion</span></td>
<td id="S2.T1.5.8.8.6" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S2.T1.5.8.8.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">16.93</span><span id="S2.T1.5.8.8.6.2" class="ltx_text" style="font-size:90%;">%</span>
</td>
<td id="S2.T1.5.8.8.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S2.T1.5.8.8.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">25.88</span><span id="S2.T1.5.8.8.7.2" class="ltx_text" style="font-size:90%;">%</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup and Results</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">Besides the ICMC-ASR corpus, we incorporated external data as well. A comprehensive presentation of the training data can be found in Table </span><a href="#S3.T2" title="Table 2 ‣ 3.1 Datasets ‣ 3 Experimental Setup and Results ‣ The RoyalFlush Automatic Speech Diarization and Recognition System for In-Car Multi-Channel Automatic Speech Recognition Challenge" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S3.SS1.p1.1.2" class="ltx_text" style="font-size:90%;">. The data flow of each training process is depicted in Fig. </span><a href="#S2.F1" title="Figure 1 ‣ 2.1 Speaker Diarization ‣ 2 System Description ‣ The RoyalFlush Automatic Speech Diarization and Recognition System for In-Car Multi-Channel Automatic Speech Recognition Challenge" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S3.SS1.p1.1.3" class="ltx_text" style="font-size:90%;"> (b) and (c). We generated a substantial amount of training data using a diverse range of data processing methods. These include DCCRN and DCCRN-VAE-based SE, WPE </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S3.SS1.p1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS1.p1.1.6" class="ltx_text" style="font-size:90%;">, BeamformIt </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS1.p1.1.7.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S3.SS1.p1.1.8.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS1.p1.1.9" class="ltx_text" style="font-size:90%;">, adding noise, overlapped speech simulation, GSS-based augmentation, and speed perturbation.</span></p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.4.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>The data utilized for training in our system.</figcaption>
<table id="S3.T2.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.5.1.1" class="ltx_tr">
<th id="S3.T2.5.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S3.T2.5.1.1.1.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<th id="S3.T2.5.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S3.T2.5.1.1.2.1" class="ltx_text" style="font-size:90%;">Training data</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.5.2.1" class="ltx_tr">
<td id="S3.T2.5.2.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S3.T2.5.2.1.1.1" class="ltx_text" style="font-size:90%;">TCN &amp; TS-VAD</span></td>
<td id="S3.T2.5.2.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S3.T2.5.2.1.2.1" class="ltx_text" style="font-size:90%;">ICMC-ASR far-field training set</span></td>
</tr>
<tr id="S3.T2.5.3.2" class="ltx_tr">
<td id="S3.T2.5.3.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S3.T2.5.3.2.1.1" class="ltx_text" style="font-size:90%;">Resnet34-TSDP</span></td>
<td id="S3.T2.5.3.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;">
<span id="S3.T2.5.3.2.2.1" class="ltx_text" style="font-size:90%;">CNCeleb </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.5.3.2.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S3.T2.5.3.2.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S3.T2.5.4.3" class="ltx_tr">
<td id="S3.T2.5.4.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S3.T2.5.4.3.1.1" class="ltx_text" style="font-size:90%;">DCCRN-VAE &amp; DCCRN</span></td>
<td id="S3.T2.5.4.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;">
<span id="S3.T2.5.4.3.2.1" class="ltx_text" style="font-size:90%;">DNS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.5.4.3.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib7" title="" class="ltx_ref">7</a><span id="S3.T2.5.4.3.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S3.T2.5.5.4" class="ltx_tr">
<td id="S3.T2.5.5.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S3.T2.5.5.4.1.1" class="ltx_text" style="font-size:90%;">HuBERT</span></td>
<td id="S3.T2.5.5.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;">
<span id="S3.T2.5.5.4.2.1" class="ltx_text" style="font-size:90%;">WenetSpeech </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T2.5.5.4.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S3.T2.5.5.4.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S3.T2.5.6.5" class="ltx_tr">
<td id="S3.T2.5.6.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S3.T2.5.6.5.1.1" class="ltx_text" style="font-size:90%;">HuBERT-ASR</span></td>
<td id="S3.T2.5.6.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S3.T2.5.6.5.2.1" class="ltx_text" style="font-size:90%;">ICMC-ASR near and far-field training set</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>System Setup</h3>

<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Speaker Diarization</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p"><span id="S3.SS2.SSS1.p1.1.1" class="ltx_text" style="font-size:90%;">For speaker diarization, we created four TS-VAD models, each uniquely integrating speaker detection and combining modules. The models were Transformer-BLSTM, Transformer-DPRNN, Conformer-BLSTM, and Conformer-DPRNN. Data augmentation, which involves adding noise, is performed on-the-fly using ICMC-ASR noise audio. In the inference stage, the TS-VAD results from different single-channel speech are fused using DOVER-Lap </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.SSS1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S3.SS2.SSS1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.SSS1.p1.1.4" class="ltx_text" style="font-size:90%;">. Several other aspects of the model bear similarity to </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.SSS1.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S3.SS2.SSS1.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.SSS1.p1.1.7" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Speech Recognition</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p"><span id="S3.SS2.SSS2.p1.1.1" class="ltx_text" style="font-size:90%;">We adopted a pre-training and fine-tuning scheme to train ASR models </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.SSS2.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S3.SS2.SSS2.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.SSS2.p1.1.4" class="ltx_text" style="font-size:90%;">. We utilized a total of four ASR models, namely HuBERT-ASR (without SE), SE-HuBERT-ASR-Part (fine-tuning only SE and ASR), SE-HuBERT-ASR-All (jointly fine-tuning all parameters), and SE-HuBERT-ASR-All-LM (incorporating a LM). During the inference stage, the results from different ASR models are fused using ROVER </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.SSS2.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S3.SS2.SSS2.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.SS2.SSS2.p1.1.7" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Experimental Results</h3>

<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Speaker Diarization</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p"><span id="S3.SS3.SSS1.p1.1.1" class="ltx_text" style="font-size:90%;">The performances of the speaker diarization models on the development set are shown in Table </span><a href="#S2.T1" title="Table 1 ‣ 2.2 Speech Recognition ‣ 2 System Description ‣ The RoyalFlush Automatic Speech Diarization and Recognition System for In-Car Multi-Channel Automatic Speech Recognition Challenge" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S3.SS3.SSS1.p1.1.2" class="ltx_text" style="font-size:90%;">. The four TS-VAD models exhibit comparable performance. However, when used as a combining module, DPRNN holds a marginal edge over BLSTM. As for speaker detection modules, Transformer and Conformer show negligible differences. Fusing the four TS-VAD models leads to a 5.21% DER, which is a 49.58% absolute reduction over the official baseline.</span></p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Speech Recognition</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p"><span id="S3.SS3.SSS2.p1.1.1" class="ltx_text" style="font-size:90%;">As shown in Table </span><a href="#S2.T1" title="Table 1 ‣ 2.2 Speech Recognition ‣ 2 System Description ‣ The RoyalFlush Automatic Speech Diarization and Recognition System for In-Car Multi-Channel Automatic Speech Recognition Challenge" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S3.SS3.SSS2.p1.1.2" class="ltx_text" style="font-size:90%;">, the SE-HuBERT-ASR-All-LM achieves the best performance among single models. Compared with the official baseline, the fusion scheme results in an absolute CER reduction of 11.15% (from 32.92% to 21.77%) on the development set and 9.31% (from 26.24% to 16.93%) on the evaluation set for track1. By integrating the results from both the fused TS-VAD models and the fused ASR models, we obtained an absolute cpCER reduction of 47.00% (from 72.88% to 25.88%) on the evaluation set of track2.</span></p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusions</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text" style="font-size:90%;">We introduced the ASDR system for the ICMC-ASR challenge. Our findings highlight the significant contribution of TS-VAD-based speaker diarization to this task. Furthermore, we demonstrate that a robust speech recognition system can be constructed under noisy conditions by collectively fine-tuning SE, SSLR, and ASR.</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
D Raj, D Povey, and S Khudanpur,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">“Gpu-accelerated guided source separation for meeting transcription,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2212.05271</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
I Medennikov, M Korenevsky, and et al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">“Target-speaker voice activity detection: A novel approach for multi-speaker diarization in a dinner party scenario,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Interspeech</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2020, pp. 274–278.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
J Tian, X Hu, and X Xu,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">“Royalflush speaker diarization system for icassp 2022 multi-channel multi-party meeting transcription challenge,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2202.04814</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Z Yin, J Tian, and et al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">“Large-scale learning on overlapped speech detection: New benchmark and new general system,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2308.05987</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Y Luo, Z Chen, and T Yoshioka,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">“Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2020, pp. 46–50.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
X Chang, T Maekaku, and et al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">“End-to-end integration of speech recognition, speech enhancement, and self-supervised learning representation,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">INTERSPEECH</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2022, vol. 2022, pp. 3819–3823.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Y Xiang, J Tian, and et al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">“A deep representation learning-based speech enhancement method using complex convolution recurrent variational autoencoder,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2024.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
W.-N Hsu, Y.-H. H Tsai, and et al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">“Hubert: How much can a bad teacher benefit asr pre-training?,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2021, pp. 6533–6537.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
L Li, R Liu, and et al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">“Cn-celeb: multi-genre speaker recognition,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Speech Communication</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, vol. 137, pp. 77–91, 2022.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
B Zhang, H Lv, and et al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">“Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2022, pp. 6182–6186.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
D Raj, L. P Garcia-Perera, and et al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">“Dover-lap: A method for combining overlap-aware diarization outputs,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">SLT</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2021, pp. 881–888.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
W Wang, X Qin, and M Li,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">“Cross-channel attention-based target speaker voice activity detection: Experimental results for the m2met challenge,”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2022, pp. 9171–9175.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
J. G Fiscus,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">“A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (rover),”
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ASRU</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 1997.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2405.05497" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2405.05498" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2405.05498">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2405.05498" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2405.05499" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Jun  5 15:14:18 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
