<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.04269] Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement</title><meta property="og:description" content="Deep learning-based speech enhancement (SE) models have achieved impressive performance in the past decade.
Numerous advanced architectures have been designed to deliver state-of-the-art performance; however, their sca…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.04269">

<!--Generated on Fri Jul  5 23:36:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechcameraready</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=1,2]WangyouZhang
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=3]KoheiSaijo
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=2]Jee-weonJung
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>[affiliation=1,2]ChendaLi
<span id="p1.3.4" class="ltx_ERROR undefined">\name</span>[affiliation=2]ShinjiWatanabe
<span id="p1.3.5" class="ltx_ERROR undefined">\name</span>[affiliation=1]YanminQian




</p>
</div>
<h1 class="ltx_title ltx_title_document">Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">Deep learning-based speech enhancement (SE) models have achieved impressive performance in the past decade.
Numerous advanced architectures have been designed to deliver state-of-the-art performance; however, their scalability potential remains unrevealed.
Meanwhile, the majority of research focuses on small-sized datasets with restricted diversity, leading to a plateau in performance improvement.
In this paper, we aim to provide new insights for addressing the above issues by exploring the scalability of SE models in terms of architectures, model sizes, compute budgets, and dataset sizes.
Our investigation involves several popular SE architectures and speech data from different domains.
Experiments reveal both similarities and distinctions between the scaling effects in SE and other tasks such as speech recognition.
These findings further provide insights into the under-explored SE directions, e.g., larger-scale multi-domain corpora and efficiently scalable architectures.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>speech enhancement, scalability, robustness, generalizability
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Speech enhancement (SE) is the task of removing undesired signals from the input speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, which often includes noise and reverberation.
The last decade has witnessed significant progress in deep learning-based SE approaches in both frequency and time domains.
The state-of-the-art (SOTA) SE approaches often feature a carefully designed architecture with sophisticated interactions between different features.
Typical architectures of SE models include the U-Net structure <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, dilated convolution-based structure <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, and dual-path structure with recurrent neural network (RNN) or attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While a wide variety of SE models have been developed in recent years and demonstrate surprisingly strong performance on specific datasets, there remain several issues.
1) Most SE research has been conducted on small-scale datasets (e.g., VoiceBank+DEMAND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>) with limited diversity. The limited data amount hinders us from understanding the scalability of the SE approach. The limited evaluation condition also makes it difficult to understand the generalizability and robustness in realistic applications.
2) While it has been commonly observed that more parameters and higher complexity often result in better performance, the scaling effect of model complexity along with data scales has not been well studied in the SE literature. In particular, it is still unclear whether a large amount of data and high-complexity models are the best way to achieve the best performance. 3) Compared to the recent trend of large models showing unprecedented performances in automatic speech recognition (ASR) and large language models (LLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, this area of research is under-explored in SE.
This calls for an extensive investigation into the scaling law of SE models as an important step in building a large SE foundation model.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">As the first step to address the above issues, in this paper, we aim to explore the scalability of single-channel SE models.
Following existing studies on scaling laws in ASR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, natural language processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, and computer vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, we explore several typical scaling factors.
Specifically, they include model architectures, model sizes, compute budgets, and dataset sizes.
We further take the model setup (causal or non-causal) into account to encompass both real-time and offline applications.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">While our investigation is driven by motivations akin to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, it distinguishes itself through several novel findings and complements the existing work in five aspects: 1) larger model complexity; 2) extensive investigation of both causal and non-causal setups; 3) multiple data scales; 4) extended coverage of SE sub-tasks; and 5) comprehensive multi-domain evaluation.
Our key contributions are summarized below:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1)</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">By covering a wider range of model complexity, we reveal the <em id="S1.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">distinct potentials of different SE architectures in low and high complexity regions</em>.
Among them, BSRNN and TF-GridNet show exceptional scalabilities respectively in low and (relatively) high complexity ranges.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2)</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We show that <em id="S1.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">scaling model complexity along with multi-domain data sizes consistently improves all metrics</em>.
This aligns with the findings in large-scale ASR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3)</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">SE models, compared to ASR and LLMs, <em id="S1.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">particularly suffer from data mismatch</em> due to the limitations of simulation-based training data.
As a result, largely increasing simulated data based on the same source corpora is not as effective as increasing data diversity.
Although we successfully scaled the data up to 157 h, further scaling up encountered various issues such as scarcity of high quality data, limited diversity, and highly imbalanced data distribution.
This calls for the construction of larger-scale multi-domain SE corpora, as we are still far away from the data scale in ASR research (e.g., 1M h).</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4)</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">Although RNN-based architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> have excelled in popular SE benchmarks, they are highly inefficient in scaling up, demanding much larger compute due to limited parallelization.
<em id="S1.I1.i4.p1.1.1" class="ltx_emph ltx_font_italic">We are still lacking an SE counterpart to transformers in ASR and LLMs that have well-recognized scalability and outstanding performance.</em></p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5)</span> 
<div id="S1.I1.i5.p1" class="ltx_para">
<p id="S1.I1.i5.p1.1" class="ltx_p">Finally, we show that model parameters are not very informative when comparing different architectures, as they usually have distinct parameter efficiency.
Instead, <em id="S1.I1.i5.p1.1.1" class="ltx_emph ltx_font_italic">model complexity in terms of computational costs (#MACs) should be compared</em>.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Model and experimental design</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Models</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">We aim to investigate SOTA SE models with diverse architectures to gain insights into their potential scalability.
Specifically, we experiment with band-split RNN (BSRNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, Conv-TasNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, DEMUCS-v4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, and TF-GridNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
We briefly introduce each model below, while their model configurations are presented in Tables <a href="#footnote1" title="footnote 1 ‣ Table 1 ‣ 2.1 Models ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S2.T2" title="Table 2 ‣ 2.1 Models ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Configurations of BSRNN models<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Detailed hyperparameter configurations of all models are available at <a target="_blank" href="https://github.com/Emrys365/se-scaling" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Emrys365/se-scaling</a>.</span></span></span>.</figcaption>
<table id="S2.T1.12" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.12.13.1" class="ltx_tr">
<th id="S2.T1.12.13.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S2.T1.12.13.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S2.T1.12.13.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S2.T1.12.13.1.2.1" class="ltx_text ltx_font_bold">Causal</span></th>
<td id="S2.T1.12.13.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S2.T1.12.13.1.3.1" class="ltx_text ltx_font_bold">#Params (M)</span></td>
<td id="S2.T1.12.13.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="2"><span id="S2.T1.12.13.1.4.1" class="ltx_text ltx_font_bold">#MACs (G/s)</span></td>
</tr>
<tr id="S2.T1.12.14.2" class="ltx_tr">
<td id="S2.T1.12.14.2.1" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T1.12.14.2.1.1" class="ltx_text ltx_font_bold">16 kHz</span></td>
<td id="S2.T1.12.14.2.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T1.12.14.2.2.1" class="ltx_text ltx_font_bold">48 kHz</span></td>
</tr>
<tr id="S2.T1.12.15.3" class="ltx_tr">
<th id="S2.T1.12.15.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="2">
<span id="S2.T1.12.15.3.1.1" class="ltx_ERROR undefined">\cellcolor</span>[HTML]EEEEEE<em id="S2.T1.12.15.3.1.2" class="ltx_emph ltx_font_italic">BSRNN</em>
</th>
<td id="S2.T1.12.15.3.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="3">
<span id="S2.T1.12.15.3.2.1" class="ltx_ERROR undefined">\cellcolor</span>[HTML]EEEEEE(sampling-frequency-independent)</td>
</tr>
<tr id="S2.T1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S2.T1.1.1.2.1" class="ltx_text">xtiny</span></th>
<th id="S2.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T1.1.1.1.pic1" class="ltx_picture" height="10.09" overflow="visible" version="1.1" width="10.09"><g transform="translate(0,10.09) matrix(1 0 0 -1 0 0) translate(0.55,0) translate(0,0.55)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 2.26 0 C 4.26 3.95 5.82 6.03 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.8pt" stroke-linecap="round"><path d="M 0 3.17 C 0.79 1.91 1.25 1.23 2.08 0" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T1.1.1.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.5</td>
<td id="S2.T1.1.1.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.1</td>
<td id="S2.T1.1.1.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.4</td>
</tr>
<tr id="S2.T1.2.2" class="ltx_tr">
<th id="S2.T1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T1.2.2.1.pic1" class="ltx_picture" height="10.02" overflow="visible" version="1.1" width="10.02"><g transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T1.2.2.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.5</td>
<td id="S2.T1.2.2.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.2</td>
<td id="S2.T1.2.2.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.6</td>
</tr>
<tr id="S2.T1.3.3" class="ltx_tr">
<th id="S2.T1.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S2.T1.3.3.2.1" class="ltx_text">tiny</span></th>
<th id="S2.T1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T1.3.3.1.pic1" class="ltx_picture" height="10.09" overflow="visible" version="1.1" width="10.09"><g transform="translate(0,10.09) matrix(1 0 0 -1 0 0) translate(0.55,0) translate(0,0.55)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 2.26 0 C 4.26 3.95 5.82 6.03 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.8pt" stroke-linecap="round"><path d="M 0 3.17 C 0.79 1.91 1.25 1.23 2.08 0" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T1.3.3.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1.3</td>
<td id="S2.T1.3.3.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.6</td>
<td id="S2.T1.3.3.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1.7</td>
</tr>
<tr id="S2.T1.4.4" class="ltx_tr">
<th id="S2.T1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T1.4.4.1.pic1" class="ltx_picture" height="10.02" overflow="visible" version="1.1" width="10.02"><g transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T1.4.4.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1.5</td>
<td id="S2.T1.4.4.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.7</td>
<td id="S2.T1.4.4.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2.2</td>
</tr>
<tr id="S2.T1.5.5" class="ltx_tr">
<th id="S2.T1.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S2.T1.5.5.2.1" class="ltx_text">small</span></th>
<th id="S2.T1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T1.5.5.1.pic1" class="ltx_picture" height="10.09" overflow="visible" version="1.1" width="10.09"><g transform="translate(0,10.09) matrix(1 0 0 -1 0 0) translate(0.55,0) translate(0,0.55)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 2.26 0 C 4.26 3.95 5.82 6.03 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.8pt" stroke-linecap="round"><path d="M 0 3.17 C 0.79 1.91 1.25 1.23 2.08 0" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T1.5.5.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">4.1</td>
<td id="S2.T1.5.5.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2.1</td>
<td id="S2.T1.5.5.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">6.4</td>
</tr>
<tr id="S2.T1.6.6" class="ltx_tr">
<th id="S2.T1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T1.6.6.1.pic1" class="ltx_picture" height="10.02" overflow="visible" version="1.1" width="10.02"><g transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T1.6.6.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">4.8</td>
<td id="S2.T1.6.6.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2.8</td>
<td id="S2.T1.6.6.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">8.5</td>
</tr>
<tr id="S2.T1.7.7" class="ltx_tr">
<th id="S2.T1.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S2.T1.7.7.2.1" class="ltx_text">medium</span></th>
<th id="S2.T1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T1.7.7.1.pic1" class="ltx_picture" height="10.09" overflow="visible" version="1.1" width="10.09"><g transform="translate(0,10.09) matrix(1 0 0 -1 0 0) translate(0.55,0) translate(0,0.55)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 2.26 0 C 4.26 3.95 5.82 6.03 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.8pt" stroke-linecap="round"><path d="M 0 3.17 C 0.79 1.91 1.25 1.23 2.08 0" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T1.7.7.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">14.3</td>
<td id="S2.T1.7.7.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">8.4</td>
<td id="S2.T1.7.7.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">25.2</td>
</tr>
<tr id="S2.T1.8.8" class="ltx_tr">
<th id="S2.T1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T1.8.8.1.pic1" class="ltx_picture" height="10.02" overflow="visible" version="1.1" width="10.02"><g transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T1.8.8.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">16.9</td>
<td id="S2.T1.8.8.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">11.2</td>
<td id="S2.T1.8.8.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">33.4</td>
</tr>
<tr id="S2.T1.9.9" class="ltx_tr">
<th id="S2.T1.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S2.T1.9.9.2.1" class="ltx_text">large</span></th>
<th id="S2.T1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T1.9.9.1.pic1" class="ltx_picture" height="10.09" overflow="visible" version="1.1" width="10.09"><g transform="translate(0,10.09) matrix(1 0 0 -1 0 0) translate(0.55,0) translate(0,0.55)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 2.26 0 C 4.26 3.95 5.82 6.03 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.8pt" stroke-linecap="round"><path d="M 0 3.17 C 0.79 1.91 1.25 1.23 2.08 0" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T1.9.9.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">52.9</td>
<td id="S2.T1.9.9.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">33.4</td>
<td id="S2.T1.9.9.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">99.9</td>
</tr>
<tr id="S2.T1.10.10" class="ltx_tr">
<th id="S2.T1.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T1.10.10.1.pic1" class="ltx_picture" height="10.02" overflow="visible" version="1.1" width="10.02"><g transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T1.10.10.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">63.1</td>
<td id="S2.T1.10.10.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">44.3</td>
<td id="S2.T1.10.10.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">132.5</td>
</tr>
<tr id="S2.T1.11.11" class="ltx_tr">
<th id="S2.T1.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S2.T1.11.11.2.1" class="ltx_text">xlarge</span></th>
<th id="S2.T1.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T1.11.11.1.pic1" class="ltx_picture" height="10.09" overflow="visible" version="1.1" width="10.09"><g transform="translate(0,10.09) matrix(1 0 0 -1 0 0) translate(0.55,0) translate(0,0.55)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 2.26 0 C 4.26 3.95 5.82 6.03 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.8pt" stroke-linecap="round"><path d="M 0 3.17 C 0.79 1.91 1.25 1.23 2.08 0" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T1.11.11.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">83.6</td>
<td id="S2.T1.11.11.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">66.1</td>
<td id="S2.T1.11.11.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">197.7</td>
</tr>
<tr id="S2.T1.12.12" class="ltx_tr">
<th id="S2.T1.12.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T1.12.12.1.pic1" class="ltx_picture" height="10.02" overflow="visible" version="1.1" width="10.02"><g transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T1.12.12.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">104.1</td>
<td id="S2.T1.12.12.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">87.9</td>
<td id="S2.T1.12.12.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">262.3</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p"><em id="S2.SS1.p2.1.1" class="ltx_emph ltx_font_italic">BSRNN</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> is a dual-path T-F domain SE model that can handle different sampling frequencies (SF).
It reduces the frequency dimension with a hand-crafted band division to lower the complexity.
Each T-F bin is projected to a high-dimensional embedding for RNN-based dual-path modeling.
We adopt the improved design in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> which combines masking and mapping to obtain the enhanced spectrum.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, BSRNN has shown superior performance over other architectures across different complexity.
Therefore, we first conduct extensive experiments with this model to investigate the scaling effect with different model setups (causal or non-causal) as shown in Table <a href="#footnote1" title="footnote 1 ‣ Table 1 ‣ 2.1 Models ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p"><em id="S2.SS1.p3.1.1" class="ltx_emph ltx_font_italic">Conv-TasNet</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> is a time-domain SE model, which consists of convolution-based encoder/decoder and stacked temporal convolutional networks (TCN). It features small kernel/stride sizes in the learnable encoder/decoder and a large receptive field thanks to the stacked TCNs.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p"><em id="S2.SS1.p4.1.1" class="ltx_emph ltx_font_italic">DEMUCS-v4</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> is a hybrid time and time-frequency (T-F) domain SE model with a U-Net architecture.
It consists of two parallel branches, i.e., time-domain U-Net and T-F domain U-Net.
The bottleneck features from both branches are fused via cross-attention in a cross-domain transformer encoder, and both branches' outputs are summed to obtain the enhanced speech.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p"><em id="S2.SS1.p5.1.1" class="ltx_emph ltx_font_italic">TF-GridNet</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> is the SOTA single-channel SE approach based on T-F dual-path modeling.
It also leverages RNNs for dual-path modeling and further enhances the sequence modeling capability by utilizing adjacent frames/frequency bins.
In addition, a cross-frame self-attention module is inserted after each dual-path modeling block to better exploit the global information.
In our preliminary experiments, we noticed that the original TF-GridNet already has very high computational costs, making it expensive to scale up.
Therefore, we focus on smaller-sized configurations for this model as shown in Table <a href="#S2.T2" title="Table 2 ‣ 2.1 Models ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Configurations of other SE models<a href="#footnote2" title="footnote 2 ‣ Table 1 ‣ 2.1 Models ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</figcaption>
<table id="S2.T2.13" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T2.13.14.1" class="ltx_tr">
<th id="S2.T2.13.14.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S2.T2.13.14.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S2.T2.13.14.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S2.T2.13.14.1.2.1" class="ltx_text ltx_font_bold">Causal</span></th>
<td id="S2.T2.13.14.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S2.T2.13.14.1.3.1" class="ltx_text ltx_font_bold">#Params (M)</span></td>
<td id="S2.T2.13.14.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="2"><span id="S2.T2.13.14.1.4.1" class="ltx_text ltx_font_bold">#MACs (G/s)</span></td>
</tr>
<tr id="S2.T2.13.15.2" class="ltx_tr">
<td id="S2.T2.13.15.2.1" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.13.15.2.1.1" class="ltx_text ltx_font_bold">16 kHz</span></td>
<td id="S2.T2.13.15.2.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T2.13.15.2.2.1" class="ltx_text ltx_font_bold">48 kHz</span></td>
</tr>
<tr id="S2.T2.13.16.3" class="ltx_tr">
<th id="S2.T2.13.16.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="2">
<span id="S2.T2.13.16.3.1.1" class="ltx_ERROR undefined">\cellcolor</span>[HTML]EEEEEE<em id="S2.T2.13.16.3.1.2" class="ltx_emph ltx_font_italic">Conv-TasNet</em>
</th>
<td id="S2.T2.13.16.3.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="3">
<span id="S2.T2.13.16.3.2.1" class="ltx_ERROR undefined">\cellcolor</span>[HTML]EEEEEE(input is always resampled to 48 kHz)</td>
</tr>
<tr id="S2.T2.1.1" class="ltx_tr">
<th id="S2.T2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">small</th>
<th id="S2.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T2.1.1.1.pic1" class="ltx_picture" height="10.02" overflow="visible" version="1.1" width="10.02"><g transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T2.1.1.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1.1</td>
<td id="S2.T2.1.1.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td id="S2.T2.1.1.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">8.9</td>
</tr>
<tr id="S2.T2.2.2" class="ltx_tr">
<th id="S2.T2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">medium</th>
<th id="S2.T2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T2.2.2.1.pic1" class="ltx_picture" height="10.02" overflow="visible" version="1.1" width="10.02"><g transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T2.2.2.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">14.3</td>
<td id="S2.T2.2.2.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td id="S2.T2.2.2.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">18.7</td>
</tr>
<tr id="S2.T2.3.3" class="ltx_tr">
<th id="S2.T2.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">large</th>
<th id="S2.T2.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T2.3.3.1.pic1" class="ltx_picture" height="10.02" overflow="visible" version="1.1" width="10.02"><g transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T2.3.3.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">52.6</td>
<td id="S2.T2.3.3.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td id="S2.T2.3.3.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">47.2</td>
</tr>
<tr id="S2.T2.4.4" class="ltx_tr">
<th id="S2.T2.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">xlarge</th>
<th id="S2.T2.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T2.4.4.1.pic1" class="ltx_picture" height="10.02" overflow="visible" version="1.1" width="10.02"><g transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T2.4.4.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">103.9</td>
<td id="S2.T2.4.4.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td id="S2.T2.4.4.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">85.4</td>
</tr>
<tr id="S2.T2.13.17.4" class="ltx_tr">
<th id="S2.T2.13.17.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="2">
<span id="S2.T2.13.17.4.1.1" class="ltx_ERROR undefined">\cellcolor</span>[HTML]EEEEEE<em id="S2.T2.13.17.4.1.2" class="ltx_emph ltx_font_italic">DEMUCS-v4</em>
</th>
<td id="S2.T2.13.17.4.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="3">
<span id="S2.T2.13.17.4.2.1" class="ltx_ERROR undefined">\cellcolor</span>[HTML]EEEEEE(input is always resampld to 48 kHz)</td>
</tr>
<tr id="S2.T2.5.5" class="ltx_tr">
<th id="S2.T2.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">tiny</th>
<th id="S2.T2.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T2.5.5.1.pic1" class="ltx_picture" height="10.02" overflow="visible" version="1.1" width="10.02"><g transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T2.5.5.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1.0</td>
<td id="S2.T2.5.5.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td id="S2.T2.5.5.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1.0</td>
</tr>
<tr id="S2.T2.6.6" class="ltx_tr">
<th id="S2.T2.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">small</th>
<th id="S2.T2.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T2.6.6.1.pic1" class="ltx_picture" height="10.02" overflow="visible" version="1.1" width="10.02"><g transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T2.6.6.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">4.1</td>
<td id="S2.T2.6.6.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td id="S2.T2.6.6.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">3.5</td>
</tr>
<tr id="S2.T2.7.7" class="ltx_tr">
<th id="S2.T2.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">medium</th>
<th id="S2.T2.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T2.7.7.1.pic1" class="ltx_picture" height="10.02" overflow="visible" version="1.1" width="10.02"><g transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T2.7.7.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">16.2</td>
<td id="S2.T2.7.7.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td id="S2.T2.7.7.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">13.0</td>
</tr>
<tr id="S2.T2.8.8" class="ltx_tr">
<th id="S2.T2.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">large</th>
<th id="S2.T2.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T2.8.8.1.pic1" class="ltx_picture" height="10.02" overflow="visible" version="1.1" width="10.02"><g transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T2.8.8.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">26.9</td>
<td id="S2.T2.8.8.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td id="S2.T2.8.8.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">17.2</td>
</tr>
<tr id="S2.T2.9.9" class="ltx_tr">
<th id="S2.T2.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">xlarge</th>
<th id="S2.T2.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T2.9.9.1.pic1" class="ltx_picture" height="10.02" overflow="visible" version="1.1" width="10.02"><g transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T2.9.9.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">79.3</td>
<td id="S2.T2.9.9.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
<td id="S2.T2.9.9.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">40.7</td>
</tr>
<tr id="S2.T2.13.18.5" class="ltx_tr">
<th id="S2.T2.13.18.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="2">
<span id="S2.T2.13.18.5.1.1" class="ltx_ERROR undefined">\cellcolor</span>[HTML]EEEEEE<em id="S2.T2.13.18.5.1.2" class="ltx_emph ltx_font_italic">TF-GridNet</em>
</th>
<td id="S2.T2.13.18.5.2" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="3">
<span id="S2.T2.13.18.5.2.1" class="ltx_ERROR undefined">\cellcolor</span>[HTML]EEEEEE(sampling-frequency-independent)</td>
</tr>
<tr id="S2.T2.10.10" class="ltx_tr">
<th id="S2.T2.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">xxtiny</th>
<th id="S2.T2.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T2.10.10.1.pic1" class="ltx_picture" height="10.02" overflow="visible" version="1.1" width="10.02"><g transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T2.10.10.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.1</td>
<td id="S2.T2.10.10.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1.9</td>
<td id="S2.T2.10.10.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">5.6</td>
</tr>
<tr id="S2.T2.11.11" class="ltx_tr">
<th id="S2.T2.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">xtiny</th>
<th id="S2.T2.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T2.11.11.1.pic1" class="ltx_picture" height="10.02" overflow="visible" version="1.1" width="10.02"><g transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T2.11.11.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.5</td>
<td id="S2.T2.11.11.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">7.4</td>
<td id="S2.T2.11.11.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">21.7</td>
</tr>
<tr id="S2.T2.12.12" class="ltx_tr">
<th id="S2.T2.12.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">tiny</th>
<th id="S2.T2.12.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T2.12.12.1.pic1" class="ltx_picture" height="10.02" overflow="visible" version="1.1" width="10.02"><g transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T2.12.12.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">1.5</td>
<td id="S2.T2.12.12.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">24.1</td>
<td id="S2.T2.12.12.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">70.5</td>
</tr>
<tr id="S2.T2.13.13" class="ltx_tr">
<th id="S2.T2.13.13.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">small</th>
<th id="S2.T2.13.13.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><svg id="S2.T2.13.13.1.pic1" class="ltx_picture" height="10.02" overflow="visible" version="1.1" width="10.02"><g transform="translate(0,10.02) matrix(1 0 0 -1 0 0) translate(0.48,0) translate(0,0.48)" fill="#000000" stroke="#000000"><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 0 0 C 3.16 3.9 5.16 5.9 9.05 9.05" style="fill:none"></path></g><g stroke-width="0.7pt" stroke-linecap="round"><path d="M 1.81 8.6 C 3.77 5.3 4.95 3.53 7.24 0.45" style="fill:none"></path></g></g></svg>
</th>
<td id="S2.T2.13.13.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">5.7</td>
<td id="S2.T2.13.13.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">89.5</td>
<td id="S2.T2.13.13.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">261.8</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS1.p6" class="ltx_para">
<p id="S2.SS1.p6.4" class="ltx_p">To allow the processing of different SFs, the simple solution is to always resample the input signal to 48 kHz for all SE models regardless of their original SF <math id="S2.SS1.p6.1.m1.1" class="ltx_Math" alttext="f_{\text{org}}" display="inline"><semantics id="S2.SS1.p6.1.m1.1a"><msub id="S2.SS1.p6.1.m1.1.1" xref="S2.SS1.p6.1.m1.1.1.cmml"><mi id="S2.SS1.p6.1.m1.1.1.2" xref="S2.SS1.p6.1.m1.1.1.2.cmml">f</mi><mtext id="S2.SS1.p6.1.m1.1.1.3" xref="S2.SS1.p6.1.m1.1.1.3a.cmml">org</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.1.m1.1b"><apply id="S2.SS1.p6.1.m1.1.1.cmml" xref="S2.SS1.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.1.m1.1.1.1.cmml" xref="S2.SS1.p6.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p6.1.m1.1.1.2.cmml" xref="S2.SS1.p6.1.m1.1.1.2">𝑓</ci><ci id="S2.SS1.p6.1.m1.1.1.3a.cmml" xref="S2.SS1.p6.1.m1.1.1.3"><mtext mathsize="70%" id="S2.SS1.p6.1.m1.1.1.3.cmml" xref="S2.SS1.p6.1.m1.1.1.3">org</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.1.m1.1c">f_{\text{org}}</annotation></semantics></math>.
The output is then downsampled back to <math id="S2.SS1.p6.2.m2.1" class="ltx_Math" alttext="f_{\text{org}}" display="inline"><semantics id="S2.SS1.p6.2.m2.1a"><msub id="S2.SS1.p6.2.m2.1.1" xref="S2.SS1.p6.2.m2.1.1.cmml"><mi id="S2.SS1.p6.2.m2.1.1.2" xref="S2.SS1.p6.2.m2.1.1.2.cmml">f</mi><mtext id="S2.SS1.p6.2.m2.1.1.3" xref="S2.SS1.p6.2.m2.1.1.3a.cmml">org</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.2.m2.1b"><apply id="S2.SS1.p6.2.m2.1.1.cmml" xref="S2.SS1.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.2.m2.1.1.1.cmml" xref="S2.SS1.p6.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p6.2.m2.1.1.2.cmml" xref="S2.SS1.p6.2.m2.1.1.2">𝑓</ci><ci id="S2.SS1.p6.2.m2.1.1.3a.cmml" xref="S2.SS1.p6.2.m2.1.1.3"><mtext mathsize="70%" id="S2.SS1.p6.2.m2.1.1.3.cmml" xref="S2.SS1.p6.2.m2.1.1.3">org</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.2.m2.1c">f_{\text{org}}</annotation></semantics></math> for generating the enhanced output as well as for loss computation.
But specifically for T-F dual-path models (BSRNN and TF-GridNet), we adopt a different approach by applying the sampling-frequency-independent (SFI) design as proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.
It uses adaptive short-time Fourier transform (STFT) window/hop sizes to handle different SFs without the need for resampling.
During training, we augment each training sample by randomly downsampling it to a new SF <math id="S2.SS1.p6.3.m3.1" class="ltx_Math" alttext="f_{\text{org}}^{\text{aug}}" display="inline"><semantics id="S2.SS1.p6.3.m3.1a"><msubsup id="S2.SS1.p6.3.m3.1.1" xref="S2.SS1.p6.3.m3.1.1.cmml"><mi id="S2.SS1.p6.3.m3.1.1.2.2" xref="S2.SS1.p6.3.m3.1.1.2.2.cmml">f</mi><mtext id="S2.SS1.p6.3.m3.1.1.2.3" xref="S2.SS1.p6.3.m3.1.1.2.3a.cmml">org</mtext><mtext id="S2.SS1.p6.3.m3.1.1.3" xref="S2.SS1.p6.3.m3.1.1.3a.cmml">aug</mtext></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.3.m3.1b"><apply id="S2.SS1.p6.3.m3.1.1.cmml" xref="S2.SS1.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.3.m3.1.1.1.cmml" xref="S2.SS1.p6.3.m3.1.1">superscript</csymbol><apply id="S2.SS1.p6.3.m3.1.1.2.cmml" xref="S2.SS1.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p6.3.m3.1.1.2.1.cmml" xref="S2.SS1.p6.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.p6.3.m3.1.1.2.2.cmml" xref="S2.SS1.p6.3.m3.1.1.2.2">𝑓</ci><ci id="S2.SS1.p6.3.m3.1.1.2.3a.cmml" xref="S2.SS1.p6.3.m3.1.1.2.3"><mtext mathsize="70%" id="S2.SS1.p6.3.m3.1.1.2.3.cmml" xref="S2.SS1.p6.3.m3.1.1.2.3">org</mtext></ci></apply><ci id="S2.SS1.p6.3.m3.1.1.3a.cmml" xref="S2.SS1.p6.3.m3.1.1.3"><mtext mathsize="70%" id="S2.SS1.p6.3.m3.1.1.3.cmml" xref="S2.SS1.p6.3.m3.1.1.3">aug</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.3.m3.1c">f_{\text{org}}^{\text{aug}}</annotation></semantics></math> in <math id="S2.SS1.p6.4.m4.6" class="ltx_Math" alttext="\{8,16,24,32,44.1,48\}" display="inline"><semantics id="S2.SS1.p6.4.m4.6a"><mrow id="S2.SS1.p6.4.m4.6.7.2" xref="S2.SS1.p6.4.m4.6.7.1.cmml"><mo stretchy="false" id="S2.SS1.p6.4.m4.6.7.2.1" xref="S2.SS1.p6.4.m4.6.7.1.cmml">{</mo><mn id="S2.SS1.p6.4.m4.1.1" xref="S2.SS1.p6.4.m4.1.1.cmml">8</mn><mo id="S2.SS1.p6.4.m4.6.7.2.2" xref="S2.SS1.p6.4.m4.6.7.1.cmml">,</mo><mn id="S2.SS1.p6.4.m4.2.2" xref="S2.SS1.p6.4.m4.2.2.cmml">16</mn><mo id="S2.SS1.p6.4.m4.6.7.2.3" xref="S2.SS1.p6.4.m4.6.7.1.cmml">,</mo><mn id="S2.SS1.p6.4.m4.3.3" xref="S2.SS1.p6.4.m4.3.3.cmml">24</mn><mo id="S2.SS1.p6.4.m4.6.7.2.4" xref="S2.SS1.p6.4.m4.6.7.1.cmml">,</mo><mn id="S2.SS1.p6.4.m4.4.4" xref="S2.SS1.p6.4.m4.4.4.cmml">32</mn><mo id="S2.SS1.p6.4.m4.6.7.2.5" xref="S2.SS1.p6.4.m4.6.7.1.cmml">,</mo><mn id="S2.SS1.p6.4.m4.5.5" xref="S2.SS1.p6.4.m4.5.5.cmml">44.1</mn><mo id="S2.SS1.p6.4.m4.6.7.2.6" xref="S2.SS1.p6.4.m4.6.7.1.cmml">,</mo><mn id="S2.SS1.p6.4.m4.6.6" xref="S2.SS1.p6.4.m4.6.6.cmml">48</mn><mo stretchy="false" id="S2.SS1.p6.4.m4.6.7.2.7" xref="S2.SS1.p6.4.m4.6.7.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p6.4.m4.6b"><set id="S2.SS1.p6.4.m4.6.7.1.cmml" xref="S2.SS1.p6.4.m4.6.7.2"><cn type="integer" id="S2.SS1.p6.4.m4.1.1.cmml" xref="S2.SS1.p6.4.m4.1.1">8</cn><cn type="integer" id="S2.SS1.p6.4.m4.2.2.cmml" xref="S2.SS1.p6.4.m4.2.2">16</cn><cn type="integer" id="S2.SS1.p6.4.m4.3.3.cmml" xref="S2.SS1.p6.4.m4.3.3">24</cn><cn type="integer" id="S2.SS1.p6.4.m4.4.4.cmml" xref="S2.SS1.p6.4.m4.4.4">32</cn><cn type="float" id="S2.SS1.p6.4.m4.5.5.cmml" xref="S2.SS1.p6.4.m4.5.5">44.1</cn><cn type="integer" id="S2.SS1.p6.4.m4.6.6.cmml" xref="S2.SS1.p6.4.m4.6.6">48</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p6.4.m4.6c">\{8,16,24,32,44.1,48\}</annotation></semantics></math> kHz.
It assists the trained SE models in generalizing to different SFs.
The SF flows of both non-SFI and SFI models are summarized below:</p>
<table id="S4.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="0" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="0" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS1.p7" class="ltx_para">
<p id="S2.SS1.p7.1" class="ltx_p">The L1-based time-domain plus frequency-domain multi-resolution loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> is adopted to train all SE models.
We use four STFT window sizes <math id="S2.SS1.p7.1.m1.4" class="ltx_Math" alttext="\{256,512,768,1024\}" display="inline"><semantics id="S2.SS1.p7.1.m1.4a"><mrow id="S2.SS1.p7.1.m1.4.5.2" xref="S2.SS1.p7.1.m1.4.5.1.cmml"><mo stretchy="false" id="S2.SS1.p7.1.m1.4.5.2.1" xref="S2.SS1.p7.1.m1.4.5.1.cmml">{</mo><mn id="S2.SS1.p7.1.m1.1.1" xref="S2.SS1.p7.1.m1.1.1.cmml">256</mn><mo id="S2.SS1.p7.1.m1.4.5.2.2" xref="S2.SS1.p7.1.m1.4.5.1.cmml">,</mo><mn id="S2.SS1.p7.1.m1.2.2" xref="S2.SS1.p7.1.m1.2.2.cmml">512</mn><mo id="S2.SS1.p7.1.m1.4.5.2.3" xref="S2.SS1.p7.1.m1.4.5.1.cmml">,</mo><mn id="S2.SS1.p7.1.m1.3.3" xref="S2.SS1.p7.1.m1.3.3.cmml">768</mn><mo id="S2.SS1.p7.1.m1.4.5.2.4" xref="S2.SS1.p7.1.m1.4.5.1.cmml">,</mo><mn id="S2.SS1.p7.1.m1.4.4" xref="S2.SS1.p7.1.m1.4.4.cmml">1024</mn><mo stretchy="false" id="S2.SS1.p7.1.m1.4.5.2.5" xref="S2.SS1.p7.1.m1.4.5.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p7.1.m1.4b"><set id="S2.SS1.p7.1.m1.4.5.1.cmml" xref="S2.SS1.p7.1.m1.4.5.2"><cn type="integer" id="S2.SS1.p7.1.m1.1.1.cmml" xref="S2.SS1.p7.1.m1.1.1">256</cn><cn type="integer" id="S2.SS1.p7.1.m1.2.2.cmml" xref="S2.SS1.p7.1.m1.2.2">512</cn><cn type="integer" id="S2.SS1.p7.1.m1.3.3.cmml" xref="S2.SS1.p7.1.m1.3.3">768</cn><cn type="integer" id="S2.SS1.p7.1.m1.4.4.cmml" xref="S2.SS1.p7.1.m1.4.4">1024</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p7.1.m1.4c">\{256,512,768,1024\}</annotation></semantics></math> when calculating the multi-resolution loss.
We use the ESPnet-SE toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> for all experiments.
All models are trained until convergence (up to 3M iterations)<a href="#footnote2" title="footnote 2 ‣ Table 1 ‣ 2.1 Models ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2406.04269/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="204" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Scaling effect of BSRNN with respect to model complexity (#MACs at 48 kHz). Each data point corresponds to an independent model. Causal model setups: (a)–(e). Non-causal model setups: (f)–(j).</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data details</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">As shown in Table <a href="#S2.T3" title="Table 3 ‣ 2.3 Evaluation metrics ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we combine three datasets from different domains to train all aforementioned SE models, i.e., VoiceBank+DEMAND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, DNS-2020 challenge data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, and WHAMR! <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
They cover various conditions including noise, reverberation, and different SFs.
We investigate three different data scales {8.8, 98.8, 156.8} h by combining the listed datasets.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">For evaluation, we combine five different test sets as listed in Table <a href="#S2.T4" title="Table 4 ‣ 2.3 Evaluation metrics ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
The first three correspond to the test sets of the corpora mentioned above, representing the matched conditions.
The rest two are challenge datasets from CHiME-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and REVERB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, which provide both simulation and real-recorded data for evaluation.
They represent mismatched conditions as they contain unseen speech/noise/reverberation during training.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Evaluation metrics</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">We adopt the following evaluation metrics to analyze the scaling effect from multiple views: PESQ-WB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, short-time objective intelligibility (STOI) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, signal-to-distortion ratio (SDR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, DNSMOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and word accuracy (WAcc)<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>WAcc is equal to <math id="footnote3.m1.1" class="ltx_Math" alttext="1-" display="inline"><semantics id="footnote3.m1.1b"><mrow id="footnote3.m1.1.1" xref="footnote3.m1.1.1.cmml"><mn id="footnote3.m1.1.1.2" xref="footnote3.m1.1.1.2.cmml">1</mn><mo id="footnote3.m1.1.1.3" xref="footnote3.m1.1.1.3.cmml">−</mo></mrow><annotation-xml encoding="MathML-Content" id="footnote3.m1.1c"><apply id="footnote3.m1.1.1.cmml" xref="footnote3.m1.1.1"><csymbol cd="latexml" id="footnote3.m1.1.1.1.cmml" xref="footnote3.m1.1.1">limit-from</csymbol><cn type="integer" id="footnote3.m1.1.1.2.cmml" xref="footnote3.m1.1.1.2">1</cn><minus id="footnote3.m1.1.1.3.cmml" xref="footnote3.m1.1.1.3"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m1.1d">1-</annotation></semantics></math> word error rate (WER).</span></span></span>.
The first three metrics are intrusive objective measures that require well-aligned reference signals.
Therefore, they are only calculated on simulated test samples.
The DNSMOS OVRL score is a non-intrusive metric predicted by a pre-trained neural network that does not require the reference signal as input.
The WAcc is evaluated using the open-source large-scale pre-trained ASR model — OWSM v3.1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
For all metrics, a higher value indicates better performance.</p>
</div>
<figure id="S2.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Details of training data.</figcaption>
<div id="S2.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:113.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-24.7pt,6.4pt) scale(0.897871753152787,0.897871753152787) ;">
<table id="S2.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T3.1.1.1.1" class="ltx_tr">
<th id="S2.T3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S2.T3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">No.</span></th>
<th id="S2.T3.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S2.T3.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<td id="S2.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S2.T3.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Condition</span></td>
<td id="S2.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="2"><span id="S2.T3.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Duration</span></td>
</tr>
<tr id="S2.T3.1.1.2.2" class="ltx_tr">
<td id="S2.T3.1.1.2.2.1" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T3.1.1.2.2.1.1" class="ltx_text ltx_font_bold">training</span></td>
<td id="S2.T3.1.1.2.2.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T3.1.1.2.2.2.1" class="ltx_text ltx_font_bold">validation</span></td>
</tr>
<tr id="S2.T3.1.1.3.3" class="ltx_tr">
<th id="S2.T3.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">1</th>
<th id="S2.T3.1.1.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">VCTK+DEMAND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</th>
<td id="S2.T3.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Noisy, 48 kHz</td>
<td id="S2.T3.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">8.8 h</td>
<td id="S2.T3.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.6 h</td>
</tr>
<tr id="S2.T3.1.1.4.4" class="ltx_tr">
<th id="S2.T3.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">2</th>
<th id="S2.T3.1.1.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">DNS-2020 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</th>
<td id="S2.T3.1.1.4.4.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">Noisy, 16 kHz</td>
<td id="S2.T3.1.1.4.4.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">90.0 h</td>
<td id="S2.T3.1.1.4.4.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">10.0 h</td>
</tr>
<tr id="S2.T3.1.1.5.5" class="ltx_tr">
<th id="S2.T3.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">3</th>
<th id="S2.T3.1.1.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">WHAMR! <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</th>
<td id="S2.T3.1.1.5.5.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">Noisy, reverberant, 16 kHz</td>
<td id="S2.T3.1.1.5.5.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">58.0 h</td>
<td id="S2.T3.1.1.5.5.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">14.7 h</td>
</tr>
<tr id="S2.T3.1.1.6.6" class="ltx_tr">
<th id="S2.T3.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="2">
<span id="S2.T3.1.1.6.6.1.1" class="ltx_ERROR undefined">\cellcolor</span>[HTML]EEEEEE<span id="S2.T3.1.1.6.6.1.2" class="ltx_text ltx_font_bold">Scale 1</span>: No. 1 (~9 h)</th>
<td id="S2.T3.1.1.6.6.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="3">
<span id="S2.T3.1.1.6.6.2.1" class="ltx_ERROR undefined">\cellcolor</span>[HTML]EEEEEE<span id="S2.T3.1.1.6.6.2.2" class="ltx_text ltx_font_bold">Scale 2</span>: Nos. 1 + 2 (~99 h)</td>
</tr>
</tbody>
<tfoot class="ltx_tfoot">
<tr id="S2.T3.1.1.7.1" class="ltx_tr">
<th id="S2.T3.1.1.7.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="5">
<span id="S2.T3.1.1.7.1.1.1" class="ltx_ERROR undefined">\cellcolor</span>[HTML]EEEEEE<span id="S2.T3.1.1.7.1.1.2" class="ltx_text ltx_font_bold">Scale 3</span>: Nos. 1 + 2 + 3 (~157 h)</th>
</tr>
</tfoot>
</table>
</span></div>
</figure>
<figure id="S2.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Details of combined evaluation data.</figcaption>
<div id="S2.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:89.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-43.7pt,9.1pt) scale(0.832376697018927,0.832376697018927) ;">
<table id="S2.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T4.1.1.1.1" class="ltx_tr">
<th id="S2.T4.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">No.</span></th>
<th id="S2.T4.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T4.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S2.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T4.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Condition</span></th>
<th id="S2.T4.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S2.T4.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Duration</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T4.1.1.2.1" class="ltx_tr">
<th id="S2.T4.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">1</th>
<th id="S2.T4.1.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">VCTK+DEMAND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</th>
<td id="S2.T4.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Noisy, 48 kHz</td>
<td id="S2.T4.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.6 h</td>
</tr>
<tr id="S2.T4.1.1.3.2" class="ltx_tr">
<th id="S2.T4.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">2</th>
<th id="S2.T4.1.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">DNS-2020 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</th>
<td id="S2.T4.1.1.3.2.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">Noisy, 16 kHz</td>
<td id="S2.T4.1.1.3.2.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.4 h (no reverb)</td>
</tr>
<tr id="S2.T4.1.1.4.3" class="ltx_tr">
<th id="S2.T4.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">3</th>
<th id="S2.T4.1.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">WHAMR! <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</th>
<td id="S2.T4.1.1.4.3.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">Noisy, reverberant, 16 kHz</td>
<td id="S2.T4.1.1.4.3.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">9.0 h</td>
</tr>
<tr id="S2.T4.1.1.5.4" class="ltx_tr">
<th id="S2.T4.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">4</th>
<th id="S2.T4.1.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">CHiME-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>
</th>
<td id="S2.T4.1.1.5.4.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">Noisy, 16 kHz</td>
<td id="S2.T4.1.1.5.4.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">2.3 h (Simu) + 2.2 h (Real)</td>
</tr>
<tr id="S2.T4.1.1.6.5" class="ltx_tr">
<th id="S2.T4.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">5</th>
<th id="S2.T4.1.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">REVERB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</th>
<td id="S2.T4.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">Reverberant, 16 kHz</td>
<td id="S2.T4.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">4.8 h (Simu) + 0.7 h (Real)</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2406.04269/assets/x2.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="392" height="110" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Scaling effect of non-causal SE models with different architectures on 157 h of training data.</figcaption>
</figure>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2406.04269/assets/x3.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="433" height="235" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Scaling effect of non-causal BSRNN with respect to dataset sizes (#Data). Main results are shown in white regions, while shaded regions illustrate the degradation caused by improper data scaling (detailed in § <a href="#S3.SS1" title="3.1 Causal vs. non-causal: a case study on BSRNN ‣ 3 Results ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>).</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Causal vs. non-causal: a case study on BSRNN</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We first study the scaling effect of non-causal and causal models based on BSRNN, as it has shown superior performance in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
We train BSRNN models with different configurations as listed in Tables <a href="#footnote1" title="footnote 1 ‣ Table 1 ‣ 2.1 Models ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#S2.T3" title="Table 3 ‣ 2.3 Evaluation metrics ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, and then evaluate them on all test data shown in Table <a href="#S2.T4" title="Table 4 ‣ 2.3 Evaluation metrics ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
All results presented below represent averages across all test corpora, unless specified otherwise.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Fig. <a href="#S2.F1" title="Figure 1 ‣ 2.1 Models ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the scaling effect of causal (above) and non-causal (below) models with respect to model complexity which is represented by #MACs at 48 kHz.
It is evident that both causal and non-causal models exhibit similar trends across different metrics. In particular, the common observations include:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><em id="S3.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">Increasing the model complexity generally improves all metrics when sufficient training data are available.</em>
This aligns with the observation in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
One major exception is the SDR, PESQ-WB, and STOI metrics on the smallest data scale (9 h), which degrade initially and then start to improve.
The initial degradation can be attributed to overfitting due to limited training data.
Combined with the latter, this phenomenon is known as the ``double descent'' <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, which shows that modern deep architectures can recover from overfitting by further increasing model parameters.
However, it has been rarely reported in the area of speech processing.
In our context, this observation implies that the BSRNN with model complexity higher than 10 G/s has more than enough capacities of modeling the 9 h training data.
This might also apply to other SE architectures.
These results may suggest that SE research should focus more on larger corpora to avoid overkill on small datasets such as VCTK+DEMAND <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> with high-complexity SE models.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><em id="S3.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">With more training data, the model performance tends to benefit more from increased model complexity.</em> The models trained on 157 h data gain more performance improvement than other models.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><em id="S3.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">When the model complexity reaches a certain threshold, the initially linear scaling relationship between performance and log-scale #MACs tend to converge.</em> The initially linear scaling part coincides with the observation in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, which only investigates the model complexity between 50 M/s and 15 G/s. Our study reveals that this scaling effect does not hold when we further increase the model complexity, implying that more training data is needed when further scaling up the model.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><em id="S3.I1.i4.p1.1.1" class="ltx_emph ltx_font_italic">More complex models tend to achieve better downstream ASR performance (WAcc), but they still underperform the baseline without SE (no processing).</em> Similar issues are also reported in prior works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, where the degradation is attributed to the artifacts introduced by SE models on mismatched data.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Comparing causal and non-causal models in Fig. <a href="#S2.F1" title="Figure 1 ‣ 2.1 Models ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we can also see that non-causal models consistently outperform causal models in all metrics.
The performance difference is especially enlarged when more training data (e.g., 157 h) are available, indicating a much higher model capacity in non-causal models.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Furthermore, we analyze the scaling effect of model performance with respect to dataset sizes.
In Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.3 Evaluation metrics ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we only present the results of non-causal models in terms of SDR and WAcc metrics, and omit the other results as they share a similar trend.
We can see that <em id="S3.SS1.p4.1.1" class="ltx_emph ltx_font_italic">all metrics keep improving monotonically with more training data</em>, which is in line with common observations.
Moreover, larger models tend to benefit more from increased data, indicating that a higher model capacity allows for better utilization of the data.
<em id="S3.SS1.p4.1.2" class="ltx_emph ltx_font_italic">Note that this does not imply that simply increasing data is always helpful.</em>
In fact, when we attempted to further increase the dataset size by simply simulating more DNS-2020 data on top of ``scale 3'' in Table <a href="#S2.T3" title="Table 3 ‣ 2.3 Evaluation metrics ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (from 157 h to ~900h), the overall performance was even degraded.
This is shown in the shaded regions in Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.3 Evaluation metrics ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
The major cause roots in the highly imbalanced data distribution, with 93% DNS-2020, 1% VCTK+DEMAND, and 6% WHAMR! data.
The trained SE model is thus biased towards DNS-2020 data and suffers from mismatch during evaluation.
A similar observation has also been reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, where increasing mismatched pre-training data does not always improve the downstream finetuning performance.
This implies the importance of well-balanced and multi-domain data when scaling up the training dataset, which has often been overlooked in prior SE studies.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Comparison of different SE architectures</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Next, we investigate the scaling effect of different SE architectures.
Given the findings from BSRNN in the previous section, we mainly focus on the 157 h data scale and non-causal model setup for the best performance.
In Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.3 Evaluation metrics ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we explore three different scaling factors, i.e., model complexity (#MACs at 48 kHz), model parameters (#Params), and compute budget (#GPU-hours).</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">In Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.3 Evaluation metrics ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a), we observe that BSRNN outperforms others in the low model complexity region (<math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mo id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><lt id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">&lt;</annotation></semantics></math>100 G/s), while TF-GridNet dominates the higher model complexity region.
They are both T-F domain dual-path models, the recently developed SOTA architecture.
The U-Net-based DEMUCS-v4 models performs better than TCN-based Conv-TasNet, but still lags behind BSRNN.
This implies that T-F domain dual-path models have great potential in both low and high-complexity configurations, where BSRNN and TF-GridNet can be chosen depending on the complexity budget.
However, <em id="S3.SS2.p2.1.1" class="ltx_emph ltx_font_italic">no single architecture in our investigation can dominate all complexity conditions</em>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">In Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.3 Evaluation metrics ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (b), we present the scaling effect of SDR performance with respect to the compute budget.
Here, we use RTX A5000 GPUs for training TF-GridNet and Tesla V100 GPUs for other models<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>The equivalent GPU hours using Tesla V100 for TF-GridNet should be even larger since RTX A5000 has faster computing speed.</span></span></span>.
It is shown that BSRNN initially has very high compute efficiency, corresponding to the low model complexity region in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.3 Evaluation metrics ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (a).
In contrast, TF-GridNet's high performance comes at a high cost, requiring more computing than other architectures even for its smallest configuration (with only 0.5 M parameters).
Although these RNN-based architectures show excellent performance, they are highly inefficient in scaling up due to the limited parallelization.
<em id="S3.SS2.p3.1.1" class="ltx_emph ltx_font_italic">This calls for more explorations on efficiently scalable architectures that can be an SE counterpart to transformers in ASR and LLMs.</em></p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Finally in Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.3 Evaluation metrics ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (c), we show that the usually reported model parameters do not have a clear correlation with the SE performance when comparing different model architectures.
This is attributed to different SE architectures having highly divergent parameter efficiency.
Therefore, we strongly recommend that <em id="S3.SS2.p4.1.1" class="ltx_emph ltx_font_italic">SE studies should provide model complexity as an informative indicator instead of solely model parameters</em>.
We also note that Fig. <a href="#S2.F3" title="Figure 3 ‣ 2.3 Evaluation metrics ‣ 2 Model and experimental design ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> lacks sufficient explorations in the large parameter region (<math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mo id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><geq id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\geq</annotation></semantics></math>100 M), which we leave for future work.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we have investigated the scalability of SE models in terms of model architectures, model setups (causal or non-causal), model complexity and sizes, compute budget, and dataset sizes, where we consumed <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S4.p1.1.m1.1a"><mo id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><gt id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">&gt;</annotation></semantics></math>25k GPU hours in total.
Our experiments on combined public corpora verified the great potential of two T-F domain dual-path models (BSRNN and TF-GridNet) respectively in both low and high model complexity conditions.
However, these RNN-based architectures are highly inefficient in scaling up due to the limited parallelization.
This suggests that more exploration in the SE architecture is needed to achieve better efficiency and scalability.
In addition, the ``double descent'' phenomenon on VCTK+DEMAND implies that a small corpus is not suitable to benchmark high-capacity SE models.
Our findings in § <a href="#S3.SS1" title="3.1 Causal vs. non-causal: a case study on BSRNN ‣ 3 Results ‣ Beyond Performance Plateaus: A Comprehensive Study on Scalability in Speech Enhancement" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> further show the necessity of gathering large-scale multi-domain SE corpora to facilitate the scaling of SE models while mitigating mismatches.
We expect the new insights gained from our investigation can inspire more research towards scalable SE models.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
P. C. Loizou, <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Speech enhancement: theory and practice</em>.   CRC press, 2013.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
D. Stoller, S. Ewert, and S. Dixon, ``Wave-U-Net: A multi-scale neural
network for end-to-end audio source separation,'' in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
19th International Society for Music Information Retrieval Conference
(ISMIR)</em>, 2018, pp. 334–340.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. Rouard, F. Massa, and A. Défossez, ``Hybrid transformers for music
source separation,'' in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE ICASSP</em>.   IEEE, 2023, pp. 1–5.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y. Luo and N. Mesgarani, ``Conv-TasNet: Surpassing ideal time–frequency
magnitude masking for speech separation,'' <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Trans. ASLP.</em>,
vol. 27, no. 8, pp. 1256–1266, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Y. Luo, Z. Chen, and T. Yoshioka, ``Dual-path RNN: efficient long sequence
modeling for time-domain single-channel speech separation,'' in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proc.
IEEE ICASSP</em>, 2020, pp. 46–50.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J. Chen, Q. Mao, and D. Liu, ``Dual-path transformer network: Direct
context-aware modeling for end-to-end monaural speech separation,'' in
<em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2020, pp. 2642–2646.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
C. Subakan, M. Ravanelli, S. Cornell, M. Bronzi, and J. Zhong, ``Attention is
all you need in speech separation,'' in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE ICASSP</em>, 2021, pp.
21–25.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Z.-Q. Wang, S. Cornell, S. Choi, Y. Lee, B.-Y. Kim <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
``TF-GridNet: Integrating full-and sub-band modeling for speech
separation,'' <em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic">IEEE/ACM Trans. ASLP.</em>, vol. 31, pp. 3221–3236, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
C. Valentini-Botinhao, X. Wang, S. Takaki, and J. Yamagishi, ``Speech
enhancement for a noise-robust text-to-speech synthesis system using deep
recurrent neural networks,'' in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2016, pp. 352–356.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``A survey of large
language models,'' <em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.18223</em>, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Robust
speech recognition via large-scale weak supervision,'' in <em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic">Proc. ICML</em>,
2023, pp. 28 492–28 518.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y. Peng, J. Tian, W. Chen, S. Arora, B. Yan <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``OWSM v3. 1:
Better and faster open Whisper-style speech models based on
E-Branchformer,'' <em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.16658</em>, 2024.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
H. Wu, X. Chen, Y.-C. Lin, K. wei Chang, H.-L. Chung <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Towards
audio language modeling – an overview,'' <em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2402.13236</em>, 2024.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J. Droppo and O. Elibol, ``Scaling laws for acoustic models,'' in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proc.
Interspeech</em>, 2021, pp. 2576–2580.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Y. Gu, P. Gurunath Shivakumar, J. Kolehmainen, A. Gandhe, A. Rastrow
<em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Scaling laws for discriminative speech recognition rescoring
models,'' in <em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2023, pp. 471–475.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
``Scaling laws for neural language models,'' <em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2001.08361</em>, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer, ``Scaling vision
transformers,'' in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2022, pp. 12 104–12 113.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
H. Chen, J. Yu, and C. Weng, ``Complexity scaling for speech denoising,'' in
<em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE ICASSP</em>, 2024, pp. 12 276–12 280.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J. Yu and Y. Luo, ``Efficient monaural speech enhancement with universal sample
rate band-split RNN,'' in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE ICASSP</em>, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
J. Yu, H. Chen, Y. Luo, R. Gu, and C. Weng, ``High fidelity speech enhancement
with band-split RNN,'' in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2023, pp. 2483–2487.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
W. Zhang, K. Saijo, Z.-Q. Wang, S. Watanabe, and Y. Qian, ``Toward universal
speech enhancement for diverse input conditions,'' in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE ASRU</em>,
2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Y.-J. Lu, S. Cornell, X. Chang, W. Zhang, C. Li <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Towards
low-distortion multi-channel speech enhancement: The ESPnet-SE submission
to the L3DAS22 challenge,'' in <em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic">Proc. IEEE ICASSP</em>, 2022, pp.
9201–9205.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
C. Li, J. Shi, W. Zhang, A. S. Subramanian, X. Chang <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
``ESPnet-SE: End-to-end speech enhancement and separation toolkit designed
for ASR integration,'' in <em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic">Proc. IEEE SLT</em>, 2021, pp. 785–792.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Y.-J. Lu, X. Chang, C. Li, W. Zhang, S. Cornell <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``ESPnet-SE++:
Speech enhancement for robust speech recognition, translation, and
understanding,'' in <em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2022, pp. 5458–5462.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``The
INTERSPEECH 2020 deep noise suppression challenge: Datasets, subjective
testing framework, and challenge results,'' in <em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>,
2020, pp. 2492–2496.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
M. Maciejewski, G. Wichern, E. McQuinn, and J. Le Roux, ``WHAMR!: Noisy and
reverberant single-channel speech separation,'' in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE ICASSP</em>,
2019, pp. 696–700.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
E. Vincent, S. Watanabe, A. A. Nugraha, J. Barker, and R. Marxer, ``An analysis
of environment, microphone and data simulation mismatches in robust speech
recognition,'' <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Computer Speech &amp; Language</em>, vol. 46, pp. 535–557,
2017.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
K. Kinoshita, M. Delcroix, T. Yoshioka, T. Nakatani, A. Sehr <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
``The REVERB challenge: A common evaluation framework for dereverberation
and recognition of reverberant speech,'' in <em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic">Proc. IEEE WASPAA</em>, 2013,
pp. 1–4.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra, ``Perceptual
evaluation of speech quality (PESQ)—a new method for speech quality
assessment of telephone networks and codecs,'' in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE ICASSP</em>,
vol. 2, 2001, pp. 749–752.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen, ``An algorithm for
intelligibility prediction of time–frequency weighted noisy speech,''
<em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. ASLP.</em>, vol. 19, no. 7, pp. 2125–2136, 2011.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
E. Vincent, R. Gribonval, and C. Févotte, ``Performance measurement in
blind audio source separation,'' <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. ASLP.</em>, vol. 14, no. 4,
pp. 1462–1469, 2006.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
C. K. Reddy, V. Gopal, and R. Cutler, ``DNSMOS P.835: A non-intrusive
perceptual objective speech quality metric to evaluate noise suppressors,''
in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE ICASSP</em>, 2022, pp. 886–890.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
M. Belkin, D. Hsu, S. Ma, and S. Mandal, ``Reconciling modern machine-learning
practice and the classical bias–variance trade-off,'' <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the National Academy of Sciences</em>, vol. 116, no. 32, pp. 15 849–15 854,
2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Deep
double descent: Where bigger models and more data hurt,'' <em id="bib.bib34.2.2" class="ltx_emph ltx_font_italic">Journal of
Statistical Mechanics: Theory and Experiment</em>, vol. 2021, no. 12, p. 124003,
2021.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
S. E. Eskimez, X. Wang, M. Tang, H. Yang, Z. Zhu <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``Human
listening and live captioning: Multi-task training for speech enhancement,''
in <em id="bib.bib35.2.2" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2021, pp. 2686–2690.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
K. Iwamoto, T. Ochiai, M. Delcroix, R. Ikeshita, H. Sato <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">et al.</em>, ``How
bad are artifacts?: Analyzing the impact of speech enhancement errors on
ASR,'' in <em id="bib.bib36.2.2" class="ltx_emph ltx_font_italic">Proc. Interspeech</em>, 2022, pp. 5418–5422.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
B. Isik, N. Ponomareva, H. Hazimeh, D. Paparas, S. Vassilvitskii <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
``Scaling laws for downstream task performance of large language models,''
<em id="bib.bib37.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2402.04177</em>, 2024.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.04268" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.04269" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.04269">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.04269" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.04270" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 23:36:14 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
