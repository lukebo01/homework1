<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.00022] Multilingual Prosody Transfer: Comparing Supervised &amp; Transfer Learning</title><meta property="og:description" content="The field of prosody transfer in speech synthesis systems is rapidly advancing. This research is focused on evaluating learning methods for adapting pre-trained monolingual text-to-speech (TTS) models to multilingual c…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multilingual Prosody Transfer: Comparing Supervised &amp; Transfer Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multilingual Prosody Transfer: Comparing Supervised &amp; Transfer Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.00022">

<!--Generated on Fri Jul  5 21:18:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Multilingual Prosody Transfer: Comparing Supervised &amp; Transfer Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Arnav Goel,  Medha Hira<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>   &amp; Anubha Gupta 
<br class="ltx_break">Indraprastha Institute of Information Technology
<br class="ltx_break">New Delhi - 110020, India 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">{arnav21519,medha21265,anubha}@iiitd.ac.in</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes">Equal Contribution</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">The field of prosody transfer in speech synthesis systems is rapidly advancing. This research is focused on evaluating learning methods for adapting pre-trained monolingual text-to-speech (TTS) models to multilingual conditions, i.e., Supervised Fine-Tuning (SFT) and Transfer Learning (TL). This comparison utilizes three distinct metrics: Mean Opinion Score (MOS), Recognition Accuracy (RA), and Mel Cepstral Distortion (MCD). Results demonstrate that, in comparison to SFT, TL leads to significantly enhanced performance, with an average MOS higher by 1.53 points, a 37.5% increase in RA, and approximately, a 7.8-point improvement in MCD. These findings are instrumental in helping build TTS models for low-resource languages.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Recent advancements in deep learning, as seen in systems such as fairseq-S2T <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>, SpeechT5 <cite class="ltx_cite ltx_citemacro_citep">(Ao et al., <a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite>, and VITS <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite> have significantly enhanced speech synthesis, paving the way for our research on controllable Text-to-Speech (TTS) systems that transfer both text and prosody to target audio. Our study focuses on multilingual prosody transfer in TTS, particularly exploring models initially trained in English and then adapted to other languages. Adapting TTS for multilingual use involves various representation learning methods, including semi-supervised and self-supervised learning <cite class="ltx_cite ltx_citemacro_citep">(Saeki et al., <a href="#bib.bib13" title="" class="ltx_ref">2023a</a>)</cite>. We assess two key approaches—supervised fine-tuning (SFT) on text-audio pairs and transfer learning (TL)—to evaluate their effectiveness in generating high-quality audio and transferring prosody in multilingual contexts.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Prosody transfer and voice conversion in TTS systems have evolved from traditional HMMs, RNNs, and CNNs to Transformer-based architectures like VTN <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a href="#bib.bib18" title="" class="ltx_ref">2023</a>; Huang et al., <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>. Recent techniques include using 1) ASR for linguistic representation <cite class="ltx_cite ltx_citemacro_citep">(Tian et al., <a href="#bib.bib17" title="" class="ltx_ref">2019</a>; Popov et al., <a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite>, 2) speaker-dependent prosody capture <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite>, 3) global cues like pitch and loudness <cite class="ltx_cite ltx_citemacro_citep">(Gururani et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>, and 4) combining local and global prosodic features <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite>. While <cite class="ltx_cite ltx_citemacro_cite">Saeki et al. (<a href="#bib.bib14" title="" class="ltx_ref">2023b</a>)</cite> explore semi-supervised learning for TTS in multilingual settings and <cite class="ltx_cite ltx_citemacro_cite">Shah et al. (<a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite> introduce a pre-trained TTS model for low-resource languages, the use of speaker embeddings for prosody transfer and adapting pre-trained English TTS systems to multilingual contexts remains less explored. Our study aims to fill this gap by investigating these methods.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">To conduct experiments in German, French, Spanish, and Dutch, we utilized segments from the VoxPopuli dataset <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite>, comprising 282, 211, 166, and 53 hours of transcribed audio, respectively. For Hindi and Tamil, we selected subsets from the Common Voice corpus <cite class="ltx_cite ltx_citemacro_citep">(Ardila et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>, amounting to 20 and 200 hours of audio of each language. To address class imbalance, we uniformly sampled data from each language to equalize dataset duration to approximately <span id="S3.p1.1.1" class="ltx_text ltx_font_bold">20 hours</span>.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">We aim to adapt pre-trained models for multilingual prosody preservation using Supervised Fine-Tuning (SFT) and Transfer Learning (TL). Our experimental setups and methodologies are as below:</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">SFT:</span> We selected SpeechT5 for SFT due to its encoder-decoder structure that generates mel-spectrograms from text input. Its audio post-net can incorporate speaker embeddings for prosody transfer <cite class="ltx_cite ltx_citemacro_citep">(Ao et al., <a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite>. We utilized x-vector embedding <cite class="ltx_cite ltx_citemacro_citep">(Snyder et al., <a href="#bib.bib16" title="" class="ltx_ref">2018</a>)</cite>, known for capturing emotional and gender characteristics in speech embedding. The choice of x-vectors is based on experiments detailed in <a href="#A1.SS2" title="A.2 Speaker Embeddings ‣ Appendix A Appendix ‣ Multilingual Prosody Transfer: Comparing Supervised &amp; Transfer Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a>. These embedding are integrated with the output of SpeechT5’s decoder to preserve prosody. SpeechT5 is fine-tuned using supervised learning on (text, spectrogram) data pairs aiming to minimise cross-entropy loss. The implementation details and plots are shown in <a href="#A1.SS3" title="A.3 Fine-Tuning Implementation Details and Plots ‣ Appendix A Appendix ‣ Multilingual Prosody Transfer: Comparing Supervised &amp; Transfer Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.3</span></a>. This technique adapts a pre-trained monolingual model to multilingual settings.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">TL:</span> To evaluate TL, we implemented the voice cloning method from <cite class="ltx_cite ltx_citemacro_cite">Jia et al. (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>. This involved a pre-trained speaker encoder designed for speaker identification, merged with a voice conversion model. For speech synthesis from text, we used MMS TTS <cite class="ltx_cite ltx_citemacro_citep">(Pratap et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite> which is a pre-trained multilingual model which does not preserve prosody. Concurrently, x-vector embedding were derived using a pre-trained encoder <cite class="ltx_cite ltx_citemacro_citep">(Ravanelli et al., <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite> from input audio. The synthesized audio from MMS and the embedding from the encoder were fed into FreeVC’s pre-trained voice conversion module <cite class="ltx_cite ltx_citemacro_citep">(li et al., <a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite> to produce prosody-preserving audio.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p">Our experiment involved regenerating input audio clips using the two described models. We assessed the quality of these generated clips using MOS, RA, and MCD. MOS evaluates the naturalness, quality, and prosody transfer of the generated speech, while RA assesses the respondents’ ability to recognize the speaker in both the original and generated audio. MOS and RA are computed based on feedback from 35 respondents<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Detailed calculation methodology and protocol are provided in Appendix <a href="#A1" title="Appendix A Appendix ‣ Multilingual Prosody Transfer: Comparing Supervised &amp; Transfer Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a></span></span></span>. Although MCD is not an ideal metric for speech quality, it is useful in this context for measuring the distortion between the generated audio clip and the original, thereby aiding in evaluating our experiment’s outcomes.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparative performance of SFT and TL on the synthesised speech quality</figcaption>
<table id="S4.T1.20" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.20.21.1" class="ltx_tr">
<th id="S4.T1.20.21.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" rowspan="2">
<span id="S4.T1.20.21.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.20.21.1.1.1.1" class="ltx_p"><span id="S4.T1.20.21.1.1.1.1.1" class="ltx_text ltx_font_bold">Language</span></span>
</span>
</th>
<th id="S4.T1.20.21.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3"><span id="S4.T1.20.21.1.2.1" class="ltx_text ltx_font_bold">Supervised Fine-Tuning (SpeechT5)</span></th>
<th id="S4.T1.20.21.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3"><span id="S4.T1.20.21.1.3.1" class="ltx_text ltx_font_bold">Transfer Learning (FreeVC)</span></th>
</tr>
<tr id="S4.T1.8.8" class="ltx_tr">
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.2.2.2.2" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.2.2.2.2.2" class="ltx_p">MOS (<math id="S4.T1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T1.1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>)<sup id="S4.T1.2.2.2.2.2.1" class="ltx_sup">⋆</sup></span>
</span>
</td>
<td id="S4.T1.3.3.3" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.3.3.1.1" class="ltx_p">Recognition Accuracy(<math id="S4.T1.3.3.3.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.3.3.3.1.1.m1.1a"><mo stretchy="false" id="S4.T1.3.3.3.1.1.m1.1.1" xref="S4.T1.3.3.3.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.1.1.m1.1b"><ci id="S4.T1.3.3.3.1.1.m1.1.1.cmml" xref="S4.T1.3.3.3.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.1.1.m1.1c">\uparrow</annotation></semantics></math>)</span>
</span>
</td>
<td id="S4.T1.4.4.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.4.4.4.1.1" class="ltx_p">MCD (<math id="S4.T1.4.4.4.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.4.4.4.1.1.m1.1a"><mo stretchy="false" id="S4.T1.4.4.4.1.1.m1.1.1" xref="S4.T1.4.4.4.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.1.1.m1.1b"><ci id="S4.T1.4.4.4.1.1.m1.1.1.cmml" xref="S4.T1.4.4.4.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.1.1.m1.1c">\downarrow</annotation></semantics></math>)</span>
</span>
</td>
<td id="S4.T1.6.6.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.6.6.6.2" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.6.6.6.2.2" class="ltx_p">MOS (<math id="S4.T1.5.5.5.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.5.5.5.1.1.m1.1a"><mo stretchy="false" id="S4.T1.5.5.5.1.1.m1.1.1" xref="S4.T1.5.5.5.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.1.1.m1.1b"><ci id="S4.T1.5.5.5.1.1.m1.1.1.cmml" xref="S4.T1.5.5.5.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.1.1.m1.1c">\uparrow</annotation></semantics></math>)<sup id="S4.T1.6.6.6.2.2.1" class="ltx_sup">⋆</sup></span>
</span>
</td>
<td id="S4.T1.7.7.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.7.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.7.7.7.1.1" class="ltx_p">Recognition Accuracy(<math id="S4.T1.7.7.7.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.7.7.7.1.1.m1.1a"><mo stretchy="false" id="S4.T1.7.7.7.1.1.m1.1.1" xref="S4.T1.7.7.7.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.7.1.1.m1.1b"><ci id="S4.T1.7.7.7.1.1.m1.1.1.cmml" xref="S4.T1.7.7.7.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.7.1.1.m1.1c">\uparrow</annotation></semantics></math>)</span>
</span>
</td>
<td id="S4.T1.8.8.8" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.8.8.8.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.8.8.8.1.1" class="ltx_p">MCD (<math id="S4.T1.8.8.8.1.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.8.8.8.1.1.m1.1a"><mo stretchy="false" id="S4.T1.8.8.8.1.1.m1.1.1" xref="S4.T1.8.8.8.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.8.1.1.m1.1b"><ci id="S4.T1.8.8.8.1.1.m1.1.1.cmml" xref="S4.T1.8.8.8.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.8.1.1.m1.1c">\downarrow</annotation></semantics></math>)</span>
</span>
</td>
</tr>
<tr id="S4.T1.10.10" class="ltx_tr">
<td id="S4.T1.10.10.3" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.10.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.10.10.3.1.1" class="ltx_p">Spanish</span>
</span>
</td>
<td id="S4.T1.9.9.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.9.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.9.9.1.1.1" class="ltx_p">2.73 <math id="S4.T1.9.9.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.9.9.1.1.1.m1.1a"><mo id="S4.T1.9.9.1.1.1.m1.1.1" xref="S4.T1.9.9.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T1.9.9.1.1.1.m1.1.1.cmml" xref="S4.T1.9.9.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.1.1.1.m1.1c">\pm</annotation></semantics></math> 0.03</span>
</span>
</td>
<td id="S4.T1.10.10.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.10.10.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.10.10.4.1.1" class="ltx_p">0.43</span>
</span>
</td>
<td id="S4.T1.10.10.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.10.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.10.10.5.1.1" class="ltx_p">23.23</span>
</span>
</td>
<td id="S4.T1.10.10.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.10.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.10.10.2.1.1" class="ltx_p">4.11 <math id="S4.T1.10.10.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.10.10.2.1.1.m1.1a"><mo id="S4.T1.10.10.2.1.1.m1.1.1" xref="S4.T1.10.10.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.2.1.1.m1.1b"><csymbol cd="latexml" id="S4.T1.10.10.2.1.1.m1.1.1.cmml" xref="S4.T1.10.10.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.2.1.1.m1.1c">\pm</annotation></semantics></math> 0.12</span>
</span>
</td>
<td id="S4.T1.10.10.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.10.10.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.10.10.6.1.1" class="ltx_p">0.81</span>
</span>
</td>
<td id="S4.T1.10.10.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.10.10.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.10.10.7.1.1" class="ltx_p">15.83</span>
</span>
</td>
</tr>
<tr id="S4.T1.12.12" class="ltx_tr">
<td id="S4.T1.12.12.3" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.12.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.12.12.3.1.1" class="ltx_p">French</span>
</span>
</td>
<td id="S4.T1.11.11.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.11.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.11.11.1.1.1" class="ltx_p">2.85 <math id="S4.T1.11.11.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.11.11.1.1.1.m1.1a"><mo id="S4.T1.11.11.1.1.1.m1.1.1" xref="S4.T1.11.11.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T1.11.11.1.1.1.m1.1.1.cmml" xref="S4.T1.11.11.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.1.1.1.m1.1c">\pm</annotation></semantics></math> 0.09</span>
</span>
</td>
<td id="S4.T1.12.12.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.12.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.12.12.4.1.1" class="ltx_p">0.48</span>
</span>
</td>
<td id="S4.T1.12.12.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.12.12.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.12.12.5.1.1" class="ltx_p">21.36</span>
</span>
</td>
<td id="S4.T1.12.12.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.12.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.12.12.2.1.1" class="ltx_p">4.26 <math id="S4.T1.12.12.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.12.12.2.1.1.m1.1a"><mo id="S4.T1.12.12.2.1.1.m1.1.1" xref="S4.T1.12.12.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.12.12.2.1.1.m1.1b"><csymbol cd="latexml" id="S4.T1.12.12.2.1.1.m1.1.1.cmml" xref="S4.T1.12.12.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.12.2.1.1.m1.1c">\pm</annotation></semantics></math> 0.09</span>
</span>
</td>
<td id="S4.T1.12.12.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.12.12.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.12.12.6.1.1" class="ltx_p">0.83</span>
</span>
</td>
<td id="S4.T1.12.12.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.12.12.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.12.12.7.1.1" class="ltx_p"><span id="S4.T1.12.12.7.1.1.1" class="ltx_text ltx_font_bold">12.54</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.14.14" class="ltx_tr">
<td id="S4.T1.14.14.3" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.14.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.14.14.3.1.1" class="ltx_p">German</span>
</span>
</td>
<td id="S4.T1.13.13.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.13.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.13.13.1.1.1" class="ltx_p">3.01 <math id="S4.T1.13.13.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.13.13.1.1.1.m1.1a"><mo id="S4.T1.13.13.1.1.1.m1.1.1" xref="S4.T1.13.13.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.13.13.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T1.13.13.1.1.1.m1.1.1.cmml" xref="S4.T1.13.13.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.13.1.1.1.m1.1c">\pm</annotation></semantics></math> 0.13</span>
</span>
</td>
<td id="S4.T1.14.14.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.14.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.14.14.4.1.1" class="ltx_p">0.52</span>
</span>
</td>
<td id="S4.T1.14.14.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.14.14.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.14.14.5.1.1" class="ltx_p">20.08</span>
</span>
</td>
<td id="S4.T1.14.14.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.14.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.14.14.2.1.1" class="ltx_p"><span id="S4.T1.14.14.2.1.1.1" class="ltx_text ltx_font_bold">4.35 <math id="S4.T1.14.14.2.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.14.14.2.1.1.1.m1.1a"><mo id="S4.T1.14.14.2.1.1.1.m1.1.1" xref="S4.T1.14.14.2.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.14.14.2.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T1.14.14.2.1.1.1.m1.1.1.cmml" xref="S4.T1.14.14.2.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.14.2.1.1.1.m1.1c">\pm</annotation></semantics></math> 0.01</span></span>
</span>
</td>
<td id="S4.T1.14.14.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.14.14.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.14.14.6.1.1" class="ltx_p"><span id="S4.T1.14.14.6.1.1.1" class="ltx_text ltx_font_bold">0.88</span></span>
</span>
</td>
<td id="S4.T1.14.14.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.14.14.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.14.14.7.1.1" class="ltx_p">13.41</span>
</span>
</td>
</tr>
<tr id="S4.T1.16.16" class="ltx_tr">
<td id="S4.T1.16.16.3" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.16.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.16.16.3.1.1" class="ltx_p">Dutch</span>
</span>
</td>
<td id="S4.T1.15.15.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.15.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.15.15.1.1.1" class="ltx_p">2.44 <math id="S4.T1.15.15.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.15.15.1.1.1.m1.1a"><mo id="S4.T1.15.15.1.1.1.m1.1.1" xref="S4.T1.15.15.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.15.15.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T1.15.15.1.1.1.m1.1.1.cmml" xref="S4.T1.15.15.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.15.1.1.1.m1.1c">\pm</annotation></semantics></math> 0.05</span>
</span>
</td>
<td id="S4.T1.16.16.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.16.16.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.16.16.4.1.1" class="ltx_p">0.45</span>
</span>
</td>
<td id="S4.T1.16.16.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.16.16.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.16.16.5.1.1" class="ltx_p">24.74</span>
</span>
</td>
<td id="S4.T1.16.16.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.16.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.16.16.2.1.1" class="ltx_p">4.15 <math id="S4.T1.16.16.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.16.16.2.1.1.m1.1a"><mo id="S4.T1.16.16.2.1.1.m1.1.1" xref="S4.T1.16.16.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.16.16.2.1.1.m1.1b"><csymbol cd="latexml" id="S4.T1.16.16.2.1.1.m1.1.1.cmml" xref="S4.T1.16.16.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.16.16.2.1.1.m1.1c">\pm</annotation></semantics></math> 0.04</span>
</span>
</td>
<td id="S4.T1.16.16.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.16.16.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.16.16.6.1.1" class="ltx_p">0.79</span>
</span>
</td>
<td id="S4.T1.16.16.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.16.16.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.16.16.7.1.1" class="ltx_p">17.21</span>
</span>
</td>
</tr>
<tr id="S4.T1.18.18" class="ltx_tr">
<td id="S4.T1.18.18.3" class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.18.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.18.18.3.1.1" class="ltx_p">Hindi</span>
</span>
</td>
<td id="S4.T1.17.17.1" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.17.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.17.17.1.1.1" class="ltx_p">2.32 <math id="S4.T1.17.17.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.17.17.1.1.1.m1.1a"><mo id="S4.T1.17.17.1.1.1.m1.1.1" xref="S4.T1.17.17.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.17.17.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T1.17.17.1.1.1.m1.1.1.cmml" xref="S4.T1.17.17.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.17.17.1.1.1.m1.1c">\pm</annotation></semantics></math> 0.06</span>
</span>
</td>
<td id="S4.T1.18.18.4" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.18.18.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.18.18.4.1.1" class="ltx_p">0.40</span>
</span>
</td>
<td id="S4.T1.18.18.5" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.18.18.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.18.18.5.1.1" class="ltx_p">25.23</span>
</span>
</td>
<td id="S4.T1.18.18.2" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.18.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.18.18.2.1.1" class="ltx_p">4.01 <math id="S4.T1.18.18.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.18.18.2.1.1.m1.1a"><mo id="S4.T1.18.18.2.1.1.m1.1.1" xref="S4.T1.18.18.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.18.18.2.1.1.m1.1b"><csymbol cd="latexml" id="S4.T1.18.18.2.1.1.m1.1.1.cmml" xref="S4.T1.18.18.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.18.18.2.1.1.m1.1c">\pm</annotation></semantics></math> 0.17</span>
</span>
</td>
<td id="S4.T1.18.18.6" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.18.18.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.18.18.6.1.1" class="ltx_p">0.82</span>
</span>
</td>
<td id="S4.T1.18.18.7" class="ltx_td ltx_align_justify ltx_border_r ltx_border_t">
<span id="S4.T1.18.18.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.18.18.7.1.1" class="ltx_p">16.28</span>
</span>
</td>
</tr>
<tr id="S4.T1.20.20" class="ltx_tr">
<td id="S4.T1.20.20.3" class="ltx_td ltx_align_justify ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S4.T1.20.20.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.20.20.3.1.1" class="ltx_p">Tamil</span>
</span>
</td>
<td id="S4.T1.19.19.1" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T1.19.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.19.19.1.1.1" class="ltx_p">2.12 <math id="S4.T1.19.19.1.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.19.19.1.1.1.m1.1a"><mo id="S4.T1.19.19.1.1.1.m1.1.1" xref="S4.T1.19.19.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.19.19.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T1.19.19.1.1.1.m1.1.1.cmml" xref="S4.T1.19.19.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.19.19.1.1.1.m1.1c">\pm</annotation></semantics></math> 0.24</span>
</span>
</td>
<td id="S4.T1.20.20.4" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T1.20.20.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.20.20.4.1.1" class="ltx_p">0.37</span>
</span>
</td>
<td id="S4.T1.20.20.5" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T1.20.20.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.20.20.5.1.1" class="ltx_p">26.21</span>
</span>
</td>
<td id="S4.T1.20.20.2" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T1.20.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.20.20.2.1.1" class="ltx_p">3.85 <math id="S4.T1.20.20.2.1.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T1.20.20.2.1.1.m1.1a"><mo id="S4.T1.20.20.2.1.1.m1.1.1" xref="S4.T1.20.20.2.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T1.20.20.2.1.1.m1.1b"><csymbol cd="latexml" id="S4.T1.20.20.2.1.1.m1.1.1.cmml" xref="S4.T1.20.20.2.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.20.20.2.1.1.m1.1c">\pm</annotation></semantics></math> 0.13</span>
</span>
</td>
<td id="S4.T1.20.20.6" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T1.20.20.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.20.20.6.1.1" class="ltx_p">0.77</span>
</span>
</td>
<td id="S4.T1.20.20.7" class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t">
<span id="S4.T1.20.20.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.20.20.7.1.1" class="ltx_p">18.54</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">Table 1 shows the results of our experiments. Audio clips generated by SpeechT5-finetuned using SFT are generally found to be noisy and have poor audio quality. This is further validated by the MOS scores which are reported at a <em id="S5.p1.1.1" class="ltx_emph ltx_font_italic">95% confidence level</em>. TL surpasses SFT by an average of 1.53 points over the six languages. Additionally, the recognition accuracy of the TL generated audio exceeds that of SFT by more than 35% on average. These scores substantiate that Transfer Learning is superior in retaining the unique characteristics of a voice. While adapting the model to another language, SFT reduces the model’s ability to generate good quality speech, let alone preserve prosody. MCD compares the mel-frequency cepstral coefficients (MFCC) of ground truth and generated speech. We used dynamic time-warping to calculate MCD in order to account for clips with varying lengths. TL yields lower MCD compared to SFT (indicating closer resemblance). The distortion is lesser by an average of 35% on all the six languages.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Work</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">Our findings highlight the superiority of transfer learning over supervised fine-tuning in adapting pre-trained models for TTS applications. This insight is particularly crucial for developing TTS models in low-resource environments, where supervised fine-tuning’s data-intensive nature can be a significant challenge. Future research will aim to establish a framework for comparing different learning methods in adapting pre-trained models to low-resource and multilingual contexts.</p>
</div>
<section id="S6.SS0.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">URM Statement</h4>

<div id="S6.SS0.SSSx1.p1" class="ltx_para ltx_noindent">
<p id="S6.SS0.SSSx1.p1.1" class="ltx_p">The authors acknowledge that at all the authors of this work meet the URM criteria of ICLR 2024 Tiny Papers Track.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ao et al. (2022)</span>
<span class="ltx_bibblock">
Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, and Furu Wei.

</span>
<span class="ltx_bibblock">Speecht5: Unified-modal encoder-decoder pre-training for spoken language processing, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ardila et al. (2020)</span>
<span class="ltx_bibblock">
Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M. Tyers, and Gregor Weber.

</span>
<span class="ltx_bibblock">Common voice: A massively-multilingual speech corpus, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. (2014)</span>
<span class="ltx_bibblock">
Houwei Cao, David G Cooper, Michael K Keutmann, Ruben C Gur, Ani Nenkova, and Ragini Verma.

</span>
<span class="ltx_bibblock">CREMA-D: Crowd-sourced emotional multimodal actors dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on affective computing</em>, 5(4):377–390, 2014.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gururani et al. (2020)</span>
<span class="ltx_bibblock">
Siddharth Gururani, Kilol Gupta, Dhaval Shah, Zahra Shakeri, and Jervis Pinto.

</span>
<span class="ltx_bibblock">Prosody transfer in neural text to speech using global pitch and loudness features, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2019)</span>
<span class="ltx_bibblock">
Wen-Chin Huang, Tomoki Hayashi, Yi-Chiao Wu, Hirokazu Kameoka, and Tomoki Toda.

</span>
<span class="ltx_bibblock">Voice transformer network: Sequence-to-sequence voice conversion using transformer with text-to-speech pretraining, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2023)</span>
<span class="ltx_bibblock">
Wen-Chin Huang, Benjamin Peloquin, Justine Kao, Changhan Wang, Hongyu Gong, Elizabeth Salesky, Yossi Adi, Ann Lee, and Peng-Jen Chen.

</span>
<span class="ltx_bibblock">A holistic cascade system, benchmark, and human evaluation protocol for expressive speech-to-speech translation, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al. (2019)</span>
<span class="ltx_bibblock">
Ye Jia, Yu Zhang, Ron J. Weiss, Quan Wang, Jonathan Shen, Fei Ren, Zhifeng Chen, Patrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, and Yonghui Wu.

</span>
<span class="ltx_bibblock">Transfer learning from speaker verification to multispeaker text-to-speech synthesis, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2021)</span>
<span class="ltx_bibblock">
Jaehyeon Kim, Jungil Kong, and Juhee Son.

</span>
<span class="ltx_bibblock">Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">li et al. (2022)</span>
<span class="ltx_bibblock">
Jingyi li, Weiping tu, and Li xiao.

</span>
<span class="ltx_bibblock">Freevc: Towards high-quality text-free one-shot voice conversion, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popov et al. (2022)</span>
<span class="ltx_bibblock">
Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail Kudinov, and Jiansheng Wei.

</span>
<span class="ltx_bibblock">Diffusion-based voice conversion with fast maximum likelihood sampling scheme, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pratap et al. (2023)</span>
<span class="ltx_bibblock">
Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, and Michael Auli.

</span>
<span class="ltx_bibblock">Scaling speech technology to 1,000+ languages, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ravanelli et al. (2021)</span>
<span class="ltx_bibblock">
Mirco Ravanelli, Titouan Parcollet, Peter Plantinga, Aku Rouhe, Samuele Cornell, Loren Lugosch, Cem Subakan, Nauman Dawalatabad, Abdelwahab Heba, Jianyuan Zhong, Ju-Chieh Chou, Sung-Lin Yeh, Szu-Wei Fu, Chien-Feng Liao, Elena Rastorgueva, François Grondin, William Aris, Hwidong Na, Yan Gao, Renato De Mori, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Speechbrain: A general-purpose speech toolkit, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saeki et al. (2023a)</span>
<span class="ltx_bibblock">
Takaaki Saeki, Heiga Zen, Zhehuai Chen, Nobuyuki Morioka, Gary Wang, Yu Zhang, Ankur Bapna, Andrew Rosenberg, and Bhuvana Ramabhadran.

</span>
<span class="ltx_bibblock">Virtuoso: Massive multilingual speech-text joint semi-supervised learning for text-to-speech.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp.  1–5, 2023a.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICASSP49357.2023.10095702</span>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saeki et al. (2023b)</span>
<span class="ltx_bibblock">
Takaaki Saeki, Heiga Zen, Zhehuai Chen, Nobuyuki Morioka, Gary Wang, Yu Zhang, Ankur Bapna, Andrew Rosenberg, and Bhuvana Ramabhadran.

</span>
<span class="ltx_bibblock">Virtuoso: Massive multilingual speech-text joint semi-supervised learning for text-to-speech, 2023b.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah et al. (2023)</span>
<span class="ltx_bibblock">
Neil Shah, Vishal Tambrahalli, Saiteja Kosgi, Niranjan Pedanekar, and Vineet Gandhi.

</span>
<span class="ltx_bibblock">Mparrottts: Multilingual multi-speaker text to speech synthesis in low resource setting, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snyder et al. (2018)</span>
<span class="ltx_bibblock">
David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur.

</span>
<span class="ltx_bibblock">X-vectors: Robust dnn embeddings for speaker recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pp.  5329–5333, 2018.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ICASSP.2018.8461375</span>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al. (2019)</span>
<span class="ltx_bibblock">
Xiaohai Tian, Eng Siong Chng, and Haizhou Li.

</span>
<span class="ltx_bibblock">A vocoder-free wavenet voice conversion with non-parallel data, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2023)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux.

</span>
<span class="ltx_bibblock">Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, and Juan Pino.

</span>
<span class="ltx_bibblock">fairseq s2t: Fast speech-to-text modeling with fairseq, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Jing-Xuan Zhang, Li-Juan Liu, Yan-Nian Chen, Ya-Jun Hu, Yuan Jiang, Zhen-Hua Ling, and Li-Rong Dai.

</span>
<span class="ltx_bibblock">Voice conversion by cascading automatic speech recognition and text-to-speech synthesis with prosody transfer, 2020.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Acronyms Used</h3>

<div id="A1.SS1.p1" class="ltx_para ltx_noindent">
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p">TL : Transfer Learning</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p">SFT : Supervised-Fine Tuning</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p">MCD : Mel Cepstral Distortion</p>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.1" class="ltx_p">MFCC : Mel-Frequency Cepstral Coefficients</p>
</div>
</li>
<li id="A1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i5.p1" class="ltx_para">
<p id="A1.I1.i5.p1.1" class="ltx_p">MOS : Mean Opinion Score</p>
</div>
</li>
<li id="A1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i6.p1" class="ltx_para">
<p id="A1.I1.i6.p1.1" class="ltx_p">ASR : Automated Speech Recognition</p>
</div>
</li>
<li id="A1.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i7.p1" class="ltx_para">
<p id="A1.I1.i7.p1.1" class="ltx_p">TTS : Text to Speech</p>
</div>
</li>
<li id="A1.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i8.p1" class="ltx_para">
<p id="A1.I1.i8.p1.1" class="ltx_p">HMM : Hidden Markov Models</p>
</div>
</li>
<li id="A1.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i9.p1" class="ltx_para">
<p id="A1.I1.i9.p1.1" class="ltx_p">RNN : Recurrent Neural Networks</p>
</div>
</li>
<li id="A1.I1.i10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i10.p1" class="ltx_para">
<p id="A1.I1.i10.p1.1" class="ltx_p">CNN : Convolutional Neural Networks</p>
</div>
</li>
<li id="A1.I1.i11" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i11.p1" class="ltx_para ltx_noindent">
<p id="A1.I1.i11.p1.1" class="ltx_p">MMS : Massively Multilingual Speech <cite class="ltx_cite ltx_citemacro_citep">(Pratap et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite></p>
</div>
</li>
</ul>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Speaker Embeddings</h3>

<div id="A1.SS2.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.p1.1" class="ltx_p">X-vector embeddings <cite class="ltx_cite ltx_citemacro_citep">(Snyder et al., <a href="#bib.bib16" title="" class="ltx_ref">2018</a>)</cite>, derived from deep neural networks, excel in capturing intricate speaker characteristics such as emotion and gender. This makes them ideal for prosody transfer tasks, allowing for the addition or alteration of emotional and gender nuances in synthesized or altered speech, thereby improving its naturalness and expressiveness. Our study examines x-vector’s efficacy in emotion and gender recognition using the CREMA-D dataset <cite class="ltx_cite ltx_citemacro_citep">(Cao et al., <a href="#bib.bib3" title="" class="ltx_ref">2014</a>)</cite>. In gender identification, the pink and blue graphs (indicating female and male speakers, respectively) show clear distinction.</p>
</div>
<div id="A1.SS2.p2" class="ltx_para ltx_noindent">
<p id="A1.SS2.p2.1" class="ltx_p">After incorporating the x-vector embeddings, we utilized them to train a multi-layer perceptron classifier. This classifier, has two hidden layers of size 128 and 64 respectively, and was trained on 80% of the data using the cross entropy loss. The ReLU activation function was used for each layer. Testing was conducted for the gender and emotion classification task where X-vectors displayed the highest accuracy.</p>
</div>
<figure id="A1.T2" class="ltx_table">
<table id="A1.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T2.1.1.1" class="ltx_tr">
<td id="A1.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="A1.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Pre-Trained Embedding</span></td>
<td id="A1.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Accuracy (Emotion)</span></td>
<td id="A1.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="A1.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Accuracy (Gender)</span></td>
</tr>
<tr id="A1.T2.1.2.2" class="ltx_tr">
<td id="A1.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">MFCC (13)</td>
<td id="A1.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">38.04%</td>
<td id="A1.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.96%</td>
</tr>
<tr id="A1.T2.1.3.3" class="ltx_tr">
<td id="A1.T2.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Wav2Vec2</td>
<td id="A1.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">46.87%</td>
<td id="A1.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">76.29%</td>
</tr>
<tr id="A1.T2.1.4.4" class="ltx_tr">
<td id="A1.T2.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">X-vector</td>
<td id="A1.T2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="A1.T2.1.4.4.2.1" class="ltx_text ltx_font_bold">61.72</span>%</td>
<td id="A1.T2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<span id="A1.T2.1.4.4.3.1" class="ltx_text ltx_font_bold">99.19</span>%</td>
</tr>
<tr id="A1.T2.1.5.5" class="ltx_tr">
<td id="A1.T2.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">ECAPA-TDNN</td>
<td id="A1.T2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47.74%</td>
<td id="A1.T2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">97.65%</td>
</tr>
<tr id="A1.T2.1.6.6" class="ltx_tr">
<td id="A1.T2.1.6.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">WavLM</td>
<td id="A1.T2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">53.27%</td>
<td id="A1.T2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">71.32%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results of pre-trained embeddings on emotion and gender recognition</figcaption>
</figure>
<figure id="A1.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F3.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:159.0pt;"><img src="/html/2406.00022/assets/xvec_emotion.png" id="A1.F3.1.g1" class="ltx_graphics ltx_img_square" width="598" height="509" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Emotion</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F3.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:159.0pt;"><img src="/html/2406.00022/assets/xvec_gender.png" id="A1.F3.2.g1" class="ltx_graphics ltx_img_square" width="598" height="524" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Gender</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>t-SNE visualisation of x-vector embeddings</figcaption>
</figure>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Fine-Tuning Implementation Details and Plots</h3>

<figure id="A1.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F5.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:294.9pt;"><img src="/html/2406.00022/assets/french-train-loss.png" id="A1.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="255" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Training Loss vs Epochs for French</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F5.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:294.9pt;"><img src="/html/2406.00022/assets/french-val-loss.png" id="A1.F5.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="276" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Validation Loss vs Epochs for French</figcaption>
</figure>
</div>
</div>
</figure>
<div id="A1.SS3.p1" class="ltx_para ltx_noindent">
<p id="A1.SS3.p1.1" class="ltx_p">The preprocessing details for the data is given as follows:</p>
<ol id="A1.I2" class="ltx_enumerate">
<li id="A1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A1.I2.i1.p1" class="ltx_para">
<p id="A1.I2.i1.p1.1" class="ltx_p">Firstly, the dataset is downloaded using an API and the audio files are extracted. Standard audio pre-processing is applied to remove noise and silence before generating speaker embeddings (x-vectors)</p>
</div>
</li>
<li id="A1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A1.I2.i2.p1" class="ltx_para">
<p id="A1.I2.i2.p1.1" class="ltx_p">Since speakers are annotated for the dataset, the audio clips belonging to speakers with clips in the range of <math id="A1.I2.i2.p1.1.m1.2" class="ltx_Math" alttext="\in(100,400)" display="inline"><semantics id="A1.I2.i2.p1.1.m1.2a"><mrow id="A1.I2.i2.p1.1.m1.2.3" xref="A1.I2.i2.p1.1.m1.2.3.cmml"><mi id="A1.I2.i2.p1.1.m1.2.3.2" xref="A1.I2.i2.p1.1.m1.2.3.2.cmml"></mi><mo id="A1.I2.i2.p1.1.m1.2.3.1" xref="A1.I2.i2.p1.1.m1.2.3.1.cmml">∈</mo><mrow id="A1.I2.i2.p1.1.m1.2.3.3.2" xref="A1.I2.i2.p1.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="A1.I2.i2.p1.1.m1.2.3.3.2.1" xref="A1.I2.i2.p1.1.m1.2.3.3.1.cmml">(</mo><mn id="A1.I2.i2.p1.1.m1.1.1" xref="A1.I2.i2.p1.1.m1.1.1.cmml">100</mn><mo id="A1.I2.i2.p1.1.m1.2.3.3.2.2" xref="A1.I2.i2.p1.1.m1.2.3.3.1.cmml">,</mo><mn id="A1.I2.i2.p1.1.m1.2.2" xref="A1.I2.i2.p1.1.m1.2.2.cmml">400</mn><mo stretchy="false" id="A1.I2.i2.p1.1.m1.2.3.3.2.3" xref="A1.I2.i2.p1.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.I2.i2.p1.1.m1.2b"><apply id="A1.I2.i2.p1.1.m1.2.3.cmml" xref="A1.I2.i2.p1.1.m1.2.3"><in id="A1.I2.i2.p1.1.m1.2.3.1.cmml" xref="A1.I2.i2.p1.1.m1.2.3.1"></in><csymbol cd="latexml" id="A1.I2.i2.p1.1.m1.2.3.2.cmml" xref="A1.I2.i2.p1.1.m1.2.3.2">absent</csymbol><interval closure="open" id="A1.I2.i2.p1.1.m1.2.3.3.1.cmml" xref="A1.I2.i2.p1.1.m1.2.3.3.2"><cn type="integer" id="A1.I2.i2.p1.1.m1.1.1.cmml" xref="A1.I2.i2.p1.1.m1.1.1">100</cn><cn type="integer" id="A1.I2.i2.p1.1.m1.2.2.cmml" xref="A1.I2.i2.p1.1.m1.2.2">400</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.I2.i2.p1.1.m1.2c">\in(100,400)</annotation></semantics></math> are only selected for fine-tuning.</p>
</div>
</li>
<li id="A1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A1.I2.i3.p1" class="ltx_para ltx_noindent">
<p id="A1.I2.i3.p1.1" class="ltx_p">For generating the text transcripts, transliteration is performed by phenome transformation on symbols not existing in English. This is particularly important in Hindi and Tamil where the lexical scripts are entirely different from English and need to be transliterated for fine-tuning to happen.</p>
</div>
</li>
</ol>
</div>
<div id="A1.SS3.p2" class="ltx_para ltx_noindent">
<p id="A1.SS3.p2.1" class="ltx_p">This preprocessing prepares our dataset for fine-tuning on any English pre-trained TTS after which we train the SpeechT5 model on text-spectrogram pairs. The following hyperparameters are set for fine-tuning SpeechT5:</p>
<ul id="A1.I3" class="ltx_itemize">
<li id="A1.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i1.p1" class="ltx_para">
<p id="A1.I3.i1.p1.1" class="ltx_p"><span id="A1.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">Learning rate:</span> <math id="A1.I3.i1.p1.1.m1.1" class="ltx_Math" alttext="1e-5" display="inline"><semantics id="A1.I3.i1.p1.1.m1.1a"><mrow id="A1.I3.i1.p1.1.m1.1.1" xref="A1.I3.i1.p1.1.m1.1.1.cmml"><mrow id="A1.I3.i1.p1.1.m1.1.1.2" xref="A1.I3.i1.p1.1.m1.1.1.2.cmml"><mn id="A1.I3.i1.p1.1.m1.1.1.2.2" xref="A1.I3.i1.p1.1.m1.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="A1.I3.i1.p1.1.m1.1.1.2.1" xref="A1.I3.i1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="A1.I3.i1.p1.1.m1.1.1.2.3" xref="A1.I3.i1.p1.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="A1.I3.i1.p1.1.m1.1.1.1" xref="A1.I3.i1.p1.1.m1.1.1.1.cmml">−</mo><mn id="A1.I3.i1.p1.1.m1.1.1.3" xref="A1.I3.i1.p1.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.I3.i1.p1.1.m1.1b"><apply id="A1.I3.i1.p1.1.m1.1.1.cmml" xref="A1.I3.i1.p1.1.m1.1.1"><minus id="A1.I3.i1.p1.1.m1.1.1.1.cmml" xref="A1.I3.i1.p1.1.m1.1.1.1"></minus><apply id="A1.I3.i1.p1.1.m1.1.1.2.cmml" xref="A1.I3.i1.p1.1.m1.1.1.2"><times id="A1.I3.i1.p1.1.m1.1.1.2.1.cmml" xref="A1.I3.i1.p1.1.m1.1.1.2.1"></times><cn type="integer" id="A1.I3.i1.p1.1.m1.1.1.2.2.cmml" xref="A1.I3.i1.p1.1.m1.1.1.2.2">1</cn><ci id="A1.I3.i1.p1.1.m1.1.1.2.3.cmml" xref="A1.I3.i1.p1.1.m1.1.1.2.3">𝑒</ci></apply><cn type="integer" id="A1.I3.i1.p1.1.m1.1.1.3.cmml" xref="A1.I3.i1.p1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.I3.i1.p1.1.m1.1c">1e-5</annotation></semantics></math></p>
</div>
</li>
<li id="A1.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i2.p1" class="ltx_para">
<p id="A1.I3.i2.p1.1" class="ltx_p"><span id="A1.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">Epochs:</span> 10000</p>
</div>
</li>
<li id="A1.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i3.p1" class="ltx_para">
<p id="A1.I3.i3.p1.1" class="ltx_p"><span id="A1.I3.i3.p1.1.1" class="ltx_text ltx_font_bold">Warmup steps:</span> 500</p>
</div>
</li>
<li id="A1.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i4.p1" class="ltx_para">
<p id="A1.I3.i4.p1.1" class="ltx_p"><span id="A1.I3.i4.p1.1.1" class="ltx_text ltx_font_bold">Train Batch Size:</span> 4</p>
</div>
</li>
<li id="A1.I3.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i5.p1" class="ltx_para">
<p id="A1.I3.i5.p1.1" class="ltx_p"><span id="A1.I3.i5.p1.1.1" class="ltx_text ltx_font_bold">Val Batch Size:</span> 4</p>
</div>
</li>
<li id="A1.I3.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i6.p1" class="ltx_para">
<p id="A1.I3.i6.p1.1" class="ltx_p"><span id="A1.I3.i6.p1.1.1" class="ltx_text ltx_font_bold">Gradient accumulation steps:</span> 8</p>
</div>
</li>
<li id="A1.I3.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i7.p1" class="ltx_para">
<p id="A1.I3.i7.p1.1" class="ltx_p"><span id="A1.I3.i7.p1.1.1" class="ltx_text ltx_font_bold">fp16:</span> True</p>
</div>
</li>
<li id="A1.I3.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i8.p1" class="ltx_para ltx_noindent">
<p id="A1.I3.i8.p1.1" class="ltx_p"><span id="A1.I3.i8.p1.1.1" class="ltx_text ltx_font_bold">Evaluation Strategy:</span> ”steps”</p>
</div>
</li>
</ul>
</div>
<figure id="A1.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F7.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:208.1pt;"><img src="/html/2406.00022/assets/french_10k_epochs_wavform.png" id="A1.F7.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="338" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Comparing Waveforms of the three audio clips</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F7.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:208.1pt;"><img src="/html/2406.00022/assets/french_10k_epochs_spectrogram.png" id="A1.F7.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="340" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Comparing Spectrograms of the three audio clips</figcaption>
</figure>
</div>
</div>
</figure>
<div id="A1.SS3.p3" class="ltx_para ltx_noindent">
<p id="A1.SS3.p3.1" class="ltx_p">Figures <a href="#A1.F5" title="Figure 5 ‣ A.3 Fine-Tuning Implementation Details and Plots ‣ Appendix A Appendix ‣ Multilingual Prosody Transfer: Comparing Supervised &amp; Transfer Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#A1.F5" title="Figure 5 ‣ A.3 Fine-Tuning Implementation Details and Plots ‣ Appendix A Appendix ‣ Multilingual Prosody Transfer: Comparing Supervised &amp; Transfer Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> display the loss curves for training and validating SpeechT5 on VoxPopuli-French, converging around 10000 epochs, showing effective model fine-tuning. Figures <a href="#A1.F7" title="Figure 7 ‣ A.3 Fine-Tuning Implementation Details and Plots ‣ Appendix A Appendix ‣ Multilingual Prosody Transfer: Comparing Supervised &amp; Transfer Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and <a href="#A1.F7" title="Figure 7 ‣ A.3 Fine-Tuning Implementation Details and Plots ‣ Appendix A Appendix ‣ Multilingual Prosody Transfer: Comparing Supervised &amp; Transfer Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> present a waveform and spectrogram generated by FreeVC and SpeechT5, respectively, using a random test set audio clip, termed <em id="A1.SS3.p3.1.1" class="ltx_emph ltx_font_italic">Reference Audio</em>. This clip, processed for x-vector embeddings, is fed into the fine-tuned SpeechT5, resulting in <em id="A1.SS3.p3.1.2" class="ltx_emph ltx_font_italic">T5 Synthesised Audio</em>, while FreeVC produces <em id="A1.SS3.p3.1.3" class="ltx_emph ltx_font_italic">VC Synthesised Audio</em>. The spectrogram reveals that VC Synthesised Audio more accurately matches the original, with a stable waveform, in contrast to the stretched, high-energy T5 Synthesised Audio, which deviates significantly from the reference.</p>
</div>
</section>
<section id="A1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>MOS and Recognition Accuracy Calculation Protocol</h3>

<div id="A1.SS4.p1" class="ltx_para ltx_noindent">
<p id="A1.SS4.p1.1" class="ltx_p">We conducted a survey where 35 candidates heard 10 sets of voice clip. One set included the ground truth (original audio) and the corresponding generated audio.</p>
</div>
<div id="A1.SS4.p2" class="ltx_para ltx_noindent">
<p id="A1.SS4.p2.1" class="ltx_p"><span id="A1.SS4.p2.1.1" class="ltx_text ltx_font_bold">MOS Score:</span> Once they heard a pair, the candidates filled out a questionnaire with 5 questions on a 1-5 rating scale.</p>
<ul id="A1.I4" class="ltx_itemize">
<li id="A1.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I4.i1.p1" class="ltx_para">
<p id="A1.I4.i1.p1.1" class="ltx_p">Rate the naturalness of the clip (assessment of non-robotic voice)</p>
</div>
</li>
<li id="A1.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I4.i2.p1" class="ltx_para">
<p id="A1.I4.i2.p1.1" class="ltx_p">Rate the emphasis and intonation of spoken words</p>
</div>
</li>
<li id="A1.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I4.i3.p1" class="ltx_para">
<p id="A1.I4.i3.p1.1" class="ltx_p">Does the speaker of the two clips sound the same</p>
</div>
</li>
<li id="A1.I4.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I4.i4.p1" class="ltx_para">
<p id="A1.I4.i4.p1.1" class="ltx_p">Evaluate rhythm and speech consistency</p>
</div>
</li>
<li id="A1.I4.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I4.i5.p1" class="ltx_para ltx_noindent">
<p id="A1.I4.i5.p1.1" class="ltx_p">Fluency of speech of the generated clip versus the input clip</p>
</div>
</li>
</ul>
</div>
<div id="A1.SS4.p3" class="ltx_para ltx_noindent">
<p id="A1.SS4.p3.1" class="ltx_p"><span id="A1.SS4.p3.1.1" class="ltx_text ltx_font_bold">Recognition Accuracy:</span> This evaluates if the clips can be identified as the same user. Yes or no response.</p>
</div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.00021" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.00022" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.00022">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.00022" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.00023" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 21:18:45 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
