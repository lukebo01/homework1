<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.05661] MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations</title><meta property="og:description" content="In recent years, self-supervised pre-training methods have gained significant traction in learning high-level information from raw speech. Among these methods, HuBERT has demonstrated SOTA performance in automatic speeâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.05661">

<!--Generated on Sat Jul  6 00:32:41 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\name</span>
<p id="p1.2" class="ltx_p">[affiliation=1]HemantYadav
<span id="p1.2.1" class="ltx_ERROR undefined">\name</span>[affiliation=2]SunayanaSitaram
<span id="p1.2.2" class="ltx_ERROR undefined">\name</span>[affiliation=1]Rajiv RatnShah


<span id="p1.2.3" class="ltx_ERROR undefined">\interspeechfinaltrue</span>

</p>
</div>
<h1 class="ltx_title ltx_title_document">MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">In recent years, self-supervised pre-training methods have gained significant traction in learning high-level information from raw speech. Among these methods, HuBERT has demonstrated SOTA performance in automatic speech recognition (ASR). However, HuBERT's performance lags behind data2vec due to disparities in pre-training strategies. In this paper, we propose (i) a Swap method to address pre-training and inference mismatch observed in HuBERT and (ii) incorporates Multicluster masked prediction loss for more effective utilization of the models capacity. The resulting method is, MS-HuBERT, an end-to-end self-supervised pre-training method for
learning robust speech representations. It beats vanilla HuBERT on the ASR Librispeech benchmark on average by a 5% margin when evaluated on different finetuning splits. Additionally, we demonstrate that the learned embeddings obtained during pre-training encode essential information for improving performance of content based tasks such as ASR.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Automatic speech recognition, Multicluster masked prediction loss, HuBERT
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the recent years, there has been a significant interest in studying self-supervised pre-training methods to learn/encode high level information present in the speech data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. These SSL methods utilize the input data itself to learn to encode useful information, with the choice of pretext task playing a pivotal role in the encoded information. The most popular pretext task used is masked predictive coding (MPC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> is one such model that popularized the masked language modelling (MLM) technique to learn high-level speech representations from raw audio by achieving state-of-the-art (SOTA) on the automatic speech recognition (ASR) task. The underlying concept of HuBERT revolves around iterative pre-training: starting with a raw audio/pseudo-label pair (x/y), the model undergoes successive training iterations where the trained model updates the pseudo-labels, iteratively refining its representations until a predefined stopping criterion is reached. However, despite its success, HuBERT falls short compared to data2vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> in ASR performance for two primary reasons: firstly, during pre-training, data2vec accesses the full context to generate continuous labels, which are updated after each gradient update step, as opposed to the fixed discrete labels utilized in HuBERT for the each iteration; and secondly, the ground truth labels are created by averaging the representations from multiple layers for loss calculation.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To bridge this gap, we propose two modifications to the HuBERT framework. Firstly, we introduce the "Swap" method to enable full context access during pre-training, thus addressing the pre-training and inference mismatch observed in HuBERT and other MLM-based methods by using both the masked and unmasked views during pre-training. Swap is motivated by a simple idea, used heavily in the field of computer vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Where two augmented views of the input are used to learn a high level representation. Given <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="e" display="inline"><semantics id="S1.p2.1.m1.1a"><mi id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><ci id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">ğ‘’</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">e</annotation></semantics></math> layers in a encoder, it is a general practice to add a similarity loss on the output embeddings of the encoder after each layer, as seen in works using U-net type architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. In contrast, our proposed Swap method simply swaps the output embedding, at certain indices, after each layer of the encoder between the masked and unmasked view of the input. Motivated by the simple fact, that the learned model is expected to generate exactly the same output regardless of the two views, Swap method guides the output after each layer to achieve the same.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Secondly, inspired by the work of Yadav et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, we incorporate a Multicluster masked prediction loss (MPL) approach. Using multiple cluster centers, also called multiple resolutions, has been investigated by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, the author introduces down-sampling and up-sampling modules within the transformer encoder after each layer to facilitate learning features at multiple resolutions. On the other hand, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> explores parallel and hierarchical variations of HuBERT with findings indicating the superiority of the hierarchical approach. This involves training multiple models, each model adds almost same parameters as the original HuBERT, at various resolutions using CNN as a down-sampling module. Lastly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> uses the fact that MPL is applied at multiple layers of encoder at different resolutions. This method does not introduce any additional parameters to the original HuBERT model, except the linear layers used for loss calculation which are discarded after the pre-training. In this work, we adopt this approach and modify it for our use case for loss calculation given its simplicity and higher performance.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">These changes align HuBERT more closely towards data2vec, primarily differing in their loss functions for pre-training and other minor changes. The goal is study how much HuBERT can be improved, with these changes, on the ASR task.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2406.05661/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="1295" height="1757" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Proposed MS-HuBERT approach, an end-to-end self supervised pre-training method to learn robust speech representations. The input raw audio is passed to a CNN encoder. Two copies of the output is created i.e., masked and unmasked. Which is passed through the Swap modified 2nd encoder. Multicluster Masked prediction loss is calculated, masked indices only, on the output embeddings from different blocks of the modified 2nd encoder.</figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Based on these observations. In this work, we propose MulticlusterSwap-HuBERT (MS-HuBERT) method, incorporates (i) our proposed Swap method to address the pre-training and inference mismatch issue, as the [MASK] symbol never appears during the inference, and (ii) the Multicluster MPL similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Our contributions are as follows:</p>
</div>
<div id="S1.p6" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose MS-HuBERT, an end-to-end self supervised pre-training method to learn robust speech representations. It combines the Swap method and Multicluster MPL with HuBERT as shown in Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We show that MS-HuBERT outperforms the original HuBERT on the ASR Librispeech benchmark by a large margin. And matches the performance of data2vec in high-resource setting.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We showcase that the embeddings acquired during pre-training encode crucial information essential for addressing content based tasks such has ASR and phoneme recognition (PR). This shows the effective utliziation of the modeling capacity.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Background</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.3" class="ltx_p">HuBERT is an iterative pre-training SSL method comprising of two encoders based on CNN (1st) and transformer (2nd), in that order, architectures. The CNN encoder serves the dual purpose of down-sampling the input data. The resulting output is passed, denoted as <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="U" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">ğ‘ˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">U</annotation></semantics></math>, to the transformer encoder and its output is used for loss calculation. During the pre-training stage, raw audio is passed to the CNN encoder and approximately <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mrow id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mn id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml">50</mn><mo id="S2.SS1.p1.2.m2.1.1.1" xref="S2.SS1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><csymbol cd="latexml" id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">50\%</annotation></semantics></math> of the output is masked, using the masking token <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="[M]" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mrow id="S2.SS1.p1.3.m3.1.2.2" xref="S2.SS1.p1.3.m3.1.2.1.cmml"><mo stretchy="false" id="S2.SS1.p1.3.m3.1.2.2.1" xref="S2.SS1.p1.3.m3.1.2.1.1.cmml">[</mo><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">M</mi><mo stretchy="false" id="S2.SS1.p1.3.m3.1.2.2.2" xref="S2.SS1.p1.3.m3.1.2.1.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.2.1.cmml" xref="S2.SS1.p1.3.m3.1.2.2"><csymbol cd="latexml" id="S2.SS1.p1.3.m3.1.2.1.1.cmml" xref="S2.SS1.p1.3.m3.1.2.2.1">delimited-[]</csymbol><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">ğ‘€</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">[M]</annotation></semantics></math> and is subsequently passed to the transformer encoder. The network is then trained to optimize to output a discrete target sequence by minimizing the masked prediction loss. The complete details can be found in the original paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>MS-HuBERT</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">MS-HuBERT augments HuBERT model in two ways (i) the Swap method and (ii) the Multicluster MPL as shown in Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Swap method is introduced to address the pre-training and inference mismatch phase in HuBERT i.e, during inference the model does not use masking. Swap method modifies the 2nd encoder of HuBERT, such that the updated model now encounters, two views of the input, both masked and unmasked inputs during pre-training. Lastly, our proposed method uses modified Multicluster MPL as proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, because of its enhanced model capacity utilization in learning features suitable for the ASR task. These changes aim to improve the ASR performance, as shown in the Table <a href="#S4.T1" title="Table 1 â€£ 4.1 Main Results: Supervised Fine-tuning and Inference â€£ 4 Results â€£ MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Swap</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.7" class="ltx_p">Given a raw audio as an input, of batch of size 1, to the 1st encoder (CNN), its output is denoted as <math id="S2.SS2.SSS1.p1.1.m1.5" class="ltx_Math" alttext="X=x_{1},x_{2},...,x_{t-1},x_{t}" display="inline"><semantics id="S2.SS2.SSS1.p1.1.m1.5a"><mrow id="S2.SS2.SSS1.p1.1.m1.5.5" xref="S2.SS2.SSS1.p1.1.m1.5.5.cmml"><mi id="S2.SS2.SSS1.p1.1.m1.5.5.6" xref="S2.SS2.SSS1.p1.1.m1.5.5.6.cmml">X</mi><mo id="S2.SS2.SSS1.p1.1.m1.5.5.5" xref="S2.SS2.SSS1.p1.1.m1.5.5.5.cmml">=</mo><mrow id="S2.SS2.SSS1.p1.1.m1.5.5.4.4" xref="S2.SS2.SSS1.p1.1.m1.5.5.4.5.cmml"><msub id="S2.SS2.SSS1.p1.1.m1.2.2.1.1.1" xref="S2.SS2.SSS1.p1.1.m1.2.2.1.1.1.cmml"><mi id="S2.SS2.SSS1.p1.1.m1.2.2.1.1.1.2" xref="S2.SS2.SSS1.p1.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S2.SS2.SSS1.p1.1.m1.2.2.1.1.1.3" xref="S2.SS2.SSS1.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS2.SSS1.p1.1.m1.5.5.4.4.5" xref="S2.SS2.SSS1.p1.1.m1.5.5.4.5.cmml">,</mo><msub id="S2.SS2.SSS1.p1.1.m1.3.3.2.2.2" xref="S2.SS2.SSS1.p1.1.m1.3.3.2.2.2.cmml"><mi id="S2.SS2.SSS1.p1.1.m1.3.3.2.2.2.2" xref="S2.SS2.SSS1.p1.1.m1.3.3.2.2.2.2.cmml">x</mi><mn id="S2.SS2.SSS1.p1.1.m1.3.3.2.2.2.3" xref="S2.SS2.SSS1.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS2.SSS1.p1.1.m1.5.5.4.4.6" xref="S2.SS2.SSS1.p1.1.m1.5.5.4.5.cmml">,</mo><mi mathvariant="normal" id="S2.SS2.SSS1.p1.1.m1.1.1" xref="S2.SS2.SSS1.p1.1.m1.1.1.cmml">â€¦</mi><mo id="S2.SS2.SSS1.p1.1.m1.5.5.4.4.7" xref="S2.SS2.SSS1.p1.1.m1.5.5.4.5.cmml">,</mo><msub id="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3" xref="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.cmml"><mi id="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.2" xref="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.2.cmml">x</mi><mrow id="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.3" xref="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.cmml"><mi id="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.2" xref="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.2.cmml">t</mi><mo id="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.1" xref="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.1.cmml">âˆ’</mo><mn id="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.3" xref="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.3.cmml">1</mn></mrow></msub><mo id="S2.SS2.SSS1.p1.1.m1.5.5.4.4.8" xref="S2.SS2.SSS1.p1.1.m1.5.5.4.5.cmml">,</mo><msub id="S2.SS2.SSS1.p1.1.m1.5.5.4.4.4" xref="S2.SS2.SSS1.p1.1.m1.5.5.4.4.4.cmml"><mi id="S2.SS2.SSS1.p1.1.m1.5.5.4.4.4.2" xref="S2.SS2.SSS1.p1.1.m1.5.5.4.4.4.2.cmml">x</mi><mi id="S2.SS2.SSS1.p1.1.m1.5.5.4.4.4.3" xref="S2.SS2.SSS1.p1.1.m1.5.5.4.4.4.3.cmml">t</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.1.m1.5b"><apply id="S2.SS2.SSS1.p1.1.m1.5.5.cmml" xref="S2.SS2.SSS1.p1.1.m1.5.5"><eq id="S2.SS2.SSS1.p1.1.m1.5.5.5.cmml" xref="S2.SS2.SSS1.p1.1.m1.5.5.5"></eq><ci id="S2.SS2.SSS1.p1.1.m1.5.5.6.cmml" xref="S2.SS2.SSS1.p1.1.m1.5.5.6">ğ‘‹</ci><list id="S2.SS2.SSS1.p1.1.m1.5.5.4.5.cmml" xref="S2.SS2.SSS1.p1.1.m1.5.5.4.4"><apply id="S2.SS2.SSS1.p1.1.m1.2.2.1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p1.1.m1.2.2.1.1.1.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.2.2.1.1.1.2">ğ‘¥</ci><cn type="integer" id="S2.SS2.SSS1.p1.1.m1.2.2.1.1.1.3.cmml" xref="S2.SS2.SSS1.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS2.SSS1.p1.1.m1.3.3.2.2.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.1.m1.3.3.2.2.2.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S2.SS2.SSS1.p1.1.m1.3.3.2.2.2.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.3.3.2.2.2.2">ğ‘¥</ci><cn type="integer" id="S2.SS2.SSS1.p1.1.m1.3.3.2.2.2.3.cmml" xref="S2.SS2.SSS1.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1">â€¦</ci><apply id="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.cmml" xref="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.2">ğ‘¥</ci><apply id="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.cmml" xref="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.3"><minus id="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.1"></minus><ci id="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.2">ğ‘¡</ci><cn type="integer" id="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.3.cmml" xref="S2.SS2.SSS1.p1.1.m1.4.4.3.3.3.3.3">1</cn></apply></apply><apply id="S2.SS2.SSS1.p1.1.m1.5.5.4.4.4.cmml" xref="S2.SS2.SSS1.p1.1.m1.5.5.4.4.4"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.1.m1.5.5.4.4.4.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.5.5.4.4.4">subscript</csymbol><ci id="S2.SS2.SSS1.p1.1.m1.5.5.4.4.4.2.cmml" xref="S2.SS2.SSS1.p1.1.m1.5.5.4.4.4.2">ğ‘¥</ci><ci id="S2.SS2.SSS1.p1.1.m1.5.5.4.4.4.3.cmml" xref="S2.SS2.SSS1.p1.1.m1.5.5.4.4.4.3">ğ‘¡</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.1.m1.5c">X=x_{1},x_{2},...,x_{t-1},x_{t}</annotation></semantics></math>, where <math id="S2.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS2.SSS1.p1.2.m2.1a"><mi id="S2.SS2.SSS1.p1.2.m2.1.1" xref="S2.SS2.SSS1.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.2.m2.1b"><ci id="S2.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.2.m2.1c">t</annotation></semantics></math> represents the total number of output tokens.
Two views of <math id="S2.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.SS2.SSS1.p1.3.m3.1a"><mi id="S2.SS2.SSS1.p1.3.m3.1.1" xref="S2.SS2.SSS1.p1.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.3.m3.1b"><ci id="S2.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.3.m3.1c">X</annotation></semantics></math> are created: (i) masked view, where on average, around 50% of these tokens are masked, meaning that half of these tokens are replaced with the <math id="S2.SS2.SSS1.p1.4.m4.1" class="ltx_Math" alttext="[m]" display="inline"><semantics id="S2.SS2.SSS1.p1.4.m4.1a"><mrow id="S2.SS2.SSS1.p1.4.m4.1.2.2" xref="S2.SS2.SSS1.p1.4.m4.1.2.1.cmml"><mo stretchy="false" id="S2.SS2.SSS1.p1.4.m4.1.2.2.1" xref="S2.SS2.SSS1.p1.4.m4.1.2.1.1.cmml">[</mo><mi id="S2.SS2.SSS1.p1.4.m4.1.1" xref="S2.SS2.SSS1.p1.4.m4.1.1.cmml">m</mi><mo stretchy="false" id="S2.SS2.SSS1.p1.4.m4.1.2.2.2" xref="S2.SS2.SSS1.p1.4.m4.1.2.1.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.4.m4.1b"><apply id="S2.SS2.SSS1.p1.4.m4.1.2.1.cmml" xref="S2.SS2.SSS1.p1.4.m4.1.2.2"><csymbol cd="latexml" id="S2.SS2.SSS1.p1.4.m4.1.2.1.1.cmml" xref="S2.SS2.SSS1.p1.4.m4.1.2.2.1">delimited-[]</csymbol><ci id="S2.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS1.p1.4.m4.1.1">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.4.m4.1c">[m]</annotation></semantics></math> token, resulting in an updated output <math id="S2.SS2.SSS1.p1.5.m5.7" class="ltx_Math" alttext="X^{m}=x_{1},[m],...,[m],x_{t}" display="inline"><semantics id="S2.SS2.SSS1.p1.5.m5.7a"><mrow id="S2.SS2.SSS1.p1.5.m5.7.7" xref="S2.SS2.SSS1.p1.5.m5.7.7.cmml"><msup id="S2.SS2.SSS1.p1.5.m5.7.7.6" xref="S2.SS2.SSS1.p1.5.m5.7.7.6.cmml"><mi id="S2.SS2.SSS1.p1.5.m5.7.7.6.2" xref="S2.SS2.SSS1.p1.5.m5.7.7.6.2.cmml">X</mi><mi id="S2.SS2.SSS1.p1.5.m5.7.7.6.3" xref="S2.SS2.SSS1.p1.5.m5.7.7.6.3.cmml">m</mi></msup><mo id="S2.SS2.SSS1.p1.5.m5.7.7.5" xref="S2.SS2.SSS1.p1.5.m5.7.7.5.cmml">=</mo><mrow id="S2.SS2.SSS1.p1.5.m5.7.7.4.4" xref="S2.SS2.SSS1.p1.5.m5.7.7.4.5.cmml"><msub id="S2.SS2.SSS1.p1.5.m5.4.4.1.1.1" xref="S2.SS2.SSS1.p1.5.m5.4.4.1.1.1.cmml"><mi id="S2.SS2.SSS1.p1.5.m5.4.4.1.1.1.2" xref="S2.SS2.SSS1.p1.5.m5.4.4.1.1.1.2.cmml">x</mi><mn id="S2.SS2.SSS1.p1.5.m5.4.4.1.1.1.3" xref="S2.SS2.SSS1.p1.5.m5.4.4.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS2.SSS1.p1.5.m5.7.7.4.4.5" xref="S2.SS2.SSS1.p1.5.m5.7.7.4.5.cmml">,</mo><mrow id="S2.SS2.SSS1.p1.5.m5.5.5.2.2.2.2" xref="S2.SS2.SSS1.p1.5.m5.5.5.2.2.2.1.cmml"><mo stretchy="false" id="S2.SS2.SSS1.p1.5.m5.5.5.2.2.2.2.1" xref="S2.SS2.SSS1.p1.5.m5.5.5.2.2.2.1.1.cmml">[</mo><mi id="S2.SS2.SSS1.p1.5.m5.1.1" xref="S2.SS2.SSS1.p1.5.m5.1.1.cmml">m</mi><mo stretchy="false" id="S2.SS2.SSS1.p1.5.m5.5.5.2.2.2.2.2" xref="S2.SS2.SSS1.p1.5.m5.5.5.2.2.2.1.1.cmml">]</mo></mrow><mo id="S2.SS2.SSS1.p1.5.m5.7.7.4.4.6" xref="S2.SS2.SSS1.p1.5.m5.7.7.4.5.cmml">,</mo><mi mathvariant="normal" id="S2.SS2.SSS1.p1.5.m5.3.3" xref="S2.SS2.SSS1.p1.5.m5.3.3.cmml">â€¦</mi><mo id="S2.SS2.SSS1.p1.5.m5.7.7.4.4.7" xref="S2.SS2.SSS1.p1.5.m5.7.7.4.5.cmml">,</mo><mrow id="S2.SS2.SSS1.p1.5.m5.6.6.3.3.3.2" xref="S2.SS2.SSS1.p1.5.m5.6.6.3.3.3.1.cmml"><mo stretchy="false" id="S2.SS2.SSS1.p1.5.m5.6.6.3.3.3.2.1" xref="S2.SS2.SSS1.p1.5.m5.6.6.3.3.3.1.1.cmml">[</mo><mi id="S2.SS2.SSS1.p1.5.m5.2.2" xref="S2.SS2.SSS1.p1.5.m5.2.2.cmml">m</mi><mo stretchy="false" id="S2.SS2.SSS1.p1.5.m5.6.6.3.3.3.2.2" xref="S2.SS2.SSS1.p1.5.m5.6.6.3.3.3.1.1.cmml">]</mo></mrow><mo id="S2.SS2.SSS1.p1.5.m5.7.7.4.4.8" xref="S2.SS2.SSS1.p1.5.m5.7.7.4.5.cmml">,</mo><msub id="S2.SS2.SSS1.p1.5.m5.7.7.4.4.4" xref="S2.SS2.SSS1.p1.5.m5.7.7.4.4.4.cmml"><mi id="S2.SS2.SSS1.p1.5.m5.7.7.4.4.4.2" xref="S2.SS2.SSS1.p1.5.m5.7.7.4.4.4.2.cmml">x</mi><mi id="S2.SS2.SSS1.p1.5.m5.7.7.4.4.4.3" xref="S2.SS2.SSS1.p1.5.m5.7.7.4.4.4.3.cmml">t</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.5.m5.7b"><apply id="S2.SS2.SSS1.p1.5.m5.7.7.cmml" xref="S2.SS2.SSS1.p1.5.m5.7.7"><eq id="S2.SS2.SSS1.p1.5.m5.7.7.5.cmml" xref="S2.SS2.SSS1.p1.5.m5.7.7.5"></eq><apply id="S2.SS2.SSS1.p1.5.m5.7.7.6.cmml" xref="S2.SS2.SSS1.p1.5.m5.7.7.6"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.5.m5.7.7.6.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.7.7.6">superscript</csymbol><ci id="S2.SS2.SSS1.p1.5.m5.7.7.6.2.cmml" xref="S2.SS2.SSS1.p1.5.m5.7.7.6.2">ğ‘‹</ci><ci id="S2.SS2.SSS1.p1.5.m5.7.7.6.3.cmml" xref="S2.SS2.SSS1.p1.5.m5.7.7.6.3">ğ‘š</ci></apply><list id="S2.SS2.SSS1.p1.5.m5.7.7.4.5.cmml" xref="S2.SS2.SSS1.p1.5.m5.7.7.4.4"><apply id="S2.SS2.SSS1.p1.5.m5.4.4.1.1.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.4.4.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.5.m5.4.4.1.1.1.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.4.4.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p1.5.m5.4.4.1.1.1.2.cmml" xref="S2.SS2.SSS1.p1.5.m5.4.4.1.1.1.2">ğ‘¥</ci><cn type="integer" id="S2.SS2.SSS1.p1.5.m5.4.4.1.1.1.3.cmml" xref="S2.SS2.SSS1.p1.5.m5.4.4.1.1.1.3">1</cn></apply><apply id="S2.SS2.SSS1.p1.5.m5.5.5.2.2.2.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.5.5.2.2.2.2"><csymbol cd="latexml" id="S2.SS2.SSS1.p1.5.m5.5.5.2.2.2.1.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.5.5.2.2.2.2.1">delimited-[]</csymbol><ci id="S2.SS2.SSS1.p1.5.m5.1.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.1.1">ğ‘š</ci></apply><ci id="S2.SS2.SSS1.p1.5.m5.3.3.cmml" xref="S2.SS2.SSS1.p1.5.m5.3.3">â€¦</ci><apply id="S2.SS2.SSS1.p1.5.m5.6.6.3.3.3.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.6.6.3.3.3.2"><csymbol cd="latexml" id="S2.SS2.SSS1.p1.5.m5.6.6.3.3.3.1.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.6.6.3.3.3.2.1">delimited-[]</csymbol><ci id="S2.SS2.SSS1.p1.5.m5.2.2.cmml" xref="S2.SS2.SSS1.p1.5.m5.2.2">ğ‘š</ci></apply><apply id="S2.SS2.SSS1.p1.5.m5.7.7.4.4.4.cmml" xref="S2.SS2.SSS1.p1.5.m5.7.7.4.4.4"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.5.m5.7.7.4.4.4.1.cmml" xref="S2.SS2.SSS1.p1.5.m5.7.7.4.4.4">subscript</csymbol><ci id="S2.SS2.SSS1.p1.5.m5.7.7.4.4.4.2.cmml" xref="S2.SS2.SSS1.p1.5.m5.7.7.4.4.4.2">ğ‘¥</ci><ci id="S2.SS2.SSS1.p1.5.m5.7.7.4.4.4.3.cmml" xref="S2.SS2.SSS1.p1.5.m5.7.7.4.4.4.3">ğ‘¡</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.5.m5.7c">X^{m}=x_{1},[m],...,[m],x_{t}</annotation></semantics></math> (view 1) and (ii) unmasked view, a duplicate of the original <math id="S2.SS2.SSS1.p1.6.m6.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.SS2.SSS1.p1.6.m6.1a"><mi id="S2.SS2.SSS1.p1.6.m6.1.1" xref="S2.SS2.SSS1.p1.6.m6.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.6.m6.1b"><ci id="S2.SS2.SSS1.p1.6.m6.1.1.cmml" xref="S2.SS2.SSS1.p1.6.m6.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.6.m6.1c">X</annotation></semantics></math>, denoted as <math id="S2.SS2.SSS1.p1.7.m7.5" class="ltx_Math" alttext="X^{c}=x_{1}^{c},x_{2}^{c},...,x_{t-1}^{c},x_{t}^{c}" display="inline"><semantics id="S2.SS2.SSS1.p1.7.m7.5a"><mrow id="S2.SS2.SSS1.p1.7.m7.5.5" xref="S2.SS2.SSS1.p1.7.m7.5.5.cmml"><msup id="S2.SS2.SSS1.p1.7.m7.5.5.6" xref="S2.SS2.SSS1.p1.7.m7.5.5.6.cmml"><mi id="S2.SS2.SSS1.p1.7.m7.5.5.6.2" xref="S2.SS2.SSS1.p1.7.m7.5.5.6.2.cmml">X</mi><mi id="S2.SS2.SSS1.p1.7.m7.5.5.6.3" xref="S2.SS2.SSS1.p1.7.m7.5.5.6.3.cmml">c</mi></msup><mo id="S2.SS2.SSS1.p1.7.m7.5.5.5" xref="S2.SS2.SSS1.p1.7.m7.5.5.5.cmml">=</mo><mrow id="S2.SS2.SSS1.p1.7.m7.5.5.4.4" xref="S2.SS2.SSS1.p1.7.m7.5.5.4.5.cmml"><msubsup id="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1" xref="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1.cmml"><mi id="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1.2.2" xref="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1.2.2.cmml">x</mi><mn id="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1.2.3" xref="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1.2.3.cmml">1</mn><mi id="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1.3" xref="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1.3.cmml">c</mi></msubsup><mo id="S2.SS2.SSS1.p1.7.m7.5.5.4.4.5" xref="S2.SS2.SSS1.p1.7.m7.5.5.4.5.cmml">,</mo><msubsup id="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2" xref="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2.cmml"><mi id="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2.2.2" xref="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2.2.2.cmml">x</mi><mn id="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2.2.3" xref="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2.2.3.cmml">2</mn><mi id="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2.3" xref="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2.3.cmml">c</mi></msubsup><mo id="S2.SS2.SSS1.p1.7.m7.5.5.4.4.6" xref="S2.SS2.SSS1.p1.7.m7.5.5.4.5.cmml">,</mo><mi mathvariant="normal" id="S2.SS2.SSS1.p1.7.m7.1.1" xref="S2.SS2.SSS1.p1.7.m7.1.1.cmml">â€¦</mi><mo id="S2.SS2.SSS1.p1.7.m7.5.5.4.4.7" xref="S2.SS2.SSS1.p1.7.m7.5.5.4.5.cmml">,</mo><msubsup id="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3" xref="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.cmml"><mi id="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.2" xref="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.2.cmml">x</mi><mrow id="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.3" xref="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.3.cmml"><mi id="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.3.2" xref="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.3.2.cmml">t</mi><mo id="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.3.1" xref="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.3.1.cmml">âˆ’</mo><mn id="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.3.3" xref="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.3.3.cmml">1</mn></mrow><mi id="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.3" xref="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.3.cmml">c</mi></msubsup><mo id="S2.SS2.SSS1.p1.7.m7.5.5.4.4.8" xref="S2.SS2.SSS1.p1.7.m7.5.5.4.5.cmml">,</mo><msubsup id="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4" xref="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4.cmml"><mi id="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4.2.2" xref="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4.2.2.cmml">x</mi><mi id="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4.2.3" xref="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4.2.3.cmml">t</mi><mi id="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4.3" xref="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4.3.cmml">c</mi></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.7.m7.5b"><apply id="S2.SS2.SSS1.p1.7.m7.5.5.cmml" xref="S2.SS2.SSS1.p1.7.m7.5.5"><eq id="S2.SS2.SSS1.p1.7.m7.5.5.5.cmml" xref="S2.SS2.SSS1.p1.7.m7.5.5.5"></eq><apply id="S2.SS2.SSS1.p1.7.m7.5.5.6.cmml" xref="S2.SS2.SSS1.p1.7.m7.5.5.6"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.7.m7.5.5.6.1.cmml" xref="S2.SS2.SSS1.p1.7.m7.5.5.6">superscript</csymbol><ci id="S2.SS2.SSS1.p1.7.m7.5.5.6.2.cmml" xref="S2.SS2.SSS1.p1.7.m7.5.5.6.2">ğ‘‹</ci><ci id="S2.SS2.SSS1.p1.7.m7.5.5.6.3.cmml" xref="S2.SS2.SSS1.p1.7.m7.5.5.6.3">ğ‘</ci></apply><list id="S2.SS2.SSS1.p1.7.m7.5.5.4.5.cmml" xref="S2.SS2.SSS1.p1.7.m7.5.5.4.4"><apply id="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1.cmml" xref="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1.1.cmml" xref="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1">superscript</csymbol><apply id="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1.2.cmml" xref="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1.2.1.cmml" xref="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1.2.2.cmml" xref="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1.2.2">ğ‘¥</ci><cn type="integer" id="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1.2.3.cmml" xref="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1.2.3">1</cn></apply><ci id="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1.3.cmml" xref="S2.SS2.SSS1.p1.7.m7.2.2.1.1.1.3">ğ‘</ci></apply><apply id="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2.cmml" xref="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2.1.cmml" xref="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2">superscript</csymbol><apply id="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2.2.cmml" xref="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2.2.1.cmml" xref="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2">subscript</csymbol><ci id="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2.2.2.cmml" xref="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2.2.2">ğ‘¥</ci><cn type="integer" id="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2.2.3.cmml" xref="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2.2.3">2</cn></apply><ci id="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2.3.cmml" xref="S2.SS2.SSS1.p1.7.m7.3.3.2.2.2.3">ğ‘</ci></apply><ci id="S2.SS2.SSS1.p1.7.m7.1.1.cmml" xref="S2.SS2.SSS1.p1.7.m7.1.1">â€¦</ci><apply id="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.cmml" xref="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.1.cmml" xref="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3">superscript</csymbol><apply id="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.cmml" xref="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.1.cmml" xref="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3">subscript</csymbol><ci id="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.2.cmml" xref="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.2">ğ‘¥</ci><apply id="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.3.cmml" xref="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.3"><minus id="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.3.1.cmml" xref="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.3.1"></minus><ci id="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.3.2.cmml" xref="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.3.2">ğ‘¡</ci><cn type="integer" id="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.3.3.cmml" xref="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.2.3.3">1</cn></apply></apply><ci id="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.3.cmml" xref="S2.SS2.SSS1.p1.7.m7.4.4.3.3.3.3">ğ‘</ci></apply><apply id="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4.cmml" xref="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4.1.cmml" xref="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4">superscript</csymbol><apply id="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4.2.cmml" xref="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4.2.1.cmml" xref="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4">subscript</csymbol><ci id="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4.2.2.cmml" xref="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4.2.2">ğ‘¥</ci><ci id="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4.2.3.cmml" xref="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4.2.3">ğ‘¡</ci></apply><ci id="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4.3.cmml" xref="S2.SS2.SSS1.p1.7.m7.5.5.4.4.4.3">ğ‘</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.7.m7.5c">X^{c}=x_{1}^{c},x_{2}^{c},...,x_{t-1}^{c},x_{t}^{c}</annotation></semantics></math> (view 2). These two views are combined, to form a batch of size 2, and is passed to the 2nd encoder.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.5" class="ltx_p">The second encoder has <math id="S2.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS2.SSS1.p2.1.m1.1a"><mi id="S2.SS2.SSS1.p2.1.m1.1.1" xref="S2.SS2.SSS1.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.1.m1.1b"><ci id="S2.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.1.m1.1c">N</annotation></semantics></math> layers, each composed of a transformer layer followed by a Swap layer as shown in Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The transformer layer is exactly similar to the original HuBERT method. The proposed Swap method's function is to swap the outputs, at the masked indices, of the transformer layer between the two views. This updated output serves as input to the next block of encoder layer, and the process repeats till the last layer. For example, the output of the transformer layer is <math id="S2.SS2.SSS1.p2.2.m2.5" class="ltx_Math" alttext="H_{m}=h_{1},h_{2},...,h_{t-1},h_{t}" display="inline"><semantics id="S2.SS2.SSS1.p2.2.m2.5a"><mrow id="S2.SS2.SSS1.p2.2.m2.5.5" xref="S2.SS2.SSS1.p2.2.m2.5.5.cmml"><msub id="S2.SS2.SSS1.p2.2.m2.5.5.6" xref="S2.SS2.SSS1.p2.2.m2.5.5.6.cmml"><mi id="S2.SS2.SSS1.p2.2.m2.5.5.6.2" xref="S2.SS2.SSS1.p2.2.m2.5.5.6.2.cmml">H</mi><mi id="S2.SS2.SSS1.p2.2.m2.5.5.6.3" xref="S2.SS2.SSS1.p2.2.m2.5.5.6.3.cmml">m</mi></msub><mo id="S2.SS2.SSS1.p2.2.m2.5.5.5" xref="S2.SS2.SSS1.p2.2.m2.5.5.5.cmml">=</mo><mrow id="S2.SS2.SSS1.p2.2.m2.5.5.4.4" xref="S2.SS2.SSS1.p2.2.m2.5.5.4.5.cmml"><msub id="S2.SS2.SSS1.p2.2.m2.2.2.1.1.1" xref="S2.SS2.SSS1.p2.2.m2.2.2.1.1.1.cmml"><mi id="S2.SS2.SSS1.p2.2.m2.2.2.1.1.1.2" xref="S2.SS2.SSS1.p2.2.m2.2.2.1.1.1.2.cmml">h</mi><mn id="S2.SS2.SSS1.p2.2.m2.2.2.1.1.1.3" xref="S2.SS2.SSS1.p2.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS2.SSS1.p2.2.m2.5.5.4.4.5" xref="S2.SS2.SSS1.p2.2.m2.5.5.4.5.cmml">,</mo><msub id="S2.SS2.SSS1.p2.2.m2.3.3.2.2.2" xref="S2.SS2.SSS1.p2.2.m2.3.3.2.2.2.cmml"><mi id="S2.SS2.SSS1.p2.2.m2.3.3.2.2.2.2" xref="S2.SS2.SSS1.p2.2.m2.3.3.2.2.2.2.cmml">h</mi><mn id="S2.SS2.SSS1.p2.2.m2.3.3.2.2.2.3" xref="S2.SS2.SSS1.p2.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS2.SSS1.p2.2.m2.5.5.4.4.6" xref="S2.SS2.SSS1.p2.2.m2.5.5.4.5.cmml">,</mo><mi mathvariant="normal" id="S2.SS2.SSS1.p2.2.m2.1.1" xref="S2.SS2.SSS1.p2.2.m2.1.1.cmml">â€¦</mi><mo id="S2.SS2.SSS1.p2.2.m2.5.5.4.4.7" xref="S2.SS2.SSS1.p2.2.m2.5.5.4.5.cmml">,</mo><msub id="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3" xref="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.cmml"><mi id="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.2" xref="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.2.cmml">h</mi><mrow id="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.3" xref="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.3.cmml"><mi id="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.3.2" xref="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.3.2.cmml">t</mi><mo id="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.3.1" xref="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.3.1.cmml">âˆ’</mo><mn id="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.3.3" xref="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.3.3.cmml">1</mn></mrow></msub><mo id="S2.SS2.SSS1.p2.2.m2.5.5.4.4.8" xref="S2.SS2.SSS1.p2.2.m2.5.5.4.5.cmml">,</mo><msub id="S2.SS2.SSS1.p2.2.m2.5.5.4.4.4" xref="S2.SS2.SSS1.p2.2.m2.5.5.4.4.4.cmml"><mi id="S2.SS2.SSS1.p2.2.m2.5.5.4.4.4.2" xref="S2.SS2.SSS1.p2.2.m2.5.5.4.4.4.2.cmml">h</mi><mi id="S2.SS2.SSS1.p2.2.m2.5.5.4.4.4.3" xref="S2.SS2.SSS1.p2.2.m2.5.5.4.4.4.3.cmml">t</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.2.m2.5b"><apply id="S2.SS2.SSS1.p2.2.m2.5.5.cmml" xref="S2.SS2.SSS1.p2.2.m2.5.5"><eq id="S2.SS2.SSS1.p2.2.m2.5.5.5.cmml" xref="S2.SS2.SSS1.p2.2.m2.5.5.5"></eq><apply id="S2.SS2.SSS1.p2.2.m2.5.5.6.cmml" xref="S2.SS2.SSS1.p2.2.m2.5.5.6"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.2.m2.5.5.6.1.cmml" xref="S2.SS2.SSS1.p2.2.m2.5.5.6">subscript</csymbol><ci id="S2.SS2.SSS1.p2.2.m2.5.5.6.2.cmml" xref="S2.SS2.SSS1.p2.2.m2.5.5.6.2">ğ»</ci><ci id="S2.SS2.SSS1.p2.2.m2.5.5.6.3.cmml" xref="S2.SS2.SSS1.p2.2.m2.5.5.6.3">ğ‘š</ci></apply><list id="S2.SS2.SSS1.p2.2.m2.5.5.4.5.cmml" xref="S2.SS2.SSS1.p2.2.m2.5.5.4.4"><apply id="S2.SS2.SSS1.p2.2.m2.2.2.1.1.1.cmml" xref="S2.SS2.SSS1.p2.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.2.m2.2.2.1.1.1.1.cmml" xref="S2.SS2.SSS1.p2.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p2.2.m2.2.2.1.1.1.2.cmml" xref="S2.SS2.SSS1.p2.2.m2.2.2.1.1.1.2">â„</ci><cn type="integer" id="S2.SS2.SSS1.p2.2.m2.2.2.1.1.1.3.cmml" xref="S2.SS2.SSS1.p2.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS2.SSS1.p2.2.m2.3.3.2.2.2.cmml" xref="S2.SS2.SSS1.p2.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.2.m2.3.3.2.2.2.1.cmml" xref="S2.SS2.SSS1.p2.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S2.SS2.SSS1.p2.2.m2.3.3.2.2.2.2.cmml" xref="S2.SS2.SSS1.p2.2.m2.3.3.2.2.2.2">â„</ci><cn type="integer" id="S2.SS2.SSS1.p2.2.m2.3.3.2.2.2.3.cmml" xref="S2.SS2.SSS1.p2.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.1">â€¦</ci><apply id="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.cmml" xref="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.1.cmml" xref="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.2.cmml" xref="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.2">â„</ci><apply id="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.3.cmml" xref="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.3"><minus id="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.3.1.cmml" xref="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.3.1"></minus><ci id="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.3.2.cmml" xref="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.3.2">ğ‘¡</ci><cn type="integer" id="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.3.3.cmml" xref="S2.SS2.SSS1.p2.2.m2.4.4.3.3.3.3.3">1</cn></apply></apply><apply id="S2.SS2.SSS1.p2.2.m2.5.5.4.4.4.cmml" xref="S2.SS2.SSS1.p2.2.m2.5.5.4.4.4"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.2.m2.5.5.4.4.4.1.cmml" xref="S2.SS2.SSS1.p2.2.m2.5.5.4.4.4">subscript</csymbol><ci id="S2.SS2.SSS1.p2.2.m2.5.5.4.4.4.2.cmml" xref="S2.SS2.SSS1.p2.2.m2.5.5.4.4.4.2">â„</ci><ci id="S2.SS2.SSS1.p2.2.m2.5.5.4.4.4.3.cmml" xref="S2.SS2.SSS1.p2.2.m2.5.5.4.4.4.3">ğ‘¡</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.2.m2.5c">H_{m}=h_{1},h_{2},...,h_{t-1},h_{t}</annotation></semantics></math> and <math id="S2.SS2.SSS1.p2.3.m3.5" class="ltx_Math" alttext="H^{c}=h_{1}^{c},h_{2}^{c},...,h_{t-1}^{c},h_{t}^{c}" display="inline"><semantics id="S2.SS2.SSS1.p2.3.m3.5a"><mrow id="S2.SS2.SSS1.p2.3.m3.5.5" xref="S2.SS2.SSS1.p2.3.m3.5.5.cmml"><msup id="S2.SS2.SSS1.p2.3.m3.5.5.6" xref="S2.SS2.SSS1.p2.3.m3.5.5.6.cmml"><mi id="S2.SS2.SSS1.p2.3.m3.5.5.6.2" xref="S2.SS2.SSS1.p2.3.m3.5.5.6.2.cmml">H</mi><mi id="S2.SS2.SSS1.p2.3.m3.5.5.6.3" xref="S2.SS2.SSS1.p2.3.m3.5.5.6.3.cmml">c</mi></msup><mo id="S2.SS2.SSS1.p2.3.m3.5.5.5" xref="S2.SS2.SSS1.p2.3.m3.5.5.5.cmml">=</mo><mrow id="S2.SS2.SSS1.p2.3.m3.5.5.4.4" xref="S2.SS2.SSS1.p2.3.m3.5.5.4.5.cmml"><msubsup id="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1" xref="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1.cmml"><mi id="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1.2.2" xref="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1.2.2.cmml">h</mi><mn id="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1.2.3" xref="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1.2.3.cmml">1</mn><mi id="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1.3" xref="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1.3.cmml">c</mi></msubsup><mo id="S2.SS2.SSS1.p2.3.m3.5.5.4.4.5" xref="S2.SS2.SSS1.p2.3.m3.5.5.4.5.cmml">,</mo><msubsup id="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2" xref="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2.cmml"><mi id="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2.2.2" xref="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2.2.2.cmml">h</mi><mn id="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2.2.3" xref="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2.2.3.cmml">2</mn><mi id="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2.3" xref="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2.3.cmml">c</mi></msubsup><mo id="S2.SS2.SSS1.p2.3.m3.5.5.4.4.6" xref="S2.SS2.SSS1.p2.3.m3.5.5.4.5.cmml">,</mo><mi mathvariant="normal" id="S2.SS2.SSS1.p2.3.m3.1.1" xref="S2.SS2.SSS1.p2.3.m3.1.1.cmml">â€¦</mi><mo id="S2.SS2.SSS1.p2.3.m3.5.5.4.4.7" xref="S2.SS2.SSS1.p2.3.m3.5.5.4.5.cmml">,</mo><msubsup id="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3" xref="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.cmml"><mi id="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.2" xref="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.2.cmml">h</mi><mrow id="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.3" xref="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.3.cmml"><mi id="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.3.2" xref="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.3.2.cmml">t</mi><mo id="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.3.1" xref="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.3.1.cmml">âˆ’</mo><mn id="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.3.3" xref="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.3.3.cmml">1</mn></mrow><mi id="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.3" xref="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.3.cmml">c</mi></msubsup><mo id="S2.SS2.SSS1.p2.3.m3.5.5.4.4.8" xref="S2.SS2.SSS1.p2.3.m3.5.5.4.5.cmml">,</mo><msubsup id="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4" xref="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4.cmml"><mi id="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4.2.2" xref="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4.2.2.cmml">h</mi><mi id="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4.2.3" xref="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4.2.3.cmml">t</mi><mi id="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4.3" xref="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4.3.cmml">c</mi></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.3.m3.5b"><apply id="S2.SS2.SSS1.p2.3.m3.5.5.cmml" xref="S2.SS2.SSS1.p2.3.m3.5.5"><eq id="S2.SS2.SSS1.p2.3.m3.5.5.5.cmml" xref="S2.SS2.SSS1.p2.3.m3.5.5.5"></eq><apply id="S2.SS2.SSS1.p2.3.m3.5.5.6.cmml" xref="S2.SS2.SSS1.p2.3.m3.5.5.6"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.3.m3.5.5.6.1.cmml" xref="S2.SS2.SSS1.p2.3.m3.5.5.6">superscript</csymbol><ci id="S2.SS2.SSS1.p2.3.m3.5.5.6.2.cmml" xref="S2.SS2.SSS1.p2.3.m3.5.5.6.2">ğ»</ci><ci id="S2.SS2.SSS1.p2.3.m3.5.5.6.3.cmml" xref="S2.SS2.SSS1.p2.3.m3.5.5.6.3">ğ‘</ci></apply><list id="S2.SS2.SSS1.p2.3.m3.5.5.4.5.cmml" xref="S2.SS2.SSS1.p2.3.m3.5.5.4.4"><apply id="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1.cmml" xref="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1.1.cmml" xref="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1">superscript</csymbol><apply id="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1.2.cmml" xref="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1.2.1.cmml" xref="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1.2.2.cmml" xref="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1.2.2">â„</ci><cn type="integer" id="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1.2.3.cmml" xref="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1.2.3">1</cn></apply><ci id="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1.3.cmml" xref="S2.SS2.SSS1.p2.3.m3.2.2.1.1.1.3">ğ‘</ci></apply><apply id="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2.cmml" xref="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2.1.cmml" xref="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2">superscript</csymbol><apply id="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2.2.cmml" xref="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2.2.1.cmml" xref="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2.2.2.cmml" xref="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2.2.2">â„</ci><cn type="integer" id="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2.2.3.cmml" xref="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2.2.3">2</cn></apply><ci id="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2.3.cmml" xref="S2.SS2.SSS1.p2.3.m3.3.3.2.2.2.3">ğ‘</ci></apply><ci id="S2.SS2.SSS1.p2.3.m3.1.1.cmml" xref="S2.SS2.SSS1.p2.3.m3.1.1">â€¦</ci><apply id="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.cmml" xref="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.1.cmml" xref="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3">superscript</csymbol><apply id="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.cmml" xref="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.1.cmml" xref="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3">subscript</csymbol><ci id="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.2.cmml" xref="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.2">â„</ci><apply id="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.3.cmml" xref="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.3"><minus id="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.3.1.cmml" xref="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.3.1"></minus><ci id="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.3.2.cmml" xref="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.3.2">ğ‘¡</ci><cn type="integer" id="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.3.3.cmml" xref="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.2.3.3">1</cn></apply></apply><ci id="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.3.cmml" xref="S2.SS2.SSS1.p2.3.m3.4.4.3.3.3.3">ğ‘</ci></apply><apply id="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4.cmml" xref="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4.1.cmml" xref="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4">superscript</csymbol><apply id="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4.2.cmml" xref="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4.2.1.cmml" xref="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4">subscript</csymbol><ci id="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4.2.2.cmml" xref="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4.2.2">â„</ci><ci id="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4.2.3.cmml" xref="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4.2.3">ğ‘¡</ci></apply><ci id="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4.3.cmml" xref="S2.SS2.SSS1.p2.3.m3.5.5.4.4.4.3">ğ‘</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.3.m3.5c">H^{c}=h_{1}^{c},h_{2}^{c},...,h_{t-1}^{c},h_{t}^{c}</annotation></semantics></math> for the masked and unmasked input respectively. The outputs at the masked indices are now swapped using the swap method i.e., the updated output are <math id="S2.SS2.SSS1.p2.4.m4.5" class="ltx_Math" alttext="H^{m}=h_{1},h_{2}^{c},...,h_{t-1}^{c},h_{t}" display="inline"><semantics id="S2.SS2.SSS1.p2.4.m4.5a"><mrow id="S2.SS2.SSS1.p2.4.m4.5.5" xref="S2.SS2.SSS1.p2.4.m4.5.5.cmml"><msup id="S2.SS2.SSS1.p2.4.m4.5.5.6" xref="S2.SS2.SSS1.p2.4.m4.5.5.6.cmml"><mi id="S2.SS2.SSS1.p2.4.m4.5.5.6.2" xref="S2.SS2.SSS1.p2.4.m4.5.5.6.2.cmml">H</mi><mi id="S2.SS2.SSS1.p2.4.m4.5.5.6.3" xref="S2.SS2.SSS1.p2.4.m4.5.5.6.3.cmml">m</mi></msup><mo id="S2.SS2.SSS1.p2.4.m4.5.5.5" xref="S2.SS2.SSS1.p2.4.m4.5.5.5.cmml">=</mo><mrow id="S2.SS2.SSS1.p2.4.m4.5.5.4.4" xref="S2.SS2.SSS1.p2.4.m4.5.5.4.5.cmml"><msub id="S2.SS2.SSS1.p2.4.m4.2.2.1.1.1" xref="S2.SS2.SSS1.p2.4.m4.2.2.1.1.1.cmml"><mi id="S2.SS2.SSS1.p2.4.m4.2.2.1.1.1.2" xref="S2.SS2.SSS1.p2.4.m4.2.2.1.1.1.2.cmml">h</mi><mn id="S2.SS2.SSS1.p2.4.m4.2.2.1.1.1.3" xref="S2.SS2.SSS1.p2.4.m4.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS2.SSS1.p2.4.m4.5.5.4.4.5" xref="S2.SS2.SSS1.p2.4.m4.5.5.4.5.cmml">,</mo><msubsup id="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2" xref="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2.cmml"><mi id="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2.2.2" xref="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2.2.2.cmml">h</mi><mn id="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2.2.3" xref="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2.2.3.cmml">2</mn><mi id="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2.3" xref="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2.3.cmml">c</mi></msubsup><mo id="S2.SS2.SSS1.p2.4.m4.5.5.4.4.6" xref="S2.SS2.SSS1.p2.4.m4.5.5.4.5.cmml">,</mo><mi mathvariant="normal" id="S2.SS2.SSS1.p2.4.m4.1.1" xref="S2.SS2.SSS1.p2.4.m4.1.1.cmml">â€¦</mi><mo id="S2.SS2.SSS1.p2.4.m4.5.5.4.4.7" xref="S2.SS2.SSS1.p2.4.m4.5.5.4.5.cmml">,</mo><msubsup id="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3" xref="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.cmml"><mi id="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.2" xref="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.2.cmml">h</mi><mrow id="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.3" xref="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.3.cmml"><mi id="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.3.2" xref="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.3.2.cmml">t</mi><mo id="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.3.1" xref="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.3.1.cmml">âˆ’</mo><mn id="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.3.3" xref="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.3.3.cmml">1</mn></mrow><mi id="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.3" xref="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.3.cmml">c</mi></msubsup><mo id="S2.SS2.SSS1.p2.4.m4.5.5.4.4.8" xref="S2.SS2.SSS1.p2.4.m4.5.5.4.5.cmml">,</mo><msub id="S2.SS2.SSS1.p2.4.m4.5.5.4.4.4" xref="S2.SS2.SSS1.p2.4.m4.5.5.4.4.4.cmml"><mi id="S2.SS2.SSS1.p2.4.m4.5.5.4.4.4.2" xref="S2.SS2.SSS1.p2.4.m4.5.5.4.4.4.2.cmml">h</mi><mi id="S2.SS2.SSS1.p2.4.m4.5.5.4.4.4.3" xref="S2.SS2.SSS1.p2.4.m4.5.5.4.4.4.3.cmml">t</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.4.m4.5b"><apply id="S2.SS2.SSS1.p2.4.m4.5.5.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5"><eq id="S2.SS2.SSS1.p2.4.m4.5.5.5.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5.5"></eq><apply id="S2.SS2.SSS1.p2.4.m4.5.5.6.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5.6"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.4.m4.5.5.6.1.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5.6">superscript</csymbol><ci id="S2.SS2.SSS1.p2.4.m4.5.5.6.2.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5.6.2">ğ»</ci><ci id="S2.SS2.SSS1.p2.4.m4.5.5.6.3.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5.6.3">ğ‘š</ci></apply><list id="S2.SS2.SSS1.p2.4.m4.5.5.4.5.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5.4.4"><apply id="S2.SS2.SSS1.p2.4.m4.2.2.1.1.1.cmml" xref="S2.SS2.SSS1.p2.4.m4.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.4.m4.2.2.1.1.1.1.cmml" xref="S2.SS2.SSS1.p2.4.m4.2.2.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p2.4.m4.2.2.1.1.1.2.cmml" xref="S2.SS2.SSS1.p2.4.m4.2.2.1.1.1.2">â„</ci><cn type="integer" id="S2.SS2.SSS1.p2.4.m4.2.2.1.1.1.3.cmml" xref="S2.SS2.SSS1.p2.4.m4.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2.cmml" xref="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2.1.cmml" xref="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2">superscript</csymbol><apply id="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2.2.cmml" xref="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2.2.1.cmml" xref="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2">subscript</csymbol><ci id="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2.2.2.cmml" xref="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2.2.2">â„</ci><cn type="integer" id="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2.2.3.cmml" xref="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2.2.3">2</cn></apply><ci id="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2.3.cmml" xref="S2.SS2.SSS1.p2.4.m4.3.3.2.2.2.3">ğ‘</ci></apply><ci id="S2.SS2.SSS1.p2.4.m4.1.1.cmml" xref="S2.SS2.SSS1.p2.4.m4.1.1">â€¦</ci><apply id="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.cmml" xref="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.1.cmml" xref="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3">superscript</csymbol><apply id="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.cmml" xref="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.1.cmml" xref="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3">subscript</csymbol><ci id="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.2.cmml" xref="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.2">â„</ci><apply id="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.3.cmml" xref="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.3"><minus id="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.3.1.cmml" xref="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.3.1"></minus><ci id="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.3.2.cmml" xref="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.3.2">ğ‘¡</ci><cn type="integer" id="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.3.3.cmml" xref="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.2.3.3">1</cn></apply></apply><ci id="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.3.cmml" xref="S2.SS2.SSS1.p2.4.m4.4.4.3.3.3.3">ğ‘</ci></apply><apply id="S2.SS2.SSS1.p2.4.m4.5.5.4.4.4.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5.4.4.4"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.4.m4.5.5.4.4.4.1.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5.4.4.4">subscript</csymbol><ci id="S2.SS2.SSS1.p2.4.m4.5.5.4.4.4.2.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5.4.4.4.2">â„</ci><ci id="S2.SS2.SSS1.p2.4.m4.5.5.4.4.4.3.cmml" xref="S2.SS2.SSS1.p2.4.m4.5.5.4.4.4.3">ğ‘¡</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.4.m4.5c">H^{m}=h_{1},h_{2}^{c},...,h_{t-1}^{c},h_{t}</annotation></semantics></math> and <math id="S2.SS2.SSS1.p2.5.m5.5" class="ltx_Math" alttext="H^{c}=h_{1}^{c},h_{2},...,h_{t-1},h_{t}^{c}" display="inline"><semantics id="S2.SS2.SSS1.p2.5.m5.5a"><mrow id="S2.SS2.SSS1.p2.5.m5.5.5" xref="S2.SS2.SSS1.p2.5.m5.5.5.cmml"><msup id="S2.SS2.SSS1.p2.5.m5.5.5.6" xref="S2.SS2.SSS1.p2.5.m5.5.5.6.cmml"><mi id="S2.SS2.SSS1.p2.5.m5.5.5.6.2" xref="S2.SS2.SSS1.p2.5.m5.5.5.6.2.cmml">H</mi><mi id="S2.SS2.SSS1.p2.5.m5.5.5.6.3" xref="S2.SS2.SSS1.p2.5.m5.5.5.6.3.cmml">c</mi></msup><mo id="S2.SS2.SSS1.p2.5.m5.5.5.5" xref="S2.SS2.SSS1.p2.5.m5.5.5.5.cmml">=</mo><mrow id="S2.SS2.SSS1.p2.5.m5.5.5.4.4" xref="S2.SS2.SSS1.p2.5.m5.5.5.4.5.cmml"><msubsup id="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1" xref="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1.cmml"><mi id="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1.2.2" xref="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1.2.2.cmml">h</mi><mn id="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1.2.3" xref="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1.2.3.cmml">1</mn><mi id="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1.3" xref="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1.3.cmml">c</mi></msubsup><mo id="S2.SS2.SSS1.p2.5.m5.5.5.4.4.5" xref="S2.SS2.SSS1.p2.5.m5.5.5.4.5.cmml">,</mo><msub id="S2.SS2.SSS1.p2.5.m5.3.3.2.2.2" xref="S2.SS2.SSS1.p2.5.m5.3.3.2.2.2.cmml"><mi id="S2.SS2.SSS1.p2.5.m5.3.3.2.2.2.2" xref="S2.SS2.SSS1.p2.5.m5.3.3.2.2.2.2.cmml">h</mi><mn id="S2.SS2.SSS1.p2.5.m5.3.3.2.2.2.3" xref="S2.SS2.SSS1.p2.5.m5.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS2.SSS1.p2.5.m5.5.5.4.4.6" xref="S2.SS2.SSS1.p2.5.m5.5.5.4.5.cmml">,</mo><mi mathvariant="normal" id="S2.SS2.SSS1.p2.5.m5.1.1" xref="S2.SS2.SSS1.p2.5.m5.1.1.cmml">â€¦</mi><mo id="S2.SS2.SSS1.p2.5.m5.5.5.4.4.7" xref="S2.SS2.SSS1.p2.5.m5.5.5.4.5.cmml">,</mo><msub id="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3" xref="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.cmml"><mi id="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.2" xref="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.2.cmml">h</mi><mrow id="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.3" xref="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.3.cmml"><mi id="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.3.2" xref="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.3.2.cmml">t</mi><mo id="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.3.1" xref="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.3.1.cmml">âˆ’</mo><mn id="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.3.3" xref="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.3.3.cmml">1</mn></mrow></msub><mo id="S2.SS2.SSS1.p2.5.m5.5.5.4.4.8" xref="S2.SS2.SSS1.p2.5.m5.5.5.4.5.cmml">,</mo><msubsup id="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4" xref="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4.cmml"><mi id="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4.2.2" xref="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4.2.2.cmml">h</mi><mi id="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4.2.3" xref="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4.2.3.cmml">t</mi><mi id="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4.3" xref="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4.3.cmml">c</mi></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.5.m5.5b"><apply id="S2.SS2.SSS1.p2.5.m5.5.5.cmml" xref="S2.SS2.SSS1.p2.5.m5.5.5"><eq id="S2.SS2.SSS1.p2.5.m5.5.5.5.cmml" xref="S2.SS2.SSS1.p2.5.m5.5.5.5"></eq><apply id="S2.SS2.SSS1.p2.5.m5.5.5.6.cmml" xref="S2.SS2.SSS1.p2.5.m5.5.5.6"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.5.m5.5.5.6.1.cmml" xref="S2.SS2.SSS1.p2.5.m5.5.5.6">superscript</csymbol><ci id="S2.SS2.SSS1.p2.5.m5.5.5.6.2.cmml" xref="S2.SS2.SSS1.p2.5.m5.5.5.6.2">ğ»</ci><ci id="S2.SS2.SSS1.p2.5.m5.5.5.6.3.cmml" xref="S2.SS2.SSS1.p2.5.m5.5.5.6.3">ğ‘</ci></apply><list id="S2.SS2.SSS1.p2.5.m5.5.5.4.5.cmml" xref="S2.SS2.SSS1.p2.5.m5.5.5.4.4"><apply id="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1.cmml" xref="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1.1.cmml" xref="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1">superscript</csymbol><apply id="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1.2.cmml" xref="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1.2.1.cmml" xref="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1.2.2.cmml" xref="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1.2.2">â„</ci><cn type="integer" id="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1.2.3.cmml" xref="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1.2.3">1</cn></apply><ci id="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1.3.cmml" xref="S2.SS2.SSS1.p2.5.m5.2.2.1.1.1.3">ğ‘</ci></apply><apply id="S2.SS2.SSS1.p2.5.m5.3.3.2.2.2.cmml" xref="S2.SS2.SSS1.p2.5.m5.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.5.m5.3.3.2.2.2.1.cmml" xref="S2.SS2.SSS1.p2.5.m5.3.3.2.2.2">subscript</csymbol><ci id="S2.SS2.SSS1.p2.5.m5.3.3.2.2.2.2.cmml" xref="S2.SS2.SSS1.p2.5.m5.3.3.2.2.2.2">â„</ci><cn type="integer" id="S2.SS2.SSS1.p2.5.m5.3.3.2.2.2.3.cmml" xref="S2.SS2.SSS1.p2.5.m5.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS2.SSS1.p2.5.m5.1.1.cmml" xref="S2.SS2.SSS1.p2.5.m5.1.1">â€¦</ci><apply id="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.cmml" xref="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.1.cmml" xref="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3">subscript</csymbol><ci id="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.2.cmml" xref="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.2">â„</ci><apply id="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.3.cmml" xref="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.3"><minus id="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.3.1.cmml" xref="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.3.1"></minus><ci id="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.3.2.cmml" xref="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.3.2">ğ‘¡</ci><cn type="integer" id="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.3.3.cmml" xref="S2.SS2.SSS1.p2.5.m5.4.4.3.3.3.3.3">1</cn></apply></apply><apply id="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4.cmml" xref="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4.1.cmml" xref="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4">superscript</csymbol><apply id="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4.2.cmml" xref="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4.2.1.cmml" xref="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4">subscript</csymbol><ci id="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4.2.2.cmml" xref="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4.2.2">â„</ci><ci id="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4.2.3.cmml" xref="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4.2.3">ğ‘¡</ci></apply><ci id="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4.3.cmml" xref="S2.SS2.SSS1.p2.5.m5.5.5.4.4.4.3">ğ‘</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.5.m5.5c">H^{c}=h_{1}^{c},h_{2},...,h_{t-1},h_{t}^{c}</annotation></semantics></math> for the masked and unmasked input, respectively.</p>
</div>
<div id="S2.SS2.SSS1.p3" class="ltx_para">
<p id="S2.SS2.SSS1.p3.1" class="ltx_p">It's important to note that there is no associated loss with the "Swap" layer. This technique indirectly encourages the model to output the same embeddings irrespective of the masked and unmasked view.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Multicluster MPL</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.2" class="ltx_p">The Multicluster MPL, inspired from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, involves the computation of masked prediction loss (MPL) across multiple layers of the transformer encoder, using multiple set of cluster centers as labels. These encoder layers are selected equidistant in between the last layer and one intermediate layer.
For instance, consider a scenario with three sets of labels as <math id="S2.SS2.SSS2.p1.1.m1.3" class="ltx_Math" alttext="(500,250,100)" display="inline"><semantics id="S2.SS2.SSS2.p1.1.m1.3a"><mrow id="S2.SS2.SSS2.p1.1.m1.3.4.2" xref="S2.SS2.SSS2.p1.1.m1.3.4.1.cmml"><mo stretchy="false" id="S2.SS2.SSS2.p1.1.m1.3.4.2.1" xref="S2.SS2.SSS2.p1.1.m1.3.4.1.cmml">(</mo><mn id="S2.SS2.SSS2.p1.1.m1.1.1" xref="S2.SS2.SSS2.p1.1.m1.1.1.cmml">500</mn><mo id="S2.SS2.SSS2.p1.1.m1.3.4.2.2" xref="S2.SS2.SSS2.p1.1.m1.3.4.1.cmml">,</mo><mn id="S2.SS2.SSS2.p1.1.m1.2.2" xref="S2.SS2.SSS2.p1.1.m1.2.2.cmml">250</mn><mo id="S2.SS2.SSS2.p1.1.m1.3.4.2.3" xref="S2.SS2.SSS2.p1.1.m1.3.4.1.cmml">,</mo><mn id="S2.SS2.SSS2.p1.1.m1.3.3" xref="S2.SS2.SSS2.p1.1.m1.3.3.cmml">100</mn><mo stretchy="false" id="S2.SS2.SSS2.p1.1.m1.3.4.2.4" xref="S2.SS2.SSS2.p1.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.1.m1.3b"><vector id="S2.SS2.SSS2.p1.1.m1.3.4.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.3.4.2"><cn type="integer" id="S2.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1">500</cn><cn type="integer" id="S2.SS2.SSS2.p1.1.m1.2.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2">250</cn><cn type="integer" id="S2.SS2.SSS2.p1.1.m1.3.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.3.3">100</cn></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.1.m1.3c">(500,250,100)</annotation></semantics></math>, where the last layer index is 12 and the intermediate layer index is 8, the multiple layers are <math id="S2.SS2.SSS2.p1.2.m2.3" class="ltx_Math" alttext="(12,10,8)" display="inline"><semantics id="S2.SS2.SSS2.p1.2.m2.3a"><mrow id="S2.SS2.SSS2.p1.2.m2.3.4.2" xref="S2.SS2.SSS2.p1.2.m2.3.4.1.cmml"><mo stretchy="false" id="S2.SS2.SSS2.p1.2.m2.3.4.2.1" xref="S2.SS2.SSS2.p1.2.m2.3.4.1.cmml">(</mo><mn id="S2.SS2.SSS2.p1.2.m2.1.1" xref="S2.SS2.SSS2.p1.2.m2.1.1.cmml">12</mn><mo id="S2.SS2.SSS2.p1.2.m2.3.4.2.2" xref="S2.SS2.SSS2.p1.2.m2.3.4.1.cmml">,</mo><mn id="S2.SS2.SSS2.p1.2.m2.2.2" xref="S2.SS2.SSS2.p1.2.m2.2.2.cmml">10</mn><mo id="S2.SS2.SSS2.p1.2.m2.3.4.2.3" xref="S2.SS2.SSS2.p1.2.m2.3.4.1.cmml">,</mo><mn id="S2.SS2.SSS2.p1.2.m2.3.3" xref="S2.SS2.SSS2.p1.2.m2.3.3.cmml">8</mn><mo stretchy="false" id="S2.SS2.SSS2.p1.2.m2.3.4.2.4" xref="S2.SS2.SSS2.p1.2.m2.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.2.m2.3b"><vector id="S2.SS2.SSS2.p1.2.m2.3.4.1.cmml" xref="S2.SS2.SSS2.p1.2.m2.3.4.2"><cn type="integer" id="S2.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS2.p1.2.m2.1.1">12</cn><cn type="integer" id="S2.SS2.SSS2.p1.2.m2.2.2.cmml" xref="S2.SS2.SSS2.p1.2.m2.2.2">10</cn><cn type="integer" id="S2.SS2.SSS2.p1.2.m2.3.3.cmml" xref="S2.SS2.SSS2.p1.2.m2.3.3">8</cn></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.2.m2.3c">(12,10,8)</annotation></semantics></math>.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.4" class="ltx_p">The Multicluster MPL is then formulated as the summation of MPL over <math id="S2.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S2.SS2.SSS2.p2.1.m1.1a"><mi id="S2.SS2.SSS2.p2.1.m1.1.1" xref="S2.SS2.SSS2.p2.1.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.1.m1.1b"><ci id="S2.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p2.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.1.m1.1c">a</annotation></semantics></math>, where <math id="S2.SS2.SSS2.p2.2.m2.9" class="ltx_Math" alttext="a={(12,500),(10,250),(8,100)}" display="inline"><semantics id="S2.SS2.SSS2.p2.2.m2.9a"><mrow id="S2.SS2.SSS2.p2.2.m2.9.9" xref="S2.SS2.SSS2.p2.2.m2.9.9.cmml"><mi id="S2.SS2.SSS2.p2.2.m2.9.9.5" xref="S2.SS2.SSS2.p2.2.m2.9.9.5.cmml">a</mi><mo id="S2.SS2.SSS2.p2.2.m2.9.9.4" xref="S2.SS2.SSS2.p2.2.m2.9.9.4.cmml">=</mo><mrow id="S2.SS2.SSS2.p2.2.m2.9.9.3.3" xref="S2.SS2.SSS2.p2.2.m2.9.9.3.4.cmml"><mrow id="S2.SS2.SSS2.p2.2.m2.7.7.1.1.1.2" xref="S2.SS2.SSS2.p2.2.m2.7.7.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.SSS2.p2.2.m2.7.7.1.1.1.2.1" xref="S2.SS2.SSS2.p2.2.m2.7.7.1.1.1.1.cmml">(</mo><mn id="S2.SS2.SSS2.p2.2.m2.1.1" xref="S2.SS2.SSS2.p2.2.m2.1.1.cmml">12</mn><mo id="S2.SS2.SSS2.p2.2.m2.7.7.1.1.1.2.2" xref="S2.SS2.SSS2.p2.2.m2.7.7.1.1.1.1.cmml">,</mo><mn id="S2.SS2.SSS2.p2.2.m2.2.2" xref="S2.SS2.SSS2.p2.2.m2.2.2.cmml">500</mn><mo stretchy="false" id="S2.SS2.SSS2.p2.2.m2.7.7.1.1.1.2.3" xref="S2.SS2.SSS2.p2.2.m2.7.7.1.1.1.1.cmml">)</mo></mrow><mo id="S2.SS2.SSS2.p2.2.m2.9.9.3.3.4" xref="S2.SS2.SSS2.p2.2.m2.9.9.3.4.cmml">,</mo><mrow id="S2.SS2.SSS2.p2.2.m2.8.8.2.2.2.2" xref="S2.SS2.SSS2.p2.2.m2.8.8.2.2.2.1.cmml"><mo stretchy="false" id="S2.SS2.SSS2.p2.2.m2.8.8.2.2.2.2.1" xref="S2.SS2.SSS2.p2.2.m2.8.8.2.2.2.1.cmml">(</mo><mn id="S2.SS2.SSS2.p2.2.m2.3.3" xref="S2.SS2.SSS2.p2.2.m2.3.3.cmml">10</mn><mo id="S2.SS2.SSS2.p2.2.m2.8.8.2.2.2.2.2" xref="S2.SS2.SSS2.p2.2.m2.8.8.2.2.2.1.cmml">,</mo><mn id="S2.SS2.SSS2.p2.2.m2.4.4" xref="S2.SS2.SSS2.p2.2.m2.4.4.cmml">250</mn><mo stretchy="false" id="S2.SS2.SSS2.p2.2.m2.8.8.2.2.2.2.3" xref="S2.SS2.SSS2.p2.2.m2.8.8.2.2.2.1.cmml">)</mo></mrow><mo id="S2.SS2.SSS2.p2.2.m2.9.9.3.3.5" xref="S2.SS2.SSS2.p2.2.m2.9.9.3.4.cmml">,</mo><mrow id="S2.SS2.SSS2.p2.2.m2.9.9.3.3.3.2" xref="S2.SS2.SSS2.p2.2.m2.9.9.3.3.3.1.cmml"><mo stretchy="false" id="S2.SS2.SSS2.p2.2.m2.9.9.3.3.3.2.1" xref="S2.SS2.SSS2.p2.2.m2.9.9.3.3.3.1.cmml">(</mo><mn id="S2.SS2.SSS2.p2.2.m2.5.5" xref="S2.SS2.SSS2.p2.2.m2.5.5.cmml">8</mn><mo id="S2.SS2.SSS2.p2.2.m2.9.9.3.3.3.2.2" xref="S2.SS2.SSS2.p2.2.m2.9.9.3.3.3.1.cmml">,</mo><mn id="S2.SS2.SSS2.p2.2.m2.6.6" xref="S2.SS2.SSS2.p2.2.m2.6.6.cmml">100</mn><mo stretchy="false" id="S2.SS2.SSS2.p2.2.m2.9.9.3.3.3.2.3" xref="S2.SS2.SSS2.p2.2.m2.9.9.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.2.m2.9b"><apply id="S2.SS2.SSS2.p2.2.m2.9.9.cmml" xref="S2.SS2.SSS2.p2.2.m2.9.9"><eq id="S2.SS2.SSS2.p2.2.m2.9.9.4.cmml" xref="S2.SS2.SSS2.p2.2.m2.9.9.4"></eq><ci id="S2.SS2.SSS2.p2.2.m2.9.9.5.cmml" xref="S2.SS2.SSS2.p2.2.m2.9.9.5">ğ‘</ci><list id="S2.SS2.SSS2.p2.2.m2.9.9.3.4.cmml" xref="S2.SS2.SSS2.p2.2.m2.9.9.3.3"><interval closure="open" id="S2.SS2.SSS2.p2.2.m2.7.7.1.1.1.1.cmml" xref="S2.SS2.SSS2.p2.2.m2.7.7.1.1.1.2"><cn type="integer" id="S2.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S2.SS2.SSS2.p2.2.m2.1.1">12</cn><cn type="integer" id="S2.SS2.SSS2.p2.2.m2.2.2.cmml" xref="S2.SS2.SSS2.p2.2.m2.2.2">500</cn></interval><interval closure="open" id="S2.SS2.SSS2.p2.2.m2.8.8.2.2.2.1.cmml" xref="S2.SS2.SSS2.p2.2.m2.8.8.2.2.2.2"><cn type="integer" id="S2.SS2.SSS2.p2.2.m2.3.3.cmml" xref="S2.SS2.SSS2.p2.2.m2.3.3">10</cn><cn type="integer" id="S2.SS2.SSS2.p2.2.m2.4.4.cmml" xref="S2.SS2.SSS2.p2.2.m2.4.4">250</cn></interval><interval closure="open" id="S2.SS2.SSS2.p2.2.m2.9.9.3.3.3.1.cmml" xref="S2.SS2.SSS2.p2.2.m2.9.9.3.3.3.2"><cn type="integer" id="S2.SS2.SSS2.p2.2.m2.5.5.cmml" xref="S2.SS2.SSS2.p2.2.m2.5.5">8</cn><cn type="integer" id="S2.SS2.SSS2.p2.2.m2.6.6.cmml" xref="S2.SS2.SSS2.p2.2.m2.6.6">100</cn></interval></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.2.m2.9c">a={(12,500),(10,250),(8,100)}</annotation></semantics></math> is a dictionary of which label set to use with which transformer encoder layer <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>In the original paper <math id="footnote1.m1.1" class="ltx_Math" alttext="a" display="inline"><semantics id="footnote1.m1.1b"><mi id="footnote1.m1.1.1" xref="footnote1.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="footnote1.m1.1c"><ci id="footnote1.m1.1.1.cmml" xref="footnote1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m1.1d">a</annotation></semantics></math> would be calculated in reverse order i.e., <math id="footnote1.m2.9" class="ltx_Math" alttext="{(8,500),(10,250),(12,100)}" display="inline"><semantics id="footnote1.m2.9b"><mrow id="footnote1.m2.9.9.3" xref="footnote1.m2.9.9.4.cmml"><mrow id="footnote1.m2.7.7.1.1.2" xref="footnote1.m2.7.7.1.1.1.cmml"><mo stretchy="false" id="footnote1.m2.7.7.1.1.2.1" xref="footnote1.m2.7.7.1.1.1.cmml">(</mo><mn id="footnote1.m2.1.1" xref="footnote1.m2.1.1.cmml">8</mn><mo id="footnote1.m2.7.7.1.1.2.2" xref="footnote1.m2.7.7.1.1.1.cmml">,</mo><mn id="footnote1.m2.2.2" xref="footnote1.m2.2.2.cmml">500</mn><mo stretchy="false" id="footnote1.m2.7.7.1.1.2.3" xref="footnote1.m2.7.7.1.1.1.cmml">)</mo></mrow><mo id="footnote1.m2.9.9.3.4" xref="footnote1.m2.9.9.4.cmml">,</mo><mrow id="footnote1.m2.8.8.2.2.2" xref="footnote1.m2.8.8.2.2.1.cmml"><mo stretchy="false" id="footnote1.m2.8.8.2.2.2.1" xref="footnote1.m2.8.8.2.2.1.cmml">(</mo><mn id="footnote1.m2.3.3" xref="footnote1.m2.3.3.cmml">10</mn><mo id="footnote1.m2.8.8.2.2.2.2" xref="footnote1.m2.8.8.2.2.1.cmml">,</mo><mn id="footnote1.m2.4.4" xref="footnote1.m2.4.4.cmml">250</mn><mo stretchy="false" id="footnote1.m2.8.8.2.2.2.3" xref="footnote1.m2.8.8.2.2.1.cmml">)</mo></mrow><mo id="footnote1.m2.9.9.3.5" xref="footnote1.m2.9.9.4.cmml">,</mo><mrow id="footnote1.m2.9.9.3.3.2" xref="footnote1.m2.9.9.3.3.1.cmml"><mo stretchy="false" id="footnote1.m2.9.9.3.3.2.1" xref="footnote1.m2.9.9.3.3.1.cmml">(</mo><mn id="footnote1.m2.5.5" xref="footnote1.m2.5.5.cmml">12</mn><mo id="footnote1.m2.9.9.3.3.2.2" xref="footnote1.m2.9.9.3.3.1.cmml">,</mo><mn id="footnote1.m2.6.6" xref="footnote1.m2.6.6.cmml">100</mn><mo stretchy="false" id="footnote1.m2.9.9.3.3.2.3" xref="footnote1.m2.9.9.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="footnote1.m2.9c"><list id="footnote1.m2.9.9.4.cmml" xref="footnote1.m2.9.9.3"><interval closure="open" id="footnote1.m2.7.7.1.1.1.cmml" xref="footnote1.m2.7.7.1.1.2"><cn type="integer" id="footnote1.m2.1.1.cmml" xref="footnote1.m2.1.1">8</cn><cn type="integer" id="footnote1.m2.2.2.cmml" xref="footnote1.m2.2.2">500</cn></interval><interval closure="open" id="footnote1.m2.8.8.2.2.1.cmml" xref="footnote1.m2.8.8.2.2.2"><cn type="integer" id="footnote1.m2.3.3.cmml" xref="footnote1.m2.3.3">10</cn><cn type="integer" id="footnote1.m2.4.4.cmml" xref="footnote1.m2.4.4">250</cn></interval><interval closure="open" id="footnote1.m2.9.9.3.3.1.cmml" xref="footnote1.m2.9.9.3.3.2"><cn type="integer" id="footnote1.m2.5.5.cmml" xref="footnote1.m2.5.5">12</cn><cn type="integer" id="footnote1.m2.6.6.cmml" xref="footnote1.m2.6.6">100</cn></interval></list></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m2.9d">{(8,500),(10,250),(12,100)}</annotation></semantics></math>.</span></span></span>. MPL is computed over the masked indices only, as depicted in Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Furthermore, given the GPU memory constraints, we randomly drop <math id="S2.SS2.SSS2.p2.3.m3.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S2.SS2.SSS2.p2.3.m3.1a"><mi id="S2.SS2.SSS2.p2.3.m3.1.1" xref="S2.SS2.SSS2.p2.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.3.m3.1b"><ci id="S2.SS2.SSS2.p2.3.m3.1.1.cmml" xref="S2.SS2.SSS2.p2.3.m3.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.3.m3.1c">d</annotation></semantics></math> items from the dictionary <math id="S2.SS2.SSS2.p2.4.m4.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S2.SS2.SSS2.p2.4.m4.1a"><mi id="S2.SS2.SSS2.p2.4.m4.1.1" xref="S2.SS2.SSS2.p2.4.m4.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.4.m4.1b"><ci id="S2.SS2.SSS2.p2.4.m4.1.1.cmml" xref="S2.SS2.SSS2.p2.4.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.4.m4.1c">a</annotation></semantics></math> for every forward pass.</p>
<p id="S2.SS2.SSS2.p2.5.1" class="ltx_p ltx_align_center"><math id="S2.SS2.SSS2.p2.5.1.m1.1" class="ltx_Math" alttext="Multicluster\ loss=\sum_{a}(MPL)" display="inline"><semantics id="S2.SS2.SSS2.p2.5.1.m1.1a"><mrow id="S2.SS2.SSS2.p2.5.1.m1.1.1" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.cmml"><mrow id="S2.SS2.SSS2.p2.5.1.m1.1.1.3" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.cmml"><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.2" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.3" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1a" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.4" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1b" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.5" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1c" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.6" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1d" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.7" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.7.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1e" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.8" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.8.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1f" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.9" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.9.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1g" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.10" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.10.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1h" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.11" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.11.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1i" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.12" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.12.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1j" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.13" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.13.cmml">r</mi><mo lspace="0.500em" rspace="0em" id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1k" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.14" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.14.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1l" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.15" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.15.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1m" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.16" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.16.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1n" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.17" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.17.cmml">s</mi></mrow><mo rspace="0.111em" id="S2.SS2.SSS2.p2.5.1.m1.1.1.2" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.2.cmml">=</mo><mrow id="S2.SS2.SSS2.p2.5.1.m1.1.1.1" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.cmml"><msub id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.2" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.2.cmml"><mo rspace="0em" id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.2.2" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.2.2.cmml">âˆ‘</mo><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.2.3" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.2.3.cmml">a</mi></msub><mrow id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.2" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.cmml"><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.2" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.1" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.3" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.1a" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.4" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.4.cmml">L</mi></mrow><mo stretchy="false" id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.3" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.5.1.m1.1b"><apply id="S2.SS2.SSS2.p2.5.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1"><eq id="S2.SS2.SSS2.p2.5.1.m1.1.1.2.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.2"></eq><apply id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3"><times id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.1"></times><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.2.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.2">ğ‘€</ci><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.3.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.3">ğ‘¢</ci><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.4.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.4">ğ‘™</ci><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.5.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.5">ğ‘¡</ci><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.6.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.6">ğ‘–</ci><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.7.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.7">ğ‘</ci><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.8.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.8">ğ‘™</ci><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.9.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.9">ğ‘¢</ci><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.10.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.10">ğ‘ </ci><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.11.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.11">ğ‘¡</ci><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.12.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.12">ğ‘’</ci><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.13.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.13">ğ‘Ÿ</ci><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.14.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.14">ğ‘™</ci><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.15.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.15">ğ‘œ</ci><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.16.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.16">ğ‘ </ci><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.3.17.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.3.17">ğ‘ </ci></apply><apply id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1"><apply id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.2.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.2.1.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.2">subscript</csymbol><sum id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.2.2.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.2.2"></sum><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.2.3.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.2.3">ğ‘</ci></apply><apply id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1"><times id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.1.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.1"></times><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.2.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.2">ğ‘€</ci><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.3.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.3">ğ‘ƒ</ci><ci id="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.4.cmml" xref="S2.SS2.SSS2.p2.5.1.m1.1.1.1.1.1.1.4">ğ¿</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.5.1.m1.1c">Multicluster\ loss=\sum_{a}(MPL)</annotation></semantics></math>.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Details</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">For all the experiments, similar to the HuBERT base model configuration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, the MS-HuBERT model comprises a CNN encoder and 12 encoder transformer layers consisting of 768-dimensional hidden states and 8 attention heads. There is no large model used for training or comparison purposes.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Datasets</span>: The ASR Librispeech benchmark dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, which is derived from the LibriVox project, is used for pre-training and supervised finetuning purposes. It has 3 splits (i) Training, comprising train-clean-100, train-clean-360, and
train-other-500, (ii) Development including dev-other and dev-clean, and (iii) Testing consists test-other and test-clean. Each data instance comprises an audio and its corresponding transcript. For pre-training MS-HuBERT, we use only the raw audios from the combined training split resulting in a total 960 hours audios. For supervised fine-tuning, three sets of Libri-Light <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>: 1 hour, 10 hour, 100 hour and the full Librispeech 960 hours dataset is used.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">pseudo-labels</span>: Six sets of pseudo-labels with varying numbers of clusters/resolutions are generated using first iteration HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Initially, a K-means model with 1000 cluster centers is trained using latent features extracted from the 6th layer of the first iteration HuBERT base. Subsequently, another K-means model with 500 cluster centers is trained using the 1000 cluster centers as features obtained in the prior step. This process is iteratively repeated four times to train four more K-means models with 250, 125, 50, and 25 cluster centers (in that order) utilizing the cluster centers extracted from the previous step. This results in a total of 6 set of pseudo labels used to calculate the Multicluster MPL.</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">Pre-training</span>: Unlike HuBERT, MS-HuBERT base incorporates 6 classification heads instead of just 1. This is because of the Multicluster MPL. This results in a total parameter count of 96.01 million, representing an increment of around 1.25 million parameters compared to HuBERT. MS-HuBERT is trained for 400,000 iterations on 32 GPUs with a batch size of at most 87.5 seconds of audio per GPU. The best model checkpoint is determined using the dev-other subset. Pre-trained models and training configurations will be made available after the acceptance.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.2" class="ltx_p">Given the memory constraints and to avoid the out-of-memory error, we randomly drop 2 clusters, and their respective layer indices, in each gradient update step. Furthermore, the intermediate layer index is chosen using the formula : <math id="S3.p5.1.m1.1" class="ltx_Math" alttext="0.25*12" display="inline"><semantics id="S3.p5.1.m1.1a"><mrow id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml"><mn id="S3.p5.1.m1.1.1.2" xref="S3.p5.1.m1.1.1.2.cmml">0.25</mn><mo lspace="0.222em" rspace="0.222em" id="S3.p5.1.m1.1.1.1" xref="S3.p5.1.m1.1.1.1.cmml">âˆ—</mo><mn id="S3.p5.1.m1.1.1.3" xref="S3.p5.1.m1.1.1.3.cmml">12</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><apply id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1"><times id="S3.p5.1.m1.1.1.1.cmml" xref="S3.p5.1.m1.1.1.1"></times><cn type="float" id="S3.p5.1.m1.1.1.2.cmml" xref="S3.p5.1.m1.1.1.2">0.25</cn><cn type="integer" id="S3.p5.1.m1.1.1.3.cmml" xref="S3.p5.1.m1.1.1.3">12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">0.25*12</annotation></semantics></math>, where <math id="S3.p5.2.m2.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S3.p5.2.m2.1a"><mn id="S3.p5.2.m2.1.1" xref="S3.p5.2.m2.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S3.p5.2.m2.1b"><cn type="integer" id="S3.p5.2.m2.1.1.cmml" xref="S3.p5.2.m2.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.2.m2.1c">12</annotation></semantics></math> is the number of transformer encoder layers.</p>
</div>
<div id="S3.p6" class="ltx_para ltx_noindent">
<p id="S3.p6.1" class="ltx_p"><span id="S3.p6.1.1" class="ltx_text ltx_font_bold">Supervised Fine-tuning and inference</span>: We follow the Wav2Vec 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> strategy to fine-tune MS-HuBERT to minimize the Connectionist Temporal Classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> loss using 8 GPUs.
The total batch size is of 200 seconds of audio per GPU and the best model checkpoint is determined by the lowest Word Error Rate (WER) achieved on the dev-other split.
For inference, 4-gram language model (LM) is used with a beam width of 500 for dev-other, dev-clean and 1500 for test-clean and test-other.
We do a conservative hyper-parameter search for the 1 hour and 10 hour splits and fixed hyper-parameter are used for the 100 and 960 hours training splits during fine-tuning. The inference hyper-parameters are searched with Ax,
a Bayesian optimization toolkit <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/facebook/Ax</span></span></span> with a beam-width of 500 using 32 trials.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Main Results: Supervised Fine-tuning and Inference</h3>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>ASR Librispeech benchmark finetuning results using a 4-gram language model. wav2vec 2.0 and data2vec are not a direct comparison to MS-HuBERT and is shown only such that the reader has a broader picture. The readers should ignore these two models until the Discussion section <a href="#S5" title="5 Discussion â€£ MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</figcaption>
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:345.9pt;height:397.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.5pt,35.1pt) scale(0.85,0.85) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Method</th>
<td id="S4.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">dev-clean</td>
<td id="S4.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">dev-other</td>
<td id="S4.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">test-clean</td>
<td id="S4.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">test-other</td>
</tr>
<tr id="S4.T1.1.1.2.2" class="ltx_tr">
<th id="S4.T1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="5"><span id="S4.T1.1.1.2.2.1.1" class="ltx_text ltx_font_bold">1hr</span></th>
</tr>
<tr id="S4.T1.1.1.3.3" class="ltx_tr">
<th id="S4.T1.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">wav2vec 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</th>
<td id="S4.T1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">5.0</td>
<td id="S4.T1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">10.8</td>
<td id="S4.T1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">5.5</td>
<td id="S4.T1.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">11.3</td>
</tr>
<tr id="S4.T1.1.1.4.4" class="ltx_tr">
<th id="S4.T1.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</th>
<td id="S4.T1.1.1.4.4.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">5.6</td>
<td id="S4.T1.1.1.4.4.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">10.9</td>
<td id="S4.T1.1.1.4.4.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">6.1</td>
<td id="S4.T1.1.1.4.4.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">11.3</td>
</tr>
<tr id="S4.T1.1.1.5.5" class="ltx_tr">
<th id="S4.T1.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</th>
<td id="S4.T1.1.1.5.5.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="S4.T1.1.1.5.5.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="S4.T1.1.1.5.5.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">5.7</td>
<td id="S4.T1.1.1.5.5.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">10.8</td>
</tr>
<tr id="S4.T1.1.1.6.6" class="ltx_tr">
<th id="S4.T1.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">data2vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</th>
<td id="S4.T1.1.1.6.6.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">4.0</td>
<td id="S4.T1.1.1.6.6.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">8.5</td>
<td id="S4.T1.1.1.6.6.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">4.6</td>
<td id="S4.T1.1.1.6.6.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">9.1</td>
</tr>
<tr id="S4.T1.1.1.7.7" class="ltx_tr">
<th id="S4.T1.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">MS-HuBERT</th>
<td id="S4.T1.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">5.6</td>
<td id="S4.T1.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">10.9</td>
<td id="S4.T1.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">5.9</td>
<td id="S4.T1.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">11.3</td>
</tr>
<tr id="S4.T1.1.1.8.8" class="ltx_tr">
<th id="S4.T1.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">- Swap</th>
<td id="S4.T1.1.1.8.8.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">5.9</td>
<td id="S4.T1.1.1.8.8.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">11.6</td>
<td id="S4.T1.1.1.8.8.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">6.2</td>
<td id="S4.T1.1.1.8.8.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">12.3</td>
</tr>
<tr id="S4.T1.1.1.9.9" class="ltx_tr">
<th id="S4.T1.1.1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="5"><span id="S4.T1.1.1.9.9.1.1" class="ltx_text ltx_font_bold">10hr</span></th>
</tr>
<tr id="S4.T1.1.1.10.10" class="ltx_tr">
<th id="S4.T1.1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">wav2vec 2.0</th>
<td id="S4.T1.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.8</td>
<td id="S4.T1.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">9.1</td>
<td id="S4.T1.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">4.3</td>
<td id="S4.T1.1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">9.5</td>
</tr>
<tr id="S4.T1.1.1.11.11" class="ltx_tr">
<th id="S4.T1.1.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">HuBERT</th>
<td id="S4.T1.1.1.11.11.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.9</td>
<td id="S4.T1.1.1.11.11.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">9.0</td>
<td id="S4.T1.1.1.11.11.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">4.3</td>
<td id="S4.T1.1.1.11.11.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">9.4</td>
</tr>
<tr id="S4.T1.1.1.12.12" class="ltx_tr">
<th id="S4.T1.1.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">WavLM</th>
<td id="S4.T1.1.1.12.12.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="S4.T1.1.1.12.12.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="S4.T1.1.1.12.12.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">4.3</td>
<td id="S4.T1.1.1.12.12.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">9.2</td>
</tr>
<tr id="S4.T1.1.1.13.13" class="ltx_tr">
<th id="S4.T1.1.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">data2vec</th>
<td id="S4.T1.1.1.13.13.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.3</td>
<td id="S4.T1.1.1.13.13.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">7.5</td>
<td id="S4.T1.1.1.13.13.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.9</td>
<td id="S4.T1.1.1.13.13.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">8.1</td>
</tr>
<tr id="S4.T1.1.1.14.14" class="ltx_tr">
<th id="S4.T1.1.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">MS-HuBERT</th>
<td id="S4.T1.1.1.14.14.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.6</td>
<td id="S4.T1.1.1.14.14.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">8.5</td>
<td id="S4.T1.1.1.14.14.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">4.1</td>
<td id="S4.T1.1.1.14.14.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">8.8</td>
</tr>
<tr id="S4.T1.1.1.15.15" class="ltx_tr">
<th id="S4.T1.1.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">- Swap</th>
<td id="S4.T1.1.1.15.15.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.8</td>
<td id="S4.T1.1.1.15.15.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">8.6</td>
<td id="S4.T1.1.1.15.15.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">4.1</td>
<td id="S4.T1.1.1.15.15.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">9.2</td>
</tr>
<tr id="S4.T1.1.1.16.16" class="ltx_tr">
<th id="S4.T1.1.1.16.16.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="5"><span id="S4.T1.1.1.16.16.1.1" class="ltx_text ltx_font_bold">100hr</span></th>
</tr>
<tr id="S4.T1.1.1.17.17" class="ltx_tr">
<th id="S4.T1.1.1.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">wav2vec 2.0</th>
<td id="S4.T1.1.1.17.17.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.7</td>
<td id="S4.T1.1.1.17.17.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">7.9</td>
<td id="S4.T1.1.1.17.17.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.4</td>
<td id="S4.T1.1.1.17.17.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">8.0</td>
</tr>
<tr id="S4.T1.1.1.18.18" class="ltx_tr">
<th id="S4.T1.1.1.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">HuBERT</th>
<td id="S4.T1.1.1.18.18.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.7</td>
<td id="S4.T1.1.1.18.18.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">7.8</td>
<td id="S4.T1.1.1.18.18.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.4</td>
<td id="S4.T1.1.1.18.18.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">8.1</td>
</tr>
<tr id="S4.T1.1.1.19.19" class="ltx_tr">
<th id="S4.T1.1.1.19.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">WavLM</th>
<td id="S4.T1.1.1.19.19.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="S4.T1.1.1.19.19.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="S4.T1.1.1.19.19.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.4</td>
<td id="S4.T1.1.1.19.19.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">7.7</td>
</tr>
<tr id="S4.T1.1.1.20.20" class="ltx_tr">
<th id="S4.T1.1.1.20.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">data2vec</th>
<td id="S4.T1.1.1.20.20.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.2</td>
<td id="S4.T1.1.1.20.20.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">6.4</td>
<td id="S4.T1.1.1.20.20.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.8</td>
<td id="S4.T1.1.1.20.20.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">6.8</td>
</tr>
<tr id="S4.T1.1.1.21.21" class="ltx_tr">
<th id="S4.T1.1.1.21.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">MS-HuBERT</th>
<td id="S4.T1.1.1.21.21.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.4</td>
<td id="S4.T1.1.1.21.21.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">7.1</td>
<td id="S4.T1.1.1.21.21.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.0</td>
<td id="S4.T1.1.1.21.21.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">7.2</td>
</tr>
<tr id="S4.T1.1.1.22.22" class="ltx_tr">
<th id="S4.T1.1.1.22.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">- Swap</th>
<td id="S4.T1.1.1.22.22.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.6</td>
<td id="S4.T1.1.1.22.22.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">6.9</td>
<td id="S4.T1.1.1.22.22.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.1</td>
<td id="S4.T1.1.1.22.22.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">7.4</td>
</tr>
<tr id="S4.T1.1.1.23.23" class="ltx_tr">
<th id="S4.T1.1.1.23.23.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="5"><span id="S4.T1.1.1.23.23.1.1" class="ltx_text ltx_font_bold">960hr</span></th>
</tr>
<tr id="S4.T1.1.1.24.24" class="ltx_tr">
<th id="S4.T1.1.1.24.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">wav2vec 2.0</th>
<td id="S4.T1.1.1.24.24.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.0</td>
<td id="S4.T1.1.1.24.24.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">5.9</td>
<td id="S4.T1.1.1.24.24.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.6</td>
<td id="S4.T1.1.1.24.24.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">6.1</td>
</tr>
<tr id="S4.T1.1.1.25.25" class="ltx_tr">
<th id="S4.T1.1.1.25.25.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">data2vec</th>
<td id="S4.T1.1.1.25.25.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="S4.T1.1.1.25.25.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="S4.T1.1.1.25.25.4" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="S4.T1.1.1.25.25.5" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">5.5</td>
</tr>
<tr id="S4.T1.1.1.26.26" class="ltx_tr">
<th id="S4.T1.1.1.26.26.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">MS-HuBERT</th>
<td id="S4.T1.1.1.26.26.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.8</td>
<td id="S4.T1.1.1.26.26.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">5.1</td>
<td id="S4.T1.1.1.26.26.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.4</td>
<td id="S4.T1.1.1.26.26.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">5.5</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 â€£ 4.1 Main Results: Supervised Fine-tuning and Inference â€£ 4 Results â€£ MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> presents the outcomes on the Librispeech ASR benchmark, where MS-HuBERT is compared with two similar approaches, HuBERT and WavLM. It is evident that MS-HuBERT yields superior results. The margin of improvement increase and the size of dataset used for fine-tuning has a direct proportionality. <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">This is a desired property of any training framework i.e., as the dataset increase the performance should increase.</span></p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Notably, upon the removal of the Swap concept, we observed a degradation in performance, particularly in low-resource settings. This proves that the Swap method does indeed contribute positively to the performance gains particularly in low resource setting.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>MS-HuBERT as a Feature Extractor</h3>

<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>SUPERB fine-tuning results. </figcaption>
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:254.8pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Method</th>
<th id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">PR</th>
<th id="S4.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">ASR</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.2.1" class="ltx_tr">
<th id="S4.T2.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">wav2vec 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</th>
<td id="S4.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">5.74</td>
<td id="S4.T2.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">6.43</td>
</tr>
<tr id="S4.T2.1.1.3.2" class="ltx_tr">
<th id="S4.T2.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">HuBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</th>
<td id="S4.T2.1.1.3.2.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">5.41</td>
<td id="S4.T2.1.1.3.2.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">6.42</td>
</tr>
<tr id="S4.T2.1.1.4.3" class="ltx_tr">
<th id="S4.T2.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">WavLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</th>
<td id="S4.T2.1.1.4.3.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">4.84</td>
<td id="S4.T2.1.1.4.3.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">6.21</td>
</tr>
<tr id="S4.T2.1.1.5.4" class="ltx_tr">
<th id="S4.T2.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:0.5pt;padding-bottom:0.5pt;">data2vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</th>
<td id="S4.T2.1.1.5.4.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">4.69</td>
<td id="S4.T2.1.1.5.4.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">4.94</td>
</tr>
<tr id="S4.T2.1.1.6.5" class="ltx_tr">
<th id="S4.T2.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">MS-HuBERT</th>
<td id="S4.T2.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">4.42</td>
<td id="S4.T2.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">5.60</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To study the information encoded/learnt at different layers of the MS-HuBERT model and how it compares to the original HuBERT, we conduct two experiments: (i) SUPERB benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and (ii) canonical correlation analysis (CCA) similarity with word labels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">SUPERB Benchmark</span>:
The SUPERB benchmark is designed to evaluate the efficacy of a pre-trained model without fine-tuning i.e., using the frozen encoder as a feature extractor. Specifically, a linear weighted sum of the output embeddings of all the encoder layers serves as a feature for solving any particular downstream task. In our study, we aim to assess the quality of 2nd encoder embeddings, from the MS-HuBERT, for tackling the speech recognition task. Thus, we employ the ASR and phoneme-recognition (PR) tasks within the SUPERB benchmark. The evaluation is on the clean split of the ASR Librispeech benchmark. The results are reported in Table <a href="#S4.T2" title="Table 2 â€£ 4.2 MS-HuBERT as a Feature Extractor â€£ 4 Results â€£ MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Clearly MS-HuBERT surpasses HuBERT and similar models by a significant margin. This shows the model's capability in encoding information crucial in solving the ASR and PR task. Except on the ASR task using data2vec.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2406.05661/assets/img/cca/MS-HuBERT_auc.png" id="S4.F2.g1" class="ltx_graphics ltx_img_landscape" width="568" height="423" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Solid lines show the CCA similarity with the word labels. Dotted lines show the AUC area under the curve for them respectively.</figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">CCA Similarity with Word Labels</span>: Following the layer-wise analysis conducted by Pasad et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, we use a modified version of canonical correlation analysis (CCA). Specifically, a projection-weighted CCA (PWCCA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.
The plots are shown in Figure <a href="#S4.F2" title="Figure 2 â€£ 4.2 MS-HuBERT as a Feature Extractor â€£ 4 Results â€£ MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. It is clear that MS-HuBERT significantly enhances the performance of word-level information across the transformer encoder layers. Additionally, we compute the area under the curve (AUC) and observe that it consistently surpasses that of Hubert. This increases the model capacity utilization compared to HuBERT. We also observe that Swap increases the CCA similarity in the later layers, which could be the reason of performance gain in low resource settings on ASR.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In comparison to data2vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, our performance on the ASR Librispeech benchmark, as illustrated in Table <a href="#S4.T1" title="Table 1 â€£ 4.1 Main Results: Supervised Fine-tuning and Inference â€£ 4 Results â€£ MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked Language Modelling methods for learning Speech Representations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, still falls short, particularly evident in low resource scenarios. This difference may stem from the inherent nature of the MLM pre-text task utilizing discrete tokens. For instance, WER metric for HuBERT and WavLM in a 1-hour setting lag behind even wav2vec 2.0. However, as the fine-tuning dataset increases, the performance gap diminishes. When leveraging the entire 960 hours of the Librispeech dataset, our performance matches that of data2vec. On the SUPERB benchmark, for the PR task, MS-HuBERT outperforms data2vec and is comparable in the context of ASR.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Given MS-HuBERT is trained using the pseudo labels generated from the the first iteration HuBERT, we posit that training two iterations using the MS-HuBERT methodology could potentially surpass data2vec on the ASR Librispeech benchmark and ASR tasks within the SUPERB benchmark.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Complexity and Computational Cost: MS-HuBERT introduces additional complexity to the pre-training process, particularly with the incorporation of the Swap method and Multicluster loss approach. This increased complexity may result in higher computational costs increasing the total time for pre-training only. During inference MS-HuBERT and HuBERT follows the same forward pass.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Our results highlight the potential of MS-HuBERT in bridging the performance gap between HuBERT and data2vec on the ASR Librispeech benchmark and content based tasks, ASR and PR, on the SUPERB benchmark.
MS-HuBERT is aimed at mitigating the pre-training and inference mismatch in masked language modeling for learning.
Building upon the HuBERT framework, MS-HuBERT incorporates two key modifications: our proposed Swap method, enabling full context access during pre-training, and the Multicluster loss approach for more effective training.
Through empirical evaluation on the ASR Librispeech benchmark, MS-HuBERT demonstrates significant performance improvements over the original HuBERT model, achieving state-of-the-art results and matching the performance of data2vec in high-resource settings.
Future research could explore further enhancements to the MS-HuBERT methodology to avoid iterative pre-training or improving the quality pseudo labels altogether. Lastly, scaling the model size is also an open question.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>ACKNOWLEDGMENTS</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Hemant Yadav is supported by Microsoft Research India PhD Fellowship program.
Rajiv Ratn Shah is partly supported by the Infosys Center for AI, the Center of Design and New Media, and the Center of Excellence in Healthcare at IIIT Delhi.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A.Â Baevski, Y.Â Zhou, A.Â Mohamed, and M.Â Auli, ``wav2vec 2.0: A framework for self-supervised learning of speech representations,'' <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol.Â 33, pp. 12â€‰449â€“12â€‰460, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
W.-N. Hsu, B.Â Bolte, Y.-H.Â H. Tsai, K.Â Lakhotia, R.Â Salakhutdinov, and A.Â Mohamed, ``Hubert: Self-supervised speech representation learning by masked prediction of hidden units,'' <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol.Â 29, pp. 3451â€“3460, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
T.Â Brown, B.Â Mann, N.Â Ryder, M.Â Subbiah, J.Â D. Kaplan, P.Â Dhariwal, A.Â Neelakantan, P.Â Shyam, G.Â Sastry, A.Â Askell <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Language models are few-shot learners,'' <em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol.Â 33, pp. 1877â€“1901, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y.-A. Chung, W.-N. Hsu, H.Â Tang, and J.Â Glass, ``An unsupervised autoregressive model for speech representation learning,'' <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.03240</em>, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A.Â v.Â d. Oord, Y.Â Li, and O.Â Vinyals, ``Representation learning with contrastive predictive coding,'' <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.03748</em>, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S.Â Schneider, A.Â Baevski, R.Â Collobert, and M.Â Auli, ``wav2vec: Unsupervised pre-training for speech recognition,'' <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.05862</em>, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y.-A. Chung, Y.Â Zhang, W.Â Han, C.-C. Chiu, J.Â Qin, R.Â Pang, and Y.Â Wu, ``W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training,'' in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.Â Â Â IEEE, 2021, pp. 244â€“250.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A.Â Baevski, M.Â Auli, and A.Â Mohamed, ``Effectiveness of self-supervised pre-training for speech recognition,'' <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.03912</em>, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S.Â Chen, C.Â Wang, Z.Â Chen, Y.Â Wu, S.Â Liu, Z.Â Chen, J.Â Li, N.Â Kanda, T.Â Yoshioka, X.Â Xiao <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Wavlm: Large-scale self-supervised pre-training for full stack speech processing,'' <em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A.Â Baevski, W.-N. Hsu, Q.Â Xu, A.Â Babu, J.Â Gu, and M.Â Auli, ``Data2vec: A general framework for self-supervised learning in speech, vision and language,'' in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.Â Â Â PMLR, 2022, pp. 1298â€“1312.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
T.Â Chen, S.Â Kornblith, M.Â Norouzi, and G.Â Hinton, ``A simple framework for contrastive learning of visual representations,'' in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.Â Â Â PMLR, 2020, pp. 1597â€“1607.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
K.Â He, H.Â Fan, Y.Â Wu, S.Â Xie, and R.Â Girshick, ``Momentum contrast for unsupervised visual representation learning,'' in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2020, pp. 9729â€“9738.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J.-B. Grill, F.Â Strub, F.Â AltchÃ©, C.Â Tallec, P.Â Richemond, E.Â Buchatskaya, C.Â Doersch, B.Â AvilaÂ Pires, Z.Â Guo, M.Â GheshlaghiÂ Azar <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Bootstrap your own latent-a new approach to self-supervised learning,'' <em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol.Â 33, pp. 21â€‰271â€“21â€‰284, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
O.Â Ronneberger, P.Â Fischer, and T.Â Brox, ``U-net: Convolutional networks for biomedical image segmentation,'' in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Medical Image Computing and Computer-Assisted Interventionâ€“MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18</em>.Â Â Â Springer, 2015, pp. 234â€“241.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A.Â Riahi and Ã‰.Â Plourde, ``Single channel speech enhancement using u-net spiking neural networks,'' in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">2023 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)</em>.Â Â Â IEEE, 2023, pp. 111â€“116.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
C.Â Macartney and T.Â Weyde, ``Improved speech enhancement with the wave-u-net,'' <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.11307</em>, 2018.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
H.Â Yadav, S.Â Sitaram, and R.Â R. Shah, ``Analysing the masked predictive coding training criterion for pre-training a speech representation model,'' in <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2023, pp. 1â€“5.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J.Â Shi, Y.Â Tang, H.Â Inaguma, H.Â GOng, J.Â Pino, and S.Â Watanabe, ``Exploration on hubert with multiple resolutions,'' <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.01084</em>, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J.Â Shi, H.Â Inaguma, X.Â Ma, I.Â Kulikov, and A.Â Sun, ``Multi-resolution hubert: Multi-resolution speech self-supervised learning with masked unit prediction,'' <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.02720</em>, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
V.Â Panayotov, G.Â Chen, D.Â Povey, and S.Â Khudanpur, ``Librispeech: an asr corpus based on public domain audio books,'' in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)</em>.Â Â Â IEEE, 2015, pp. 5206â€“5210.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J.Â Kahn, M.Â RiviÃ¨re, W.Â Zheng, E.Â Kharitonov, Q.Â Xu, P.Â E. MazarÃ©, J.Â Karadayi, V.Â Liptchinsky, R.Â Collobert, C.Â Fuegen, T.Â Likhomanenko, G.Â Synnaeve, A.Â Joulin, A.Â Mohamed, and E.Â Dupoux, ``Libri-light: A benchmark for asr with limited or no supervision,'' in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2020, pp. 7669â€“7673, <a target="_blank" href="https://github.com/facebookresearch/libri-light" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/libri-light</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A.Â Graves, S.Â FernÃ¡ndez, F.Â Gomez, and J.Â Schmidhuber, ``Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,'' in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd international conference on Machine learning</em>, 2006, pp. 369â€“376.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S.-w. Yang, P.-H. Chi, Y.-S. Chuang, C.-I.Â J. Lai, K.Â Lakhotia, Y.Â Y. Lin, A.Â T. Liu, J.Â Shi, X.Â Chang, G.-T. Lin <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, ``Superb: Speech processing universal performance benchmark,'' <em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.01051</em>, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A.Â Pasad, B.Â Shi, and K.Â Livescu, ``Comparative layer-wise analysis of self-supervised speech models,'' in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2023, pp. 1â€“5.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
A.Â Pasad, J.-C. Chou, and K.Â Livescu, ``Layer-wise analysis of a self-supervised speech representation model,'' in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.Â Â Â IEEE, 2021, pp. 914â€“921.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A.Â Morcos, M.Â Raghu, and S.Â Bengio, ``Insights on representational similarity in neural networks with canonical correlation,'' <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol.Â 31, 2018.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:title" content="{Under review}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.05660" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.05661" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.05661">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.05661" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.05662" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Jul  6 00:32:41 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
