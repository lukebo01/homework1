<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.09215] Generating Data with Text-to-Speech and Large-Language Models for Conversational Speech Recognition</title><meta property="og:description" content="Currently, a common approach in many speech processing tasks is to leverage large scale pre-trained models by fine-tuning them on in-domain data for a particular application.
Yet obtaining even a small amount of such d…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Generating Data with Text-to-Speech and Large-Language Models for Conversational Speech Recognition">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Generating Data with Text-to-Speech and Large-Language Models for Conversational Speech Recognition">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.09215">

<!--Generated on Thu Sep  5 17:42:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\interspeechfinaltrue</span><span id="p1.2" class="ltx_ERROR undefined">\name</span>
<p id="p1.3" class="ltx_p">[affiliation=1]SamueleCornell*
<span id="p1.3.1" class="ltx_ERROR undefined">\name</span>[affiliation=2]JordanDarefsky*
<span id="p1.3.2" class="ltx_ERROR undefined">\name</span>[affiliation=2]ZhiyaoDuan
<span id="p1.3.3" class="ltx_ERROR undefined">\name</span>[affiliation=1]ShinjiWatanabe







</p>
</div>
<h1 class="ltx_title ltx_title_document">Generating Data with Text-to-Speech and Large-Language Models for Conversational Speech Recognition</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Currently, a common approach in many speech processing tasks is to leverage large scale pre-trained models by fine-tuning them on in-domain data for a particular application.
Yet obtaining even a small amount of such data can be problematic, especially for sensitive domains and conversational speech scenarios, due to both privacy issues and annotation costs. To address this, synthetic data generation using single speaker datasets has been employed. Yet, for multi-speaker cases, such an approach often requires extensive manual effort and is prone to domain mismatches.
In this work, we propose a synthetic data generation pipeline for multi-speaker conversational ASR, leveraging a large language model (LLM) for content creation and a conversational multi-speaker text-to-speech (TTS) model for speech synthesis. We conduct evaluation by fine-tuning the Whisper ASR model for telephone and distant conversational speech settings, using both in-domain data and generated synthetic data.
Our results show that the proposed method is able to significantly outperform classical multi-speaker generation approaches that use external, non-conversational speech datasets.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>generative synthetic data, multi-talker speech recognition, text-to-speech, conversational speech processing
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_note_type">footnotetext: </span>These authors contributed equally to this work.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Current robust speech processing methods are considerably data hungry. For example, state-of-the-art <span title="" class="ltx_glossaryref">automatic speech recognition (ASR)</span> systems require tens or even hundreds of thousands of hours of training data in order to achieve enough robustness in different domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
Such a vast amount of training data is leveraged either explicitly by training from scratch on a large amount of data or implicitly by fine-tuning/adapting a pre-trained ``foundation'' model that was originally trained, in a supervised or unsupervised manner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, on a large dataset.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Nevertheless, for some domains, obtaining even a small portion of in-domain supervised data for fine-tuning can be problematic due to potential privacy concerns or prohibitive expense.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">This is especially true for sensitive application scenarios, including medical, government, and law enforcement settings. Moreover, due to increasing regulatory attention, even scaling in-domain training data is potentially becoming more difficult.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Aside from privacy issues,
applications that require recordings with multiple speakers are also inherently difficult, time-consuming and costly to annotate and thus obtain in scale.
Prominent examples are meeting scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> including doctor-patient recordings, speech captioning, speech analytics and so on.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Despite the difficulties associated with obtaining data for multi-speaker scenarios, there are speech processing approaches that require multi-speaker conversational data for training. These approaches have proven to be effective as demonstrated in recent speech processing challenges <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Prominent examples are end-to-end neural diarization (EEND) and most target speaker voice activity detection (TS-VAD) approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, as well as multi-speaker ASR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Lack of annotated in-domain conversational data at scale is a significant issue for these techniques, which is only partly mitigated by leveraging foundation models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
Consequently, many of these approaches have to rely on synthetic data to increase dataset size. This is commonly achieved by artificially overlapping clips from existing datasets and adding noise and reverberation.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">While several toolkits have been proposed to ease the workload  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, creating synthetic datasets remains more art than science, as it often requires extensive hand-tuning, domain knowledge, heuristics, and significant trial and error. Crucially, this process is highly prone to the introduction of unwanted biases in the resulting dataset, leading to a performance drop due to domain mismatch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The aforementioned difficulties motivate the development of more automated, machine learning based approaches for synthetic data creation.
Several methods have in fact explored this direction, primarily focusing on improving <span title="" class="ltx_glossaryref">ASR</span> performance by leveraging synthetic data created with <span title="" class="ltx_glossaryref">text-to-speech (TTS)</span> models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> or leveraging <span title="" class="ltx_glossaryref">ASR</span> and <span title="" class="ltx_glossaryref">TTS</span> cycle-consistency during training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> for semi-supervised training.
However, these approaches focus on single-speaker scenarios and thus cannot be directly applied to domains where multi-speaker conversational <span title="" class="ltx_glossaryref">ASR</span> is required.
In parallel, recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> on speech summarization and audio captioning have shown how <span title="" class="ltx_glossaryref">large-language models (LLM)</span> s can be leveraged effectively for synthetic audio data augmentation.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">Building upon this previous research, in this work we explore using <span title="" class="ltx_glossaryref">TTS</span> models along with <span title="" class="ltx_glossaryref">LLM</span> s to generate multi-speaker conversational data.
We focus on two-speaker <span title="" class="ltx_glossaryref">ASR</span> on real-world telephone (Fisher <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>) and distant speech recognition settings (Mixer 6 Speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>) by fine-tuning Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
The contributions of this work are the following: 1) We propose a synthetic data generation pipeline for conversational ASR using LLMs for content generation and a conversational multi-speaker TTS model for speech generation; 2) We perform a systematic investigation on the use of synthetic data for training multi-speaker ASR models with three different approaches: using ``classical'' LibriSpeech based multi-speaker simulation, using a conventional <span title="" class="ltx_glossaryref">state-of-the-art (SotA)</span> <span title="" class="ltx_glossaryref">TTS</span> model, and using a recently proposed conversational <span title="" class="ltx_glossaryref">TTS</span> model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method under study</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Our approach is summarized in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Conversational TTS generation ‣ 2 Method under study ‣ Generating Data with Text-to-Speech and Large-Language Models for Conversational Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
We explore the use of a pre-trained chat-optimized LLM for creating short conversation transcripts between two participants from scratch for when in-domain conversational transcriptions are not available or would be costly to obtain.
Specifically, we use the recently released Llama 3 8B Instruct model and few-shot prompt it with 8 text-prompt examples randomly selected from a 1000-example subset of Spotify Podcasts dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> used to train Parakeet (the text data was transcribed using Whisper-D, described in  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>). That is, for each new example we want to generate, we randomly select a subset of eight text samples from our Parakeet subset to use as the few-shot prompt.
This procedure could also be used to augment existing in-domain text-only data. It could also be worth exploring fine-tuning on in-domain data instead of prompting.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">These LLM obtained transcripts are then used to generate synthesized speech through a multi-speaker <span title="" class="ltx_glossaryref">TTS</span> model. The resulting data, consisting of ground truth multi-speaker transcripts and the synthesized multi-speaker mixture can then be used for training or fine-tuning purposes, i.e. in Sec. <a href="#S4" title="4 Experiments ‣ Generating Data with Text-to-Speech and Large-Language Models for Conversational Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> for adapting Whisper to perform multi-speaker ASR.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Conversational TTS generation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The effectiveness of this approach will heavily depend on the capability of the <span title="" class="ltx_glossaryref">TTS</span> model used. While we expect <span title="" class="ltx_glossaryref">LLM</span> s will be proficient in generating conversational transcripts as shown in previous work on summarization  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, most <span title="" class="ltx_glossaryref">TTS</span> models are not capable of synthesizing multi-speaker conversational data. Although one could naively generate each speaker’s utterances independently and then stitch them together, such an approach would fail to capture real conversational speech turn-taking dynamics and para-linguistic subtleties such as changes in intonation, etc., and would therefore potentially introduce a domain mismatch in the generated audio.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Recently, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> a conversational <span title="" class="ltx_glossaryref">TTS</span> model, Parakeet, has been proposed. Parakeet’s training dataset includes  60,000 hours of Spotify Podcasts data, much of which is multi-speaker. It is therefore able to directly generate two-speaker short conversations of up to 30 seconds when given a text prompt in the style of the one in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1 Conversational TTS generation ‣ 2 Method under study ‣ Generating Data with Text-to-Speech and Large-Language Models for Conversational Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, i.e. with speaker-id related tags [S1] and [S2].
We use a diffusion version of Parakeet that, similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> autoregressively generates blocks of continuous latents using latent diffusion on each block. The autoencoder is trained to map 44,100 Hz audio to 16-channel dimensional latents, with a time downsampling factor of 1024. Each diffusion block consists of 128 (time-wise) latent vectors, which correspond to approximately three seconds of audio.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">LLM-generated transcripts and speech examples are available online<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://popcornell.github.io/SynthConvASRDemo" title="" class="ltx_ref ltx_href">popcornell.github.io/SynthConvASRDemo</a></span></span></span>.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2408.09215/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="300" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Block diagram of the proposed approach.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental setup</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Evaluation data</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In this work, we focus on two-speaker multi-speaker conversational ASR.
This focus is due to the limitations of Parakeet, whose generations tend to lose correctness as the number of unique speakers in the text prompt increases. Furthermore, we also consider scenarios with relatively high <span title="" class="ltx_glossaryref">signal-to-noise ratio (SNR)</span>; tackling more complex settings such as CHiME-6 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> requires modeling of background noise and dynamic acoustic conditions (as the participants move, reverberation can change significantly).
We thus perform our experiments using two conversational speech datasets with these characteristics: Fisher Corpus (both Part 1 and Part 2) and Mixer 6 Speech.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Fisher</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.11" class="ltx_p">Fisher consists of <math id="S3.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="11699" display="inline"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><mn id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml">11699</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.1b"><cn type="integer" id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1">11699</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.1c">11699</annotation></semantics></math> telephone conversations between two English speakers sampled at <math id="S3.SS1.SSS1.p1.2.m2.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S3.SS1.SSS1.p1.2.m2.1a"><mn id="S3.SS1.SSS1.p1.2.m2.1.1" xref="S3.SS1.SSS1.p1.2.m2.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.2.m2.1b"><cn type="integer" id="S3.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p1.2.m2.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.2.m2.1c">8</annotation></semantics></math> kHz. Each conversation is around <math id="S3.SS1.SSS1.p1.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS1.SSS1.p1.3.m3.1a"><mn id="S3.SS1.SSS1.p1.3.m3.1.1" xref="S3.SS1.SSS1.p1.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.3.m3.1b"><cn type="integer" id="S3.SS1.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p1.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.3.m3.1c">10</annotation></semantics></math> minutes long. We use the train, validation, and test split from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> (<math id="S3.SS1.SSS1.p1.4.m4.1" class="ltx_Math" alttext="11577" display="inline"><semantics id="S3.SS1.SSS1.p1.4.m4.1a"><mn id="S3.SS1.SSS1.p1.4.m4.1.1" xref="S3.SS1.SSS1.p1.4.m4.1.1.cmml">11577</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.4.m4.1b"><cn type="integer" id="S3.SS1.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p1.4.m4.1.1">11577</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.4.m4.1c">11577</annotation></semantics></math>, <math id="S3.SS1.SSS1.p1.5.m5.1" class="ltx_Math" alttext="61" display="inline"><semantics id="S3.SS1.SSS1.p1.5.m5.1a"><mn id="S3.SS1.SSS1.p1.5.m5.1.1" xref="S3.SS1.SSS1.p1.5.m5.1.1.cmml">61</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.5.m5.1b"><cn type="integer" id="S3.SS1.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS1.p1.5.m5.1.1">61</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.5.m5.1c">61</annotation></semantics></math> and <math id="S3.SS1.SSS1.p1.6.m6.1" class="ltx_Math" alttext="61" display="inline"><semantics id="S3.SS1.SSS1.p1.6.m6.1a"><mn id="S3.SS1.SSS1.p1.6.m6.1.1" xref="S3.SS1.SSS1.p1.6.m6.1.1.cmml">61</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.6.m6.1b"><cn type="integer" id="S3.SS1.SSS1.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS1.p1.6.m6.1.1">61</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.6.m6.1c">61</annotation></semantics></math> conversations of respectively <math id="S3.SS1.SSS1.p1.7.m7.1" class="ltx_Math" alttext="1960" display="inline"><semantics id="S3.SS1.SSS1.p1.7.m7.1a"><mn id="S3.SS1.SSS1.p1.7.m7.1.1" xref="S3.SS1.SSS1.p1.7.m7.1.1.cmml">1960</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.7.m7.1b"><cn type="integer" id="S3.SS1.SSS1.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS1.p1.7.m7.1.1">1960</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.7.m7.1c">1960</annotation></semantics></math> h, <math id="S3.SS1.SSS1.p1.8.m8.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S3.SS1.SSS1.p1.8.m8.1a"><mn id="S3.SS1.SSS1.p1.8.m8.1.1" xref="S3.SS1.SSS1.p1.8.m8.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.8.m8.1b"><cn type="integer" id="S3.SS1.SSS1.p1.8.m8.1.1.cmml" xref="S3.SS1.SSS1.p1.8.m8.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.8.m8.1c">7</annotation></semantics></math> h and <math id="S3.SS1.SSS1.p1.9.m9.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S3.SS1.SSS1.p1.9.m9.1a"><mn id="S3.SS1.SSS1.p1.9.m9.1.1" xref="S3.SS1.SSS1.p1.9.m9.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.9.m9.1b"><cn type="integer" id="S3.SS1.SSS1.p1.9.m9.1.1.cmml" xref="S3.SS1.SSS1.p1.9.m9.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.9.m9.1c">7</annotation></semantics></math> h).
The Fisher recordings originally separate each of the speakers into different channels;
however, since our focus is on general single-channel conversational speech processing, we mixdown the two channels to mono. We also resample the signal to <math id="S3.SS1.SSS1.p1.10.m10.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S3.SS1.SSS1.p1.10.m10.1a"><mn id="S3.SS1.SSS1.p1.10.m10.1.1" xref="S3.SS1.SSS1.p1.10.m10.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.10.m10.1b"><cn type="integer" id="S3.SS1.SSS1.p1.10.m10.1.1.cmml" xref="S3.SS1.SSS1.p1.10.m10.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.10.m10.1c">16</annotation></semantics></math> kHz as we use Whisper which was trained on <math id="S3.SS1.SSS1.p1.11.m11.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S3.SS1.SSS1.p1.11.m11.1a"><mn id="S3.SS1.SSS1.p1.11.m11.1.1" xref="S3.SS1.SSS1.p1.11.m11.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.11.m11.1b"><cn type="integer" id="S3.SS1.SSS1.p1.11.m11.1.1.cmml" xref="S3.SS1.SSS1.p1.11.m11.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.11.m11.1c">16</annotation></semantics></math> kHz data (see Sec. <a href="#S3.SS3" title="3.3 ASR System ‣ 3 Experimental setup ‣ Generating Data with Text-to-Speech and Large-Language Models for Conversational Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Mixer <math id="S3.SS1.SSS2.1.m1.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S3.SS1.SSS2.1.m1.1b"><mn id="S3.SS1.SSS2.1.m1.1.1" xref="S3.SS1.SSS2.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.1.m1.1c"><cn type="integer" id="S3.SS1.SSS2.1.m1.1.1.cmml" xref="S3.SS1.SSS2.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.1.m1.1d">6</annotation></semantics></math> Speech</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.5" class="ltx_p">As an additional scenario, we consider Mixer 6 Speech. Specifically we use the version re-annotated for the CHiME-7 challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
It consists of two-speaker interviews of approximately <math id="S3.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="15" display="inline"><semantics id="S3.SS1.SSS2.p1.1.m1.1a"><mn id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml">15</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.1b"><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1">15</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.1c">15</annotation></semantics></math> minutes (sampled at 16 kHz) recorded by 14 different far-field recording devices.
In this work we only use recordings from the tabletop microphone device (CH04).
We use the splitting from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, where full long-form annotation is only available for the development (<math id="S3.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="59" display="inline"><semantics id="S3.SS1.SSS2.p1.2.m2.1a"><mn id="S3.SS1.SSS2.p1.2.m2.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.cmml">59</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.1b"><cn type="integer" id="S3.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1">59</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.1c">59</annotation></semantics></math> interviews, <math id="S3.SS1.SSS2.p1.3.m3.1" class="ltx_Math" alttext="15" display="inline"><semantics id="S3.SS1.SSS2.p1.3.m3.1a"><mn id="S3.SS1.SSS2.p1.3.m3.1.1" xref="S3.SS1.SSS2.p1.3.m3.1.1.cmml">15</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.3.m3.1b"><cn type="integer" id="S3.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1">15</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.3.m3.1c">15</annotation></semantics></math> h) and evaluation sets (<math id="S3.SS1.SSS2.p1.4.m4.1" class="ltx_Math" alttext="23" display="inline"><semantics id="S3.SS1.SSS2.p1.4.m4.1a"><mn id="S3.SS1.SSS2.p1.4.m4.1.1" xref="S3.SS1.SSS2.p1.4.m4.1.1.cmml">23</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.4.m4.1b"><cn type="integer" id="S3.SS1.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS2.p1.4.m4.1.1">23</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.4.m4.1c">23</annotation></semantics></math> interviews, <math id="S3.SS1.SSS2.p1.5.m5.1" class="ltx_Math" alttext="13" display="inline"><semantics id="S3.SS1.SSS2.p1.5.m5.1a"><mn id="S3.SS1.SSS2.p1.5.m5.1.1" xref="S3.SS1.SSS2.p1.5.m5.1.1.cmml">13</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.5.m5.1b"><cn type="integer" id="S3.SS1.SSS2.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS2.p1.5.m5.1.1">13</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.5.m5.1c">13</annotation></semantics></math> h).
Here we further split the development set into an adaptation portion and a validation portion of respectively 2:30 h and
4 h after discarding utterance groups longer than 30 s as done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
This further split allows us to compare the use of synthetic data versus in-domain data for fine-tuning.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Baseline Methods</h3>

<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>NeMo multi-speaker simulation tool</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.4" class="ltx_p">We consider two baseline methods. The first method we consider is a ``classical'' synthetic speech generation method, where single speaker speech from one high quality speech dataset (e.g. LibriSpeech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>) is used to construct conversation-style synthetic recordings by artificially overlapping single speaker utterances and contaminating them by adding noise, artificial <span title="" class="ltx_glossaryref">room impulse response (RIR)</span> or other transforms (e.g. clipping, microphone transfer function etc.).
We make use of the <span title="" class="ltx_glossaryref">SotA</span> NeMo multi-speaker simulation tool <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> (NeMo MSS in the following).
We use LibriSpeech train-clean <math id="S3.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="360" display="inline"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mn id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml">360</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><cn type="integer" id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">360</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">360</annotation></semantics></math> and <math id="S3.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><mn id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><cn type="integer" id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">100</annotation></semantics></math> portions and generate <math id="S3.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.SS2.SSS1.p1.3.m3.1a"><mn id="S3.SS2.SSS1.p1.3.m3.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.3.m3.1b"><cn type="integer" id="S3.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.3.m3.1c">100</annotation></semantics></math> h of short conversations between two speakers of up to <math id="S3.SS2.SSS1.p1.4.m4.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S3.SS2.SSS1.p1.4.m4.1a"><mn id="S3.SS2.SSS1.p1.4.m4.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.4.m4.1b"><cn type="integer" id="S3.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.4.m4.1c">30</annotation></semantics></math> seconds in length.
For Mixer 6 Speech experiments, we additionally use the built-in <span title="" class="ltx_glossaryref">RIR</span> simulation in order to generate simulated far-field speech.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>xTTS-v2</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.3" class="ltx_p">The second baseline method we consider is the approach outlined in Section <a href="#S2" title="2 Method under study ‣ Generating Data with Text-to-Speech and Large-Language Models for Conversational Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, where a standard <span title="" class="ltx_glossaryref">TTS</span> model is used to generate the training data.
We explore this using the Coqui xTTS-v2 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> (denoted simply as xTTS in Sec. <a href="#S4" title="4 Experiments ‣ Generating Data with Text-to-Speech and Large-Language Models for Conversational Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>)
In detail, for each utterance group in the training dataset (either LLM-generated or taken from a text-only corpus) we sample two speaker ids from LibriSpeech train-clean <math id="S3.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="360" display="inline"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><mn id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml">360</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><cn type="integer" id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1">360</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">360</annotation></semantics></math> and <math id="S3.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S3.SS2.SSS2.p1.2.m2.1a"><mn id="S3.SS2.SSS2.p1.2.m2.1.1" xref="S3.SS2.SSS2.p1.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.2.m2.1b"><cn type="integer" id="S3.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.2.m2.1c">100</annotation></semantics></math> and then two corresponding LibriSpeech enrollment utterances to condition xTTS-v2 for the generated TTS id.
We then generate each utterance in the utterance group independently via xTTS-v2 and truncate excessive leading and trailing silence regions using Silero VAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. The generated audio is then resampled to <math id="S3.SS2.SSS2.p1.3.m3.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S3.SS2.SSS2.p1.3.m3.1a"><mn id="S3.SS2.SSS2.p1.3.m3.1.1" xref="S3.SS2.SSS2.p1.3.m3.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.3.m3.1b"><cn type="integer" id="S3.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.3.m3.1c">16</annotation></semantics></math> kHz and mixed together by randomly adding start time offsets based on the order of the sentences in the utterance group transcript, ensuring that utterances from the same speaker do not overlap.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span><span title="" class="ltx_glossaryref">ASR</span> System</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In our experiments, which focus on two-speaker conversational speech, we use the method proposed in  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> where Whisper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is adapted to perform multi-speaker ASR through fine-tuning with a <span title="" class="ltx_glossaryref">serialized output training (SOT)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> objective on utterance groups.
This approach aligns with common practices in the field where a model pre-trained on a large amount of data (i.e. a foundation model) is fine-tuned/adapted for a particular domain or application of interest.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.11" class="ltx_p">Compared to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, in our experiments we focus only on standard <span title="" class="ltx_glossaryref">SOT</span> without considering timestamps and use only Whisper medium.
We use low-rank adapters (LoRA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> while the rest of the model is kept frozen.
During each fine-tuning experiment a linear warm-up schedule is employed for the first <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">N</annotation></semantics></math> epoch, then the learning rate is linearly decayed over a maximum of <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mn id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><cn type="integer" id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">20</annotation></semantics></math> epochs. The <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="L^{2}" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><msup id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml">L</mi><mn id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">superscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2">𝐿</ci><cn type="integer" id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">L^{2}</annotation></semantics></math> norm of the gradients is clipped to <math id="S3.SS3.p2.4.m4.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.SS3.p2.4.m4.1a"><mn id="S3.SS3.p2.4.m4.1.1" xref="S3.SS3.p2.4.m4.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m4.1b"><cn type="integer" id="S3.SS3.p2.4.m4.1.1.cmml" xref="S3.SS3.p2.4.m4.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m4.1c">5</annotation></semantics></math>.
One LoRA adapter for each linear layer in the model (i.e. for each query, key, value and feed-forward network layer) is used. For each adapter we set the LORA rank to <math id="S3.SS3.p2.5.m5.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S3.SS3.p2.5.m5.1a"><mn id="S3.SS3.p2.5.m5.1.1" xref="S3.SS3.p2.5.m5.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m5.1b"><cn type="integer" id="S3.SS3.p2.5.m5.1.1.cmml" xref="S3.SS3.p2.5.m5.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m5.1c">64</annotation></semantics></math>, alpha to <math id="S3.SS3.p2.6.m6.1" class="ltx_Math" alttext="128" display="inline"><semantics id="S3.SS3.p2.6.m6.1a"><mn id="S3.SS3.p2.6.m6.1.1" xref="S3.SS3.p2.6.m6.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m6.1b"><cn type="integer" id="S3.SS3.p2.6.m6.1.1.cmml" xref="S3.SS3.p2.6.m6.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m6.1c">128</annotation></semantics></math>, and dropout to <math id="S3.SS3.p2.7.m7.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="S3.SS3.p2.7.m7.1a"><mn id="S3.SS3.p2.7.m7.1.1" xref="S3.SS3.p2.7.m7.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m7.1b"><cn type="float" id="S3.SS3.p2.7.m7.1.1.cmml" xref="S3.SS3.p2.7.m7.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m7.1c">0.1</annotation></semantics></math>.
In our preliminary experiments on the full Fisher training set, we found that this configuration yields the best results, even when compared to fine-tuning the entire model.
If validation loss does not improve for <math id="S3.SS3.p2.8.m8.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS3.p2.8.m8.1a"><mn id="S3.SS3.p2.8.m8.1.1" xref="S3.SS3.p2.8.m8.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.8.m8.1b"><cn type="integer" id="S3.SS3.p2.8.m8.1.1.cmml" xref="S3.SS3.p2.8.m8.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.8.m8.1c">2</annotation></semantics></math> consecutive epochs the training is stopped.
We tune the batch size, number of warm-up epochs (<math id="S3.SS3.p2.9.m9.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS3.p2.9.m9.1a"><mi id="S3.SS3.p2.9.m9.1.1" xref="S3.SS3.p2.9.m9.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.9.m9.1b"><ci id="S3.SS3.p2.9.m9.1.1.cmml" xref="S3.SS3.p2.9.m9.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.9.m9.1c">N</annotation></semantics></math>) and the value of the maximum learning rate for each set of experiments. Parakeet synthesized audio is resampled to <math id="S3.SS3.p2.10.m10.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S3.SS3.p2.10.m10.1a"><mn id="S3.SS3.p2.10.m10.1.1" xref="S3.SS3.p2.10.m10.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.10.m10.1b"><cn type="integer" id="S3.SS3.p2.10.m10.1.1.cmml" xref="S3.SS3.p2.10.m10.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.10.m10.1c">16</annotation></semantics></math> kHz in our experiments. In Fisher experiments, for all synthetic data, we use on-the-fly resampling to simulate telephone <math id="S3.SS3.p2.11.m11.1" class="ltx_Math" alttext="3400" display="inline"><semantics id="S3.SS3.p2.11.m11.1a"><mn id="S3.SS3.p2.11.m11.1.1" xref="S3.SS3.p2.11.m11.1.1.cmml">3400</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.11.m11.1b"><cn type="integer" id="S3.SS3.p2.11.m11.1.1.cmml" xref="S3.SS3.p2.11.m11.1.1">3400</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.11.m11.1c">3400</annotation></semantics></math> Hz band-limiting.

<br class="ltx_break">In Mixer 6 experiments, only for xTTS and Parakeet, we contaminate the data with reverberation using random RIRs obtained from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. This of course is less realistic than the <span title="" class="ltx_glossaryref">RIR</span> simulation used in NeMo MSS as the <span title="" class="ltx_glossaryref">RIR</span> is the same for both speakers.
We make our fine-tuning code publicly available<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/popcornell/ASRLightningFT" title="" class="ltx_ref ltx_href">github.com/popcornell/ASRLightningFT</a></span></span></span>.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Evaluation Setup</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">For each dataset, we run our experiments using the same setup as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, where oracle <span title="" class="ltx_glossaryref">voice activity detection (VAD)</span> is used and the dataset is divided into several utterance groups <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
Continuing to follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, we then perform evaluation for each utterance group independently and accumulate <span title="" class="ltx_glossaryref">word error rate (WER)</span> statistics over the whole dataset (insertions, deletions etc.).
We choose this evaluation method because we only focus on multi-speaker ASR, and an evaluation which considers the whole conversation (e.g. as in CHiME-6/7) would require a diarization component, which would add significant complexity.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">We thus consider <span title="" class="ltx_glossaryref">concatenated minimum permutation WER (cpWER)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. This is the same as <span title="" class="ltx_glossaryref">WER</span> in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, with the best permutation evaluated for each utterance group independently.
We also consider <span title="" class="ltx_glossaryref">multi-input multi-output WER (MIMO-WER)</span>, which is more tolerant than <span title="" class="ltx_glossaryref">cpWER</span> to speaker assignment errors. We use the Meeteval toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> to compute both scores.
Whisper text normalization is used both during training and scoring.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Fisher</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Fisher ‣ 4 Experiments ‣ Generating Data with Text-to-Speech and Large-Language Models for Conversational Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> we report results obtained on the Fisher test set as defined in Sec. <a href="#S3.SS1.SSS1" title="3.1.1 Fisher ‣ 3.1 Evaluation data ‣ 3 Experimental setup ‣ Generating Data with Text-to-Speech and Large-Language Models for Conversational Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1.1</span></a> with different data used for fine-tuning.
As a baseline, in the first row, we report the results with no adaptation. In the second panel, we report results on in-domain Fisher training data adaptation. We observe only a modest difference between using the full training set or a 80 h data subset, which is likely because we are leveraging a strong pre-trained model.
In the third and fourth panels, we report results obtained with synthetic data approaches.
In particular, for the two <span title="" class="ltx_glossaryref">TTS</span> approaches (xTTS and Parakeet), we consider two opposite situations: a best-case/oracle scenario where we use in-domain conversation transcriptions and another one where we suppose we have none and thus we use as input Llama-3 random generated utterance groups transcripts (LLM<sub id="S4.SS1.p1.1.1" class="ltx_sub"><span id="S4.SS1.p1.1.1.1" class="ltx_text ltx_font_italic">rnd</span></sub>) as described in Sec. <a href="#S2" title="2 Method under study ‣ Generating Data with Text-to-Speech and Large-Language Models for Conversational Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We observe that xTTS-based generation outperforms NeMo MSS when Fisher only transcriptions (Fisher) are used. When LLM generated transcriptions are used (LLM<sub id="S4.SS1.p2.1.1" class="ltx_sub"><span id="S4.SS1.p2.1.1.1" class="ltx_text ltx_font_italic">rnd</span></sub>), xTTS performance is on par/slightly worse than NeMo MSS.
In contrast, when using Parakeet, the difference between using LLM generated transcripts versus the Fisher training set transcriptions is modest, and interestingly, the generated transcripts yield the best performance.
In general, while the performance gain compared to the baseline synthetic data approaches (xTTS and NeMo MSS) is significant, there remains a substantial gap compared to using in-domain data (Fisher). It appears that this gap cannot be bridged solely by scaling the amount of synthetic data.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">In Figure <a href="#S4.F2" title="Figure 2 ‣ 4.1 Fisher ‣ 4 Experiments ‣ Generating Data with Text-to-Speech and Large-Language Models for Conversational Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we report cpWER on Fisher for different amounts of adaptation data, both from Fisher training set and from synthetic approaches.
For modest amounts of data (less than 5 h) the proposed approach is competitive to using in-domain data; however, as the amount of adaptation data is scaled, performance saturates quickly: The improvement between 50 h and 5 h is marginal when compared to the one afforded by using in-domain data.
This trend is also observed for the other synthetic data approaches and suggests that there is some inherent mismatch in all of the synthetic data approaches tested that prevents effective scaling.
At least for Parakeet, results suggest that this mismatch seems to be more related to the signal/acoustic content rather than the transcription semantic content as the gap between using Fisher transcriptions and LLM-generated transcription is modest.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Multi-speaker ASR results on Fisher test set with different adaptation data.</figcaption>
<table id="S4.T1.11" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.11.12.1" class="ltx_tr">
<th id="S4.T1.11.12.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:2.1pt;padding-right:2.1pt;">Adaptation Data</th>
<th id="S4.T1.11.12.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" style="padding-left:2.1pt;padding-right:2.1pt;">amount</th>
<td id="S4.T1.11.12.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.1pt;padding-right:2.1pt;">cpWER</td>
<td id="S4.T1.11.12.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.1pt;padding-right:2.1pt;">MIMO-WER</td>
</tr>
<tr id="S4.T1.11.13.2" class="ltx_tr">
<th id="S4.T1.11.13.2.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;"></th>
<th id="S4.T1.11.13.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;">(hours)</th>
<td id="S4.T1.11.13.2.3" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">(%)</td>
<td id="S4.T1.11.13.2.4" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">(%)</td>
</tr>
<tr id="S4.T1.11.14.3" class="ltx_tr">
<th id="S4.T1.11.14.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">-</th>
<th id="S4.T1.11.14.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">0</th>
<td id="S4.T1.11.14.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">44.94</td>
<td id="S4.T1.11.14.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">26.15</td>
</tr>
<tr id="S4.T1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">Fisher</th>
<th id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="1960" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mn id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml">1960</mn><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><cn type="integer" id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">1960</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">1960</annotation></semantics></math></th>
<td id="S4.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">13.76</td>
<td id="S4.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">13.58</td>
</tr>
<tr id="S4.T1.2.2" class="ltx_tr">
<th id="S4.T1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;">Fisher</th>
<th id="S4.T1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;"><math id="S4.T1.2.2.1.m1.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S4.T1.2.2.1.m1.1a"><mn id="S4.T1.2.2.1.m1.1.1" xref="S4.T1.2.2.1.m1.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.1.m1.1b"><cn type="integer" id="S4.T1.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.1.m1.1c">80</annotation></semantics></math></th>
<td id="S4.T1.2.2.3" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">15.43</td>
<td id="S4.T1.2.2.4" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">14.94</td>
</tr>
<tr id="S4.T1.3.3" class="ltx_tr">
<th id="S4.T1.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">NeMo MSS</th>
<th id="S4.T1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><math id="S4.T1.3.3.1.m1.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S4.T1.3.3.1.m1.1a"><mn id="S4.T1.3.3.1.m1.1.1" xref="S4.T1.3.3.1.m1.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.1.m1.1b"><cn type="integer" id="S4.T1.3.3.1.m1.1.1.cmml" xref="S4.T1.3.3.1.m1.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.1.m1.1c">80</annotation></semantics></math></th>
<td id="S4.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">34.37</td>
<td id="S4.T1.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">26.51</td>
</tr>
<tr id="S4.T1.4.4" class="ltx_tr">
<th id="S4.T1.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;">xTTS (Fisher)</th>
<th id="S4.T1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;"><math id="S4.T1.4.4.1.m1.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S4.T1.4.4.1.m1.1a"><mn id="S4.T1.4.4.1.m1.1.1" xref="S4.T1.4.4.1.m1.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.1.m1.1b"><cn type="integer" id="S4.T1.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.1.m1.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.1.m1.1c">80</annotation></semantics></math></th>
<td id="S4.T1.4.4.3" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">24.88</td>
<td id="S4.T1.4.4.4" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">24.07</td>
</tr>
<tr id="S4.T1.6.6" class="ltx_tr">
<th id="S4.T1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;">xTTS (LLM<sub id="S4.T1.5.5.1.1" class="ltx_sub"><span id="S4.T1.5.5.1.1.1" class="ltx_text ltx_font_italic">rnd</span></sub>)</th>
<th id="S4.T1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;"><math id="S4.T1.6.6.2.m1.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S4.T1.6.6.2.m1.1a"><mn id="S4.T1.6.6.2.m1.1.1" xref="S4.T1.6.6.2.m1.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.2.m1.1b"><cn type="integer" id="S4.T1.6.6.2.m1.1.1.cmml" xref="S4.T1.6.6.2.m1.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.2.m1.1c">80</annotation></semantics></math></th>
<td id="S4.T1.6.6.3" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">34.65</td>
<td id="S4.T1.6.6.4" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">28.31</td>
</tr>
<tr id="S4.T1.7.7" class="ltx_tr">
<th id="S4.T1.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">Parakeet (Fisher)</th>
<th id="S4.T1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><math id="S4.T1.7.7.1.m1.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S4.T1.7.7.1.m1.1a"><mn id="S4.T1.7.7.1.m1.1.1" xref="S4.T1.7.7.1.m1.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.1.m1.1b"><cn type="integer" id="S4.T1.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.1.m1.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.1.m1.1c">80</annotation></semantics></math></th>
<td id="S4.T1.7.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">21.44</td>
<td id="S4.T1.7.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">21.00</td>
</tr>
<tr id="S4.T1.9.9" class="ltx_tr">
<th id="S4.T1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;">Parakeet (LLM<sub id="S4.T1.8.8.1.1" class="ltx_sub"><span id="S4.T1.8.8.1.1.1" class="ltx_text ltx_font_italic">rnd</span></sub>)</th>
<th id="S4.T1.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;"><math id="S4.T1.9.9.2.m1.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S4.T1.9.9.2.m1.1a"><mn id="S4.T1.9.9.2.m1.1.1" xref="S4.T1.9.9.2.m1.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.2.m1.1b"><cn type="integer" id="S4.T1.9.9.2.m1.1.1.cmml" xref="S4.T1.9.9.2.m1.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.2.m1.1c">80</annotation></semantics></math></th>
<td id="S4.T1.9.9.3" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">20.41</td>
<td id="S4.T1.9.9.4" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">19.48</td>
</tr>
<tr id="S4.T1.11.11" class="ltx_tr">
<th id="S4.T1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:2.1pt;padding-right:2.1pt;">Parakeet (LLM<sub id="S4.T1.10.10.1.1" class="ltx_sub"><span id="S4.T1.10.10.1.1.1" class="ltx_text ltx_font_italic">rnd</span></sub>)</th>
<th id="S4.T1.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" style="padding-left:2.1pt;padding-right:2.1pt;"><math id="S4.T1.11.11.2.m1.1" class="ltx_Math" alttext="160" display="inline"><semantics id="S4.T1.11.11.2.m1.1a"><mn id="S4.T1.11.11.2.m1.1.1" xref="S4.T1.11.11.2.m1.1.1.cmml">160</mn><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.2.m1.1b"><cn type="integer" id="S4.T1.11.11.2.m1.1.1.cmml" xref="S4.T1.11.11.2.m1.1.1">160</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.2.m1.1c">160</annotation></semantics></math></th>
<td id="S4.T1.11.11.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.1pt;padding-right:2.1pt;">19.93</td>
<td id="S4.T1.11.11.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.1pt;padding-right:2.1pt;">19.45</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2408.09215/assets/fisher.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="580" height="465" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Multi-speaker ASR results on Fisher test set for different adaptation data sources and quantity.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Mixer 6 Speech</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In Table <a href="#S4.T2" title="Table 2 ‣ 4.3 Further discussion &amp; remarks ‣ 4 Experiments ‣ Generating Data with Text-to-Speech and Large-Language Models for Conversational Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we show results obtained on Mixer 6.
The trends observed are consistent with the Fisher experiments, despite the rather naive artificial reverberation strategy used for xTTS and Parakeet experiments.
This confirms that the proposed approach can also be effective for far-field multi-speaker synthetic data, at least when compared to the classical approach (NeMo MSS results) and when available in-domain data is very scarce (here 2:30 h).
Parakeet (LLM<sub id="S4.SS2.p1.1.1" class="ltx_sub"><span id="S4.SS2.p1.1.1.1" class="ltx_text ltx_font_italic">rnd</span></sub>, 80 h) also compares favorably with the third and fourth rows, where we report the results of using the Fisher full 1960 h training set and a 80 h subset respectively for adaptation. For these Fisher experiments, to reduce the mismatch due to the telephone lower sampling frequency, we apply telephone band-limiting to Mixer 6 in the inference phase. We also contaminate the Fisher 6 training data with reverberation as done for Parakeet and xTTS as described in Sec. <a href="#S3.SS3" title="3.3 ASR System ‣ 3 Experimental setup ‣ Generating Data with Text-to-Speech and Large-Language Models for Conversational Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Further discussion &amp; remarks</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Considering both Fisher and Mixer 6 experiments, the fact that Parakeet+LLM<sub id="S4.SS3.p1.1.1" class="ltx_sub"><span id="S4.SS3.p1.1.1.1" class="ltx_text ltx_font_italic">rnd</span></sub> improves considerably over NeMo MSS while xTTS fails suggests that turn-taking and para-linguistics may play a considerable role for multi-talker ASR.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.10" class="ltx_p">Finally, for both Mixer 6 Speech and Fisher scenarios, we tried using <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="50\,h" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">50</mn><mo lspace="0.170em" rspace="0em" id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><times id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">50</cn><ci id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">50\,h</annotation></semantics></math> of synthetic LLM<sub id="S4.SS3.p2.10.1" class="ltx_sub"><span id="S4.SS3.p2.10.1.1" class="ltx_text ltx_font_italic">rnd</span></sub> data to augment a portion of in-domain data (<math id="S4.SS3.p2.3.m3.1" class="ltx_Math" alttext="5\,h" display="inline"><semantics id="S4.SS3.p2.3.m3.1a"><mrow id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml"><mn id="S4.SS3.p2.3.m3.1.1.2" xref="S4.SS3.p2.3.m3.1.1.2.cmml">5</mn><mo lspace="0.170em" rspace="0em" id="S4.SS3.p2.3.m3.1.1.1" xref="S4.SS3.p2.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS3.p2.3.m3.1.1.3" xref="S4.SS3.p2.3.m3.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1"><times id="S4.SS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1.1"></times><cn type="integer" id="S4.SS3.p2.3.m3.1.1.2.cmml" xref="S4.SS3.p2.3.m3.1.1.2">5</cn><ci id="S4.SS3.p2.3.m3.1.1.3.cmml" xref="S4.SS3.p2.3.m3.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">5\,h</annotation></semantics></math> and <math id="S4.SS3.p2.4.m4.1" class="ltx_Math" alttext="50\,h" display="inline"><semantics id="S4.SS3.p2.4.m4.1a"><mrow id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml"><mn id="S4.SS3.p2.4.m4.1.1.2" xref="S4.SS3.p2.4.m4.1.1.2.cmml">50</mn><mo lspace="0.170em" rspace="0em" id="S4.SS3.p2.4.m4.1.1.1" xref="S4.SS3.p2.4.m4.1.1.1.cmml">​</mo><mi id="S4.SS3.p2.4.m4.1.1.3" xref="S4.SS3.p2.4.m4.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><apply id="S4.SS3.p2.4.m4.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1"><times id="S4.SS3.p2.4.m4.1.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1.1"></times><cn type="integer" id="S4.SS3.p2.4.m4.1.1.2.cmml" xref="S4.SS3.p2.4.m4.1.1.2">50</cn><ci id="S4.SS3.p2.4.m4.1.1.3.cmml" xref="S4.SS3.p2.4.m4.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">50\,h</annotation></semantics></math>) by mixing the two or by training on synthetic data and then fine-tuning on in-domain data.
However, in most instances, this approach does not result in any improvement over using solely the in-domain data; in the xTTS and NeMo MSS cases we even observe performance degradation.
For example, by combining <math id="S4.SS3.p2.5.m5.1" class="ltx_Math" alttext="50\,h" display="inline"><semantics id="S4.SS3.p2.5.m5.1a"><mrow id="S4.SS3.p2.5.m5.1.1" xref="S4.SS3.p2.5.m5.1.1.cmml"><mn id="S4.SS3.p2.5.m5.1.1.2" xref="S4.SS3.p2.5.m5.1.1.2.cmml">50</mn><mo lspace="0.170em" rspace="0em" id="S4.SS3.p2.5.m5.1.1.1" xref="S4.SS3.p2.5.m5.1.1.1.cmml">​</mo><mi id="S4.SS3.p2.5.m5.1.1.3" xref="S4.SS3.p2.5.m5.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.5.m5.1b"><apply id="S4.SS3.p2.5.m5.1.1.cmml" xref="S4.SS3.p2.5.m5.1.1"><times id="S4.SS3.p2.5.m5.1.1.1.cmml" xref="S4.SS3.p2.5.m5.1.1.1"></times><cn type="integer" id="S4.SS3.p2.5.m5.1.1.2.cmml" xref="S4.SS3.p2.5.m5.1.1.2">50</cn><ci id="S4.SS3.p2.5.m5.1.1.3.cmml" xref="S4.SS3.p2.5.m5.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.5.m5.1c">50\,h</annotation></semantics></math> of Parakeet (LLM<sub id="S4.SS3.p2.10.2" class="ltx_sub"><span id="S4.SS3.p2.10.2.1" class="ltx_text ltx_font_italic">rnd</span></sub>) and <math id="S4.SS3.p2.7.m7.1" class="ltx_Math" alttext="50\,h" display="inline"><semantics id="S4.SS3.p2.7.m7.1a"><mrow id="S4.SS3.p2.7.m7.1.1" xref="S4.SS3.p2.7.m7.1.1.cmml"><mn id="S4.SS3.p2.7.m7.1.1.2" xref="S4.SS3.p2.7.m7.1.1.2.cmml">50</mn><mo lspace="0.170em" rspace="0em" id="S4.SS3.p2.7.m7.1.1.1" xref="S4.SS3.p2.7.m7.1.1.1.cmml">​</mo><mi id="S4.SS3.p2.7.m7.1.1.3" xref="S4.SS3.p2.7.m7.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.7.m7.1b"><apply id="S4.SS3.p2.7.m7.1.1.cmml" xref="S4.SS3.p2.7.m7.1.1"><times id="S4.SS3.p2.7.m7.1.1.1.cmml" xref="S4.SS3.p2.7.m7.1.1.1"></times><cn type="integer" id="S4.SS3.p2.7.m7.1.1.2.cmml" xref="S4.SS3.p2.7.m7.1.1.2">50</cn><ci id="S4.SS3.p2.7.m7.1.1.3.cmml" xref="S4.SS3.p2.7.m7.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.7.m7.1c">50\,h</annotation></semantics></math> of original Fisher training data the model achieved a cpWER of <math id="S4.SS3.p2.8.m8.1" class="ltx_Math" alttext="15.74" display="inline"><semantics id="S4.SS3.p2.8.m8.1a"><mn id="S4.SS3.p2.8.m8.1.1" xref="S4.SS3.p2.8.m8.1.1.cmml">15.74</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.8.m8.1b"><cn type="float" id="S4.SS3.p2.8.m8.1.1.cmml" xref="S4.SS3.p2.8.m8.1.1">15.74</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.8.m8.1c">15.74</annotation></semantics></math>% which is only marginally better than the <math id="S4.SS3.p2.9.m9.1" class="ltx_Math" alttext="16.36" display="inline"><semantics id="S4.SS3.p2.9.m9.1a"><mn id="S4.SS3.p2.9.m9.1.1" xref="S4.SS3.p2.9.m9.1.1.cmml">16.36</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.9.m9.1b"><cn type="float" id="S4.SS3.p2.9.m9.1.1.cmml" xref="S4.SS3.p2.9.m9.1.1">16.36</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.9.m9.1c">16.36</annotation></semantics></math>% obtained with only <math id="S4.SS3.p2.10.m10.1" class="ltx_Math" alttext="50\,h" display="inline"><semantics id="S4.SS3.p2.10.m10.1a"><mrow id="S4.SS3.p2.10.m10.1.1" xref="S4.SS3.p2.10.m10.1.1.cmml"><mn id="S4.SS3.p2.10.m10.1.1.2" xref="S4.SS3.p2.10.m10.1.1.2.cmml">50</mn><mo lspace="0.170em" rspace="0em" id="S4.SS3.p2.10.m10.1.1.1" xref="S4.SS3.p2.10.m10.1.1.1.cmml">​</mo><mi id="S4.SS3.p2.10.m10.1.1.3" xref="S4.SS3.p2.10.m10.1.1.3.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.10.m10.1b"><apply id="S4.SS3.p2.10.m10.1.1.cmml" xref="S4.SS3.p2.10.m10.1.1"><times id="S4.SS3.p2.10.m10.1.1.1.cmml" xref="S4.SS3.p2.10.m10.1.1.1"></times><cn type="integer" id="S4.SS3.p2.10.m10.1.1.2.cmml" xref="S4.SS3.p2.10.m10.1.1.2">50</cn><ci id="S4.SS3.p2.10.m10.1.1.3.cmml" xref="S4.SS3.p2.10.m10.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.10.m10.1c">50\,h</annotation></semantics></math> of Fisher (Figure <a href="#S4.F2" title="Figure 2 ‣ 4.1 Fisher ‣ 4 Experiments ‣ Generating Data with Text-to-Speech and Large-Language Models for Conversational Speech Recognition" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
Interestingly, negligible or no improvement was also observed when the in-domain data was more modest (5 h).
This may be due to the fact that we are leveraging a strong pre-trained model, and thus the quality of adaptation data rather than quantity matters most.
Future work should explore adaptation of the <span title="" class="ltx_glossaryref">TTS</span> model to generate synthetic audio that better matches distribution of in-domain data.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Multi-speaker ASR results on Mixer 6 Speech eval set with different adaptation data.</figcaption>
<table id="S4.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.3.4.1" class="ltx_tr">
<th id="S4.T2.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:2.1pt;padding-right:2.1pt;">Adaptation Data</th>
<th id="S4.T2.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" style="padding-left:2.1pt;padding-right:2.1pt;">amount</th>
<td id="S4.T2.3.4.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.1pt;padding-right:2.1pt;">cpWER</td>
<td id="S4.T2.3.4.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.1pt;padding-right:2.1pt;">MIMO-WER</td>
</tr>
<tr id="S4.T2.3.5.2" class="ltx_tr">
<th id="S4.T2.3.5.2.1" class="ltx_td ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;"></th>
<th id="S4.T2.3.5.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;">(hours)</th>
<td id="S4.T2.3.5.2.3" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">(%)</td>
<td id="S4.T2.3.5.2.4" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">(%)</td>
</tr>
<tr id="S4.T2.3.6.3" class="ltx_tr">
<th id="S4.T2.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">-</th>
<th id="S4.T2.3.6.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">0</th>
<td id="S4.T2.3.6.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">43.67</td>
<td id="S4.T2.3.6.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">32.16</td>
</tr>
<tr id="S4.T2.3.7.4" class="ltx_tr">
<th id="S4.T2.3.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">Mixer6</th>
<th id="S4.T2.3.7.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">2.30</th>
<td id="S4.T2.3.7.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">20.36</td>
<td id="S4.T2.3.7.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">19.77</td>
</tr>
<tr id="S4.T2.3.8.5" class="ltx_tr">
<th id="S4.T2.3.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;">Fisher</th>
<th id="S4.T2.3.8.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;">1960</th>
<td id="S4.T2.3.8.5.3" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">20.83</td>
<td id="S4.T2.3.8.5.4" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">20.33</td>
</tr>
<tr id="S4.T2.3.9.6" class="ltx_tr">
<th id="S4.T2.3.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;">Fisher</th>
<th id="S4.T2.3.9.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;">80</th>
<td id="S4.T2.3.9.6.3" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">22.12</td>
<td id="S4.T2.3.9.6.4" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">21.36</td>
</tr>
<tr id="S4.T2.3.10.7" class="ltx_tr">
<th id="S4.T2.3.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;">NeMo MSS</th>
<th id="S4.T2.3.10.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;">80</th>
<td id="S4.T2.3.10.7.3" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">36.71</td>
<td id="S4.T2.3.10.7.4" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">28.21</td>
</tr>
<tr id="S4.T2.3.11.8" class="ltx_tr">
<th id="S4.T2.3.11.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;">xTTS (Mixer6)</th>
<th id="S4.T2.3.11.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;">2.30</th>
<td id="S4.T2.3.11.8.3" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">25.99</td>
<td id="S4.T2.3.11.8.4" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">24.47</td>
</tr>
<tr id="S4.T2.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;">xTTS (LLM<sub id="S4.T2.1.1.1.1" class="ltx_sub"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_italic">rnd</span></sub>)</th>
<th id="S4.T2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;">80</th>
<td id="S4.T2.1.1.3" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">35.65</td>
<td id="S4.T2.1.1.4" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">30.18</td>
</tr>
<tr id="S4.T2.3.12.9" class="ltx_tr">
<th id="S4.T2.3.12.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">Parakeet (Mixer6)</th>
<th id="S4.T2.3.12.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">2.30</th>
<td id="S4.T2.3.12.9.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">23.52</td>
<td id="S4.T2.3.12.9.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;">22.82</td>
</tr>
<tr id="S4.T2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;">Parakeet (LLM<sub id="S4.T2.2.2.1.1" class="ltx_sub"><span id="S4.T2.2.2.1.1.1" class="ltx_text ltx_font_italic">rnd</span></sub>)</th>
<th id="S4.T2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;">2.30</th>
<td id="S4.T2.2.2.3" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">23.70</td>
<td id="S4.T2.2.2.4" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;">22.12</td>
</tr>
<tr id="S4.T2.3.3" class="ltx_tr">
<th id="S4.T2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:2.1pt;padding-right:2.1pt;">Parakeet (LLM<sub id="S4.T2.3.3.1.1" class="ltx_sub"><span id="S4.T2.3.3.1.1.1" class="ltx_text ltx_font_italic">rnd</span></sub>)</th>
<th id="S4.T2.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" style="padding-left:2.1pt;padding-right:2.1pt;">80</th>
<td id="S4.T2.3.3.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.1pt;padding-right:2.1pt;">21.25</td>
<td id="S4.T2.3.3.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.1pt;padding-right:2.1pt;">20.17</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we study the use of synthetically generated data for multi-speaker <span title="" class="ltx_glossaryref">ASR</span>, focusing on the two-speaker case.
We explore different strategies of generating synthetic data, comparing artificially overlapped data and <span title="" class="ltx_glossaryref">SotA</span> conventional <span title="" class="ltx_glossaryref">TTS</span> models with a novel conversational <span title="" class="ltx_glossaryref">TTS</span> model, Parakeet, capable of natively generating multi-speaker utterances.
Our results show that our approach using Parakeet significantly outperforms previous <span title="" class="ltx_glossaryref">SotA</span> multi-speaker simulation techniques.
Furthermore, when in-domain data is limited to only a few hours, our approach achieves performance reasonably close to that of using in-domain data; however, when more in-domain data is available, our approach lags behind using real data. For Mixer 6, our approach also obtains results comparable to using external real-world multi-speaker data (Fisher).
Overall, our experiments suggest that the LLM generated transcripts are reliable but that there is currently a performance gap compared to using in-domain audio data (when enough in-domain data exists).</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Limitations of our work include that we only consider two-speaker conversational speech, short 30-second conversations, and relatively high SNR scenarios. These constraints were primarily imposed by the current limitations of the Parakeet TTS model, and thus improvement of TTS capabilities is crucial to increasing synthetic data viability. For example, to tackle more complex noisy/reverberant scenarios, the TTS model needs to incorporate acoustic scenario modeling, e.g. via acoustic style transfer techniques or even few-shot adaptation on some in-domain data (e.g. via <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>). Another possible limitation is that Parakeet itself is trained on text-audio pairs where the text is ``synthetic'', i.e. Whisper-D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> is used to generate multi-speaker transcriptions for Spotify podcast audio which is then used for Parakeet training. Since Whisper-D is fine-tuned from Whisper using a small number of annotated multi-speaker examples (and Whisper itself is likely trained on a sizeable quantity of multi-speaker data), there is an indirect but somewhat circular dependency on the existence of ground-truth annotations. Also, Parakeet's weakness in generating consistent 3/4-speaker conversational data could in part be due to limitations of Whisper-D. Future work could potentially explore the joint bootstrapping of audio-to-text and text-to-audio models.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgments</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">S. Cornell was supported by IC Postdoctoral
Research Fellowship Program at CMU via ORISE through an agreement between U.S. DoE and ODNI.
We'd like to thank Google's TPU Research Cloud (TRC), which provided compute for generating synthetic Parakeet samples and Llama synthetic text utterances. Our work would not have been possible without their support.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:80%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:80%;">
A. Radford </span><em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib1.3.3" class="ltx_text" style="font-size:80%;">, ``Robust speech recognition via large-scale weak supervision,'' in </span><em id="bib.bib1.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">ICML</em><span id="bib.bib1.5.5" class="ltx_text" style="font-size:80%;">.   PMLR, 2023.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:80%;">
Y. Peng </span><em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib2.3.3" class="ltx_text" style="font-size:80%;">, ``Reproducing whisper-style training using an open-source toolkit and publicly available data,'' in </span><em id="bib.bib2.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of ASRU</em><span id="bib.bib2.5.5" class="ltx_text" style="font-size:80%;">.   IEEE, 2023.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:80%;">
N. Kanda </span><em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib3.3.3" class="ltx_text" style="font-size:80%;">, ``Large-scale pre-training of end-to-end multi-talker asr for meeting transcription with single distant microphone,'' </span><em id="bib.bib3.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:2103.16776</em><span id="bib.bib3.5.5" class="ltx_text" style="font-size:80%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:80%;">
A. Baevski </span><em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib4.3.3" class="ltx_text" style="font-size:80%;">, ``wav2vec 2.0: A framework for self-supervised learning of speech representations,'' </span><em id="bib.bib4.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Advances in neural information processing systems</em><span id="bib.bib4.5.5" class="ltx_text" style="font-size:80%;">, vol. 33, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:80%;">
W.-N. Hsu </span><em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib5.3.3" class="ltx_text" style="font-size:80%;">, ``Hubert: Self-supervised speech representation learning by masked prediction of hidden units,'' </span><em id="bib.bib5.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">IEEE/ACM TASLP</em><span id="bib.bib5.5.5" class="ltx_text" style="font-size:80%;">, vol. 29, 2021.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:80%;">
S. Chen </span><em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib6.3.3" class="ltx_text" style="font-size:80%;">, ``WavLM: Large-scale self-supervised pre-training for full stack speech processing,'' </span><em id="bib.bib6.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">IEEE Journal of Selected Topics in Signal Processing</em><span id="bib.bib6.5.5" class="ltx_text" style="font-size:80%;">, vol. 16, no. 6, 2022.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:80%;">
S. Watanabe </span><em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib7.3.3" class="ltx_text" style="font-size:80%;">, ``CHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,'' in </span><em id="bib.bib7.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">CHiME Workshop</em><span id="bib.bib7.5.5" class="ltx_text" style="font-size:80%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:80%;">
S. Cornell </span><em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib8.3.3" class="ltx_text" style="font-size:80%;">, ``The CHiME-7 DASR challenge: Distant meeting transcription with multiple devices in diverse scenarios,'' </span><em id="bib.bib8.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">CHiME Workshop</em><span id="bib.bib8.5.5" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:80%;">
N. Ryant </span><em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib9.3.3" class="ltx_text" style="font-size:80%;">, ``The third dihard diarization challenge,'' </span><em id="bib.bib9.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of Interspeech</em><span id="bib.bib9.5.5" class="ltx_text" style="font-size:80%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:80%;">
Y. Fujita </span><em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib10.3.3" class="ltx_text" style="font-size:80%;">, ``End-to-end neural speaker diarization with self-attention,'' in </span><em id="bib.bib10.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of ASRU</em><span id="bib.bib10.5.5" class="ltx_text" style="font-size:80%;">.   IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:80%;">
K. Kinoshita, M. Delcroix, and N. Tawara, ``Integrating end-to-end neural and clustering-based diarization: Getting the best of both worlds,'' in </span><em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of ICASSP</em><span id="bib.bib11.3.3" class="ltx_text" style="font-size:80%;">.   IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:80%;">
F. Landini </span><em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib12.3.3" class="ltx_text" style="font-size:80%;">, ``From simulated mixtures to simulated conversations as training data for end-to-end neural diarization,'' 2022.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:80%;">
I. Medennikov </span><em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib13.3.3" class="ltx_text" style="font-size:80%;">, ``Target-speaker voice activity detection: a novel approach for multi-speaker diarization in a dinner party scenario,'' </span><em id="bib.bib13.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of Interspeech</em><span id="bib.bib13.5.5" class="ltx_text" style="font-size:80%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:80%;">
N. Tawara </span><em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib14.3.3" class="ltx_text" style="font-size:80%;">, ``Ntt speaker diarization system for chime-7: multi-domain, multi-microphone end-to-end and vector clustering diarization,'' </span><em id="bib.bib14.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">CHiME Workshop</em><span id="bib.bib14.5.5" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:80%;">
N. Kanda </span><em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib15.3.3" class="ltx_text" style="font-size:80%;">, ``Serialized output training for end-to-end overlapped speech recognition,'' </span><em id="bib.bib15.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of Interspeech</em><span id="bib.bib15.5.5" class="ltx_text" style="font-size:80%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:80%;">
——, ``Investigation of end-to-end speaker-attributed asr for continuous multi-talker recordings,'' in </span><em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of SLT</em><span id="bib.bib16.3.3" class="ltx_text" style="font-size:80%;">.   IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:80%;">
Z. Huang </span><em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib17.3.3" class="ltx_text" style="font-size:80%;">, ``Adapting self-supervised models to multi-talker speech recognition using speaker embeddings,'' in </span><em id="bib.bib17.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of ICASSP</em><span id="bib.bib17.5.5" class="ltx_text" style="font-size:80%;">.   IEEE, 2023.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:80%;">
S. Cornell </span><em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib18.3.3" class="ltx_text" style="font-size:80%;">, ``One model to rule them all? towards end-to-end joint speaker diarization and speech recognition,'' in </span><em id="bib.bib18.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of ICASSP</em><span id="bib.bib18.5.5" class="ltx_text" style="font-size:80%;">.   IEEE, 2024.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:80%;">
C. Li </span><em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib19.3.3" class="ltx_text" style="font-size:80%;">, ``Adapting multi-lingual asr models for handling multiple talkers,'' </span><em id="bib.bib19.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of Interspeech</em><span id="bib.bib19.5.5" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:80%;">
T. Cord-Landwehr </span><em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib20.3.3" class="ltx_text" style="font-size:80%;">, ``Mms-msg: A multi-purpose multi-speaker mixture signal generator,'' in </span><em id="bib.bib20.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of IWAENC</em><span id="bib.bib20.5.5" class="ltx_text" style="font-size:80%;">.   IEEE, 2022.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:80%;">
T. J. Park </span><em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib21.3.3" class="ltx_text" style="font-size:80%;">, ``Property-aware multi-speaker data simulation: A probabilistic modelling technique for synthetic data generation,'' </span><em id="bib.bib21.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of Interspeech</em><span id="bib.bib21.5.5" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:80%;">
A. Rosenberg </span><em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib22.3.3" class="ltx_text" style="font-size:80%;">, ``Speech recognition with augmented synthesized speech,'' in </span><em id="bib.bib22.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of ASRU</em><span id="bib.bib22.5.5" class="ltx_text" style="font-size:80%;">.   IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:80%;">
Z. Chen </span><em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib23.3.3" class="ltx_text" style="font-size:80%;">, ``Improving speech recognition using gan-based speech synthesis and contrastive unspoken text selection.'' in </span><em id="bib.bib23.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of Interspeech</em><span id="bib.bib23.5.5" class="ltx_text" style="font-size:80%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:80%;">
N. Rossenbach </span><em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib24.3.3" class="ltx_text" style="font-size:80%;">, ``Generating synthetic audio data for attention-based speech recognition systems,'' in </span><em id="bib.bib24.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of ICASSP</em><span id="bib.bib24.5.5" class="ltx_text" style="font-size:80%;">.   IEEE, 2020.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:80%;">
A. Tjandra, S. Sakti, and S. Nakamura, ``Machine speech chain,'' </span><em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">IEEE/ACM TASLP</em><span id="bib.bib25.3.3" class="ltx_text" style="font-size:80%;">, vol. 28, 2020.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:80%;">
A. Fazel </span><em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib26.3.3" class="ltx_text" style="font-size:80%;">, ``SynthASR: Unlocking synthetic data for speech recognition,'' </span><em id="bib.bib26.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of Interspeech</em><span id="bib.bib26.5.5" class="ltx_text" style="font-size:80%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:80%;">
X. Zheng </span><em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib27.3.3" class="ltx_text" style="font-size:80%;">, ``Using synthetic audio to improve the recognition of out-of-vocabulary words in end-to-end asr systems,'' in </span><em id="bib.bib27.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of ICASSP</em><span id="bib.bib27.5.5" class="ltx_text" style="font-size:80%;">.   IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:80%;">
S. Ueno </span><em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib28.3.3" class="ltx_text" style="font-size:80%;">, ``Data augmentation for asr using tts via a discrete representation,'' in </span><em id="bib.bib28.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of ASRU</em><span id="bib.bib28.5.5" class="ltx_text" style="font-size:80%;">.   IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:80%;">
T.-Y. Hu </span><em id="bib.bib29.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib29.3.3" class="ltx_text" style="font-size:80%;">, ``Synt++: Utilizing imperfect synthetic data to improve speech recognition,'' in </span><em id="bib.bib29.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of ICASSP</em><span id="bib.bib29.5.5" class="ltx_text" style="font-size:80%;">.   IEEE, 2022.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:80%;">
M. Soleymanpour </span><em id="bib.bib30.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib30.3.3" class="ltx_text" style="font-size:80%;">, ``Synthesizing dysarthric speech using multi-speaker tts for dysarthric speech recognition,'' in </span><em id="bib.bib30.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of ICASSP</em><span id="bib.bib30.5.5" class="ltx_text" style="font-size:80%;">.   IEEE, 2022.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:80%;">
E. Casanova </span><em id="bib.bib31.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib31.3.3" class="ltx_text" style="font-size:80%;">, ``Asr data augmentation in low-resource settings using cross-lingual multi-speaker tts and cross-lingual voice conversion,'' in </span><em id="bib.bib31.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of Interspeech</em><span id="bib.bib31.5.5" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:80%;">
T. Hori </span><em id="bib.bib32.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib32.3.3" class="ltx_text" style="font-size:80%;">, ``Cycle-consistency training for end-to-end speech recognition,'' in </span><em id="bib.bib32.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of ICASSP</em><span id="bib.bib32.5.5" class="ltx_text" style="font-size:80%;">.   IEEE, 2019.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:80%;">
M. K. Baskar </span><em id="bib.bib33.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib33.3.3" class="ltx_text" style="font-size:80%;">, ``Eat: Enhanced asr-tts for self-supervised speech recognition,'' in </span><em id="bib.bib33.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of ICASSP</em><span id="bib.bib33.5.5" class="ltx_text" style="font-size:80%;">.   IEEE, 2021.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:80%;">
J.-w. Jung </span><em id="bib.bib34.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib34.3.3" class="ltx_text" style="font-size:80%;">, ``Augsumm: towards generalizable speech summarization using synthetic labels from large language model,'' </span><em id="bib.bib34.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of ICASSP</em><span id="bib.bib34.5.5" class="ltx_text" style="font-size:80%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:80%;">
S.-L. Wu </span><em id="bib.bib35.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib35.3.3" class="ltx_text" style="font-size:80%;">, ``Improving audio captioning models with fine-grained audio features, text embedding supervision, and llm mix-up augmentation,'' in </span><em id="bib.bib35.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of ICASSP</em><span id="bib.bib35.5.5" class="ltx_text" style="font-size:80%;">.   IEEE, 2024.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:80%;">
C. Cieri, D. Miller, and K. Walker, ``The Fisher corpus: A resource for the next generations of speech-to-text.'' in </span><em id="bib.bib36.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">LREC</em><span id="bib.bib36.3.3" class="ltx_text" style="font-size:80%;">, 2004.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:80%;">
L. Brandschain </span><em id="bib.bib37.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib37.3.3" class="ltx_text" style="font-size:80%;">, ``The Mixer 6 corpus: Resources for cross-channel and text independent speaker recognition,'' in </span><em id="bib.bib37.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">LREC</em><span id="bib.bib37.5.5" class="ltx_text" style="font-size:80%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:80%;">
J. Darefsky, G. Zhu, and Z. Duan, ``Parakeet,'' 2024. [Online]. Available: </span><a target="_blank" href="https://jordandarefsky.com/blog/2024/parakeet/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:80%;">https://jordandarefsky.com/blog/2024/parakeet/</a><span id="bib.bib38.2.2" class="ltx_text" style="font-size:80%;">
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:80%;">
A. Clifton </span><em id="bib.bib39.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib39.3.3" class="ltx_text" style="font-size:80%;">, ``100,000 podcasts: A spoken english document corpus,'' in </span><em id="bib.bib39.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proceedings of the 28th International Conference on Computational Linguistics</em><span id="bib.bib39.5.5" class="ltx_text" style="font-size:80%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:80%;">
Z. Liu </span><em id="bib.bib40.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib40.3.3" class="ltx_text" style="font-size:80%;">, ``Autoregressive diffusion transformer for text-to-speech synthesis,'' </span><em id="bib.bib40.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:2406.05551</em><span id="bib.bib40.5.5" class="ltx_text" style="font-size:80%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:80%;">
G. Morrone </span><em id="bib.bib41.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib41.3.3" class="ltx_text" style="font-size:80%;">, ``End-to-end integration of speech separation and voice activity detection for low-latency diarization of telephone conversations,'' </span><em id="bib.bib41.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Speech Communication</em><span id="bib.bib41.5.5" class="ltx_text" style="font-size:80%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:80%;">
V. Panayotov </span><em id="bib.bib42.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib42.3.3" class="ltx_text" style="font-size:80%;">, ``Librispeech: an asr corpus based on public domain audio books,'' in </span><em id="bib.bib42.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of ICASSP</em><span id="bib.bib42.5.5" class="ltx_text" style="font-size:80%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:80%;">
E. Casanova </span><em id="bib.bib43.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib43.3.3" class="ltx_text" style="font-size:80%;">, ``Xtts: a massively multilingual zero-shot text-to-speech model,'' </span><em id="bib.bib43.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv e-prints</em><span id="bib.bib43.5.5" class="ltx_text" style="font-size:80%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:80%;">
S. Team, ``Silero vad: pre-trained enterprise-grade voice activity detector (vad), number detector and language classifier,'' </span><a target="_blank" href="https://github.com/snakers4/silero-vad" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:80%;">https://github.com/snakers4/silero-vad</a><span id="bib.bib44.2.2" class="ltx_text" style="font-size:80%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:80%;">
E. J. Hu </span><em id="bib.bib45.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib45.3.3" class="ltx_text" style="font-size:80%;">, ``Lora: Low-rank adaptation of large language models,'' </span><em id="bib.bib45.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">arXiv preprint arXiv:2106.09685</em><span id="bib.bib45.5.5" class="ltx_text" style="font-size:80%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:80%;">
T. Ko </span><em id="bib.bib46.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib46.3.3" class="ltx_text" style="font-size:80%;">, ``A study on data augmentation of reverberant speech for robust speech recognition,'' in </span><em id="bib.bib46.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proc. of ICASSP</em><span id="bib.bib46.5.5" class="ltx_text" style="font-size:80%;">.   IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:80%;">
T. von Neumann </span><em id="bib.bib47.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">et al.</em><span id="bib.bib47.3.3" class="ltx_text" style="font-size:80%;">, ``MeetEval: A toolkit for computation of word error rates for meeting transcription systems,'' </span><em id="bib.bib47.4.4" class="ltx_emph ltx_font_italic" style="font-size:80%;">CHiME Workshop</em><span id="bib.bib47.5.5" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:80%;">
L. Zhang, A. Rao, and M. Agrawala, ``Adding conditional control to text-to-image diffusion models,'' in </span><em id="bib.bib48.2.2" class="ltx_emph ltx_font_italic" style="font-size:80%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib48.3.3" class="ltx_text" style="font-size:80%;">, 2023.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.09213" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.09215" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.09215">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.09215" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.09216" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 17:42:33 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
